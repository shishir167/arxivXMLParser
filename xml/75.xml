<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:44:49Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|74001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01566</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01566</id><created>2015-03-05</created><authors><author><keyname>Tataria</keyname><forenames>Harsh</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author></authors><title>Coordinated Two-Tier Heterogeneous Cellular Networks with Leakage Based
  Beamforming</title><categories>cs.IT math.IT</categories><comments>7 pages, 8 figures, submitted to IEEE International Conference on
  Communications (ICC) 4th International Workshop on Small Cells and 5G
  (SmallNets), London, UK, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate the rate gains achieved by two-tier
heterogeneous cellular networks (HetNets) with varying degrees of coordination
between macrocell and microcell base stations (BSs). We show that without the
presence of coordination, network densification does not provide any gain in
the sum rate and rapidly decreases the mean per-user
signal-to-interference-plus-noise-ratio (SINR). Our results show that
coordination reduces the rate of SINR decay with increasing numbers of
microcell BSs in the system. Validity of the analytically approximated mean
per-user SINR over a wide range of signal-to-noise-ratio (SNR) is demonstrated
via comparison with the simulated results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01570</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01570</id><created>2015-03-05</created><authors><author><keyname>Hillion</keyname><forenames>Erwan</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>A proof of the Shepp-Olkin entropy concavity conjecture</title><categories>math.PR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the Shepp--Olkin conjecture, which states that the entropy of the
sum of independent Bernoulli random variables is concave in the parameters of
the individual random variables. Our proof is a refinement of an argument
previously presented by the same authors, which resolved the conjecture in the
monotonic case (where all the parameters are simultaneously increasing). In
fact, we show that the monotonic case is the worst case, using a careful
analysis of concavity properties of the derivatives of the probability mass
function. We propose a generalization of Shepp and Olkin's original conjecture,
to consider Renyi and Tsallis entropies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01578</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01578</id><created>2015-03-05</created><updated>2015-06-05</updated><authors><author><keyname>Chun</keyname><forenames>Sanghyuk</forenames></author><author><keyname>Noh</keyname><forenames>Yung-Kyun</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author></authors><title>Scalable Iterative Algorithm for Robust Subspace Clustering</title><categories>cs.DS cs.LG</categories><comments>This paper has been withdrawn by the author due to an error in the
  initialization section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering (SC) is a popular method for dimensionality reduction of
high-dimensional data, where it generalizes Principal Component Analysis (PCA).
Recently, several methods have been proposed to enhance the robustness of PCA
and SC, while most of them are computationally very expensive, in particular,
for high dimensional large-scale data. In this paper, we develop much faster
iterative algorithms for SC, incorporating robustness using a {\em non-squared}
$\ell_2$-norm objective. The known implementations for optimizing the objective
would be costly due to the alternative optimization of two separate objectives:
optimal cluster-membership assignment and robust subspace selection, while the
substitution of one process to a faster surrogate can cause failure in
convergence. To address the issue, we use a simplified procedure requiring
efficient matrix-vector multiplications for subspace update instead of solving
an expensive eigenvector problem at each iteration, in addition to release
nested robust PCA loops. We prove that the proposed algorithm monotonically
converges to a local minimum with approximation guarantees, e.g., it achieves
2-approximation for the robust PCA objective. In our experiments, the proposed
algorithm is shown to converge at an order of magnitude faster than known
algorithms optimizing the same objective, and have outperforms prior subspace
clustering methods in accuracy and running time for MNIST dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01588</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01588</id><created>2015-03-05</created><updated>2015-05-04</updated><authors><author><keyname>Goldwasser</keyname><forenames>Shafi</forenames></author><author><keyname>Kalai</keyname><forenames>Yael Tauman</forenames></author><author><keyname>Park</keyname><forenames>Sunoo</forenames></author></authors><title>Adaptively Secure Coin-Flipping, Revisited</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The full-information model was introduced by Ben-Or and Linial in 1985 to
study collective coin-flipping: the problem of generating a common bounded-bias
bit in a network of $n$ players with $t=t(n)$ faults. They showed that the
majority protocol can tolerate $t=O(\sqrt n)$ adaptive corruptions, and
conjectured that this is optimal in the adaptive setting. Lichtenstein, Linial,
and Saks proved that the conjecture holds for protocols in which each player
sends a single bit. Their result has been the main progress on the conjecture
in the last 30 years.
  In this work we revisit this question and ask: what about protocols involving
longer messages? Can increased communication allow for a larger fraction of
faulty players?
  We introduce a model of strong adaptive corruptions, where in each round, the
adversary sees all messages sent by honest parties and, based on the message
content, decides whether to corrupt a party (and intercept his message) or not.
We prove that any one-round coin-flipping protocol, regardless of message
length, is secure against at most $\tilde{O}(\sqrt n)$ strong adaptive
corruptions. Thus, increased message length does not help in this setting.
  We then shed light on the connection between adaptive and strongly adaptive
adversaries, by proving that for any symmetric one-round coin-flipping protocol
secure against $t$ adaptive corruptions, there is a symmetric one-round
coin-flipping protocol secure against $t$ strongly adaptive corruptions.
Returning to the standard adaptive model, we can now prove that any symmetric
one-round protocol with arbitrarily long messages can tolerate at most
$\tilde{O}(\sqrt n)$ adaptive corruptions.
  At the heart of our results lies a novel use of the Minimax Theorem and a new
technique for converting any one-round secure protocol into a protocol with
messages of $polylog(n)$ bits. This technique may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01592</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01592</id><created>2015-03-05</created><updated>2015-06-08</updated><authors><author><keyname>Hamann</keyname><forenames>Matthias</forenames></author><author><keyname>Wei&#xdf;auer</keyname><forenames>Daniel</forenames></author></authors><title>Bounding connected tree-width</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diestel and M\&quot;uller showed that the connected tree-width of a graph $G$,
i.e., the minimum width of any tree-decomposition with connected parts, can be
bounded in terms of the tree-width of $G$ and the largest length of a geodesic
cycle in $G$. We improve their bound to one that is of correct order of
magnitude. Finally, we construct a graph whose connected tree-width exceeds the
connected order of any of its brambles. This disproves a conjecture by Diestel
and M\&quot;uller asserting an analogue of tree-width duality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01596</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01596</id><created>2015-03-05</created><updated>2015-03-09</updated><authors><author><keyname>Ahn</keyname><forenames>Sungjin</forenames></author><author><keyname>Korattikara</keyname><forenames>Anoop</forenames></author><author><keyname>Liu</keyname><forenames>Nathan</forenames></author><author><keyname>Rajan</keyname><forenames>Suju</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Large-Scale Distributed Bayesian Matrix Factorization using Stochastic
  Gradient MCMC</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite having various attractive qualities such as high prediction accuracy
and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix
Factorization has not been widely adopted because of the prohibitive cost of
inference. In this paper, we propose a scalable distributed Bayesian matrix
factorization algorithm using stochastic gradient MCMC. Our algorithm, based on
Distributed Stochastic Gradient Langevin Dynamics, can not only match the
prediction accuracy of standard MCMC methods like Gibbs sampling, but at the
same time is as fast and simple as stochastic gradient descent. In our
experiments, we show that our algorithm can achieve the same level of
prediction accuracy as Gibbs sampling an order of magnitude faster. We also
show that our method reduces the prediction error as fast as distributed
stochastic gradient descent, achieving a 4.1% improvement in RMSE for the
Netflix dataset and an 1.8% for the Yahoo music dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01603</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01603</id><created>2015-03-05</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author><author><keyname>Bareinboim</keyname><forenames>Elias</forenames></author></authors><title>External Validity: From Do-Calculus to Transportability Across
  Populations</title><categories>stat.ME cs.AI</categories><comments>Published in at http://dx.doi.org/10.1214/14-STS486 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org). arXiv admin note: text overlap with
  arXiv:1312.7485</comments><proxy>vtex</proxy><report-no>IMS-STS-STS486</report-no><journal-ref>Statistical Science 2014, Vol. 29, No. 4, 579-595</journal-ref><doi>10.1214/14-STS486</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalizability of empirical findings to new environments, settings or
populations, often called &quot;external validity,&quot; is essential in most scientific
explorations. This paper treats a particular problem of generalizability,
called &quot;transportability,&quot; defined as a license to transfer causal effects
learned in experimental studies to a new population, in which only
observational studies can be conducted. We introduce a formal representation
called &quot;selection diagrams&quot; for expressing knowledge about differences and
commonalities between populations of interest and, using this representation,
we reduce questions of transportability to symbolic derivations in the
do-calculus. This reduction yields graph-based procedures for deciding, prior
to observing any data, whether causal effects in the target population can be
inferred from experimental findings in the study population. When the answer is
affirmative, the procedures identify what experimental and observational
findings need be obtained from the two populations, and how they can be
combined to ensure bias-free transport.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01604</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01604</id><created>2015-03-05</created><authors><author><keyname>Jaffke</keyname><forenames>Lars</forenames></author><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author></authors><title>MSOL-Definability Equals Recognizability for Halin Graphs and Bounded
  Degree $k$-Outerplanar Graphs</title><categories>cs.LO math.CO</categories><comments>39 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most famous algorithmic meta-theorems states that every graph
property that can be defined by a sentence in counting monadic second order
logic (CMSOL) can be checked in linear time for graphs of bounded treewidth,
which is known as Courcelle's Theorem. These algorithms are constructed as
finite state tree automata, and hence every CMSOL-definable graph property is
recognizable. Courcelle also conjectured that the converse holds, i.e. every
recognizable graph property is definable in CMSOL for graphs of bounded
treewidth. We prove this conjecture for a number of special cases in a stronger
form. That is, we show that each recognizable property is definable in MSOL,
i.e. the counting operation is not needed in our expressions. We give proofs
for Halin graphs, bounded degree $k$-outerplanar graphs and some related graph
classes. We furthermore show that the conjecture holds for any graph class that
admits tree decompositions that can be defined in MSOL, thus providing a useful
tool for future proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01613</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01613</id><created>2015-03-05</created><updated>2015-04-02</updated><authors><author><keyname>Bennett</keyname><forenames>Patrick</forenames></author><author><keyname>Bonacina</keyname><forenames>Ilario</forenames></author><author><keyname>Galesi</keyname><forenames>Nicola</forenames></author><author><keyname>Huynh</keyname><forenames>Tony</forenames></author><author><keyname>Molloy</keyname><forenames>Mike</forenames></author><author><keyname>Wollan</keyname><forenames>Paul</forenames></author></authors><title>Space proof complexity for random 3-CNFs</title><categories>cs.CC math.CO math.PR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.1619</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the space complexity of refuting $3$-CNFs in Resolution and
algebraic systems. We prove that every Polynomial Calculus with Resolution
refutation of a random $3$-CNF $\phi$ in $n$ variables requires, with high
probability, $\Omega(n)$ distinct monomials to be kept simultaneously in
memory. The same construction also proves that every Resolution refutation
$\phi$ requires, with high probability, $\Omega(n)$ clauses each of width
$\Omega(n)$ to be kept at the same time in memory. This gives a $\Omega(n^2)$
lower bound for the total space needed in Resolution to refute $\phi$. These
results are best possible (up to a constant factor).
  The main technical innovation is a variant of Hall's Lemma. We show that in
bipartite graphs $G$ with bipartition $(L,R)$ and left-degree at most 3, $L$
can be covered by certain families of disjoint paths, called VW-matchings,
provided that $L$ expands in $R$ by a factor of $(2-\epsilon)$, for $\epsilon &lt;
1/23$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01620</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01620</id><created>2015-03-05</created><updated>2015-07-13</updated><authors><author><keyname>Abdoli</keyname><forenames>Mohsen</forenames></author><author><keyname>Sarikhani</keyname><forenames>Hossein</forenames></author><author><keyname>Ghanbari</keyname><forenames>Mohammad</forenames></author><author><keyname>Brault</keyname><forenames>Patrice</forenames></author></authors><title>Gaussian Mixture Model Based Contrast Enhancement</title><categories>cs.MM</categories><journal-ref>Image Processing, IET, Vol. 9, No. 7, pp. 569-577, 2015</journal-ref><doi>10.1049/iet-ipr.2014.0583</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method for enhancing low contrast images is proposed. This
method, called Gaussian Mixture Model based Contrast Enhancement (GMMCE),
brings into play the Gaussian mixture modeling of histograms to model the
content of the images. Based on the fact that each homogeneous area in natural
images has a Gaussian-shaped histogram, it decomposes the narrow histogram of
low contrast images into a set of scaled and shifted Gaussians. The individual
histograms are then stretched by increasing their variance parameters, and are
diffused on the entire histogram by scattering their mean parameters, to build
a broad version of the histogram. The number of Gaussians as well as their
parameters are optimized to set up a GMM with lowest approximation error and
highest similarity to the original histogram. Compared to the existing
histogram-based methods, the experimental results show that the quality of
GMMCE enhanced pictures are mostly consistent and outperform other benchmark
methods. Additionally, the computational complexity analysis show that GMMCE is
a low complexity method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01624</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01624</id><created>2015-03-05</created><authors><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author><author><keyname>Danek</keyname><forenames>Agnieszka</forenames></author><author><keyname>Niemiec</keyname><forenames>Marcin</forenames></author></authors><title>GDC 2: Compression of large collections of genomes</title><categories>cs.DS cs.CE q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fall of prices of the high-throughput genome sequencing changes the
landscape of modern genomics. A number of large scale projects aimed at
sequencing many human genomes are in progress. Genome sequencing also becomes
an important aid in the personalized medicine. One of the significant side
effects of this change is a necessity of storage and transfer of huge amounts
of genomic data. In this paper we deal with the problem of compression of large
collections of complete genomic sequences. We propose an algorithm that is able
to compress the collection of 1092 human diploid genomes about 9,500 times.
This result is about 4 times better than what is offered by the other existing
compressors. Moreover, our algorithm is very fast as it processes the data with
speed 200MB/s on a modern workstation. In a consequence the proposed algorithm
allows storing the complete genomic collections at low cost, e.g., the examined
collection of 1092 human genomes needs only about 700MB when compressed, what
can be compared to about 6.7 TB of uncompressed FASTA files. The source code is
available at
http://sun.aei.polsl.pl/REFRESH/index.php?page=projects&amp;project=gdc&amp;subpage=about.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01626</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01626</id><created>2015-03-05</created><updated>2016-01-14</updated><authors><author><keyname>Nussinov</keyname><forenames>Z.</forenames></author><author><keyname>Ronhovde</keyname><forenames>P.</forenames></author><author><keyname>Hu</keyname><forenames>Dandan</forenames></author><author><keyname>Chakrabarty</keyname><forenames>S.</forenames></author><author><keyname>Sahu</keyname><forenames>M.</forenames></author><author><keyname>Sun</keyname><forenames>Bo</forenames></author><author><keyname>Mauro</keyname><forenames>N. A.</forenames></author><author><keyname>Sahu</keyname><forenames>K. K.</forenames></author></authors><title>Inference of hidden structures in complex physical systems by
  multi-scale clustering</title><categories>cond-mat.mtrl-sci cond-mat.stat-mech cs.CV physics.data-an</categories><comments>25 pages, 16 Figures; a review of earlier works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the application of a relatively new branch of statistical
physics--&quot;community detection&quot;-- to data mining. In particular, we focus on the
diagnosis of materials and automated image segmentation. Community detection
describes the quest of partitioning a complex system involving many elements
into optimally decoupled subsets or communities of such elements. We review a
multiresolution variant which is used to ascertain structures at different
spatial and temporal scales. Significant patterns are obtained by examining the
correlations between different independent solvers. Similar to other
combinatorial optimization problems in the NP complexity class, community
detection exhibits several phases. Typically, illuminating orders are revealed
by choosing parameters that lead to extremal information theory correlations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01628</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01628</id><created>2015-03-05</created><authors><author><keyname>Atminas</keyname><forenames>A.</forenames></author><author><keyname>Brignall</keyname><forenames>R.</forenames></author><author><keyname>Lozin</keyname><forenames>V.</forenames></author><author><keyname>Stacho</keyname><forenames>J.</forenames></author></authors><title>Minimal Classes of Graphs of Unbounded Clique-width and
  Well-quasi-ordering</title><categories>math.CO cs.DM</categories><comments>20 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Daligault, Rao and Thomass\'e proposed in 2010 a fascinating conjecture
connecting two seemingly unrelated notions: clique-width and
well-quasi-ordering. They asked if the clique-width of graphs in a hereditary
class which is well-quasi-ordered under labelled induced subgraphs is bounded
by a constant. This is equivalent to asking whether every hereditary class of
unbounded clique-width has a labelled infinite antichain. We believe the answer
to this question is positive and propose a stronger conjecture stating that
every minimal hereditary class of graphs of unbounded clique-width has a
canonical labelled infinite antichain. To date, only two hereditary classes are
known to be minimal with respect to clique-width and each of them is known to
contain a canonical antichain. In the present paper, we discover two more
minimal hereditary classes of unbounded clique-width and show that both of them
contain canonical antichains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01640</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01640</id><created>2015-03-05</created><updated>2015-05-18</updated><authors><author><keyname>Dai</keyname><forenames>Jifeng</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks
  for Semantic Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent leading approaches to semantic segmentation rely on deep convolutional
networks trained with human-annotated, pixel-level segmentation masks. Such
pixel-accurate supervision demands expensive labeling effort and limits the
performance of deep networks that usually benefit from more training data. In
this paper, we propose a method that achieves competitive accuracy but only
requires easily obtained bounding box annotations. The basic idea is to iterate
between automatically generating region proposals and training convolutional
networks. These two steps gradually recover segmentation masks for improving
the networks, and vise versa. Our method, called BoxSup, produces competitive
results supervised by boxes only, on par with strong baselines fully supervised
by masks under the same setting. By leveraging a large amount of bounding
boxes, BoxSup further unleashes the power of deep convolutional networks and
yields state-of-the-art results on PASCAL VOC 2012 and PASCAL-CONTEXT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01643</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01643</id><created>2015-03-05</created><updated>2015-12-03</updated><authors><author><keyname>Gnilke</keyname><forenames>Oliver W.</forenames></author><author><keyname>Greferath</keyname><forenames>Marcus</forenames></author><author><keyname>Pav&#x10d;evi&#x107;</keyname><forenames>Mario Osvin</forenames></author></authors><title>Mosaics of Combinatorial Designs</title><categories>math.CO cs.DM</categories><msc-class>05B30, 05B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Looking at incidence matrices of $t$-$(v,k,\lambda)$ designs as $v \times b$
matrices with $2$ possible entries, each of which indicates incidences of a
$t$-design, we introduce the notion of a $c$-mosaic of designs, having the same
number of points and blocks, as a matrix with $c$ different entries, such that
each entry defines incidences of a design. In fact, a $v \times b$ matrix is
decomposed in $c$ incidence matrices of designs, each denoted by a different
colour, hence this decomposition might be seen as a tiling of a matrix with
incidence matrices of designs as well. These mosaics have applications in
experiment design when considering a simultaneous run of several different
experiments. We have constructed infinite series of examples of mosaics and
state some probably non-trivial open problems. Furthermore we extend our
definition to the case of $q$-analogues of designs in a meaningful way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01646</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01646</id><created>2015-03-05</created><authors><author><keyname>Hooshmand</keyname><forenames>Sahar</forenames></author><author><keyname>Avilaq</keyname><forenames>Ali Jamali</forenames></author><author><keyname>Rezaie</keyname><forenames>Amir Hossein</forenames></author></authors><title>Video-Based Facial Expression Recognition Using Local Directional Binary
  Pattern</title><categories>cs.CV</categories><comments>9 pages, 8 figures, 3 tables in IJASCSE 2015 volume 4 issue 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic facial expression analysis is a challenging issue and influenced so
many areas such as human computer interaction. Due to the uncertainties of the
light intensity and light direction, the face gray shades are uneven and the
expression recognition rate under simple Local Binary Pattern is not ideal and
promising. In this paper we propose two state-of-the-art descriptors for
person-independent facial expression recognition. First the face regions of the
whole images in a video sequence are modeled with Volume Local Directional
Binary pattern (VLDBP), which is an extended version of the LDBP operator,
incorporating movement and appearance together. To make the survey
computationally simple and easy to expand, only the co-occurrences of the Local
Directional Binary Pattern on three orthogonal planes (LDBP-TOP) are debated.
After extracting the feature vectors the K-Nearest Neighbor classifier was used
to recognize the expressions. The proposed methods are applied to the videos of
the Extended Cohn-Kanade database (CK+) and the experimental outcomes
demonstrate that the offered techniques achieve more accuracy in comparison
with the classic and traditional algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01647</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01647</id><created>2015-03-05</created><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Liu</keyname><forenames>Xianming</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Zhou</keyname><forenames>Jiayu</forenames></author><author><keyname>Qi</keyname><forenames>Guo-Jun</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Decentralized Recommender Systems</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a decentralized recommender system by formulating the
popular collaborative filleting (CF) model into a decentralized matrix
completion form over a set of users. In such a way, data storages and
computations are fully distributed. Each user could exchange limited
information with its local neighborhood, and thus it avoids the centralized
fusion. Advantages of the proposed system include a protection on user privacy,
as well as better scalability and robustness. We compare our proposed algorithm
with several state-of-the-art algorithms on the FlickerUserFavor dataset, and
demonstrate that the decentralized algorithm can gain a competitive performance
to others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01655</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01655</id><created>2015-03-05</created><updated>2015-03-12</updated><authors><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Barrena</keyname><forenames>Ander</forenames></author><author><keyname>Soroa</keyname><forenames>Aitor</forenames></author></authors><title>Studying the Wikipedia Hyperlink Graph for Relatedness and
  Disambiguation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperlinks and other relations in Wikipedia are a extraordinary resource
which is still not fully understood. In this paper we study the different types
of links in Wikipedia, and contrast the use of the full graph with respect to
just direct links. We apply a well-known random walk algorithm on two tasks,
word relatedness and named-entity disambiguation. We show that using the full
graph is more effective than just direct links by a large margin, that
non-reciprocal links harm performance, and that there is no benefit from
categories and infoboxes, with coherent results on both tasks. We set new
state-of-the-art figures for systems based on Wikipedia links, comparable to
systems exploiting several information sources and/or supervised machine
learning. Our approach is open source, with instruction to reproduce results,
and amenable to be integrated with complementary text-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01657</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01657</id><created>2015-03-05</created><authors><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Shao</keyname><forenames>Zhuhong</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Color Image Classification via Quaternion Principal Component Analysis
  Network</title><categories>cs.CV</categories><comments>9 figures,5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Principal Component Analysis Network (PCANet), which is one of the
recently proposed deep learning architectures, achieves the state-of-the-art
classification accuracy in various databases. However, the performance of
PCANet may be degraded when dealing with color images. In this paper, a
Quaternion Principal Component Analysis Network (QPCANet), which is an
extension of PCANet, is proposed for color images classification. Compared to
PCANet, the proposed QPCANet takes into account the spatial distribution
information of color images and ensures larger amount of intra-class invariance
of color images. Experiments conducted on different color image datasets such
as Caltech-101, UC Merced Land Use, Georgia Tech face and CURet have revealed
that the proposed QPCANet achieves higher classification accuracy than PCANet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01663</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01663</id><created>2015-03-05</created><authors><author><keyname>Feldman</keyname><forenames>Dan</forenames></author><author><keyname>Volkov</keyname><forenames>Mikhail</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Dimensionality Reduction of Massive Sparse Datasets Using Coresets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a practical solution with performance guarantees to
the problem of dimensionality reduction for very large scale sparse matrices.
We show applications of our approach to computing the low rank approximation
(reduced SVD) of such matrices. Our solution uses coresets, which is a subset
of $O(k/\eps^2)$ scaled rows from the $n\times d$ input matrix, that
approximates the sub of squared distances from its rows to every
$k$-dimensional subspace in $\REAL^d$, up to a factor of $1\pm\eps$. An open
theoretical problem has been whether we can compute such a coreset that is
independent of the input matrix and also a weighted subset of its rows. %An
open practical problem has been whether we can compute a non-trivial
approximation to the reduced SVD of very large databases such as the Wikipedia
document-term matrix in a reasonable time. We answer this question
affirmatively. % and demonstrate an algorithm that efficiently computes a low
rank approximation of the entire English Wikipedia. Our main technical result
is a novel technique for deterministic coreset construction that is based on a
reduction to the problem of $\ell_2$ approximation for item frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01669</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01669</id><created>2015-03-05</created><updated>2015-11-17</updated><authors><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author></authors><title>More on Decomposing Coverings by Octants</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we improve our upper bound given earlier by showing that every
9-fold covering of a point set in the space by finitely many translates of an
octant decomposes into two coverings, and our lower bound by a construction for
a 4-fold covering that does not decompose into two coverings. The same bounds
also hold for coverings of points in $\R^2$ by finitely many homothets or
translates of a triangle. We also prove that certain dynamic interval coloring
problems are equivalent to the above question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01673</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01673</id><created>2015-03-05</created><updated>2015-06-12</updated><authors><author><keyname>Kandasamy</keyname><forenames>Kirthevasan</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author></authors><title>High Dimensional Bayesian Optimisation and Bandits via Additive Models</title><categories>stat.ML cs.LG</categories><comments>Proceedings of The 32nd International Conference on Machine Learning
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Optimisation (BO) is a technique used in optimising a
$D$-dimensional function which is typically expensive to evaluate. While there
have been many successes for BO in low dimensions, scaling it to high
dimensions has been notoriously difficult. Existing literature on the topic are
under very restrictive settings. In this paper, we identify two key challenges
in this endeavour. We tackle these challenges by assuming an additive structure
for the function. This setting is substantially more expressive and contains a
richer class of functions than previous work. We prove that, for additive
functions the regret has only linear dependence on $D$ even though the function
depends on all $D$ dimensions. We also demonstrate several other statistical
and computational benefits in our framework. Via synthetic examples, a
scientific simulation and a face detection problem we demonstrate that our
method outperforms naive BO on additive functions and on several examples where
the function is not additive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01676</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01676</id><created>2015-03-05</created><updated>2016-02-08</updated><authors><author><keyname>Khan</keyname><forenames>Imran</forenames></author><author><keyname>Belqasmi</keyname><forenames>Fatna</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author><author><keyname>Crespi</keyname><forenames>Noel</forenames></author><author><keyname>Morrow</keyname><forenames>Monique</forenames></author><author><keyname>Polako</keyname><forenames>Paul</forenames></author></authors><title>Wireless Sensor Network Virtualization: A Survey</title><categories>cs.NI</categories><comments>Accepted for publication on 3rd March 2015 in forthcoming issue of
  IEEE Communication Surveys and Tutorials. This version has NOT been
  proof-read and may have some some inconsistencies. Please refer to final
  version published in IEEE Xplore using DOI reference
  http://dx.doi.org/10.1109/COMST.2015.2412971</comments><journal-ref>Khan, I.; Belqasmi, F.; Glitho, R.; Crespi, N.; Morrow, M.;
  Polakos, P., &quot;Wireless Sensor Network Virtualization: A Survey,&quot; in
  Communications Surveys &amp; Tutorials, IEEE , vol.18, no.1, pp.553-576,
  Firstquarter 2016</journal-ref><doi>10.1109/COMST.2015.2412971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are the key components of the emerging
Internet-of-Things (IoT) paradigm. They are now ubiquitous and used in a
plurality of application domains. WSNs are still domain specific and usually
deployed to support a specific application. However, as WSN nodes are becoming
more and more powerful, it is getting more and more pertinent to research how
multiple applications could share a very same WSN infrastructure.
Virtualization is a technology that can potentially enable this sharing. This
paper is a survey on WSN virtualization. It provides a comprehensive review of
the state-of-the-art and an in-depth discussion of the research issues. We
introduce the basics of WSN virtualization and motivate its pertinence with
carefully selected scenarios. Existing works are presented in detail and
critically evaluated using a set of requirements derived from the scenarios.
The pertinent research projects are also reviewed. Several research issues are
also discussed with hints on how they could be tackled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01706</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01706</id><created>2015-03-05</created><updated>2015-07-29</updated><authors><author><keyname>Grimm</keyname><forenames>Carsten</forenames></author></authors><title>Efficient Farthest-Point Queries in Two-Terminal Series-Parallel
  Networks</title><categories>cs.DS</categories><comments>To appear in the proceedings of the 41st Workshop on Graph-Theoretic
  Concepts in Computer Science (WG2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the continuum of points along the edges of a network, i.e., a
connected, undirected graph with positive edge weights. We measure the distance
between these points in terms of the weighted shortest path distance, called
the network distance. Within this metric space, we study farthest points and
farthest distances. We introduce a data structure supporting queries for the
farthest distance and the farthest points on two-terminal series-parallel
networks. This data structure supports farthest-point queries in $O(k + \log
n)$ time after $O(n \log p)$ construction time, where $k$ is the number of
farthest points, $n$ is the size of the network, and $p$ parallel operations
are required to generate the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01707</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01707</id><created>2015-03-05</created><updated>2016-01-12</updated><authors><author><keyname>Bonifati</keyname><forenames>Angela</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author><author><keyname>Torlone</keyname><forenames>Riccardo</forenames></author><author><keyname>Bussche</keyname><forenames>Jan Van den</forenames></author></authors><title>Mapping-equivalence and oid-equivalence of single-function
  object-creating conjunctive queries</title><categories>cs.DB cs.AI cs.LO</categories><comments>This revised version has been accepted on 11 January 2016 for
  publication in The VLDB Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conjunctive database queries have been extended with a mechanism for object
creation to capture important applications such as data exchange, data
integration, and ontology-based data access. Object creation generates new
object identifiers in the result, that do not belong to the set of constants in
the source database. The new object identifiers can be also seen as Skolem
terms. Hence, object-creating conjunctive queries can also be regarded as
restricted second-order tuple-generating dependencies (SO tgds), considered in
the data exchange literature.
  In this paper, we focus on the class of single-function object-creating
conjunctive queries, or sifo CQs for short. We give a new characterization for
oid-equivalence of sifo CQs that is simpler than the one given by Hull and
Yoshikawa and places the problem in the complexity class NP. Our
characterization is based on Cohen's equivalence notions for conjunctive
queries with multiplicities. We also solve the logical entailment problem for
sifo CQs, showing that also this problem belongs to NP. Results by Pichler et
al. have shown that logical equivalence for more general classes of SO tgds is
either undecidable or decidable with as yet unknown complexity upper bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01713</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01713</id><created>2015-03-05</created><authors><author><keyname>Grassi</keyname><forenames>Giulio</forenames></author><author><keyname>Pesavento</keyname><forenames>Davide</forenames></author><author><keyname>Pau</keyname><forenames>Giovanni</forenames></author><author><keyname>Zhang</keyname><forenames>Lixia</forenames></author><author><keyname>Fdida</keyname><forenames>Serge</forenames></author></authors><title>Navigo: Interest Forwarding by Geolocations in Vehicular Named Data
  Networking</title><categories>cs.NI</categories><doi>10.1109/WoWMoM.2015.7158165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes Navigo, a location based packet forwarding mechanism for
vehicular Named Data Networking (NDN). Navigo takes a radically new approach to
address the challenges of frequent connectivity disruptions and sudden network
changes in a vehicle network. Instead of forwarding packets to a specific
moving car, Navigo aims to fetch specific pieces of data from multiple
potential carriers of the data. The design provides (1) a mechanism to bind NDN
data names to the producers' geographic area(s); (2) an algorithm to guide
Interests towards data producers using a specialized shortest path over the
road topology; and (3) an adaptive discovery and selection mechanism that can
identify the best data source across multiple geographic areas, as well as
quickly react to changes in the V2X network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01720</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01720</id><created>2015-03-05</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Shiragur</keyname><forenames>Kirankumar</forenames></author></authors><title>How friends and non-determinism affect opinion dynamics</title><categories>cs.DS cs.SI nlin.AO</categories><comments>14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hegselmann-Krause system (HK system for short) is one of the most popular
models for the dynamics of opinion formation in multiagent systems. Agents are
modeled as points in opinion space, and at every time step, each agent moves to
the mass center of all the agents within unit distance. The rate of convergence
of HK systems has been the subject of several recent works. In this work, we
investigate two natural variations of the HK system and their effect on the
dynamics. In the first variation, we only allow pairs of agents who are friends
in an underlying social network to communicate with each other. In the second
variation, agents may not move exactly to the mass center but somewhere close
to it. The dynamics of both variants are qualitatively very different from that
of the classical HK system. Nevertheless, we prove that both these systems
converge in polynomial number of non-trivial steps, regardless of the social
network in the first variant and noise patterns in the second variant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01723</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01723</id><created>2015-03-05</created><authors><author><keyname>Moten</keyname><forenames>Rod</forenames></author></authors><title>Modelling the Semantic Web using a Type System</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for modeling the Semantic Web as a type system. By
using a type system, we can use symbolic representation for representing linked
data. Objects with only data properties and references to external resources
are represented as terms in the type system. Triples are represented
symbolically using type constructors as the predicates. In our type system, we
allow users to add analytics that utilize machine learning or knowledge
discovery to perform inductive reasoning over data. These analytics can be used
by the inference engine when performing reasoning to answer a query.
Furthermore, our type system defines a means to resolve semantic heterogeneity
on-the-fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01732</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01732</id><created>2015-03-05</created><authors><author><keyname>Hellweger</keyname><forenames>Stefan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Pekka</forenames></author></authors><title>The Contemporary Understanding of User Experience in Practice</title><categories>cs.HC</categories><comments>8 pages, 1 figure, 3 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  User Experience (UX) has been a buzzword in agile literature in recent years.
However, often UX remains as a vague concept and it may be hard to understand
the very nature of it in the context of agile software development. This paper
explores the multifaceted UX literature, emphasizes the multi-dimensional
nature of the concept and organizes the current state-of-the-art knowledge. As
a starting point to better understand the contemporary meaning of UX assigned
by practitioners, we selected four UX blogs and performed an analysis using a
framework derived from the literature review. The preliminary results show that
the practitioners more often focus on interaction between product and user and
view UX from design perspective predominantly. While the economical perspective
receives little attention in literature, it is evident in practitioners
writings. Our study opens up a promising line of request of the contemporary
meaning of UX in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01737</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01737</id><created>2015-03-05</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Min-Max Kernels</title><categories>stat.ML cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The min-max kernel is a generalization of the popular resemblance kernel
(which is designed for binary data). In this paper, we demonstrate, through an
extensive classification study using kernel machines, that the min-max kernel
often provides an effective measure of similarity for nonnegative data. As the
min-max kernel is nonlinear and might be difficult to be used for industrial
applications with massive data, we show that the min-max kernel can be
linearized via hashing techniques. This allows practitioners to apply min-max
kernel to large-scale applications using well matured linear algorithms such as
linear SVM or logistic regression.
  The previous remarkable work on consistent weighted sampling (CWS) produces
samples in the form of ($i^*, t^*$) where the $i^*$ records the location (and
in fact also the weights) information analogous to the samples produced by
classical minwise hashing on binary data. Because the $t^*$ is theoretically
unbounded, it was not immediately clear how to effectively implement CWS for
building large-scale linear classifiers. In this paper, we provide a simple
solution by discarding $t^*$ (which we refer to as the &quot;0-bit&quot; scheme). Via an
extensive empirical study, we show that this 0-bit scheme does not lose
essential information. We then apply the &quot;0-bit&quot; CWS for building linear
classifiers to approximate min-max kernel classifiers, as extensively validated
on a wide range of publicly available classification datasets. We expect this
work will generate interests among data mining practitioners who would like to
efficiently utilize the nonlinear information of non-binary and nonnegative
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01752</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01752</id><created>2015-03-05</created><updated>2015-10-14</updated><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Efficient Inverse Maintenance and Faster Algorithms for Linear
  Programming</title><categories>cs.DS cs.NA math.OC</categories><comments>In an older version of this paper, we mistakenly claimed an improved
  running time for Dikin walk by noting solely the improved running time for
  linear system solving and ignoring the determinant computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the following inverse maintenance problem: given
$A \in \mathbb{R}^{n\times d}$ and a number of rounds $r$, we receive a
$n\times n$ diagonal matrix $D^{(k)}$ at round $k$ and we wish to maintain an
efficient linear system solver for $A^{T}D^{(k)}A$ under the assumption
$D^{(k)}$ does not change too rapidly. This inverse maintenance problem is the
computational bottleneck in solving multiple optimization problems. We show how
to solve this problem with $\tilde{O}(nnz(A)+d^{\omega})$ preprocessing time
and amortized $\tilde{O}(nnz(A)+d^{2})$ time per round, improving upon previous
running times for solving this problem.
  Consequently, we obtain the fastest known running times for solving multiple
problems including, linear programming and computing a rounding of a polytope.
In particular given a feasible point in a linear program with $d$ variables,
$n$ constraints, and constraint matrix $A\in\mathbb{R}^{n\times d}$, we show
how to solve the linear program in time
$\tilde{O}(nnz(A)+d^{2})\sqrt{d}\log(\epsilon^{-1}))$. We achieve our results
through a novel combination of classic numerical techniques of low rank update,
preconditioning, and fast matrix multiplication as well as recent work on
subspace embeddings and spectral sparsification that we hope will be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01793</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01793</id><created>2015-03-05</created><authors><author><keyname>Wen</keyname><forenames>Min</forenames></author><author><keyname>Ehlers</keyname><forenames>Ruediger</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Correct-by-synthesis reinforcement learning with temporal logic
  constraints</title><categories>cs.LO cs.GT cs.LG cs.SY</categories><comments>8 pages, 3 figures, 2 tables, submitted to IROS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a problem on the synthesis of reactive controllers that optimize
some a priori unknown performance criterion while interacting with an
uncontrolled environment such that the system satisfies a given temporal logic
specification. We decouple the problem into two subproblems. First, we extract
a (maximally) permissive strategy for the system, which encodes multiple
(possibly all) ways in which the system can react to the adversarial
environment and satisfy the specifications. Then, we quantify the a priori
unknown performance criterion as a (still unknown) reward function and compute
an optimal strategy for the system within the operating envelope allowed by the
permissive strategy by using the so-called maximin-Q learning algorithm. We
establish both correctness (with respect to the temporal logic specifications)
and optimality (with respect to the a priori unknown performance criterion) of
this two-step technique for a fragment of temporal logic specifications. For
specifications beyond this fragment, correctness can still be preserved, but
the learned strategy may be sub-optimal. We present an algorithm to the overall
problem, and demonstrate its use and computational requirements on a set of
robot motion planning examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01800</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01800</id><created>2015-03-05</created><updated>2015-03-29</updated><authors><author><keyname>Kahou</keyname><forenames>Samira Ebrahimi</forenames></author><author><keyname>Bouthillier</keyname><forenames>Xavier</forenames></author><author><keyname>Lamblin</keyname><forenames>Pascal</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Michalski</keyname><forenames>Vincent</forenames></author><author><keyname>Konda</keyname><forenames>Kishore</forenames></author><author><keyname>Jean</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Froumenty</keyname><forenames>Pierre</forenames></author><author><keyname>Dauphin</keyname><forenames>Yann</forenames></author><author><keyname>Boulanger-Lewandowski</keyname><forenames>Nicolas</forenames></author><author><keyname>Ferrari</keyname><forenames>Raul Chandias</forenames></author><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Warde-Farley</keyname><forenames>David</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Pal</keyname><forenames>Christopher</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>EmoNets: Multimodal deep learning approaches for emotion recognition in
  video</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of the emotion recognition in the wild (EmotiW) Challenge is to
assign one of seven emotions to short video clips extracted from Hollywood
style movies. The videos depict acted-out emotions under realistic conditions
with a large degree of variation in attributes such as pose and illumination,
making it worthwhile to explore approaches which consider combinations of
features from multiple modalities for label assignment. In this paper we
present our approach to learning several specialist models using deep learning
techniques, each focusing on one modality. Among these are a convolutional
neural network, focusing on capturing visual information in detected faces, a
deep belief net focusing on the representation of the audio stream, a K-Means
based &quot;bag-of-mouths&quot; model, which extracts visual features around the mouth
region and a relational autoencoder, which addresses spatio-temporal aspects of
videos. We explore multiple methods for the combination of cues from these
modalities into one common classifier. This achieves a considerably greater
accuracy than predictions from our strongest single-modality classifier. Our
method was the winning submission in the 2013 EmotiW challenge and achieved a
test set accuracy of 47.67% on the 2014 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01804</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01804</id><created>2015-03-05</created><authors><author><keyname>Kadambi</keyname><forenames>Achuta</forenames></author><author><keyname>Taamazyan</keyname><forenames>Vage</forenames></author><author><keyname>Jayasuriya</keyname><forenames>Suren</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author></authors><title>Frequency Domain TOF: Encoding Object Depth in Modulation Frequency</title><categories>cs.CV cs.GR</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time of flight cameras may emerge as the 3-D sensor of choice. Today, time of
flight sensors use phase-based sampling, where the phase delay between emitted
and received, high-frequency signals encodes distance. In this paper, we
present a new time of flight architecture that relies only on frequency---we
refer to this technique as frequency-domain time of flight (FD-TOF). Inspired
by optical coherence tomography (OCT), FD-TOF excels when frequency bandwidth
is high. With the increasing frequency of TOF sensors, new challenges to time
of flight sensing continue to emerge. At high frequencies, FD-TOF offers
several potential benefits over phase-based time of flight methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01806</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01806</id><created>2015-03-05</created><updated>2016-03-08</updated><authors><author><keyname>Bibak</keyname><forenames>Khodakhast</forenames></author><author><keyname>Kapron</keyname><forenames>Bruce M.</forenames></author><author><keyname>Srinivasan</keyname><forenames>Venkatesh</forenames></author><author><keyname>Tauraso</keyname><forenames>Roberto</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Restricted linear congruences</title><categories>math.NT cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, using properties of Ramanujan sums and of the discrete Fourier
transform of arithmetic functions, we give an explicit formula for the number
of solutions of the linear congruence $a_1x_1+\cdots +a_kx_k\equiv b \pmod{n}$,
with $\gcd(x_i,n)=t_i$ ($1\leq i\leq k$), where $a_1,t_1,\ldots,a_k,t_k, b,n$
($n\geq 1$) are arbitrary integers. Some special cases of this problem have
been already studied in many papers. The problem is very well-motivated and in
addition to number theory has intriguing applications in combinatorics,
computer science, and cryptography, among other areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01811</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01811</id><created>2015-03-05</created><updated>2015-06-18</updated><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>Optimally Combining Classifiers Using Unlabeled Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a worst-case analysis of aggregation of classifier ensembles for
binary classification. The task of predicting to minimize error is formulated
as a game played over a given set of unlabeled data (a transductive setting),
where prior label information is encoded as constraints on the game. The
minimax solution of this game identifies cases where a weighted combination of
the classifiers can perform significantly better than any single classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01812</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01812</id><created>2015-03-05</created><authors><author><keyname>Ayala-Rivera</keyname><forenames>Vanessa</forenames></author><author><keyname>McDonagh</keyname><forenames>Patrick</forenames></author><author><keyname>Cerqueus</keyname><forenames>Thomas</forenames></author><author><keyname>Murphy</keyname><forenames>Liam</forenames></author></authors><title>Ontology-Based Quality Evaluation of Value Generalization Hierarchies
  for Data Anonymization</title><categories>cs.DB</categories><comments>18 pages, 7 figures, presented in the Privacy in Statistical
  Databases Conference 2014 (Ibiza, Spain)</comments><acm-class>H.2.0; D.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In privacy-preserving data publishing, approaches using Value Generalization
Hierarchies (VGHs) form an important class of anonymization algorithms. VGHs
play a key role in the utility of published datasets as they dictate how the
anonymization of the data occurs. For categorical attributes, it is imperative
to preserve the semantics of the original data in order to achieve a higher
utility. Despite this, semantics have not being formally considered in the
specification of VGHs. Moreover, there are no methods that allow the users to
assess the quality of their VGH. In this paper, we propose a measurement
scheme, based on ontologies, to quantitatively evaluate the quality of VGHs, in
terms of semantic consistency and taxonomic organization, with the aim of
producing higher-quality anonymizations. We demonstrate, through a case study,
how our evaluation scheme can be used to compare the quality of multiple VGHs
and can help to identify faulty VGHs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01817</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01817</id><created>2015-03-05</created><authors><author><keyname>Thomee</keyname><forenames>Bart</forenames></author><author><keyname>Shamma</keyname><forenames>David A.</forenames></author><author><keyname>Friedland</keyname><forenames>Gerald</forenames></author><author><keyname>Elizalde</keyname><forenames>Benjamin</forenames></author><author><keyname>Ni</keyname><forenames>Karl</forenames></author><author><keyname>Poland</keyname><forenames>Douglas</forenames></author><author><keyname>Borth</keyname><forenames>Damian</forenames></author><author><keyname>Li</keyname><forenames>Li-Jia</forenames></author></authors><title>The New Data and New Challenges in Multimedia Research</title><categories>cs.MM cs.CY</categories><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M),
the largest public multimedia collection that has ever been released. The
dataset contains a total of 100 million media objects, of which approximately
99.2 million are photos and 0.8 million are videos, all of which carry a
Creative Commons license. Each media object in the dataset is represented by
several pieces of metadata, e.g. Flickr identifier, owner name, camera, title,
tags, geo, media source. The collection provides a comprehensive snapshot of
how photos and videos were taken, described, and shared over the years, from
the inception of Flickr in 2004 until early 2014. In this article we explain
the rationale behind its creation, as well as the implications the dataset has
for science, research, engineering, and development. We further present several
new challenges in multimedia research that can now be expanded upon with our
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01820</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01820</id><created>2015-03-05</created><authors><author><keyname>Hu</keyname><forenames>Ninghang</forenames></author><author><keyname>Englebienne</keyname><forenames>Gwenn</forenames></author><author><keyname>Lou</keyname><forenames>Zhongyu</forenames></author><author><keyname>Kr&#xf6;se</keyname><forenames>Ben</forenames></author></authors><title>Latent Hierarchical Model for Activity Recognition</title><categories>cs.RO cs.AI cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel hierarchical model for human activity recognition. In
contrast to approaches that successively recognize actions and activities, our
approach jointly models actions and activities in a unified framework, and
their labels are simultaneously predicted. The model is embedded with a latent
layer that is able to capture a richer class of contextual information in both
state-state and observation-state pairs. Although loops are present in the
model, the model has an overall linear-chain structure, where the exact
inference is tractable. Therefore, the model is very efficient in both
inference and learning. The parameters of the graphical model are learned with
a Structured Support Vector Machine (Structured-SVM). A data-driven approach is
used to initialize the latent variables; therefore, no manual labeling for the
latent states is required. The experimental results from using two benchmark
datasets show that our model outperforms the state-of-the-art approach, and our
model is computationally more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01824</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01824</id><created>2015-03-05</created><authors><author><keyname>Kim</keyname><forenames>Minyoung</forenames></author><author><keyname>Rigazio</keyname><forenames>Luca</forenames></author></authors><title>Deep Clustered Convolutional Kernels</title><categories>cs.LG cs.NE</categories><comments>draft</comments><journal-ref>JMLR: Workshop and Conference Proceedings 44 (2015) 160-172</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have recently achieved state of the art performance
thanks to new training algorithms for rapid parameter estimation and new
regularization methods to reduce overfitting. However, in practice the network
architecture has to be manually set by domain experts, generally by a costly
trial and error procedure, which often accounts for a large portion of the
final system performance. We view this as a limitation and propose a novel
training algorithm that automatically optimizes network architecture, by
progressively increasing model complexity and then eliminating model redundancy
by selectively removing parameters at training time. For convolutional neural
networks, our method relies on iterative split/merge clustering of
convolutional kernels interleaved by stochastic gradient descent. We present a
training algorithm and experimental results on three different vision tasks,
showing improved performance compared to similarly sized hand-crafted
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01832</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01832</id><created>2015-03-05</created><updated>2015-09-03</updated><authors><author><keyname>Cui</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Jiang</keyname><forenames>Nianjuan</forenames></author><author><keyname>Tang</keyname><forenames>Chengzhou</forenames></author><author><keyname>Tan</keyname><forenames>Ping</forenames></author></authors><title>Linear Global Translation Estimation with Feature Tracks</title><categories>cs.CV</categories><comments>Changes: 1. Adopt BMVC2015 style; 2. Combine sections 3 and 5; 3.
  Move &quot;Evaluation on synthetic data&quot; out to supplementary file; 4. Divide
  subsection &quot;Evaluation on general data&quot; to subsections &quot;Experiment on
  sequential data&quot; and &quot;Experiment on unordered Internet data&quot;; 5. Change Fig.
  1 and Fig.8; 6. Move Fig. 6 and Fig. 7 to supplementary file; 7 Change some
  symbols; 8. Correct some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives a novel linear position constraint for cameras seeing a
common scene point, which leads to a direct linear method for global camera
translation estimation. Unlike previous solutions, this method deals with
collinear camera motion and weak image association at the same time. The final
linear formulation does not involve the coordinates of scene points, which
makes it efficient even for large scale data. We solve the linear equation
based on $L_1$ norm, which makes our system more robust to outliers in
essential matrices and feature correspondences. We experiment this method on
both sequentially captured images and unordered Internet images. The
experiments demonstrate its strength in robustness, accuracy, and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01837</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01837</id><created>2015-03-05</created><authors><author><keyname>Wang</keyname><forenames>Menghan</forenames></author><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author></authors><title>Combinatorial rigidity and independence of generalized pinned
  subspace-incidence constraint systems</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a hypergraph $H$ with $m$ hyperedges and a set $X$ of $m$ \emph{pins},
i.e.\ globally fixed subspaces in Euclidean space $\mathbb{R}^d$, a
\emph{pinned subspace-incidence system} is the pair $(H, X)$, with the
constraint that each pin in $X$ lies on the subspace spanned by the point
realizations in $\mathbb{R}^d$ of vertices of the corresponding hyperedge of
$H$. We are interested in combinatorial characterization of pinned
subspace-incidence systems that are \emph{minimally rigid}, i.e.\ those systems
that are guaranteed to generically yield a locally unique realization. As is
customary, this is accompanied by a characterization of generic independence as
well as rigidity.
  In a previous paper \cite{sitharam2014incidence}, we used pinned
subspace-incidence systems towards solving the \emph{fitted dictionary
learning} problem, i.e.\ dictionary learning with specified underlying
hypergraph, and gave a combinatorial characterization of minimal rigidity for a
more restricted version of pinned subspace-incidence system, with $H$ being a
uniform hypergraph and pins in $X$ being 1-dimension subspaces. Moreover in a
recent paper \cite{Baker2015}, the special case of pinned line incidence
systems was used to model biomaterials such as cellulose and collagen fibrils
in cell walls. In this paper, we extend the combinatorial characterization to
general pinned subspace-incidence systems, with $H$ being a non-uniform
hypergraph and pins in $X$ being subspaces with arbitrary dimension. As there
are generally many data points per subspace in a dictionary learning problem,
which can only be modeled with pins of dimension larger than $1$, such an
extension enables application to a much larger class of fitted dictionary
learning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01838</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01838</id><created>2015-03-05</created><updated>2015-06-08</updated><authors><author><keyname>Meng</keyname><forenames>Fandong</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Wang</keyname><forenames>Mingxuan</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Jiang</keyname><forenames>Wenbin</forenames></author><author><keyname>Liu</keyname><forenames>Qun</forenames></author></authors><title>Encoding Source Language with Convolutional Neural Network for Machine
  Translation</title><categories>cs.CL cs.LG cs.NE</categories><comments>Accepted as a full paper at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed neural network joint model (NNJM) (Devlin et al., 2014)
augments the n-gram target language model with a heuristically chosen source
context window, achieving state-of-the-art performance in SMT. In this paper,
we give a more systematic treatment by summarizing the relevant source
information through a convolutional architecture guided by the target
information. With different guiding signals during decoding, our specifically
designed convolution+gating architectures can pinpoint the parts of a source
sentence that are relevant to predicting a target word, and fuse them with the
context of entire source sentence to form a unified representation. This
representation, together with target language words, are fed to a deep neural
network (DNN) to form a stronger NNJM. Experiments on two NIST Chinese-English
translation tasks show that the proposed model can achieve significant
improvements over the previous NNJM by up to +1.08 BLEU points on average
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01839</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01839</id><created>2015-03-05</created><updated>2015-05-28</updated><authors><author><keyname>Staple</keyname><forenames>Douglas B.</forenames></author></authors><title>The combinatorial algorithm for computing $\pi(x)$</title><categories>math.NT cs.DS</categories><comments>12 pages</comments><msc-class>11N05 (Primary), 11Y16, 11-04 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes recent advances in the combinatorial method for
computing $\pi(x)$, the number of primes $\leq x$. In particular, the memory
usage has been reduced by a factor of $\log x$, and modifications for shared-
and distributed-memory parallelism have been incorporated. The resulting method
computes $\pi(x)$ with complexity $O(x^{2/3}\mathrm{log}^{-2}x)$ in time and
$O(x^{1/3}\mathrm{log}^{2}x)$ in space. The algorithm has been implemented and
used to compute $\pi(10^n)$ for $1 \leq n \leq 26$ and $\pi(2^m)$ for $1\leq m
\leq 86$. The mathematics presented here is consistent with and builds on that
of previous authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01847</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01847</id><created>2015-03-05</created><authors><author><keyname>Rao</keyname><forenames>V. Sree Hari</forenames></author><author><keyname>Kumar</keyname><forenames>M. Naresh</forenames></author></authors><title>Estimation of the parameters of an infectious disease model using neural
  networks</title><categories>cs.NE</categories><comments>17 pages, 11 figures</comments><journal-ref>Nonlinear Analysis: Real World Applications 11(2010) 1810-1818</journal-ref><doi>10.1016/j.nonrwa.2009.04.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a realistic mathematical model taking into account
the mutual interference among the interacting populations. This model attempts
to describe the control (vaccination) function as a function of the number of
infective individuals, which is an improvement over the existing susceptible
?infective epidemic models. Regarding the growth of the epidemic as a nonlinear
phenomenon we have developed a neural network architecture to estimate the
vital parameters associated with this model. This architecture is based on a
recently developed new class of neural networks known as co-operative and
supportive neural networks. The application of this architecture to the present
study involves preprocessing of the input data, and this renders an efficient
estimation of the rate of spread of the epidemic. It is observed that the
proposed new neural network outperforms a simple feed-forward neural network
and polynomial regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01850</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01850</id><created>2015-03-06</created><authors><author><keyname>Hellweger</keyname><forenames>Stefan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofeng</forenames></author></authors><title>What is User Experience Really: towards a UX Conceptual Framework</title><categories>cs.HC</categories><comments>4 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1503.01732</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For more then a decade the term User Experience (UX) has been highly debated
and defined in many ways. However, often UX remains as a vague concept and it
may be hard to understand the very nature of it. In this paper we aimed at
providing a better understanding of this concept. We explored the multi-faceted
UX literature, reviewing the current state-of- the-art knowledge and
emphasizing the multi-dimensional nature of the concept. Based on the
literature review we built a conceptual framework of UX using the elements that
are linked to it and reported in different studies. To show the potential use
of the framework, we examined the UX delivered by different phone applications
on different mobile devices using the elements in the framework. Several
interesting insights have been obtained in terms of how the phone applications
deliver different UX. Our study opens up a promising line of investigating the
contemporary meaning of UX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01868</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01868</id><created>2015-03-06</created><updated>2015-08-10</updated><authors><author><keyname>Cao</keyname><forenames>Wenfei</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>A Novel Tensor Robust PCA Approach for Background Subtraction from
  Compressive Measurements</title><categories>cs.CV</categories><comments>17 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background subtraction has been a fundamental and widely studied task in
video analysis, with a wide range of applications in video surveillance,
teleconferencing and 3D modeling. Recently, motivated by compressive imaging,
background subtraction from compressive measurements (BSCM) is becoming an
active research task in video surveillance. In this paper, we propose a novel
tensor-based robust PCA (TenRPCA) approach for BSCM by decomposing video frames
into backgrounds with spatial-temporal correlations and foregrounds with
spatio-temporal continuity in a tensor framework. In this approach, we use 3D
total variation to enhance the spatio-temporal continuity of foregrounds, and
Tucker decomposition to model the spatio-temporal correlations of video
background. Based on this idea, we design a basic tensor RPCA model over the
video frames, dubbed holistic TenRPCA model (H-TenRPCA). To characterize the
correlations among the groups of similar 3D patches of video background, we
further design a patch-group-based tensor RPCA model (PG-TenRPCA) by joint
tensor Tucker decompositions of 3D patch groups for modeling the video
background. Efficient algorithms using alternating direction method of
multipliers (ADMM) are developed to solve the proposed models. Extensive
experiments on simulated and real-world videos demonstrate the superiority of
the proposed models over the existing state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01874</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01874</id><created>2015-03-06</created><authors><author><keyname>Das</keyname><forenames>Anupam</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author></authors><title>Exploring Ways To Mitigate Sensor-Based Smartphone Fingerprinting</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern smartphones contain motion sensors, such as accelerometers and
gyroscopes. These sensors have many useful applications; however, they can also
be used to uniquely identify a phone by measuring anomalies in the signals,
which are a result from manufacturing imperfections. Such measurements can be
conducted surreptitiously in the browser and can be used to track users across
applications, websites, and visits.
  We analyze techniques to mitigate such device fingerprinting either by
calibrating the sensors to eliminate the signal anomalies, or by adding noise
that obfuscates the anomalies. To do this, we first develop a highly accurate
fingerprinting mechanism that combines multiple motion sensors and makes use of
(inaudible) audio stimulation to improve detection. We then collect
measurements from a large collection of smartphones and evaluate the impact of
calibration and obfuscation techniques on the classifier accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01881</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01881</id><created>2015-03-06</created><updated>2015-11-23</updated><authors><author><keyname>Parolo</keyname><forenames>Pietro Della Briotta</forenames></author><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Attention decay in science</title><categories>physics.soc-ph cs.CY cs.DL cs.SI physics.data-an</categories><comments>Published version. 14 pages, 9 Figures,</comments><journal-ref>Journal of Informetrics, 9, 734-745 (2015)</journal-ref><doi>10.1016/j.joi.2015.07.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exponential growth in the number of scientific papers makes it
increasingly difficult for researchers to keep track of all the publications
relevant to their work. Consequently, the attention that can be devoted to
individual papers, measured by their citation counts, is bound to decay
rapidly. In this work we make a thorough study of the life-cycle of papers in
different disciplines. Typically, the citation rate of a paper increases up to
a few years after its publication, reaches a peak and then decreases rapidly.
This decay can be described by an exponential or a power law behavior, as in
ultradiffusive processes, with exponential fitting better than power law for
the majority of cases. The decay is also becoming faster over the years,
signaling that nowadays papers are forgotten more quickly. However, when time
is counted in terms of the number of published papers, the rate of decay of
citations is fairly independent of the period considered. This indicates that
the attention of scholars depends on the number of published items, and not on
real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01883</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01883</id><created>2015-03-06</created><authors><author><keyname>Serr&#xe0;</keyname><forenames>Joan</forenames></author><author><keyname>Serra</keyname><forenames>Isabel</forenames></author><author><keyname>Corral</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>Arcos</keyname><forenames>Josep Lluis</forenames></author></authors><title>Ranking and significance of variable-length similarity-based time series
  motifs</title><categories>cs.LG</categories><comments>20 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection of very similar patterns in a time series, commonly called
motifs, has received continuous and increasing attention from diverse
scientific communities. In particular, recent approaches for discovering
similar motifs of different lengths have been proposed. In this work, we show
that such variable-length similarity-based motifs cannot be directly compared,
and hence ranked, by their normalized dissimilarities. Specifically, we find
that length-normalized motif dissimilarities still have intrinsic dependencies
on the motif length, and that lowest dissimilarities are particularly affected
by this dependency. Moreover, we find that such dependencies are generally
non-linear and change with the considered data set and dissimilarity measure.
Based on these findings, we propose a solution to rank those motifs and measure
their significance. This solution relies on a compact but accurate model of the
dissimilarity space, using a beta distribution with three parameters that
depend on the motif length in a non-linear way. We believe the incomparability
of variable-length dissimilarities could go beyond the field of time series,
and that similar modeling strategies as the one used here could be of help in a
more broad context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01889</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01889</id><created>2015-03-06</created><authors><author><keyname>Huangfu</keyname><forenames>Q.</forenames></author><author><keyname>Hall</keyname><forenames>J. A. J.</forenames></author></authors><title>Parallelizing the dual revised simplex method</title><categories>math.OC cs.NA</categories><comments>27 pages</comments><report-no>ERGO-14-011</report-no><msc-class>90C05, 90C06, 65K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the design and implementation of two parallel dual
simplex solvers for general large scale sparse linear programming problems. One
approach, called PAMI, extends a relatively unknown pivoting strategy called
suboptimization and exploits parallelism across multiple iterations. The other,
called SIP, exploits purely single iteration parallelism by overlapping
computational components when possible. Computational results show that the
performance of PAMI is superior to that of the leading open-source simplex
solver, and that SIP complements PAMI in achieving speedup when PAMI results in
slowdown. One of the authors has implemented the techniques underlying PAMI
within the FICO Xpress simplex solver and this paper presents computational
results demonstrating their value. This performance increase is sufficiently
valuable for the achievement to be used as the basis of promotional material by
FICO. In developing the first parallel revised simplex solver of general
utility and commercial importance, this work represents a significant
achievement in computational optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01890</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01890</id><created>2015-03-06</created><authors><author><keyname>Moustakas</keyname><forenames>Aristides</forenames></author><author><keyname>Evans</keyname><forenames>Matthew R.</forenames></author></authors><title>Coupling models of cattle and farms with models of badgers for
  predicting the dynamics of bovine tuberculosis (TB)</title><categories>q-bio.PE cs.CE math.DS stat.AP stat.CO</categories><journal-ref>Stochastic Environmental Research and Risk Assessment (2015), Vol
  29, Issue 3, pp 623-635</journal-ref><doi>10.1007/s00477-014-1016-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bovine TB is a major problem for the agricultural industry in several
countries. TB can be contracted and spread by species other than cattle and
this can cause a problem for disease control. In the UK and Ireland, badgers
are a recognised reservoir of infection and there has been substantial
discussion about potential control strategies. We present a coupling of
individual based models of bovine TB in badgers and cattle, which aims to
capture the key details of the natural history of the disease and of both
species at approximately county scale. The model is spatially explicit it
follows a very large number of cattle and badgers on a different grid size for
each species and includes also winter housing. We show that the model can
replicate the reported dynamics of both cattle and badger populations as well
as the increasing prevalence of the disease in cattle. Parameter space used as
input in simulations was swept out using Latin hypercube sampling and
sensitivity analysis to model outputs was conducted using mixed effect models.
By exploring a large and computationally intensive parameter space we show that
of the available control strategies it is the frequency of TB testing and
whether or not winter housing is practised that have the most significant
effects on the number of infected cattle, with the effect of winter housing
becoming stronger as farm size increases. Whether badgers were culled or not
explained about 5%, while the accuracy of the test employed to detect infected
cattle explained less than 3% of the variance in the number of infected cattle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01895</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01895</id><created>2015-03-06</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Aghaee</keyname><forenames>Saeed</forenames></author><author><keyname>Blackwell</keyname><forenames>Alan</forenames></author></authors><title>Natural Notation for the Domestic Internet of Things</title><categories>cs.HC</categories><comments>Proceedings of the 5th International symposium on End-User
  Development (IS-EUD), Madrid, Spain, May, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study explores the use of natural language to give instructions that
might be interpreted by Internet of Things (IoT) devices in a domestic `smart
home' environment. We start from the proposition that reminders can be
considered as a type of end-user programming, in which the executed actions
might be performed either by an automated agent or by the author of the
reminder. We conducted an experiment in which people wrote sticky notes
specifying future actions in their home. In different conditions, these notes
were addressed to themselves, to others, or to a computer agent.We analyse the
linguistic features and strategies that are used to achieve these tasks,
including the use of graphical resources as an informal visual language. The
findings provide a basis for design guidance related to end-user development
for the Internet of Things.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01903</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01903</id><created>2015-03-06</created><authors><author><keyname>Mousnier</keyname><forenames>A.</forenames></author><author><keyname>Vural</keyname><forenames>E.</forenames></author><author><keyname>Guillemot</keyname><forenames>C.</forenames></author></authors><title>Partial light field tomographic reconstruction from a fixed-camera focal
  stack</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel approach to partially reconstruct
high-resolution 4D light fields from a stack of differently focused photographs
taken with a fixed camera. First, a focus map is calculated from this stack
using a simple approach combining gradient detection and region expansion with
graph-cut. Then, this focus map is converted into a depth map thanks to the
calibration of the camera. We proceed after this with the tomographic
reconstruction of the epipolar images by back-projecting the focused regions of
the scene only. We call it masked back-projection. The angles of
back-projection are calculated from the depth map. Thanks to the high angular
resolution we achieve by suitably exploiting the image content captured over a
large interval of focus distances, we are able to render puzzling perspective
shifts although the original photographs were taken from a single fixed camera
at a fixed position.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01910</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01910</id><created>2015-03-06</created><authors><author><keyname>Kamble</keyname><forenames>Vijay</forenames></author><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author><author><keyname>Silveira</keyname><forenames>Fernando</forenames></author></authors><title>Sequential Relevance Maximization with Binary Feedback</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by online settings where users can provide explicit feedback about
the relevance of products that are sequentially presented to them, we look at
the recommendation process as a problem of dynamically optimizing this
relevance feedback. Such an algorithm optimizes the fine tradeoff between
presenting the products that are most likely to be relevant, and learning the
preferences of the user so that more relevant recommendations can be made in
the future.
  We assume a standard predictive model inspired by collaborative filtering, in
which a user is sampled from a distribution over a set of possible types. For
every product category, each type has an associated relevance feedback that is
assumed to be binary: the category is either relevant or irrelevant. Assuming
that the user stays for each additional recommendation opportunity with
probability $\beta$ independent of the past, the problem is to find a policy
that maximizes the expected number of recommendations that are deemed relevant
in a session.
  We analyze this problem and prove key structural properties of the optimal
policy. Based on these properties, we first present an algorithm that strikes a
balance between recursion and dynamic programming to compute this policy. We
further propose and analyze two heuristic policies: a `farsighted' greedy
policy that attains at least $1-\beta$ factor of the optimal payoff, and a
naive greedy policy that attains at least $\frac{1-\beta}{1+\beta}$ factor of
the optimal payoff in the worst case. Extensive simulations show that these
heuristics are very close to optimal in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01913</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01913</id><created>2015-03-06</created><authors><author><keyname>Michail</keyname><forenames>Othon</forenames></author></authors><title>Terminating Distributed Construction of Shapes and Patterns in a Fair
  Solution of Automata</title><categories>cs.DC</categories><comments>39 pages, 10 figures</comments><acm-class>C.2.4; C.2.1; F.1.1; F.1.2; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a solution of automata similar to Population Protocols and
Network Constructors. The automata (or nodes) move passively in a well-mixed
solution and can cooperate by interacting in pairs. Every such interaction may
result in an update of the local states of the nodes. Additionally, the nodes
may also choose to connect to each other in order to start forming some
required structure. We may think of such nodes as the smallest possible
programmable pieces of matter. The model that we introduce here is a more
applied version of Network Constructors, imposing physical (or geometrical)
constraints on the connections. Each node can connect to other nodes only via a
very limited number of local ports, therefore at any given time it has only a
bounded number of neighbors. Connections are always made at unit distance and
are perpendicular to connections of neighboring ports. We show that this
restricted model is still capable of forming very practical 2D or 3D shapes. We
provide direct constructors for some basic shape construction problems. We then
develop new techniques for determining the constructive capabilities of our
model. One of the main novelties of our approach, concerns our attempt to
overcome the inability of such systems to detect termination. In particular, we
exploit the assumptions that the system is well-mixed and has a unique leader,
in order to give terminating protocols that are correct with high probability
(w.h.p.). This allows us to develop terminating subroutines that can be
sequentially composed to form larger modular protocols. One of our main results
is a terminating protocol counting the size $n$ of the system w.h.p.. We then
use this protocol as a subroutine in order to develop our universal
constructors, establishing that the nodes can self-organize w.h.p. into
arbitrarily complex shapes while still detecting termination of the
construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01916</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01916</id><created>2015-03-06</created><authors><author><keyname>Meeds</keyname><forenames>Edward</forenames></author><author><keyname>Leenders</keyname><forenames>Robert</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Hamiltonian ABC</title><categories>stat.ML cs.LG q-bio.QM</categories><comments>Submission to UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Bayesian computation (ABC) is a powerful and elegant framework
for performing inference in simulation-based models. However, due to the
difficulty in scaling likelihood estimates, ABC remains useful for relatively
low-dimensional problems. We introduce Hamiltonian ABC (HABC), a set of
likelihood-free algorithms that apply recent advances in scaling Bayesian
learning using Hamiltonian Monte Carlo (HMC) and stochastic gradients. We find
that a small number forward simulations can effectively approximate the ABC
gradient, allowing Hamiltonian dynamics to efficiently traverse parameter
spaces. We also describe a new simple yet general approach of incorporating
random seeds into the state of the Markov chain, further reducing the random
walk behavior of HABC. We demonstrate HABC on several typical ABC problems, and
show that HABC samples comparably to regular Bayesian inference using true
gradients on a high-dimensional problem from machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01918</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01918</id><created>2015-03-06</created><authors><author><keyname>Kristan</keyname><forenames>Matej</forenames></author><author><keyname>Sulic</keyname><forenames>Vildana</forenames></author><author><keyname>Kovacic</keyname><forenames>Stanislav</forenames></author><author><keyname>Pers</keyname><forenames>Janez</forenames></author></authors><title>Fast image-based obstacle detection from unmanned surface vehicles</title><categories>cs.CV</categories><comments>This is an extended version of the ACCV2014 paper [Kristan et al.,
  2014] submitted to a journal. [Kristan et al., 2014] M. Kristan, J. Pers, V.
  Sulic, S. Kovacic, A graphical model for rapid obstacle image-map estimation
  from unmanned surface vehicles, in Proc. Asian Conf. Computer Vision, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Obstacle detection plays an important role in unmanned surface vehicles
(USV). The USVs operate in highly diverse environments in which an obstacle may
be a floating piece of wood, a scuba diver, a pier, or a part of a shoreline,
which presents a significant challenge to continuous detection from images
taken onboard. This paper addresses the problem of online detection by
constrained unsupervised segmentation. To this end, a new graphical model is
proposed that affords a fast and continuous obstacle image-map estimation from
a single video stream captured onboard a USV. The model accounts for the
semantic structure of marine environment as observed from USV by imposing weak
structural constraints. A Markov random field framework is adopted and a highly
efficient algorithm for simultaneous optimization of model parameters and
segmentation mask estimation is derived. Our approach does not require
computationally intensive extraction of texture features and comfortably runs
in real-time. The algorithm is tested on a new, challenging, dataset for
segmentation and obstacle detection in marine environments, which is the
largest annotated dataset of its kind. Results on this dataset show that our
model outperforms the related approaches, while requiring a fraction of
computational effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01919</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01919</id><created>2015-03-06</created><authors><author><keyname>S&#xf8;nderby</keyname><forenames>S&#xf8;ren Kaae</forenames></author><author><keyname>S&#xf8;nderby</keyname><forenames>Casper Kaae</forenames></author><author><keyname>Nielsen</keyname><forenames>Henrik</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>Convolutional LSTM Networks for Subcellular Localization of Proteins</title><categories>q-bio.QM cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning is widely used to analyze biological sequence data.
Non-sequential models such as SVMs or feed-forward neural networks are often
used although they have no natural way of handling sequences of varying length.
Recurrent neural networks such as the long short term memory (LSTM) model on
the other hand are designed to handle sequences. In this study we demonstrate
that LSTM networks predict the subcellular location of proteins given only the
protein sequence with high accuracy (0.902) outperforming current state of the
art algorithms. We further improve the performance by introducing convolutional
filters and experiment with an attention mechanism which lets the LSTM focus on
specific parts of the protein. Lastly we introduce new visualizations of both
the convolutional filters and the attention mechanisms and show how they can be
used to extract biological relevant knowledge from the LSTM networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01934</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01934</id><created>2015-03-06</created><authors><author><keyname>Moulick</keyname><forenames>Subhayan Roy</forenames></author><author><keyname>Arora</keyname><forenames>Siddharth</forenames></author><author><keyname>Jain</keyname><forenames>Chirag</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Reliable SVD based Semi-blind and Invisible Watermarking Schemes</title><categories>cs.MM</categories><comments>11 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semi-blind watermarking scheme is presented based on Singular Value
Decomposition (SVD), which makes essential use of the fact that, the SVD
subspace preserves significant amount of information of an image and is a one
way decomposition. The principal components are used, along with the
corresponding singular vectors of the watermark image to watermark the target
image. For further security, the semi-blind scheme is extended to an invisible
hash based watermarking scheme. The hash based scheme commits a watermark with
a key such that, it is incoherent with the actual watermark, and can only be
extracted using the key. Its security is analyzed in the random oracle model
and shown to be unforgeable, invisible and satisfying the property of
non-repudiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01954</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01954</id><created>2015-03-06</created><updated>2015-09-21</updated><authors><author><keyname>Probst</keyname><forenames>Malte</forenames></author></authors><title>Denoising Autoencoders for fast Combinatorial Black Box Optimization</title><categories>cs.NE</categories><comments>corrected typos and small inconsistencies</comments><acm-class>I.2.6; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of Distribution Algorithms (EDAs) require flexible probability
models that can be efficiently learned and sampled. Autoencoders (AE) are
generative stochastic networks with these desired properties. We integrate a
special type of AE, the Denoising Autoencoder (DAE), into an EDA and evaluate
the performance of DAE-EDA on several combinatorial optimization problems with
a single objective. We asses the number of fitness evaluations as well as the
required CPU times. We compare the results to the performance to the Bayesian
Optimization Algorithm (BOA) and RBM-EDA, another EDA which is based on a
generative neural network which has proven competitive with BOA. For the
considered problem instances, DAE-EDA is considerably faster than BOA and
RBM-EDA, sometimes by orders of magnitude. The number of fitness evaluations is
higher than for BOA, but competitive with RBM-EDA. These results show that DAEs
can be useful tools for problems with low but non-negligible fitness evaluation
costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01955</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01955</id><created>2015-03-06</created><authors><author><keyname>Hemenway</keyname><forenames>Brett</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>Linear-time list recovery of high-rate expander codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that expander codes, when properly instantiated, are high-rate list
recoverable codes with linear-time list recovery algorithms. List recoverable
codes have been useful recently in constructing efficiently list-decodable
codes, as well as explicit constructions of matrices for compressive sensing
and group testing. Previous list recoverable codes with linear-time decoding
algorithms have all had rate at most 1/2; in contrast, our codes can have rate
$1 - \epsilon$ for any $\epsilon &gt; 0$. We can plug our high-rate codes into a
construction of Meir (2014) to obtain linear-time list recoverable codes of
arbitrary rates, which approach the optimal trade-off between the number of
non-trivial lists provided and the rate of the code. While list-recovery is
interesting on its own, our primary motivation is applications to
list-decoding. A slight strengthening of our result would implies linear-time
and optimally list-decodable codes for all rates, and our work is a step in the
direction of solving this important problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01958</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01958</id><created>2015-03-06</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Deckelbaum</keyname><forenames>Alan</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>Mechanism Design via Optimal Transport</title><categories>cs.GT</categories><acm-class>J.4</acm-class><doi>10.1145/2482540.2482593</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal mechanisms have been provided in quite general multi-item settings,
as long as each bidder's type distribution is given explicitly by listing every
type in the support along with its associated probability. In the implicit
setting, e.g. when the bidders have additive valuations with independent and/or
continuous values for the items, these results do not apply, and it was
recently shown that exact revenue optimization is intractable, even when there
is only one bidder. Even for item distributions with special structure, optimal
mechanisms have been surprisingly rare and the problem is challenging even in
the two-item case. In this paper, we provide a framework for designing optimal
mechanisms using optimal transport theory and duality theory. We instantiate
our framework to obtain conditions under which only pricing the grand bundle is
optimal in multi-item settings (complementing the work of [Manelli and Vincent
2006], as well as to characterize optimal two-item mechanisms. We use our
results to derive closed-form descriptions of the optimal mechanism in several
two-item settings, exhibiting also a setting where a continuum of lotteries is
necessary for revenue optimization but a closed-form representation of the
mechanism can still be found efficiently using our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01960</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01960</id><created>2015-03-06</created><authors><author><keyname>Gutierrez</keyname><forenames>Claudio</forenames></author></authors><title>Conferences vs. Journals: Throwing the baby out with the bath water?</title><categories>cs.DL</categories><comments>Written in 2010 / circulated privately / never published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Criticism of the conference model should be put in context. Evidences suggest
that the essential features of this model have emerged as responses to
challenges posed by current trends of scientific research and the impact of the
new techno-economic paradigm, the age of Information and Communication
Technology. This context seems indispensable when discussing today's problems
of scientific evaluation, in particular the Conference vs. Journal (CvJ)
debate. This debate, also, would benefit from systematic historical and
sociological studies of these practices. In this note we briefly develop these
arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01967</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01967</id><created>2015-03-06</created><updated>2015-12-01</updated><authors><author><keyname>Rodis</keyname><forenames>Panteleimon</forenames></author></authors><title>Information entropy as an anthropomorphic concept</title><categories>cs.IT cs.AI math.IT</categories><comments>Improvements in mathematical definitions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to E.T. Jaynes and E.P. Wigner, entropy is an anthropomorphic
concept in the sense that in a physical system correspond many thermodynamic
systems. The physical system can be examined from many points of view each time
examining different variables and calculating entropy differently. In this
paper we discuss how this concept may be applied in information entropy; how
Shannon's definition of entropy can fit in Jayne's and Wigner's statement. This
is achieved by generalizing Shannon's notion of information entropy and this is
the main contribution of the paper. Then we discuss how entropy under these
considerations may be used for the comparison of password complexity and as a
measure of diversity useful in the analysis of the behavior of genetic
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01981</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01981</id><created>2015-03-06</created><updated>2015-07-30</updated><authors><author><keyname>Platzer</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>A Uniform Substitution Calculus for Differential Dynamic Logic</title><categories>cs.LO cs.PL math.LO</categories><msc-class>03F03, 03B70, 34A38</msc-class><acm-class>F.4.1; F.3.1; F.3.2; I.2.3</acm-class><doi>10.1007/978-3-319-21401-6_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new proof calculus for differential dynamic logic
(dL) that is entirely based on uniform substitution, a proof rule that
substitutes a formula for a predicate symbol everywhere. Uniform substitutions
make it possible to rely on axioms rather than axiom schemata, substantially
simplifying implementations. Instead of nontrivial schema variables and
soundness-critical side conditions on the occurrence patterns of variables, the
resulting calculus adopts only a finite number of ordinary dL formulas as
axioms. The static semantics of differential dynamic logic is captured
exclusively in uniform substitutions and bound variable renamings as opposed to
being spread in delicate ways across the prover implementation. In addition to
sound uniform substitutions, this paper introduces differential forms for
differential dynamic logic that make it possible to internalize differential
invariants, differential substitutions, and derivations as first-class axioms
in dL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01986</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01986</id><created>2015-03-06</created><updated>2015-03-16</updated><authors><author><keyname>Rabin</keyname><forenames>Julien</forenames></author><author><keyname>Papadakis</keyname><forenames>Nicolas</forenames></author></authors><title>Convex Color Image Segmentation with Optimal Transport Distances</title><categories>cs.CV</categories><comments>A short version of this report has been submitted to the Fifth
  International Conference on Scale Space and Variational Methods in Computer
  Vision (SSVM) 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This work is about the use of regularized optimal-transport distances for
convex, histogram-based image segmentation. In the considered framework, fixed
exemplar histograms define a prior on the statistical features of the two
regions in competition. In this paper, we investigate the use of various
transport-based cost functions as discrepancy measures and rely on a
primal-dual algorithm to solve the obtained convex optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.01993</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.01993</id><created>2015-03-06</created><updated>2015-08-17</updated><authors><author><keyname>Soltani</keyname><forenames>Sara</forenames></author><author><keyname>Andersen</keyname><forenames>Martin S.</forenames></author><author><keyname>Hansen</keyname><forenames>Per Christian</forenames></author></authors><title>Tomographic Image Reconstruction using Training images</title><categories>cs.CV math.NA</categories><comments>25 pages, 12 figures</comments><msc-class>65F22, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and examine an algorithm for tomographic image reconstruction
where prior knowledge about the solution is available in the form of training
images. We first construct a nonnegative dictionary based on prototype elements
from the training images; this problem is formulated as a regularized
non-negative matrix factorization. Incorporating the dictionary as a prior in a
convex reconstruction problem, we then find an approximate solution with a
sparse representation in the dictionary. The dictionary is applied to
non-overlapping patches of the image, which reduces the computational
complexity compared to other algorithms. Computational experiments clarify the
choice and interplay of the model parameters and the regularization parameters,
and we show that in few-projection low-dose settings our algorithm is
competitive with total variation regularization and tends to include more
texture and more correct edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02007</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02007</id><created>2015-03-06</created><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author><author><keyname>Holzleitner</keyname><forenames>Manuel</forenames></author></authors><title>Qualitative Analysis of Integration Adapter Modeling</title><categories>cs.SE</categories><comments>7 pages, Quantitative Analysis of Adapter Types and Tasks and
  Qualitative Modeling User Studies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integration Adapters are a fundamental part of an integration system, since
they provide (business) applications access to its messaging channel. However,
their modeling and configuration remain under-represented. In previous work,
the integration control and data flow syntax and semantics have been expressed
in the Business Process Model and Notation (BPMN) as a semantic model for
message-based integration, while adapter and the related quality of service
modeling were left for further studies.
  In this work we specify common adapter capabilities and derive general
modeling patterns, for which we define a compliant representation in BPMN. The
patterns extend previous work by the adapter flow, evaluated syntactically and
semantically for common adapter characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02009</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02009</id><created>2015-03-05</created><authors><author><keyname>Consoli</keyname><forenames>Sergio</forenames></author><author><keyname>P&#xe8;rez</keyname><forenames>Jos&#xe8; Andr&#xe8;s Moreno</forenames></author><author><keyname>Mladenovic</keyname><forenames>Nenad</forenames></author></authors><title>Towards an intelligent VNS heuristic for the k-labelled spanning forest
  problem</title><categories>cs.OH cs.AI</categories><comments>2 pages, Fifteenth International Conference on Computer Aided Systems
  Theory (EUROCAST 2015), Las Palmas de Gran Canaria, Spain</comments><journal-ref>Computer Aided Systems Theory, pages 79-80 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a currently ongoing project, we investigate a new possibility for solving
the k-labelled spanning forest (kLSF) problem by an intelligent Variable
Neighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given
an undirected input graph G and an integer positive value k, and the aim is to
find a spanning forest of G having the minimum number of connected components
and the upper bound k on the number of labels to use. The problem is related to
the minimum labelling spanning tree (MLST) problem, whose goal is to get the
spanning tree of the input graph with the minimum number of labels, and has
several applications in the real world, where one aims to ensure connectivity
by means of homogeneous connections. The Int-VNS metaheuristic that we propose
for the kLSF problem is derived from the promising intelligent VNS strategy
recently proposed for the MLST problem, and integrates the basic VNS for the
kLSF problem with other complementary approaches from machine learning,
statistics and experimental algorithmics, in order to produce high-quality
performance and to completely automate the resulting strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02031</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02031</id><created>2015-03-06</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Kulkarni</keyname><forenames>Vivek</forenames></author><author><keyname>Thakurta</keyname><forenames>Abhradeep</forenames></author><author><keyname>Williams</keyname><forenames>Oliver</forenames></author></authors><title>To Drop or Not to Drop: Robustness, Consistency and Differential Privacy
  Properties of Dropout</title><categories>cs.LG cs.NE stat.ML</categories><comments>Currently under review for ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training deep belief networks (DBNs) requires optimizing a non-convex
function with an extremely large number of parameters. Naturally, existing
gradient descent (GD) based methods are prone to arbitrarily poor local minima.
In this paper, we rigorously show that such local minima can be avoided (upto
an approximation error) by using the dropout technique, a widely used heuristic
in this domain. In particular, we show that by randomly dropping a few nodes of
a one-hidden layer neural network, the training objective function, up to a
certain approximation error, decreases by a multiplicative factor.
  On the flip side, we show that for training convex empirical risk minimizers
(ERM), dropout in fact acts as a &quot;stabilizer&quot; or regularizer. That is, a simple
dropout based GD method for convex ERMs is stable in the face of arbitrary
changes to any one of the training points. Using the above assertion, we show
that dropout provides fast rates for generalization error in learning (convex)
generalized linear models (GLM). Moreover, using the above mentioned stability
properties of dropout, we design dropout based differentially private
algorithms for solving ERMs. The learned GLM thus, preserves privacy of each of
the individual training points while providing accurate predictions for new
test points. Finally, we empirically validate our stability assertions for
dropout in the context of convex ERMs and show that surprisingly, dropout
significantly outperforms (in terms of prediction accuracy) the L2
regularization based methods for several benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02041</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02041</id><created>2015-03-06</created><updated>2015-06-11</updated><authors><author><keyname>Gangeh</keyname><forenames>Mehrdad J.</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author></authors><title>On the Invariance of Dictionary Learning and Sparse Representation to
  Projecting Data to a Discriminative Space</title><categories>cs.CV</categories><comments>We would like to withdraw this paper as it seems that the proof
  provided in the paper is not including all the cases mentioned</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, it is proved that dictionary learning and sparse
representation is invariant to a linear transformation. It subsumes the special
case of transforming/projecting the data into a discriminative space. This is
important because recently, supervised dictionary learning algorithms have been
proposed, which suggest to include the category information into the learning
of dictionary to improve its discriminative power. Among them, there are some
approaches that propose to learn the dictionary in a discriminative projected
space. To this end, two approaches have been proposed: first, assigning the
discriminative basis as the dictionary and second, perform dictionary learning
in the projected space. Based on the invariance of dictionary learning to any
transformation in general, and to a discriminative space in particular, we
advocate the first approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02045</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02045</id><created>2015-03-01</created><authors><author><keyname>Routtenberg</keyname><forenames>Tirza</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author></authors><title>Estimation after Parameter Selection: Performance Analysis and
  Estimation Methods</title><categories>cs.IT math.IT</categories><comments>A submitted paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical parameter estimation problems, prescreening and parameter
selection are performed prior to estimation. In this paper, we consider the
problem of estimating a preselected unknown deterministic parameter chosen from
a parameter set based on observations according to a predetermined selection
rule, $\Psi$. The data-based parameter selection process may impact the
subsequent estimation by introducing a selection bias and creating coupling
between decoupled parameters. This paper introduces a post-selection mean
squared error (PSMSE) criterion as a performance measure. A corresponding
Cram\'er-Rao-type bound on the PSMSE of any $\Psi$-unbiased estimator is
derived, where the $\Psi$-unbiasedness is in the Lehmann-unbiasedness sense.
The post-selection maximum-likelihood (PSML) estimator is presented .It is
proved that if there exists an $\Psi$-unbiased estimator that achieves the
$\Psi$-Cram\'er-Rao bound (CRB), i.e. an $\Psi$-efficient estimator, then it is
produced by the PSML estimator. In addition, iterative methods are developed
for the practical implementation of the PSML estimator. Finally, the proposed
$\Psi$-CRB and PSML estimator are examined in estimation after parameter
selection with different distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02064</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02064</id><created>2015-03-06</created><authors><author><keyname>Sadigh</keyname><forenames>Arash Khoshkbar</forenames></author><author><keyname>Hydari</keyname><forenames>Mojtaba</forenames></author><author><keyname>Tedde</keyname><forenames>Marco</forenames></author><author><keyname>Arghandeh</keyname><forenames>Reza</forenames></author><author><keyname>Smedley</keyname><forenames>Keyue</forenames></author><author><keyname>von Meier</keyname><forenames>Alexandra</forenames></author></authors><title>A Unified Platform Enabling Power System Circuit Model Data Transfer
  Among Different Software</title><categories>cs.SY</categories><comments>5 Pages ISGT2015 Washington DC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diversity of software packages to simulate the power system circuits is
considerable. It is challenging to transfer power system circuit model data
(PSCMD) among different software tools and rebuild the same circuit in the
second software environment. This paper proposes a unified platform (UP) where
PSCMD are stored in a spreadsheet file with a defined format. Script-based
PSCMD transfer applications, written in MATLAB, have been developed for a set
of software to read the circuit model data from the UP spreadsheet and
reconstruct the circuit in the destination software. This significantly eases
the process of transferring circuit model data between each pair of software
tools. In this paper ETAP, OpenDSS, Grid LabD, and DEW are considered. In order
to test the developed PSCMD transfer applications, circuit model data of a test
circuit and an actual sample circuit from a Californian utility company, both
built in CYME, were exported into the spreadsheet file according to the UP
format. Thereafter, circuit model data were imported successfully from the
spreadsheet files into all above mentioned software using the PSCMD transfer
applications developed for each software individually. Finally, load flow
analysis is performed in all software and the obtained results match with each
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02086</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02086</id><created>2015-03-06</created><updated>2015-06-29</updated><authors><author><keyname>Purohit</keyname><forenames>Hemant</forenames><affiliation>Ohio Center of Excellence in Knowledge-enabled Computing</affiliation><affiliation>Department of Computer Science and Engineering</affiliation></author><author><keyname>Banerjee</keyname><forenames>Tanvi</forenames><affiliation>Ohio Center of Excellence in Knowledge-enabled Computing</affiliation><affiliation>Department of Computer Science and Engineering</affiliation></author><author><keyname>Hampton</keyname><forenames>Andrew</forenames><affiliation>Ohio Center of Excellence in Knowledge-enabled Computing</affiliation><affiliation>Department of Psychology</affiliation></author><author><keyname>Shalin</keyname><forenames>Valerie L.</forenames><affiliation>Ohio Center of Excellence in Knowledge-enabled Computing</affiliation><affiliation>Department of Psychology</affiliation></author><author><keyname>Bhandutia</keyname><forenames>Nayanesh</forenames><affiliation>United Nations Population Fund Headquarters NYC, USA</affiliation></author><author><keyname>Sheth</keyname><forenames>Amit P.</forenames><affiliation>Ohio Center of Excellence in Knowledge-enabled Computing</affiliation><affiliation>Department of Computer Science and Engineering</affiliation></author></authors><title>Gender-Based Violence in 140 Characters or Fewer: A #BigData Case Study
  of Twitter</title><categories>cs.SI cs.CY</categories><acm-class>H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public institutions are increasingly reliant on data from social media sites
to measure public attitude and provide timely public engagement. Such reliance
includes the exploration of public views on important social issues such as
gender-based violence (GBV). In this study, we examine big (social) data
consisting of nearly fourteen million tweets collected from Twitter over a
period of ten months to analyze public opinion regarding GBV, highlighting the
nature of tweeting practices by geographical location and gender. We
demonstrate the utility of Computational Social Science to mine insight from
the corpus while accounting for the influence of both transient events and
sociocultural factors. We reveal public awareness regarding GBV tolerance and
suggest opportunities for intervention and the measurement of intervention
effectiveness assisting both governmental and non-governmental organizations in
policy development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02090</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02090</id><created>2015-03-06</created><authors><author><keyname>Imbiriba</keyname><forenames>T.</forenames><affiliation>Federal University of Santa Catarina, Florian&#xf3;polis, SC, Brazil</affiliation></author><author><keyname>Bermudez</keyname><forenames>J. C. M.</forenames><affiliation>Federal University of Santa Catarina, Florian&#xf3;polis, SC, Brazil</affiliation></author><author><keyname>Richard</keyname><forenames>C.</forenames><affiliation>Universit&#xe9; de Nice Sophia-Antipolis, CNRS, Nice, France</affiliation></author><author><keyname>Tourneret</keyname><forenames>J. -Y.</forenames><affiliation>University of Toulouse, IRIT-ENSEEIHT, CNRS, Toulouse, France</affiliation></author></authors><title>Band selection in RKHS for fast nonlinear unmixing of hyperspectral
  images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The profusion of spectral bands generated by the acquisition process of
hyperspectral images generally leads to high computational costs. Such
difficulties arise in particular with nonlinear unmixing methods, which are
naturally more complex than linear ones. This complexity, associated with the
high redundancy of information within the complete set of bands, make the
search of band selection algorithms relevant. With this work, we propose a band
selection strategy in reproducing kernel Hilbert spaces that allows to
drastically reduce the processing time required by nonlinear unmixing
techniques. Simulation results show a complexity reduction of two orders of
magnitude without compromising unmixing performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02101</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02101</id><created>2015-03-06</created><authors><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Huang</keyname><forenames>Furong</forenames></author><author><keyname>Jin</keyname><forenames>Chi</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author></authors><title>Escaping From Saddle Points --- Online Stochastic Gradient for Tensor
  Decomposition</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze stochastic gradient descent for optimizing non-convex functions.
In many cases for non-convex functions the goal is to find a reasonable local
minimum, and the main concern is that gradient updates are trapped in saddle
points. In this paper we identify strict saddle property for non-convex problem
that allows for efficient optimization. Using this property we show that
stochastic gradient descent converges to a local minimum in a polynomial number
of iterations. To the best of our knowledge this is the first work that gives
global convergence guarantees for stochastic gradient descent on non-convex
functions with exponentially many local minima and saddle points. Our analysis
can be applied to orthogonal tensor decomposition, which is widely used in
learning a rich class of latent variable models. We propose a new optimization
formulation for the tensor decomposition problem that has strict saddle
property. As a result we get the first online algorithm for orthogonal tensor
decomposition with global convergence guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02108</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02108</id><created>2015-03-06</created><updated>2015-08-12</updated><authors><author><keyname>Huang</keyname><forenames>Zhen</forenames></author><author><keyname>Siniscalchi</keyname><forenames>Sabato Marco</forenames></author><author><keyname>Chen</keyname><forenames>I-Fan</forenames></author><author><keyname>Wu</keyname><forenames>Jiadong</forenames></author><author><keyname>Lee</keyname><forenames>Chin-Hui</forenames></author></authors><title>Maximum a Posteriori Adaptation of Network Parameters in Deep Models</title><categories>cs.LG cs.CL cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian approach to adapting parameters of a well-trained
context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to
improve automatic speech recognition performance. Given an abundance of DNN
parameters but with only a limited amount of data, the effectiveness of the
adapted DNN model can often be compromised. We formulate maximum a posteriori
(MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an
augmented linear hidden networks connected to the output tied states, or
senones, and compare it to feature space MAP linear regression previously
proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street
Journal task demonstrate the feasibility of the proposed framework. In
supervised adaptation, the proposed MAP adaptation approach provides more than
10% relative error reduction and consistently outperforms the conventional
transformation based methods. Furthermore, we present an initial attempt to
generate hierarchical priors to improve adaptation efficiency and effectiveness
with limited adaptation data by exploiting similarities among senones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02118</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02118</id><created>2015-03-06</created><authors><author><keyname>Sichani</keyname><forenames>Arash Kh.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>Parameterization of Stabilizing Linear Coherent Quantum Controllers</title><categories>quant-ph cs.SY math.OC</categories><comments>11 pages, 4 figures, a version of this paper is to appear in the
  Proceedings of the 10th Asian Control Conference, Kota Kinabalu, Malaysia, 31
  May - 3 June, 2015</comments><msc-class>81Q93, 93D15, 47N70, 93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with application of the classical Youla-Ku\v{c}era
parameterization to finding a set of linear coherent quantum controllers that
stabilize a linear quantum plant. The plant and controller are assumed to
represent open quantum harmonic oscillators modelled by linear quantum
stochastic differential equations. The interconnections between the plant and
the controller are assumed to be established through quantum bosonic fields. In
this framework, conditions for the stabilization of a given linear quantum
plant via linear coherent quantum feedback are addressed using a stable
factorization approach. The class of stabilizing quantum controllers is
parameterized in the frequency domain. Also, this approach is used in order to
formulate coherent quantum weighted $H_2$ and $H_\infty$ control problems for
linear quantum systems in the frequency domain. Finally, a projected gradient
descent scheme is proposed to solve the coherent quantum weighted $H_2$ control
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02120</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02120</id><created>2015-03-06</created><updated>2015-07-28</updated><authors><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Clark</keyname><forenames>Eric M.</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Identifying missing dictionary entries with frequency-conserving context
  models</title><categories>cs.CL cs.IT math.IT stat.ML</categories><comments>16 pages, 6 figures, and 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an effort to better understand meaning from natural language texts, we
explore methods aimed at organizing lexical objects into contexts. A number of
these methods for organization fall into a family defined by word ordering.
Unlike demographic or spatial partitions of data, these collocation models are
of special importance for their universal applicability. While we are
interested here in text and have framed our treatment appropriately, our work
is potentially applicable to other areas of research (e.g., speech, genomics,
and mobility patterns) where one has ordered categorical data, (e.g., sounds,
genes, and locations). Our approach focuses on the phrase (whether word or
larger) as the primary meaning-bearing lexical unit and object of study. To do
so, we employ our previously developed framework for generating word-conserving
phrase-frequency data. Upon training our model with the Wiktionary---an
extensive, online, collaborative, and open-source dictionary that contains over
100,000 phrasal-definitions---we develop highly effective filters for the
identification of meaningful, missing phrase-entries. With our predictions we
then engage the editorial community of the Wiktionary and propose short lists
of potential missing entries for definition, developing a breakthrough, lexical
extraction technique, and expanding our knowledge of the defined English
lexicon of phrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02122</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02122</id><created>2015-03-06</created><authors><author><keyname>Sichani</keyname><forenames>Arash Kh.</forenames></author><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Robust Mean Square Stability of Open Quantum Stochastic Systems with
  Hamiltonian Perturbations in a Weyl Quantization Form</title><categories>quant-ph cs.SY math.OC</categories><comments>11 pages, Proceedings of the Australian Control Conference, Canberra,
  17-18 November, 2014, pp. 83-88</comments><msc-class>81Q93, 81S05, 81S22, 81S25, 93D09, 93E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with open quantum systems whose dynamic variables
satisfy canonical commutation relations and are governed by quantum stochastic
differential equations. The latter are driven by quantum Wiener processes which
represent external boson fields. The system-field coupling operators are linear
functions of the system variables. The Hamiltonian consists of a nominal
quadratic function of the system variables and an uncertain perturbation which
is represented in a Weyl quantization form. Assuming that the nominal linear
quantum system is stable, we develop sufficient conditions on the perturbation
of the Hamiltonian which guarantee robust mean square stability of the
perturbed system. Examples are given to illustrate these results for a class of
Hamiltonian perturbations in the form of trigonometric polynomials of the
system variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02123</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02123</id><created>2015-03-06</created><authors><author><keyname>Compagno</keyname><forenames>Alberto</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>To NACK or not to NACK? Negative Acknowledgments in Information-Centric
  Networking</title><categories>cs.NI</categories><comments>10 pages, 7 figures</comments><doi>10.1109/ICCCN.2015.7288477</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-Centric Networking (ICN) is an internetworking paradigm that
offers an alternative to the current IP\nobreakdash-based Internet
architecture. ICN's most distinguishing feature is its emphasis on information
(content) instead of communication endpoints. One important open issue in ICN
is whether negative acknowledgments (NACKs) at the network layer are useful for
notifying downstream nodes about forwarding failures, or requests for incorrect
or non-existent information. In benign settings, NACKs are beneficial for ICN
architectures, such as CCNx and NDN, since they flush state in routers and
notify consumers. In terms of security, NACKs seem useful as they can help
mitigating so-called Interest Flooding attacks. However, as we show in this
paper, network-layer NACKs also have some unpleasant security implications. We
consider several types of NACKs and discuss their security design requirements
and implications. We also demonstrate that providing secure NACKs triggers the
threat of producer-bound flooding attacks. Although we discuss some potential
countermeasures to these attacks, the main conclusion of this paper is that
network-layer NACKs are best avoided, at least for security reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02128</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02128</id><created>2015-03-06</created><updated>2015-06-17</updated><authors><author><keyname>Tang</keyname><forenames>Qingming</forenames></author><author><keyname>Yang</keyname><forenames>Chao</forenames></author><author><keyname>Peng</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>Exact Hybrid Covariance Thresholding for Joint Graphical Lasso</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of estimating multiple related Gaussian
graphical models from a $p$-dimensional dataset consisting of different
classes. Our work is based upon the formulation of this problem as group
graphical lasso. This paper proposes a novel hybrid covariance thresholding
algorithm that can effectively identify zero entries in the precision matrices
and split a large joint graphical lasso problem into small subproblems. Our
hybrid covariance thresholding method is superior to existing uniform
thresholding methods in that our method can split the precision matrix of each
individual class using different partition schemes and thus split group
graphical lasso into much smaller subproblems, each of which can be solved very
fast. In addition, this paper establishes necessary and sufficient conditions
for our hybrid covariance thresholding algorithm. The superior performance of
our thresholding method is thoroughly analyzed and illustrated by a few
experiments on simulated data and real gene expression data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02129</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02129</id><created>2015-03-06</created><updated>2015-06-17</updated><authors><author><keyname>Tang</keyname><forenames>Qingming</forenames></author><author><keyname>Sun</keyname><forenames>Siqi</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the network structure underlying data is an important problem in
machine learning. This paper introduces a novel prior to study the inference of
scale-free networks, which are widely used to model social and biological
networks. The prior not only favors a desirable global node degree
distribution, but also takes into consideration the relative strength of all
the possible edges adjacent to the same node and the estimated degree of each
individual node.
  To fulfill this, ranking is incorporated into the prior, which makes the
problem challenging to solve. We employ an ADMM (alternating direction method
of multipliers) framework to solve the Gaussian Graphical model regularized by
this prior. Our experiments on both synthetic and real data show that our prior
not only yields a scale-free network, but also produces many more correctly
predicted edges than the others such as the scale-free inducing prior, the
hub-inducing prior and the $l_1$ norm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02136</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02136</id><created>2015-03-07</created><authors><author><keyname>Dhokley</keyname><forenames>Waheeda</forenames></author><author><keyname>Munifa</keyname><forenames>Khan</forenames></author><author><keyname>Nazia</keyname><forenames>Shaikh</forenames></author><author><keyname>Saiqua</keyname><forenames>Shaikh</forenames></author></authors><title>An Improved Image Mosaicing Algorithm for Damaged Documents</title><categories>cs.CV</categories><comments>6 pages, 10 figures</comments><journal-ref>IJASCSE, Volume 4, Issue 2, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is a common phenomenon in day to day life; where in some of the document
gets damaged. Out of several reasons, the main reason for documents getting
damaged is shredding by hands. Recovery of such documents is essential. Manual
recovery of such damaged document is tedious and time consuming task. In this
paper, we are describing an algorithm which recovers the original document from
such shredded pieces of the same. In order to implement this, we are using a
simple technique called Image Mosaicing. In this technique a complete new image
is developed using two or more torn fragments. For simplicity of
implementation, we are considering only two torn pieces of a document that will
be mosaiced together. The successful implementation of this algorithm would
lead to recovery of important information which in turn would be beneficial in
various fields such as forensic sciences, archival study, etc
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02143</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02143</id><created>2015-03-07</created><authors><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Sun</keyname><forenames>Xingping</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author></authors><title>Model selection of polynomial kernel regression</title><categories>cs.LG</categories><comments>29 pages, 4 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial kernel regression is one of the standard and state-of-the-art
learning strategies. However, as is well known, the choices of the degree of
polynomial kernel and the regularization parameter are still open in the realm
of model selection. The first aim of this paper is to develop a strategy to
select these parameters. On one hand, based on the worst-case learning rate
analysis, we show that the regularization term in polynomial kernel regression
is not necessary. In other words, the regularization parameter can decrease
arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On
the other hand,taking account of the implementation of the algorithm, the
regularization term is required. Summarily, the effect of the regularization
term in polynomial kernel regression is only to circumvent the &quot; ill-condition&quot;
of the kernel matrix. Based on this, the second purpose of this paper is to
propose a new model selection strategy, and then design an efficient learning
algorithm. Both theoretical and experimental analysis show that the new
strategy outperforms the previous one. Theoretically, we prove that the new
learning strategy is almost optimal if the regression function is smooth.
Experimentally, it is shown that the new strategy can significantly reduce the
computational burden without loss of generalization capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02144</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02144</id><created>2015-03-07</created><authors><author><keyname>Yang</keyname><forenames>Linxiao</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>Sparse Bayesian Dictionary Learning with a Gaussian Hierarchical Model</title><categories>cs.LG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dictionary learning problem whose objective is to design a
dictionary such that the signals admits a sparse or an approximate sparse
representation over the learned dictionary. Such a problem finds a variety of
applications such as image denoising, feature extraction, etc. In this paper,
we propose a new hierarchical Bayesian model for dictionary learning, in which
a Gaussian-inverse Gamma hierarchical prior is used to promote the sparsity of
the representation. Suitable priors are also placed on the dictionary and the
noise variance such that they can be reasonably inferred from the data. Based
on the hierarchical model, a variational Bayesian method and a Gibbs sampling
method are developed for Bayesian inference. The proposed methods have the
advantage that they do not require the knowledge of the noise variance \emph{a
priori}. Numerical results show that the proposed methods are able to learn the
dictionary with an accuracy better than existing methods, particularly for the
case where there is a limited number of training signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02155</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02155</id><created>2015-03-07</created><authors><author><keyname>Stiakogiannakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Touati</keyname><forenames>Corinne</forenames></author></authors><title>Adaptive Power Allocation and Control in Time-Varying Multi-Carrier MIMO
  Networks</title><categories>cs.IT math.IT</categories><comments>25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the fundamental trade-off between radiated power
and achieved throughput in wireless multi-carrier, multiple-input and
multiple-output (MIMO) systems that vary with time in an unpredictable fashion
(e.g. due to changes in the wireless medium or the users' QoS requirements).
Contrary to the static/stationary channel regime, there is no optimal power
allocation profile to target (either static or in the mean), so the system's
users must adapt to changes in the environment &quot;on the fly&quot;, without being able
to predict the system's evolution ahead of time. In this dynamic context, we
formulate the users' power/throughput trade-off as an online optimization
problem and we provide a matrix exponential learning algorithm that leads to no
regret - i.e. the proposed transmit policy is asymptotically optimal in
hindsight, irrespective of how the system evolves over time. Furthermore, we
also examine the robustness of the proposed algorithm under imperfect channel
state information (CSI) and we show that it retains its regret minimization
properties under very mild conditions on the measurement noise statistics. As a
result, users are able to track the evolution of their individually optimum
transmit profiles remarkably well, even under rapidly changing network
conditions and high uncertainty. Our theoretical analysis is validated by
extensive numerical simulations corresponding to a realistic network deployment
and providing further insights in the practical implementation aspects of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02164</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02164</id><created>2015-03-07</created><authors><author><keyname>Zhang</keyname><forenames>Shubao</forenames></author><author><keyname>Qian</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>A Nonconvex Approach for Structured Sparse Learning</title><categories>cs.IT cs.LG math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.4575</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse learning is an important topic in many areas such as machine learning,
statistical estimation, signal processing, etc. Recently, there emerges a
growing interest on structured sparse learning. In this paper we focus on the
$\ell_q$-analysis optimization problem for structured sparse learning ($0&lt; q
\leq 1$). Compared to previous work, we establish weaker conditions for exact
recovery in noiseless case and a tighter non-asymptotic upper bound of estimate
error in noisy case. We further prove that the nonconvex $\ell_q$-analysis
optimization can do recovery with a lower sample complexity and in a wider
range of cosparsity than its convex counterpart. In addition, we develop an
iteratively reweighted method to solve the optimization problem under the
variational framework. Theoretical analysis shows that our method is capable of
pursuing a local minima close to the global minima. Also, empirical results of
preliminary computational experiments illustrate that our nonconvex method
outperforms both its convex counterpart and other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02173</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02173</id><created>2015-03-07</created><updated>2015-12-17</updated><authors><author><keyname>Guth</keyname><forenames>Larry</forenames></author><author><keyname>Zahl</keyname><forenames>Joshua</forenames></author></authors><title>Algebraic curves, rich points, and doubly-ruled surfaces</title><categories>math.AG cs.CG math.CO</categories><comments>34 pages, 0 figures. v2: Minor tweak to the definition of the affine
  Chow variety of curves. references updated and typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of collections of algebraic curves in three dimensions
that have many curve-curve incidences. In particular, let $k$ be a field and
let $\mathcal{L}$ be a collection of $n$ space curves in $k^3$, with
$n&lt;\!\!&lt;(\operatorname{char}(k))^2$ or $\operatorname{char}(k)=0$. Then either
A) there are at most $O(n^{3/2})$ points in $k^3$ hit by at least two curves,
or B) at least $\Omega(n^{1/2})$ curves from $\mathcal{L}$ must lie on a
bounded-degree surface, and many of the curves must form two &quot;rulings&quot; of this
surface.
  We also develop several new tools including a generalization of the classical
flecnode polynomial of Salmon and new algebraic techniques for dealing with
this generalized flecnode polynomial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02192</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02192</id><created>2015-03-07</created><authors><author><keyname>de Figueiredo</keyname><forenames>Felipe A. P.</forenames></author><author><keyname>Miranda</keyname><forenames>Joao Paulo</forenames></author><author><keyname>Figueiredo</keyname><forenames>Fabricio L.</forenames></author><author><keyname>Cardoso</keyname><forenames>Fabbryccio A. C. M.</forenames></author></authors><title>Uplink Performance Evaluation of Massive MU-MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The present paper deals with an OFDM-based uplink within a multi-user MIMO
(MU-MIMO) system where a massive MIMO approach is employed. In this context,
the linear detectors Minimum Mean-Squared Error (MMSE), Zero Forcing (ZF) and
Maximum Ratio Combining (MRC) are considered and assessed. This papers includes
Bit Error Rate (BER) results for uncoded QPSK/OFDM transmissions through a flat
Rayleigh fading channel under the assumption of perfect power control and
channel estimation. BER results are obtained through Monte Carlo simulations.
Performance results are discussed in detail and we confirm the achievable
&quot;massive MIMO&quot; effects, even for a reduced complexity detection technique, when
the number of receive antennas at BS is much larger than the number of transmit
antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02193</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02193</id><created>2015-03-07</created><updated>2015-08-24</updated><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Charikar</keyname><forenames>Moses</forenames></author><author><keyname>Lai</keyname><forenames>Kevin A.</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>Label optimal regret bounds for online local learning</title><categories>cs.LG</categories><comments>13 pages; Changes from previous version: small changes to proofs of
  Theorems 1 &amp; 2, a small rewrite of introduction as well (this version is the
  same as camera-ready copy in COLT '15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We resolve an open question from (Christiano, 2014b) posed in COLT'14
regarding the optimal dependency of the regret achievable for online local
learning on the size of the label set. In this framework the algorithm is shown
a pair of items at each step, chosen from a set of $n$ items. The learner then
predicts a label for each item, from a label set of size $L$ and receives a
real valued payoff. This is a natural framework which captures many interesting
scenarios such as collaborative filtering, online gambling, and online max cut
among others. (Christiano, 2014a) designed an efficient online learning
algorithm for this problem achieving a regret of $O(\sqrt{nL^3T})$, where $T$
is the number of rounds. Information theoretically, one can achieve a regret of
$O(\sqrt{n \log L T})$. One of the main open questions left in this framework
concerns closing the above gap.
  In this work, we provide a complete answer to the question above via two main
results. We show, via a tighter analysis, that the semi-definite programming
based algorithm of (Christiano, 2014a), in fact achieves a regret of
$O(\sqrt{nLT})$. Second, we show a matching computational lower bound. Namely,
we show that a polynomial time algorithm for online local learning with lower
regret would imply a polynomial time algorithm for the planted clique problem
which is widely believed to be hard. We prove a similar hardness result under a
related conjecture concerning planted dense subgraphs that we put forth. Unlike
planted clique, the planted dense subgraph problem does not have any known
quasi-polynomial time algorithms.
  Computational lower bounds for online learning are relatively rare, and we
hope that the ideas developed in this work will lead to lower bounds for other
online learning scenarios as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02196</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02196</id><created>2015-03-07</created><authors><author><keyname>Datta</keyname><forenames>Mrinmoy</forenames></author><author><keyname>Ghorpade</keyname><forenames>Sudhir R.</forenames></author></authors><title>Higher Weights of Affine Grassmann Codes and Their Duals</title><categories>cs.IT math.CO math.IT</categories><comments>13 pages; to appear in the Proceedings of AGCT (Luminy, France, June
  2013)</comments><msc-class>Primary 15A03, 11T06 05E99 Secondary 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of determining the higher weights or the generalized
Hamming weights of affine Grassmann codes and their duals. Several initial as
well as terminal higher weights of affine Grassmann codes of an arbitrary level
are determined explicitly. In the case of duals of these codes, we give a
formula for many initial as well as terminal higher weights. As a special case,
we obtain an alternative simpler proof of the formula of Beelen et al for the
minimum distance of the dual of an affine Grasmann code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02200</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02200</id><created>2015-03-07</created><updated>2015-10-01</updated><authors><author><keyname>Adamczyk</keyname><forenames>Marek</forenames></author><author><keyname>Borodin</keyname><forenames>Allan</forenames></author><author><keyname>Ferraioli</keyname><forenames>Diodato</forenames></author><author><keyname>de Keijzer</keyname><forenames>Bart</forenames></author><author><keyname>Leonardi</keyname><forenames>Stefano</forenames></author></authors><title>Sequential Posted Price Mechanisms with Correlated Valuations</title><categories>cs.GT</categories><comments>29 pages, To appear in WINE 2015</comments><acm-class>J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the revenue performance of sequential posted price mechanisms and
some natural extensions, for a general setting where the valuations of the
buyers are drawn from a correlated distribution. Sequential posted price
mechanisms are conceptually simple mechanisms that work by proposing a
take-it-or-leave-it offer to each buyer. We apply sequential posted price
mechanisms to single-parameter multi-unit settings in which each buyer demands
only one item and the mechanism can assign the service to at most k of the
buyers. For standard sequential posted price mechanisms, we prove that with the
valuation distribution having finite support, no sequential posted price
mechanism can extract a constant fraction of the optimal expected revenue, even
with unlimited supply. We extend this result to the the case of a continuous
valuation distribution when various standard assumptions hold simultaneously.
In fact, it turns out that the best fraction of the optimal revenue that is
extractable by a sequential posted price mechanism is proportional to ratio of
the highest and lowest possible valuation. We prove that for two simple
generalizations of these mechanisms, a better revenue performance can be
achieved: if the sequential posted price mechanism has for each buyer the
option of either proposing an offer or asking the buyer for its valuation, then
a Omega(1/max{1,d}) fraction of the optimal revenue can be extracted, where d
denotes the degree of dependence of the valuations, ranging from complete
independence (d=0) to arbitrary dependence (d=n-1). Moreover, when we
generalize the sequential posted price mechanisms further, such that the
mechanism has the ability to make a take-it-or-leave-it offer to the i-th buyer
that depends on the valuations of all buyers except i's, we prove that a
constant fraction (2-sqrt{e})/4~0.088 of the optimal revenue can be always be
extracted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02207</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02207</id><created>2015-03-07</created><authors><author><keyname>Beelen</keyname><forenames>Peter</forenames></author><author><keyname>Ghorpade</keyname><forenames>Sudhir R.</forenames></author><author><keyname>Hasan</keyname><forenames>Sartaj Ul</forenames></author></authors><title>Linear Codes associated to Determinantal Varieties</title><categories>math.CO cs.IT math.AG math.IT</categories><comments>12 pages; to appear in Discrete Math</comments><msc-class>Primary. 94B05, 14M12 Secondary 14G15, 13C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of linear codes associated to projective algebraic
varieties defined by the vanishing of minors of a fixed size of a generic
matrix. It is seen that the resulting code has only a small number of distinct
weights. The case of varieties defined by the vanishing of 2 x 2 minors is
considered in some detail. Here we obtain the complete weight distribution.
Moreover, several generalized Hamming weights are determined explicitly and it
is shown that the first few of them coincide with the distinct nonzero weights.
One of the tools used is to determine the maximum possible number of matrices
of rank 1 in a linear space of matrices of a given dimension over a finite
field. In particular, we determine the structure and the maximum possible
dimension of linear spaces of matrices in which every nonzero matrix has rank
1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02208</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02208</id><created>2015-03-07</created><updated>2015-05-23</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Davies</keyname><forenames>Sylvie</forenames></author></authors><title>Quotient Complexities of Atoms in Regular Ideal Languages</title><categories>cs.FL</categories><comments>17 pages, 4 figures, two tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A (left) quotient of a language $L$ by a word $w$ is the language
$w^{-1}L=\{x\mid wx\in L\}$. The quotient complexity of a regular language $L$
is the number of quotients of $L$; it is equal to the state complexity of $L$,
which is the number of states in a minimal deterministic finite automaton
accepting $L$. An atom of $L$ is an equivalence class of the relation in which
two words are equivalent if for each quotient, they either are both in the
quotient or both not in it; hence it is a non-empty intersection of
complemented and uncomplemented quotients of $L$. A right (respectively, left
and two-sided) ideal is a language $L$ over an alphabet $\Sigma$ that satisfies
$L=L\Sigma^*$ (respectively, $L=\Sigma^*L$ and $L=\Sigma^*L\Sigma^*$). We
compute the maximal number of atoms and the maximal quotient complexities of
atoms of right, left and two-sided regular ideals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02210</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02210</id><created>2015-03-07</created><updated>2015-06-06</updated><authors><author><keyname>Facchini</keyname><forenames>Alessandro</forenames><affiliation>University of Warsaw</affiliation></author><author><keyname>Hirai</keyname><forenames>Yoichi</forenames><affiliation>National Institute of Advanced Industrial Science and Technology</affiliation></author><author><keyname>Marx</keyname><forenames>Maarten</forenames><affiliation>University of Amsterdam</affiliation></author><author><keyname>Sherkhonov</keyname><forenames>Evgeny</forenames><affiliation>University of Amsterdam</affiliation></author></authors><title>Containment for Conditional Tree Patterns</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:4) 2015</journal-ref><doi>10.2168/LMCS-11(2:4)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Conditional Tree Pattern (CTP) expands an XML tree pattern with labels
attached to the descendant edges. These labels can be XML element names or
Boolean CTPs. The meaning of a descendant edge labelled by A and ending in a
node labelled by B is a path of child steps ending in a B node such that all
intermediate nodes are A nodes. In effect this expresses the until B, A holds
construction from temporal logic.This paper studies the containment problem for
CTP. For tree patterns (TP), this problem is known to be coNP-complete. We show
that it is PSPACE-complete for CTP. This increase in complexity is due to the
fact that CTP is expressive enough to encode an unrestricted form of label
negation: ${*}\setminus a$, meaning &quot;any node except an a-node&quot;. Containment of
TP expanded with this type of negation is already PSPACE-hard. CTP is a
positive, forward, first order fragment of Regular XPath. Unlike TP, CTP
expanded with disjunction is not equivalent to unions of CTP's. Like TP, CTP is
a natural fragment to consider: CTP is closed under intersections and CTP with
disjunction is equally expressive as positive existential first order logic
expanded with the until operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02216</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02216</id><created>2015-03-07</created><authors><author><keyname>Yang</keyname><forenames>Yuning</forenames></author><author><keyname>Mehrkanoon</keyname><forenames>Siamak</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>Higher order Matching Pursuit for Low Rank Tensor Learning</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank tensor learning, such as tensor completion and multilinear multitask
learning, has received much attention in recent years. In this paper, we
propose higher order matching pursuit for low rank tensor learning problems
with a convex or a nonconvex cost function, which is a generalization of the
matching pursuit type methods. At each iteration, the main cost of the proposed
methods is only to compute a rank-one tensor, which can be done efficiently,
making the proposed methods scalable to large scale problems. Moreover, storing
the resulting rank-one tensors is of low storage requirement, which can help to
break the curse of dimensionality. The linear convergence rate of the proposed
methods is established in various circumstances. Along with the main methods,
we also provide a method of low computational complexity for approximately
computing the rank-one tensors, with provable approximation ratio, which helps
to improve the efficiency of the main methods and to analyze the convergence
rate. Experimental results on synthetic as well as real datasets verify the
efficiency and effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02217</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02217</id><created>2015-03-07</created><authors><author><keyname>Smarandache</keyname><forenames>Roxana</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Bounding the Bethe and the Degree-$M$ Bethe Permanents</title><categories>cs.IT cs.CC math-ph math.CO math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently conjectured that the permanent of a ${P}$-lifting
$\theta^{\uparrow{P}}$ of a matrix $\theta$ of degree $M$ is less than or equal
to the $M$th power of the permanent perm$(\theta)$, i.e.,
perm$(\theta^{\uparrow{P}})\leq(\text{perm}(\theta))^M$ and, consequently, that
the degree-$M$ Bethe permanent $\text{perm}_{M,\mathrm{B}} (\theta)$ of a
matrix $\theta$ is less than or equal to the permanent perm$(\theta)$ of
$\theta$, i.e., perm$_{M, \mathrm{B}} (\theta)\leq \text{perm}(\theta)$. In
this paper, we prove these related conjectures and show in addition a few
properties of the permanent of block matrices that are lifts of a matrix. As a
corollary, we obtain an alternative proof of the inequality perm$_{\mathrm{B}}
(\theta)\leq \text{perm}(\theta)$ on the Bethe permanent of the base matrix
$\theta$ that uses only the combinatorial definition of the Bethe permanent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02230</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02230</id><created>2015-03-07</created><authors><author><keyname>Ong</keyname><forenames>Hao Yi</forenames></author><author><keyname>Deolalikar</keyname><forenames>Sunil</forenames></author><author><keyname>Peng</keyname><forenames>Mark</forenames></author></authors><title>Player Behavior and Optimal Team Composition for Online Multiplayer
  Games</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider clustering player behavior and learning the optimal team
composition for multiplayer online games. The goal is to determine a set of
descriptive play style groupings and learn a predictor for win/loss outcomes.
The predictor takes in as input the play styles of the participants in each
team; i.e., the various team compositions in a game. Our framework uses
unsupervised learning to find behavior clusters, which are, in turn, used with
classification algorithms to learn the outcome predictor. For our numerical
experiments, we consider League of Legends, a popular team-based role-playing
game developed by Riot Games. We observe the learned clusters to not only
corroborate well with game knowledge, but also provide insights surprising to
expert players. We also demonstrate that game outcomes can be predicted with
fairly high accuracy given team composition-based features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02233</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02233</id><created>2015-03-07</created><authors><author><keyname>Stickley</keyname><forenames>Nathaniel R.</forenames></author><author><keyname>Aragon-Calvo</keyname><forenames>Miguel A.</forenames></author></authors><title>StratOS: A Big Data Framework for Scientific Computing</title><categories>astro-ph.IM cs.DC</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce StratOS, a Big Data platform for general computing that allows a
datacenter to be treated as a single computer. With StratOS, the process of
writing a massively parallel program for a datacenter is no more complicated
than writing a Python script for a desktop computer. Users can run pre-existing
analysis software on data distributed over thousands of machines with just a
few keystrokes. This greatly reduces the time required to develop distributed
data analysis pipelines. The platform is built upon industry-standard,
open-source Big Data technologies, from which it inherits fast data throughput
and fault tolerance. StratOS enhances these technologies by adding an intuitive
user interface, automated task monitoring, and other usability features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02239</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02239</id><created>2015-03-07</created><authors><author><keyname>Feng</keyname><forenames>Ruyong</forenames></author></authors><title>On the Computation of the Galois Group of Linear Difference Equations</title><categories>cs.SC</categories><comments>23 pages</comments><msc-class>39A06, 68W30</msc-class><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that determines the Galois group of linear difference
equations with rational function coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02240</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02240</id><created>2015-03-07</created><authors><author><keyname>Sinha</keyname><forenames>Abhinav</forenames></author><author><keyname>Anastasopoulos</keyname><forenames>Achilleas</forenames></author></authors><title>A General Mechanism Design Methodology for Social Utility Maximization
  with Linear Constraints</title><categories>cs.GT</categories><journal-ref>ACM SIGMETRICS Performance Evaluation Review, Volume 42 Issue 3,
  December 2014, pages 12-15</journal-ref><doi>10.1145/2695533.2695538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social utility maximization refers to the process of allocating resources in
such a way that the sum of agents' utilities is maximized under the system
constraints. Such allocation arises in several problems in the general area of
communications, including unicast (and multicast multi-rate) service on the
Internet, as well as in applications with (local) public goods, such as power
allocation in wireless networks, spectrum allocation, etc. Mechanisms that
implement such allocations in Nash equilibrium have also been studied but
either they do not possess full implementation property, or are given in a
case-by-case fashion, thus obscuring fundamental understanding of these
problems.
  In this paper we propose a unified methodology for creating mechanisms that
fully implement, in Nash equilibria, social utility maximizing functions
arising in various contexts where the constraints are convex. The construction
of the mechanism is done in a systematic way by considering the dual
optimization problem. In addition to the required properties of efficiency and
individual rationality that such mechanisms ought to satisfy, three additional
design goals are the focus of this paper: a) the size of the message space
scaling linearly with the number of agents (even if agents' types are entire
valuation functions), b) allocation being feasible on and off equilibrium, and
c) strong budget balance at equilibrium and also off equilibrium whenever
demand is feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02241</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02241</id><created>2015-03-07</created><authors><author><keyname>Arnon</keyname><forenames>Dan</forenames><affiliation>EMC Corporation</affiliation></author><author><keyname>Sharma</keyname><forenames>Navindra</forenames><affiliation>EMC Corporation</affiliation></author></authors><title>An Analysis of a Virtually Synchronous Protocol</title><categories>cs.DC</categories><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise-scale systems such as those used for cloud computing require a
scalable and highly available infrastructure. One crucial ingredient of such an
infrastructure is the ability to replicate data coherently among a group of
cooperating processes in the presence of process failures and group membership
changes. The last few decades have seen prolific research into efficient
protocols for such data replication. One family of such protocols are the
virtually synchronous protocols. Virtually synchronous protocols achieve their
efficiency by limiting their synchronicity guarantee to messages that bear a
causal relationship to each other. Such protocols have found wide-ranging
commercial uses over the years. One protocol in particular, the CBCAST protocol
developed by Birman, Schiper and Stephenson in 1991 and used in their ISIS
platform was particularly promising due to its unique no-wait properties, but
has suffered from seemingly intractable race conditions. In this paper we
describe a corrected version of this protocol and prove its formal properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02244</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02244</id><created>2015-03-07</created><updated>2015-04-27</updated><authors><author><keyname>Saldi</keyname><forenames>Naci</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Asymptotic Optimality of Finite Approximations to Markov Decision
  Processes with Borel Spaces</title><categories>math.OC cs.SY</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculating optimal policies is known to be computationally difficult for
Markov decision processes with Borel state and action spaces and for partially
observed Markov decision processes even with finite state and action spaces.
This paper studies finite-state approximations of discrete time Markov decision
processes with Borel state and action spaces, for both discounted and average
costs criteria. The stationary policies thus obtained are shown to approximate
the optimal stationary policy with arbitrary precision under mild technical
conditions. For compact-state MDPs, we obtain explicit rates of convergence
bounds quantifying how the approximation improves as the size of the
approximating finite state space increases. Using information theoretic
arguments, the order optimality of the obtained rates of convergence is
established for a large class of problems. We also show that, as a
pre-processing setup, action space can taken to be finite with sufficiently
large number points for the finite-state approximation problem; thereby, well
known algorithms, such as value or policy iteration, Q-learning, etc., can be
used to calculate near optimal policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02261</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02261</id><created>2015-03-08</created><authors><author><keyname>Jhawar</keyname><forenames>Ravi</forenames></author><author><keyname>Kordy</keyname><forenames>Barbara</forenames></author><author><keyname>Mauw</keyname><forenames>Sjouke</forenames></author><author><keyname>Radomirovic</keyname><forenames>Sasa</forenames></author><author><keyname>Trujillo-Rasua</keyname><forenames>Rolando</forenames></author></authors><title>Attack Trees with Sequential Conjunction</title><categories>cs.CR</categories><comments>This is an extended version of the work published at IFIP SEC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first formal foundation of SAND attack trees which are a
popular extension of the well-known attack trees. The SAND attack tree
formalism increases the expressivity of attack trees by introducing the
sequential conjunctive operator SAND. This operator enables the modeling of
ordered events.
  We give a semantics to SAND attack trees by interpreting them as sets of
series-parallel graphs and propose a complete axiomatization of this semantics.
We define normal forms for SAND attack trees and a term rewriting system which
allows identification of semantically equivalent trees. Finally, we formalize
how to quantitatively analyze SAND attack trees using attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02266</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02266</id><created>2015-03-08</created><updated>2015-05-21</updated><authors><author><keyname>Keshtkarjahromi</keyname><forenames>Yasaman</forenames></author><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Ansari</keyname><forenames>Rashid</forenames></author><author><keyname>Khokhar</keyname><forenames>Ashfaq</forenames></author></authors><title>Network Coding for Cooperative Mobile Devices with Multiple Interfaces</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation among mobile devices and utilizing multiple interfaces such as
cellular and local area links simultaneously are promising to meet the
increasing throughput demand over cellular links. In particular, when mobile
devices are in the close proximity of each other and are interested in the same
content, device-to-device connections such as WiFi-Direct, in addition to
cellular links, can be utilized to construct a cooperative system. However, it
is crucial to understand the potential of network coding for cooperating mobile
devices with multiple interfaces. In this paper, we consider this problem, and
(i) develop network coding schemes for cooperative mobile devices with multiple
interfaces, and (ii) characterize the performance of network coding by using
the number of transmissions to recover all packets as a performance metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02276</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02276</id><created>2015-03-08</created><authors><author><keyname>Benerjee</keyname><forenames>Krishna Gopal</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author></authors><title>Tradeoff for Heterogeneous Distributed Storage Systems between Storage
  and Repair Cost</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures, draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider heterogeneous distributed storage systems (DSSs)
having flexible reconstruction degree, where each node in the system has
dynamic repair bandwidth and dynamic storage capacity. In particular, a data
collector can reconstruct the file at time $t$ using some arbitrary nodes in
the system and for a node failure the system can be repaired by some set of
arbitrary nodes. Using $min$-$cut$ bound, we investigate the fundamental
tradeoff between storage and repair cost for our model of heterogeneous DSS. In
particular, the problem is formulated as bi-objective optimization linear
programing problem. For an arbitrary DSS, it is shown that the calculated
$min$-$cut$ bound is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02286</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02286</id><created>2015-03-08</created><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Three-Source Extractors for Polylogarithmic Min-Entropy</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the study of constructing explicit extractors for independent
general weak random sources. The ultimate goal is to give a construction that
matches what is given by the probabilistic method --- an extractor for two
independent $n$-bit weak random sources with min-entropy as small as $\log
n+O(1)$. Previously, the best known result in the two-source case is an
extractor by Bourgain \cite{Bourgain05}, which works for min-entropy $0.49n$;
and the best known result in the general case is an earlier work of the author
\cite{Li13b}, which gives an extractor for a constant number of independent
sources with min-entropy $\mathsf{polylog(n)}$. However, the constant in the
construction of \cite{Li13b} depends on the hidden constant in the best known
seeded extractor, and can be large; moreover the error in that construction is
only $1/\mathsf{poly(n)}$.
  In this paper, we make two important improvements over the result in
\cite{Li13b}. First, we construct an explicit extractor for \emph{three}
independent sources on $n$ bits with min-entropy $k \geq \mathsf{polylog(n)}$.
In fact, our extractor works for one independent source with poly-logarithmic
min-entropy and another independent block source with two blocks each having
poly-logarithmic min-entropy. Thus, our result is nearly optimal, and the next
step would be to break the $0.49n$ barrier in two-source extractors. Second, we
improve the error of the extractor from $1/\mathsf{poly(n)}$ to
$2^{-k^{\Omega(1)}}$, which is almost optimal and crucial for cryptographic
applications. Some of the techniques developed here may be of independent
interests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02291</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02291</id><created>2015-03-08</created><updated>2016-02-01</updated><authors><author><keyname>Funke</keyname><forenames>Jan</forenames></author><author><keyname>Moreno-Noguer</keyname><forenames>Francesc</forenames></author><author><keyname>Cardona</keyname><forenames>Albert</forenames></author><author><keyname>Cook</keyname><forenames>Matthew</forenames></author></authors><title>TED: A Tolerant Edit Distance for Segmentation Evaluation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel error measure to compare a segmentation
against ground truth. This measure, which we call Tolerant Edit Distance (TED),
is motivated by two observations: (1) Some errors, like small boundary shifts,
are tolerable in practice. Which errors are tolerable is application dependent
and should be a parameter of the measure. (2) Non-tolerable errors have to be
corrected manually. The time needed to do so should be reflected by the error
measure. Using integer linear programming, the TED finds the minimal weighted
sum of split and merge errors exceeding a given tolerance criterion, and thus
provides a time-to-fix estimate. In contrast to commonly used measures like
Rand index or variation of information, the TED (1) does not count small, but
tolerable, differences, (2) provides intuitive numbers, (3) gives a time-to-fix
estimate, and (4) can localize and classify the type of errors. By supporting
both isotropic and anisotropic volumes and having a flexible tolerance
criterion, the TED can be adapted to different requirements. On example
segmentations for 3D neuron segmentation, we demonstrate that the TED is
capable of counting topological errors, while ignoring small boundary shifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02292</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02292</id><created>2015-03-08</created><authors><author><keyname>Niu</keyname><forenames>Yong</forenames></author><author><keyname>Gao</keyname><forenames>Chuhan</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Exploiting Device-to-Device Communications in Joint Scheduling of Access
  and Backhaul for mmWave Small Cells</title><categories>cs.NI</categories><comments>16 pages, 26 figures, a journal paper, Accepted by IEEE JSAC Special
  Issue on Recent Advances in Heterogeneous Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of mobile data demand, there has been an increasing
interest in deploying small cells of higher frequency bands underlying the
conventional homogeneous macrocell network, which is usually referred to as
heterogeneous cellular networks, to significantly boost the overall network
capacity. With vast amounts of spectrum available in the millimeter wave
(mmWave) band, small cells at mmWave frequencies are able to provide
multi-gigabit access data rates, while the wireless backhaul in the mmWave band
is emerging as a cost-effective solution to provide high backhaul capacity to
connect access points (APs) of the small cells. In order to operate the mobile
network optimally, it is necessary to jointly design the radio access and
backhaul networks. Meanwhile, direct transmissions between devices should also
be considered to improve system performance and enhance the user experience. In
this paper, we propose a joint transmission scheduling scheme for the radio
access and backhaul of small cells in the mmWave band, termed D2DMAC, where a
path selection criterion is designed to enable device-to-device transmissions
for performance improvement. In D2DMAC, a concurrent transmission scheduling
algorithm is proposed to fully exploit spatial reuse in mmWave networks.
Through extensive simulations under various traffic patterns and user
deployments, we demonstrate D2DMAC achieves near-optimal performance in some
cases, and outperforms other protocols significantly in terms of delay and
throughput. Furthermore, we also analyze the impact of path selection on the
performance improvement of D2DMAC under different selected parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02295</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02295</id><created>2015-03-08</created><updated>2015-03-10</updated><authors><author><keyname>Osegi</keyname><forenames>N. E.</forenames></author><author><keyname>Enyindah</keyname><forenames>P.</forenames></author></authors><title>GOptimaEmbed: A SmartSMS-SQLDatabaseManagementSystem for Low-Cost
  Microcontrollers</title><categories>cs.DB</categories><comments>To be published</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The era of the Internet of things, machine-to-machine and human to machine
computing has heralded the development of a modern-day smart industry in which
humanoids can co-operate,co-exist and interact seamlessly.Currently, there are
many projects in this area of smart communication and thus giving rise to an
industry electrified by smart things.In this paper we present a novel smart
database management system (dbms),GOptimaEmbed, for intelligent querying of
databases in device constrained embedded systems. The system uses genetic
algorithms as main search engine and simplifies the query process using stored
in-memory model based on an invented device dependent
Short-messaging-Structured Query Language SMS SQL schema translator. In
addition, querying is done over the air using integrated GSM module in the
smart space. The system has been applied to querying a plant database and
results were quite satisfactory.
  Keywords. GOptimaEmbed,smart dbms, genetic algorithms, SMS SQL
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02298</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02298</id><created>2015-03-08</created><authors><author><keyname>Robertson</keyname><forenames>Neil</forenames></author><author><keyname>Seymour</keyname><forenames>P. D.</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Cyclically five-connected cubic graphs</title><categories>math.CO cs.DM</categories><comments>47 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cubic graph $G$ is cyclically 5-connected if $G$ is simple, 3-connected,
has at least 10 vertices and for every set $F$ of edges of size at most four,
at most one component of $G\backslash F$ contains circuits. We prove that if
$G$ and $H$ are cyclically 5-connected cubic graphs and $H$ topologically
contains $G$, then either $G$ and $H$ are isomorphic, or (modulo well-described
exceptions) there exists a cyclically 5-connected cubic graph $G'$ such that
$H$ topologically contains $G'$ and $G'$ is obtained from $G$ in one of the
following two ways. Either $G'$ is obtained from $G$ by subdividing two
distinct edges of $G$ and joining the two new vertices by an edge, or $G'$ is
obtained from $G$ by subdividing each edge of a circuit of length five and
joining the new vertices by a matching to a new circuit of length five disjoint
from $G$ in such a way that the cyclic orders of the two circuits agree. We
prove a companion result, where by slightly increasing the connectivity of $H$
we are able to eliminate the second construction. We also prove versions of
both of these results when $G$ is almost cyclically 5-connected in the sense
that it satisfies the definition except for 4-edge cuts such that one side is a
circuit of length four. In this case $G'$ is required to be almost cyclically
5-connected and to have fewer circuits of length four than $G$. In particular,
if $G$ has at most one circuit of length four, then $G'$ is required to be
cyclically 5-connected. However, in this more general setting the operations
describing the possible graphs $G'$ are more complicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02300</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02300</id><created>2015-03-08</created><authors><author><keyname>Shi</keyname><forenames>Zhenwu</forenames></author><author><keyname>Zhang</keyname><forenames>Fumin</forenames></author></authors><title>Model Predictive Control under Timing Constraints induced by Controller
  Area Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When multiple model predictive controllers are implemented on a shared
control area network (CAN), their performance may degrade due to the
inhomogeneous timing and delays among messages. The priority based real-time
scheduling of messages on the CAN introduces complex timing of events,
especially when the types and number of messages change at runtime. This paper
introduces a novel hybrid timing model to make runtime predictions on the
timing of the messages for a finite time window. Controllers can be designed
using the optimization algorithms for model predictive control by considering
the timing as optimization constraints. This timing model allows multiple
controllers to share a CAN without significant degradation in the controller
performance. The timing model also provides a convenient way to check the
schedulability of messages on the CAN at runtime. Simulation results
demonstrate that the timing model is accurate and computationally efficient to
meet the needs of real-time implementation. Simulation results also demonstrate
that model predictive controllers designed when considering the timing
constraints have superior performance than the controllers designed without
considering the timing constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02302</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02302</id><created>2015-03-08</created><authors><author><keyname>Schwartz</keyname><forenames>Richard A</forenames></author><author><keyname>Torre</keyname><forenames>Gabriele</forenames></author><author><keyname>Massone</keyname><forenames>Anna Maria</forenames></author><author><keyname>Piana</keyname><forenames>Michele</forenames></author></authors><title>DESAT: an SSW tool for SDO/AIA image de-saturation</title><categories>astro-ph.IM cs.CV</categories><msc-class>85-08, 68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saturation affects a significant rate of images recorded by the Atmospheric
Imaging Assembly on the Solar Dynamics Observatory. This paper describes a
computational method and a technological pipeline for the de-saturation of such
images, based on several mathematical ingredients like Expectation
Maximization, image correlation and interpolation. An analysis of the
computational properties and demands of the pipeline, together with an
assessment of its reliability are performed against a set of data recorded from
the Feburary 25 2014 flaring event.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02304</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02304</id><created>2015-03-08</created><authors><author><keyname>Singh</keyname><forenames>Kirat Pal</forenames></author><author><keyname>Kumar</keyname><forenames>Dilip</forenames></author></authors><title>Efficient Hardware Design and Implementation of Encrypted MIPS Processor</title><categories>cs.CR cs.AR</categories><comments>1st International Conference on Innovations and Advancements in
  Information and Communication Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes the design and hardware implementation of 32-bit
encrypted MIPS processor based on MIPS pipeline architecture. The organization
of pipeline stages in such a way that pipeline can be clocked at high
frequency. Encryption and Decryption blocks of data encryption standard (DES)
cryptosystem and dependency among themselves are explained in detail with the
help of block diagram. In order to increase the processor functionality and
performance, especially for security applications we include three new
instructions 32-bit LKLW, LKUW and CRYPT. The design has been synthesized at
40nm process technology targeting using Xilinx Virtex-6 device. The encrypted
MIPS pipeline processor can work at 218MHz at synthesis level and 744MHz at
simulation level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02313</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02313</id><created>2015-03-08</created><updated>2016-01-04</updated><authors><author><keyname>Liu</keyname><forenames>Ling</forenames></author><author><keyname>Yan</keyname><forenames>Yanfei</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author></authors><title>Achieving Secrecy Capacity of the Gaussian Wiretap Channel with Polar
  Lattices</title><categories>cs.IT math.IT</categories><comments>35 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, an explicit wiretap coding scheme based on polar lattices is
proposed to achieve the secrecy capacity of the additive white Gaussian noise
(AWGN) wiretap channel. Firstly, polar lattices are used to construct
secrecy-good lattices for the mod-$\Lambda_s$ Gaussian wiretap channel. Then we
propose an explicit shaping scheme to remove this mod-$\Lambda_s$ front end and
extend polar lattices to the genuine Gaussian wiretap channel. The shaping
technique is based on the lattice Gaussian distribution, which leads to a
binary asymmetric channel at each level for the multilevel lattice codes. By
employing the asymmetric polar coding technique, we construct an AWGN-good
lattice and a secrecy-good lattice with optimal shaping simultaneously. As a
result, the encoding complexity for the sender and the decoding complexity for
the legitimate receiver are both O(N logN log(logN)). The proposed scheme is
proven to be semantically secure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02314</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02314</id><created>2015-03-08</created><authors><author><keyname>Al-Ameen</keyname><forenames>Mahdi Nasrullah</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author><author><keyname>Scielzo</keyname><forenames>Shannon</forenames></author></authors><title>Towards Making Random Passwords Memorable: Leveraging Users' Cognitive
  Ability Through Multiple Cues</title><categories>cs.HC</categories><comments>Will appear at CHI 2015 Conference, to be held at Seoul, Korea</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the choice, users produce passwords reflecting common strategies and
patterns that ease recall but offer uncertain and often weak security.
System-assigned passwords provide measurable security but suffer from poor
memorability. To address this usability-security tension, we argue that systems
should assign random passwords but also help with memorization and recall. We
investigate the feasibility of this approach with CuedR, a novel
cued-recognition authentication scheme that provides users with multiple cues
(visual, verbal, and spatial) and lets them choose the cues that best fit their
learning process for later recognition of system-assigned keywords. In our lab
study, all 37 of our participants could log in within three attempts one week
after registration (mean login time: 38.0 seconds). A pilot study on using
multiple CuedR passwords also showed 100% recall within three attempts. Based
on our results, we suggest appropriate applications for CuedR, such as
financial and e-commerce accounts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02318</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02318</id><created>2015-03-08</created><updated>2015-05-26</updated><authors><author><keyname>Deza</keyname><forenames>Arturo</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>Understanding Image Virality</title><categories>cs.SI cs.CV</categories><comments>Pre-print, IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virality of online content on social networking websites is an important but
esoteric phenomenon often studied in fields like marketing, psychology and data
mining. In this paper we study viral images from a computer vision perspective.
We introduce three new image datasets from Reddit, and define a virality score
using Reddit metadata. We train classifiers with state-of-the-art image
features to predict virality of individual images, relative virality in pairs
of images, and the dominant topic of a viral image. We also compare machine
performance to human performance on these tasks. We find that computers perform
poorly with low level features, and high level information is critical for
predicting virality. We encode semantic information through relative
attributes. We identify the 5 key visual attributes that correlate with
virality. We create an attribute-based characterization of images that can
predict relative virality with 68.10% accuracy (SVM+Deep Relative Attributes)
-- better than humans at 60.12%. Finally, we study how human prediction of
image virality varies with different `contexts' in which the images are viewed,
such as the influence of neighbouring images, images recently viewed, as well
as the image title or caption. This work is a first step in understanding the
complex but important phenomenon of image virality. Our datasets and
annotations will be made publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02319</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02319</id><created>2015-03-08</created><authors><author><keyname>Marti</keyname><forenames>Johannes</forenames></author><author><keyname>Seifan</keyname><forenames>Fatemeh</forenames></author><author><keyname>Venema</keyname><forenames>Yde</forenames></author></authors><title>Uniform Interpolation for Coalgebraic Fixpoint Logic</title><categories>cs.LO</categories><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the connection between automata and logic to prove that a wide class
of coalgebraic fixpoint logics enjoys uniform interpolation. To this aim, first
we generalize one of the central results in coalgebraic automata theory, namely
closure under projection, which is known to hold for weak-pullback preserving
functors, to a more general class of functors, i.e.; functors with
quasi-functorial lax extensions. Then we will show that closure under
projection implies definability of the bisimulation quantifier in the language
of coalgebraic fixpoint logic, and finally we prove the uniform interpolation
theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02328</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02328</id><created>2015-03-08</created><authors><author><keyname>Wu</keyname><forenames>Mike</forenames></author></authors><title>Financial Market Prediction</title><categories>cs.CE cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given financial data from popular sites like Yahoo and the London Exchange,
the presented paper attempts to model and predict stocks that can be considered
&quot;good investments&quot;. Stocks are characterized by 125 features ranging from gross
domestic product to EDIBTA, and are labeled by discrepancies between stock and
market price returns. An artificial neural network (Self-Organizing Map) is
fitted to train on more than a million data points to predict &quot;good
investments&quot; given testing stocks from 2013 and after.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02330</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02330</id><created>2015-03-08</created><authors><author><keyname>Huber</keyname><forenames>Patrik</forenames></author><author><keyname>Feng</keyname><forenames>Zhen-Hua</forenames></author><author><keyname>Christmas</keyname><forenames>William</forenames></author><author><keyname>Kittler</keyname><forenames>Josef</forenames></author><author><keyname>R&#xe4;tsch</keyname><forenames>Matthias</forenames></author></authors><title>Fitting 3D Morphable Models using Local Features</title><categories>cs.CV</categories><comments>Submitted to ICIP 2015; 4 pages, 4 figures</comments><msc-class>68T45</msc-class><acm-class>I.4.8; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel fitting method that uses local image
features to fit a 3D Morphable Model to 2D images. To overcome the obstacle of
optimising a cost function that contains a non-differentiable feature
extraction operator, we use a learning-based cascaded regression method that
learns the gradient direction from data. The method allows to simultaneously
solve for shape and pose parameters. Our method is thoroughly evaluated on
Morphable Model generated data and first results on real data are presented.
Compared to traditional fitting methods, which use simple raw features like
pixel colour or edge maps, local features have been shown to be much more
robust against variations in imaging conditions. Our approach is unique in that
we are the first to use local features to fit a Morphable Model.
  Because of the speed of our method, it is applicable for realtime
applications. Our cascaded regression framework is available as an open source
library (https://github.com/patrikhuber).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02332</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02332</id><created>2015-03-08</created><authors><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Paschalidis</keyname><forenames>Ioannis Ch.</forenames></author></authors><title>Robust Anomaly Detection in Dynamic Networks</title><categories>cs.NI stat.AP</categories><comments>6 pages. MED conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two robust methods for anomaly detection in dynamic networks in
which the properties of normal traffic are time-varying. We formulate the
robust anomaly detection problem as a binary composite hypothesis testing
problem and propose two methods: a model-free and a model-based one, leveraging
techniques from the theory of large deviations. Both methods require a family
of Probability Laws (PLs) that represent normal properties of traffic. We
devise a two-step procedure to estimate this family of PLs. We compare the
performance of our robust methods and their vanilla counterparts, which assume
that normal traffic is stationary, on a network with a diurnal normal pattern
and a common anomaly related to data exfiltration. Simulation results show that
our robust methods perform better than their vanilla counterparts in dynamic
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02335</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02335</id><created>2015-03-08</created><authors><author><keyname>Narasimhan</keyname><forenames>Karthik</forenames></author><author><keyname>Barzilay</keyname><forenames>Regina</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>An Unsupervised Method for Uncovering Morphological Chains</title><categories>cs.CL</categories><comments>11 pages, Appearing in the Transactions of the Association for
  Computational Linguistics (TACL), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most state-of-the-art systems today produce morphological analysis based only
on orthographic patterns. In contrast, we propose a model for unsupervised
morphological analysis that integrates orthographic and semantic views of
words. We model word formation in terms of morphological chains, from base
words to the observed words, breaking the chains into parent-child relations.
We use log-linear models with morpheme and word-level features to predict
possible parents, including their modifications, for each word. The limited set
of candidate parents for each word render contrastive estimation feasible. Our
model consistently matches or outperforms five state-of-the-art systems on
Arabic, English and Turkish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02337</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02337</id><created>2015-03-08</created><authors><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Paschalidis</keyname><forenames>Ioannis Ch.</forenames></author></authors><title>Botnet Detection using Social Graph Analysis</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages. Allerton Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signature-based botnet detection methods identify botnets by recognizing
Command and Control (C\&amp;C) traffic and can be ineffective for botnets that use
new and sophisticate mechanisms for such communications. To address these
limitations, we propose a novel botnet detection method that analyzes the
social relationships among nodes. The method consists of two stages: (i)
anomaly detection in an &quot;interaction&quot; graph among nodes using large deviations
results on the degree distribution, and (ii) community detection in a social
&quot;correlation&quot; graph whose edges connect nodes with highly correlated
communications. The latter stage uses a refined modularity measure and
formulates the problem as a non-convex optimization problem for which
appropriate relaxation strategies are developed. We apply our method to
real-world botnet traffic and compare its performance with other community
detection methods. The results show that our approach works effectively and the
refined modularity measure improves the detection accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02339</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02339</id><created>2015-03-08</created><updated>2015-08-21</updated><authors><author><keyname>Gerstoft</keyname><forenames>Peter</forenames></author><author><keyname>Xenaki</keyname><forenames>Angeliki</forenames></author><author><keyname>Mecklenbr&#xe4;uker</keyname><forenames>Christoph F.</forenames></author></authors><title>Multiple and single snapshot compressive beamforming</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>In press Journal of Acoustical Society of America</comments><doi>10.1121/1.4929941</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a sound field observed on a sensor array, compressive sensing (CS)
reconstructs the direction-of-arrival (DOA) of multiple sources using a
sparsity constraint. The DOA estimation is posed as an underdetermined problem
by expressing the acoustic pressure at each sensor as a phase-lagged
superposition of source amplitudes at all hypothetical DOAs. Regularizing with
an $\ell_1$-norm constraint renders the problem solvable with convex
optimization, and promoting sparsity gives high-resolution DOA maps. Here, the
sparse source distribution is derived using maximum a posteriori (MAP)
estimates for both single and multiple snapshots. CS does not require inversion
of the data covariance matrix and thus works well even for a single snapshot
where it gives higher resolution than conventional beamforming. For multiple
snapshots, CS outperforms conventional high-resolution methods, even with
coherent arrivals and at low signal-to-noise ratio. The superior resolution of
CS is demonstrated with vertical array data from the SWellEx96 experiment for
coherent multi-paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02346</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02346</id><created>2015-03-08</created><updated>2015-11-11</updated><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>One Scan 1-Bit Compressed Sensing</title><categories>stat.ME cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on $\alpha$-stable random projections with small $\alpha$, we develop a
simple algorithm for compressed sensing (sparse signal recovery) by utilizing
only the signs (i.e., 1-bit) of the measurements. Using only 1-bit information
of the measurements results in substantial cost reduction in collection,
storage, communication, and decoding for compressed sensing. The proposed
algorithm is efficient in that the decoding procedure requires only one scan of
the coordinates. Our analysis can precisely show that, for a $K$-sparse signal
of length $N$, $12.3K\log N/\delta$ measurements (where $\delta$ is the
confidence) would be sufficient for recovering the support and the signs of the
signal. While the method is very robust against typical measurement noises, we
also provide the analysis of the scheme under random flipping of the signs of
the measurements.
  \noindent Compared to the well-known work on 1-bit marginal regression (which
can also be viewed as a one-scan method), the proposed algorithm requires
orders of magnitude fewer measurements. Compared to 1-bit Iterative Hard
Thresholding (IHT) (which is not a one-scan algorithm), our method is still
significantly more accurate. Furthermore, the proposed method is reasonably
robust against random sign flipping while IHT is known to be very sensitive to
this type of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02348</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02348</id><created>2015-03-08</created><authors><author><keyname>Hajipour</keyname><forenames>Javad</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Buffer Aided Relaying Improves Both Throughput and End-to-End Delay</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Buffer aided relaying has recently attracted a lot of attention due to the
improvement in the system throughput. However, a side effect usually deemed is
that buffering at relay nodes results in the increase in packet delays. In this
paper, we study the effect of buffering relays on the end-to-end delay of
users' data, from the time they arrive at source until delivery to the
destination. We use simple discussions to provide an insight on the overall
waiting time of the packets in the system. By studying the Bernoulli
distributed channel conditions, and using intuitive generalizations, we
conclude that the use of buffers at relays improves not only throughput, but
ironically the end-to-end delay as well. Computer simulations in the settings
of practical systems confirm the above results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02349</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02349</id><created>2015-03-08</created><updated>2015-05-03</updated><authors><author><keyname>Carneiro</keyname><forenames>Mario</forenames></author></authors><title>Arithmetic in Metamath, Case Study: Bertrand's Postulate</title><categories>math.LO cs.LO</categories><comments>16 pages, 0 figures; submitted to CICM 2015</comments><msc-class>65Y04 (Primary) 03B35, 03F03, 11A51 (Secondary)</msc-class><acm-class>F.4.1; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike some other formal systems, the proof system Metamath has no built-in
concept of &quot;decimal number&quot; in the sense that arbitrary digit strings are not
recognized by the system without prior definition. We present a system of
theorems and definitions and an algorithm to apply these as basic operations to
perform arithmetic calculations with a number of steps proportional to an
arbitrary-precision arithmetic calculation. We consider as case study the
formal proof of Bertrand's postulate, which required the calculation of many
small primes. Using a Mathematica implementation, we were able to complete the
first formal proof in Metamath using numbers larger than 10. Applications to
the mechanization of Metamath proofs are discussed, and a heuristic argument
for the feasability of large proofs such as Tom Hales' proof of the Kepler
conjecture is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02351</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02351</id><created>2015-03-08</created><authors><author><keyname>Schwing</keyname><forenames>Alexander G.</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Fully Connected Deep Structured Networks</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks with many layers have recently been shown to
achieve excellent results on many high-level tasks such as image
classification, object detection and more recently also semantic segmentation.
Particularly for semantic segmentation, a two-stage procedure is often
employed. Hereby, convolutional networks are trained to provide good local
pixel-wise features for the second step being traditionally a more global
graphical model. In this work we unify this two-stage process into a single
joint training algorithm. We demonstrate our method on the semantic image
segmentation task and show encouraging results on the challenging PASCAL VOC
2012 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02353</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02353</id><created>2015-03-08</created><updated>2015-05-19</updated><authors><author><keyname>Pandurangan</keyname><forenames>Gopal</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Scquizzato</keyname><forenames>Michele</forenames></author></authors><title>Almost Optimal Distributed Algorithms for Large-Scale Graph Problems</title><categories>cs.DC cs.DS</categories><acm-class>C.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the increasing need to understand the algorithmic foundations of
distributed large-scale graph computations, we study a number of fundamental
graph problems in a message-passing model for distributed computing where $k
\geq 2$ machines jointly perform computations on graphs with $n$ nodes
(typically, $n \gg k$). The graph is assumed to be randomly partitioned among
the $k$ machines, a common implementation in many real-world systems. The
communication is point-to-point, and the goal is to minimize the time
complexity, i.e., the number of communication rounds of the computation.
  Our main result is an (almost) optimal distributed randomized algorithm for
graph connectivity. Our algorithm runs in $\tilde{O}(n/k^2)$ rounds
($\tilde{O}$ notation hides a $\text{polylog}(n)$ factor and an additive
$\text{polylog}(n)$ term). This improves over the best previously known bound
of $\tilde{O}(n/k)$ [Klauck et al., SODA 2015], and is optimal (up to a
polylogarithmic factor) in view of the existing lower bound of
$\tilde{\Omega}(n/k^2)$. Our improved algorithm uses a bunch of techniques,
including linear graph sketching, which for the first time is applied in a
non-trivial way in distributed computing. We then present fast randomized
algorithms for computing minimum spanning trees, (approximate) min-cuts, and
for many graph verification problems. All these algorithms take
$\tilde{O}(n/k^2)$ rounds, and are optimal up to polylogarithmic factors. We
also show an almost matching lower bound of $\tilde{\Omega}(n/k^2)$ for many
graph verification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02354</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02354</id><created>2015-03-08</created><authors><author><keyname>Yang</keyname><forenames>Xinghua</forenames></author><author><keyname>Qiao</keyname><forenames>Fei</forenames></author><author><keyname>Wei</keyname><forenames>Qi</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>A General Scheme for Noise-Tolerant Logic Design Based on Probabilistic
  and DCVS Approaches</title><categories>cs.AR</categories><comments>4 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a general circuit scheme for noise-tolerant logic design based
on Markov Random Field theory and differential Cascade Voltage Switch technique
has been proposed, which is an extension of the work in [1-3], [4]. A block
with only four transistors has been successfully inserted to the original
circuit scheme from [3] and extensive simulation results show that our proposed
design can operate correctly with the input signal of 1 dB signal-noise-ratio.
When using the evaluation parameter from [5], the output value of our design
decreases by 76.5% on average than [3] which means that superior noise-immunity
could be obtained through our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02357</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02357</id><created>2015-03-08</created><updated>2015-06-23</updated><authors><author><keyname>Tu</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Hu</keyname><forenames>Baotian</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author></authors><title>Context-Dependent Translation Selection Using Convolutional Neural
  Network</title><categories>cs.CL cs.LG cs.NE</categories><comments>Short version is accepted by ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for translation selection in statistical machine
translation, in which a convolutional neural network is employed to judge the
similarity between a phrase pair in two languages. The specifically designed
convolutional architecture encodes not only the semantic similarity of the
translation pair, but also the context containing the phrase in the source
language. Therefore, our approach is able to capture context-dependent semantic
similarities of translation pairs. We adopt a curriculum learning strategy to
train the model: we classify the training examples into easy, medium, and
difficult categories, and gradually build the ability of representing phrase
and sentence level context by using training examples from easy to difficult.
Experimental results show that our approach significantly outperforms the
baseline system by up to 1.4 BLEU points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02364</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02364</id><created>2015-03-08</created><updated>2015-04-26</updated><authors><author><keyname>Shang</keyname><forenames>Lifeng</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author></authors><title>Neural Responding Machine for Short-Text Conversation</title><categories>cs.CL cs.AI cs.NE</categories><comments>accepted as a full paper at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Neural Responding Machine (NRM), a neural network-based response
generator for Short-Text Conversation. NRM takes the general encoder-decoder
framework: it formalizes the generation of response as a decoding process based
on the latent representation of the input text, while both encoding and
decoding are realized with recurrent neural networks (RNN). The NRM is trained
with a large amount of one-round conversation data collected from a
microblogging service. Empirical study shows that NRM can generate
grammatically correct and content-wise appropriate responses to over 75% of the
input text, outperforming state-of-the-arts in the same setting, including
retrieval-based and SMT-based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02367</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02367</id><created>2015-03-08</created><updated>2015-03-10</updated><authors><author><keyname>Shao</keyname><forenames>Sihua</forenames></author><author><keyname>Khreishah</keyname><forenames>Abdallah</forenames></author><author><keyname>Ayyash</keyname><forenames>Moussa</forenames></author><author><keyname>Rahaim</keyname><forenames>Michael B.</forenames></author><author><keyname>Elgala</keyname><forenames>Hany</forenames></author><author><keyname>Jungnickel</keyname><forenames>Volker</forenames></author><author><keyname>Schulz</keyname><forenames>Dominic</forenames></author><author><keyname>Little</keyname><forenames>Thomas D. C.</forenames></author></authors><title>Design of a visible-light-communication enhanced WiFi system</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communication (VLC) has wide unlicensed bandwidth, enables
communication in radio frequency (RF) sensitive environments, realizes
energy-efficient data transmission, and has the potential to boost the capacity
of wireless access networks through spatial reuse. On the other hand, WiFi
provides more coverage than VLC and does not suffer from the likelihood of
blockage due to the light of sight (LOS) requirement of VLC. In order to take
the advantages of both WiFi and VLC, we propose and implement two heterogeneous
systems with Internet access. One is the hybrid WiFi-VLC system, utilizing
unidirectional VLC channel as downlink and reserving the WiFi back-channel as
uplink. The asymmetric solution resolves the optical uplink challenges and
benefits from the full-duplex communication based on VLC. To further enhance
the robustness and increase throughput, the other system is presented, in which
we aggregate WiFi and VLC in parallel by leveraging the bonding technique in
Linux operating system. Online experiment results reveal that the hybrid system
outperforms the conventional WiFi for the crowded environments in terms of
throughput and web page loading time; and also demonstrate the further improved
performance of the aggregated system when considering the blocking duration and
the distance between access point and user device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02368</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02368</id><created>2015-03-09</created><updated>2016-03-07</updated><authors><author><keyname>Aberger</keyname><forenames>Christopher R.</forenames></author><author><keyname>Tu</keyname><forenames>Susan</forenames></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>EmptyHeaded: A Relational Engine for Graph Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two types of high-performance graph processing engines: low- and
high-level engines. Low-level engines (Galois, PowerGraph, Snap) provide
optimized data structures and computation models but require users to write
low-level imperative code; hence ensuring that efficiency is the burden of the
user. In high-level engines users write in query languages like datalog
(SociaLite) or SQL (Grail). High-level engines are easier to use but are orders
of magnitude slower than the low-level engines. We present EmptyHeaded, a
high-level engine that supports a rich datalog- like query language capable of
expressing standard graph benchmarks and achieves performance comparable to
that of low-level engines. EmptyHeaded uses a new class of join algorithms that
satisfy strong theoretical guarantees but have thus far not achieved
performance comparable to specialized graph processing engines. To achieve high
performance, EmptyHeaded introduces a new join engine architecture, including a
novel query optimizer and data layouts that leverage single-instruction
multiple data (SIMD) parallelism. With this architecture, EmptyHeaded
outperforms high-level approaches by up to three orders of magnitude on graph
pattern queries, PageRank, and Single-Source Shortest Paths (SSSP), and is an
order of magnitude faster than many standard low-level baselines. We validate
that EmptyHeaded competes with the best-of-breed low-level engine (Galois),
achieving comparable performance on PageRank and at most 3x worse performance
on SSSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02372</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02372</id><created>2015-03-09</created><authors><author><keyname>Babar</keyname><forenames>Zunaira</forenames></author><author><keyname>Botsinis</keyname><forenames>Panagiotis</forenames></author><author><keyname>Alanis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ng</keyname><forenames>Soon Xin</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>The Road From Classical to Quantum Codes: A Hashing Bound Approaching
  Design Procedure</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>IEEE Access, vol.PP, no.99, pp.1,1 (2015)</journal-ref><doi>10.1109/ACCESS.2015.2405533</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powerful Quantum Error Correction Codes (QECCs) are required for stabilizing
and protecting fragile qubits against the undesirable effects of quantum
decoherence. Similar to classical codes, hashing bound approaching QECCs may be
designed by exploiting a concatenated code structure, which invokes iterative
decoding. Therefore, in this paper we provide an extensive step-by-step
tutorial for designing EXtrinsic Information Transfer (EXIT) chart aided
concatenated quantum codes based on the underlying quantum-to-classical
isomorphism. These design lessons are then exemplified in the context of our
proposed Quantum Irregular Convolutional Code (QIRCC), which constitutes the
outer component of a concatenated quantum code. The proposed QIRCC can be
dynamically adapted to match any given inner code using EXIT charts, hence
achieving a performance close to the hashing bound. It is demonstrated that our
QIRCC-based optimized design is capable of operating within 0.4 dB of the noise
limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02373</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02373</id><created>2015-03-09</created><updated>2015-09-16</updated><authors><author><keyname>Yan</keyname><forenames>Bowen</forenames></author><author><keyname>Luo</keyname><forenames>Jianxi</forenames></author></authors><title>Measuring Technological Distance for Patent Mapping</title><categories>cs.SI cs.CY cs.DL physics.soc-ph</categories><comments>27 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works in the information science literature have presented cases of
using patent databases and patent classification information to construct
network maps of technology fields, which aim to aid in competitive intelligence
analysis and innovation decision making. Constructing such a patent network
requires a proper measure of the distance between different classes of patents
in the patent classification systems. Despite the existence of various distance
measures in the literature, it is unclear how to consistently assess and
compare them, and which ones to select for constructing patent technology
network maps. This ambiguity has limited the development and applications of
such technology maps. Herein, we propose to compare alternative distance
measures and identify the superior ones by analyzing the differences and
similarities in the structural properties of resulting patent network maps.
Using United States patent data from 1976 to 2006 and International Patent
Classification system, we compare 12 representative distance measures, which
quantify inter-field knowledge base proximity, field-crossing diversification
likelihood or frequency of innovation agents, and co-occurrences of patent
classes in the same patents. Our comparative analyses suggest the patent
technology network maps based on normalized co-reference and inventor
diversification likelihood measures are the best representatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02377</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02377</id><created>2015-03-09</created><authors><author><keyname>Gao</keyname><forenames>Xianyi</forenames></author><author><keyname>Clark</keyname><forenames>Gradeigh D.</forenames></author><author><keyname>Lindqvist</keyname><forenames>Janne</forenames></author></authors><title>Of Two Minds, Multiple Addresses, and One History: Characterizing
  Opinions, Knowledge, and Perceptions of Bitcoin Across Groups</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital currencies represent a new method for exchange and investment that
differs strongly from any other fiat money seen throughout history. A digital
currency makes it possible to perform all financial transactions without the
intervention of a third party to act as an arbiter of verification; payments
can be made between two people with degrees of anonymity, across continents, at
any denomination, and without any transaction fees going to a central
authority. The most successful example of this is Bitcoin, introduced in 2008,
which has experienced a recent boom of popularity, media attention, and
investment. With this surge of attention, we became interested in finding out
how people both inside and outside the Bitcoin community perceive Bitcoin --
what do they think of it, how do they feel, and how knowledgeable they are.
Towards this end, we conducted the first interview study (N = 20) with
participants to discuss Bitcoin and other related financial topics. Some of our
major findings include: not understanding how Bitcoin works is not a barrier
for entry, although non-user participants claim it would be for them and that
user participants are in a state of cognitive dissonance concerning the role of
governments in the system. Our findings, overall, contribute to knowledge
concerning Bitcoin and attitudes towards digital currencies in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02379</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02379</id><created>2015-03-09</created><updated>2015-04-03</updated><authors><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Digital Cancelation of Self-Interference for Single-Frequency
  Full-Duplex Relay Stations via Sampled-Data Control</title><categories>cs.SY cs.IT math.IT</categories><comments>SICE Journal of Control, Measurement, and System Integration (to
  appear); 7 pages, 14 figures. arXiv admin note: substantial text overlap with
  arXiv:1412.3238, arXiv:1407.7083</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose sampled-data design of digital filters that
cancel the continuous-time effect of coupling waves in a single-frequency
full-duplex relay station. In this study, we model a relay station as a
continuoustime system while conventional researches treat it as a discrete-time
system. For a continuous-time model, we propose digital feedback canceler based
on the sampled-data H-infinity control theory to cancel coupling waves taking
intersample behavior into account. We also propose robust control against
unknown multipath interference. Simulation results are shown to illustrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02386</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02386</id><created>2015-03-09</created><authors><author><keyname>Hansen</keyname><forenames>Johan P.</forenames></author></authors><title>Riemann-Roch Spaces and Linear Network Codes</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><msc-class>68M10, 90B18, 94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct linear network codes utilizing algebraic curves over finite
fields and certain associated Riemann-Roch spaces and present methods to obtain
their parameters.
  In particular we treat the Hermitian curve and the curves associated with the
Suzuki and Ree groups all having the maximal number of points for curves of
their respective genera.
  Linear network coding transmits information in terms of a basis of a vector
space and the information is received as a basis of a possibly altered vector
space. Ralf Koetter and Frank R. Kschischang
%\cite{DBLP:journals/tit/KoetterK08} introduced a metric on the set of vector
spaces and showed that a minimal distance decoder for this metric achieves
correct decoding if the dimension of the intersection of the transmitted and
received vector space is sufficiently large.
  The vector spaces in our construction have minimal distance bounded from
below in the above metric making them suitable for linear network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02388</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02388</id><created>2015-03-09</created><authors><author><keyname>Crick</keyname><forenames>Tom</forenames></author><author><keyname>Ishtiaq</keyname><forenames>Samin</forenames></author><author><keyname>Hall</keyname><forenames>Benjamin A.</forenames></author></authors><title>Towards &quot;Reproducibility-as-a-Service&quot;</title><categories>cs.SE cs.CE cs.CY</categories><comments>Invited submission to Journal of Open Research Software; 10 pages,
  LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reproduction and replication of novel results has become a major issue
for a number of scientific disciplines. In computer science and related
computational disciplines such as systems biology, the issues closely revolve
around the ability to implement novel algorithms and models. Taking an approach
from the literature and applying it to a new codebase frequently requires local
knowledge missing from the published manuscripts and project websites.
Alongside this issue, benchmarking, and the development of fair -- and publicly
available -- benchmark sets present another barrier.
  In this paper, we outline several suggestions to address these issues, driven
by specific examples from a range of scientific domains. Finally, based on
these suggestions, we propose a new open automated platform for scientific
software development which effectively abstracts specific dependencies from the
individual researcher and their workstation, allowing easy sharing and
reproduction of results. This new cyberinfrastructure for computational science
offers the potential to incentivise a culture change and drive the adoption of
new techniques to improve the efficiency of scientific exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02389</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02389</id><created>2015-03-09</created><authors><author><keyname>Huleihel</keyname><forenames>Wasim</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Exact Random Coding Error Exponents for the Two-User Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about exact error exponents for the two-user interference
channel under the random coding regime. Specifically, we first analyze the
standard random coding ensemble, where the codebooks are comprised of
independently and identically distributed (i.i.d.) codewords. For this
ensemble, we focus on optimum decoding, which is in contrast to other,
heuristic decoding rules that have been used in the literature (e.g., joint
typicality decoding, treating interference as noise, etc.). The fact that the
interfering signal is a codeword, and not an i.i.d. noise process, complicates
the application of conventional techniques of performance analysis of the
optimum decoder. Also, unfortunately, these conventional techniques result in
loose bounds. Using analytical tools rooted in statistical physics, as well as
advanced union bounds, we derive exact single-letter formulas for the random
coding error exponents. We compare our results with the best known lower bound
on the error exponent, and show that our exponents can be strictly better. It
turns out that the methods employed in this paper, can also be used to analyze
more complicated coding ensembles. Accordingly, as an example, using the same
techniques, we find exact formulas for the error exponent associated with the
Han-Kobayashi (HK) random coding ensemble, which is based on superposition
coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02391</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02391</id><created>2015-03-09</created><authors><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Liu</keyname><forenames>Si</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Liu</keyname><forenames>Luoqi</forenames></author><author><keyname>Dong</keyname><forenames>Jian</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Deep Human Parsing with Active Template Regression</title><categories>cs.CV</categories><comments>This manuscript is the accepted version for IEEE Transactions on
  Pattern Analysis and Machine Intelligence (TPAMI) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the human parsing task, namely decomposing a human image into
semantic fashion/body regions, is formulated as an Active Template Regression
(ATR) problem, where the normalized mask of each fashion/body item is expressed
as the linear combination of the learned mask templates, and then morphed to a
more precise mask with the active shape parameters, including position, scale
and visibility of each semantic region. The mask template coefficients and the
active shape parameters together can generate the human parsing results, and
are thus called the structure outputs for human parsing. The deep Convolutional
Neural Network (CNN) is utilized to build the end-to-end relation between the
input human image and the structure outputs for human parsing. More
specifically, the structure outputs are predicted by two separate networks. The
first CNN network is with max-pooling, and designed to predict the template
coefficients for each label mask, while the second CNN network is without
max-pooling to preserve sensitivity to label mask position and accurately
predict the active shape parameters. For a new image, the structure outputs of
the two networks are fused to generate the probability of each label for each
pixel, and super-pixel smoothing is finally used to refine the human parsing
result. Comprehensive evaluations on a large dataset well demonstrate the
significant superiority of the ATR framework over other state-of-the-arts for
human parsing. In particular, the F1-score reaches $64.38\%$ by our ATR
framework, significantly higher than $44.76\%$ based on the state-of-the-art
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02398</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02398</id><created>2015-03-09</created><updated>2015-09-11</updated><authors><author><keyname>Seibert</keyname><forenames>Matthias</forenames></author><author><keyname>W&#xf6;rmann</keyname><forenames>Julian</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>Learning Co-Sparse Analysis Operators with Separable Structures</title><categories>cs.LG stat.ML</categories><comments>11 pages double column, 4 figures, 3 tables</comments><doi>10.1109/TSP.2015.2481875</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the co-sparse analysis model a set of filters is applied to a signal out
of the signal class of interest yielding sparse filter responses. As such, it
may serve as a prior in inverse problems, or for structural analysis of signals
that are known to belong to the signal class. The more the model is adapted to
the class, the more reliable it is for these purposes. The task of learning
such operators for a given class is therefore a crucial problem. In many
applications, it is also required that the filter responses are obtained in a
timely manner, which can be achieved by filters with a separable structure. Not
only can operators of this sort be efficiently used for computing the filter
responses, but they also have the advantage that less training samples are
required to obtain a reliable estimate of the operator. The first contribution
of this work is to give theoretical evidence for this claim by providing an
upper bound for the sample complexity of the learning process. The second is a
stochastic gradient descent (SGD) method designed to learn an analysis operator
with separable structures, which includes a novel and efficient step size
selection rule. Numerical experiments are provided that link the sample
complexity to the convergence speed of the SGD algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02401</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02401</id><created>2015-03-09</created><authors><author><keyname>Magdy</keyname><forenames>Walid</forenames></author><author><keyname>Darwish</keyname><forenames>Kareem</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>#FailedRevolutions: Using Twitter to Study the Antecedents of ISIS
  Support</title><categories>cs.SI physics.soc-ph</categories><comments>Submitted to ICWSM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within a fairly short amount of time, the Islamic State of Iraq and Syria
(ISIS) has managed to put large swaths of land in Syria and Iraq under their
control. To many observers, the sheer speed at which this &quot;state&quot; was
established was dumbfounding. To better understand the roots of this
organization and its supporters we present a study using data from Twitter. We
start by collecting large amounts of Arabic tweets referring to ISIS and
classify them into pro-ISIS and anti-ISIS. This classification turns out to be
easily done simply using the name variants used to refer to the organization:
the full name and the description as &quot;state&quot; is associated with support,
whereas abbreviations usually indicate opposition. We then &quot;go back in time&quot; by
analyzing the historic timelines of both users supporting and opposing and look
at their pre-ISIS period to gain insights into the antecedents of support. To
achieve this, we build a classifier using pre-ISIS data to &quot;predict&quot;, in
retrospect, who will support or oppose the group. The key story that emerges is
one of frustration with failed Arab Spring revolutions. ISIS supporters largely
differ from ISIS opposition in that they refer a lot more to Arab Spring
uprisings that failed. We also find temporal patterns in the support and
opposition which seems to be linked to major news, such as reported territorial
gains, reports on gruesome acts of violence, and reports on airstrikes and
foreign intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02406</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02406</id><created>2015-03-09</created><authors><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Noga</forenames></author></authors><title>Deep Learning and the Information Bottleneck Principle</title><categories>cs.LG</categories><comments>5 pages, 2 figures, Invited paper to ITW 2015; 2015 IEEE Information
  Theory Workshop (ITW) (IEEE ITW 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the
information bottleneck (IB) principle. We first show that any DNN can be
quantified by the mutual information between the layers and the input and
output variables. Using this representation we can calculate the optimal
information theoretic limits of the DNN and obtain finite sample generalization
bounds. The advantage of getting closer to the theoretical limit is
quantifiable both by the generalization bound and by the network's simplicity.
We argue that both the optimal architecture, number of layers and
features/connections at each layer, are related to the bifurcation points of
the information bottleneck tradeoff, namely, relevant compression of the input
layer with respect to the output layer. The hierarchical representations at the
layered network naturally correspond to the structural phase transitions along
the information curve. We believe that this new insight can lead to new
optimality bounds and deep learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02408</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02408</id><created>2015-03-09</created><updated>2015-08-18</updated><authors><author><keyname>Penjor</keyname><forenames>Sonam</forenames></author><author><keyname>Zander</keyname><forenames>Par-Ola</forenames></author></authors><title>Predicting Virtual Learning Environment adoption - A case study</title><categories>cs.CY</categories><comments>In review - comments welcome also from other colleagues!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose - To qualify the significance of Rogers' Diffusion of Innovations
theory with regard to Virtual Learning Environments. To apply an existing
Diffusion of Innovations instrument on a case organisation, the Royal
University of Bhutan (RUB), in order to compare its results with previous
findings. Descriptive statistics and logistic regression analysis were deployed
to analyze adopter group memberships and predictor significance in Virtual
Learning Environment adoption and use. Findings - The Diffusion of Innovations
theory is not stable across organizations when it comes to predicting different
user categories or the distribution of users. However, it was possible to
achieve reliable results for virtual learning environments within a particular
organization. Research limitations ND implications - The study questions
scholarly attempts to establish models of this type across organizations.
Practical implications - Professionals should be aware that
cross-organizational generalizations from Diffusion Of Innovation findings
within the domain of virtual learning environments may be very unreliable.
Originality and value - The study challenges the massively cited Diffusion of
Innovation literature. It provides data from Bhutan, which is underrepresented
in empirical investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02413</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02413</id><created>2015-03-09</created><authors><author><keyname>Shabtai</keyname><forenames>Galia</forenames></author><author><keyname>Raz</keyname><forenames>Danny</forenames></author><author><keyname>Shavitt</keyname><forenames>Yuval</forenames></author></authors><title>Stochastic Service Placement</title><categories>cs.DS cs.NI cs.PF</categories><comments>27 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource allocation for cloud services is a complex task due to the diversity
of the services and the dynamic workloads. One way to address this is by
overprovisioning which results in high cost due to the unutilized resources. A
much more economical approach, relying on the stochastic nature of the demand,
is to allocate just the right amount of resources and use additional more
expensive mechanisms in case of overflow situations where demand exceeds the
capacity. In this paper we study this approach and show both by comprehensive
analysis for independent normal distributed demands and simulation on synthetic
data that it is significantly better than currently deployed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02416</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02416</id><created>2015-03-09</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author></authors><title>Approximating LZ77 in Small Space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a positive \(\epsilon \leq 1\) and read-only access to a string \(S
[1..n]\) whose LZ77 parse consists of $z$ phrases, with high probability we can
build an LZ77-like parse of $S$ that consists of $\Oh{z / \epsilon}$ phrases
using $\Oh{n^{1 + \epsilon}}$ time, $\Oh{n^{1 + \epsilon} / B}$ I/Os (where $B$
is the size of a disk block) and $\Oh{z / \epsilon}$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02417</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02417</id><created>2015-03-09</created><authors><author><keyname>Shareghi</keyname><forenames>Ehsan</forenames></author><author><keyname>Haffari</keyname><forenames>Gholamreza</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Nicholson</keyname><forenames>Ann</forenames></author></authors><title>Structured Prediction of Sequences and Trees using Infinite Contexts</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linguistic structures exhibit a rich array of global phenomena, however
commonly used Markov models are unable to adequately describe these phenomena
due to their strong locality assumptions. We propose a novel hierarchical model
for structured prediction over sequences and trees which exploits global
context by conditioning each generation decision on an unbounded context of
prior decisions. This builds on the success of Markov models but without
imposing a fixed bound in order to better represent global phenomena. To
facilitate learning of this large and unbounded model, we use a hierarchical
Pitman-Yor process prior which provides a recursive form of smoothing. We
propose prediction algorithms based on A* and Markov Chain Monte Carlo
sampling. Empirical results demonstrate the potential of our model compared to
baseline finite-context Markov models on part-of-speech tagging and syntactic
parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02422</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02422</id><created>2015-03-09</created><updated>2015-04-17</updated><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Timed pushdown automata revisited</title><categories>cs.FL cs.LO</categories><comments>full technical report of LICS'15 paper</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper contains two results on timed extensions of pushdown automata
(PDA). As our first result we prove that the model of dense-timed PDA of
Abdulla et al. collapses: it is expressively equivalent to dense-timed PDA with
timeless stack. Motivated by this result, we advocate the framework of
first-order definable PDA, a specialization of PDA in sets with atoms, as the
right setting to define and investigate timed extensions of PDA. The general
model obtained in this way is Turing complete. As our second result we prove
NEXPTIME upper complexity bound for the non-emptiness problem for an expressive
subclass. As a byproduct, we obtain a tight EXPTIME complexity bound for a more
restrictive subclass of PDA with timeless stack, thus subsuming the complexity
bound known for dense-timed PDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02427</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02427</id><created>2015-03-09</created><updated>2015-06-12</updated><authors><author><keyname>Wang</keyname><forenames>Mingxuan</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Liu</keyname><forenames>Qun</forenames></author></authors><title>Syntax-based Deep Matching of Short Texts</title><categories>cs.CL cs.LG cs.NE</categories><comments>Accepted by IJCAI-2015 as full paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tasks in natural language processing, ranging from machine translation
to question answering, can be reduced to the problem of matching two sentences
or more generally two short texts. We propose a new approach to the problem,
called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The
approach consists of two components, 1) a mining algorithm to discover patterns
for matching two short-texts, defined in the product space of dependency trees,
and 2) a deep neural network for matching short texts using the mined patterns,
as well as a learning algorithm to build the network having a sparse structure.
We test our algorithm on the problem of matching a tweet and a response in
social media, a hard matching problem proposed in [Wang et al., 2013], and show
that DeepMatch$_{tree}$ can outperform a number of competitor models including
one without using dependency trees and one based on word-embedding, all with
large margins
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02434</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02434</id><created>2015-03-09</created><authors><author><keyname>Mansutti</keyname><forenames>Alessio</forenames></author><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>Distributed execution of bigraphical reactive systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bigraph embedding problem is crucial for many results and tools about
bigraphs and bigraphical reactive systems (BRS). Current algorithms for
computing bigraphical embeddings are centralized, i.e. designed to run locally
with a complete view of the guest and host bigraphs. In order to deal with
large bigraphs, and to parallelize reactions, we present a decentralized
algorithm, which distributes both state and computation over several concurrent
processes. This allows for distributed, parallel simulations where
non-interfering reactions can be carried out concurrently; nevertheless, even
in the worst case the complexity of this distributed algorithm is no worse than
that of a centralized algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02442</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02442</id><created>2015-03-09</created><authors><author><keyname>Mehraghdam</keyname><forenames>Sevil</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>Specification of Complex Structures in Distributed Service Function
  Chaining Using a YANG Data Model</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While services benefit from distributed cloud centers running in isolation,
allowing multiple centers to cooperate on implementing services unlocks the
full power of distributed cloud computing. Distributed cloud services are
typically set up by chaining together a number of functions that are specified
with an implicit order. They can incorporate complex structures, e.g., include
functions that classify and forward flows over distinct branches and functions
that are traversed by certain types of flows but skipped by others. These
requirements need specification techniques more powerful than existing
graph-based ones. We present a context-free grammar for abstract description of
service function chaining structures and a concrete syntax based on the YANG
data modeling language that can easily be translated into an explicit
configuration of service functions. Finally, we present examples of using our
models for complex services within common use cases of service function
chaining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02445</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02445</id><created>2015-03-09</created><updated>2015-04-01</updated><authors><author><keyname>Uzair</keyname><forenames>Muhammad</forenames></author><author><keyname>Shafait</keyname><forenames>Faisal</forenames></author><author><keyname>Ghanem</keyname><forenames>Bernard</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Representation Learning with Deep Extreme Learning Machines for
  Efficient Image Set Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient and accurate joint representation of a collection of images, that
belong to the same class, is a major research challenge for practical image set
classification. Existing methods either make prior assumptions about the data
structure, or perform heavy computations to learn structure from the data
itself. In this paper, we propose an efficient image set representation that
does not make any prior assumptions about the structure of the underlying data.
We learn the non-linear structure of image sets with Deep Extreme Learning
Machines (DELM) that are very efficient and generalize well even on a limited
number of training samples. Extensive experiments on a broad range of public
datasets for image set classification (Honda/UCSD, CMU Mobo, YouTube
Celebrities, Celebrity-1000, ETH-80) show that the proposed algorithm
consistently outperforms state-of-the-art image set classification methods both
in terms of speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02447</identifier>
 <datestamp>2015-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02447</id><created>2015-03-09</created><updated>2015-08-05</updated><authors><author><keyname>Bonsangue</keyname><forenames>Marcello M.</forenames><affiliation>Leiden University</affiliation></author><author><keyname>Hansen</keyname><forenames>Helle Hvid</forenames><affiliation>Radboud University Nijmegen</affiliation></author><author><keyname>Kurz</keyname><forenames>Alexander</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Rot</keyname><forenames>Jurriaan</forenames><affiliation>Leiden University</affiliation></author></authors><title>Presenting Distributive Laws</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:2) 2015</journal-ref><doi>10.2168/LMCS-11(3:2)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributive laws of a monad T over a functor F are categorical tools for
specifying algebra-coalgebra interaction. They proved to be important for
solving systems of corecursive equations, for the specification of well-behaved
structural operational semantics and, more recently, also for enhancements of
the bisimulation proof method. If T is a free monad, then such distributive
laws correspond to simple natural transformations. However, when T is not free
it can be rather difficult to prove the defining axioms of a distributive law.
In this paper we describe how to obtain a distributive law for a monad with an
equational presentation from a distributive law for the underlying free monad.
We apply this result to show the equivalence between two different
representations of context-free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02464</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02464</id><created>2015-03-09</created><updated>2015-03-26</updated><authors><author><keyname>Sgattoni</keyname><forenames>Andrea</forenames></author><author><keyname>Fedeli</keyname><forenames>Luca</forenames></author><author><keyname>Sinigardi</keyname><forenames>Stefano</forenames></author><author><keyname>Marocchino</keyname><forenames>Alberto</forenames></author><author><keyname>Macchi</keyname><forenames>Andrea</forenames></author><author><keyname>Weinberg</keyname><forenames>Volker</forenames></author><author><keyname>Karmakar</keyname><forenames>Anupam</forenames></author></authors><title>Optimising PICCANTE - an Open Source Particle-in-Cell Code for Advanced
  Simulations on Tier-0 Systems</title><categories>cs.DC physics.comp-ph physics.plasm-ph</categories><comments>8 pages, 6 figures, PRACE Whitepaper</comments><report-no>WP209</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a detailed strong and weak scaling analysis of PICCANTE, an open
source, massively parallel, fully-relativistic Particle-In-Cell (PIC) code. PIC
codes are widely used in plasma physics and astrophysics to study the cases
where kinetic effects are relevant. PICCANTE is primarily developed to study
laser-plasma interaction. Within a PRACE Preparatory Access Project, various
revisions of different routines of the code have been analysed on the HPC
systems JUQUEEN at Juelich Supercomputing Centre (JSC), Germany, and FERMI at
CINECA, Italy, to improve scalability and I/O performance of the application.
The diagnostic tool Scalasca is used to identify suboptimal routines. Different
output strategies are discussed. The detailed strong and weak scaling behaviour
of the improved code are presented in comparison with the original version of
the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02466</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02466</id><created>2015-03-09</created><authors><author><keyname>Qadar</keyname><forenames>Muhammad Ali</forenames></author><author><keyname>Zhaowen</keyname><forenames>Yan</forenames></author></authors><title>Brain Tumor Segmentation: A Comparative Analysis</title><categories>cs.CV</categories><comments>8 Pages, 8 Figues, International Journal of Computer Science (IJCSI)</comments><journal-ref>Muhammad Ali Qadar, Yan Zhaowen, Brain Tumor Segmentation: A
  Comparative Analysis, IJCSI International Journal of Computer Science Issues,
  Volume 11, Issue 6, No 1, November 2014,1694-0784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Five different threshold segmentation based approaches have been reviewed and
compared over here to extract the tumor from set of brain images. This research
focuses on the analysis of image segmentation methods, a comparison of five
semi-automated methods have been undertaken for evaluating their relative
performance in the segmentation of tumor. Consequently, results are compared on
the basis of quantitative and qualitative analysis of respective methods. The
purpose of this study was to analytically identify the methods, most suitable
for application for a particular genre of problems. The results show that of
the region growing segmentation performed better than rest in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02479</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02479</id><created>2015-03-09</created><authors><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author></authors><title>Cournot Games with Uncertainty: Coalitions, Competition, and Efficiency</title><categories>cs.GT cs.SY math.OC q-fin.EC</categories><comments>Submitted to EC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the impact of group formations on the efficiency of Cournot
games where producers face uncertainties. In particular, we study a market
model where producers must determine their output before an uncertainty
production capacity is realized. In contrast to standard Cournot models, we
show that the game is not efficient when there are many small producers.
Instead, producers tend to act conservatively to hedge against their risks. We
show that in the presence of uncertainty, the game becomes efficient when
producers are allowed to take advantage of diversity to form groups of certain
sizes. We characterize the trade-off between market power and uncertainty
reduction as a function of group size. Namely, we show that when there are N
producers present, competition between groups of size square root of N results
in equilibria that are socially optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02504</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02504</id><created>2015-03-09</created><updated>2015-10-02</updated><authors><author><keyname>Iliopoulos</keyname><forenames>Vasileios</forenames></author></authors><title>The Quicksort algorithm and related topics</title><categories>cs.DS math.CO</categories><comments>PhD thesis. Reference [23] was missing in first version. It now reads
  correctly in page 142, Section 5.6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sorting algorithms have attracted a great deal of attention and study, as
they have numerous applications to Mathematics, Computer Science and related
fields. In this thesis, we first deal with the mathematical analysis of the
Quicksort algorithm and its variants. Specifically, we study the time
complexity of the algorithm and we provide a complete demonstration of the
variance of the number of comparisons required, a known result but one whose
detailed proof is not easy to read out of the literature. We also examine
variants of Quicksort, where multiple pivots are chosen for the partitioning of
the array.
  The rest of this work is dedicated to the analysis of finding the true order
by further pairwise comparisons when a partial order compatible with the true
order is given in advance. We discuss a number of cases where the partially
ordered sets arise at random. To this end, we employ results from Graph and
Information Theory. Finally, we obtain an alternative bound on the number of
linear extensions when the partially ordered set arises from a random graph,
and discuss the possible application of Shellsort in merging chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02510</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02510</id><created>2015-03-09</created><updated>2015-04-17</updated><authors><author><keyname>Le</keyname><forenames>Phong</forenames></author><author><keyname>Zuidema</keyname><forenames>Willem</forenames></author></authors><title>Compositional Distributional Semantics with Long Short Term Memory</title><categories>cs.CL cs.AI cs.LG</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are proposing an extension of the recursive neural network that makes use
of a variant of the long short-term memory architecture. The extension allows
information low in parse trees to be stored in a memory register (the `memory
cell') and used much later higher up in the parse tree. This provides a
solution to the vanishing gradient problem and allows the network to capture
long range dependencies. Experimental results show that our composition
outperformed the traditional neural-network composition on the Stanford
Sentiment Treebank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02516</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02516</id><created>2015-03-09</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Deckelbaum</keyname><forenames>Alan</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>Optimal Pricing is Hard</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that computing the revenue-optimal deterministic auction in
unit-demand single-buyer Bayesian settings, i.e. the optimal item-pricing, is
computationally hard even in single-item settings where the buyer's value
distribution is a sum of independently distributed attributes, or multi-item
settings where the buyer's values for the items are independent. We also show
that it is intractable to optimally price the grand bundle of multiple items
for an additive bidder whose values for the items are independent. These
difficulties stem from implicit definitions of a value distribution. We provide
three instances of how different properties of implicit distributions can lead
to intractability: the first is a #P-hardness proof, while the remaining two
are reductions from the SQRT-SUM problem of Garey, Graham, and Johnson. While
simple pricing schemes can oftentimes approximate the best scheme in revenue,
they can have drastically different underlying structure. We argue therefore
that either the specification of the input distribution must be highly
restricted in format, or it is necessary for the goal to be mere approximation
to the optimal scheme's revenue instead of computing properties of the scheme
itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02517</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02517</id><created>2015-03-09</created><updated>2015-03-18</updated><authors><author><keyname>Okengwu</keyname><forenames>Ugochi A.</forenames></author><author><keyname>Nwachukwu</keyname><forenames>Enoch O.</forenames></author><author><keyname>Osegi</keyname><forenames>Emmanuel N.</forenames></author></authors><title>Modified Dijkstra Algorithm with Invention Hierarchies Applied to a
  Conic Graph</title><categories>cs.DS</categories><comments>Paper Proposals</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A modified version of the Dijkstra algorithm using an inventive contraction
hierarchy is proposed. The algorithm considers a directed acyclic graph with a
conical or semi-circular structure for which a pair of edges is chosen
iteratively from multi-sources. The algorithm obtains minimum paths by using a
comparison process. The comparison process follows a mathematical construction
routine that considers a forward and backward check such that only paths with
minimum lengths are selected. In addition, the algorithm automatically invents
a new path by computing the absolute edge difference for the minimum edge pair
and its succeeding neighbour in O (n) time. The invented path is approximated
to the hidden path using a fitness criterion. The proposed algorithm extends
the multi-source multi-destination problem to include those paths for which a
path mining redirection from multi-sources to multi-destinations is a minimum.
The algorithm has been applied to a hospital locator path finding system and
the results were quite satisfactory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02519</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02519</id><created>2015-03-09</created><authors><author><keyname>Tan</keyname><forenames>Xin</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Akyildiz</keyname><forenames>Ian F.</forenames></author></authors><title>A Testbed of Magnetic Induction-based Communication System for
  Underground Applications</title><categories>physics.ins-det cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless underground sensor networks (WUSNs) can enable many important
applications such as intelligent agriculture, pipeline fault diagnosis, mine
disaster rescue, concealed border patrol, crude oil exploration, among others.
The key challenge to realize WUSNs is the wireless communication in underground
environments. Most existing wireless communication systems utilize the dipole
antenna to transmit and receive propagating electromagnetic (EM) waves, which
do not work well in underground environments due to the very high material
absorption loss. The Magnetic Induction (MI) technique provides a promising
alternative solution that could address the current problem in underground.
Although the MI-based underground communication has been intensively
investigated theoretically, to date, seldom effort has been made in developing
a testbed for the MI-based underground communication that can validate the
theoretical results. In this paper, a testbed of MI-based communication system
is designed and implemented in an in-lab underground environment. The testbed
realizes and tests not only the original MI mechanism that utilizes single coil
but also recent developed techniques that use the MI waveguide and the
3-directional (3D) MI coils. The experiments are conducted in an in-lab
underground environment with reconfigurable environmental parameters such as
soil composition and water content. This paper provides the principles and
guidelines for developing the MI underground communications testbed, which is
very complicated and time-consuming due to the new communication mechanism and
the new wireless transmission medium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02521</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02521</id><created>2015-03-09</created><updated>2015-10-14</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>A Single-Pass Classifier for Categorical Data</title><categories>cs.AI</categories><comments>New test results and additional information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new method for classifying a dataset that partitions
elements into different categories. It has relations with neural networks but a
slightly different structure, requiring only a single pass through the
classifier to generate the weight sets. A grid structure is required and a
novel idea of converting a 1-D row of real values into a 2-D structure of value
bands. Each cell in the band can then store a cell weight value and also a set
of weights that represent its own importance to each of the output categories.
For any input that needs to be categorised, all of the output weight lists for
each relevant input cell can be retrieved and summed to produce a probability
for what the correct output category is. So the relative importance of each
input point to the output is distributed to each cell. The bands possibly work
like hidden layers of neurons, but separate ones for each input variable. The
construction process itself can simply be the reinforcement of the weight
values, without requiring an iterative adjustment process, making it
potentially much faster. For online or partial updating, it can possibly
include a competitive process as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02531</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02531</id><created>2015-03-09</created><authors><author><keyname>Hinton</keyname><forenames>Geoffrey</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Dean</keyname><forenames>Jeff</forenames></author></authors><title>Distilling the Knowledge in a Neural Network</title><categories>stat.ML cs.LG cs.NE</categories><comments>NIPS 2014 Deep Learning Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A very simple way to improve the performance of almost any machine learning
algorithm is to train many different models on the same data and then to
average their predictions. Unfortunately, making predictions using a whole
ensemble of models is cumbersome and may be too computationally expensive to
allow deployment to a large number of users, especially if the individual
models are large neural nets. Caruana and his collaborators have shown that it
is possible to compress the knowledge in an ensemble into a single model which
is much easier to deploy and we develop this approach further using a different
compression technique. We achieve some surprising results on MNIST and we show
that we can significantly improve the acoustic model of a heavily used
commercial system by distilling the knowledge in an ensemble of models into a
single model. We also introduce a new type of ensemble composed of one or more
full models and many specialist models which learn to distinguish fine-grained
classes that the full models confuse. Unlike a mixture of experts, these
specialist models can be trained rapidly and in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02536</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02536</id><created>2015-03-05</created><authors><author><keyname>Bouton</keyname><forenames>E. A.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>Santos-Magalhaes</keyname><forenames>N. S.</forenames></author></authors><title>Genomic Imaging Based on Codongrams and a^2grams</title><categories>q-bio.OT cs.CE</categories><comments>7 pages, 3 figures</comments><journal-ref>WSEAS Trans. on Biology and Biomedicine, vol.1, n.2, pp.255-260,
  April 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces new tools for genomic signal processing, which can
assist for genomic attribute extracting or describing biologically meaningful
features embedded in a DNA. The codongrams and a2grams are offered as an
alternative to spectrograms and scalograms. Twenty different a^2grams are
defined for a genome, one for each amino acid (valgram is an a^2gram for
valine; alagram is an a^2gram for alanine and so on). They provide information
about the distribution and occurrence of the investigated amino acid. In
particular, the metgram can be used to find out potential start position of
genes within a genome. This approach can help implementing a new diagnosis test
for genetic diseases by providing a type of DNA-medical imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02550</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02550</id><created>2015-03-09</created><authors><author><keyname>Malyshev</keyname><forenames>D. S.</forenames></author><author><keyname>Lobanova</keyname><forenames>O. O.</forenames></author></authors><title>The coloring problem for $\{P_5,\bar{P_5}\}$-free graphs and
  $\{P_5,K_p-e\}$-free graphs is polynomial</title><categories>cs.DM</categories><msc-class>05C15, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that determining the chromatic number of a $\{P_5,\bar{P_5}\}$-free
graph or a $\{P_5,K_p-e\}$-free graph can be done in polynomial time
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02551</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02551</id><created>2015-03-09</created><updated>2015-06-09</updated><authors><author><keyname>Jitkrittum</keyname><forenames>Wittawat</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author><author><keyname>Eslami</keyname><forenames>S. M. Ali</forenames></author><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Szab&#xf3;</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>Kernel-Based Just-In-Time Learning for Passing Expectation Propagation
  Messages</title><categories>stat.ML cs.LG</categories><comments>accepted to UAI 2015. Correct typos. Add more content to the
  appendix. Main results unchanged</comments><msc-class>62F15, 46e22, 62-09, 62F30</msc-class><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient nonparametric strategy for learning a message
operator in expectation propagation (EP), which takes as input the set of
incoming messages to a factor node, and produces an outgoing message as output.
This learned operator replaces the multivariate integral required in classical
EP, which may not have an analytic expression. We use kernel-based regression,
which is trained on a set of probability distributions representing the
incoming messages, and the associated outgoing messages. The kernel approach
has two main advantages: first, it is fast, as it is implemented using a novel
two-layer random feature representation of the input message distributions;
second, it has principled uncertainty estimates, and can be cheaply updated
online, meaning it can request and incorporate new training data when it
encounters inputs on which it is uncertain. In experiments, our approach is
able to solve learning problems where a single message operator is required for
multiple, substantially different data sets (logistic regression for a variety
of classification problems), where it is essential to accurately assess
uncertainty and to efficiently and robustly update the message operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02557</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02557</id><created>2015-03-09</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author></authors><title>On Monotonicity and Propagation of Order Properties</title><categories>math.OC cs.SY</categories><comments>Part of the paper is to appear in American Control Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a link between monotonicity of deterministic dynamical systems
and propagation of order by Markov processes is established. The order
propagation has received considerable attention in the literature, however,
this notion is still not fully understood. The main contribution of this paper
is a study of the order propagation in the deterministic setting, which
potentially can provide new techniques for analysis in the stochastic one. We
take a close look at the propagation of the so-called increasing and increasing
convex orders. Infinitesimal characterisations of these orders are derived,
which resemble the well-known Kamke conditions for monotonicity. It is shown
that increasing order is equivalent to the standard monotonicity, while the
class of systems propagating the increasing convex order is equivalent to the
class of monotone systems with convex vector fields. The paper is concluded by
deriving a novel result on order propagating diffusion processes and an
application of this result to biological processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02563</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02563</id><created>2015-03-09</created><authors><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Soria-Comas</keyname><forenames>Jordi</forenames></author><author><keyname>Ciobotaru</keyname><forenames>Oana</forenames></author></authors><title>Co-Utility: Self-Enforcing Protocols without Coordination Mechanisms</title><categories>cs.GT</categories><comments>Proceedings of the 2015 International Conference on Industrial
  Engineering and Operations Management-IEOM 2015, Dubai, United Arab Emirates,
  March 3-5, 2015. To appear in IEEE Explore</comments><msc-class>91Axx</msc-class><acm-class>K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing some task among a set of agents requires the use of some protocol
that regulates the interactions between them. If those agents are rational,
they may try to subvert the protocol for their own benefit, in an attempt to
reach an outcome that provides greater utility. We revisit the traditional
notion of self-enforcing protocols implemented using existing game-theoretic
solution concepts, we describe its shortcomings in real-world applications, and
we propose a new notion of self-enforcing protocols, namely co-utile protocols.
The latter represent a solution concept that can be implemented without a
coordination mechanism in situations when traditional self-enforcing protocols
need a coordination mechanism. Co-utile protocols are preferable in
decentralized systems of rational agents because of their efficiency and
fairness. We illustrate the application of co-utile protocols to information
technology, specifically to preserving the privacy of query profiles of
database/search engine users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02570</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02570</id><created>2015-03-05</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Santos-Magalhaes</keyname><forenames>N. S.</forenames></author></authors><title>The Genetic Code revisited: Inner-to-outer map, 2D-Gray map, and
  World-map Genetic Representations</title><categories>q-bio.OT cs.CE</categories><comments>6 pages, 5 figures</comments><journal-ref>Lecture Notes in Computer Science, LNCS 3124, Heidelberg: Springer
  Verlag, vol.1, pp.526-531, 2004</journal-ref><doi>10.1007/b99377</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to represent the genetic code? Despite the fact that it is extensively
known, the DNA mapping into proteins remains as one of the relevant discoveries
of genetics. However, modern genomic signal processing usually requires
converting symbolic-DNA strings into complex-valued signals in order to take
full advantage of a broad variety of digital processing techniques. The genetic
code is revisited in this paper, addressing alternative representations for it,
which can be worthy for genomic signal processing. Three original
representations are discussed. The inner-to-outer map builds on the unbalanced
role of nucleotides of a 'codon' and it seems to be suitable for handling
information-theory-based matter. The two-dimensional-Gray map representation is
offered as a mathematically structured map that can help interpreting
spectrograms or scalograms. Finally, the world-map representation for the
genetic code is investigated, which can particularly be valuable for
educational purposes -besides furnishing plenty of room for application of
distance-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02574</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02574</id><created>2015-03-06</created><authors><author><keyname>Szab&#xf3;</keyname><forenames>Gy&#xf6;rgy</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author></authors><title>Congestion phenomena caused by matching pennies in evolutionary games</title><categories>physics.soc-ph cond-mat.stat-mech cs.GT</categories><comments>7 pages, 6 figures</comments><journal-ref>Physical Review E 91 (2015) 032110</journal-ref><doi>10.1103/PhysRevE.91.032110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary social dilemma games are extended by an additional
matching-pennies game that modifies the collected payoffs. In a spatial version
players are distributed on a square lattice and interact with their neighbors.
Firstly, we show that the matching-pennies game can be considered as the
microscopic force of the Red Queen effect that breaks the detailed balance and
induces eddies in the microscopic probability currents if the strategy update
is analogous to the Glauber dynamics for the kinetic Ising models. The
resulting loops in probability current breaks symmetry between the
chessboard-like arrangements of strategies via a bottleneck effect occurring
along the four-edge loops in the microscopic states. The impact of this
congestion is analogous to the application of a staggered magnetic field in the
Ising model, that is, the order-disorder critical transition is wiped out by
noise. It is illustrated that the congestion induced symmetry breaking can be
beneficial for the whole community within a certain region of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02577</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02577</id><created>2015-03-09</created><authors><author><keyname>Silva</keyname><forenames>G. Jer&#xf4;nimo da</forenames><suffix>Jr.</suffix></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>New Algorithms for Computing a Single Component of the Discrete Fourier
  Transform</title><categories>cs.DM cs.DS stat.ME</categories><comments>4 pages, 3 figures, 1 table. In: 10th International Symposium on
  Communication Theory and Applications, Ambleside, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the theory and hardware implementation of two new
algorithms for computing a single component of the discrete Fourier transform.
In terms of multiplicative complexity, both algorithms are more efficient, in
general, than the well known Goertzel Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02578</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02578</id><created>2015-03-09</created><authors><author><keyname>Khademian</keyname><forenames>Mahdi</forenames></author><author><keyname>Homayounpour</keyname><forenames>Mohammad Mehdi</forenames></author></authors><title>Modeling State-Conditional Observation Distribution using Weighted
  Stereo Samples for Factorial Speech Processing Models</title><categories>cs.LG cs.AI cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the role of factorial speech processing models in
noise-robust automatic speech recognition tasks. Factorial models can embed
non-stationary noise models using Markov chains as one of its source chain. The
paper proposes a modeling scheme for modeling state-conditional observation
distribution of factorial models based on weighted stereo samples. This scheme
is an extension to previous single pass retraining for ideal model compensation
and here we used it to construct ideal state-conditional observation
distributions. Experiments of this paper over the set A of the Aurora 2 dataset
shows that by considering noise models with multiple states, system performance
can be improved especially in low SNR conditions up to 4% absolute word
recognition performance. In addition to its power in accurate representation of
state-conditional observation distribution, it has an important advantage over
previous methods by providing the opportunity to independently select feature
spaces for both source and corrupted features. This opens a new window for
seeking better feature spaces appropriate for noise-robust tasks independent
from clean speech feature space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02592</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02592</id><created>2015-03-09</created><authors><author><keyname>Sorenson</keyname><forenames>Jonathan P.</forenames></author></authors><title>Two Compact Incremental Prime Sieves</title><categories>cs.DS math.NT</categories><msc-class>Primary 11Y16, 68Q25, Secondary 11Y11, 11A51</msc-class><acm-class>F.2.1; I.1.2; E.1</acm-class><journal-ref>LMS Jour. Comp. Math. 18 (2015) 675-683</journal-ref><doi>10.1112/S1461157015000194</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prime sieve is an algorithm that finds the primes up to a bound $n$. We say
that a prime sieve is incremental, if it can quickly determine if $n+1$ is
prime after having found all primes up to $n$. We say a sieve is compact if it
uses roughly $\sqrt{n}$ space or less. In this paper we present two new
results:
  (1) We describe the rolling sieve, a practical, incremental prime sieve that
takes $O(n\log\log n)$ time and $O(\sqrt{n}\log n)$ bits of space, and
  (2) We show how to modify the sieve of Atkin and Bernstein (2004) to obtain a
sieve that is simultaneously sublinear, compact, and incremental.
  The second result solves an open problem given by Paul Pritchard in 1994.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02596</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02596</id><created>2015-03-09</created><updated>2015-05-24</updated><authors><author><keyname>Pimentel-Alarc&#xf3;n</keyname><forenames>Daniel L.</forenames></author><author><keyname>Boston</keyname><forenames>Nigel</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author></authors><title>A Characterization of Deterministic Sampling Patterns for Low-Rank
  Matrix Completion</title><categories>stat.ML cs.LG math.AG</categories><comments>This update includes a generalized version of the results in version
  1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank matrix completion (LRMC) problems arise in a wide variety of
applications. Previous theory mainly provides conditions for completion under
missing-at-random samplings. An incomplete $d \times N$ matrix is
$\textit{finitely completable}$ if there are at most finitely many rank-$r$
matrices that agree with all its observed entries. Finite completability is the
tipping point in LRMC, as a few additional samples of a finitely completable
matrix guarantee its $\textit{unique}$ completability. The main contribution of
this paper is a full characterization of finitely completable observation sets.
We use this characterization to derive sufficient deterministic sampling
conditions for unique completability. We also show that under uniform random
sampling schemes, these conditions are satisfied with high probability if at
least $\mathscr{O}(\max\{r,\log d \})$ entries per column are observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02603</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02603</id><created>2015-03-09</created><updated>2015-08-27</updated><authors><author><keyname>Shifrin</keyname><forenames>Mark</forenames></author></authors><title>An asymptotically optimal policy and state-space collapse for the
  multi-class shared queue</title><categories>cs.PF math.PR</categories><comments>arXiv admin note: text overlap with arXiv:1412.6775</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-class G/G/1 queue with a finite shared buffer. There is
task admission and server scheduling control which aims to minimize the cost
which consists of holding and rejection components. We construct a policy that
is asymptotically optimal in the heavy traffic limit. The policy stems from
solution to Harrison-Taksar (HT) free boundary problem and is expressed by a
single free boundary point. We show that the HT problem solution translated
into the queuelength processes follows a specific {\it triangular} form. This
form implies the queuelength control policy which is different from the known
$c\mu$ priority rule and has a novel structure.
  We exemplify that the probabilistic methods we exploit can be successfully
applied to solving scheduling and admission problems in cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02619</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02619</id><created>2015-03-09</created><authors><author><keyname>Mishkin</keyname><forenames>Dmytro</forenames></author><author><keyname>Matas</keyname><forenames>Jiri</forenames></author><author><keyname>Perdoch</keyname><forenames>Michal</forenames></author></authors><title>MODS: Fast and Robust Method for Two-View Matching</title><categories>cs.CV</categories><comments>27 pages, 12 figures. Preprint submitted to CVIU. arXiv admin note:
  text overlap with arXiv:1306.3855</comments><doi>10.1016/j.cviu.2015.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel algorithm for wide-baseline matching called MODS - Matching On Demand
with view Synthesis algorithm (MODS) - is presented. The MODS algorithm is
experimentally shown to solve a broader range of wide-baseline problems than
the state of the art while being nearly as fast as standard matchers on simple
problems. The apparent robustness vs. speed trade-off is finessed by the use of
progressively more time-consuming feature detectors and by on-demand generation
of synthesized images that is performed until a reliable estimate of geometry
is obtained.
  We also introduce an improved method for tentative correspondence selection,
applicable both with and without view synthesis. A modification of the standard
first to second nearest distance rule increases the number of correct matches
by 5-20% at no additional computational cost.
  Performance of the MODS algorithm is evaluated on standard publicly available
datasets and on a new set of geometrically challenging wide baseline problems
that is made public together with the ground truth. Experiments show that the
MODS outperforms the state-of-the-art in robustness and speed. Moreover, MODS
performs well on other classes of difficult two-view problems like matching of
images from different modalities, with wide temporal baseline or with
significant lighting changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02626</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02626</id><created>2015-03-09</created><authors><author><keyname>Windridge</keyname><forenames>David</forenames></author></authors><title>On the Intrinsic Limits to Representationally-Adaptive Machine-Learning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning is a familiar problem setting within Machine-Learning in
which data is presented serially in time to a learning agent, requiring it to
progressively adapt within the constraints of the learning algorithm. More
sophisticated variants may involve concepts such as transfer-learning which
increase this adaptive capability, enhancing the learner's cognitive capacities
in a manner that can begin to imitate the open-ended learning capabilities of
human beings.
  We shall argue in this paper, however, that a full realization of this notion
requires that, in addition to the capacity to adapt to novel data, autonomous
online learning must ultimately incorporate the capacity to update its own
representational capabilities in relation to the data. We therefore enquire
about the philosophical limits of this process, and argue that only fully
embodied learners exhibiting an a priori perception-action link in order to
ground representational adaptations are capable of exhibiting the full range of
human cognitive capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02642</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02642</id><created>2015-02-19</created><authors><author><keyname>Zhou</keyname><forenames>Yanzi</forenames></author><author><keyname>Takahashi</keyname><forenames>Ryo</forenames></author><author><keyname>Hikihara</keyname><forenames>Takashi</forenames></author></authors><title>Security of Power Packet Dispatching Using Differential Chaos Shift
  Keying</title><categories>cs.IT cs.NI math.IT nlin.CD</categories><comments>9 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates and confirms one advantageous function of a power
packet dispatching system, which has been proposed by authors' group with being
apart from the conventional power distribution system. Here is focused on the
function to establish the security of power packet dispatching for prohibiting
not only information but also power of power packet from being stolen by
attackers. For the purpose of protecting power packets, we introduce a simple
encryption of power packets before sending them. Encryption scheme based on
chaotic signal is one possibility for this purpose. This paper adopts the
Differential Chaos Shift Keying (DCSK) scheme for the encryption, those are
partial power packet encryption and whole power packet encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02654</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02654</id><created>2015-03-09</created><updated>2015-05-22</updated><authors><author><keyname>Mills</keyname><forenames>K. Alex</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>R.</forenames></author><author><keyname>Mittal</keyname><forenames>Neeraj</forenames></author></authors><title>Algorithms for Replica Placement in High-Availability Storage</title><categories>cs.DS cs.DC</categories><comments>22 pages, 7 figures, 4 algorithm listings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new model of causal failure is presented and used to solve a novel replica
placement problem in data centers. The model describes dependencies among
system components as a directed graph. A replica placement is defined as a
subset of vertices in such a graph. A criterion for optimizing replica
placements is formalized and explained. In this work, the optimization goal is
to avoid choosing placements in which a single failure event is likely to wipe
out multiple replicas. Using this criterion, a fast algorithm is given for the
scenario in which the dependency model is a tree. The main contribution of the
paper is an $O(n + \rho \log \rho)$ dynamic programming algorithm for placing
$\rho$ replicas on a tree with $n$ vertices. This algorithm exhibits the
interesting property that only two subproblems need to be recursively
considered at each stage. An $O(n^2 \rho)$ greedy algorithm is also briefly
reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02656</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02656</id><created>2015-03-09</created><updated>2015-11-29</updated><authors><author><keyname>Chen</keyname><forenames>Kongyang</forenames></author><author><keyname>Tan</keyname><forenames>Guang</forenames></author></authors><title>Modeling and Improving the Energy Performance of GPS Receivers for
  Mobile Applications</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrated GPS receivers have become a basic module in today's mobile
devices. While serving as the cornerstone for location based services, GPS
modules have a serious battery drain problem due to high computation load. This
paper aims to reveal the impact of key software parameters on hardware energy
consumption, by establishing an energy model for a standard GPS receiver
architecture as found in both academic and industrial designs. In particular,
our measurements show that the receiver's energy consumption is in large part
linear with the number of tracked satellites. This leads to a design of
selective tracking algorithm that provides similar positioning accuracy (around
12m) with a subset of selected satellites, which translates to an energy saving
of 20.9-23.1\% on the Namuru board.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02675</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02675</id><created>2015-03-09</created><updated>2015-03-18</updated><authors><author><keyname>Arth</keyname><forenames>Clemens</forenames></author><author><keyname>Pirchheim</keyname><forenames>Christian</forenames></author><author><keyname>Ventura</keyname><forenames>Jonathan</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author></authors><title>Global 6DOF Pose Estimation from Untextured 2D City Models</title><categories>cs.CV</categories><comments>9 pages excluding supplementary material</comments><acm-class>I.2.10; I.3.7; I.4.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a method for estimating the 3D pose for the camera of a mobile
device in outdoor conditions, using only an untextured 2D model. Previous
methods compute only a relative pose using a SLAM algorithm, or require many
registered images, which are cumbersome to acquire. By contrast, our method
returns an accurate, absolute camera pose in an absolute referential using
simple 2D+height maps, which are broadly available, to refine a first estimate
of the pose provided by the device's sensors. We show how to first estimate the
camera absolute orientation from straight line segments, and then how to
estimate the translation by aligning the 2D map with a semantic segmentation of
the input image. We demonstrate the robustness and accuracy of our approach on
a challenging dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02678</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02678</id><created>2015-03-10</created><updated>2015-03-27</updated><authors><author><keyname>Karimi</keyname><forenames>Kamran</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Diwakar</forenames></author><author><keyname>Mirjafari</keyname><forenames>Parissa</forenames></author></authors><title>When In-Memory Computing is Slower than Heavy Disk Usage</title><categories>cs.OH</categories><comments>This paper has been withdrawn by the authors for personal reasons. No
  further revisions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disk access latency and transfer times are often considered to have a major
and detrimental impact on the running time of software. Developers are often
advised to favour in-memory operations and minimise disk access. Furthermore,
diskless computer architectures are being studied and designed to remove this
bottleneck all together, to improve application performance in areas such as
High Performance Computing, Big Data, and Business Intelligence. In this paper
we use code inspired by real, production software, to show that in-memory
operations are not always a guarantee for high performance, and may actually
cause a considerable slow-down. We also show how small code changes can have
dramatic effects on running times. We argue that a combination of system-level
improvements and better developer awareness and coding practices are necessary
to ensure in-memory computing can achieve its full potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02705</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02705</id><created>2015-03-09</created><updated>2015-06-14</updated><authors><author><keyname>Li</keyname><forenames>Sen</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Lian</keyname><forenames>Jianming</forenames></author><author><keyname>Kalsi</keyname><forenames>Karanjit</forenames></author></authors><title>A Mechanism Design Approach for Coordination of Thermostatically
  Controlled Loads</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the coordination of a population of thermostatically
controlled loads (TCLs) with unknown parameters to achieve group objectives.
The problem involves designing the device bidding and market clearing
strategies to motivate self-interested users to realize efficient energy
allocation subject to a peak energy constraint. This coordination problem is
formulated as a mechanism design problem, and we propose a mechanism to
implement the social choice function in dominant strategy equilibrium. The
proposed mechanism consists of a novel bidding and clearing strategy that
incorporates the internal dynamics of TCLs in the market mechanism design, and
we show it can realize the team optimal solution. A learning scheme is proposed
to address the unknown load model parameters. Numerical simulations are
performed to validate the effectiveness of the proposed coordination framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02725</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02725</id><created>2015-03-09</created><updated>2015-03-30</updated><authors><author><keyname>Sharma</keyname><forenames>Abhishek</forenames></author><author><keyname>Tuzel</keyname><forenames>Oncel</forenames></author><author><keyname>Jacobs</keyname><forenames>David W.</forenames></author></authors><title>Deep Hierarchical Parsing for Semantic Segmentation</title><categories>cs.CV</categories><comments>IEEE CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a learning-based approach to scene parsing inspired by
the deep Recursive Context Propagation Network (RCPN). RCPN is a deep
feed-forward neural network that utilizes the contextual information from the
entire image, through bottom-up followed by top-down context propagation via
random binary parse trees. This improves the feature representation of every
super-pixel in the image for better classification into semantic categories. We
analyze RCPN and propose two novel contributions to further improve the model.
We first analyze the learning of RCPN parameters and discover the presence of
bypass error paths in the computation graph of RCPN that can hinder contextual
propagation. We propose to tackle this problem by including the classification
loss of the internal nodes of the random parse trees in the original RCPN loss
function. Secondly, we use an MRF on the parse tree nodes to model the
hierarchical dependency present in the output. Both modifications provide
performance boosts over the original RCPN and the new system achieves
state-of-the-art performance on Stanford Background, SIFT-Flow and Daimler
urban datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02727</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02727</id><created>2015-03-09</created><updated>2015-08-05</updated><authors><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author><author><keyname>Xu</keyname><forenames>Lina</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Li</keyname><forenames>Yun</forenames></author><author><keyname>Kelly</keyname><forenames>Kevin</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Video Compressive Sensing for Spatial Multiplexing Cameras using
  Motion-Flow Models</title><categories>cs.CV</categories><comments>in SIAM Journal on Imaging Sciences, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial multiplexing cameras (SMCs) acquire a (typically static) scene
through a series of coded projections using a spatial light modulator (e.g., a
digital micro-mirror device) and a few optical sensors. This approach finds use
in imaging applications where full-frame sensors are either too expensive
(e.g., for short-wave infrared wavelengths) or unavailable. Existing SMC
systems reconstruct static scenes using techniques from compressive sensing
(CS). For videos, however, existing acquisition and recovery methods deliver
poor quality. In this paper, we propose the CS multi-scale video (CS-MUVI)
sensing and recovery framework for high-quality video acquisition and recovery
using SMCs. Our framework features novel sensing matrices that enable the
efficient computation of a low-resolution video preview, while enabling
high-resolution video recovery using convex optimization. To further improve
the quality of the reconstructed videos, we extract optical-flow estimates from
the low-resolution previews and impose them as constraints in the recovery
procedure. We demonstrate the efficacy of our CS-MUVI framework for a host of
synthetic and real measured SMC video data, and we show that high-quality
videos can be recovered at roughly $60\times$ compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02729</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02729</id><created>2015-03-09</created><authors><author><keyname>Althoff</keyname><forenames>Tim</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Donor Retention in Online Crowdfunding Communities: A Case Study of
  DonorsChoose.org</title><categories>cs.CY cs.SI</categories><comments>preprint version of WWW 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online crowdfunding platforms like DonorsChoose.org and Kickstarter allow
specific projects to get funded by targeted contributions from a large number
of people. Critical for the success of crowdfunding communities is recruitment
and continued engagement of donors. With donor attrition rates above 70%, a
significant challenge for online crowdfunding platforms as well as traditional
offline non-profit organizations is the problem of donor retention.
  We present a large-scale study of millions of donors and donations on
DonorsChoose.org, a crowdfunding platform for education projects. Studying an
online crowdfunding platform allows for an unprecedented detailed view of how
people direct their donations. We explore various factors impacting donor
retention which allows us to identify different groups of donors and quantify
their propensity to return for subsequent donations. We find that donors are
more likely to return if they had a positive interaction with the receiver of
the donation. We also show that this includes appropriate and timely
recognition of their support as well as detailed communication of their impact.
Finally, we discuss how our findings could inform steps to improve donor
retention in crowdfunding communities and non-profit organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02732</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02732</id><created>2015-03-09</created><authors><author><keyname>Ramli</keyname><forenames>Carroline Dewi Puspa Kencana</forenames></author></authors><title>Detecting Incompleteness, Conflicting and Unreachability XACML Policies
  using Answer Set Programming</title><categories>cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1206.5327</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, XACML is a popular access control policy language that is used
widely in many applications. Policies in XACML are built based on many
components over distributed resources. Due to the expressiveness of XACML, it
is not trivial for policy administrators to understand the overall effect and
consequences of XACML policies they have written. In this paper we show a
mechanism and a tool how to analyses big access control policies sets such as
(i) incompleteness policies, (ii) conflicting policies, and (iii) unreachable
policies. To detect these problems we present a method using Answer Set
Programming (ASP) in the context of XACML 3.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02735</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02735</id><created>2015-03-09</created><authors><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Urgaonkar</keyname><forenames>Rahul</forenames></author><author><keyname>Chan</keyname><forenames>Kevin</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author><author><keyname>Zafer</keyname><forenames>Murtaza</forenames></author><author><keyname>Leung</keyname><forenames>Kin K.</forenames></author></authors><title>Dynamic Service Placement for Mobile Micro-Clouds with Predicted Future
  Costs</title><categories>cs.DC cs.NI math.OC</categories><comments>in Proc. of IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seamless computing and data access is enabled by the emerging technology of
mobile micro-clouds (MMCs). Different from traditional centralized clouds, an
MMC is typically connected directly to a wireless base-station and provides
services to a small group of users, which allows users to have instantaneous
access to cloud services. Due to the limited coverage area of base-stations and
the dynamic nature of mobile users, network background traffic, etc., the
question of where to place the services to cope with these dynamics arises. In
this paper, we focus on dynamic service placement for MMCs. We consider the
case where there is an underlying mechanism to predict the future costs of
service hosting and migration, and the prediction error is assumed to be
bounded. Our goal is to find the optimal service placement sequence which
minimizes the average cost over a given time. To solve this problem, we first
propose a method which solves for the optimal placement sequence for a specific
look-ahead time-window, based on the predicted costs in this time-window. We
show that this problem is equivalent to a shortest-path problem and propose an
algorithm with polynomial time-complexity to find its solution. Then, we
propose a method to find the optimal look-ahead window size, which minimizes an
upper bound of the average cost. Finally, we evaluate the effectiveness of the
proposed approach by simulations with real-world user-mobility traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02737</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02737</id><created>2015-03-09</created><authors><author><keyname>Basu</keyname><forenames>K.</forenames></author><author><keyname>Owen</keyname><forenames>A. B.</forenames></author></authors><title>Scrambled geometric net integration over general product spaces</title><categories>cs.NA math.NA stat.CO</categories><comments>29 pages; 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quasi-Monte Carlo (QMC) sampling has been developed for integration over
$[0,1]^s$ where it has superior accuracy to Monte Carlo (MC) for integrands of
bounded variation. Scrambled net quadrature gives allows replication based
error estimation for QMC with at least the same accuracy and for smooth enough
integrands even better accuracy than plain QMC. Integration over triangles,
spheres, disks and Cartesian products of such spaces is more difficult for QMC
because the induced integrand on a unit cube may fail to have the desired
regularity. In this paper, we present a construction of point sets for
numerical integration over Cartesian products of $s$ spaces of dimension $d$,
with triangles ($d=2$) being of special interest. The point sets are
transformations of randomized $(t,m,s)$-nets using recursive geometric
partitions. The resulting integral estimates are unbiased and their variance is
$o(1/n)$ for any integrand in $L^2$ of the product space. Under smoothness
assumptions on the integrand, our randomized QMC algorithm has variance
$O(n^{-1 - 2/d} (\log n)^{s-1})$, for integration over $s$-fold Cartesian
products of $d$-dimensional domains, compared to $O(n^{-1})$ for ordinary Monte
Carlo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02746</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02746</id><created>2015-03-09</created><authors><author><keyname>Babai</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Wilmes</keyname><forenames>John</forenames></author></authors><title>Asymptotic Delsarte cliques in distance-regular graphs</title><categories>math.CO cs.DM cs.DS</categories><comments>10 pages</comments><msc-class>05E30 (primary), 68R05, 68R10, 68Q25, 05C99 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new bound on the parameter $\lambda$ (number of common neighbors of
a pair of adjacent vertices) in a distance-regular graph $G$, improving and
generalizing bounds for strongly regular graphs by Spielman (1996) and Pyber
(2014). The new bound is one of the ingredients of recent progress on the
complexity of testing isomorphism of strongly regular graphs (Babai, Chen, Sun,
Teng, Wilmes 2013). The proof is based on a clique geometry found by Metsch
(1991) under certain constraints on the parameters. We also give a simplified
proof of the following asymptotic consequence of Metsch's result: if $k\mu =
o(\lambda^2)$ then each edge of $G$ belongs to a unique maximal clique of size
asymptotically equal to $\lambda$, and all other cliques have size
$o(\lambda)$. Here $k$ denotes the degree and $\mu$ the number of common
neighbors of a pair of vertices at distance 2. We point out that Metsch's
cliques are &quot;asymptotically Delsarte&quot; when $k\mu = o(\lambda^2)$, so families
of distance-regular graphs with parameters satisfying $k\mu = o(\lambda^2)$ are
&quot;asymptotically Delsarte-geometric.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02747</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02747</id><created>2015-03-09</created><authors><author><keyname>Shi</keyname><forenames>Zheng</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Tam</keyname><forenames>Kam-Weng</forenames></author></authors><title>Rate Selection for Cooperative HARQ-CC Systems over Time-Correlated
  Nakagami-m Fading Channels</title><categories>cs.IT math.IT</categories><comments>6 Pages; 4 Figures, 2015 ICC Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of rate selection for the cooperative hybrid
automatic repeat request with chase combination (HARQ-CC) system, where time
correlated Nakagami-m fading channels are considered. To deal with this
problem, the closed-form cumulative distribution function (CDF) for the combine
SNRs through maximal ratio combining (MRC) is first derived as a generalized
Fox's $\bar H$ function. By using this result, outage probability and
delay-limited throughput (DLT) are derived in closed forms, which then enables
the rate selection for maximum DLT. These analytical results are validated via
Monte Carlo simulations. The impacts of time correlation and channel
fading-order parameter $m$ upon outage probability, DLT and the optimal rate
are investigated thoroughly. It is found that the system can achieve more
diversity gain from less correlated channels, and the outage probability of
cooperative HARQ-CC system decreases with the increase of $m$, and etc.
Furthermore, the optimal rate increases with the number of retransmissions,
while it decreases with the increase of the channel time correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02754</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02754</id><created>2015-03-09</created><authors><author><keyname>Bao</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Hua-Wei</forenames></author><author><keyname>Jin</keyname><forenames>Xiaolong</forenames></author><author><keyname>Cheng</keyname><forenames>Xue-Qi</forenames></author></authors><title>Modeling and Predicting Popularity Dynamics of Microblogs using
  Self-Excited Hawkes Processes</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to model and predict the popularity dynamics of individual user
generated items on online media has important implications in a wide range of
areas. In this paper, we propose a probabilistic model using a Self-Excited
Hawkes Process(SEHP) to characterize the process through which individual
microblogs gain their popularity. This model explicitly captures the triggering
effect of each forwarding, distinguishing itself from the reinforced Poisson
process based model where all previous forwardings are simply aggregated as a
single triggering effect. We validate the proposed model by applying it on Sina
Weibo, the most popular microblogging network in China. Experimental results
demonstrate that the SEHP model consistently outperforms the model based on
reinforced Poisson process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02761</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02761</id><created>2015-03-09</created><updated>2015-03-12</updated><authors><author><keyname>Bargi</keyname><forenames>Ava</forenames></author><author><keyname>Da Xu</keyname><forenames>Richard Yi</forenames></author><author><keyname>Piccardi</keyname><forenames>Massimo</forenames></author></authors><title>An Adaptive Online HDP-HMM for Segmentation and Classification of
  Sequential Data</title><categories>stat.ML cs.LG</categories><comments>23 pages, 9 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years, the desire and need to understand sequential data has
been increasing, with particular interest in sequential contexts such as
patient monitoring, understanding daily activities, video surveillance, stock
market and the like. Along with the constant flow of data, it is critical to
classify and segment the observations on-the-fly, without being limited to a
rigid number of classes. In addition, the model needs to be capable of updating
its parameters to comply with possible evolutions. This interesting problem,
however, is not adequately addressed in the literature since many studies focus
on offline classification over a pre-defined class set. In this paper, we
propose a principled solution to this gap by introducing an adaptive online
system based on Markov switching models with hierarchical Dirichlet process
priors. This infinite adaptive online approach is capable of segmenting and
classifying the sequential data over unlimited number of classes, while meeting
the memory and delay constraints of streaming contexts. The model is further
enhanced by introducing a learning rate, responsible for balancing the extent
to which the model sustains its previous learning (parameters) or adapts to the
new streaming observations. Experimental results on several variants of
stationary and evolving synthetic data and two video datasets, TUM Assistive
Kitchen and collatedWeizmann, show remarkable performance in segmentation and
classification, particularly for evolutionary sequences with changing
distributions and/or containing new, unseen classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02766</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02766</id><created>2015-03-10</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Generating Single Peaked Votes</title><categories>cs.GT cs.MA</categories><comments>Keywords: voting, social choice, single peaked votes, Impartial
  Culture</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how to generate singled peaked votes uniformly from the Impartial
Culture model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02773</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02773</id><created>2015-03-10</created><authors><author><keyname>Tedder</keyname><forenames>Marc</forenames></author></authors><title>Simpler, Linear-Time Transitive Orientation via Lexicographic
  Breadth-First Search</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparability graphs are the undirected graphs whose edges can be directed so
that the resulting directed graph is transitive. They are related to posets and
have applications in scheduling theory. This paper considers the problem of
finding a transitive orientation of a comparability graph, a requirement for
many of its applications. A linear-time algorithm is presented based on an
elegant partition refinement scheme developed elsewhere for the problem. The
algorithm is intended as a simpler and more practical alternative to the
existing lineartime solution, which is commonly understood to be difficult and
mainly of theoretical value. It accomplishes this by using Lexicographic
Breadth-First Search to achieve the same effect as produced by modular
decomposition in the earlier linear-time algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02774</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02774</id><created>2015-03-10</created><authors><author><keyname>Hesterberg</keyname><forenames>Adam</forenames></author><author><keyname>Lincoln</keyname><forenames>Andrea</forenames></author><author><keyname>Lynch</keyname><forenames>Jayson</forenames></author></authors><title>Improved Connectivity Condition for Byzantine Fault Tolerance</title><categories>cs.DC cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a network in which some pairs of nodes can communicate freely, and some
subsets of the nodes could be faulty and colluding to disrupt communication,
when can messages reliably be sent from one given node to another? We give a
new characterization of when the agreement problem can be solved and provide an
agreement algorithm which can reach agreement when the number of Byzantine
nodes along each minimal vertex cut is bounded. Our new bound holds for a
strict superset of cases than the previously known bound. We show that the new
bound is tight. Furthermore, we show that this algorithm does not require the
processes to know the graph structure, as the previously known algorithm did.
Finally, we explore some of the situations in which we can reach agreement if
we assume that individual nodes or entire subgraphs are trustworthy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02779</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02779</id><created>2015-03-10</created><authors><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>On metric properties of maps between Hamming spaces and related graph
  homomorphisms</title><categories>math.CO cs.IT math.IT</categories><msc-class>05B40, 11H71, 52C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mapping of $k$-bit strings into $n$-bit strings is called an
$(\alpha,\beta)$-map if $k$-bit strings which are more than $\alpha k$ apart
are mapped to $n$-bit strings that are more than $\beta n$ apart. This is a
relaxation of the classical error-correcting codes problem ($\alpha=0$).
  The question is equivalent to existence of graph homomorphisms between
certain graphs on the hypercube. Tools based on Schrijver's $\theta$-function
are developed for testing when such homomorphisms are possible.
  For $n&gt;k$ the non-existence results on $(\alpha,\beta)$ are proved by
invoking the asymptotic results on $\theta$-function of McEliece, Rodemich,
Rumsey and Welch (1977), Samorodnitsky (2001) as well as an exact solution of
Delsarte's linear program for $d&gt;n/2$. Among other things, these bounds show
that for $\beta&gt;1/2$ and $n/k$ -- integer, the repetition map achieving
$\alpha=\beta$ is best possible.
  For $n&lt;k$ a quantitative version of the no-homomorphism lemma is used
together with Kleitman's theorem, which precisely characterizes the
diameter-volume tradeoff in Hamming space.
  Finally, the question of constructing good linear $(\alpha,\beta)$ maps is
shown to be equivalent to finding certain extremal configurations of points in
(finite) projective spaces. Consequently, implications of our results for
projective geometry over $\FF_2$ is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02781</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02781</id><created>2015-03-10</created><authors><author><keyname>Roughan</keyname><forenames>Matthew</forenames></author><author><keyname>Tuke</keyname><forenames>Jonathan</forenames></author></authors><title>Unravelling Graph-Exchange File Formats</title><categories>cs.DB cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A graph is used to represent data in which the relationships between the
objects in the data are at least as important as the objects themselves. Over
the last two decades nearly a hundred file formats have been proposed or used
to provide portable access to such data. This paper seeks to review these
formats, and provide some insight to both reduce the ongoing creation of
unnecessary formats, and guide the development of new formats where needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02782</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02782</id><created>2015-03-10</created><updated>2015-04-01</updated><authors><author><keyname>Matth&#xe9;</keyname><forenames>Maximilian</forenames></author><author><keyname>Gaspar</keyname><forenames>Ivan</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Reduced Complexity Calculation of LMMSE Filter Coefficients for GFDM</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Electronics Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A low-complexity algorithm for calculation of the LMMSE filter coefficients
for GFDM in a block-fading multipath environment is derived in this letter. The
simplification is based on the block circularity of the involved matrices. The
proposal reduces complexity from cubic to squared order. The proposed approach
can be generalized to other waveforms with circular pulse shaping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02784</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02784</id><created>2015-03-10</created><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Cantoni</keyname><forenames>Michael</forenames></author></authors><title>Promoting Truthful Behaviour in Participatory-Sensing Mechanisms</title><categories>cs.GT cs.SY math.OC</categories><comments>IEEE Signal Processing Letters, In Press</comments><doi>10.1109/LSP.2015.2412122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the interplay between a class of nonlinear estimators and
strategic sensors is studied in several participatory-sensing scenarios. It is
shown that for the class of estimators, if the strategic sensors have access to
noiseless measurements of the to-be-estimated-variable, truth-telling is an
equilibrium of the game that models the interplay between the sensors and the
estimator. Furthermore, performance of the proposed estimators is examined in
the case that the strategic sensors form coalitions and in the presence of
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02801</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02801</id><created>2015-03-10</created><authors><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author><author><keyname>Xu</keyname><forenames>Bo</forenames></author><author><keyname>Tian</keyname><forenames>Guanhua</forenames></author><author><keyname>Zhao</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Fangyuan</forenames></author><author><keyname>Hao</keyname><forenames>Hongwei</forenames></author></authors><title>Short Text Hashing Improved by Integrating Multi-Granularity Topics and
  Tags</title><categories>cs.IR cs.CL</categories><comments>12 pages, accepted at CICLing 2015</comments><doi>10.1007/978-3-319-18111-0_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to computational and storage efficiencies of compact binary codes,
hashing has been widely used for large-scale similarity search. Unfortunately,
many existing hashing methods based on observed keyword features are not
effective for short texts due to the sparseness and shortness. Recently, some
researchers try to utilize latent topics of certain granularity to preserve
semantic similarity in hash codes beyond keyword matching. However, topics of
certain granularity are not adequate to represent the intrinsic semantic
information. In this paper, we present a novel unified approach for short text
Hashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, we
propose a selection method to choose the optimal multi-granularity topics
depending on the type of dataset, and design two distinct hashing strategies to
incorporate multi-granularity topics. We also propose a simple and effective
method to exploit tags to enhance the similarity of related texts. We carry out
extensive experiments on one short text dataset as well as on one normal text
dataset. The results demonstrate that our approach is effective and
significantly outperforms baselines on several evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02809</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02809</id><created>2015-03-10</created><authors><author><keyname>Kim</keyname><forenames>Na-Rae</forenames></author><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author></authors><title>A Universal Channel Model for Molecular Communication Systems with
  Metal-Oxide Detectors</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an end-to-end channel model for molecular
communication systems with metal-oxide sensors. In particular, we focus on the
recently developed table top molecular communication platform. The system is
separated into two parts: the propagation and the sensor detection. There is
derived, based on this, a more realistic end-to-end channel model. However,
since some of the coefficients in the derived models are unknown, we collect a
great deal of experimental data to estimate these coefficients and evaluate how
they change with respect to the different system parameters. Finally, a noise
model is derived for the system to complete an end-to-end system model for the
tabletop platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02815</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02815</id><created>2015-03-10</created><authors><author><keyname>Huang</keyname><forenames>Wenzhu</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>On the Performance of Multi-Antenna Wireless-Powered Communications with
  Energy Beamforming</title><categories>cs.IT math.IT</categories><comments>Accepted to appear in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the average throughput performance of energy
beamforming in a multi-antenna wireless-powered communication network (WPCN).
The considered network consists of one hybrid access-point (AP) with multiple
antennas and a single antenna user. The user does not have constant power
supply and thus needs to harvest energy from the signals broadcast by the AP in
the downlink (DL), before sending its data back to the AP with the harvested
energy in the uplink (UL). We derive closed-form expressions of the average
throughput and their asymptotic expressions at high SNR for both delay-limited
and delay-tolerant transmission modes. The optimal DL energy harvesting time,
which maximizes the system throughput, is then obtained for high SNR. All
analytical expressions are validated by numerical simulations. The impact of
various parameters, such as the AP transmit power, the energy harvesting time,
and the number of antennas, on the system throughput is also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02817</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02817</id><created>2015-03-10</created><authors><author><keyname>Yuan</keyname><forenames>Ming</forenames></author><author><keyname>Zhou</keyname><forenames>Ding-Xuan</forenames></author></authors><title>Minimax Optimal Rates of Estimation in High Dimensional Additive Models:
  Universal Phase Transition</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish minimax optimal rates of convergence for estimation in a high
dimensional additive model assuming that it is approximately sparse. Our
results reveal an interesting phase transition behavior universal to this class
of high dimensional problems. In the {\it sparse regime} when the components
are sufficiently smooth or the dimensionality is sufficiently large, the
optimal rates are identical to those for high dimensional linear regression,
and therefore there is no additional cost to entertain a nonparametric model.
Otherwise, in the so-called {\it smooth regime}, the rates coincide with the
optimal rates for estimating a univariate function, and therefore they are
immune to the &quot;curse of dimensionality&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02821</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02821</id><created>2015-03-10</created><updated>2015-03-16</updated><authors><author><keyname>Chynonova</keyname><forenames>Maryna</forenames></author></authors><title>Multiuser Scheduling for Simultaneous Wireless Information and Power
  Transfer Systems</title><categories>cs.IT math.IT</categories><comments>Master Thesis, Institute for Digital Communications,
  Friedrich-Alexander-Universit\&quot;at Erlangen-N\&quot;urnberg, Germany
  http://www.idc.lnt.de/en/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we study the downlink multiuser scheduling and power
allocation problem for systems with simultaneous wireless information and power
transfer (SWIPT). In the first part of the thesis, we focus on multiuser
scheduling. We design optimal scheduling algorithms that maximize the long-term
average system throughput under different fairness requirements, such as
proportional fairness and equal throughput fairness. In particular, the
algorithm designs are formulated as non-convex optimization problems which take
into account the minimum required average sum harvested energy in the system.
The problems are solved by using convex optimization techniques and the
proposed optimization framework reveals the tradeoff between the long-term
average system throughput and the sum harvested energy in multiuser systems
with fairness constraints. Simulation results demonstrate that substantial
performance gains can be achieved by the proposed optimization framework
compared to existing suboptimal scheduling algorithms from the literature. In
the second part of the thesis, we investigate the joint user scheduling and
power allocation algorithm design for SWIPT systems. The algorithm design is
formulated as a non-convex optimization problem which maximizes the achievable
rate subject to a minimum required average power transfer. Subsequently, the
non-convex optimization problem is reformulated by big-M method which can be
solved optimally. Furthermore, we show that joint power allocation and user
scheduling is an efficient way to enlarge the feasible trade-off region for
improving the system performance in terms of achievable data rate and harvested
energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02825</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02825</id><created>2015-03-10</created><authors><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>Davies</keyname><forenames>Adam</forenames></author></authors><title>The Digital Life of Walkable Streets</title><categories>cs.SI cs.CY</categories><comments>10 pages, 7 figures, Proceedings of International World Wide Web
  Conference (WWW 2015)</comments><acm-class>H.4.m</acm-class><doi>10.1145/2736277.2741631</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Walkability has many health, environmental, and economic benefits. That is
why web and mobile services have been offering ways of computing walkability
scores of individual street segments. Those scores are generally computed from
survey data and manual counting (of even trees). However, that is costly, owing
to the high time, effort, and financial costs. To partly automate the
computation of those scores, we explore the possibility of using the social
media data of Flickr and Foursquare to automatically identify safe and walkable
streets. We find that unsafe streets tend to be photographed during the day,
while walkable streets are tagged with walkability-related keywords. These
results open up practical opportunities (for, e.g., room booking services,
urban route recommenders, and real-estate sites) and have theoretical
implications for researchers who might resort to the use social media data to
tackle previously unanswered questions in the area of walkability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02828</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02828</id><created>2015-03-10</created><updated>2015-03-17</updated><authors><author><keyname>Tan</keyname><forenames>Mingkui</forenames></author><author><keyname>Xiao</keyname><forenames>Shijie</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Xu</keyname><forenames>Dong</forenames></author><author><keyname>Hengel</keyname><forenames>Anton Van Den</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author></authors><title>Scalable Nuclear-norm Minimization by Subspace Pursuit Proximal
  Riemannian Gradient</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nuclear-norm regularization plays a vital role in many learning tasks, such
as low-rank matrix recovery (MR), and low-rank representation (LRR). Solving
this problem directly can be computationally expensive due to the unknown rank
of variables or large-rank singular value decompositions (SVDs). To address
this, we propose a proximal Riemannian gradient (PRG) scheme which can
efficiently solve trace-norm regularized problems defined on real-algebraic
variety $\mMLr$ of real matrices of rank at most $r$. Based on PRG, we further
present a simple and novel subspace pursuit (SP) paradigm for general
trace-norm regularized problems without the explicit rank constraint $\mMLr$.
The proposed paradigm is very scalable by avoiding large-rank SVDs. Empirical
studies on several tasks, such as matrix completion and LRR based subspace
clustering, demonstrate the superiority of the proposed paradigms over existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02831</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02831</id><created>2015-03-10</created><updated>2015-03-21</updated><authors><author><keyname>Peppas</keyname><forenames>Kostas P.</forenames></author><author><keyname>Mathiopoulos</keyname><forenames>P. Takis</forenames></author></authors><title>Free Space Optical Communication with Spatial Modulation and Coherent
  Detection over H-K Atmospheric Turbulence Channels</title><categories>cs.IT math.IT</categories><doi>10.1109/JLT.2015.2465385</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of optical spatial modulation (OSM), which has been recently emerged
as a power and bandwidth efficient pulsed modulation technique for indoor
optical wireless communication, is proposed as a simple, low-complexity means
of achieving spatial diversity in coherent free space optical (FSO)
communication systems. In doing so, this paper makes several novel
contributions as follows. It presents a generic analytical framework for
obtaining the Average Bit Error Probability (ABEP) of uncoded OSM with coherent
detection in the presence of turbulence-induced fading.
  Although the framework is general enough to accommodate any type of models
based on turbulence scattering, the focus in this paper is the H-K
distribution. Although this distribution represents a very general scattering
model valid over a wide range of atmospheric conditions, it is has not been
considered in the past in conjunction with FSO systems possibly because of its
mathematical complexity. The proposed analytical framework yields exact
performance evaluation results for MIMO systems with two transmit- and an
arbitrary number of receive apertures. In addition, tight upper bounds are
derived for the error probability for OSM systems with an arbitrary number of
transmit apertures as well as for convolutionally encoded signals. The
performance of OSM is compared to that of well established coherent FSO
schemes, employing spatial diversity at the transmitter or the receiver only.
Specifically, it is shown that OSM can offer comparable performance with
conventional coherent FSO schemes while outperforming the latter in terms of
spectral efficiency and hardware complexity. Various numerical performance
evaluation results are also presented and compared with equivalent results
obtained by Monte Carlo simulations which verify the accuracy of the derived
analytical expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02834</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02834</id><created>2015-03-10</created><authors><author><keyname>Dud&#xed;k</keyname><forenames>Miroslav</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>Doubly Robust Policy Evaluation and Optimization</title><categories>stat.ME cs.AI</categories><comments>Published in at http://dx.doi.org/10.1214/14-STS500 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS500</report-no><journal-ref>Statistical Science 2014, Vol. 29, No. 4, 485-511</journal-ref><doi>10.1214/14-STS500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sequential decision making in environments where rewards are only
partially observed, but can be modeled as a function of observed contexts and
the chosen action by the decision maker. This setting, known as contextual
bandits, encompasses a wide variety of applications such as health care,
content recommendation and Internet advertising. A central task is evaluation
of a new policy given historic data consisting of contexts, actions and
received rewards. The key challenge is that the past data typically does not
faithfully represent proportions of actions taken by a new policy. Previous
approaches rely either on models of rewards or models of the past policy. The
former are plagued by a large bias whereas the latter have a large variance. In
this work, we leverage the strengths and overcome the weaknesses of the two
approaches by applying the doubly robust estimation technique to the problems
of policy evaluation and optimization. We prove that this approach yields
accurate value estimates when we have either a good (but not necessarily
consistent) model of rewards or a good (but not necessarily consistent) model
of past policy. Extensive empirical comparison demonstrates that the doubly
robust estimation uniformly improves over existing techniques, achieving both
lower variance in value estimation and better policies. As such, we expect the
doubly robust approach to become common practice in policy evaluation and
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02835</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02835</id><created>2015-03-10</created><authors><author><keyname>Belmonte</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Higashikawa</keyname><forenames>Yuya</forenames></author><author><keyname>Katoh</keyname><forenames>Naoki</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author></authors><title>Polynomial-time approximability of the k-Sink Location problem</title><categories>cs.DS</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dynamic network ${\cal N} = (G,c,\tau,S)$ where $G=(V,E)$ is a graph,
integers $\tau(e)$ and $c(e)$ represent, for each edge $e\in E$, the time
required to traverse edge $e$ and its nonnegative capacity, and the set
$S\subseteq V$ is a set of sources. In the $k$-{\sc Sink Location} problem, one
is given as input a dynamic network ${\cal N}$ where every source $u\in S$ is
given a nonnegative supply value $\sigma(u)$. The task is then to find a set of
sinks $X = \{x_1,\ldots,x_k\}$ in $G$ that minimizes the routing time of all
supply to $X$. Note that, in the case where $G$ is an undirected graph, the
optimal position of the sinks in $X$ needs not be at vertices, and can be
located along edges. Hoppe and Tardos showed that, given an instance of
$k$-{\sc Sink Location} and a set of $k$ vertices $X\subseteq V$, one can find
an optimal routing scheme of all the supply in $G$ to $X$ in polynomial time,
in the case where graph $G$ is directed. Note that when $G$ is directed, this
suffices to obtain polynomial-time solvability of the $k$-{\sc Sink Location}
problem, since any optimal position will be located at vertices of $G$.
However, the computational complexity of the $k$-{\sc Sink Location} problem on
general undirected graphs is still open. In this paper, we show that the
$k$-{\sc Sink Location} problem admits a fully polynomial-time approximation
scheme (FPTAS) for every fixed $k$, and that the problem is $W[1]$-hard when
parameterized by $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02839</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02839</id><created>2015-03-10</created><updated>2016-02-03</updated><authors><author><keyname>A</keyname><forenames>Krishna Chaitanya</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author><author><keyname>Mukherji</keyname><forenames>Utpal</forenames></author></authors><title>Learning Equilibria of a Stochastic Game on Gaussian Interference
  Channels with Incomplete Information</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wireless communication system in which $N$ transmitter-receiver
pairs want to communicate with each other. Each transmitter transmits data at a
certain rate using a power that depends on the channel gain to its receiver. If
a receiver can successfully receive the message, it sends an acknowledgment
(ACK), else it sends a negative ACK (NACK). Each user aims to maximize its
probability of successful transmission. We formulate this problem as a
stochastic game and propose a fully distributed learning algorithm to find a
correlated equilibrium (CE). In addition, we use a no regret algorithm to find
a coarse correlated equilibrium (CCE) for our power allocation game. We also
propose a fully distributed learning algorithm to find a Pareto optimal
solution. In general Pareto points do not guarantee fairness among the users,
therefore we also propose an algorithm to compute a Nash bargaining solution
which is Pareto optimal and provides fairness among users. Finally, under the
same game theoretic setup, we study these equilibria and Pareto points when
each transmitter sends data at multiple rates rather than at a fixed rate. We
compare the sum rate obtained at the CE, CCE, Nash bargaining solution and the
Pareto point and also via some other well known recent algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02840</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02840</id><created>2015-03-10</created><updated>2015-03-11</updated><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM, IMJ</affiliation></author><author><keyname>Lecomte</keyname><forenames>Dominique</forenames><affiliation>IMJ</affiliation></author><author><keyname>Simonnet</keyname><forenames>Pierre</forenames><affiliation>SPE</affiliation></author></authors><title>An Upper Bound on the Complexity of Recognizable Tree Languages</title><categories>cs.FL math.GN math.LO</categories><proxy>Ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The third author noticed in his 1992 PhD Thesis [Sim92] that every regular
tree language of infinite trees is in a class $\Game (D\_n({\bf\Sigma}^0\_2))$
for some natural number $n\geq 1$, where $\Game$ is the game quantifier. We
first give a detailed exposition of this result. Next, using an embedding of
the Wadge hierarchy of non self-dual Borel subsets of the Cantor space
$2^\omega$ into the class ${\bf\Delta}^1\_2$, and the notions of Wadge degree
and Veblen function, we argue that this upper bound on the topological
complexity of regular tree languages is much better than the usual
${\bf\Delta}^1\_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02843</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02843</id><created>2015-03-10</created><authors><author><keyname>Cenedese</keyname><forenames>Angelo</forenames><affiliation>University of Padova Italy</affiliation></author><author><keyname>Michielan</keyname><forenames>Marco</forenames><affiliation>University of Padova Italy</affiliation></author><author><keyname>Tramarin</keyname><forenames>Federico</forenames><affiliation>National Research Council of Italy CNR-IEIIT</affiliation></author><author><keyname>Vitturi</keyname><forenames>Stefano</forenames><affiliation>National Research Council of Italy CNR-IEIIT</affiliation></author></authors><title>An Energy Efficient Ethernet Strategy Based on Traffic Prediction and
  Shaping</title><categories>cs.NI cs.SY</categories><comments>11 pages, 7 figures, journal paper, submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, different communities in computer science, telecommunication and
control systems have devoted a huge effort towards the design of energy
efficient solutions for data transmission and network management. This paper
collocates along this research line and presents a novel energy efficient
strategy conceived for Ethernet networks. The proposed strategy combines the
statistical properties of the network traffic with the opportunities offered by
the IEEE 802.3az amendment to the Ethernet standard, called Energy Efficient
Ethernet (EEE). This strategy exploits the possibility of predicting the
incoming traffic from the analysis of the current data flow, which typically
presents a self-similar behavior. Based on the prediction, Ethernet links can
then be put in a low power consumption state for the intervals of time in which
traffic is expected to be of low intensity. Theoretical bounds are derived that
detail how the performance figures depend on the parameters of the designed
strategy and scale with respect to the traffic load. Furthermore, simulations
results, based on both real and synthetic traffic traces, are presented to
prove the effectiveness of the strategy, which leads to considerable energy
savings at the cost of only a limited bounded delay in data delivery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02852</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02852</id><created>2015-03-10</created><authors><author><keyname>Hwang</keyname><forenames>Kyuyeon</forenames></author><author><keyname>Sung</keyname><forenames>Wonyong</forenames></author></authors><title>Single stream parallelization of generalized LSTM-like RNNs on a GPU</title><categories>cs.NE cs.LG</categories><comments>Accepted by the 40th IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP) 2015</comments><doi>10.1109/ICASSP.2015.7178129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNNs) have shown outstanding performance on
processing sequence data. However, they suffer from long training time, which
demands parallel implementations of the training procedure. Parallelization of
the training algorithms for RNNs are very challenging because internal
recurrent paths form dependencies between two different time frames. In this
paper, we first propose a generalized graph-based RNN structure that covers the
most popular long short-term memory (LSTM) network. Then, we present a
parallelization approach that automatically explores parallelisms of arbitrary
RNNs by analyzing the graph structure. The experimental results show that the
proposed approach shows great speed-up even with a single training stream, and
further accelerates the training when combined with multiple parallel training
streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02859</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02859</id><created>2015-03-10</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Napel</keyname><forenames>Stefan</forenames></author></authors><title>Dimension of the Lisbon voting rules in the EU Council: a challenge and
  new world record</title><categories>math.OC cs.GT</categories><comments>8 pages, 1 figure</comments><msc-class>90C06, 05B40, 91B12, 91A12</msc-class><acm-class>G.1.6; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new voting system of the Council of the European Union cannot be
represented as the intersection of six or fewer weighted games, i.e., its
dimension is at least 7. This sets a new record for real-world voting bodies. A
heuristic combination of different discrete optimization methods yields a
representation as the intersection of 13368 weighted games. Determination of
the exact dimension is posed as a challenge to the community. The system's
Boolean dimension is proven to be 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02868</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02868</id><created>2015-03-10</created><authors><author><keyname>Belovs</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Blais</keyname><forenames>Eric</forenames></author></authors><title>Quantum Algorithm for Monotonicity Testing on the Hypercube</title><categories>quant-ph cs.DS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we develop a bounded-error quantum algorithm that makes $\tilde
O(n^{1/4}\varepsilon^{-1/2})$ queries to a Boolean function $f$, accepts a
monotone function, and rejects a function that is $\varepsilon$-far from being
monotone. This gives a super-quadratic improvement compared to the best known
randomized algorithm for all $\varepsilon = o(1)$. The improvement is cubic
when $\varepsilon = 1/\sqrt{n}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02872</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02872</id><created>2015-03-10</created><authors><author><keyname>Ahn</keyname><forenames>Min-Woo</forenames></author><author><keyname>Jung</keyname><forenames>Woo-Sung</forenames></author></authors><title>Accuracy Test for Link Prediction in terms of Similarity Index: The Case
  of WS and BA Models</title><categories>physics.soc-ph cs.SI physics.data-an</categories><doi>10.1016/j.physa.2015.01.083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction is a technique that uses the topological information in a
given network to infer the missing links in it. Since past research on link
prediction has primarily focused on enhancing performance for given empirical
systems, negligible attention has been devoted to link prediction with regard
to network models. In this paper, we thus apply link prediction to two network
models: The Watts-Strogatz (WS) model and Barab\'asi-Albert (BA) model. We
attempt to gain a better understanding of the relation between accuracy and
each network parameter (mean degree, the number of nodes and the rewiring
probability in the WS model) through network models. Six similarity indices are
used, with precision and area under the ROC curve (AUC) value as the accuracy
metrics. We observe a positive correlation between mean degree and accuracy,
and size independence of the AUC value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02877</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02877</id><created>2015-03-10</created><authors><author><keyname>Huusari</keyname><forenames>Timo</forenames></author><author><keyname>Choi</keyname><forenames>Yang-Seok</forenames></author><author><keyname>Liikkanen</keyname><forenames>Petteri</forenames></author><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Talwar</keyname><forenames>Shilpa</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Wideband Self-Adaptive RF Cancellation Circuit for Full-Duplex Radio:
  Operating Principle and Measurements</title><categories>cs.IT math.IT</categories><comments>7 pages, to be presented in 2015 IEEE 81st Vehicular Technology
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel RF circuit architecture for self-interference
cancellation in inband full-duplex radio transceivers. The developed canceller
is able to provide wideband cancellation with waveform bandwidths in the order
of 100 MHz or beyond and contains also self-adaptive or self-healing features
enabling automatic tracking of time-varying self-interference channel
characteristics. In addition to architecture and operating principle
descriptions, we also provide actual RF measurements at 2.4 GHz ISM band
demonstrating the achievable cancellation levels with different bandwidths and
when operating in different antenna configurations and under low-cost highly
nonlinear power amplifier. In a very challenging example with a 100 MHz
waveform bandwidth, around 41 dB total cancellation is obtained while the
corresponding cancellation figure is close to 60 dB with the more conventional
20 MHz carrier bandwidth. Also, efficient tracking in time-varying reflection
scenarios is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02878</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02878</id><created>2015-03-10</created><authors><author><keyname>De Angelis</keyname><forenames>Alessio</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Mobile Node Localization via Pareto Optimization: Algorithm and
  Fundamental Performance Limitations</title><categories>cs.IT cs.RO math.IT math.OC math.ST stat.TH</categories><comments>IEEE Journal on Selected Areas in Communications (To Appear), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate estimation of the position of network nodes is essential, e.g., in
localization, geographic routing, and vehicular networks. Unfortunately,
typical positioning techniques based on ranging or on velocity and angular
measurements are inherently limited. To overcome the limitations of specific
positioning techniques, the fusion of multiple and heterogeneous sensor
information is an appealing strategy. In this paper, we investigate the
fundamental performance of linear fusion of multiple measurements of the
position of mobile nodes, and propose a new distributed recursive position
estimator. The Cram\'er-Rao lower bounds for the parametric and a-posteriori
cases are investigated. The proposed estimator combines information coming from
ranging, speed, and angular measurements, which is jointly fused by a Pareto
optimization problem where the mean and the variance of the localization error
are simultaneously minimized. A distinguished feature of the method is that it
assumes a very simple dynamical model of the mobility and therefore it is
applicable to a large number of scenarios providing good performance. The main
challenge is the characterization of the statistical information needed to
model the Fisher information matrix and the Pareto optimization problem. The
proposed analysis is validated by Monte Carlo simulations, and the performance
is compared to several Kalman-based filters, commonly employed for localization
and sensor fusion. Simulation results show that the proposed estimator
outperforms the traditional approaches that are based on the extended Kalman
filter when no assumption on the model of motion is used. In such a scenario,
better performance is achieved by the proposed method, but at the price of an
increased computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02880</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02880</id><created>2015-03-10</created><authors><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>On the Approximability of Independent Set Problem on Power Law Graphs</title><categories>cs.DS cs.DM math.CO math.OC</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first nonconstant lower bounds for the approximability of the
Independent Set Problem on the Power Law Graphs. These bounds are of the form
$n^{\epsilon}$ in the case when the power law exponent satisfies $\beta &lt;1$. In
the case when $\beta =1$, the lower bound is of the form $\log (n)^{\epsilon}$.
The embedding technique used in the proof could also be of independent
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02884</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02884</id><created>2015-03-10</created><authors><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Andrieu</keyname><forenames>Vincent</forenames></author><author><keyname>Prieur</keyname><forenames>Christophe</forenames></author></authors><title>Interconnecting a System Having a Single Input-to-State Gain With a
  System Having a Region-Dependent Input-to-State Gain</title><categories>math.OC cs.SY</categories><journal-ref>Proceedings of the 52nd IEEE Conference on Decision and Control
  (CDC'13), Florence, Italy, pp. 624 - 629, 2013</journal-ref><doi>10.1109/CDC.2013.6759951</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an ISS system, by analyzing local and non-local properties, it is
obtained different input-to-state gains. The interconnection of a system having
two input-to-state gains with a system having a single ISS gain is analyzed. By
employing the Small Gain Theorem for the local (resp. non-local) gains
composition, it is concluded about the local (resp. global) stability of the
origin (resp. of a compact set). Additionally, if the region of local stability
of the origin strictly includes the region attraction of the compact set, then
it is shown that the origin is globally asymptotically stable. An example
illustrates the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02892</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02892</id><created>2015-03-10</created><authors><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Andrieu</keyname><forenames>Vincent</forenames></author><author><keyname>Prieur</keyname><forenames>Christophe</forenames></author></authors><title>Combining a backstepping controller with a local stabilizer</title><categories>math.OC cs.SY</categories><journal-ref>Proceedings of the American Control Conference (ACC'11), San
  Francisco, CA, pp. 3197 - 3202, 2011</journal-ref><doi>10.1109/ACC.2011.5991251</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider nonlinear control systems for which there exist some structural
obstacles to the design of classical continuous stabilizing feedback laws. More
precisely, it is studied systems for which the backstepping tool for the design
of stabilizers can not be applied. On the contrary, it leads to feedback laws
such that the origin of the closed-loop system is not globally asymptotically
stable, but a suitable attractor (strictly containing the origin) is
practically asymptotically stable. Then, a design method is suggested to build
a hybrid feedback law combining a backstepping controller with a locally
stabilizing controller. The results are illustrated for a nonlinear system
which, due to the structure of the system, does not have a priori any globally
stabilizing backstepping controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02893</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02893</id><created>2015-03-10</created><authors><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Qu</keyname><forenames>Xiaobo</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Ye</keyname><forenames>Gui-Bo</forenames></author></authors><title>Robust recovery of complex exponential signals from random Gaussian
  projections via low rank Hankel matrix reconstruction</title><categories>cs.IT math.IT math.NA math.OC stat.ML</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores robust recovery of a superposition of $R$ distinct
complex exponential functions from a few random Gaussian projections. We assume
that the signal of interest is of $2N-1$ dimensional and $R&lt;&lt;2N-1$. This
framework covers a large class of signals arising from real applications in
biology, automation, imaging science, etc. To reconstruct such a signal, our
algorithm is to seek a low-rank Hankel matrix of the signal by minimizing its
nuclear norm subject to the consistency on the sampled data. Our theoretical
results show that a robust recovery is possible as long as the number of
projections exceeds $O(R\ln^2N)$. No incoherence or separation condition is
required in our proof. Our method can be applied to spectral compressed sensing
where the signal of interest is a superposition of $R$ complex sinusoids.
Compared to existing results, our result here does not need any separation
condition on the frequencies, while achieving better or comparable bounds on
the number of measurements. Furthermore, our method provides theoretical
guidance on how many samples are required in the state-of-the-art non-uniform
sampling in NMR spectroscopy. The performance of our algorithm is further
demonstrated by numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02903</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02903</id><created>2015-03-10</created><authors><author><keyname>Chang</keyname><forenames>Moonjeong</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Two-step Input Spatial Auditory BCI for Japanese Kana Characters</title><categories>q-bio.NC cs.HC</categories><comments>7 pages, 2 figures, accepted for publication in Advances in Cognitive
  Neurodynamics Volume 5 -- Proceedings of the 5th International Conference on
  Cognitive Neurodynamics (ICCN 2015)</comments><acm-class>H.5.2; H.1.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present an auditory stimulus optimization and a pilot study of a two-step
input speller application combined with a spatial auditory brain-computer
interface (saBCI) for paralyzed users. The application has been developed for
45, out of 48 defining the full set, Japanese kana characters in a two-step
input procedure setting for an easy-to-use BCI-speller interface. The user
first selects the representative letter of a subset, defining the second step.
In the second step, the final choice is made. At each interfacing step, the
choices are classified based on the P300 event related potential (ERP)
responses captured in the EEG, as in the classic oddball paradigm. The BCI
online experiment and EEG responses classification results of the pilot study
confirm the effectiveness of the proposed spelling method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02905</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02905</id><created>2015-03-10</created><authors><author><keyname>Sasaki</keyname><forenames>Tatsuya</forenames></author><author><keyname>Uchida</keyname><forenames>Satoshi</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author></authors><title>Voluntary rewards mediate the evolution of pool punishment for
  maintaining public goods in large populations</title><categories>physics.soc-ph cs.GT nlin.AO q-bio.PE</categories><comments>20 pages, 4 figures, supplementary information (10 pages, 8 figures)</comments><journal-ref>Scientific Reports 5, Article number: 8917 (published 10 March
  2015)</journal-ref><doi>10.1038/srep08917</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Punishment is a popular tool when governing commons in situations where free
riders would otherwise take over. It is well known that sanctioning systems,
such as the police and courts, are costly and thus can suffer from those who
free ride on other's efforts to maintain the sanctioning systems (second-order
free riders). Previous game-theory studies showed that if populations are very
large, pool punishment rarely emerges in public good games, even when
participation is optional, because of second-order free riders. Here we show
that a matching fund for rewarding cooperation leads to the emergence of pool
punishment, despite the presence of second-order free riders. We demonstrate
that reward funds can pave the way for a transition from a population of free
riders to a population of pool punishers. A key factor in promoting the
transition is also to reward those who contribute to pool punishment, yet not
abstaining from participation. Reward funds eventually vanish in raising pool
punishment, which is sustainable by punishing the second-order free riders.
This suggests that considering the interdependence of reward and punishment may
help to better understand the origins and transitions of social norms and
institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02911</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02911</id><created>2015-03-10</created><authors><author><keyname>Acosta</keyname><forenames>Maribel</forenames></author><author><keyname>Simperl</keyname><forenames>Elena</forenames></author><author><keyname>Fl&#xf6;ck</keyname><forenames>Fabian</forenames></author><author><keyname>Vidal</keyname><forenames>Maria-Esther</forenames></author><author><keyname>Studer</keyname><forenames>Rudi</forenames></author></authors><title>RDF-Hunter: Automatically Crowdsourcing the Execution of Queries Against
  RDF Data Sets</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years, a large number of RDF data sets has become available on
the Web. However, due to the semi-structured nature of RDF data, missing values
affect answer completeness of queries that are posed against this data. To
overcome this limitation, we propose RDF-Hunter, a novel hybrid query
processing approach that brings together machine and human computation to
execute queries against RDF data. We develop a novel quality model and query
engine in order to enable RDF-Hunter to on the fly decide which parts of a
query should be executed through conventional technology or crowd computing. To
evaluate RDF-Hunter, we created a collection of 50 SPARQL queries against the
DBpedia data set, executed them using our hybrid query engine, and analyzed the
accuracy of the outcomes obtained from the crowd. The experiments clearly show
that the overall approach is feasible and produces query results that reliably
and significantly enhance completeness of automatic query processing responses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02917</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02917</id><created>2015-03-10</created><authors><author><keyname>Weis</keyname><forenames>Karl-Heinz</forenames></author></authors><title>A Case Based Reasoning Approach for Answer Reranking in Question
  Answering</title><categories>cs.AI</categories><comments>in Proceedings Informatik 2013, Koblenz, Germany, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document I present an approach to answer validation and reranking for
question answering (QA) systems. A cased-based reasoning (CBR) system judges
answer candidates for questions from annotated answer candidates for earlier
questions. The promise of this approach is that user feedback will result in
improved answers of the QA system, due to the growing case base. In the paper,
I present the adequate structuring of the case base and the appropriate
selection of relevant similarity measures, in order to solve the answer
validation problem. The structural case base is built from annotated MultiNet
graphs, which provide representations for natural language expressions, and
corresponding graph similarity measures. I cover a priori relations to
experienced answer candidates for former questions. I compare the CBR System
results to current approaches in an experiment integrating CBR into an existing
framework for answer validation and reranking. This integration is achieved by
adding CBR-related features to the input of a learned ranking model that
determines the final answer ranking. In the experiments based on QA@CLEF
questions, the best learned models make heavy use of CBR features. Observing
the results with a continually growing case base, I present a positive effect
of the size of the case base on the accuracy of the CBR subsystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02920</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02920</id><created>2015-03-10</created><authors><author><keyname>Chen</keyname><forenames>Jianer</forenames></author><author><keyname>Xu</keyname><forenames>Chao</forenames></author></authors><title>Dealing With 4-Variables by Resolution: An Improved MaxSAT Algorithm</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study techniques for solving the Maximum Satisfiability problem (MaxSAT).
Our focus is on variables of degree 4. We identify cases for degree-4 variables
and show how the resolution principle and the kernelization techniques can be
nicely integrated to achieve more efficient algorithms for the MaxSAT problem.
As a result, we present an algorithm of time $O^*(1.3248^k)$ for the MaxSAT
problem, improving the previous best upper bound $O^*(1.358^k)$ by Ivan
Bliznets and Alexander.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02927</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02927</id><created>2015-03-10</created><authors><author><keyname>Song</keyname><forenames>Lin</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>Broadcasting Correlated Vector Gaussians</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of sending two correlated vector Gaussian sources over a
bandwidth-matched two-user scalar Gaussian broadcast channel is studied in this
work, where each receiver wishes to reconstruct its target source under a
covariance distortion constraint. We derive a lower bound on the optimal
tradeoff between the transmit power and the achievable reconstruction
distortion pair. Our derivation is based on a new bounding technique which
involves the introduction of appropriate remote sources. Furthermore, it is
shown that this lower bound is achievable by a class of hybrid schemes for the
special case where the weak receiver wishes to reconstruct a scalar source
under the mean squared error distortion constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02935</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02935</id><created>2015-03-10</created><authors><author><keyname>Aranovskiy</keyname><forenames>Stanislav</forenames></author><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author><author><keyname>Cisneros</keyname><forenames>Rafael</forenames></author></authors><title>Robust PI Passivity-based Control of Nonlinear Systems: Application to
  Port-Hamiltonian Systems and Temperature Regulation</title><categories>cs.SY</categories><comments>An abridged version will appear in American Control Conference,
  ACC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of control of partially known nonlinear
systems, which have an open-loop stable equilibrium, but we would like to add a
PI controller to regulate its behavior around another operating point. Our main
contribution is the identification of a class of systems for which a globally
stable PI can be designed knowing only the systems input matrix and measuring
only the actuated coordinates. The construction of the PI is done invoking
passivity theory. The difficulties encountered in the design of adaptive PI
controllers with the existing theoretical tools are also discussed. As an
illustration of the theory, we consider port--Hamiltonian systems and a class
of thermal processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02940</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02940</id><created>2015-03-10</created><authors><author><keyname>Montoya</keyname><forenames>Gabriela</forenames></author><author><keyname>Skaf-Molli</keyname><forenames>Hala</forenames></author><author><keyname>Molli</keyname><forenames>Pascal</forenames></author><author><keyname>Vidal</keyname><forenames>Maria-Esther</forenames></author></authors><title>Efficient Query Processing for SPARQL Federations with Replicated
  Fragments</title><categories>cs.DB</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low reliability and availability of public SPARQL endpoints prevent
real-world applications from exploiting all the potential of these querying
infras-tructures. Fragmenting data on servers can improve data availability but
degrades performance. Replicating fragments can offer new tradeoff between
performance and availability. We propose FEDRA, a framework for querying Linked
Data that takes advantage of client-side data replication, and performs a
source selection algorithm that aims to reduce the number of selected public
SPARQL endpoints, execution time, and intermediate results. FEDRA has been
implemented on the state-of-the-art query engines ANAPSID and FedX, and
empirically evaluated on a variety of real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02945</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02945</id><created>2015-03-10</created><updated>2015-11-19</updated><authors><author><keyname>Zhan</keyname><forenames>Zhifang</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Guo</keyname><forenames>Di</forenames></author><author><keyname>Liu</keyname><forenames>Yunsong</forenames></author><author><keyname>Chen</keyname><forenames>Zhong</forenames></author><author><keyname>Qu</keyname><forenames>Xiaobo</forenames></author></authors><title>Fast Multi-class Dictionaries Learning with Geometrical Directions in
  MRI Reconstruction</title><categories>cs.CV math.OC physics.med-ph</categories><comments>13 pages, 15 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Improve the reconstructed image with fast and multi-class
dictionaries learning when magnetic resonance imaging is accelerated by
undersampling the k-space data. Methods: A fast orthogonal dictionary learning
method is introduced into magnetic resonance image reconstruction to providing
adaptive sparse representation of images. To enhance the sparsity, image is
divided into classified patches according to the same geometrical direction and
dictionary is trained within each class. A new sparse reconstruction model with
the multi-class dictionaries is proposed and solved using a fast alternating
direction method of multipliers. Results: Experiments on phantom and brain
imaging data with acceleration factor up to 10 and various undersampling
patterns are conducted. The proposed method is compared with state-of-the-art
magnetic resonance image reconstruction methods. Conclusion: Artifacts are
better suppressed and image edges are better preserved than the compared
methods. Besides, the computation of the proposed approach is much faster than
the typical K-SVD dictionary learning method in magnetic resonance image
reconstruction. Significance: The proposed method can be exploited in
undersapmled magnetic resonance imaging to reduce data acquisition time and
reconstruct images with better image quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02946</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02946</id><created>2015-03-10</created><updated>2015-03-15</updated><authors><author><keyname>Diehl</keyname><forenames>Frederik</forenames></author><author><keyname>Jauch</keyname><forenames>Andreas</forenames></author></authors><title>apsis - Framework for Automated Optimization of Machine Learning Hyper
  Parameters</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The apsis toolkit presented in this paper provides a flexible framework for
hyperparameter optimization and includes both random search and a bayesian
optimizer. It is implemented in Python and its architecture features
adaptability to any desired machine learning code. It can easily be used with
common Python ML frameworks such as scikit-learn. Published under the MIT
License other researchers are heavily encouraged to check out the code,
contribute or raise any suggestions. The code can be found at
github.com/FrederikDiehl/apsis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02948</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02948</id><created>2015-03-10</created><updated>2015-04-17</updated><authors><author><keyname>Bromberger</keyname><forenames>Martin</forenames></author><author><keyname>Sturm</keyname><forenames>Thomas</forenames></author><author><keyname>Weidenbach</keyname><forenames>Christoph</forenames></author></authors><title>Linear Integer Arithmetic Revisited</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider feasibility of linear integer programs in the context of
verification systems such as SMT solvers or theorem provers. Although
satisfiability of linear integer programs is decidable, many state-of-the-art
solvers neglect termination in favor of efficiency. It is challenging to design
a solver that is both terminating and practically efficient. Recent work by
Jovanovic and de Moura constitutes an important step into this direction. Their
algorithm CUTSAT is sound, but does not terminate, in general. In this paper we
extend their CUTSAT algorithm by refined inference rules, a new type of
conflicting core, and a dedicated rule application strategy. This leads to our
algorithm CUTSAT++, which guarantees termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02951</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02951</id><created>2015-03-10</created><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Xia</keyname><forenames>Bainan</forenames></author><author><keyname>Geng</keyname><forenames>Xinbo</forenames></author><author><keyname>Ming</keyname><forenames>Hao</forenames></author><author><keyname>Shakkottai</keyname><forenames>Srinivas</forenames></author><author><keyname>Subramanian</keyname><forenames>Vijay</forenames></author><author><keyname>Xie</keyname><forenames>Le</forenames></author></authors><title>Energy Coupon: A Mean Field Game Perspective on Demand Response in Smart
  Grids</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of a Load Serving Entity (LSE) trying to reduce its
exposure to electricity market volatility by incentivizing demand response in a
Smart Grid setting. We focus on the day-ahead electricity market, wherein the
LSE has a good estimate of the statistics of the wholesale price of electricity
at different hours in the next day, and wishes its customers to move a part of
their power consumption to times of low mean and variance in price. Based on
the time of usage, the LSE awards a differential number of &quot;Energy Coupons&quot; to
each customer in proportion to the customer's electricity usage at that time. A
lottery is held periodically in which the coupons held by all the customers are
used as lottery tickets.
  Our study takes the form of a Mean Field Game, wherein each customer models
the number of coupons that each of its opponents possesses via a distribution,
and plays a best response pattern of electricity usage by trading off the
utility of winning at the lottery versus the discomfort suffered by changing
its usage pattern. The system is at a Mean Field Equilibrium (MFE) if the
number of coupons that the customer receives is itself a sample drawn from the
assumed distribution. We show the existence of an MFE, and characterize the
mean field customer policy as having a multiple-threshold structure in which
customers who have won too frequently or infrequently have low incentives to
participate. We then numerically study the system with a candidate application
of air conditioning during the summer months in the state of Texas. Besides
verifying our analytical results, we show that the LSE can potentially attain
quite substantial savings using our scheme. Our techniques can also be applied
to resource sharing problems in other \emph{societal} networks such as
transportation or communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02955</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02955</id><created>2015-03-10</created><updated>2016-03-03</updated><authors><author><keyname>Miller</keyname><forenames>Konstantin</forenames></author><author><keyname>Al-Tamimi</keyname><forenames>Abdel-Karim</forenames></author><author><keyname>Wolisz</keyname><forenames>Adam</forenames></author></authors><title>Low-Delay Adaptive Video Streaming Based on Short-Term TCP Throughput
  Prediction</title><categories>cs.NI cs.MM</categories><comments>Technical Report TKN-15-001, Telecommunication Networks Group,
  Technische Universitaet Berlin. Updated by TR TKN-16-001, available at
  http://arxiv.org/abs/1603.00859</comments><report-no>TKN-15-001</report-no><acm-class>C.2.1; C.2.4; C.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, HTTP-Based Adaptive Streaming has become the de facto standard for
video streaming over the Internet. It allows the client to adapt media
characteristics to varying network conditions in order to maximize Quality of
Experience (QoE). In the case of live streaming this task becomes particularly
challenging. An important factor than might help improving performance is the
capability to correctly predict network throughput dynamics on short to medium
timescales. It becomes notably difficult in wireless networks that are often
subject to continuous throughput fluctuations.
  In the present work, we develop an adaptation algorithm for HTTP-Based
Adaptive Live Streaming that, for each adaptation decision, maximizes a
QoE-based utility function depending on the probability of playback
interruptions, average video quality, and the amount of video quality
fluctuations. To compute the utility function the algorithm leverages
throughput predictions, and dynamically estimated prediction accuracy.
  We are trying to close the gap created by the lack of studies analyzing TCP
throughput on short to medium timescales. We study several time series
prediction methods and their error distributions. We observe that Simple Moving
Average performs best in most cases. We also observe that the relative
underestimation error is best represented by a truncated normal distribution,
while the relative overestimation error is best represented by a Lomax
distribution. Moreover, underestimations and overestimations exhibit a temporal
correlation that we use to further improve prediction accuracy.
  We compare the proposed algorithm with a baseline approach that uses a fixed
margin between past throughput and selected media bit rate, and an oracle-based
approach that has perfect knowledge over future throughput for a certain time
horizon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02970</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02970</id><created>2015-03-10</created><authors><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author><author><keyname>Pilz</keyname><forenames>Alexander</forenames></author></authors><title>Ham-Sandwich Cuts for Abstract Order Types</title><categories>cs.CG</categories><comments>A preliminary version of this paper appeared in the proceedings of
  ISAAC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear-time ham-sandwich cut algorithm of Lo, Matou\v{s}ek, and Steiger
for bi-chromatic finite point sets in the plane works by appropriately
selecting crossings of the lines in the dual line arrangement with a set of
well-chosen vertical lines. We consider the setting where we are not given the
coordinates of the point set, but only the orientation of each point triple
(the order type) and give a deterministic linear-time algorithm for the
mentioned sub-algorithm. This yields a linear-time ham-sandwich cut algorithm
even in our restricted setting. We also show that our methods are applicable to
abstract order types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02971</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02971</id><created>2015-03-10</created><updated>2015-05-21</updated><authors><author><keyname>Teucke</keyname><forenames>Andreas</forenames></author><author><keyname>Weidenbach</keyname><forenames>Christoph</forenames></author></authors><title>First-Order Logic Theorem Proving and Model Building via Approximation
  and Instantiation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider first-order logic theorem proving and model
building via approximation and instantiation. Given a clause set we propose its
approximation into a simplified clause set where satisfiability is decidable.
The approximation extends the signature and preserves unsatisfiability: if the
simplified clause set is satisfiable in some model, so is the original clause
set in the same model interpreted in the original signature. A refutation
generated by a decision procedure on the simplified clause set can then either
be lifted to a refutation in the original clause set, or it guides a refinement
excluding the previously found unliftable refutation. This way the approach is
refutationally complete. We do not step-wise lift refutations but conflicting
cores, finite unsatisfiable clause sets representing at least one refutation.
The approach is dual to many existing approaches in the literature because our
approximation preserves unsatisfiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02974</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02974</id><created>2015-03-10</created><authors><author><keyname>Wade</keyname><forenames>Matthew J.</forenames></author><author><keyname>Curtis</keyname><forenames>Thomas P.</forenames></author><author><keyname>Davenport</keyname><forenames>Russell J.</forenames></author></authors><title>Modelling Computational Resources for Next Generation Sequencing
  Bioinformatics Analysis of 16S rRNA Samples</title><categories>q-bio.GN cs.CE cs.PF</categories><comments>23 pages, 8 figures</comments><acm-class>D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the rapidly evolving domain of next generation sequencing and
bioinformatics analysis, data generation is one aspect that is increasing at a
concomitant rate. The burden associated with processing large amounts of
sequencing data has emphasised the need to allocate sufficient computing
resources to complete analyses in the shortest possible time with manageable
and predictable costs. A novel method for predicting time to completion for a
popular bioinformatics software (QIIME), was developed using key variables
characteristic of the input data assumed to impact processing time. Multiple
Linear Regression models were developed to determine run time for two denoising
algorithms and a general bioinformatics pipeline. The models were able to
accurately predict clock time for denoising sequences from a naturally
assembled community dataset, but not an artificial community. Speedup and
efficiency tests for AmpliconNoise also highlighted that caution was needed
when allocating resources for parallel processing of data. Accurate modelling
of computational processing time using easily measurable predictors can assist
NGS analysts in determining resource requirements for bioinformatics software
and pipelines. Whilst demonstrated on a specific group of scripts, the
methodology can be extended to encompass other packages running on multiple
architectures, either in parallel or sequentially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02985</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02985</id><created>2015-03-10</created><authors><author><keyname>Gao</keyname><forenames>Peng</forenames></author><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev</forenames></author><author><keyname>Thomas</keyname><forenames>Kurt</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author></authors><title>SybilFrame: A Defense-in-Depth Framework for Structure-Based Sybil
  Detection</title><categories>cs.SI cs.CR</categories><comments>17 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sybil attacks are becoming increasingly widespread, and pose a significant
threat to online social systems; a single adversary can inject multiple
colluding identities in the system to compromise security and privacy. Recent
works have leveraged the use of social network-based trust relationships to
defend against Sybil attacks. However, existing defenses are based on
oversimplified assumptions, which do not hold in real world social graphs. In
this work, we propose SybilFrame, a defense-in-depth framework for mitigating
the problem of Sybil attacks when the oversimplified assumptions are relaxed.
Our framework is able to incorporate prior information about users and edges in
the social graph. We validate our framework on synthetic and real world network
topologies, including a large-scale Twitter dataset with 20M nodes and 265M
edges, and demonstrate that our scheme performs an order of magnitude better
than previous structure-based approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02986</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02986</id><created>2015-03-10</created><updated>2015-05-11</updated><authors><author><keyname>Mhaske</keyname><forenames>Swapnil</forenames></author><author><keyname>Kee</keyname><forenames>Hojin</forenames></author><author><keyname>Ly</keyname><forenames>Tai</forenames></author><author><keyname>Aziz</keyname><forenames>Ahsan</forenames></author><author><keyname>Spasojevic</keyname><forenames>Predrag</forenames></author></authors><title>Strategies for High-Throughput FPGA-based QC-LDPC Decoder Architecture</title><categories>cs.AR cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose without loss of generality strategies to achieve a high-throughput
FPGA-based architecture for a QC-LDPC code based on a circulant-1 identity
matrix construction. We present a novel representation of the parity-check
matrix (PCM) providing a multi-fold throughput gain. Splitting of the node
processing algorithm enables us to achieve pipelining of blocks and hence
layers. By partitioning the PCM into not only layers but superlayers we derive
an upper bound on the pipelining depth for the compact representation. To
validate the architecture, a decoder for the IEEE 802.11n (2012) QC-LDPC is
implemented on the Xilinx Kintex-7 FPGA with the help of the FPGA IP compiler
[2] available in the NI LabVIEW Communication System Design Suite (CSDS) which
offers an automated and systematic compilation flow where an optimized hardware
implementation from the LDPC algorithm was generated in approximately 3
minutes, achieving an overall throughput of 608Mb/s (at 260MHz). As per our
knowledge this is the fastest implementation of the IEEE 802.11n QC-LDPC
decoder using an algorithmic compiler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02994</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02994</id><created>2015-03-10</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Quantum Structure in Cognition, Origins, Developments, Successes and
  Expectations</title><categories>cs.AI quant-ph</categories><comments>25 pages. arXiv admin note: text overlap with arXiv:1412.8704</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an overview of the results we have attained in the last decade on
the identification of quantum structures in cognition and, more specifically,
in the formalization and representation of natural concepts. We firstly discuss
the quantum foundational reasons that led us to investigate the mechanisms of
formation and combination of concepts in human reasoning, starting from the
empirically observed deviations from classical logical and probabilistic
structures. We then develop our quantum-theoretic perspective in Fock space
which allows successful modeling of various sets of cognitive experiments
collected by different scientists, including ourselves. In addition, we
formulate a unified explanatory hypothesis for the presence of quantum
structures in cognitive processes, and discuss our recent discovery of further
quantum aspects in concept combinations, namely, 'entanglement' and
'indistinguishability'. We finally illustrate perspectives for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.02997</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.02997</id><created>2015-03-10</created><authors><author><keyname>Newman</keyname><forenames>No'am</forenames></author></authors><title>Spreadsheets in an ERP environment: not what the doctor ordered</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Modern ERP systems contain flexible report generators but the tendency exists
for users to export data to spreadsheets for manipulation, reporting and
decision making. A purported reason for this is that some users are more
familiar with personal reporting tools (spreadsheets) as opposed to enterprise
reporting tools. The author's doctoral research intends to measure the extent
of spreadsheet usage in ERP environments and to determine which factors
facilitate this.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03004</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03004</id><created>2015-03-10</created><updated>2015-03-25</updated><authors><author><keyname>Ros</keyname><forenames>German</forenames></author><author><keyname>Guerrero</keyname><forenames>Julio</forenames></author></authors><title>Fast and Robust Fixed-Rank Matrix Recovery</title><categories>cs.CV cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of efficient sparse fixed-rank (S-FR) matrix
decomposition, i.e., splitting a corrupted matrix $M$ into an uncorrupted
matrix $L$ of rank $r$ and a sparse matrix of outliers $S$. Fixed-rank
constraints are usually imposed by the physical restrictions of the system
under study. Here we propose a method to perform accurate and very efficient
S-FR decomposition that is more suitable for large-scale problems than existing
approaches. Our method is a grateful combination of geometrical and algebraical
techniques, which avoids the bottleneck caused by the Truncated SVD (TSVD).
Instead, a polar factorization is used to exploit the manifold structure of
fixed-rank problems as the product of two Stiefel and an SPD manifold, leading
to a better convergence and stability. Then, closed-form projectors help to
speed up each iteration of the method. We introduce a novel and fast projector
for the $\text{SPD}$ manifold and a proof of its validity. Further acceleration
is achieved using a Nystrom scheme. Extensive experiments with synthetic and
real data in the context of robust photometric stereo and spectral clustering
show that our proposals outperform the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03008</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03008</id><created>2015-03-10</created><authors><author><keyname>Hamrit</keyname><forenames>Nidhal</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>Reversibility in the Extended Measurement-based Quantum Computation</title><categories>quant-ph cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When applied on some particular quantum entangled states, measurements are
universal for quantum computing. In particular, despite the fondamental
probabilistic evolution of quantum measurements, any unitary evolution can be
simulated by a measurement-based quantum computer (MBQC). We consider the
extended version of the MBQC where each measurement can occur not only in the
(X,Y)-plane of the Bloch sphere but also in the (X,Z)- and (Y,Z)-planes. The
existence of a gflow in the underlying graph of the computation is a necessary
and sufficient condition for a certain kind of determinism. We extend the
focused gflow (a gflow in a particular normal form) defined for the (X,Y)-plane
to the extended case, and we provide necessary and sufficient conditions for
the existence of such normal forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03009</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03009</id><created>2015-03-10</created><updated>2015-04-27</updated><authors><author><keyname>Bhagoji</keyname><forenames>Arjun</forenames></author><author><keyname>Sarvepalli</keyname><forenames>Pradeep</forenames></author></authors><title>Equivalence of 2D color codes (without translational symmetry) to
  surface codes</title><categories>quant-ph cs.IT math.IT</categories><comments>Title slightly changed. Revised introduction, with minor corrections
  and additional references to related work. Expanded version of the manuscript
  submitted to ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent work, Bombin, Duclos-Cianci, and Poulin showed that every local
translationally invariant 2D topological stabilizer code is locally equivalent
to a finite number of copies of Kitaev's toric code. For 2D color codes,
Delfosse relaxed the constraint on translation invariance and mapped a 2D color
code onto three surface codes. In this paper, we propose an alternate map based
on linear algebra. We show that any 2D color code can be mapped onto exactly
two copies of a related surface code. The surface code in our map is induced by
the color code and easily derived from the color code. Furthermore, our map
does not require any ancilla qubits for the surface codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03011</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03011</id><created>2015-03-10</created><authors><author><keyname>Patro</keyname><forenames>S. Gopal Krishna</forenames></author><author><keyname>Sahoo</keyname><forenames>Pragyan Parimita</forenames></author><author><keyname>Panda</keyname><forenames>Ipsita</forenames></author><author><keyname>Sahu</keyname><forenames>Kishore Kumar</forenames></author></authors><title>Technical Analysis on Financial Forecasting</title><categories>cs.NE</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial forecasting is an estimation of future financial outcomes for a
company, industry, country using historical internal accounting and sales data.
We may predict the future outcome of BSE_SENSEX practically by some soft
computing techniques and can also optimized using PSO (Particle Swarm
Optimization), EA (Evolutionary Algorithm) or DEA (Differential Evolutionary
Algorithm) etc. PSO is a biologically inspired computational search &amp;
optimization method developed in 1995 by Dr. Eberhart and Dr. Kennedy based on
the social behaviors of fish schooling or birds flocking. PSO is a promising
method to train Artificial Neural Network (ANN). It is easy to implement then
Genetic Algorithm except few parameters are adjusted. PSO is a random &amp; pattern
search technique based on populating of particle. In PSO, the particles are
having some position and velocity in the search space. Two terms are used in
PSO one is Local Best and another one is Global Best. To optimize problems that
are like Irregular, Noisy, Change over time, Static etc. PSO uses a classic
optimization method such as Gradient Decent &amp; Quasi-Newton Methods. The
observation and review of few related studies in the last few years, focusing
on function of PSO, modification of PSO and operation that have implemented
using PSO like function optimization, ANN Training &amp; Fuzzy Control etc.
Differential Evolution is an efficient EA technique for optimization of
numerical problems, financial problems etc. PSO technique is introduced due to
the swarming behavior of animals which is the collective behavior of similar
size that aggregates together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03012</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03012</id><created>2015-03-10</created><authors><author><keyname>Mayne</keyname><forenames>Richard</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Jones</keyname><forenames>Jeff</forenames></author></authors><title>On the role of the plasmodial cytoskeleton in facilitating intelligent
  behaviour in slime mould Physarum polycephalum</title><categories>cs.ET</categories><comments>25 pages, 10 figures</comments><journal-ref>Communicative &amp; Integrative Biology 7 (1), e32097, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The plasmodium of slime mould Physarum polycephalum behaves as an amorphous
reaction-diffusion computing substrate and is capable of apparently intelligent
behaviour. But how does intelligence emerge in an acellular organism? Through a
range of laboratory experiments, we visualise the plasmodial cytoskeleton, a
ubiquitous cellular protein scaffold whose functions are manifold and essential
to life, and discuss its putative role as a network for transducing,
transmitting and structuring data streams within the plasmodium. Through a
range of computer modelling techniques, we demonstrate how emergent behaviour,
and hence computational intelligence, may occur in cytoskeletal communications
networks. Specifically, we model the topology of both the actin and tubulin
cytoskeletal networks and discuss how computation may occur therein.
Furthermore, we present bespoke cellular automata and particle swarm models for
the computational process within the cytoskeleton and observe the incidence of
emergent patterns in both. Our work grants unique insight into the origins of
natural intelligence; the results presented here are therefore readily
transferable to the fields of natural computation, cell biology and biomedical
science. We conclude by discussing how our results may alter our biological,
computational and philosophical understanding of intelligence and
consciousness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03013</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03013</id><created>2015-03-10</created><updated>2015-03-10</updated><authors><author><keyname>Chung</keyname><forenames>MinKeun</forenames></author><author><keyname>Sim</keyname><forenames>Min Soo</forenames></author><author><keyname>Kim</keyname><forenames>Jaeweon</forenames></author><author><keyname>Kim</keyname><forenames>Dong Ku</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Prototyping Real-Time Full Duplex Radios</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present a real-time full duplex radio system for 5G
wireless networks. Full duplex radios are capable of opening new possibilities
in contexts of high traffic demand where there are limited radio resources. A
critical issue, however, to implementing full duplex radios, in real wireless
environments, is being able to cancel self-interference. To overcome the
self-interference challenge, we prototype our design on a software-defined
radio (SDR) platform. This design combines a dual-polarization antenna-based
analog part with a digital self-interference canceller that operates in
real-time. Prototype test results confirm that the proposed full-duplex system
achieves about 1.9 times higher throughput than a half-duplex system. This
article concludes with a discussion of implementationchallenges that remain for
researchers seeking the most viable solution for full duplex communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03016</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03016</id><created>2015-03-10</created><updated>2015-06-04</updated><authors><author><keyname>Boxer</keyname><forenames>Laurence</forenames></author><author><keyname>Staecker</keyname><forenames>P. Christopher</forenames></author></authors><title>Remarks on pointed digital homotopy</title><categories>math.CO cs.CV math.GN</categories><comments>major new section, some errors corrected</comments><msc-class>55P10, 68R10</msc-class><acm-class>I.4.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and explore in detail a pair of digital images with
$c_u$-adjacencies that are homotopic but not pointed homotopic. For two digital
loops $f,g: [0,m]_Z \rightarrow X$ with the same basepoint, we introduce the
notion of {\em tight at the basepoint (TAB)} pointed homotopy, which is more
restrictive than ordinary pointed homotopy and yields some different results.
  We present a variant form of the digital fundamental group. Based on what we
call {\em eventually constant} loops, this version of the fundamental group is
equivalent to that of Boxer (1999), but offers the advantage that eventually
constant maps are often easier to work with than the trivial extensions that
are key to the development of the fundamental group in Boxer (1999) and many
subsequent papers.
  We show that homotopy equivalent digital images have isomorphic fundamental
groups, even when the homotopy equivalence does not preserve the basepoint.
This assertion appeared in Boxer (2005), but there was an error in the proof;
here, we correct the error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03021</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03021</id><created>2015-03-10</created><authors><author><keyname>Salnikov</keyname><forenames>Vsevolod</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>OpenStreetCab: Exploiting Taxi Mobility Patterns in New York City to
  Reduce Commuter Costs</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>in NetMob 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of Uber as the global alternative taxi operator has attracted a lot
of interest recently. Aside from the media headlines which discuss the new
phenomenon, e.g. on how it has disrupted the traditional transportation
industry, policy makers, economists, citizens and scientists have engaged in a
discussion that is centred around the means to integrate the new generation of
the sharing economy services in urban ecosystems. In this work, we aim to shed
new light on the discussion, by taking advantage of a publicly available
longitudinal dataset that describes the mobility of yellow taxis in New York
City. In addition to movement, this data contains information on the fares paid
by the taxi customers for each trip. As a result we are given the opportunity
to provide a first head to head comparison between the iconic yellow taxi and
its modern competitor, Uber, in one of the world's largest metropolitan
centres. We identify situations when Uber X, the cheapest version of the Uber
taxi service, tends to be more expensive than yellow taxis for the same
journey. We also demonstrate how Uber's economic model effectively takes
advantage of well known patterns in human movement. Finally, we take our
analysis a step further by proposing a new mobile application that compares
taxi prices in the city to facilitate traveller's taxi choices, hoping to
ultimately to lead to a reduction of commuter costs. Our study provides a case
on how big datasets that become public can improve urban services for consumers
by offering the opportunity for transparency in economic sectors that lack up
to date regulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03022</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03022</id><created>2015-03-06</created><authors><author><keyname>Quiero</keyname><forenames>Felipe</forenames></author><author><keyname>Quintana</keyname><forenames>Fabian</forenames></author><author><keyname>Bennun</keyname><forenames>Leonardo</forenames></author></authors><title>A novel method based on cross correlation maximization, for pattern
  matching by means of a single parameter. Application to the human voice</title><categories>stat.AP cs.SD</categories><comments>13 pages, 11 figures</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work develops a cross correlation maximization technique, based on
statistical concepts, for pattern matching purposes in time series. The
technique analytically quantifies the extent of similitude between a known
signal within a group of data, by means of a single parameter. Specifically,
the method was applied to voice recognition problem, by selecting samples from
a given individual recordings of the 5 vowels, in Spanish. The frequency of
acquisition of the data was 11.250 Hz. A certain distinctive interval was
established from each vowel time series as a representative test function and
it was compared both to itself and to the rest of the vowels by means of an
algorithm, for a subsequent graphic illustration of the results.
  We conclude that for a minimum distinctive length, the method meets
resemblance between every vowel with itself, and also an irrefutable difference
with the rest of the vowels for an estimate length of 30 points (~2 10-3 s).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03047</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03047</id><created>2015-03-10</created><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>Stability Analysis of Large-Scale Distributed Networked Control Systems
  with Random Communication Delays: A Switched System Approach</title><categories>cs.SY math.DS</categories><doi>10.1016/j.sysconle.2015.08.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the stability analysis of large-scale distributed
networked control systems with random communication delays between linearly
interconnected subsystems. The stability analysis is performed in the Markov
jump linear system framework. There have been considerable researches on
stability analysis of Markov jump systems, however, these methods are not
applicable to large-scale systems because large numbers of subsystems result in
an extremely large number of the switching modes. To avoid this scalability
issue, we propose a new reduced mode model for stability analysis, which is
computationally efficient. We also consider the case in which the transition
probabilities for the Markov jump process contain uncertainties. We provide a
new method that estimates bounds for uncertain Markov transition probability
matrix to guarantee the system stability. The efficiency and the usefulness of
the proposed methods are verified through examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03049</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03049</id><created>2015-03-10</created><updated>2015-09-28</updated><authors><author><keyname>Datta</keyname><forenames>Mrinmoy</forenames></author><author><keyname>Ghorpade</keyname><forenames>Sudhir R.</forenames></author></authors><title>On a conjecture of Tsfasman and an inequality of Serre for the number of
  points on hypersurfaces over finite fields</title><categories>math.AG cs.IT math.IT</categories><comments>Revised version; 9 pages; to appear in Moscow Math J</comments><msc-class>Primary 14G15, 11G25, 14G05, Secondary 11T27, 94B27, 51E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a short proof of an inequality, conjectured by Tsfasman and proved by
Serre, for the maximum number of points on hypersurfaces over finite fields.
Further, we consider a conjectural extension, due to Tsfasman and Boguslavsky,
of this inequality to an explicit formula for the maximum number of common
solutions of a system of linearly independent multivariate homogeneous
polynomials of the same degree with coefficients in a finite field. This
conjecture is shown to be false, in general, but is also shown to hold in the
affirmative in a special case. Applications to generalized Hamming weights of
projective Reed-Muller codes are outlined and a comparison with an older
conjecture of Lachaud and a recent result of Couvreur is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03061</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03061</id><created>2015-03-10</created><authors><author><keyname>Borassi</keyname><forenames>Michele</forenames></author><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author></authors><title>Hyperbolicity Measures &quot;Democracy&quot; in Real-World Networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 92, 032812 (2015)</journal-ref><doi>10.1103/PhysRevE.92.032812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the hyperbolicity of real-world networks, a geometric quantity
that measures if a space is negatively curved. In our interpretation, a network
with small hyperbolicity is &quot;aristocratic&quot;, because it contains a small set of
vertices involved in many shortest paths, so that few elements &quot;connect&quot; the
systems, while a network with large hyperbolicity has a more &quot;democratic&quot;
structure with a larger number of crucial elements.
  We prove mathematically the soundness of this interpretation, and we derive
its consequences by analyzing a large dataset of real-world networks. We
confirm and improve previous results on hyperbolicity, and we analyze them in
the light of our interpretation.
  Moreover, we study (for the first time in our knowledge) the hyperbolicity of
the neighborhood of a given vertex. This allows to define an &quot;influence area&quot;
for the vertices in the graph. We show that the influence area of the highest
degree vertex is small in what we define &quot;local&quot; networks, like most social or
peer-to-peer networks. On the other hand, if the network is built in order to
reach a &quot;global&quot; goal, as in metabolic networks or autonomous system networks,
the influence area is much larger, and it can contain up to half the vertices
in the graph. In conclusion, our newly introduced approach allows to
distinguish the topology and the structure of various complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03105</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03105</id><created>2015-03-10</created><updated>2015-06-30</updated><authors><author><keyname>Chalermsook</keyname><forenames>Parinya</forenames></author><author><keyname>Goswami</keyname><forenames>Mayank</forenames></author><author><keyname>Kozma</keyname><forenames>Laszlo</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Saranurak</keyname><forenames>Thatchaphol</forenames></author></authors><title>Self-Adjusting Binary Search Trees: What Makes Them Tick?</title><categories>cs.DS math.CO</categories><comments>Substantial revision, additional results. To be presented at ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Splay trees (Sleator and Tarjan) satisfy the so-called access lemma. Many of
the nice properties of splay trees follow from it. What makes self-adjusting
binary search trees (BSTs) satisfy the access lemma? After each access,
self-adjusting BSTs replace the search path by a tree on the same set of nodes
(the after-tree). We identify two simple combinatorial properties of the search
path and the after-tree that imply the access lemma. Our main result (i)
implies the access lemma for all minimally self-adjusting BST algorithms for
which it was known to hold: splay trees and their generalization to the class
of local algorithms (Subramanian, Georgakopoulos and Mc-Clurkin), as well as
Greedy BST, introduced by Demaine et al. and shown to satisfy the access lemma
by Fox, (ii) implies that BST algorithms based on &quot;strict&quot; depth-halving
satisfy the access lemma, addressing an open question that was raised several
times since 1985, and (iii) yields an extremely short proof for the O(log n log
log n) amortized access cost for the path-balance heuristic (proposed by
Sleator), matching the best known bound (Balasubramanian and Raman) to a
lower-order factor.
  One of our combinatorial properties is locality. We show that any
BST-algorithm that satisfies the access lemma via the sum-of-log (SOL)
potential is necessarily local. The other property states that the sum of the
number of leaves of the after-tree plus the number of side alternations in the
search path must be at least a constant fraction of the length of the search
path. We show that a weak form of this property is necessary for sequential
access to be linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03107</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03107</id><created>2015-03-10</created><updated>2015-03-12</updated><authors><author><keyname>Biasse</keyname><forenames>Jean-Francois</forenames></author></authors><title>A fast algorithm for finding a short generator of a principal ideal of
  $\mathbb{Q}(\zeta_{2^n})$</title><categories>math.NT cs.CR</categories><comments>This note was rewritten several times and submitted way too soon. It
  contains too many mistakes for public dissemination</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present a heuristic algorithm that computes the ideal class
group, the unit group and a generator of a principal ideal in
$\mathbb{Q}(\zeta_{2^n})$ in time $2^{N^{1/2+\varepsilon}}$ for $N:= 2^n$ and
arbitrarily small $\varepsilon$. This yields an attack on the cryptographic
schemes relying on the hardness of finding a short generator of a principal
ideal of $\mathbb{Q}(\zeta_{2^n})$ such as such as the homomorphic encryption
scheme of Vercauteren and Smart, and the multilinear maps of Garg, Gentry and
Halevi.
  We rely on recent work from Campbel, Groves and Shepherd which was formalized
and partially disseminated by Cramer, Ducas, Peikert and Regev. They claimed
that when the generator of an ideal is small enough, finding it polynomially
reduces to finding an arbitrary generator. Combined with the work of Biasse and
Fieker on principal ideal computation, this yields an attack against schemes
relying on the hardness of finding a short generator of a principal ideal in
time $2^{N^{2/3+\varepsilon}}$ for $N:=2^n$ and arbitrarily small $\varepsilon
&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03113</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03113</id><created>2015-03-10</created><authors><author><keyname>Dorman</keyname><forenames>A. M.</forenames></author></authors><title>Hardware Probing Interface and Test Robustness</title><categories>cs.OH</categories><comments>7 pages, 1 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computerized integrity test of an electronic product hardware interface and
product probing validation are considered. Integrity testing is based on a
current voltage characteristic measurement, when a small voltage and/or current
stimuli are applied to the product pads including power supply circuitry pads,
so that the product is not normally powered on. Test fixture needles validation
is a part of a self test maintenance scenario designed to predict deterioration
of product probing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03122</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03122</id><created>2015-03-10</created><updated>2015-06-05</updated><authors><author><keyname>Mireault</keyname><forenames>Paul</forenames></author></authors><title>Structured Spreadsheet Modeling and Implementation</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets</comments><acm-class>H.4.1; H.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Developing an error-free spreadsheet has been a problem since the beginning
of end-user computing. In this paper, we present a methodology that separates
the modeling from the implementation. Using proven techniques from Information
Systems and Software Engineering, we present strict, but simple, rules
governing the implementation from the model. The resulting spreadsheet should
be easier to understand, audit and maintain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03124</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03124</id><created>2015-03-10</created><authors><author><keyname>Ghozlan</keyname><forenames>Hassan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Models and Information Rates for Multiuser Optical Fiber Channels with
  Nonlinearity and Dispersion</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete-time interference channel models are developed for information
transmission over optical fiber using wavelength-division multiplexing. A set
of coupled nonlinear Schroedinger equations forms the basis of the models. The
first model is memoryless and captures the nonlinear phenomena of cross-phase
modulation but ignores dispersion. For the case of two carriers, a new
technique called interference focusing is proposed where each carrier achieves
the capacity pre-log 1, thereby doubling the pre-log of 1/2 achieved by using
conventional methods. For more than two carriers, interference focusing is also
useful under certain conditions. The second model captures the nonlinear
phenomena of cross-phase modulation in addition to dispersion due to group
velocity mismatch. Moreover, the model captures the effect of filtering at the
receivers. In a 3-user system, it is shown that all users can achieve the
maximum pre-log factor 1 simultaneously by using interference focusing, a
time-limited pulse and a bank of filters at the receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03128</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03128</id><created>2015-03-10</created><updated>2015-03-13</updated><authors><author><keyname>Wang</keyname><forenames>Da</forenames></author><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Using Straggler Replication to Reduce Latency in Large-scale Parallel
  Computing (Extended Version)</title><categories>cs.DC</categories><comments>Extended version of the 4-page paper submitted to ACM SIGMETRICS
  Workshop on Distributed Cloud Computing (DCC) 2015</comments><acm-class>C.4; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users expect fast and fluid response from today's cloud infrastructure.
Large-scale computing frameworks such as MapReduce divide jobs into many
parallel tasks and execute them on different machines to enable faster
processing. But the tasks on the slowest machines (straggling tasks) become the
bottleneck in the completion of the job. One way to combat the variability in
machine response time, is to add replicas of straggling tasks and wait for one
copy to finish.
  In this paper we analyze how task replication strategies can be used to
reduce latency, and their impact on the cost of computing resources. We use
extreme value theory to show that the tail of the execution time distribution
is the key factor in characterizing the trade-off between latency and computing
cost. From this trade-off we can determine which task replication strategies
reduce latency, without a large increase in computing cost. We also propose a
heuristic algorithm to search for the best replication strategies when it is
difficult to fit a simple distribution to model the empirical behavior of task
execution time, and use the proposed analysis techniques. Evaluation of the
heuristic policies on Google Trace data shows a significant latency reduction
compared to the replication strategy used in MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03130</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03130</id><created>2015-03-10</created><authors><author><keyname>Ghozlan</keyname><forenames>Hassan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Models and Information Rates for Wiener Phase Noise Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A waveform channel is considered where the transmitted signal is corrupted by
Wiener phase noise and additive white Gaussian noise. A discrete-time channel
model that takes into account the effect of filtering on the phase noise is
developed. The model is based on a multi-sample receiver, i.e., an
integrate-and-dump filter whose output is sampled at a rate higher than the
signaling rate. It is shown that, at high signal-to-noise ratio (SNR), the
multi-sample receiver achieves a rate that grows logarithmically with the SNR
if the number of samples per symbol (oversampling factor) grows with the cubic
root of the SNR. Moreover, the pre-log factor is at least 1/2 and can be
achieved by amplitude modulation. Numerical simulations are used to compute
lower bounds on the information rates achieved by the multi-sample receiver.
The simulations show that oversampling is beneficial for both strong and weak
phase noise at high SNR. In fact, the information rates are sometimes
substantially larger than when using commonly-used approximate discrete-time
models. Finally, for an approximate discrete-time model of the multi-sample
receiver, the capacity pre-log at high SNR is at least 3/4 if the number of
samples per symbol grows with the square root of the SNR. The analysis shows
that phase modulation achieves a pre-log of at least 1/4 while amplitude
modulation achieves a pre-log of 1/2. This is strictly greater than the
capacity pre-log of the (approximate) discrete-time Wiener phase noise channel
with only one sample per symbol, which is 1/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03131</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03131</id><created>2015-03-10</created><authors><author><keyname>Han</keyname><forenames>Haoying</forenames></author><author><keyname>Yu</keyname><forenames>Xiang</forenames></author><author><keyname>Long</keyname><forenames>Ying</forenames></author></authors><title>Discovering functional zones using bus smart card data and points of
  interest in Beijing</title><categories>cs.CY cs.SI</categories><comments>21 pages,12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cities comprise various functional zones, including residential, educational,
commercial zones, etc. It is important for urban planners to identify different
functional zones and understand their spatial structure within the city in
order to make better urban plans. In this research, we used 77976010 bus smart
card records of Beijing City in one week in April 2008 and converted them into
two-dimensional time series data of each bus platform, Then, through data
mining in the big database system and previous studies on citizens' trip
behavior, we established the DZoF (discovering zones of different functions)
model based on SCD (smart card Data) and POIs (points of interest), and pooled
the results at the TAZ (traffic analysis zone) level. The results suggested
that DzoF model and cluster analysis based on dimensionality reduction and EM
(expectation-maximization) algorithm can identify functional zones that well
match the actual land uses in Beijing. The methodology in the present research
can help urban planners and the public understand the complex urban spatial
structure and contribute to the academia of urban geography and urban planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03132</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03132</id><created>2015-03-10</created><authors><author><keyname>Ohzeki</keyname><forenames>Masayuki</forenames></author></authors><title>L_1-regularized Boltzmann machine learning using majorizer minimization</title><categories>stat.ML cond-mat.dis-nn cs.LG</categories><comments>16pages, 6 figures</comments><doi>10.7566/JPSJ.84.054801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an inference method to estimate sparse interactions and biases
according to Boltzmann machine learning. The basis of this method is $L_1$
regularization, which is often used in compressed sensing, a technique for
reconstructing sparse input signals from undersampled outputs. $L_1$
regularization impedes the simple application of the gradient method, which
optimizes the cost function that leads to accurate estimations, owing to the
cost function's lack of smoothness. In this study, we utilize the majorizer
minimization method, which is a well-known technique implemented in
optimization problems, to avoid the non-smoothness of the cost function. By
using the majorizer minimization method, we elucidate essentially relevant
biases and interactions from given data with seemingly strongly-correlated
components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03137</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03137</id><created>2015-03-10</created><authors><author><keyname>Bihani</keyname><forenames>Sipra</forenames></author><author><keyname>Hartman</keyname><forenames>Michael</forenames></author><author><keyname>Sobiegalla</keyname><forenames>Florian</forenames></author><author><keyname>Rosenberg</keyname><forenames>Amanda</forenames></author></authors><title>Comparing Network Structures of Commercial and Non-commercial Biohacking
  Online-communities</title><categories>cs.SI</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/28</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper compares two biohacking groups, Bulletproof Executive and DIYbio,
whose distinct goals result in differences in social network structures,
activities and entry points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03144</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03144</id><created>2015-03-10</created><updated>2015-09-03</updated><authors><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques E.</forenames></author></authors><title>Control Contraction Metrics: Convex and Intrinsic Criteria for Nonlinear
  Feedback Design</title><categories>cs.SY math.OC</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contraction analysis uses differential dynamics and Riemannian metrics to
show that all solutions of a particular system converge exponentially. In this
paper we generalize this approach to problems in control design, giving
sufficient conditions for exponential stabilizability of all trajectories of a
nonlinear control system. The conditions can be expressed in terms of a dual
metric as (convex) pointwise linear matrix inequalities. We show that for
feedback linearizable systems the conditions are necessary as well as
sufficient. We also show how computation can be simplified through the use of a
virtual dynamical systems, and derive novel convex criteria for exponential
convergence to a flow-invariant nonlinear manifold. Straightforward extensions
allow approximate optimal and robust control, generalizing well-known LMI
conditions for linear systems. The proposed techniques are illustrated with a
variety of examples, including some utilizing sum-of-squares programming for
computation of the metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03148</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03148</id><created>2015-03-10</created><authors><author><keyname>Jayadeva</keyname></author><author><keyname>Soman</keyname><forenames>Sumit</forenames></author><author><keyname>Bhaya</keyname><forenames>Amit</forenames></author></authors><title>A Neurodynamical System for finding a Minimal VC Dimension Classifier</title><categories>cs.LG stat.ML</categories><comments>15 pages, 3 figures</comments><msc-class>70G660, 68T05</msc-class><acm-class>I.5.1; I.5.5; G.1.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed Minimal Complexity Machine (MCM) finds a hyperplane
classifier by minimizing an exact bound on the Vapnik-Chervonenkis (VC)
dimension. The VC dimension measures the capacity of a learning machine, and a
smaller VC dimension leads to improved generalization. On many benchmark
datasets, the MCM generalizes better than SVMs and uses far fewer support
vectors than the number used by SVMs. In this paper, we describe a neural
network based on a linear dynamical system, that converges to the MCM solution.
The proposed MCM dynamical system is conducive to an analogue circuit
implementation on a chip or simulation using Ordinary Differential Equation
(ODE) solvers. Numerical experiments on benchmark datasets from the UCI
repository show that the proposed approach is scalable and accurate, as we
obtain improved accuracies and fewer number of support vectors (upto 74.3%
reduction) with the MCM dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03155</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03155</id><created>2015-03-10</created><authors><author><keyname>Chung</keyname><forenames>Fan</forenames></author><author><keyname>Simpson</keyname><forenames>Olivia</forenames></author></authors><title>Computing Heat Kernel Pagerank and a Local Clustering Algorithm</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heat kernel pagerank is a variation of Personalized PageRank given in an
exponential formulation. In this work, we present a sublinear time algorithm
for approximating the heat kernel pagerank of a graph. The algorithm works by
simulating random walks of bounded length and runs in time
$O\big(\frac{\log(\epsilon^{-1})\log
n}{\epsilon^3\log\log(\epsilon^{-1})}\big)$, assuming performing a random walk
step and sampling from a distribution with bounded support take constant time.
  The quantitative ranking of vertices obtained with heat kernel pagerank can
be used for local clustering algorithms. We present an efficient local
clustering algorithm that finds cuts by performing a sweep over a heat kernel
pagerank vector, using the heat kernel pagerank approximation algorithm as a
subroutine. Specifically, we show that for a subset $S$ of Cheeger ratio
$\phi$, many vertices in $S$ may serve as seeds for a heat kernel pagerank
vector which will find a cut of conductance $O(\sqrt{\phi})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03157</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03157</id><created>2015-03-10</created><authors><author><keyname>Chung</keyname><forenames>Fan</forenames></author><author><keyname>Simpson</keyname><forenames>Olivia</forenames></author></authors><title>Solving Local Linear Systems with Boundary Conditions Using Heat Kernel
  Pagerank</title><categories>cs.DS</categories><journal-ref>Internet Mathematics, Vol. 11, Iss. 4-5, 2015 p. 449-471</journal-ref><doi>10.1080/15427951.2015.1009522</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient algorithm for solving local linear systems with a
boundary condition using the Green's function of a connected induced subgraph
related to the system. We introduce the method of using the Dirichlet heat
kernel pagerank vector to approximate local solutions to linear systems in the
graph Laplacian satisfying given boundary conditions over a particular subset
of vertices. With an efficient algorithm for approximating Dirichlet heat
kernel pagerank, our local linear solver algorithm computes an approximate
local solution with multiplicative and additive error $\epsilon$ by performing
$O(\epsilon^{-5}s^3\log(s^3\epsilon^{-1})\log n)$ random walk steps, where $n$
is the number of vertices in the full graph and $s$ is the size of the local
system on the induced subgraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03163</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03163</id><created>2015-03-10</created><authors><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>Fu</keyname><forenames>Yanwei</forenames></author><author><keyname>Zang</keyname><forenames>Andi</forenames></author><author><keyname>Sigal</keyname><forenames>Leonid</forenames></author><author><keyname>Agam</keyname><forenames>Gady</forenames></author></authors><title>Learning Classifiers from Synthetic Data Using a Multichannel
  Autoencoder</title><categories>cs.CV cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for using synthetic data to help learning classifiers.
Synthetic data, even is generated based on real data, normally results in a
shift from the distribution of real data in feature space. To bridge the gap
between the real and synthetic data, and jointly learn from synthetic and real
data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by
suing MCAE, it is possible to learn a better feature representation for
classification. To evaluate the proposed approach, we conduct experiments on
two types of datasets. Experimental results on two datasets validate the
efficiency of our MCAE model and our methodology of generating synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03165</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03165</id><created>2015-03-11</created><updated>2015-05-29</updated><authors><author><keyname>Ding</keyname><forenames>Ni</forenames></author><author><keyname>Kennedy</keyname><forenames>Rodney A.</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Iterative Merging Algorithm for Cooperative Data Exchange</title><categories>cs.DS cs.IT math.IT</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding the minimum sum-rate strategy in
cooperative data exchange systems that do not allow packet-splitting (NPS-CDE).
In an NPS-CDE system, there are a number of geographically close cooperative
clients who send packets to help the others recover a packet set. A minimum
sum-rate strategy is the strategy that achieves universal recovery (the
situation when all the clients recover the whole packet set) with the the
minimal sum-rate (the total number of transmissions). We propose an iterative
merging (IM) algorithm that recursively merges client sets based on a lower
estimate of the minimum sum-rate and updates to the value of the minimum
sum-rate. We also show that a minimum sum-rate strategy can be learned by
allocating rates for the local recovery in each merged client set in the IM
algorithm. We run an experiment to show that the complexity of the IM algorithm
is lower than that of the existing deterministic algorithm when the number of
clients is lower than $94$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03166</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03166</id><created>2015-03-08</created><authors><author><keyname>Singh</keyname><forenames>Kirat Pal</forenames></author><author><keyname>Parmar</keyname><forenames>Shivani</forenames></author></authors><title>Design of High Performance MIPS Cryptography Processor Based on T-DES
  Algorithm</title><categories>cs.AR</categories><comments>International Journal of Engineering Research &amp; Technology. arXiv
  admin note: substantial text overlap with arXiv:1306.1916, arXiv:1503.02304</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes the design of high performance MIPS Cryptography
processor based on triple data encryption standard. The organization of
pipeline stages in such a way that pipeline can be clocked at high frequency.
Encryption and Decryption blocks of triple data encryption standard (T-DES)
crypto system and dependency among themselves are explained in detail with the
help of block diagram. In order to increase the processor functionality and
performance, especially for security applications we include three new 32-bit
instructions LKLW, LKUW and CRYPT. The design has been synthesized at 40nm
process technology targeting using Xilinx Virtex-6 device. The overall MIPS
Crypto processor works at 209MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03167</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03167</id><created>2015-03-11</created><updated>2015-06-21</updated><authors><author><keyname>Kulkarni</keyname><forenames>Tejas D.</forenames></author><author><keyname>Whitney</keyname><forenames>Will</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Deep Convolutional Inverse Graphics Network</title><categories>cs.CV cs.GR cs.LG cs.NE</categories><comments>First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a
model that learns an interpretable representation of images. This
representation is disentangled with respect to transformations such as
out-of-plane rotations and lighting variations. The DC-IGN model is composed of
multiple layers of convolution and de-convolution operators and is trained
using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a
training procedure to encourage neurons in the graphics code layer to represent
a specific transformation (e.g. pose or light). Given a single input image, our
model can generate new images of the same object with variations in pose and
lighting. We present qualitative and quantitative results of the model's
efficacy at learning a 3D rendering engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03168</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03168</id><created>2015-03-10</created><authors><author><keyname>Grace</keyname><forenames>G. Hannah</forenames></author><author><keyname>Desikan</keyname><forenames>Kalyani</forenames></author></authors><title>Experimental Estimation of Number of Clusters Based on Cluster Quality</title><categories>cs.IR</categories><comments>12 pages, 9 figures</comments><journal-ref>Journal of mathematics and computer science, Vol12 (2014), 304-315</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text Clustering is a text mining technique which divides the given set of
text documents into significant clusters. It is used for organizing a huge
number of text documents into a well-organized form. In the majority of the
clustering algorithms, the number of clusters must be specified apriori, which
is a drawback of these algorithms. The aim of this paper is to show
experimentally how to determine the number of clusters based on cluster
quality. Since partitional clustering algorithms are well-suited for clustering
large document datasets, we have confined our analysis to a partitional
clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03169</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03169</id><created>2015-03-10</created><authors><author><keyname>R</keyname><forenames>Jithin</forenames></author><author><keyname>Chandran</keyname><forenames>Priya</forenames></author></authors><title>Dynamic Partitioning of Physical Memory Among Virtual Machines,
  ASMI:Architectural Support for Memory Isolation</title><categories>cs.AR</categories><comments>Rejected this short paper by the VEE 2015 Conference conducted by ACM
  due to the lack of implementation details</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing relies on secure and efficient virtualization. Software level
security solutions compromise the performance of virtual machines (VMs), as a
large amount of computational power would be utilized for running the security
modules. Moreover, software solutions are only as secure as the level that they
work on. For example a security module on a hypervisor cannot provide security
in the presence of an infected hypervisor. It is a challenge for virtualization
technology architects to enhance the security of VMs without degrading their
performance. Currently available server machines are not fully equipped to
support a secure VM environment without compromising on performance. A few
hardware modifications have been introduced by manufactures like Intel and AMD
to provide a secure VM environment with low performance degradation. In this
paper we propose a novel memory architecture model named \textit{ Architectural
Support for Memory Isolation(ASMI)}, that can achieve a true isolated physical
memory region to each VM without degrading performance. Along with true memory
isolation, ASMI is designed to provide lower memory access times, better
utilization of available memory, support for DMA isolation and support for
platform independence for users of VMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03170</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03170</id><created>2015-03-11</created><updated>2015-04-08</updated><authors><author><keyname>Rathore</keyname><forenames>Abhishek</forenames></author></authors><title>Min Morse: Approximability &amp; Applications</title><categories>cs.CG</categories><comments>80pages, 25 figures, preprint (yet to be submitted), resolves an open
  problem in computational topology, Nearly linear time Approximation Algorithm
  with widescale applications in computational topology, section on MWUM-based
  solution expanded/made clearer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We resolve an open problem posed by Joswig et al. by providing an
$\tilde{O}(N)$ time, $O(\log^2(N))$-factor approximation algorithm for the
min-Morse unmatched problem (MMUP) Let $\Lambda$ be the no. of critical cells
of the optimal discrete Morse function and $N$ be the total no. of cells of a
regular cell complex K. The goal of MMUP is to find $\Lambda$ for a given
complex K. To begin with, we apply an approx. preserving graph reduction on
MMUP to obtain a new problem namely the min-partial order problem (min-POP)(a
strict generalization of the min-feedback arc set problem). The reduction
involves introduction of rigid edges which are edges that demand strict
inclusion in output solution. To solve min-POP, we use the Leighton- Rao
divide-&amp;-conquer paradigm that provides solutions to SDP-formulated instances
of min-directed balanced cut with rigid edges (min-DBCRE). Our first algorithm
for min-DBCRE extends Agarwal et al.'s rounding procedure for digraph
formulation of ARV-algorithm to handle rigid edges. Our second algorithm to
solve min-DBCRE SDP, adapts Arora et al.'s primal dual MWUM. In terms of
applications, under the mild assumption1 of the size of topological features
being significantly smaller compared to the size of the complex, we obtain an
(a) $\tilde{O}(N)$ algorithm for computing homology groups $H_i(K,A)$ of a
simplicial complex K, (where A is an arbitrary Abelian group.) (b) an
$\tilde{O}(N^2)$ algorithm for computing persistent homology and (c) an
$\tilde{O}(N)$ algorithm for computing the optimal discrete Morse-Witten
function compatible with input scalar function as simple consequences of our
approximation algorithm for MMUP thereby giving us the best known complexity
bounds for each of these applications under the aforementioned assumption. Such
an assumption is realistic in applied settings, and often a characteristic of
modern massive datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03175</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03175</id><created>2015-03-11</created><authors><author><keyname>Kumar</keyname><forenames>Udit</forenames></author><author><keyname>Soman</keyname><forenames>Sumit</forenames></author><author><keyname>Jayadeva</keyname></author></authors><title>Benchmarking NLopt and state-of-art algorithms for Continuous Global
  Optimization via Hybrid IACO$_\mathbb{R}$</title><categories>cs.NE</categories><comments>24 pages, 10 figures</comments><msc-class>80M50</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comparative analysis of the performance of the
Incremental Ant Colony algorithm for continuous optimization
($IACO_\mathbb{R}$), with different algorithms provided in the NLopt library.
The key objective is to understand how the various algorithms in the NLopt
library perform in combination with the Multi Trajectory Local Search (Mtsls1)
technique. A hybrid approach has been introduced in the local search strategy
by the use of a parameter which allows for probabilistic selection between
Mtsls1 and a NLopt algorithm. In case of stagnation, the algorithm switch is
made based on the algorithm being used in the previous iteration. The paper
presents an exhaustive comparison on the performance of these approaches on
Soft Computing (SOCO) and Congress on Evolutionary Computation (CEC) 2014
benchmarks. For both benchmarks, we conclude that the best performing algorithm
is a hybrid variant of Mtsls1 with BFGS for local search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03176</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03176</id><created>2015-03-11</created><updated>2015-04-22</updated><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Towards a Science of Trust</title><categories>cs.CR</categories><comments>19 pages, 3 tables, HoTSoS Bootcamp 2015, Urbana-Champaign April
  21-22, 2015, proceedings published by ACM; in v2 the explanation of the NP
  decision rule is added, and a table formatting error is fixed</comments><acm-class>K.4.4; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diverse views of science of security have opened up several alleys
towards applying the methods of science to security. We pursue a different kind
of connection between science and security. This paper explores the idea that
security is not just a suitable subject for science, but that the process of
security is also similar to the process of science. This similarity arises from
the fact that both science and security depend on the methods of inductive
inference. Because of this dependency, a scientific theory can never be
definitely proved, but can only be disproved by new evidence, and improved into
a better theory. Because of the same dependency, every security claim and
method has a lifetime, and always eventually needs to be improved.
  In this general framework of security-as-science, we explore the ways to
apply the methods of scientific induction in the process of trust. The process
of trust building and updating is viewed as hypothesis testing. We propose to
formulate the trust hypotheses by the methods of algorithmic learning, and to
build more robust trust testing and vetting methodologies on the solid
foundations of statistical inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03184</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03184</id><created>2015-03-11</created><authors><author><keyname>Choudhary</keyname><forenames>Sunav</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Fundamental Limits of Blind Deconvolution Part II: Sparsity-Ambiguity
  Trade-offs</title><categories>cs.IT math.IT</categories><comments>19 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind deconvolution is an ubiquitous non-linear inverse problem in
applications like wireless communications and image processing. This problem is
generally ill-posed since signal identifiability is a key concern, and there
have been efforts to use sparse models for regularizing blind deconvolution to
promote signal identifiability. Part I of this two-part paper establishes a
measure theoretically tight characterization of the ambiguity space for blind
deconvolution and unidentifiability of this inverse problem under unconstrained
inputs. Part II of this paper analyzes the identifiability of the
canonical-sparse blind deconvolution problem and establishes surprisingly
strong negative results on the sparsity-ambiguity trade-off scaling laws.
Specifically, the ill-posedness of canonical-sparse blind deconvolution is
quantified by exemplifying the dimension of the unidentifiable signal sets. An
important conclusion of the paper is that canonical sparsity occurring
naturally in applications is insufficient and that coding is necessary for
signal identifiability in blind deconvolution. The methods developed herein are
applied to a second-hop channel estimation problem to show that a family of
subspace coded signals (including repetition coding and geometrically decaying
signals) are unidentifiable under blind deconvolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03185</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03185</id><created>2015-03-11</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Testing randomness by Matching Pennies</title><categories>cs.GT</categories><comments>14 pages, 1 table</comments><msc-class>91A26, 68Q32</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the game of Matching Pennies, Alice and Bob each hold a penny, and at
every tick of the clock they simultaneously display the head or the tail sides
of their coins. If they both display the same side, then Alice wins Bob's
penny; if they display different sides, then Bob wins Alice's penny. To avoid
giving the opponent a chance to win, both players seem to have nothing else to
do but to randomly play heads and tails with equal frequencies. However, while
not losing in this game is easy, not missing an opportunity to win is not.
Randomizing your own moves can be made easy. Recognizing when the opponent's
moves are not random can be arbitrarily hard.
  The notion of randomness is central in game theory, but it is usually taken
for granted. The notion of outsmarting is not central in game theory, but it is
central in the practice of gaming. We pursue the idea that these two notions
can be usefully viewed as two sides of the same coin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03187</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03187</id><created>2015-03-11</created><updated>2015-03-16</updated><authors><author><keyname>Shao</keyname><forenames>Wen-Ze</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Simple, Accurate, and Robust Nonparametric Blind Super-Resolution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple, accurate, and robust approach to single image
nonparametric blind Super-Resolution (SR). This task is formulated as a
functional to be minimized with respect to both an intermediate super-resolved
image and a nonparametric blur-kernel. The proposed approach includes a
convolution consistency constraint which uses a non-blind learning-based SR
result to better guide the estimation process. Another key component is the
unnatural bi-l0-l2-norm regularization imposed on the super-resolved, sharp
image and the blur-kernel, which is shown to be quite beneficial for estimating
the blur-kernel accurately. The numerical optimization is implemented by
coupling the splitting augmented Lagrangian and the conjugate gradient (CG).
Using the pre-estimated blur-kernel, we finally reconstruct the SR image by a
very simple non-blind SR method that uses a natural image prior. The proposed
approach is demonstrated to achieve better performance than the recent method
by Michaeli and Irani [2] in both terms of the kernel estimation accuracy and
image SR quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03191</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03191</id><created>2015-03-11</created><updated>2015-03-11</updated><authors><author><keyname>Ward</keyname><forenames>Ben</forenames></author><author><keyname>Bastian</keyname><forenames>John</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Pooley</keyname><forenames>Daniel</forenames></author><author><keyname>Bari</keyname><forenames>Rajendra</forenames></author><author><keyname>Berger</keyname><forenames>Bettina</forenames></author><author><keyname>Tester</keyname><forenames>Mark</forenames></author></authors><title>A model-based approach to recovering the structure of a plant from
  images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for recovering the structure of a plant directly from a
small set of widely-spaced images. Structure recovery is more complex than
shape estimation, but the resulting structure estimate is more closely related
to phenotype than is a 3D geometric model. The method we propose is applicable
to a wide variety of plants, but is demonstrated on wheat. Wheat is made up of
thin elements with few identifiable features, making it difficult to analyse
using standard feature matching techniques. Our method instead analyses the
structure of plants using only their silhouettes. We employ a generate-and-test
method, using a database of manually modelled leaves and a model for their
composition to synthesise plausible plant structures which are evaluated
against the images. The method is capable of efficiently recovering accurate
estimates of plant structure in a wide variety of imaging scenarios, with no
manual intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03195</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03195</id><created>2015-03-11</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz</forenames></author></authors><title>Reconciling a component and process view</title><categories>cs.SE</categories><comments>Preprint, 7th International Workshop on Modeling in Software
  Engineering (MiSE) at ICSE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many cases we need to represent on the same abstraction level not only
system components but also processes within the system, and if for both
representation different frameworks are used, the system model becomes hard to
read and to understand. We suggest a solution how to cover this gap and to
reconcile component and process views on system representation: a formal
framework that gives the advantage of solving design problems for large-scale
component systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03199</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03199</id><created>2015-03-11</created><authors><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author></authors><title>Persistence of activity on Twitter triggered by a natural disaster: A
  data analysis</title><categories>physics.soc-ph cs.SI</categories><comments>2 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we list the results of a simple analysis of a Twitter dataset:
the complete dataset of Japanese tweets in the 1-week period after the Great
East Japan earthquake, which occurred on March 11, 2011. Our data analysis
shows how people reacted to the earthquake on Twitter and how some users went
inactive in the long-term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03202</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03202</id><created>2015-03-11</created><updated>2015-03-13</updated><authors><author><keyname>Cerlinca</keyname><forenames>Marius Cristian</forenames></author><author><keyname>Cerlinca</keyname><forenames>Tudor Ioan</forenames></author><author><keyname>Turcu</keyname><forenames>Cristina Elena</forenames></author><author><keyname>Prodan</keyname><forenames>Remus Catalin</forenames></author><author><keyname>Giza-Belciug</keyname><forenames>Felicia Florentina</forenames></author></authors><title>Proiectarea si implementarea unui portal HL7</title><categories>cs.SE</categories><comments>4 pages, in Romanian; Distributed Systems (2009) Suceava</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces some techniques used in developing and implementing an
HL7 clinical data portal used in client-server architecture. The HL7 portal is
used by nonHL7 applications that need medical data from HL7 servers. Also, the
portal can translate a large number of HL7 terms between an indefinite number
of languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03208</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03208</id><created>2015-03-11</created><authors><author><keyname>Vadoodparast</keyname><forenames>M.</forenames></author><author><keyname>Hamdan</keyname><forenames>A. Razak</forenames></author><author><keyname>Hafiz</keyname></author></authors><title>Fraudulent Electronic transaction detection using KDA Model</title><categories>cs.DB cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering analysis and Datamining methodologies were applied to the problem
of identifying illegal and fraud transactions. The researchers independently
developed model and software using data provided by a bank and using Rapidminer
modeling tool. The research objectives are to propose dynamic model and
mechanism to cover fraud detection system limitations. KDA model as proposed
model can detect 68.75% of fraudulent transactions with online dynamic modeling
and 81.25% in offline mode and the Fraud Detection System &amp; Decision Support
System. Software propose a good supporting procedure to detect fraudulent
transaction dynamically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03211</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03211</id><created>2015-03-11</created><authors><author><keyname>Orove</keyname><forenames>J. O.</forenames></author><author><keyname>Osegi</keyname><forenames>N. E.</forenames></author><author><keyname>Eke</keyname><forenames>B. O.</forenames></author></authors><title>A Multi-Gene Genetic Programming Application for Predicting Students
  Failure at School</title><categories>cs.CY cs.AI cs.NE</categories><comments>14 pages, 9 figures, Journal paper. arXiv admin note: text overlap
  with arXiv:1403.0623 by other authors</comments><journal-ref>Afr J. of Comp &amp; ICTs. Vol 7, No. 3. Pp21-34</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Several efforts to predict student failure rate (SFR) at school accurately
still remains a core problem area faced by many in the educational sector. The
procedure for forecasting SFR are rigid and most often times require data
scaling or conversion into binary form such as is the case of the logistic
model which may lead to lose of information and effect size attenuation. Also,
the high number of factors, incomplete and unbalanced dataset, and black boxing
issues as in Artificial Neural Networks and Fuzzy logic systems exposes the
need for more efficient tools. Currently the application of Genetic Programming
(GP) holds great promises and has produced tremendous positive results in
different sectors. In this regard, this study developed GPSFARPS, a software
application to provide a robust solution to the prediction of SFR using an
evolutionary algorithm known as multi-gene genetic programming. The approach is
validated by feeding a testing data set to the evolved GP models. Result
obtained from GPSFARPS simulations show its unique ability to evolve a suitable
failure rate expression with a fast convergence at 30 generations from a
maximum specified generation of 500. The multi-gene system was also able to
minimize the evolved model expression and accurately predict student failure
rate using a subset of the original expression
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03215</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03215</id><created>2015-03-11</created><authors><author><keyname>Vidhya</keyname><forenames>B.</forenames></author><author><keyname>Joseph</keyname><forenames>Mary</forenames></author><author><keyname>Girinath</keyname><forenames>D. Rajini</forenames></author><author><keyname>Malathi</keyname><forenames>A.</forenames></author></authors><title>Environment Based Secure Transfer of Data in Wireless Sensor Networks</title><categories>cs.CR</categories><comments>12, VOL 4,NO 1, FEBRUARY 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Most critical sensor readings (Top-k Monitoring) in environment monitoring
system are important to many wireless sensor applications. In such
applications, sensor nodes transmit the data continuously for a specific time
period to the storage nodes. It is responsible for transferring the received
results to the Authority on Top-k Query request from them. Dummy data's were
added into the original text data to secure the data against adversary in case
of hacking the sensor and storage nodes. If storage node gets hacked by
adversary, false details will be sent to the authority. An effective technique
named aggregate signature to validate the source of the message and also to
protect the data against latest security attacks, cryptography technique
combined with steganography has been introduced. Indexed based scheme for the
database access has also been proposed, to validate the resources against
availability before forwarding the data fetch request to storage nodes from
Authority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03223</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03223</id><created>2015-03-11</created><authors><author><keyname>Barletta</keyname><forenames>Luca</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Lower Bound on the Capacity of Continuous-Time Wiener Phase Noise
  Channels</title><categories>cs.IT math.IT</categories><comments>Extended version of a paper submitted to ISIT 2015. 9 pages and 1
  figure. arXiv admin note: text overlap with arXiv:1411.0390</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A continuous-time Wiener phase noise channel with an integrate-and-dump
multi-sample receiver is studied.
  A lower bound to the capacity with an average input power constraint is
derived, and a high signal-to-noise ratio (SNR) analysis is performed.
  The capacity pre-log depends on the oversampling factor, and amplitude and
phase modulation do not equally contribute to capacity at high SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03231</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03231</id><created>2015-03-11</created><authors><author><keyname>Mota</keyname><forenames>Joao F. C.</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Adaptive-Rate Sparse Signal Reconstruction With Application in
  Compressive Background Subtraction</title><categories>math.OC cs.CV cs.IT math.IT stat.ML</categories><comments>submitted to IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze an online algorithm for reconstructing a sequence of
signals from a limited number of linear measurements. The signals are assumed
sparse, with unknown support, and evolve over time according to a generic
nonlinear dynamical model. Our algorithm, based on recent theoretical results
for $\ell_1$-$\ell_1$ minimization, is recursive and computes the number of
measurements to be taken at each time on-the-fly. As an example, we apply the
algorithm to compressive video background subtraction, a problem that can be
stated as follows: given a set of measurements of a sequence of images with a
static background, simultaneously reconstruct each image while separating its
foreground from the background. The performance of our method is illustrated on
sequences of real images: we observe that it allows a dramatic reduction in the
number of measurements with respect to state-of-the-art compressive background
subtraction schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03233</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03233</id><created>2015-03-11</created><authors><author><keyname>Dorri</keyname><forenames>Ali</forenames></author><author><keyname>Kamel</keyname><forenames>Seyed Reza</forenames></author><author><keyname>Kheirkhah</keyname><forenames>Esmaeil</forenames></author></authors><title>Security challenges in mobile ad hoc networks:a survey</title><categories>cs.NI cs.CR</categories><comments>2 Figures, 2 Tables</comments><journal-ref>IJCSES Vol 6 No 1 2015</journal-ref><doi>10.5121/ijcses.2015.6102</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  MANET is a kind of Ad hoc network with mobile, wireless nodes. Because of its
special characteristics like dynamic topology, hop-by-hop communications and
easy and quick setup, MANET faced lots of challenges allegorically routing,
security and clustering. The security challenges arise due to MANETs
self-configuration and self-maintenance capabilities. In this paper, we present
an elaborate view of issues in MANET security. Based on MANETs special
characteristics, we define three security parameters for MANET. In addition we
divided MANET security into two different aspects and discussed each one in
details. A comprehensive analysis in security aspects of MANET and defeating
approaches is presented. In addition, defeating approaches against attacks have
been evaluated in some important metrics. After analyses and evaluations,
future scopes of work have been presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03238</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03238</id><created>2015-03-11</created><authors><author><keyname>Grabocka</keyname><forenames>Josif</forenames></author><author><keyname>Wistuba</keyname><forenames>Martin</forenames></author><author><keyname>Schmidt-Thieme</keyname><forenames>Lars</forenames></author></authors><title>Scalable Discovery of Time-Series Shapelets</title><categories>cs.LG</categories><comments>Under review in the journal &quot;Knowledge and Information Systems&quot;
  (KAIS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-series classification is an important problem for the data mining
community due to the wide range of application domains involving time-series
data. A recent paradigm, called shapelets, represents patterns that are highly
predictive for the target variable. Shapelets are discovered by measuring the
prediction accuracy of a set of potential (shapelet) candidates. The candidates
typically consist of all the segments of a dataset, therefore, the discovery of
shapelets is computationally expensive. This paper proposes a novel method that
avoids measuring the prediction accuracy of similar candidates in Euclidean
distance space, through an online clustering pruning technique. In addition,
our algorithm incorporates a supervised shapelet selection that filters out
only those candidates that improve classification accuracy. Empirical evidence
on 45 datasets from the UCR collection demonstrate that our method is 3-4
orders of magnitudes faster than the fastest existing shapelet-discovery
method, while providing better prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03244</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03244</id><created>2015-03-11</created><authors><author><keyname>Hu</keyname><forenames>Baotian</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Chen</keyname><forenames>Qingcai</forenames></author></authors><title>Convolutional Neural Network Architectures for Matching Natural Language
  Sentences</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic matching is of central importance to many natural language tasks
\cite{bordes2014semantic,RetrievalQA}. A successful matching algorithm needs to
adequately model the internal structures of language objects and the
interaction between them. As a step toward this goal, we propose convolutional
neural network models for matching two sentences, by adapting the convolutional
strategy in vision and speech. The proposed models not only nicely represent
the hierarchical structures of sentences with their layer-by-layer composition
and pooling, but also capture the rich matching patterns at different levels.
Our models are rather generic, requiring no prior knowledge on language, and
can hence be applied to matching tasks of different nature and in different
languages. The empirical study on a variety of matching tasks demonstrates the
efficacy of the proposed model on a variety of matching tasks and its
superiority to competitor models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03250</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03250</id><created>2015-03-11</created><updated>2015-03-20</updated><authors><author><keyname>Dazzi</keyname><forenames>Patrizio</forenames></author><author><keyname>Mordacchini</keyname><forenames>Matteo</forenames></author><author><keyname>Ricci</keyname><forenames>Laura</forenames></author></authors><title>Epidemic Information Diffusion: A Simple Solution to Support
  Community-based Recommendations in P2P Overlays</title><categories>cs.SI cs.NI</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemic protocols proved to be very efficient solutions for supporting
dynamic and complex information diffusion in highly dis- tributed computing
infrastructures, like P2P environments. They are useful bricks for building and
maintaining virtual network topologies, in the form of overlay networks as well
as to support pervasive diffusion of information when it is injected into the
network. This paper proposes a simple architecture exploiting the features of
epidemic approaches to foster a collaborative percolation of information
between computing nodes belonging to the network aimed at building a system
that groups similar users and spread useful information among them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03256</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03256</id><created>2015-03-11</created><authors><author><keyname>Badjana</keyname><forenames>H&#xe8;ou Mal&#xe9;ki</forenames></author><author><keyname>Zander</keyname><forenames>Franziska</forenames></author><author><keyname>Kralisch</keyname><forenames>Sven</forenames></author><author><keyname>Helmschrot</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Fl&#xfc;gel</keyname><forenames>Wolfgang-Albert</forenames></author></authors><title>An information system for integrated land and water resources management
  in the Kara River basin (Togo and Benin)</title><categories>cs.CY</categories><comments>13 pages, 6 figures</comments><journal-ref>International Journal of Database Management Systems (IJDMS)
  Vol.7, No.1, February 2015</journal-ref><doi>10.5121/ijdms.2015.7102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prerequisite for integrated land and water resources management (ILWRM) is
a holistic river basin assessment. The latter requires information and data
from different scientific disciplines but also appropriate data management
systems to store and manage historical and real time data, set up protocols
that facilitate data and information access and sharing among different
stakeholders, and triggering further collaboration among different institutions
in support of watershed-based assessment, management and planning. In West
Africa in general and especially in the transboundary Volta River basin where
different environmental data are collected and managed by different agencies in
different countries and also where data access and dissemination are very
challenging and difficult tasks, comprehensive river basin information systems
are required. This paper presents the Oti River Basin Information System
(OtiRBIS), a web-based data storage, management andanalysis platform that
addresses these needs and facilitates ILWRM implementation in the Kara river
basin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03261</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03261</id><created>2015-03-11</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Approximation of Statistical Analysis and Estimation by Morphological
  Adaptation in a Model of Slime Mould</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  True slime mould Physarum polycephalum approximates a range of complex
computations via growth and adaptation of its proto- plasmic transport network,
stimulating a large body of recent research into how such a simple organism can
perform such complex feats. The properties of networks constructed by slime
mould are known to be in- fluenced by the local distribution of stimuli within
its environment. But can the morphological adaptation of slime mould yield any
information about the global statistical properties of its environment? We
explore this possibility using a particle based model of slime mould. We
demonstrate how morphological adaptation in blobs of virtual slime mould may be
used as a simple computational mechanism that can coarsely approx- imate
statistical analysis, estimation and tracking. Preliminary results include the
approximation of the geometric centroid of 2D shapes, ap- proximation of
arithmetic mean from spatially represented sorted and unsorted data
distributions, and the estimation and dynamical tracking of moving object
position in the presence of noise contaminated input stimuli. The results
suggest that it is possible to utilise collectives of very simple components
with limited individual computational ability (for ex- ample swarms of simple
robotic devices) to extract statistical features from complex datasets by means
of material adaptation and sensorial fusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03264</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03264</id><created>2015-03-11</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Material Approximation of Data Smoothing and Spline Curves Inspired by
  Slime Mould</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a particle model of Physarum displaying emer- gent morphological
adaptation behaviour we demonstrate how a minimal approach to collective
material computation may be used to transform and summarise properties of
spatially represented datasets. We find that the virtual material relaxes more
strongly to high-frequency changes in data which can be used for the smoothing
(or filtering) of data by ap- proximating moving average and low-pass filters
in 1D datasets. The relaxation and minimisation properties of the model enable
the spatial computation of B-spline curves (approximating splines) in 2D
datasets. Both clamped and unclamped spline curves, of open and closed shapes,
can be represented and the degree of spline curvature corresponds to the
relaxation time of the material. The material computation of spline curves also
includes novel quasi-mechanical properties including unwind- ing of the shape
between control points and a preferential adhesion to longer, straighter paths.
Interpolating splines could not directly be ap- proximated due to the formation
and evolution of Steiner points at nar- row vertices, but were approximated
after rectilinear pre-processing of the source data. This pre-processing was
further simplified by transform- ing the original data to contain the material
inside the polyline. These exemplar results expand the repertoire of spatially
represented uncon- ventional computing devices by demonstrating a simple,
collective and distributed approach to data and curve smoothing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03265</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03265</id><created>2015-03-11</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author></authors><title>A Morphological Adaptation Approach to Path Planning Inspired by Slime
  Mould</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a particle model of slime mould we demonstrate scoping experiments
which explore how path planning may be performed by morphological adaptation.
We initially demonstrate simple path planning by a shrinking blob of virtual
plasmodium between two attractant sources within a polygonal arena. We examine
the case where multiple paths are required and the subsequent selection of a
single path from multiple options. Collision-free paths are implemented via
repulsion from the borders of the arena. Finally, obstacle avoidance is
implemented by repulsion from obstacles as they are uncovered by the shrinking
blob. These examples show proof-of-concept results of path planning by
morphological adaptation which complement existing research on path planning in
novel computing substrates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03266</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03266</id><created>2015-03-11</created><authors><author><keyname>Amor</keyname><forenames>Selma Belhadj</forenames></author></authors><title>A New Coding Scheme for Discrete Memoryless MACs with Common
  Rate-Limited Feedback</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, submitted to the European Conference on Networks
  and Communications 2015 (EuCNC'2015), Paris, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new coding scheme for the discrete memoryless two-user
multi-access channel (MAC) with rate-limited feedback. Our scheme combines
ideas from the Venkataramanan-Pradhan scheme for perfect feedback with ideas
from the Shaviv-Steinberg scheme for rate-limited feedback.
  Our achievable region includes the Shaviv-Steinberg achievable region and
this inclusion can be strict. For general MACs and for sufficiently large
feedback rates, our scheme outperforms the Shaviv-Steinberg scheme as it
achieves the same rate region as the Venkataramanan-Pradhan scheme for perfect
feedback (which cannot be achieved by the Shaviv-Steinberg scheme).
Furthermore, we numerically evaluate our achievable region with a specific
(Gaussian) choice of random variables for the memoryless two-user Gaussian MAC.
Our simulation results show that for some parameters of the Gaussian MAC and
the feedback rate, our scheme achieves a strictly larger sum-rate than the
Shaviv-Steinberg scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03267</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03267</id><created>2015-03-11</created><authors><author><keyname>Jannach</keyname><forenames>Dietmar</forenames></author><author><keyname>Schmitz</keyname><forenames>Thomas</forenames></author></authors><title>Using Calculation Fragments for Spreadsheet Testing and Debugging</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of automated techniques and tools were proposed in the research
literature over the years which aim to support the spreadsheet developer in the
process of testing and debugging a faulty spreadsheet. One underlying
assumption of many of these approaches is that the spreadsheet developer is
capable of providing test cases or is at least reliably able to determine
whether a calculated value in a certain cell is correct given the current set
of inputs.
  Since real-world spreadsheets can be complex, we argue that these assumptions
might be too strong in some situations. We therefore propose to support the
user during testing and debugging by automatically computing spreadsheet
fragments of manageable size. The spreadsheet developer can then verify the
correctness of a smaller set of formulas for which the calculated output can be
more easily validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03270</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03270</id><created>2015-03-11</created><authors><author><keyname>Bhalla</keyname><forenames>Vandna</forenames></author><author><keyname>Chaudhury</keyname><forenames>Santanu</forenames></author><author><keyname>Jain</keyname><forenames>Arihant</forenames></author></authors><title>A Novel Hybrid CNN-AIS Visual Pattern Recognition Engine</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Machine learning methods are used today for most recognition problems.
Convolutional Neural Networks (CNN) have time and again proved successful for
many image processing tasks primarily for their architecture. In this paper we
propose to apply CNN to small data sets like for example, personal albums or
other similar environs where the size of training dataset is a limitation,
within the framework of a proposed hybrid CNN-AIS model. We use Artificial
Immune System Principles to enhance small size of training data set. A layer of
Clonal Selection is added to the local filtering and max pooling of CNN
Architecture. The proposed Architecture is evaluated using the standard MNIST
dataset by limiting the data size and also with a small personal data sample
belonging to two different classes. Experimental results show that the proposed
hybrid CNN-AIS based recognition engine works well when the size of training
data is limited in size
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03275</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03275</id><created>2015-03-11</created><authors><author><keyname>Radford</keyname><forenames>William</forenames></author><author><keyname>Gall&#xe9;</keyname><forenames>Matthias</forenames></author></authors><title>&quot;Roles for the boys?&quot; Mining cast lists for gender and role
  distributions over time</title><categories>cs.CY</categories><acm-class>I.2.7</acm-class><doi>10.1145/2740908.2743056</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Film and television play an important role in popular culture, however
studies that require watching and annotating video are time-consuming and
expensive to run at scale. We explore information mined from media database
cast lists to explore onscreen gender depictions and how they change over time.
We find differences between web-mediated onscreen gender proportions and those
from US Census data. We propose these methodologies are a useful adjunct to
traditional analysis that allow researchers to explore the relationship between
online and onscreen gender depictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03278</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03278</id><created>2015-03-11</created><updated>2015-10-05</updated><authors><author><keyname>Brodu</keyname><forenames>Nicolas</forenames></author><author><keyname>Yahia</keyname><forenames>Hussein</forenames></author></authors><title>Stochastic Texture Difference for Scale-Dependent Data Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces the Stochastic Texture Difference method for
analyzing data at prescribed spatial and value scales. This method relies on
constrained random walks around each pixel, describing how nearby image values
typically evolve on each side of this pixel. Textures are represented as
probability distributions of such random walks, so a texture difference
operator is statistically defined as a distance between these distributions in
a suitable reproducing kernel Hilbert space. The method is thus not limited to
scalar pixel values: any data type for which a kernel is available may be
considered, from color triplets and multispectral vector data to strings,
graphs, and more. By adjusting the size of the neighborhoods that are compared,
the method is implicitly scale-dependent. It is also able to focus on either
small changes or large gradients. We demonstrate how it can be used to infer
spatial and data value characteristic scales in measured signals and natural
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03283</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03283</id><created>2015-03-11</created><authors><author><keyname>Venkateswarlu</keyname><forenames>Ayineedi</forenames></author><author><keyname>Sarkar</keyname><forenames>Santanu</forenames></author><author><keyname>Mali</keyname><forenames>A. Sai</forenames></author></authors><title>On Acyclic Edge-Coloring of Complete Bipartite Graphs</title><categories>cs.DM math.CO</categories><comments>17 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acyclic edge-coloring of a graph is a proper edge-coloring without
bichromatic ($2$-colored) cycles. The acyclic chromatic index of a graph $G$,
denoted by $a'(G)$, is the least integer $k$ such that $G$ admits an acyclic
edge-coloring using $k$ colors. Let $\Delta = \Delta(G)$ denote the maximum
degree of a vertex in a graph $G$. A complete bipartite graph with $n$ vertices
on each side is denoted by $K_{n,n}$. Basavaraju, Chandran and Kummini proved
that $a'(K_{n,n}) \ge n+2 = \Delta + 2$ when $n$ is odd. Basavaraju and
Chandran provided an acyclic edge-coloring of $K_{p,p}$ using $p+2$ colors and
thus establishing $a'(K_{p,p}) = p+2 = \Delta + 2$ when $p$ is an odd prime.
The main tool in their approach is perfect $1$-factorization of $K_{p,p}$.
Recently, following their approach, Venkateswarlu and Sarkar have shown that
$K_{2p-1,2p-1}$ admits an acyclic edge-coloring using $2p+1$ colors which
implies that $a'(K_{2p-1,2p-1}) = 2p+1 = \Delta + 2$, where $p$ is an odd
prime. In this paper, we generalize this approach and present a general
framework to possibly get an acyclic edge-coloring of $K_{n,n}$ which possess a
perfect $1$-factorization using $n+2 = \Delta+2$ colors. In this general
framework, we show that $K_{p^2,p^2}$ admits an acyclic edge-coloring using
$p^2+2$ colors and thus establishing $a'(K_{p^2,p^2}) = p^2+2 = \Delta + 2$
when $p\ge 5$ is an odd prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03284</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03284</id><created>2015-03-11</created><authors><author><keyname>Dazzi</keyname><forenames>Patrizio</forenames></author></authors><title>Tools and Models for High Level Parallel and Grid Programming</title><categories>cs.DC cs.SE</categories><comments>PhD Thesis, 2008, IMT Institute for Advanced Studies, Lucca. arXiv
  admin note: text overlap with arXiv:1002.2722 by other authors</comments><doi>10.6092/imtlucca/e-theses/12</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  When algorithmic skeletons were first introduced by Cole in late 1980 the
idea had an almost immediate success. The skeletal approach has been proved to
be effective when application algorithms can be expressed in terms of skeletons
composition. However, despite both their effectiveness and the progress made in
skeletal systems design and implementation, algorithmic skeletons remain absent
from mainstream practice. Cole and other researchers, focused the problem. They
recognized the issues affecting skeletal systems and stated a set of principles
that have to be tackled in order to make them more effective and to take
skeletal programming into the parallel mainstream. In this thesis we propose
tools and models for addressing some among the skeletal programming
environments issues. We describe three novel approaches aimed at enhancing
skeletons based systems from different angles. First, we present a model we
conceived that allows algorithmic skeletons customization exploiting the macro
data-flow abstraction. Then we present two results about the exploitation of
meta-programming techniques for the run-time generation and optimization of
macro data-flow graphs. In particular, we show how to generate and how to
optimize macro data-flow graphs accordingly both to programmers provided
non-functional requirements and to execution platform features. The last result
we present are the Behavioural Skeletons, an approach aimed at addressing the
limitations of skeletal programming environments when used for the development
of component-based Grid applications. We validated all the approaches
conducting several test, performed exploiting a set of tools we developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03287</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03287</id><created>2015-03-11</created><authors><author><keyname>Ginda</keyname><forenames>Michael</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Borner</keyname><forenames>Katy</forenames></author></authors><title>Modelling the Structure and Dynamics of Science Using Books</title><categories>cs.DL physics.soc-ph</categories><comments>data and large scale maps http://cns.iu.edu/2015-ModSci.html, Ginda,
  Michael, Andrea Scharnhorst, and Katy B\&quot;orner. &quot;Modelling Science&quot;. In
  Theories of Informetrics: A Festschrift in Honor of Blaise Cronin, edited by
  Sugimoto, Cassidy. Munich: De Gruyter Saur</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific research is a major driving force in a knowledge based economy.
Income, health and wellbeing depend on scientific progress. The better we
understand the inner workings of the scientific enterprise, the better we can
prompt, manage, steer, and utilize scientific progress. Diverse indicators and
approaches exist to evaluate and monitor research activities, from calculating
the reputation of a researcher, institution, or country to analyzing and
visualizing global brain circulation. However, there are very few predictive
models of science that are used by key decision makers in academia, industry,
or government interested to improve the quality and impact of scholarly
efforts. We present a novel 'bibliographic bibliometric' analysis which we
apply to a large collection of books relevant for the modelling of science. We
explain the data collection together with the results of the data analyses and
visualizations. In the final section we discuss how the analysis of books that
describe different modelling approaches can inform the design of new models of
science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03289</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03289</id><created>2015-03-11</created><authors><author><keyname>Ma</keyname><forenames>Leanne</forenames></author><author><keyname>Matsuzawa</keyname><forenames>Yoshiaki</forenames></author><author><keyname>Kici</keyname><forenames>Derya</forenames></author><author><keyname>Scardamalia</keyname><forenames>Marlene</forenames></author></authors><title>An Exploration of Rotating Leadership in a Knowledge Building Community</title><categories>cs.SI</categories><comments>Proceedings of the 5th International Conference on Collaborative
  Innovation Networks COINs15, Tokyo, Japan March 12-14, 2015
  (arXiv:1502.01142)</comments><report-no>coins15/2015/24</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study aims to investigate the COINs concept of rotating leadership
within a Knowledge Building context. Individual and group level leadership
patterns in a grade 4 science class were explored through temporal
visualization of betweenness centrality. Results indicate that the student
network was relatively decentralized, with almost all students leading the
group at different points in time. Rotating leadership appears to be an
emergent phenomenon of Knowledge Building, and we suggest that it has the
potential to be an indicator of collective cognitive responsibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03291</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03291</id><created>2015-03-11</created><authors><author><keyname>Pasdeloup</keyname><forenames>Bastien</forenames></author><author><keyname>Alami</keyname><forenames>R&#xe9;da</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Toward An Uncertainty Principle For Weighted Graphs</title><categories>cs.DM</categories><comments>Submitted to EUSIPCO 2015, 5 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The uncertainty principle states that a signal cannot be localized both in
time and frequency. With the aim of extending this result to signals on graphs,
Agaskar&amp;Lu introduce notions of graph and spectral spreads. They show that a
graph uncertainty principle holds for some families of unweighted graphs. This
principle states that a signal cannot be simultaneously localized both in graph
and spectral domains. In this paper, we aim to extend their work to weighted
graphs. We show that a naive extension of their definitions leads to
inconsistent results such as discontinuity of the graph spread when regarded as
a function of the graph structure. To circumvent this problem, we propose
another definition of graph spread that relies on an inverse similarity matrix.
We also discuss the choice of the distance function that appears in this
definition. Finally, we compute and plot uncertainty curves for families of
weighted graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03292</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03292</id><created>2015-03-11</created><updated>2015-06-14</updated><authors><author><keyname>Hooshmand</keyname><forenames>Reza</forenames></author></authors><title>Improving GGH Public Key Scheme Using Low Density Lattice Codes</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goldreich-Goldwasser-Halevi (GGH) public key cryptosystem is an instance of
lattice-based cryptosystems whose security is based on the hardness of lattice
problems. In fact, GGH cryptosystem is the lattice version of the first
code-based cryptosystem, proposed by McEliece. However, it has a number of
drawbacks such as; large public key length and low security level. On the other
hand, Low Density Lattice Codes (LDLCs) are the practical classes of lattice
codes which can achieve capacity on the additive white Gaussian noise (AWGN)
channel with low complexity decoding algorithm. This paper introduces a public
key cryptosystem based on LDLCs to withdraw the drawbacks of GGH cryptosystem.
To reduce the key length, we employ the generator matrix of the used LDLC in
Hermite normal form (HNF) as the public key. Also, by exploiting the linear
decoding complexity of the used LDLC, the decryption complexity is decreased
compared with GGH cryptosystem. These increased efficiencies allow us to use
the bigger values of security parameters. Moreover, we exploit the special
Gaussian vector whose variance is upper bounded by the Poltyrev limit as the
perturbation vector. These techniques can resist the proposed scheme against
the most efficient attacks to the GGH-like cryptosystems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03293</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03293</id><created>2015-03-11</created><authors><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>Freire</keyname><forenames>E. S. V.</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Fourier Codes</title><categories>cs.IT cs.DM math.IT</categories><comments>6 pages, 2 tables. In: 10th International Symposium on Communication
  Theory and Applications 2009, Ambleside, Lake District, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new family of error-correcting codes, called Fourier codes, is introduced.
The code parity-check matrix, dimension and an upper bound on its minimum
distance are obtained from the eigenstructure of the Fourier number theoretic
transform. A decoding technique for such codes is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03308</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03308</id><created>2015-03-11</created><authors><author><keyname>Alaka</keyname><forenames>S. P.</forenames></author><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Generalized Spatial Modulation in Indoor Wireless Visible Light
  Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the performance of generalized spatial
modulation (GSM) in indoor wireless visible light communication (VLC) systems.
GSM uses $N_t$ light emitting diodes (LED), but activates only $N_a$ of them at
a given time. Spatial modulation and spatial multiplexing are special cases of
GSM with $N_{a}=1$ and $N_{a}=N_t$, respectively. We first derive an analytical
upper bound on the bit error rate (BER) for maximum likelihood (ML) detection
of GSM in VLC systems. Analysis and simulation results show that the derived
upper bound is very tight at medium to high signal-to-noise ratios (SNR). The
channel gains and channel correlations influence the GSM performance such that
the best BER is achieved at an optimum LED spacing. Also, for a fixed
transmission efficiency, the performance of GSM in VLC improves as the
half-power semi-angle of the LEDs is decreased. We then compare the performance
of GSM in VLC systems with those of other MIMO schemes such as spatial
multiplexing (SMP), space shift keying (SSK), generalized space shift keying
(GSSK), and spatial modulation (SM). Analysis and simulation results show that
GSM in VLC outperforms the other considered MIMO schemes at moderate to high
SNRs; for example, for 8 bits per channel use, GSM outperforms SMP and GSSK by
about 21 dB, and SM by about 10 dB at $10^{-4}$ BER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03309</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03309</id><created>2015-03-11</created><authors><author><keyname>Dr&#xe4;xler</keyname><forenames>Martin</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>Dynamic Backhaul Network Configuration in SDN-based Cloud RANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coordination of base stations in mobile access networks is an important
approach to reduce harmful interference and to deliver high data rates to the
users. Such coordination mechanisms, like Coordinated Multi-Point (CoMP) where
multiple BSs transmit data to a user equipment, can be easily implemented when
centralizing the data processing of the base stations, known as Cloud RAN.
  This centralization also imposes significant requirements on the backhaul
network for high capacities and low latencies for the connections to the base
stations. These requirements can be mitigated by (a) a flexible placement of
the base station data processing functionality and by (b) dynamically assigning
backhaul network resources. We show how these two techniques increase the
feasibility of base station coordination in dense mobile access networks by
using a heuristic algorithm.
  We furthermore present a prototype implementation of our approach based on
software defined networking (SDN) with OpenDaylight and Maxinet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03314</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03314</id><created>2015-03-11</created><authors><author><keyname>Feller</keyname><forenames>Christian</forenames></author><author><keyname>Ebenbauer</keyname><forenames>Christian</forenames></author></authors><title>Relaxed Logarithmic Barrier Function Based Model Predictive Control of
  Linear Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the use of relaxed logarithmic barrier
functions in the context of linear model predictive control. We present results
that allow to guarantee asymptotic stability of the corresponding closed-loop
system, and discuss further properties like performance and constraint
satisfaction in dependence of the underlying relaxation. The proposed
stabilizing MPC schemes are not necessarily based on an explicit terminal set
or state constraint and allow to characterize the stabilizing control input
sequence as the minimizer of a globally defined, continuously differentiable,
and strongly convex function. The results are illustrated by means of a
numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03318</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03318</id><created>2015-03-11</created><updated>2015-08-18</updated><authors><author><keyname>Bo&#x142;t</keyname><forenames>Witold</forenames></author><author><keyname>Baetens</keyname><forenames>Jan M.</forenames></author><author><keyname>DeBaets</keyname><forenames>Bernard</forenames></author></authors><title>On the decomposition of stochastic cellular automata</title><categories>cs.FL math.DS nlin.CG</categories><comments>Submitted to Journal of Computation Science, Special Issue on
  Cellular Automata Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present two interesting properties of stochastic cellular
automata that can be helpful in analyzing the dynamical behavior of such
automata. The first property allows for calculating cell-wise probability
distributions over the state set of a stochastic cellular automaton, i.e.
images that show the average state of each cell during the evolution of the
stochastic cellular automaton. The second property shows that stochastic
cellular automata are equivalent to so-called stochastic mixtures of
deterministic cellular automata. Based on this property, any stochastic
cellular automaton can be decomposed into a set of deterministic cellular
automata, each of which contributes to the behavior of the stochastic cellular
automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03321</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03321</id><created>2015-03-11</created><updated>2015-05-22</updated><authors><author><keyname>Shalygo</keyname><forenames>Yuri</forenames></author></authors><title>The Kinetic Basis of Morphogenesis</title><categories>cs.FL</categories><comments>8 pages. Submitted to the 13th European Conference on Artificial Life
  (ECAL-2015) on March 10, 2015. Accepted on April 28, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown recently (Shalygo, 2014) that stationary and dynamic
patterns can arise in the proposed one-component model of the analog
(continuous state) kinetic automaton, or kinon for short, defined as a
reflexive dynamical system with active transport. This paper presents
extensions of the model, which increase further its complexity and tunability,
and shows that the extended kinon model can produce spatio-temporal patterns
pertaining not only to pattern formation but also to morphogenesis in real
physical and biological systems. The possible applicability of the model to
morphogenetic engineering and swarm robotics is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03324</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03324</id><created>2015-03-11</created><updated>2016-01-23</updated><authors><author><keyname>Drabent</keyname><forenames>W&#x142;odzimierz</forenames></author></authors><title>On definite program answers and least Herbrand models</title><categories>cs.LO</categories><comments>11 pages. This version - small changes, version 2 - technical core of
  the paper corrected, simplified and improved. To appear in Theory and
  Practice of Logic Programming (TPLP)</comments><msc-class>68N17, 03B10</msc-class><acm-class>D.1.6, F.3.2, F.4.0, D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sufficient and necessary condition is given under which least Herbrand
models exactly characterize the answers of definite clause programs.
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03345</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03345</id><created>2015-03-11</created><updated>2015-10-27</updated><authors><author><keyname>Chappelon</keyname><forenames>Jonathan</forenames></author><author><keyname>Larsson</keyname><forenames>Urban</forenames></author><author><keyname>Matsuura</keyname><forenames>Akihiro</forenames></author></authors><title>Two-Player Tower of Hanoi</title><categories>cs.GT cs.DM</categories><comments>15 pages, 7 figures, 1 table</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tower of Hanoi game is a classical puzzle in recreational mathematics,
which also has a strong record in pure mathematics. In a borderland between
these two areas we find the characterization of the minimal number of moves,
which is $2^n-1$, to transfer a tower of $n$ disks. But there are also other
variations to the game, involving for example move edges weighted by real
numbers. This gives rise to a similar type of problem, but where the final
score seeks to be optimized. We study extensions of the one-player setting to
two players, invoking classical winning conditions in combinatorial game theory
such as the player who moves last wins, or the highest score wins. Here we
solve both these winning conditions on three heaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03349</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03349</id><created>2015-03-11</created><updated>2015-05-14</updated><authors><author><keyname>Sanl&#x131;</keyname><forenames>Ceyda</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>Local variation of hashtag spike trains and popularity in Twitter</title><categories>cs.SI cs.CY</categories><comments>7 pages, 7 figures</comments><journal-ref>PLoS ONE 10(7): e0131704 (2015)</journal-ref><doi>10.1371/journal.pone.0131704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We draw a parallel between hashtag time series and neuron spike trains. In
each case, the process presents complex dynamic patterns including temporal
correlations, burstiness, and all other types of nonstationarity. We propose
the adoption of the so-called local variation in order to uncover salient
dynamics, while properly detrending for the time-dependent features of a
signal. The methodology is tested on both real and randomized hashtag spike
trains, and identifies that popular hashtags present regular and so less bursty
behavior, suggesting its potential use for predicting online popularity in
social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03351</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03351</id><created>2015-03-11</created><updated>2015-07-27</updated><authors><author><keyname>Yin</keyname><forenames>Yitong</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>Sampling colorings almost uniformly in sparse random graphs</title><categories>cs.DS</categories><comments>The paper has been withdrawn by the authors since the result has been
  generalized and incorporated in their new work, arXiv:1507.07225</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of sampling proper $q$-colorings from uniform distribution has
been extensively studied. Most of existing samplers require $q\ge \alpha
\Delta+\beta$ for some constants $\alpha$ and $\beta$, where $\Delta$ is the
maximum degree of the graph. The problem becomes more challenging when the
underlying graph has unbounded degree since even the decision of
$q$-colorability becomes nontrivial in this situation. The Erd\H{o}s-R\'{e}nyi
random graph $\mathcal{G}(n,d/n)$ is a typical class of such graphs and has
received a lot of recent attention. In this case, the performance of a sampler
is usually measured by the relation between $q$ and the average degree $d$. We
are interested in the fully polynomial-time almost uniform sampler (FPAUS) and
the state-of-the-art with such sampler for proper $q$-coloring on
$\mathcal{G}(n,d/n)$ requires that $q\ge 5.5d$.
  In this paper, we design an FPAUS for proper $q$-colorings on
$\mathcal{G}(n,d/n)$ by requiring that $q\ge 3d+O(1)$, which improves the best
bound for the problem so far. Our sampler is based on the spatial mixing
property of $q$-coloring on random graphs. The core of the sampler is a
deterministic algorithm to estimate the marginal probability on blocks, which
is computed by a novel block version of recursion for $q$-coloring on unbounded
degree graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03354</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03354</id><created>2015-03-11</created><authors><author><keyname>Podlaski</keyname><forenames>Krzysztof</forenames></author><author><keyname>H&#x142;oba&#x17c;</keyname><forenames>Artur</forenames></author><author><keyname>Milczarski</keyname><forenames>Piotr</forenames></author></authors><title>New Method for Public Key Distribution Based on Social Networks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of communication in everyday life becomes very important. On the
other hand, all existing encryption protocols require from user additional
knowledge end resources. In this paper we discuss the problem of public key
distribution between interested parties. We propose to use a popular social
media as a channel to publish public keys. This way of key distribution allows
also easily connect owner of the key with real person institution (what is not
always easy). Recognizing that the mobile devices become the main tool of
communication, we present description of mobile application that uses proposed
security methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03355</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03355</id><created>2015-03-11</created><authors><author><keyname>Papalexakis</keyname><forenames>Evangelos E.</forenames></author></authors><title>Automatic Unsupervised Tensor Mining with Quality Assessment</title><categories>stat.ML cs.LG cs.NA stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular tool for unsupervised modelling and mining multi-aspect data is
tensor decomposition. In an exploratory setting, where and no labels or ground
truth are available how can we automatically decide how many components to
extract? How can we assess the quality of our results, so that a domain expert
can factor this quality measure in the interpretation of our results? In this
paper, we introduce AutoTen, a novel automatic unsupervised tensor mining
algorithm with minimal user intervention, which leverages and improves upon
heuristics that assess the result quality. We extensively evaluate AutoTen's
performance on synthetic data, outperforming existing baselines on this very
hard problem. Finally, we apply AutoTen on a variety of real datasets,
providing insights and discoveries. We view this work as a step towards a fully
automated, unsupervised tensor mining tool that can be easily adopted by
practitioners in academia and industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03359</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03359</id><created>2015-03-11</created><authors><author><keyname>Sabag</keyname><forenames>Oron</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>The Feedback Capacity of the $(1,\infty)$-RLL Input-Constrained Erasure
  Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The input-constrained erasure channel with feedback is considered, where the
binary input sequence contains no consecutive ones, i.e., it satisfies the
$(1,\infty)$-RLL constraint. We derive the capacity for this setting, which can
be expressed as $C_{\epsilon}=\max_{0 \leq p \leq
\frac{1}{2}}\frac{H_{b}(p)}{p+\frac{1}{1-\epsilon}}$, where $\epsilon$ is the
erasure probability and $ H_{b}(\cdot)$ is the binary entropy function.
Moreover, we prove that a-priori knowledge of the erasure at the encoder does
not increase the feedback capacity. The feedback capacity was calculated using
an equivalent dynamic programming (DP) formulation with an optimal
average-reward that is equal to the capacity. Furthermore, we obtained an
optimal encoding procedure from the solution of the DP, leading to a
capacity-achieving, zero-error coding scheme for our setting. DP is thus shown
to be a tool not only for solving optimization problems such as capacity
calculation, but also for constructing optimal coding schemes. The derived
capacity expression also serves as the only non-trivial upper bound known on
the capacity of the input-constrained erasure channel without feedback, a
problem that is still open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03361</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03361</id><created>2015-03-11</created><authors><author><keyname>Kim</keyname><forenames>Su Min</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author><author><keyname>Sung</keyname><forenames>Dan Keun</forenames></author></authors><title>Joint Link Adaptation and User Scheduling with HARQ in Multi-Cell
  Environments</title><categories>cs.IT math.IT</categories><comments>Accepted for publication to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-cell interference (ICI) is one of the most critical factors affecting
performance of cellular networks. In this paper, we investigate a joint link
adaptation and user scheduling problem for multi-cell downlink employing HARQ
techniques, where the ICI exists among cells. We first propose an approximation
method on aggregated ICI for analyzing an effective
signal-to-interference-and-noise ratio (SINR) with the HARQ technique at users,
named identical path-loss approximation (IPLA). Based on the proposed IPLA, we
propose a transmission rate selection algorithm maximizing an expected
throughput at each user. We also propose a simple but effective cross-layer
framework jointly combining transmission rate adaptation and user scheduling
techniques, considering both HARQ and ICI. It is shown that statistical
distribution of the effective SINR at users based on the IPLA agrees well with
the empirical distribution, while the conventional Gaussian approximation (GA)
does not work well in the case that dominant ICIs exist. Thus, IPLA enables
base stations to choose more accurate transmission rates. Furthermore, the
proposed IPLA-based cross-layer policy outperforms existing policies in terms
of both system throughput and user fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03366</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03366</id><created>2015-03-11</created><authors><author><keyname>Suryaprakash</keyname><forenames>Vinay</forenames></author><author><keyname>Rost</keyname><forenames>Peter</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>Are Heterogeneous Cloud-Based Radio Access Networks Cost Effective?</title><categories>cs.NI</categories><comments>accepted for publication, 2015 IEEE Journal on Selected Areas in
  Communication</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile networks of the future are predicted to be much denser than today's
networks in order to cater to increasing user demands. In this context, cloud
based radio access networks have garnered significant interest as a cost
effective solution to the problem of coping with denser networks and providing
higher data rates. However, to the best knowledge of the authors, a
quantitative analysis of the cost of such networks is yet to be undertaken.
This paper develops a theoretic framework that enables computation of the
deployment cost of a network (modeled using various spatial point processes) to
answer the question posed by the paper's title. Then, the framework obtained is
used along with a complexity model, which enables computing the information
processing costs of a network, to compare the deployment cost of a cloud based
network against that of a traditional LTE network, and to analyze why they are
more economical. Using this framework and an exemplary budget, this paper shows
that cloud-based radio access networks require approximately 10 to 15% less
capital expenditure per square kilometer than traditional LTE networks. It also
demonstrates that the cost savings depend largely on the costs of base stations
and the mix of backhaul technologies used to connect base stations with data
centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03378</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03378</id><created>2015-03-11</created><authors><author><keyname>Saar</keyname><forenames>T&#xf5;nis</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author><author><keyname>Kaljuve</keyname><forenames>Marti</forenames></author><author><keyname>Semenenko</keyname><forenames>Nataliia</forenames></author></authors><title>Browserbite: Cross-Browser Testing via Image Processing</title><categories>cs.SE</categories><comments>28 pages, 16 figures</comments><acm-class>D.2.5; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-browser compatibility testing is concerned with identifying perceptible
differences in the way a Web page is rendered across different browsers or
configurations thereof. Existing automated cross-browser compatibility testing
methods are generally based on Document Object Model (DOM) analysis, or in some
cases, a combination of DOM analysis with screenshot capture and image
processing. DOM analysis however may miss incompatibilities that arise not
during DOM construction, but rather during rendering. Conversely, DOM analysis
produces false alarms because different DOMs may lead to identical or
sufficiently similar renderings. This paper presents a novel method for
cross-browser testing based purely on image processing. The method relies on
image segmentation to extract regions from a Web page and computer vision
techniques to extract a set of characteristic features from each region.
Regions extracted from a screenshot taken on a baseline browser are compared
against regions extracted from the browser under test based on characteristic
features. A machine learning classifier is used to determine if differences
between two matched regions should be classified as an incompatibility. An
evaluation involving 140 pages shows that the proposed method achieves an
F-score exceeding 0.9, outperforming a state-of-the-art cross-browser testing
tool based on DOM analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03383</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03383</id><created>2015-03-08</created><authors><author><keyname>Chen</keyname><forenames>Yannan</forenames></author><author><keyname>Qi</keyname><forenames>Liqun</forenames></author><author><keyname>Wang</keyname><forenames>Qun</forenames></author></authors><title>An Explicit SOS Decomposition of A Fourth Order Four Dimensional Hankel
  Tensor with A Symmetric Generating Vector</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we construct explicit SOS decomposition of A Fourth Order Four
Dimensional Hankel Tensor with A Symmetric Generating Vector, at the critical
value. This is a supplementary note to Paper [3].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03388</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03388</id><created>2015-03-11</created><authors><author><keyname>Galloway</keyname><forenames>Kevin S.</forenames></author><author><keyname>Dey</keyname><forenames>Biswadip</forenames></author></authors><title>Station Keeping through Beacon-referenced Cyclic Pursuit</title><categories>cs.SY cs.RO</categories><msc-class>93A14, 93C85</msc-class><acm-class>I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a modification of cyclic constant bearing (CB)
pursuit in a multi-agent system in which each agent pays attention to a
neighbor and a beacon. The problem admits shape equilibria with collective
circling about the beacon, with the circling radius and angular separation of
agents determined by choice of parameters in the feedback law. Stability of
circling shape equilibria is shown for a 2-agent system, and the results are
demonstrated on a collective of mobile robots tracked by a motion capture
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03392</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03392</id><created>2015-03-11</created><updated>2015-07-24</updated><authors><author><keyname>Christodoulou</keyname><forenames>George</forenames></author><author><keyname>Sgouritsa</keyname><forenames>Alkmini</forenames></author></authors><title>Designing Networks with Good Equilibria under Uncertainty</title><categories>cs.GT</categories><comments>This version has additional results about stochastic input</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing network cost-sharing protocols with good
equilibria under uncertainty. The underlying game is a multicast game in a
rooted undirected graph with nonnegative edge costs. A set of k terminal
vertices or players need to establish connectivity with the root. The social
optimum is the Minimum Steiner Tree. We are interested in situations where the
designer has incomplete information about the input. We propose two different
models, the adversarial and the stochastic. In both models, the designer has
prior knowledge of the underlying metric but the requested subset of the
players is not known and is activated either in an adversarial manner
(adversarial model) or is drawn from a known probability distribution
(stochastic model).
  In the adversarial model, the designer's goal is to choose a single,
universal protocol that has low Price of Anarchy (PoA) for all possible
requested subsets of players. The main question we address is: to what extent
can prior knowledge of the underlying metric help in the design? We first
demonstrate that there exist graphs (outerplanar) where knowledge of the
underlying metric can dramatically improve the performance of good network
design. Then, in our main technical result, we show that there exist graph
metrics, for which knowing the underlying metric does not help and any
universal protocol has PoA of $\Omega(\log k)$, which is tight. We attack this
problem by developing new techniques that employ powerful tools from extremal
combinatorics, and more specifically Ramsey Theory in high dimensional
hypercubes.
  Then we switch to the stochastic model, where each player is independently
activated. We show that there exists a randomized ordered protocol that
achieves constant PoA. By using standard derandomization techniques, we produce
a deterministic ordered protocol with constant PoA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03394</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03394</id><created>2015-03-10</created><authors><author><keyname>Kiermaier</keyname><forenames>Michael</forenames></author><author><keyname>Wassermann</keyname><forenames>Alfred</forenames></author><author><keyname>Zwanzger</keyname><forenames>Johannes</forenames></author></authors><title>New upper bounds on binary linear codes and a \$\mathbb Z_4\$-code with
  a better-than-linear Gray image</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using integer linear programming and table-lookups we prove that there is no
binary linear [1988,12,992] code. As a byproduct, the non-existence of binary
linear [324,10,160], [356,10,176], [772,11,384], and [836,11,416] codes is
shown. On the other hand, there exists a linear (994,4^6,992) code over Z_4.
Its Gray image is a binary non-linear (1988,2^12,992) code. Therefore, we can
add one more code to the small list of Z_4-codes for which it is known that the
Gray image is better than any binary linear code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03400</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03400</id><created>2015-03-11</created><updated>2015-08-30</updated><authors><author><keyname>Chand</keyname><forenames>Dhruv</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Karthik</forenames></author><author><keyname>KK</keyname><forenames>Nisha</forenames></author><author><keyname>Sinha</keyname><forenames>Mudit</forenames></author><author><keyname>Sriram</keyname><forenames>Shreya</forenames></author></authors><title>Get 'em Moles! : Learning Spelling and Pronunciation through an
  Educational Game</title><categories>cs.HC</categories><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Get 'em Moles! is a single-player educational game inspired by the classic
arcade game Whac-A-Mole. Primarily designed for touchscreen devices, Get 'em
Moles! aims to teach English spelling and pronunciation through engaging game
play. This paper describes the game, design decisions in the form of elements
that support learning, preliminary play-testing results, and future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03401</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03401</id><created>2015-03-11</created><authors><author><keyname>Amalfitano</keyname><forenames>Domenico</forenames></author><author><keyname>Amatucci</keyname><forenames>Nicola</forenames></author><author><keyname>De Simone</keyname><forenames>Vincenzo</forenames></author><author><keyname>Fasolino</keyname><forenames>Anna Rita</forenames></author><author><keyname>Tramontana</keyname><forenames>Porfirio</forenames></author></authors><title>Toward Reverse Engineering of VBA Based Excel Spreadsheet Applications</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Modern spreadsheet systems can be used to implement complex spreadsheet
applications including data sheets, customized user forms and executable
procedures written in a scripting language. These applications are often
developed by practitioners that do not follow any software engineering practice
and do not produce any design documentation. Thus, spreadsheet applications may
be very difficult to be maintained or restructured. In this position paper we
present in a nutshell two reverse engineering techniques and a tool that we are
currently realizing for the abstraction of conceptual data models and business
logic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03403</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03403</id><created>2015-03-11</created><updated>2015-08-30</updated><authors><author><keyname>Chand</keyname><forenames>Dhruv</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Karthik</forenames></author><author><keyname>KK</keyname><forenames>Nisha</forenames></author><author><keyname>Sinha</keyname><forenames>Mudit</forenames></author><author><keyname>Sriram</keyname><forenames>Shreya</forenames></author></authors><title>Bublz! : Playing with Bubbles to Develop Mathematical Thinking</title><categories>cs.HC math.HO</categories><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We encounter mathematical problems in various forms in our lives, thus making
mathematical thinking an important human ability. In this paper, we present
Bublz!, a simple, click-driven game for children to engage in and develop
mathematical thinking in an enjoyable manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03417</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03417</id><created>2015-03-11</created><updated>2015-10-17</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>Upper Bounds on the Relative Entropy and R\'enyi Divergence as a
  Function of Total Variation Distance for Finite Alphabets</title><categories>cs.IT math.IT math.PR</categories><comments>Proceedings of the 2015 IEEE Information Theory Workshop, pp.
  214-218, Jeju, Korea, October 2015. This presents in part the work which is
  available at arXiv:1508.00335</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new upper bound on the relative entropy is derived as a function of the
total variation distance for probability measures defined on a common finite
alphabet. The bound improves a previously reported bound by Csisz\'ar and
Talata. It is further extended to an upper bound on the R\'enyi divergence of
an arbitrary non-negative order (including $\infty$) as a function of the total
variation distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03429</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03429</id><created>2015-03-11</created><updated>2015-09-25</updated><authors><author><keyname>Ngo</keyname><forenames>Dat Tien</forenames></author><author><keyname>Park</keyname><forenames>Sanghuyk</forenames></author><author><keyname>Jorstad</keyname><forenames>Anne</forenames></author><author><keyname>Crivellaro</keyname><forenames>Alberto</forenames></author><author><keyname>Yoo</keyname><forenames>Chang</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Dense image registration and deformable surface reconstruction in
  presence of occlusions and minimal texture</title><categories>cs.CV</categories><comments>In Proceedings of International Conference on Computer Vision, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deformable surface tracking from monocular images is well-known to be
under-constrained. Occlusions often make the task even more challenging, and
can result in failure if the surface is not sufficiently textured. In this
work, we explicitly address the problem of 3D reconstruction of poorly
textured, occluded surfaces, proposing a framework based on a template-matching
approach that scales dense robust features by a relevancy score. Our approach
is extensively compared to current methods employing both local feature
matching and dense template alignment. We test on standard datasets as well as
on a new dataset (that will be made publicly available) of a sparsely textured,
occluded surface. Our framework achieves state-of-the-art results for both well
and poorly textured, occluded surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03430</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03430</id><created>2015-03-11</created><updated>2015-08-10</updated><authors><author><keyname>Feghali</keyname><forenames>Carl</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Paulusma</keyname><forenames>Daniel</forenames></author></authors><title>Kempe Equivalence of Colourings of Cubic Graphs</title><categories>cs.DM math.CO</categories><comments>very minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$ and a proper vertex colouring of $G$, a Kempe chain
is a subset of $V$ that induces a maximal connected subgraph of $G$ in which
every vertex has one of two colours. To make a Kempe change is to obtain one
colouring from another by exchanging the colours of vertices in a Kempe chain.
Two colourings are Kempe equivalent if each can be obtained from the other by a
series of Kempe changes. A conjecture of Mohar asserts that, for $k \geq 3$,
all $k$-colourings of $k$-regular graphs that are not complete are Kempe
equivalent. We address the case $k=3$ by showing that all $3$-colourings of a
cubic graph $G$ are Kempe equivalent unless $G$ is the complete graph $K_4$ or
the triangular prism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03438</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03438</id><created>2015-03-11</created><updated>2015-12-12</updated><authors><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Chintala</keyname><forenames>Soumith</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Piantino</keyname><forenames>Serkan</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Tygert</keyname><forenames>Mark</forenames></author></authors><title>A mathematical motivation for complex-valued convolutional networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>11 pages, 3 figures; this is the retitled version submitted to the
  journal, &quot;Neural Computation&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complex-valued convolutional network (convnet) implements the repeated
application of the following composition of three operations, recursively
applying the composition to an input vector of nonnegative real numbers: (1)
convolution with complex-valued vectors followed by (2) taking the absolute
value of every entry of the resulting vectors followed by (3) local averaging.
For processing real-valued random vectors, complex-valued convnets can be
viewed as &quot;data-driven multiscale windowed power spectra,&quot; &quot;data-driven
multiscale windowed absolute spectra,&quot; &quot;data-driven multiwavelet absolute
values,&quot; or (in their most general configuration) &quot;data-driven nonlinear
multiwavelet packets.&quot; Indeed, complex-valued convnets can calculate multiscale
windowed spectra when the convnet filters are windowed complex-valued
exponentials. Standard real-valued convnets, using rectified linear units
(ReLUs), sigmoidal (for example, logistic or tanh) nonlinearities, max.
pooling, etc., do not obviously exhibit the same exact correspondence with
data-driven wavelets (whereas for complex-valued convnets, the correspondence
is much more than just a vague analogy). Courtesy of the exact correspondence,
the remarkably rich and rigorous body of mathematical analysis for wavelets
applies directly to (complex-valued) convnets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03449</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03449</id><created>2015-03-11</created><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>The Value of Local Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. arXiv admin
  note: text overlap with arXiv:1301.5309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacity region of the two-user interference channel with local
delayed channel state information at the transmitters. In our model,
transmitters have local mismatched outdated knowledge of the channel gains. We
propose a transmission strategy that only relies on the delayed knowledge of
the outgoing links at each transmitter and achieves the outer-bound for the
scenario in which transmitters learn the entire channel state with delay. Our
result reveals the subset of the channel state information that affects the
capacity region the most.
  We also identify cases in which local delayed knowledge of the channel state
does not provide any gain over the zero knowledge assumption. To do so, we
revisit a long-known intuition about interference channels that as long as the
marginal distributions at the receivers are conserved, the capacity remains the
same. We take this intuition and impose a certain spatial correlation among
channel gains such that the marginal distributions remain unchanged. Then we
provide an outer-bound on the capacity region of the channel with correlation
that matches the capacity region when transmitters do not have access to
channel state information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03452</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03452</id><created>2015-03-11</created><authors><author><keyname>Fouce</keyname><forenames>Sergi Casanova</forenames></author><author><keyname>Puglisi</keyname><forenames>Silvia</forenames></author><author><keyname>Igartua</keyname><forenames>Monica Aguilar</forenames></author></authors><title>Design and implementation of an Android application (MobilitApp+) to
  analyze the mobility patterns of citizens in the Metropolitan Region of
  Barcelona</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In our project we have designed an Android application to obtain mobility
data of the citizens in the metropolitan area of Barcelona. Our implementation
synchronously obtains in background on the one hand, periodic location updates
and, on the other hand, the type of activity citizens are doing. At the end of
the day, all this data is processed and sent to a server where are stored to
obtain mobility patterns from citizens that could help to improve the current
transportation infrastructure. MobilitApp is fully functional and stable
although the results can be improved in some situations. In future releases we
will implement machine learning technics to obtain significant improvements,
especially in the activity recognition modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03460</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03460</id><created>2015-03-11</created><authors><author><keyname>Radi</keyname><forenames>Mohammed</forenames></author></authors><title>Efficient Service Broker Policy For Large-Scale Cloud Environments</title><categories>cs.DC</categories><journal-ref>(2015). International Journal of Computer Science Issues, 12(1):
  85-90</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms, policies, and methodologies are necessary to achieve high user
satisfaction and practical utilization in cloud computing by ensuring the
efficient and fair allocation of every computing resource. Whenever a new job
arrives in cloud environments, the service broker is responsible for selecting
the data center that will execute that job. Selecting data centers serves an
important function in enhancing the performance of a cloud environment. This
study proposes a new service broker policy for large-scale cloud applications
based on the round-robin algorithm. The proposed policy is implemented and
evaluated using a CloudAnalyst simulator. It is then compared with three
existing policies in terms of overall average response time by using different
virtual machine load balancing algorithms. Simulation results show that the
proposed policy improves the overall average response time relative to that of
the other policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03462</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03462</id><created>2015-03-11</created><updated>2015-12-09</updated><authors><author><keyname>Nivasch</keyname><forenames>Gabriel</forenames></author></authors><title>On the zone of a circle in an arrangement of lines</title><categories>cs.CG</categories><comments>More presentation improvements. 23 pages, 5 figures</comments><msc-class>52C45, 52C30, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal L$ be a set of $n$ lines in the plane, and let $C$ be a convex
curve in the plane, like a circle or a parabola. The &quot;zone&quot; of $C$ in $\mathcal
L$, denoted $\mathcal Z(C,\mathcal L)$, is defined as the set of all faces in
the arrangement $\mathcal A(\mathcal L)$ that are intersected by $C$.
Edelsbrunner et al. (1992) showed that the complexity (total number of edges or
vertices) of $\mathcal Z(C,\mathcal L)$ is at most $O(n\alpha(n))$, where
$\alpha$ is the inverse Ackermann function, by translating the sequence of
edges of $\mathcal Z(C,\mathcal L)$ into a sequence $S$ that avoids the
subsequence $ababa$. Whether the worst-case complexity of $\mathcal
Z(C,\mathcal L)$ is only linear is a longstanding open problem.
  In this paper we provide evidence that, if $C$ is a circle or a parabola,
then the zone of $C$ has at most linear complexity: We show that a certain
configuration of segments with endpoints on $C$ is impossible. As a
consequence, the Hart-Sharir sequences, which are essentially the only known
way to construct $ababa$-free sequences of superlinear length, cannot occur in
$S$.
  Hence, if it could be shown that every family of superlinear-length,
$ababa$-free sequences must eventually contain all Hart-Sharir sequences, that
would settle the zone problem for a circle/parabola.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03463</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03463</id><created>2015-03-11</created><authors><author><keyname>Maia</keyname><forenames>Pedro</forenames></author><author><keyname>Mendes</keyname><forenames>Jorge</forenames></author><author><keyname>Cunha</keyname><forenames>J&#xe1;come</forenames></author><author><keyname>Reb&#xea;lo</keyname><forenames>Henrique</forenames></author><author><keyname>Saraiva</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Towards the Design and Implementation of Aspect-Oriented Programming for
  Spreadsheets</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A spreadsheet usually starts as a simple and single-user software artifact,
but, as frequent as in other software systems, quickly evolves into a complex
system developed by many actors. Often, different users work on different
aspects of the same spreadsheet: while a secretary may be only involved in
adding plain data to the spreadsheet, an accountant may define new business
rules, while an engineer may need to adapt the spreadsheet content so it can be
used by other software systems. Unfortunately, spreadsheet systems do not offer
modular mechanisms, and as a consequence, some of the previous tasks may be
defined by adding intrusive &quot;code&quot; to the spreadsheet.
  In this paper we go through the design and implementation of an
aspect-oriented language for spreadsheets so that users can work on different
aspects of a spreadsheet in a modular way. For example, aspects can be defined
in order to introduce new business rules to an existing spreadsheet, or to
manipulate the spreadsheet data to be ported to another system. Aspects are
defined as aspect-oriented program specifications that are dynamically woven
into the underlying spreadsheet by an aspect weaver. In this aspect-oriented
style of spreadsheet development, different users develop, or reuse, aspects
without adding intrusive code to the original spreadsheet. Such code is
added/executed by the spreadsheet weaving mechanism proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03465</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03465</id><created>2015-03-11</created><updated>2015-11-04</updated><authors><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author><author><keyname>Kaser</keyname><forenames>Owen</forenames></author></authors><title>Faster 64-bit universal hashing using carry-less multiplications</title><categories>cs.DS</categories><doi>10.1007/s13389-015-0110-5</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Intel and AMD support the Carry-less Multiplication (CLMUL) instruction set
in their x64 processors. We use CLMUL to implement an almost universal 64-bit
hash family (CLHASH). We compare this new family with what might be the fastest
almost universal family on x64 processors (VHASH). We find that CLHASH is at
least 60% faster. We also compare CLHASH with a popular hash function designed
for speed (Google's CityHash). We find that CLHASH is 40% faster than CityHash
on inputs larger than 64 bytes and just as fast otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03467</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03467</id><created>2015-03-11</created><updated>2016-03-08</updated><authors><author><keyname>Owhadi</keyname><forenames>Houman</forenames></author></authors><title>Multigrid with rough coefficients and Multiresolution operator
  decomposition from Hierarchical Information Games</title><categories>math.NA cs.AI math.ST stat.TH</categories><comments>60 pages, 11 figures, presented at SIAM CSE 15. To appear in SIAM
  review (research spotlights)</comments><msc-class>68T99, 65N55, 65F99, 65N75, 62C99, 42C40, 60G42, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a near-linear complexity (geometric and meshless/algebraic)
multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients
with rigorous a-priori accuracy and performance estimates. The method is
discovered through a decision/game theory formulation of the problems of (1)
identifying restriction and interpolation operators (2) recovering a signal
from incomplete measurements based on norm constraints on its image under a
linear operator (3) gambling on the value of the solution of the PDE based on a
hierarchy of nested measurements of its solution or source term. The resulting
elementary gambles form a hierarchy of (deterministic) basis functions of
$H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands
with respect to the scalar product induced by the energy norm of the PDE (2)
enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce
an orthogonal multiresolution operator decomposition. The operating diagram of
the multigrid method is that of an inverted pyramid in which gamblets are
computed locally (by virtue of their exponential decay), hierarchically (from
fine to coarse scales) and the PDE is decomposed into a hierarchy of
independent linear systems with uniformly bounded condition numbers. The
resulting algorithm is parallelizable both in space (via localization) and in
bandwith/subscale (subscales can be computed independently from each other).
Although the method is deterministic it has a natural Bayesian interpretation
under the measure of probability emerging (as a mixed strategy) from the
information game formulation and multiresolution approximations form a
martingale with respect to the filtration induced by the hierarchy of nested
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03468</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03468</id><created>2015-03-11</created><authors><author><keyname>Dias</keyname><forenames>Elis&#xe2;ngela Silva</forenames></author><author><keyname>Castonguay</keyname><forenames>Diane</forenames></author><author><keyname>Dourado</keyname><forenames>Mitre Costa</forenames></author></authors><title>Algorithms and Properties for Positive Symmetrizable Matrices</title><categories>cs.DM</categories><comments>10 pages, submitted to International Journal of Applied Mathmatics
  (IJAM)</comments><msc-class>05B20, 13F60, 15A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrices are the most common representations of graphs. They are also used
for the representation of algebras and cluster algebras. This paper shows some
properties of matrices in order to facilitate the understanding and locating
symmetrizable matrices with specific characteristics, called positive
quasi-Cartan companion matrices. Here, symmetrizable matrix are those which are
symmetric when multiplied by a diagonal matrix with positive entries called
symmetrizer matrix. Four algorithms are developed: one to decide whether there
is a symmetrizer matrix; second to find such symmetrizer matrix; another to
decide whether the matrix is positive or not; and the last to find a positive
quasi-Cartan companion matrix, if there exists. The third algorithm is used to
prove that the problem to decide if a matrix has a positive quasi-Cartan
companion is NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03488</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03488</id><created>2015-03-07</created><updated>2016-02-10</updated><authors><author><keyname>Murphy</keyname><forenames>Robert A.</forenames></author></authors><title>Estimating the Mean Number of K-Means Clusters to Form</title><categories>cs.LG</categories><comments>These writings are part of a longer writing which has been submitted
  for publication. I plan to replace this writing (and the other 2 writings)
  with the single writing that has been submitted for publication. The other
  writings to be withdraw are 1501.07227 and 1412.4178</comments><msc-class>60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilizing the sample size of a dataset, the random cluster model is employed
in order to derive an estimate of the mean number of K-Means clusters to form
during classification of a dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03491</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03491</id><created>2015-03-11</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>Properties of simple sets in digital spaces. Contractions of simple sets
  preserving the homotopy type of a digital space</title><categories>cs.CV cs.DM math.AT</categories><comments>7 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1412.0218</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A point of a digital space is called simple if it can be deleted from the
space without altering topology. This paper introduces the notion simple set of
points of a digital space. The definition is based on contractible spaces and
contractible transformations. A set of points in a digital space is called
simple if it can be contracted to a point without changing topology of the
space. It is shown that contracting a simple set of points does not change the
homotopy type of a digital space, and the number of points in a digital space
without simple points can be reduces by contracting simple sets. Using the
process of contracting, we can substantially compress a digital space while
preserving the topology. The paper proposes a method for thinning a digital
space which shows that this approach can contribute to computer science such as
medical imaging, computer graphics and pattern analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03492</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03492</id><created>2015-03-10</created><authors><author><keyname>Lebert</keyname><forenames>Jan</forenames></author><author><keyname>K&#xfc;nneke</keyname><forenames>Lutz</forenames></author><author><keyname>Hagemann</keyname><forenames>Johannes</forenames></author><author><keyname>Kramer</keyname><forenames>Stephan C.</forenames></author></authors><title>Parallel Statistical Multi-resolution Estimation</title><categories>physics.comp-ph cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss several strategies to implement Dykstra's projection algorithm on
NVIDIA's compute unified device architecture (CUDA). Dykstra's algorithm is the
central step in and the computationally most expensive part of statistical
multi-resolution methods. It projects a given vector onto the intersection of
convex sets. Compared with a CPU implementation our CUDA implementation is one
order of magnitude faster. For a further speed up and to reduce memory
consumption we have developed a new variant, which we call incomplete Dykstra's
algorithm. Implemented in CUDA it is one order of magnitude faster than the
CUDA implementation of the standard Dykstra algorithm. As sample application we
discuss using the incomplete Dykstra's algorithm as preprocessor for the
recently developed super-resolution optical fluctuation imaging (SOFI) method
(Dertinger et al. 2009). We show that statistical multi-resolution estimation
can enhance the resolution improvement of the plain SOFI algorithm just as the
Fourier-reweighting of SOFI. The results are compared in terms of their power
spectrum and their Fourier ring correlation (Saxton and Baumeister 1982). The
Fourier ring correlation indicates that the resolution for typical second order
SOFI images can be improved by about 30 per cent. Our results show that a
careful parallelization of Dykstra's algorithm enables its use in large-scale
statistical multi-resolution analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03506</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03506</id><created>2015-03-11</created><authors><author><keyname>Wachinger</keyname><forenames>Christian</forenames></author><author><keyname>Golland</keyname><forenames>Polina</forenames></author></authors><title>Diverse Landmark Sampling from Determinantal Point Processes for
  Scalable Manifold Learning</title><categories>cs.LG cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High computational costs of manifold learning prohibit its application for
large point sets. A common strategy to overcome this problem is to perform
dimensionality reduction on selected landmarks and to successively embed the
entire dataset with the Nystr\&quot;om method. The two main challenges that arise
are: (i) the landmarks selected in non-Euclidean geometries must result in a
low reconstruction error, (ii) the graph constructed from sparsely sampled
landmarks must approximate the manifold well. We propose the sampling of
landmarks from determinantal distributions on non-Euclidean spaces. Since
current determinantal sampling algorithms have the same complexity as those for
manifold learning, we present an efficient approximation running in linear
time. Further, we recover the local geometry after the sparsification by
assigning each landmark a local covariance matrix, estimated from the original
point set. The resulting neighborhood selection based on the Bhattacharyya
distance improves the embedding of sparsely sampled manifolds. Our experiments
show a significant performance improvement compared to state-of-the-art
landmark selection techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03511</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03511</id><created>2015-03-11</created><authors><author><keyname>de Andrade</keyname><forenames>Paulo R. M.</forenames></author><author><keyname>Albuquerque</keyname><forenames>Adriano B.</forenames></author><author><keyname>Frota</keyname><forenames>Ot&#xe1;vio F.</forenames></author><author><keyname>Silveira</keyname><forenames>Robson V</forenames></author><author><keyname>da Silva</keyname><forenames>F&#xe1;tima A.</forenames></author></authors><title>Cross platform app: a comparative study</title><categories>cs.SE cs.CY cs.SI</categories><comments>8 pages, 3 figures, 2 tables in International Journal of Computer
  Science &amp; Information Technology (IJCSIT) Vol 7, No 1, February 2015</comments><doi>10.5121/ijcsit.2015.7104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of mobile applications is now so common that users now expect
companies whose services which they consume already have an application to
provide these services or a mobile version of your site, but this is not always
simple to do or cheap. Thus, the hybrid development has emerged as a potential
alternative to this need. The evolution of this new paradigm has taken the
attention of researchers and companies as viable alternative to the mobile
development. This paper shows how hybrid development can be an alternative for
companies provide their services with a low investment and still offer a great
service to their clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03512</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03512</id><created>2015-03-11</created><updated>2015-05-30</updated><authors><author><keyname>Pechenick</keyname><forenames>Eitan Adam</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Is language evolution grinding to a halt?: Exploring the life and death
  of words in English fiction</title><categories>cs.CL cs.IT math.IT physics.soc-ph stat.AP</categories><comments>11 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Google Books corpus, derived from millions of books in a range of major
languages, would seem to offer many possibilities for research into cultural,
social, and linguistic evolution. In a previous work, we found that the 2009
and 2012 versions of the unfiltered English data set as well as the 2009
version of the English Fiction data set are all heavily saturated with
scientific and medical literature, rendering them unsuitable for rigorous
analysis. By contrast, the 2012 version of English Fiction appeared to be
uncompromised, and we use this data set to explore language dynamics for
English from 1820--2000. We critique a previous method for measuring birth and
death rates of words, and provide a robust, principled to examining the volume
of word flux across various relative frequency usage thresholds. We use the
contributions to the Jensen-Shannon divergence of words crossing thresholds
between consecutive decades to illuminate the major driving factors behind the
flux. We find that while individual word usage may vary greatly, the overall
statistical structure of the language appears to remain fairly stable. We also
find indications that scholarly works about fiction are strongly represented in
the 2012 English Fiction corpus, and suggest that a future revision of the
corpus should attempt to separate critical works from fiction itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03514</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03514</id><created>2015-03-11</created><authors><author><keyname>Rivera-Rubio</keyname><forenames>Jose</forenames></author><author><keyname>Alexiou</keyname><forenames>Ioannis</forenames></author><author><keyname>Bharath</keyname><forenames>Anil A.</forenames></author></authors><title>Appearance-based indoor localization: A comparison of patch descriptor
  performance</title><categories>cs.CV cs.RO</categories><comments>Accepted for publication on Pattern Recognition Letters</comments><msc-class>68T45, 68T40</msc-class><doi>10.1016/j.patrec.2015.03.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision is one of the most important of the senses, and humans use it
extensively during navigation. We evaluated different types of image and video
frame descriptors that could be used to determine distinctive visual landmarks
for localizing a person based on what is seen by a camera that they carry. To
do this, we created a database containing over 3 km of video-sequences with
ground-truth in the form of distance travelled along different corridors. Using
this database, the accuracy of localization - both in terms of knowing which
route a user is on - and in terms of position along a certain route, can be
evaluated. For each type of descriptor, we also tested different techniques to
encode visual structure and to search between journeys to estimate a user's
position. The techniques include single-frame descriptors, those using
sequences of frames, and both colour and achromatic descriptors. We found that
single-frame indexing worked better within this particular dataset. This might
be because the motion of the person holding the camera makes the video too
dependent on individual steps and motions of one particular journey. Our
results suggest that appearance-based information could be an additional source
of navigational data indoors, augmenting that provided by, say, radio signal
strength indicators (RSSIs). Such visual information could be collected by
crowdsourcing low-resolution video feeds, allowing journeys made by different
users to be associated with each other, and location to be inferred without
requiring explicit mapping. This offers a complementary approach to methods
based on simultaneous localization and mapping (SLAM) algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03517</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03517</id><created>2015-03-11</created><authors><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Rahimian</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Switching to Learn</title><categories>cs.LG math.OC stat.ML</categories><comments>6 pages, To appear in American Control Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A network of agents attempt to learn some unknown state of the world drawn by
nature from a finite set. Agents observe private signals conditioned on the
true state, and form beliefs about the unknown state accordingly. Each agent
may face an identification problem in the sense that she cannot distinguish the
truth in isolation. However, by communicating with each other, agents are able
to benefit from side observations to learn the truth collectively. Unlike many
distributed algorithms which rely on all-time communication protocols, we
propose an efficient method by switching between Bayesian and non-Bayesian
regimes. In this model, agents exchange information only when their private
signals are not informative enough; thence, by switching between the two
regimes, agents efficiently learn the truth using only a few rounds of
communications. The proposed algorithm preserves learnability while incurring a
lower communication cost. We also verify our theoretical findings by simulation
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03524</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03524</id><created>2015-03-11</created><authors><author><keyname>Kafsi</keyname><forenames>Mohamed</forenames></author><author><keyname>Cramer</keyname><forenames>Henriette</forenames></author><author><keyname>Thomee</keyname><forenames>Bart</forenames></author><author><keyname>Shamma</keyname><forenames>David A.</forenames></author></authors><title>Describing and Understanding Neighborhood Characteristics through Online
  Social Media</title><categories>stat.ML cs.SI</categories><comments>Accepted in WWW 2015, 2015, Florence, Italy</comments><report-no>ACM 978-1-4503-3469-3/15/05</report-no><doi>10.1145/2736277.2741133</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geotagged data can be used to describe regions in the world and discover
local themes. However, not all data produced within a region is necessarily
specifically descriptive of that area. To surface the content that is
characteristic for a region, we present the geographical hierarchy model (GHM),
a probabilistic model based on the assumption that data observed in a region is
a random mixture of content that pertains to different levels of a hierarchy.
We apply the GHM to a dataset of 8 million Flickr photos in order to
discriminate between content (i.e., tags) that specifically characterizes a
region (e.g., neighborhood) and content that characterizes surrounding areas or
more general themes. Knowledge of the discriminative and non-discriminative
terms used throughout the hierarchy enables us to quantify the uniqueness of a
given region and to compare similar but distant regions. Our evaluation
demonstrates that our model improves upon traditional Naive Bayes
classification by 47% and hierarchical TF-IDF by 27%. We further highlight the
differences and commonalities with human reasoning about what is locally
characteristic for a neighborhood, distilled from ten interviews and a survey
that covered themes such as time, events, and prior regional knowledge
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03525</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03525</id><created>2015-03-11</created><updated>2015-06-26</updated><authors><author><keyname>Lois</keyname><forenames>Brian</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>Online Matrix Completion and Online Robust PCA</title><categories>cs.IT math.IT stat.ML</categories><comments>Presented at ISIT (IEEE Intnl. Symp. on Information Theory), 2015.
  Submitted to IEEE Transactions on Information Theory. This version: changes
  are in blue; the main changes are just to explain the model assumptions
  better (added based on ISIT reviewers' comments)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies two interrelated problems - online robust PCA (RPCA) and
online low-rank matrix completion (MC). In recent work by Cand\`{e}s et al.,
RPCA has been defined as a problem of separating a low-rank matrix (true data),
$L:=[\ell_1, \ell_2, \dots \ell_{t}, \dots , \ell_{t_{\max}}]$ and a sparse
matrix (outliers), $S:=[x_1, x_2, \dots x_{t}, \dots, x_{t_{\max}}]$ from their
sum, $M:=L+S$. Our work uses this definition of RPCA. An important application
where both these problems occur is in video analytics in trying to separate
sparse foregrounds (e.g., moving objects) and slowly changing backgrounds.
  While there has been a large amount of recent work on both developing and
analyzing batch RPCA and batch MC algorithms, the online problem is largely
open. In this work, we develop a practical modification of our recently
proposed algorithm to solve both the online RPCA and online MC problems. The
main contribution of this work is that we obtain correctness results for the
proposed algorithms under mild assumptions. The assumptions that we need are:
(a) a good estimate of the initial subspace is available (easy to obtain using
a short sequence of background-only frames in video surveillance); (b) the
$\ell_t$'s obey a `slow subspace change' assumption; (c) the basis vectors for
the subspace from which $\ell_t$ is generated are dense (non-sparse); (d) the
support of $x_t$ changes by at least a certain amount at least every so often;
and (e) algorithm parameters are appropriately set
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03528</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03528</id><created>2015-03-11</created><authors><author><keyname>Lopez</keyname><forenames>Gustavo V.</forenames></author><author><keyname>Montes</keyname><forenames>Gustavo</forenames></author></authors><title>Study of decoherence of entangled states made up of two basic states in
  a linear chain of three qubits</title><categories>quant-ph cs.IT math.IT</categories><comments>9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Lindblad approach to study decoherence of quantum systems, we study the
decoherence and decay of entangled states, formed by two basic states of a
chain of thee qubits. We look on these states for a possible regular dependence
on their decay as a function of their energy separation between the basic
states under different type of environments. We found not regular or
significant dependence on this energy separation for the type of environment
considered .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03535</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03535</id><created>2015-03-11</created><updated>2015-06-12</updated><authors><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Firat</keyname><forenames>Orhan</forenames></author><author><keyname>Xu</keyname><forenames>Kelvin</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Barrault</keyname><forenames>Loic</forenames></author><author><keyname>Lin</keyname><forenames>Huei-Chi</forenames></author><author><keyname>Bougares</keyname><forenames>Fethi</forenames></author><author><keyname>Schwenk</keyname><forenames>Holger</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On Using Monolingual Corpora in Neural Machine Translation</title><categories>cs.CL</categories><comments>9 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent work on end-to-end neural network-based architectures for machine
translation has shown promising results for En-Fr and En-De translation.
Arguably, one of the major factors behind this success has been the
availability of high quality parallel corpora. In this work, we investigate how
to leverage abundant monolingual corpora for neural machine translation.
Compared to a phrase-based and hierarchical baseline, we obtain up to $1.96$
BLEU improvement on the low-resource language pair Turkish-English, and $1.59$
BLEU on the focused domain task of Chinese-English chat messages. While our
method was initially targeted toward such tasks with less parallel data, we
show that it also extends to high resource languages such as Cs-En and De-En
where we obtain an improvement of $0.39$ and $0.47$ BLEU scores over the neural
machine translation baselines, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03537</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03537</id><created>2015-03-11</created><authors><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Zargham</keyname><forenames>Michael</forenames></author><author><keyname>Nowzari</keyname><forenames>Cameron</forenames></author><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Pappas</keyname><forenames>George</forenames></author></authors><title>Bio-Inspired Framework for Allocation of Protection Resources in
  Cyber-Physical Networks</title><categories>cs.SI cs.SY math.OC physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1309.6270</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter, we consider the problem of designing protection strategies
to contain spreading processes in complex cyber-physical networks. We
illustrate our ideas using a family of bio-motivated spreading models
originally proposed in the epidemiological literature, e.g., the
Susceptible-Infected-Susceptible (SIS) model. We first introduce a framework in
which we are allowed to distribute two types of resources in order to contain
the spread, namely, (i) preventive resources able to reduce the spreading rate,
and (ii) corrective resources able to increase the recovery rate of nodes in
which the resources are allocated. In practice, these resources have an
associated cost that depends on either the resiliency level achieved by the
preventive resource, or the restoration efficiency of the corrective resource.
We present a mathematical framework, based on dynamic systems theory and convex
optimization, to find the cost-optimal distribution of protection resources in
a network to contain the spread. We also present two extensions to this
framework in which (i) we consider generalized epidemic models, beyond the
simple SIS model, and (ii) we assume uncertainties in the contact network in
which the spreading is taking place. We compare these protection strategies
with common heuristics previously proposed in the literature and illustrate our
results with numerical simulations using the air traffic network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03553</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03553</id><created>2015-03-11</created><authors><author><keyname>Nakahara</keyname><forenames>Yasuhiro</forenames></author><author><keyname>Washizawa</keyname><forenames>Teruyoshi</forenames></author></authors><title>Accelerating DEM simulations on GPUs by reducing the impact of warp
  divergences</title><categories>cs.DC</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A way to accelerate DEM calculations on the GPUs is developed. We examined
how warp divergences take place in the contact detection and the force
calculations taking account of the GPU architecture. Then we showed a strategy
to reduce the impact of the warp divergences on the runtime of the DEM force
calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03562</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03562</id><created>2015-03-11</created><updated>2015-03-22</updated><authors><author><keyname>Cheng</keyname><forenames>Zhiyong</forenames></author><author><keyname>Soudry</keyname><forenames>Daniel</forenames></author><author><keyname>Mao</keyname><forenames>Zexi</forenames></author><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author></authors><title>Training Binary Multilayer Neural Networks for Image Classification
  using Expectation Backpropagation</title><categories>cs.NE cs.CV cs.LG</categories><comments>8 pages with 1 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to Multilayer Neural Networks with real weights, Binary Multilayer
Neural Networks (BMNNs) can be implemented more efficiently on dedicated
hardware. BMNNs have been demonstrated to be effective on binary classification
tasks with Expectation BackPropagation (EBP) algorithm on high dimensional text
datasets. In this paper, we investigate the capability of BMNNs using the EBP
algorithm on multiclass image classification tasks. The performances of binary
neural networks with multiple hidden layers and different numbers of hidden
units are examined on MNIST. We also explore the effectiveness of image spatial
filters and the dropout technique in BMNNs. Experimental results on MNIST
dataset show that EBP can obtain 2.12% test error with binary weights and 1.66%
test error with real weights, which is comparable to the results of standard
BackPropagation algorithm on fully connected MNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03571</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03571</id><created>2015-03-11</created><authors><author><keyname>Spivak</keyname><forenames>David I.</forenames></author><author><keyname>Schultz</keyname><forenames>Patrick</forenames></author><author><keyname>Wisnesky</keyname><forenames>Ryan</forenames></author></authors><title>A Purely Equational Formalism for Functorial Data Migration</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a simple equational formalism for expressing
functorial data migration. A graphical IDE and implementation of this formalism
are available at categoricaldata.net/fql.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03573</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03573</id><created>2015-03-11</created><updated>2015-03-23</updated><authors><author><keyname>Chudnovsky</keyname><forenames>Maria</forenames></author><author><keyname>Maceli</keyname><forenames>Peter</forenames></author><author><keyname>Zhong</keyname><forenames>Mingxian</forenames></author></authors><title>Three-coloring graphs with no induced seven-vertex path II : using a
  triangle</title><categories>cs.DM math.CO</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give a polynomial time algorithm which determines if a
given graph containing a triangle and no induced seven-vertex path is
3-colorable, and gives an explicit coloring if one exists. In previous work, we
gave a polynomial time algorithm for three-coloring triangle-free graphs with
no induced seven-vertex path. Combined, our work shows that three-coloring a
graph with no induced seven-vertex path can be done in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03576</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03576</id><created>2015-03-12</created><authors><author><keyname>Meshgi</keyname><forenames>Hadi</forenames></author><author><keyname>Zhao</keyname><forenames>Dongmei</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author></authors><title>Optimal Resource Allocation in Multicast Device-to-Device Communications
  Underlaying LTE Networks</title><categories>cs.NI math.OC</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a framework for resource allocations for multicast
device-to-device (D2D) communications underlaying a cellular network. The
objective is to maximize the sum throughput of active cellular users (CUs) and
feasible D2D groups in a cell, while meeting a certain
signal-to-interferenceplus- noise ratio (SINR) constraint for both the CUs and
D2D groups. We formulate the problem of power and channel allocation as a mixed
integer nonlinear programming (MINLP) problem where one D2D group can reuse the
channels of multiple CUs and the channel of each CU can be reused by multiple
D2D groups. Distinct from existing approaches in the literature, our
formulation and solution methods provide an effective and flexible means to
utilize radio resources in cellular networks and share them with multicast
groups without causing harmful interference to each other. A variant of the
generalized bender decomposition (GBD) is applied to optimally solve the MINLP
problem. A greedy algorithm and a low-complexity heuristic solution are then
devised. The performance of all schemes is evaluated through extensive
simulations. Numerical results demonstrate that the proposed greedy algorithm
can achieve closeto- optimal performance, and the heuristic algorithm provides
good performance, though inferior than that of the greedy, with much lower
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03578</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03578</id><created>2015-03-12</created><authors><author><keyname>Tang</keyname><forenames>Jian</forenames></author><author><keyname>Qu</keyname><forenames>Meng</forenames></author><author><keyname>Wang</keyname><forenames>Mingzhe</forenames></author><author><keyname>Zhang</keyname><forenames>Ming</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author></authors><title>LINE: Large-scale Information Network Embedding</title><categories>cs.LG</categories><comments>WWW 2015</comments><doi>10.1145/2736277.2741093</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper studies the problem of embedding very large information networks
into low-dimensional vector spaces, which is useful in many tasks such as
visualization, node classification, and link prediction. Most existing graph
embedding methods do not scale for real world information networks which
usually contain millions of nodes. In this paper, we propose a novel network
embedding method called the &quot;LINE,&quot; which is suitable for arbitrary types of
information networks: undirected, directed, and/or weighted. The method
optimizes a carefully designed objective function that preserves both the local
and global network structures. An edge-sampling algorithm is proposed that
addresses the limitation of the classical stochastic gradient descent and
improves both the effectiveness and the efficiency of the inference. Empirical
experiments prove the effectiveness of the LINE on a variety of real-world
information networks, including language networks, social networks, and
citation networks. The algorithm is very efficient, which is able to learn the
embedding of a network with millions of vertices and billions of edges in a few
hours on a typical single machine. The source code of the LINE is available
online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03579</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03579</id><created>2015-03-12</created><authors><author><keyname>Subramanian</keyname><forenames>Thiruselvan</forenames></author><author><keyname>Savarimuthu</keyname><forenames>Nickolas</forenames></author></authors><title>A Study on Optimized Resource Provisioning in Federated Cloud</title><categories>cs.DC</categories><comments>2013 International Conference on Computing, Cybernetics and
  Intelligent Information Systems (CCIIS) ,VIT UNIVERSITY, VELLORE, Tamilnadu,
  India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing changed the way of computing as utility services offered
through public network. Selecting multiple providers for various computational
requirements improves performance and minimizes cost of cloud services than
choosing a single cloud provider. Federated cloud improves scalability, cost
minimization, performance maximization, collaboration with other providers,
multi-site deployment for fault tolerance and recovery, reliability and less
energy consumption. Both providers and consumers could benefit from federated
cloud where providers serve the consumers by satisfying Service Level
Agreement, minimizing overall management and infrastructure cost; consumers get
best services with less deployment cost and high availability. Efficient
provisioning of resources to consumers in federated cloud is a challenging
task. In this paper, the benefits of utilizing services from federated cloud,
architecture with various coupling levels, different optimized resource
provisioning methods and challenges associated with it are discussed and a
comparative study is carried out over these aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03584</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03584</id><created>2015-03-12</created><authors><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Liu</keyname><forenames>Huai</forenames></author><author><keyname>Laali</keyname><forenames>Mohsen</forenames></author><author><keyname>Schmidt</keyname><forenames>Heinz W.</forenames></author></authors><title>Human Factors in Software Reliability Engineering</title><categories>cs.SE cs.HC</categories><comments>Preprint, Workshop on Applications of Human Error Research to Improve
  Software Engineering (WAHESE) at ICSE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present our vision of the integration of human factors
engineering into the software development process. The aim of this approach is
to improve the quality of software and to deal with human errors in a
systematic way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03585</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03585</id><created>2015-03-12</created><updated>2015-11-18</updated><authors><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author><author><keyname>Weiss</keyname><forenames>Eric A.</forenames></author><author><keyname>Maheswaranathan</keyname><forenames>Niru</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author></authors><title>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title><categories>cs.LG cond-mat.dis-nn q-bio.NC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in machine learning involves modeling complex data-sets
using highly flexible families of probability distributions in which learning,
sampling, inference, and evaluation are still analytically or computationally
tractable. Here, we develop an approach that simultaneously achieves both
flexibility and tractability. The essential idea, inspired by non-equilibrium
statistical physics, is to systematically and slowly destroy structure in a
data distribution through an iterative forward diffusion process. We then learn
a reverse diffusion process that restores structure in data, yielding a highly
flexible and tractable generative model of the data. This approach allows us to
rapidly learn, sample from, and evaluate probabilities in deep generative
models with thousands of layers or time steps, as well as to compute
conditional and posterior probabilities under the learned model. We
additionally release an open source reference implementation of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03593</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03593</id><created>2015-03-12</created><authors><author><keyname>Saxena</keyname><forenames>Sapna</forenames></author><author><keyname>Kapoor</keyname><forenames>Bhanu</forenames></author></authors><title>State of the art parallel approaches for RSA public key based
  cryptosystem</title><categories>cs.CR</categories><comments>IJCSA February 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RSA is one of the most popular Public Key Cryptography based algorithm mainly
used for digital signatures, encryption/decryption etc. It is based on the
mathematical scheme of factorization of very large integers which is a
compute-intensive process and takes very long time as well as power to perform.
Several scientists are working throughout the world to increase the speedup and
to decrease the power consumption of RSA algorithm while keeping the security
of the algorithm intact. One popular technique which can be used to enhance the
performance of RSA is parallel programming. In this paper we are presenting the
survey of various parallel implementations of RSA algorithm involving variety
of hardware and software implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03594</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03594</id><created>2015-03-12</created><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Haghtalab</keyname><forenames>Nika</forenames></author><author><keyname>Urner</keyname><forenames>Ruth</forenames></author></authors><title>Efficient Learning of Linear Separators under Bounded Noise</title><categories>cs.LG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the learnability of linear separators in $\Re^d$ in the presence of
bounded (a.k.a Massart) noise. This is a realistic generalization of the random
classification noise model, where the adversary can flip each example $x$ with
probability $\eta(x) \leq \eta$. We provide the first polynomial time algorithm
that can learn linear separators to arbitrarily small excess error in this
noise model under the uniform distribution over the unit ball in $\Re^d$, for
some constant value of $\eta$. While widely studied in the statistical learning
theory community in the context of getting faster convergence rates,
computationally efficient algorithms in this model had remained elusive. Our
work provides the first evidence that one can indeed design algorithms
achieving arbitrarily small excess error in polynomial time under this
realistic noise model and thus opens up a new and exciting line of research.
  We additionally provide lower bounds showing that popular algorithms such as
hinge loss minimization and averaging cannot lead to arbitrarily small excess
error under Massart noise, even under the uniform distribution. Our work
instead, makes use of a margin based technique developed in the context of
active learning. As a result, our algorithm is also an active learning
algorithm with label complexity that is only a logarithmic the desired excess
error $\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03597</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03597</id><created>2015-03-12</created><updated>2015-09-07</updated><authors><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>Nonadaptive group testing with random set of defectives via
  constant-weight codes</title><categories>cs.IT math.IT</categories><comments>Currently under submission. Major revision from previous version.
  arXiv admin note: substantial text overlap with arXiv:1111.5003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a group testing scheme, a set of tests is designed to identify a small
number $t$ of defective items that are present among a large number $N$ of
items. Each test takes as input a group of items and produces a binary output
indicating whether any defective item is present in the group. In a
non-adaptive scheme the tests have to be designed in one-shot. In this setting,
designing a testing scheme is equivalent to the construction of a disjunct
matrix, an $M \times N$ binary matrix where the union of supports of any $t$
columns does not contain the support of any other column. In principle, one
wants to have such a matrix with minimum possible number $M$ of rows. In this
paper we consider the scenario where defective items are random and follow
simple probability distributions. In particular we consider the cases where 1)
each item can be defective independently with probability $\frac{t}{N}$ and 2)
each $t$-set of items can be defective with uniform probability. In both cases
our aim is to design a testing matrix that successfully identifies the set of
defectives with high probability. Both of these models have been studied in the
literature before and it is known that $O(t\log N)$ tests are necessary as well
as sufficient (via random coding) in both cases. Our main focus is explicit
deterministic construction of the test matrices amenable to above scenarios.
With our relaxed requirements, we show that using an explicit constant-weight
code we may achieve a number of tests equal to $O(t \frac{\log^2 N}{ \log t})$
for both the first and the second cases. While only away by a factor of
$\frac{\log N}{\log t}$ from the optimal number of tests, this is the best set
of parameters one can obtain from a deterministic construction and our main
contribution lies in relating the group testing properties to parameters of
constant-weight codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03600</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03600</id><created>2015-03-12</created><authors><author><keyname>Koo</keyname><forenames>Bonhong</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Detection Algorithms for Molecular MIMO</title><categories>cs.ET</categories><comments>6 pages, 6 figures, 2015 IEEE ICC accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel design for molecular communication in which
both the transmitter and the receiver have, in a 3-dimensional environment,
multiple bulges (in RF communication this corresponds to antenna). The proposed
system consists of a fluid medium, information molecules, a transmitter, and a
receiver. We simulate the system with a one-shot signal to obtain the channel's
finite impulse response. We then incorporate this result within our
mathematical analysis to determine interference. Molecular communication has a
great need for low complexity, hence, the receiver may have incomplete
information regarding the system and the channel state. Thus, for the cases of
limited information set at the receiver, we propose three detection algorithms,
namely adaptive thresholding, practical zero forcing, and Genie-aided zero
forcing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03605</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03605</id><created>2015-03-12</created><updated>2015-12-21</updated><authors><author><keyname>Sysala</keyname><forenames>Stanislav</forenames></author><author><keyname>Cermak</keyname><forenames>Martin</forenames></author><author><keyname>Koudelka</keyname><forenames>Tomas</forenames></author><author><keyname>Kruis</keyname><forenames>Jaroslav</forenames></author><author><keyname>Zeman</keyname><forenames>Jan</forenames></author><author><keyname>Blaheta</keyname><forenames>Radim</forenames></author></authors><title>An improved return-mapping scheme for nonsmooth yield surfaces: PART I -
  the Haigh-Westergaard coordinates</title><categories>cs.CE</categories><comments>25 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the numerical solution of elastoplastic constitutive
initial value problems. An improved form of the implicit return-mapping scheme
for nonsmooth yield surfaces is proposed that systematically builds on a
subdifferential formulation of the flow rule. The main advantage of this
approach is that the treatment of singular points, such as apices or edges at
which the flow direction is multivalued involves only a uniquely defined set of
non-linear equations, similarly to smooth yield surfaces. This paper (PART I)
is focused on isotropic models containing: $a)$ yield surfaces with one or two
apices (singular points) laying on the hydrostatic axis; $b)$ plastic
pseudo-potentials that are independent of the Lode angle; $c)$ nonlinear
isotropic hardening (optionally). It is shown that for some models the improved
integration scheme also enables to a priori decide about a type of the return
and investigate existence, uniqueness and semismoothness of discretized
constitutive operators in implicit form. Further, the semismooth Newton method
is introduced to solve incremental boundary-value problems. The paper also
contains numerical examples related to slope stability with available Matlab
implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03606</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03606</id><created>2015-03-12</created><authors><author><keyname>S.</keyname><forenames>Nagaraja</forenames></author><author><keyname>J.</keyname><forenames>Prabhakar C.</forenames></author></authors><title>Low-Level Features for Image Retrieval Based on Extraction of
  Directional Binary Patterns and Its Oriented Gradients Histogram</title><categories>cs.CV cs.IR</categories><comments>7 Figures, 5 Tables 16 Pages in Computer Applications: An
  International Journal (CAIJ), Vol.2, No.1, February 2015</comments><doi>10.5121/caij.2015.2102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach for image retrieval based on
extraction of low level features using techniques such as Directional Binary
Code, Haar Wavelet transform and Histogram of Oriented Gradients. The DBC
texture descriptor captures the spatial relationship between any pair of
neighbourhood pixels in a local region along a given direction, while Local
Binary Patterns descriptor considers the relationship between a given pixel and
its surrounding neighbours. Therefore, DBC captures more spatial information
than LBP and its variants, also it can extract more edge information than LBP.
Hence, we employ DBC technique in order to extract grey level texture feature
from each RGB channels individually and computed texture maps are further
combined which represents colour texture features of an image. Then, we
decomposed the extracted colour texture map and original image using Haar
wavelet transform. Finally, we encode the shape and local features of wavelet
transformed images using Histogram of Oriented Gradients for content based
image retrieval. The performance of proposed method is compared with existing
methods on two databases such as Wang's corel image and Caltech 256. The
evaluation results show that our approach outperforms the existing methods for
image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03607</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03607</id><created>2015-03-12</created><authors><author><keyname>Izadpanah</keyname><forenames>Najva</forenames></author></authors><title>A divisive hierarchical clustering-based method for indexing image
  information</title><categories>cs.IR</categories><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.6, No.1, February 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most practical applications of image retrieval, high-dimensional feature
vectors are required, but current multi-dimensional indexing structures lose
their efficiency with growth of dimensions. Our goal is to propose a divisive
hierarchical clustering-based multi-dimensional indexing structure which is
efficient in high-dimensional feature spaces. A projection pursuit method has
been used for finding a component of the data, which data's projections onto it
maximizes the approximation of negentropy for preparing essential information
in order to partitioning of the data space. Various tests and experimental
results on high-dimensional datasets indicate the performance of proposed
method in comparison with others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03608</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03608</id><created>2015-03-12</created><updated>2015-04-27</updated><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author></authors><title>Regularization Parameter Selection Method for Sign LMS with Reweighted
  L1-Norm Constriant Algorithm</title><categories>cs.IT math.IT</categories><comments>19 pages, 5 figures, submitted for journal. arXiv admin note: text
  overlap with arXiv:1503.00800</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadband frequency-selective fading channels usually have the inherent
sparse nature. By exploiting the sparsity, adaptive sparse channel estimation
(ASCE) algorithms, e.g., least mean square with reweighted L1-norm constraint
(LMS-RL1) algorithm, could bring a considerable performance gain under
assumption of additive white Gaussian noise (AWGN). In practical scenario of
wireless systems, however, channel estimation performance is often deteriorated
by unexpected non-Gaussian mixture noises which include AWGN and impulsive
noises. To design stable communication systems, sign LMS-RL1 (SLMS-RL1)
algorithm is proposed to remove the impulsive noise and to exploit channel
sparsity simultaneously. It is well known that regularization parameter (REPA)
selection of SLMS-RL1 is a very challenging issue. In the worst case,
inappropriate REPA may even result in unexpected instable convergence of
SLMS-RL1 algorithm. In this paper, Monte Carlo based selection method is
proposed to select suitable REPA so that SLMS-RL1 can achieve two goals: stable
convergence as well as usage sparsity information. Simulation results are
provided to corroborate our studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03613</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03613</id><created>2015-03-12</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Ohannessian</keyname><forenames>Mesrob I.</forenames></author></authors><title>On the Impossibility of Learning the Missing Mass</title><categories>stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH</categories><comments>16 pages</comments><msc-class>62G05, 62G20, 62G32 (Primary), 62C20, 62E10, 62N99 (Secondary)</msc-class><acm-class>G.3; I.2.6; I.2.7; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that one cannot learn the probability of rare events without
imposing further structural assumptions. The event of interest is that of
obtaining an outcome outside the coverage of an i.i.d. sample from a discrete
distribution. The probability of this event is referred to as the &quot;missing
mass&quot;. The impossibility result can then be stated as: the missing mass is not
distribution-free PAC-learnable in relative error. The proof is
semi-constructive and relies on a coupling argument using a dithered geometric
distribution. This result formalizes the folklore that in order to predict rare
events, one necessarily needs distributions with &quot;heavy tails&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03614</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03614</id><created>2015-03-12</created><authors><author><keyname>Raheja</keyname><forenames>Jagdish L.</forenames></author><author><keyname>Singhal</keyname><forenames>A.</forenames></author><author><keyname>Chaudhary</keyname><forenames>A.</forenames></author></authors><title>Android based Portable Hand Sign Recognition System</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These days mobile devices like phones or tablets are very common among people
of all age. They are connected with network and provide seamless communications
through internet or cellular services. These devices can be a big help for the
people who are not able to communicate properly and even in emergency
conditions. A disabled person who is not able to speak or a person who speak a
different language, these devices can be a boon for them as understanding,
translating and speaking systems for these people. This chapter discusses a
portable android based hand sign recognition system which can be used by
disabled people. This chapter shows a part of on-going project. Computer Vision
based techniques were used for image analysis and PCA was used after image
tokenizer for recognition. This method was tested with webcam results to make
system more robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03621</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03621</id><created>2015-03-12</created><updated>2015-09-08</updated><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Yang</keyname><forenames>Yingzhen</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Designing A Composite Dictionary Adaptively From Joint Examples</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complementary behaviors of external and internal examples in
image restoration, and are motivated to formulate a composite dictionary design
framework. The composite dictionary consists of the global part learned from
external examples, and the sample-specific part learned from internal examples.
The dictionary atoms in both parts are further adaptively weighted to emphasize
their model statistics. Experiments demonstrate that the joint utilization of
external and internal examples leads to substantial improvements, with
successful applications in image denoising and super resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03630</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03630</id><created>2015-03-12</created><authors><author><keyname>Deng</keyname><forenames>Liang-Jian</forenames></author><author><keyname>Guo</keyname><forenames>Weihong</forenames></author><author><keyname>Huang</keyname><forenames>Ting-Zhu</forenames></author></authors><title>Single image super-resolution by approximated Heaviside functions</title><categories>cs.CV cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image super-resolution is a process to enhance image resolution. It is widely
used in medical imaging, satellite imaging, target recognition, etc. In this
paper, we conduct continuous modeling and assume that the unknown image
intensity function is defined on a continuous domain and belongs to a space
with a redundant basis. We propose a new iterative model for single image
super-resolution based on an observation: an image is consisted of smooth
components and non-smooth components, and we use two classes of approximated
Heaviside functions (AHFs) to represent them respectively. Due to sparsity of
the non-smooth components, a $L_{1}$ model is employed. In addition, we apply
the proposed iterative model to image patches to reduce computation and
storage. Comparisons with some existing competitive methods show the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03635</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03635</id><created>2015-03-12</created><authors><author><keyname>Garimella</keyname><forenames>Kiran</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author><author><keyname>Sozio</keyname><forenames>Mauro</forenames></author></authors><title>Scalable Facility Location for Massive Graphs on Pregel-like Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new scalable algorithm for facility location. Facility location
is a classic problem, where the goal is to select a subset of facilities to
open, from a set of candidate facilities F , in order to serve a set of clients
C. The objective is to minimize the total cost of opening facilities plus the
cost of serving each client from the facility it is assigned to. In this work,
we are interested in the graph setting, where the cost of serving a client from
a facility is represented by the shortest-path distance on the graph. This
setting allows to model natural problems arising in the Web and in social media
applications. It also allows to leverage the inherent sparsity of such graphs,
as the input is much smaller than the full pairwise distances between all
vertices.
  To obtain truly scalable performance, we design a parallel algorithm that
operates on clusters of shared-nothing machines. In particular, we target
modern Pregel-like architectures, and we implement our algorithm on Apache
Giraph. Our solution makes use of a recent result to build sketches for massive
graphs, and of a fast parallel algorithm to find maximal independent sets, as
building blocks. In so doing, we show how these problems can be solved on a
Pregel-like architecture, and we investigate the properties of these
algorithms. Extensive experimental results show that our algorithm scales
gracefully to graphs with billions of edges, while obtaining values of the
objective function that are competitive with a state-of-the-art sequential
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03637</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03637</id><created>2015-03-12</created><updated>2015-09-04</updated><authors><author><keyname>Arrigoni</keyname><forenames>Federica</forenames></author><author><keyname>Rossi</keyname><forenames>Beatrice</forenames></author><author><keyname>Fusiello</keyname><forenames>Andrea</forenames></author></authors><title>On Computing the Translations Norm in the Epipolar Graph</title><categories>cs.CV</categories><comments>Accepted at 3DV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of recovering the unknown norm of relative
translations between cameras based on the knowledge of relative rotations and
translation directions. We provide theoretical conditions for the solvability
of such a problem, and we propose a two-stage method to solve it. First, a
cycle basis for the epipolar graph is computed, then all the scaling factors
are recovered simultaneously by solving a homogeneous linear system. We
demonstrate the accuracy of our solution by means of synthetic and real
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03639</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03639</id><created>2015-03-12</created><authors><author><keyname>Sarasvathi</keyname><forenames>V.</forenames></author><author><keyname>Iyengar</keyname><forenames>N. Ch. S. N.</forenames></author><author><keyname>Saha</keyname><forenames>Snehanshu</forenames></author></authors><title>QoS Guaranteed Intelligent Routing Using Hybrid PSO-GA in Wireless Mesh
  Networks</title><categories>cs.NI</categories><comments>15 pages in Cybernetics and Information Technologies,Volume 15, No 1,
  2015</comments><doi>10.1515/cait-2015-0007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Multi-Channel Multi-Radio Wireless Mesh Networks (MCMR-WMN), finding the
optimal routing by satisfying the Quality of Service (QoS) constraints is an
ambitious task. Multiple paths are available from the source node to the
gateway for reliability, and sometimes it is necessary to deal with failures of
the link in WMN. A major challenge in a MCMR-WMN is finding the routing with
QoS satisfied and an interference free path from the redundant paths, in order
to transmit the packets through this path. The Particle Swarm Optimization
(PSO) is an optimization technique to find the candidate solution in the search
space optimally, and it applies artificial intelligence to solve the routing
problem. On the other hand, the Genetic Algorithm (GA) is a population based
meta-heuristic optimization algorithm inspired by the natural evolution, such
as selection,mutation and crossover. PSO can easily fall into a local optimal
solution, at the same time GA is not suitable for dynamic data due to the
underlying dynamic network. In this paper we propose an optimal intelligent
routing, using a Hybrid PSO-GA, which also meets the QoS constraints. Moreover,
it integrates the strength of PSO and GA. The QoS constraints, such as
bandwidth, delay, jitter and interference are transformed into penalty
functions. The simulation results show that the hybrid approach outperforms PSO
and GA individually, and it takes less convergence time comparatively, keeping
away from converging prematurely.
  Keywords: Wireless mesh networks, Multi-radio, Multi-channel, Particle swarm
optimization, Genetic algorithm, Quality of service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03642</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03642</id><created>2015-03-12</created><authors><author><keyname>Yao</keyname><forenames>Chang</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Chang</keyname><forenames>Pengfei</forenames></author><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Ooi</keyname><forenames>Beng Chin</forenames></author><author><keyname>Wong</keyname><forenames>Weng-Fai</forenames></author><author><keyname>Zhang</keyname><forenames>Meihui</forenames></author></authors><title>DGCC:A New Dependency Graph based Concurrency Control Protocol for
  Multicore Database Systems</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicore CPUs and large memories are increasingly becoming the norm in
modern computer systems. However, current database management systems (DBMSs)
are generally ineffective in exploiting the parallelism of such systems. In
particular, contention can lead to a dramatic fall in performance. In this
paper, we propose a new concurrency control protocol called DGCC (Dependency
Graph based Concurrency Control) that separates concurrency control from
execution. DGCC builds dependency graphs for batched transactions before
executing them. Using these graphs, contentions within the same batch of
transactions are resolved before execution. As a result, the execution of the
transactions does not need to deal with contention while maintaining full
equivalence to that of serialized execution. This better exploits multicore
hardware and achieves higher level of parallelism. To facilitate DGCC, we have
also proposed a system architecture that does not have certain centralized
control components yielding better scalability, as well as supports a more
efficient recovery mechanism. Our extensive experimental study shows that DGCC
achieves up to four times higher throughput compared to that of
state-of-the-art concurrency control protocols for high contention workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03650</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03650</id><created>2015-03-12</created><authors><author><keyname>Wang</keyname><forenames>Weiqing</forenames></author><author><keyname>Yin</keyname><forenames>Hongzhi</forenames></author><author><keyname>Chen</keyname><forenames>Ling</forenames></author><author><keyname>Sun</keyname><forenames>Yizhou</forenames></author><author><keyname>Sadiq</keyname><forenames>Shazia</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaofang</forenames></author></authors><title>Geo-SAGE: A Geographical Sparse Additive Generative Model for Spatial
  Item Recommendation</title><categories>cs.IR cs.DB cs.SI</categories><comments>10 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of location-based social networks (LBSNs), spatial
item recommendation has become an important means to help people discover
attractive and interesting venues and events, especially when users travel out
of town. However, this recommendation is very challenging compared to the
traditional recommender systems. A user can visit only a limited number of
spatial items, leading to a very sparse user-item matrix. Most of the items
visited by a user are located within a short distance from where he/she lives,
which makes it hard to recommend items when the user travels to a far away
place. Moreover, user interests and behavior patterns may vary dramatically
across different geographical regions. In light of this, we propose Geo-SAGE, a
geographical sparse additive generative model for spatial item recommendation
in this paper. Geo-SAGE considers both user personal interests and the
preference of the crowd in the target region, by exploiting both the
co-occurrence pattern of spatial items and the content of spatial items. To
further alleviate the data sparsity issue, Geo-SAGE exploits the geographical
correlation by smoothing the crowd's preferences over a well-designed spatial
index structure called spatial pyramid. We conduct extensive experiments to
evaluate the performance of our Geo-SAGE model on two real large-scale
datasets. The experimental results clearly demonstrate our Geo-SAGE model
outperforms the state-of-the-art in the two tasks of both out-of-town and
home-town recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03653</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03653</id><created>2015-03-12</created><updated>2015-04-27</updated><authors><author><keyname>Yao</keyname><forenames>Chang</forenames></author><author><keyname>Agrawal</keyname><forenames>Divyakant</forenames></author><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Ooi</keyname><forenames>Beng Chin</forenames></author><author><keyname>Wu</keyname><forenames>Sai</forenames></author></authors><title>Adaptive Logging for Distributed In-memory Databases</title><categories>cs.DB</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new type of logs, the command log, is being employed to replace the
traditional data log (e.g., ARIES log) in the in-memory databases. Instead of
recording how the tuples are updated, a command log only tracks the
transactions being executed, thereby effectively reducing the size of the log
and improving the performance. Command logging on the other hand increases the
cost of recovery, because all the transactions in the log after the last
checkpoint must be completely redone in case of a failure. In this paper, we
first extend the command logging technique to a distributed environment, where
all the nodes can perform recovery in parallel. We then propose an adaptive
logging approach by combining data logging and command logging. The percentage
of data logging versus command logging becomes an optimization between the
performance of transaction processing and recovery to suit different OLTP
applications. Our experimental study compares the performance of our proposed
adaptive logging, ARIES-style data logging and command logging on top of
H-Store. The results show that adaptive logging can achieve a 10x boost for
recovery and a transaction throughput that is comparable to that of command
logging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03660</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03660</id><created>2015-03-12</created><authors><author><keyname>Fernando</keyname><forenames>Zeon Trevor</forenames></author></authors><title>Capturing, Documenting and Visualizing Search Contexts for building
  Multimedia Corpora</title><categories>cs.IR cs.CY cs.HC cs.SE</categories><comments>Undergraduate (B.Tech Hons, Computer Science) Thesis Report, 2014,
  Vellore Institute of Technology, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Social Science research, multimedia documents are often collected to
answer particular research questions like: &quot;Which of the aesthetic properties
of a photo are considered important on the web&quot; or &quot;How has Street Art
developed over the past 50 years&quot;. Therefore, a researcher generally issues
multiple queries to a number of search engines. This activity may span over
long time intervals and results in a collection which can be further analyzed.
Documenting the collection building process which includes the context of the
carried out searches is imperative for social scientists to reproduce their
research. Such context documentation consists of several user actions and
search attributes like: the issued queries; the results clicked and saved;
duration a particular result was viewed for; the set of results that was
displayed but neither clicked, nor saved; as well as user annotations like
comments or tags. In this work we will describe a search process tracking
module and a search history visualization module. These modules can be
integrated into keyword based search systems through a REST API which was
developed to help capture, document and revisit past search contexts while
building a web corpora. Finally, we detail the implementation of how the module
was integrated into the LearnWeb2.0 platform - a multimedia web2.0 search and
sharing application which can obtain resources from various web2.0 tools such
as Youtube, Bing, Flickr, etc using keyword search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03669</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03669</id><created>2015-03-12</created><authors><author><keyname>Adamaszek</keyname><forenames>Michal</forenames></author><author><keyname>Adams</keyname><forenames>Henry</forenames></author></authors><title>The Vietoris-Rips complexes of a circle</title><categories>math.AT cs.CG math.CO math.MG</categories><report-no>CPH-SYM-DNRF92</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a metric space X and a distance threshold r&gt;0, the Vietoris-Rips
simplicial complex has as its simplices the finite subsets of X of diameter
less than r. A theorem of Jean-Claude Hausmann states that if X is a Riemannian
manifold and r is sufficiently small, then the Vietoris-Rips complex is
homotopy equivalent to the original manifold. Little is known about the
behavior of Vietoris-Rips complexes for larger values of r, even though these
complexes arise naturally in applications using persistent homology. We show
that as r increases, the Vietoris-Rips complex of the circle obtains the
homotopy types of the circle, the 3-sphere, the 5-sphere, the 7-sphere, ...,
until finally it is contractible. As our main tool we introduce a directed
graph invariant, the winding fraction, which in some sense is dual to the
circular chromatic number. Using the winding fraction we classify the homotopy
types of the Vietoris-Rips complex of an arbitrary (possibly infinite) subset
of the circle, and we study the expected homotopy type of the Vietoris-Rips
complex of a uniformly random sample from the circle. Moreover, we show that as
the distance parameter increases, the ambient Cech complex of the circle also
obtains the homotopy types of the circle, the 3-sphere, the 5-sphere, the
7-sphere, ..., until finally it is contractible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03674</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03674</id><created>2015-03-12</created><authors><author><keyname>Manjula</keyname><forenames>G. R.</forenames></author><author><keyname>Danti</keyname><forenames>Ajit</forenames></author></authors><title>A novel hash based least significant bit (2-3-3) image steganography in
  spatial domain</title><categories>cs.MM</categories><comments>10 pages International journal of security privacy and trust
  management Feb 2015 issue</comments><doi>10.5121/ijsptm.2015.4102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel 2-3-3 LSB insertion method. The image
steganography takes the advantage of human eye limitation. It uses color image
as cover media for embedding secret message.The important quality of a
steganographic system is to be less distortive while increasing the size of the
secret message. In this paper a method is proposed to embed a color secret
image into a color cover image. A 2-3-3 LSB insertion method has been used for
image steganography. Experimental results show an improvement in the Mean
squared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the
proposed technique over the base technique of hash based 3-3-2 LSB insertion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03701</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03701</id><created>2015-03-12</created><updated>2015-11-13</updated><authors><author><keyname>Jojic</keyname><forenames>Nebojsa</forenames></author><author><keyname>Perina</keyname><forenames>Alessandro</forenames></author><author><keyname>Kim</keyname><forenames>Dongwoo</forenames></author></authors><title>Hierarchical learning of grids of microtopics</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The counting grid is a grid of microtopics, sparse word/feature
distributions. The generative model associated with the grid does not use these
microtopics individually. Rather, it groups them in overlapping rectangular
windows and uses these grouped microtopics as either mixture or admixture
components. This paper builds upon the basic counting grid model and it shows
that hierarchical reasoning helps avoid bad local minima, produces better
classification accuracy and, most interestingly, allows for extraction of large
numbers of coherent microtopics even from small datasets. We evaluate this in
terms of consistency, diversity and clarity of the indexed content, as well as
in a user study on word intrusion tasks. We demonstrate that these models work
well as a technique for embedding raw images and discuss interesting parallels
between hierarchical CG models and other deep architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03712</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03712</id><created>2015-03-12</created><updated>2015-07-08</updated><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Levy</keyname><forenames>Kfir Y.</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>On Graduated Optimization for Stochastic Non-Convex Problems</title><categories>cs.LG math.OC</categories><comments>17 pages</comments><msc-class>68</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graduated optimization approach, also known as the continuation method,
is a popular heuristic to solving non-convex problems that has received renewed
interest over the last decade. Despite its popularity, very little is known in
terms of theoretical convergence analysis. In this paper we describe a new
first-order algorithm based on graduated optimiza- tion and analyze its
performance. We characterize a parameterized family of non- convex functions
for which this algorithm provably converges to a global optimum. In particular,
we prove that the algorithm converges to an {\epsilon}-approximate solution
within O(1/\epsilon^2) gradient-based steps. We extend our algorithm and
analysis to the setting of stochastic non-convex optimization with noisy
gradient feedback, attaining the same convergence rate. Additionally, we
discuss the setting of zero-order optimization, and devise a a variant of our
algorithm which converges at rate of O(d^2/\epsilon^4).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03715</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03715</id><created>2015-03-12</created><updated>2016-01-25</updated><authors><author><keyname>Reissig</keyname><forenames>Gunther</forenames></author><author><keyname>Weber</keyname><forenames>Alexander</forenames></author><author><keyname>Rungger</keyname><forenames>Matthias</forenames></author></authors><title>Feedback Refinement Relations for the Synthesis of Symbolic Controllers</title><categories>math.OC cs.LO cs.SY</categories><comments>Section VIII.D (Th VIII.7) added; Prop V.3 corrected; example in
  Section IX.A extended</comments><msc-class>Primary 93B51, Secondary, 93B52, 93C10, 93C30, 93C55, 93C57, 93C65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an abstraction and refinement methodology for the automated
controller synthesis to enforce general predefined specifications. The designed
controllers require quantized (or symbolic) state information only and can be
interfaced with the system via a static quantizer. Both features are
particularly important with regard to any practical implementation of the
designed controllers and, as we prove, are characterized by the existence of a
feedback refinement relation between plant and abstraction. Feedback refinement
relations are a novel concept introduced in this paper. Our work builds on a
general notion of system with set-valued dynamics and possibly
non-deterministic quantizers to permit the synthesis of controllers that
robustly, and provably, enforce the specification in the presence of various
types of uncertainties and disturbances. We identify a class of abstractions
that is canonical in a well-defined sense, and provide a method to efficiently
compute canonical abstractions. We demonstrate the practicality of our approach
on two examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03732</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03732</id><created>2015-03-12</created><authors><author><keyname>Vaufreydaz</keyname><forenames>Dominique</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Johal</keyname><forenames>Wafa</forenames><affiliation>LIG</affiliation></author><author><keyname>Combe</keyname><forenames>Claudine</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Starting engagement detection towards a companion robot using multimodal
  features</title><categories>cs.RO cs.CV</categories><proxy>ccsd</proxy><journal-ref>Robotics and Autonomous Systems, Elsevier, 2015, Robotics and
  Autonomous Systems, pp.25</journal-ref><doi>10.1016/j.robot.2015.01.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of intentions is a subconscious cognitive process vital to human
communication. This skill enables anticipation and increases the quality of
interactions between humans. Within the context of engagement, non-verbal
signals are used to communicate the intention of starting the interaction with
a partner. In this paper, we investigated methods to detect these signals in
order to allow a robot to know when it is about to be addressed. Originality of
our approach resides in taking inspiration from social and cognitive sciences
to perform our perception task. We investigate meaningful features, i.e. human
readable features, and elicit which of these are important for recognizing
someone's intention of starting an interaction. Classically, spatial
information like the human position and speed, the human-robot distance are
used to detect the engagement. Our approach integrates multimodal features
gathered using a companion robot equipped with a Kinect. The evaluation on our
corpus collected in spontaneous conditions highlights its robustness and
validates the use of such a technique in a real environment. Experimental
validation shows that multimodal features set gives better precision and recall
than using only spatial and speed features. We also demonstrate that 7 selected
features are sufficient to provide a good starting engagement detection score.
In our last investigation, we show that among our full 99 features set, the
space reduction is not a solved task. This result opens new researches
perspectives on multimodal engagement detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03739</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03739</id><created>2015-03-12</created><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Price of Stability in Games of Incomplete Information</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question of whether price of stability results (existence of
equilibria with low social cost) are robust to incomplete information. We show
that this is the case in potential games, if the underlying algorithmic social
cost minimization problem admits a constant factor approximation algorithm via
strict cost-sharing schemes. Roughly, if the existence of an
$\alpha$-approximate equilibrium in the complete information setting was proven
via the potential method, then there also exists a $\alpha\cdot
\beta$-approximate Bayes-Nash equilibrium in the incomplete information
setting, where $\beta$ is the approximation factor of the strict-cost sharing
scheme algorithm. We apply our approach to Bayesian versions of the archetypal,
in the price of stability analysis, network design models and show the
existence of $O(\log(n))$-approximate Bayes-Nash equilibria in several games
whose complete information counterparts have been well-studied, such as
undirected network design games, multi-cast games and covering games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03741</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03741</id><created>2015-03-12</created><authors><author><keyname>Hafez</keyname><forenames>Samir F.</forenames></author><author><keyname>Selim</keyname><forenames>Mazen M.</forenames></author><author><keyname>Zayed</keyname><forenames>Hala H.</forenames></author></authors><title>2D Face Recognition System Based on Selected Gabor Filters and Linear
  Discriminant Analysis LDA</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach for face recognition system. The method is based on
2D face image features using subset of non-correlated and Orthogonal Gabor
Filters instead of using the whole Gabor Filter Bank, then compressing the
output feature vector using Linear Discriminant Analysis (LDA). The face image
has been enhanced using multi stage image processing technique to normalize it
and compensate for illumination variation. Experimental results show that the
proposed system is effective for both dimension reduction and good recognition
performance when compared to the complete Gabor filter bank. The system has
been tested using CASIA, ORL and Cropped YaleB 2D face images Databases and
achieved average recognition rate of 98.9 %.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03746</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03746</id><created>2015-03-11</created><authors><author><keyname>Xie</keyname><forenames>Wen-Jie</forenames></author><author><keyname>Li</keyname><forenames>Ming-Xia</forenames></author><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Tan</keyname><forenames>Qun-Zhao</forenames></author><author><keyname>Podobnik</keyname><forenames>Boris</forenames></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>Division of labor, skill complementarity, and heterophily in
  socioeconomic networks</title><categories>physics.soc-ph cs.SI</categories><comments>14 Latex pages + 3 figures</comments><journal-ref>Sci. Rep. 6, 18727 (2016)</journal-ref><doi>10.1038/srep18727</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constituents of complex systems interact with each other and self-organize to
form complex networks. Empirical results show that the link formation process
of many real networks follows either the global principle of popularity or the
local principle of similarity or a tradeoff between the two. In particular, it
has been shown that in social networks individuals exhibit significant
homophily when choosing their collaborators. We demonstrate, however, that in
populations in which there is a division of labor, skill complementarity is an
important factor in the formation of socioeconomic networks and an individual's
choice of collaborators is strongly affected by heterophily. We analyze 124
evolving virtual worlds of a popular &quot;massively multiplayer online role-playing
game&quot; (MMORPG) in which people belong to three different professions and are
allowed to work and interact with each other in a somewhat realistic manner. We
find evidence of heterophily in the formation of collaboration networks, where
people prefer to forge social ties with people who have professions different
from their own. We then construct an economic model to quantify the heterophily
by assuming that individuals in socioeconomic systems choose collaborators that
are of maximum utility. The results of model calibration confirm the presence
of heterophily. Both empirical analysis and model calibration show that the
heterophilous feature is persistent along the evolution of virtual worlds. We
also find that the degree of complementarity in virtual societies is positively
correlated with their economic output. Our work sheds new light on the
scientific research utility of virtual worlds for studying human behaviors in
complex socioeconomic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03752</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03752</id><created>2015-03-12</created><updated>2015-03-12</updated><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author></authors><title>Manipulation and abuse on social media</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>ACM SIGWEB Newsletter, Spring 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computer science research community has became increasingly interested in
the study of social media due to their pervasiveness in the everyday life of
millions of individuals. Methodological questions and technical challenges
abound as more and more data from social platforms become available for
analysis. This data deluge not only yields the unprecedented opportunity to
unravel questions about online individuals' behavior at scale, but also allows
to explore the potential perils that the massive adoption of social media
brings to our society. These communication channels provide plenty of
incentives (both economical and social) and opportunities for abuse. As social
media activity became increasingly intertwined with the events in the offline
world, individuals and organizations have found ways to exploit these platforms
to spread misinformation, to attack and smear others, or to deceive and
manipulate. During crises, social media have been effectively used for
emergency response, but fear-mongering actions have also triggered mass
hysteria and panic. Criminal gangs and terrorist organizations like ISIS adopt
social media for propaganda and recruitment. Synthetic activity and social bots
have been used to coordinate orchestrated astroturf campaigns, to manipulate
political elections and the stock market. The lack of effective content
verification systems on many of these platforms, including Twitter and
Facebook, rises concerns when younger users become exposed to cyber-bulling,
harassment, or hate speech, inducing risks like depression and suicide. This
article illustrates some of the recent advances facing these issues and
discusses what it remains to be done, including the challenges to address in
the future to make social media a more useful and accessible, safer and
healthier environment for all users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03753</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03753</id><created>2015-03-11</created><authors><author><keyname>Roy</keyname><forenames>Senjuti Basu</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author><author><keyname>Liu</keyname><forenames>Rui</forenames></author></authors><title>From Group Recommendations to Group Formation</title><categories>cs.IR cs.DB</categories><comments>14 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been significant recent interest in the area of group
recommendations, where, given groups of users of a recommender system, one
wants to recommend top-k items to a group that maximize the satisfaction of the
group members, according to a chosen semantics of group satisfaction. Examples
semantics of satisfaction of a recommended itemset to a group include the
so-called least misery (LM) and aggregate voting (AV). We consider the
complementary problem of how to form groups such that the users in the formed
groups are most satisfied with the suggested top-k recommendations. We assume
that the recommendations will be generated according to one of the two group
recommendation semantics - LM or AV. Rather than assuming groups are given, or
rely on ad hoc group formation dynamics, our framework allows a strategic
approach for forming groups of users in order to maximize satisfaction. We show
that the problem is NP-hard to solve optimally under both semantics.
Furthermore, we develop two efficient algorithms for group formation under LM
and show that they achieve bounded absolute error. We develop efficient
heuristic algorithms for group formation under AV. We validate our results and
demonstrate the scalability and effectiveness of our group formation algorithms
on two large real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03763</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03763</id><created>2015-03-12</created><authors><author><keyname>de Souza</keyname><forenames>M. M. Campello</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author><author><keyname>Vasconcelos</keyname><forenames>M. M.</forenames></author></authors><title>The Discrete Cosine Transform over Prime Finite Fields</title><categories>cs.DM cs.DS</categories><comments>5 pages, 1 table, Lecture Notes in Computer Science, LNCS 3124,
  Heidelberg: Springer Verlag, 2004, vol.1, pp.482-487, 2004</comments><doi>10.1007/978-3-540-27824-5_65</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines finite field trigonometry as a tool to construct
trigonometric digital transforms. In particular, by using properties of the
k-cosine function over GF(p), the Finite Field Discrete Cosine Transform
(FFDCT) is introduced. The FFDCT pair in GF(p) is defined, having blocklengths
that are divisors of (p+1)/2. A special case is the Mersenne FFDCT, defined
when p is a Mersenne prime. In this instance blocklengths that are powers of
two are possible and radix-2 fast algorithms can be used to compute the
transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03767</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03767</id><created>2015-03-12</created><authors><author><keyname>Pathania</keyname><forenames>Deepika</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Exploiting Near Time Forecasting From Social Network To Decongest
  Traffic</title><categories>cs.MA cs.SI</categories><comments>Longer version of &quot;Social Network driven Traffic Decongestion using
  near time Forecasting&quot; poster to appear in AAMAS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preventing traffic congestion by forecasting near time traffic flows is an
important problem as it leads to effective use of transport resources. Social
network provides information about activities of humans and social events.
Thus, with the help of social network, we can extract which humans will attend
a particular event (in near time) and can estimate flow of traffic based on it.
This opens up a wide area of research which poses need to have a framework for
traffic management that can capture essential parameters of real-life behaviour
and provide a way to iterate upon and evaluate new ideas. In this paper, we
present building blocks of a framework and a system to simulate a city with its
transport system, humans and their social network. We emphasize on relevant
parameters selected and modular design of the framework. Our framework defines
metrics to evaluate congestion avoidance strategies. To show utility of the
framework, we present experimental studies of few strategies on a public
transport system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03769</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03769</id><created>2015-03-09</created><authors><author><keyname>Junjie</keyname><forenames>Wang</forenames></author><author><keyname>Lingling</keyname><forenames>Zhao</forenames></author><author><keyname>Xiaohong</keyname><forenames>Su</forenames></author><author><keyname>Peijun</keyname><forenames>Ma</forenames></author></authors><title>Distributed Computation Particle PHD filter</title><categories>stat.CO cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle probability hypothesis density filtering has become a promising
means for multi-target tracking due to its capability of handling an unknown
and time-varying number of targets in non-linear non-Gaussian system. However,
its computational complexity grows linearly with the number of measurements and
particles assigned to each target, and this can be very time consuming
especially when numerous targets and clutter exist in the surveillance region.
Addressing this issue, we present a distributed computation particle PHD filter
for target tracking. Its framework consists of several local particle PHD
filters at each processing element and a central unit. Each processing element
takes responsibility for part particles but full measurements and provides
local estimates; central unit controls particle exchange between processing
elements and specifies a fusion rule to match and fuse the estimates from
different local filters. The proposed framework is suitable for parallel
implementation and maintains the tracking accuracy. Simulations verify the
proposed method can provide comparative accuracy as well as a significant
speedup with the standard particle PHD filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03771</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03771</id><created>2015-03-12</created><authors><author><keyname>Ohn-Bar</keyname><forenames>Eshed</forenames></author><author><keyname>Trivedi</keyname><forenames>Mohan M.</forenames></author></authors><title>Learning to Detect Vehicles by Clustering Appearance Patterns</title><categories>cs.CV</categories><comments>Preprint version of our T-ITS 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies efficient means for dealing with intra-category diversity
in object detection. Strategies for occlusion and orientation handling are
explored by learning an ensemble of detection models from visual and
geometrical clusters of object instances. An AdaBoost detection scheme is
employed with pixel lookup features for fast detection. The analysis provides
insight into the design of a robust vehicle detection system, showing promise
in terms of detection performance and orientation estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03787</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03787</id><created>2015-03-12</created><authors><author><keyname>B&#xe1;tfai</keyname><forenames>Norbert</forenames></author></authors><title>Are there intelligent Turing machines?</title><categories>cs.AI</categories><msc-class>68Q05</msc-class><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new computing model based on the cooperation among
Turing machines called orchestrated machines. Like universal Turing machines,
orchestrated machines are also designed to simulate Turing machines but they
can also modify the original operation of the included Turing machines to
create a new layer of some kind of collective behavior. Using this new model we
can define some interested notions related to cooperation ability of Turing
machines such as the intelligence quotient or the emotional intelligence
quotient for Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03790</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03790</id><created>2015-03-12</created><updated>2015-08-03</updated><authors><author><keyname>Karapanos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Marforio</keyname><forenames>Claudio</forenames></author><author><keyname>Soriente</keyname><forenames>Claudio</forenames></author><author><keyname>Capkun</keyname><forenames>Srdjan</forenames></author></authors><title>Sound-Proof: Usable Two-Factor Authentication Based on Ambient Sound</title><categories>cs.CR cs.HC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-factor authentication protects online accounts even if passwords are
leaked. Most users, however, prefer password-only authentication. One reason
why two-factor authentication is so unpopular is the extra steps that the user
must complete in order to log in. Currently deployed two-factor authentication
mechanisms require the user to interact with his phone to, for example, copy a
verification code to the browser. Two-factor authentication schemes that
eliminate user-phone interaction exist, but require additional software to be
deployed.
  In this paper we propose Sound-Proof, a usable and deployable two-factor
authentication mechanism. Sound-Proof does not require interaction between the
user and his phone. In Sound-Proof the second authentication factor is the
proximity of the user's phone to the device being used to log in. The proximity
of the two devices is verified by comparing the ambient noise recorded by their
microphones. Audio recording and comparison are transparent to the user, so
that the user experience is similar to the one of password-only authentication.
Sound-Proof can be easily deployed as it works with current phones and major
browsers without plugins. We build a prototype for both Android and iOS. We
provide empirical evidence that ambient noise is a robust discriminant to
determine the proximity of two devices both indoors and outdoors, and even if
the phone is in a pocket or purse. We conduct a user study designed to compare
the perceived usability of Sound-Proof with Google 2-Step Verification.
Participants ranked Sound-Proof as more usable and the majority would be
willing to use Sound-Proof even for scenarios in which two-factor
authentication is optional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03791</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03791</id><created>2015-03-12</created><updated>2015-05-21</updated><authors><author><keyname>Andres</keyname><forenames>Bjoern</forenames></author></authors><title>Lifting of Multicuts</title><categories>cs.DM cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every simple, undirected graph $G = (V, E)$, a one-to-one relation exists
between the decompositions and the multicuts of $G$. A decomposition of $G$ is
a partition $\Pi$ of $V$ such that, for every $U \in \Pi$, the subgraph of $G$
induced by $U$ is connected. A multicut of $G$ is an $M \subseteq E$ such that,
for every (chordless) cycle $C \subseteq E$ of $G$, $|M \cap C| \neq 1$. The
multicut related to a decomposition is the set of edges that straddle distinct
components. The characteristic function $x \in \{0, 1\}^E$ of a multicut $M =
x^{-1}(1)$ of $G$ makes explicit, for every pair $\{v,w\} \in E$ of neighboring
nodes, whether $v$ and $w$ are in distinct components. In order to make
explicit also for non-neighboring nodes, specifically, for all $\{v,w\} \in E'$
with $E \subseteq E' \subseteq {V \choose 2}$, whether $v$ and $w$ are in
distinct components, we define a lifting of the multicuts of $G$ to multicuts
of $G' = (V, E')$. We show that, if $G$ is connected, the convex hull of the
characteristic functions of those multicuts of $G'$ that are lifted from $G$ is
an $|E'|$-dimensional polytope in $\mathbb{R}^{E'}$. We establish properties of
trivial facets of this polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03794</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03794</id><created>2015-03-12</created><authors><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author><author><keyname>Sousa</keyname><forenames>V. L.</forenames></author><author><keyname>N.</keyname><forenames>H. A.</forenames></author><author><keyname>de Souza</keyname><forenames>R. M. Campello</forenames></author></authors><title>Radix-2 Fast Hartley Transform Revisited</title><categories>cs.DM</categories><comments>5 pages, 4 figures: Anais do I Congresso de Inform\'atica da
  Amaz\^onia, 2001. v.1.pp.285-292</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Fast algorithm for the Discrete Hartley Transform (DHT) is presented, which
resembles radix-2 fast Fourier Transform (FFT). Although fast DHTs are already
known, this new approach bring some light about the deep relationship between
fast DHT algorithms and a multiplication-free fast algorithm for the Hadamard
Transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03818</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03818</id><created>2015-03-12</created><updated>2015-03-19</updated><authors><author><keyname>Aubakir</keyname><forenames>Bauyrzhan</forenames></author><author><keyname>Kappasov</keyname><forenames>Zhanat</forenames></author><author><keyname>Shintemirov</keyname><forenames>Almas</forenames></author></authors><title>Practical Realization of the Self-Balancing Robot Using Infrared Sensors</title><categories>cs.RO</categories><comments>Proceedings of the 5th International Competition of the student IT
  projects, KBTU, Kazakhstan, 2012, 6 pages, 8 figures, in Russian Language</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The idea of a two wheel self-balancing robot has become very popular among
control system researchers worldwide over the last decade. This paper presents
a one variant of the implementation of the self-balancing robot using the VEX
Robotics Kit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03821</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03821</id><created>2015-03-12</created><authors><author><keyname>Kirthi</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>A Class of Random Sequences for Key Generation</title><categories>cs.CR</categories><comments>7 pages; 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates randomness properties of sequences derived from
Fibonacci and Gopala-Hemachandra sequences modulo m for use in key distribution
applications. We show that for sequences modulo a prime a binary random
sequence B(n) is obtained based on whether the period is p-1 (or a divisor) or
2p+2 (or a divisor). For the more general case of arbitrary m, we use the
property if the period is a multiple of 8 or not. The sequences for prime
modulo have much better autocorrelation properties. These are good candidates
for key distribution since the generation process is not computationally
complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03832</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03832</id><created>2015-03-12</created><updated>2015-06-17</updated><authors><author><keyname>Schroff</keyname><forenames>Florian</forenames></author><author><keyname>Kalenichenko</keyname><forenames>Dmitry</forenames></author><author><keyname>Philbin</keyname><forenames>James</forenames></author></authors><title>FaceNet: A Unified Embedding for Face Recognition and Clustering</title><categories>cs.CV</categories><comments>Also published, in Proceedings of the IEEE Computer Society
  Conference on Computer Vision and Pattern Recognition 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite significant recent advances in the field of face recognition,
implementing face verification and recognition efficiently at scale presents
serious challenges to current approaches. In this paper we present a system,
called FaceNet, that directly learns a mapping from face images to a compact
Euclidean space where distances directly correspond to a measure of face
similarity. Once this space has been produced, tasks such as face recognition,
verification and clustering can be easily implemented using standard techniques
with FaceNet embeddings as feature vectors.
  Our method uses a deep convolutional network trained to directly optimize the
embedding itself, rather than an intermediate bottleneck layer as in previous
deep learning approaches. To train, we use triplets of roughly aligned matching
/ non-matching face patches generated using a novel online triplet mining
method. The benefit of our approach is much greater representational
efficiency: we achieve state-of-the-art face recognition performance using only
128-bytes per face.
  On the widely used Labeled Faces in the Wild (LFW) dataset, our system
achieves a new record accuracy of 99.63%. On YouTube Faces DB it achieves
95.12%. Our system cuts the error rate in comparison to the best published
result by 30% on both datasets.
  We also introduce the concept of harmonic embeddings, and a harmonic triplet
loss, which describe different versions of face embeddings (produced by
different networks) that are compatible to each other and allow for direct
comparison between each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03851</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03851</id><created>2015-03-12</created><updated>2015-10-30</updated><authors><author><keyname>Makarychev</keyname><forenames>Konstantin</forenames></author><author><keyname>Makarychev</keyname><forenames>Yury</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Satisfiability of Ordering CSPs Above Average</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the satisfiability of ordering constraint satisfaction problems
(CSPs) above average. We prove the conjecture of Gutin, van Iersel, Mnich, and
Yeo that the satisfiability above average of ordering CSPs of arity $k$ is
fixed-parameter tractable for every $k$. Previously, this was only known for
$k=2$ and $k=3$. We also generalize this result to more general classes of
CSPs, including CSPs with predicates defined by linear inequalities.
  To obtain our results, we prove a new Bonami-type inequality for the
Efron-Stein decomposition. The inequality applies to functions defined on
arbitrary product probability spaces. In contrast to other variants of the
Bonami Inequality, it does not depend on the mass of the smallest atom in the
probability space. We believe that this inequality is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03877</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03877</id><created>2015-03-12</created><authors><author><keyname>Chaudhary</keyname><forenames>Ruchi</forenames></author><author><keyname>Fernandez-Baca</keyname><forenames>David</forenames></author><author><keyname>Burleigh</keyname><forenames>J. Gordon</forenames></author></authors><title>Constructing and Employing Tree Alignment Graphs for Phylogenetic
  Synthesis</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree alignment graphs (TAGs) provide an intuitive data structure for storing
phylogenetic trees that exhibits the relationships of the individual input
trees and can potentially account for nested taxonomic relationships. This
paper provides a theoretical foundation for the use of TAGs in phylogenetics.
We provide a formal definition of TAG that - unlike previous definition - does
not depend on the order in which input trees are provided. In the consensus
case, when all input trees have the same leaf labels, we describe algorithms
for constructing majority-rule and strict consensus trees using the TAG. When
the input trees do not have identical sets of leaf labels, we describe how to
determine if the input trees are compatible and, if they are compatible, to
construct a supertree that contains the input trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03880</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03880</id><created>2015-03-12</created><updated>2015-08-27</updated><authors><author><keyname>Leduc-Primeau</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Modeling and Energy Optimization of LDPC Decoder Circuits with Timing
  Violations</title><categories>cs.IT cs.AR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a quasi-synchronous design approach for signal processing
circuits, in which timing violations are permitted, but without the need for a
hardware compensation mechanism. The error-correction performance of
low-density parity-check (LDPC) code ensembles is evaluated using density
evolution while taking into account the effect of timing faults, and a method
for accurately modeling the effect of faults at a high level of abstraction is
presented. Following this, several quasi-synchronous LDPC decoder circuits are
designed based on the offset min-sum algorithm, providing a 23%-40% reduction
in energy consumption or energy-delay product, while achieving the same
performance and occupying the same area as conventional synchronous circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03884</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03884</id><created>2015-03-12</created><authors><author><keyname>Cristina</keyname><forenames>Turcu</forenames></author><author><keyname>Tudor</keyname><forenames>Cerlinca</forenames></author><author><keyname>Marius</keyname><forenames>Cerlinca</forenames></author><author><keyname>Remus</keyname><forenames>Prodan</forenames></author><author><keyname>Cornel</keyname><forenames>Turcu</forenames></author><author><keyname>Felicia</keyname><forenames>G&#xee;za</forenames></author></authors><title>An RFID-based Clinical Information System for Identification and
  Monitoring of Patients</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Managing health-care records and information is an imperative necessity. Most
patient health records are stored in separate systems and there are still huge
paper trails of records that health-care providers must keep to comply with
different regulations. This paper proposes an RFID-based system, named SIMOPAC,
that integrate RFID technology in health care in order to make patient
emergency care as efficient and risk-free as possible, by providing doctors
with as much information about a patient as quickly as possible. Every hospital
could use SIMOPAC with their existing system in order to promote patient safety
and optimize hospital workflow. We will concentrate on the RFID technology and
how it could be used in emergency care. We describe a general purpose
architecture and data model that is designed for collecting ambulatory data
from various existing devices and systems, as well as for storing and
presenting clinically significant information to the emergency care physician.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03887</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03887</id><created>2015-03-12</created><authors><author><keyname>Ungureanu</keyname><forenames>Ioan</forenames></author><author><keyname>Turcu</keyname><forenames>Cristina Elena</forenames></author><author><keyname>Turcu</keyname><forenames>Cornel</forenames></author><author><keyname>Gaitan</keyname><forenames>Vasile Gheorghita</forenames></author></authors><title>Intelligent Device Used by an Infotmation System for Identifying and
  Monitoring of Patients</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper consists in defining the hardware and software
architecture of an embedded system, based on RFID technology, in order to
identify patients and to achieve real time information concerning the patients
biometric data, which might be used in different points of the health system
(laboratory, family physician, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03888</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03888</id><created>2015-03-12</created><updated>2015-05-10</updated><authors><author><keyname>Macdonald</keyname><forenames>Jeremy</forenames></author><author><keyname>Myasnikov</keyname><forenames>Alexei</forenames></author><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author><author><keyname>Vassileva</keyname><forenames>Svetla</forenames></author></authors><title>Logspace and compressed-word computations in nilpotent groups</title><categories>math.GR cs.CC</categories><comments>Updated bibliography, improved results and exposition in Section 5</comments><msc-class>20F10, 20F14, 20F18, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For finitely generated nilpotent groups, we employ Mal'cev coordinates to
solve several classical algorithmic problems efficiently. Computation of normal
forms, the membership problem, the conjugacy problem, and computation of
presentations for subgroups are solved using only logarithmic space and
quasilinear time. Logarithmic space presentation-uniform versions of these
algorithms are provided. Compressed-word versions of the same problems, in
which each input word is provided as a straight-line program, are solved in
polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03893</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03893</id><created>2015-03-12</created><authors><author><keyname>Yu</keyname><forenames>Felix X.</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Rowley</keyname><forenames>Henry</forenames></author><author><keyname>Chang</keyname><forenames>Shih-Fu</forenames></author></authors><title>Compact Nonlinear Maps and Circulant Extensions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel approximation via nonlinear random feature maps is widely used in
speeding up kernel machines. There are two main challenges for the conventional
kernel approximation methods. First, before performing kernel approximation, a
good kernel has to be chosen. Picking a good kernel is a very challenging
problem in itself. Second, high-dimensional maps are often required in order to
achieve good performance. This leads to high computational cost in both
generating the nonlinear maps, and in the subsequent learning and prediction
process. In this work, we propose to optimize the nonlinear maps directly with
respect to the classification objective in a data-dependent fashion. The
proposed approach achieves kernel approximation and kernel learning in a joint
framework. This leads to much more compact maps without hurting the
performance. As a by-product, the same framework can also be used to achieve
more compact kernel maps to approximate a known kernel. We also introduce
Circulant Nonlinear Maps, which uses a circulant-structured projection matrix
to speed up the nonlinear maps for high-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03900</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03900</id><created>2015-03-12</created><authors><author><keyname>Shiromoto</keyname><forenames>Humberto Stein</forenames></author><author><keyname>Andrieu</keyname><forenames>Vincent</forenames></author><author><keyname>Prieur</keyname><forenames>Christophe</forenames></author></authors><title>Union d'une commande par backstepping avec une commande locale</title><categories>math.OC cs.SY</categories><comments>in French, in Proceedings of the 7e Conf\'erence Internationale
  Francophone d'Automatique (CIFA'12), Grenoble, France, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems for which the backstepping technique cannot be applied are
considered. A criterion for the design of a hybrid feedback law is proposed by
blending a local stabilizer with a backstepping controller. This hybrid
feedback law renders the origin globally asymptotically stable for the
closed-loop system. The selection criterion is based on choice of the size of a
compact set included in the basin of attraction of the local controller. The
results are illustrated by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03903</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03903</id><created>2015-03-12</created><authors><author><keyname>Kundu</keyname><forenames>Abhisek</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Approximating Sparse PCA from Incomplete Data</title><categories>cs.LG cs.IT cs.NA math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how well one can recover sparse principal components of a data
matrix using a sketch formed from a few of its elements. We show that for a
wide class of optimization problems, if the sketch is close (in the spectral
norm) to the original data matrix, then one can recover a near optimal solution
to the optimization problem by using the sketch. In particular, we use this
approach to obtain sparse principal components and show that for \math{m} data
points in \math{n} dimensions, \math{O(\epsilon^{-2}\tilde k\max\{m,n\})}
elements gives an \math{\epsilon}-additive approximation to the sparse PCA
problem (\math{\tilde k} is the stable rank of the data matrix). We demonstrate
our algorithms extensively on image, text, biological and financial data. The
results show that not only are we able to recover the sparse PCAs from the
incomplete data, but by using our sparse sketch, the running time drops by a
factor of five or more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03905</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03905</id><created>2015-03-12</created><authors><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author><author><keyname>Wu</keyname><forenames>Yi</forenames></author></authors><title>Local Distribution and the Symmetry Gap: Approximability of Multiway
  Partitioning Problems</title><categories>cs.DS</categories><comments>This is the full version of our SODA 2013 paper. Full proofs have
  been included and one erroneous claim has been removed (brute-force rounding
  for Min-CSP problems)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the approximability of multiway partitioning problems, examples of
which include Multiway Cut, Node-weighted Multiway Cut, and Hypergraph Multiway
Cut. We investigate these problems from the point of view of two possible
generalizations: as Min-CSPs, and as Submodular Multiway Partition problems.
These two generalizations lead to two natural relaxations, the Basic LP, and
the Lovasz relaxation. We show that the Lovasz relaxation gives a
(2-2/k)-approximation for Submodular Multiway Partition with $k$ terminals,
improving a recent 2-approximation. We prove that this factor is optimal in two
senses: (1) A (2-2/k-\epsilon)-approximation for Submodular Multiway Partition
with k terminals would require exponentially many value queries. (2) For
Hypergraph Multiway Cut and Node-weighted Multiway Cut with k terminals, both
special cases of Submodular Multiway Partition, we prove that a
(2-2/k-\epsilon)-approximation is NP-hard, assuming the Unique Games
Conjecture.
  Both our hardness results are more general: (1) We show that the notion of
symmetry gap, previously used for submodular maximization problems, also
implies hardness results for submodular minimization problems. (2) Assuming the
Unique Games Conjecture, we show that the Basic LP gives an optimal
approximation for every Min-CSP that includes the Not-Equal predicate.
  Finally, we connect the two hardness techniques by proving that the
integrality gap of the Basic LP coincides with the symmetry gap of the
multilinear relaxation (for a related instance). This shows that the appearance
of the same hardness threshold for a Min-CSP and the related submodular
minimization problem is not a coincidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03909</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03909</id><created>2015-03-12</created><authors><author><keyname>Hosseinmardi</keyname><forenames>Homa</forenames></author><author><keyname>Mattson</keyname><forenames>Sabrina Arredondo</forenames></author><author><keyname>Rafiq</keyname><forenames>Rahat Ibn</forenames></author><author><keyname>Han</keyname><forenames>Richard</forenames></author><author><keyname>Lv</keyname><forenames>Qin</forenames></author><author><keyname>Mishra</keyname><forenames>Shivakant</forenames></author></authors><title>Detection of Cyberbullying Incidents on the Instagram Social Network</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyberbullying is a growing problem affecting more than half of all American
teens. The main goal of this paper is to investigate fundamentally new
approaches to understand and automatically detect incidents of cyberbullying
over images in Instagram, a media-based mobile social network. To this end, we
have collected a sample Instagram data set consisting of images and their
associated comments, and designed a labeling study for cyberbullying as well as
image content using human labelers at the crowd-sourced Crowdflower Web site.
An analysis of the labeled data is then presented, including a study of
correlations between different features and cyberbullying as well as
cyberaggression. Using the labeled data, we further design and evaluate the
accuracy of a classifier to automatically detect incidents of cyberbullying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03912</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03912</id><created>2015-03-12</created><authors><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Claussen</keyname><forenames>Holger</forenames></author><author><keyname>Jafari</keyname><forenames>Amir H.</forenames></author></authors><title>Towards 1 Gbps/UE in Cellular Systems: Understanding Ultra-Dense Small
  Cell Deployments</title><categories>cs.NI</categories><report-no>1553-877X</report-no><journal-ref>IEEE Communications Surveys &amp; Tutorials, vol. PP, no 99, Page(s):1
  - 24, Jun., 2015</journal-ref><doi>10.1109/COMST.2015.2439636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Todays heterogeneous networks comprised of mostly macrocells and indoor small
cells will not be able to meet the upcoming traffic demands. Indeed, it is
forecasted that at least a 100x network capacity increase will be required to
meet the traffic demands in 2020. As a result, vendors and operators are now
looking at using every tool at hand to improve network capacity. In this epic
campaign, three paradigms are noteworthy, i.e., network densification, the use
of higher frequency bands and spectral efficiency enhancement techniques. This
paper aims at bringing further common understanding and analysing the potential
gains and limitations of these three paradigms, together with the impact of
idle mode capabilities at the small cells as well as the user equipment density
and distribution in outdoor scenarios. Special attention is paid to network
densification and its implications when transitioning to ultra-dense small cell
deployments. Simulation results show that network densification with an average
inter site distance of 35 m can increase the cell- edge UE throughput by up to
48x, while the use of the 10GHz band with a 500MHz bandwidth can increase the
network capacity up to 5x. The use of beamforming with up to 4 antennas per
small cell base station lacks behind with cell-edge throughput gains of up to
1.49x. Our study also shows how network densifications reduces multi-user
diversity, and thus proportional fair alike schedulers start losing their
advantages with respect to round robin ones. The energy efficiency of these
ultra-dense small cell deployments is also analysed, indicating the need for
energy harvesting approaches to make these deployments energy- efficient.
Finally, the top ten challenges to be addressed to bring ultra-dense small cell
deployments to reality are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03913</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03913</id><created>2015-03-12</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Mandal</keyname><forenames>Soham</forenames></author><author><keyname>Das</keyname><forenames>Nandan K</forenames></author><author><keyname>Dey</keyname><forenames>Subhadip</forenames></author><author><keyname>Mitra</keyname><forenames>Asish</forenames></author><author><keyname>Ghosh</keyname><forenames>Nirmalya</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K</forenames></author></authors><title>Diagnosing Heterogeneous Dynamics for CT Scan Images of Human Brain in
  Wavelet and MFDFA domain</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CT scan images of human brain of a particular patient in different cross
sections are taken, on which wavelet transform and multi-fractal analysis are
applied. The vertical and horizontal unfolding of images are done before
analyzing these images. A systematic investigation of de-noised CT scan images
of human brain in different cross-sections are carried out through wavelet
normalized energy and wavelet semi-log plots, which clearly points out the
mismatch between results of vertical and horizontal unfolding. The mismatch of
results confirms the heterogeneity in spatial domain. Using the multi-fractal
de-trended fluctuation analysis (MFDFA), the mismatch between the values of
Hurst exponent and width of singularity spectrum by vertical and horizontal
unfolding confirms the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03920</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03920</id><created>2015-03-12</created><authors><author><keyname>Alqhtani</keyname><forenames>Samar M.</forenames></author><author><keyname>Luo</keyname><forenames>Suhuai</forenames></author><author><keyname>Regan</keyname><forenames>Brian</forenames></author></authors><title>Fusing Text and Image for Event Detection in Twitter</title><categories>cs.IR cs.MM</categories><comments>9 Pages, 4 figuers</comments><doi>10.5121/ijma.2015.7103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we develop an accurate and effective event detection
method to detect events from a Twitter stream, which uses visual and textual
information to improve the performance of the mining process. The method
monitors a Twitter stream to pick up tweets having texts and images and stores
them into a database. This is followed by applying a mining algorithm to detect
an event. The procedure starts with detecting events based on text only by
using the feature of the bag-of-words which is calculated using the term
frequency-inverse document frequency (TF-IDF) method. Then it detects the event
based on image only by using visual features including histogram of oriented
gradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color
histogram. K nearest neighbours (Knn) classification is used in the detection.
The final decision of the event detection is made based on the reliabilities of
text only detection and image only detection. The experiment result showed that
the proposed method achieved high accuracy of 0.94, comparing with 0.89 with
texts only, and 0.86 with images only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03923</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03923</id><created>2015-03-12</created><updated>2015-05-05</updated><authors><author><keyname>Dembo</keyname><forenames>Amir</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Sen</keyname><forenames>Subhabrata</forenames></author></authors><title>Extremal Cuts of Sparse Random Graphs</title><categories>math.PR cs.DM math.CO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Erd\H{o}s-R\'enyi random graphs with average degree $\gamma$, and
uniformly random $\gamma$-regular graph on $n$ vertices, we prove that with
high probability the size of both the Max-Cut and maximum bisection are
$n\Big(\frac{\gamma}{4} + {{\sf P}}_* \sqrt{\frac{\gamma}{4}} +
o(\sqrt{\gamma})\Big) + o(n)$ while the size of the minimum bisection is
$n\Big(\frac{\gamma}{4}-{{\sf P}}_*\sqrt{\frac{\gamma}{4}} +
o(\sqrt{\gamma})\Big) + o(n)$. Our derivation relates the free energy of the
anti-ferromagnetic Ising model on such graphs to that of the
Sherrington-Kirkpatrick model, with ${{\sf P}}_* \approx 0.7632$ standing for
the ground state energy of the latter, expressed analytically via Parisi's
formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03940</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03940</id><created>2015-03-12</created><authors><author><keyname>Sun</keyname><forenames>Yixin</forenames></author><author><keyname>Edmundson</keyname><forenames>Anne</forenames></author><author><keyname>Vanbever</keyname><forenames>Laurent</forenames></author><author><keyname>Li</keyname><forenames>Oscar</forenames></author><author><keyname>Rexford</keyname><forenames>Jennifer</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author><author><keyname>Mittal</keyname><forenames>Prateek</forenames></author></authors><title>RAPTOR: Routing Attacks on Privacy in Tor</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tor network is a widely used system for anonymous communication. However,
Tor is known to be vulnerable to attackers who can observe traffic at both ends
of the communication path. In this paper, we show that prior attacks are just
the tip of the iceberg. We present a suite of new attacks, called Raptor, that
can be launched by Autonomous Systems (ASes) to compromise user anonymity.
First, AS-level adversaries can exploit the asymmetric nature of Internet
routing to increase the chance of observing at least one direction of user
traffic at both ends of the communication. Second, AS-level adversaries can
exploit natural churn in Internet routing to lie on the BGP paths for more
users over time. Third, strategic adversaries can manipulate Internet routing
via BGP hijacks (to discover the users using specific Tor guard nodes) and
interceptions (to perform traffic analysis). We demonstrate the feasibility of
Raptor attacks by analyzing historical BGP data and Traceroute data as well as
performing real-world attacks on the live Tor network, while ensuring that we
do not harm real users. In addition, we outline the design of two monitoring
frameworks to counter these attacks: BGP monitoring to detect control-plane
attacks, and Traceroute monitoring to detect data-plane anomalies. Overall, our
work motivates the design of anonymity systems that are aware of the dynamics
of Internet routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03942</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03942</id><created>2015-03-12</created><authors><author><keyname>Yu</keyname><forenames>Mingchao</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>On Minimizing the Average Packet Decoding Delay in Wireless Network
  Coded Broadcast</title><categories>cs.IT math.IT</categories><comments>5 figures, submitted to NetCod 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting in which a sender wishes to broadcast a block of K data
packets to a set of wireless receivers, where each of the receivers has a
subset of the data packets already available to it (e.g., from prior
transmissions) and wants the rest of the packets. Our goal is to find a linear
network coding scheme that yields the minimum average packet decoding delay
(APDD), i.e., the average time it takes for a receiver to decode a data packet.
Our contributions can be summarized as follows. First, we prove that this
problem is NP-hard by presenting a reduction from the hypergraph coloring
problem. Next, we show that %\alexn{an MDS-based solution or} a random linear
network coding (RLNC) provides an approximate solution to this problem with
approximation ratio $2$ with high probability. Next, we present a methodology
for designing specialized approximation algorithms for this problem that
outperform RLNC solutions while maintaining the same throughput. In a special
case of practical interest with a small number of wanted packets our solution
can achieve an approximation ratio (4-2/K)/3. Finally, we conduct an
experimental study that demonstrates the advantages of the presented
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03952</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03952</id><created>2015-03-13</created><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author><author><keyname>Gupta</keyname><forenames>Vijay</forenames></author></authors><title>A Switched Dynamical System Framework for Analysis of Massively Parallel
  Asynchronous Numerical Algorithms</title><categories>cs.SY math.DS</categories><comments>ACC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the near future, massively parallel computing systems will be necessary to
solve computation intensive applications. The key bottleneck in massively
parallel implementation of numerical algorithms is the synchronization of data
across processing elements (PEs) after each iteration, which results in
significant idle time. Thus, there is a trend towards relaxing the
synchronization and adopting an asynchronous model of computation to reduce
idle time. However, it is not clear what is the effect of this relaxation on
the stability and accuracy of the numerical algorithm. In this paper we present
a new framework to analyze such algorithms. We treat the computation in each PE
as a dynamical system and model the asynchrony as stochastic switching. The
overall system is then analyzed as a switched dynamical system. However,
modeling of massively parallel numerical algorithms as switched dynamical
systems results in a very large number of modes, which makes current analysis
tools available for such systems computationally intractable. We develop new
techniques that circumvent this scalability issue. The framework is presented
on a one-dimensional heat equation as a case study and the proposed analysis
framework is verified by solving the partial differential equation (PDE) in a
$\mathtt{nVIDIA\: Tesla^{\scriptsize{TM}}}$ GPU machine, with asynchronous
communication between cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03954</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03954</id><created>2015-03-13</created><authors><author><keyname>Liao</keyname><forenames>Yun</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author></authors><title>Full-Duplex Cognitive Radio: A New Design Paradigm for Enhancing
  Spectrum Usage</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth of demand for ever-increasing data rate, spectrum
resources have become more and more scarce. As a promising technique to
increase the efficiency of the spectrum utilization, cognitive radio (CR)
technique has the great potential to meet such a requirement by allowing
un-licensed users to coexist in licensed bands. In conventional CR systems, the
spectrum sensing is performed at the beginning of each time slot before the
data transmission. This unfortunately results in two major problems: 1)
transmission time reduction due to sensing, and 2) sensing accuracy impairment
due to data transmission. To tackle these problems, in this paper we present a
new design paradigm for future CR by exploring the full-duplex (FD) techniques
to achieve the simultaneous spectrum sensing and data transmission. With FD
radios equipped at the secondary users (SUs), SUs can simultaneously sense and
access the vacant spectrum, and thus, significantly improve sensing
performances and meanwhile increase data transmission efficiency. The aim of
this article is to transform the promising conceptual framework into the
practical wireless network design by addressing a diverse set of challenges
such as protocol design and theoretical analysis. Several application scenarios
with FD enabled CR are elaborated, and key open research directions and novel
algorithms in these systems are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03957</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03957</id><created>2015-03-13</created><authors><author><keyname>Singh</keyname><forenames>Prabhjot</forenames></author><author><keyname>Dhawan</keyname><forenames>Sumit</forenames></author><author><keyname>Agarwal</keyname><forenames>Shubham</forenames></author><author><keyname>Thakur</keyname><forenames>Narina</forenames></author></authors><title>Implementation of an efficient Fuzzy Logic based Information Retrieval
  System</title><categories>cs.IR</categories><comments>arXiv admin note: substantial text overlap with
  http://ntz-develop.blogspot.in/ ,
  http://www.micsymposium.org/mics2012/submissions/mics2012_submission_8.pdf ,
  http://www.slideshare.net/JeffreyStricklandPhD/predictive-modeling-and-analytics-selectchapters-41304405
  by other authors</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper exemplifies the implementation of an efficient Information
Retrieval (IR) System to compute the similarity between a dataset and a query
using Fuzzy Logic. TREC dataset has been used for the same purpose. The dataset
is parsed to generate keywords index which is used for the similarity
comparison with the user query. Each query is assigned a score value based on
its fuzzy similarity with the index keywords. The relevant documents are
retrieved based on the score value. The performance and accuracy of the
proposed fuzzy similarity model is compared with Cosine similarity model using
Precision-Recall curves. The results prove the dominance of Fuzzy Similarity
based IR system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03961</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03961</id><created>2015-03-13</created><authors><author><keyname>Qiang</keyname><forenames>Runwei</forenames></author><author><keyname>Fan</keyname><forenames>Feifan</forenames></author><author><keyname>Lv</keyname><forenames>Chao</forenames></author><author><keyname>Yang</keyname><forenames>Jianwu</forenames></author></authors><title>Knowledge-based Query Expansion in Real-Time Microblog Search</title><categories>cs.IR</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the length of microblog texts, such as tweets, is strictly limited to
140 characters, traditional Information Retrieval techniques suffer from the
vocabulary mismatch problem severely and cannot yield good performance in the
context of microblogosphere. To address this critical challenge, in this paper,
we propose a new language modeling approach for microblog retrieval by
inferring various types of context information. In particular, we expand the
query using knowledge terms derived from Freebase so that the expanded one can
better reflect users' search intent. Besides, in order to further satisfy
users' real-time information need, we incorporate temporal evidences into the
expansion method, which can boost recent tweets in the retrieval results with
respect to a given topic. Experimental results on two official TREC Twitter
corpora demonstrate the significant superiority of our approach over baseline
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03964</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03964</id><created>2015-03-13</created><authors><author><keyname>Yoshida</keyname><forenames>Shunsuke</forenames></author><author><keyname>Hisakado</keyname><forenames>Masato</forenames></author><author><keyname>Mori</keyname><forenames>Shintaro</forenames></author></authors><title>Interactive Restless Multi-armed Bandit Game and Swarm Intelligence
  Effect</title><categories>cs.AI cs.LG physics.data-an stat.ML</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain the conditions for the emergence of the swarm intelligence effect
in an interactive game of restless multi-armed bandit (rMAB). A player competes
with multiple agents. Each bandit has a payoff that changes with a probability
$p_{c}$ per round. The agents and player choose one of three options: (1)
Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among
$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good
bandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:
(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability
for Observe in learning. The parameters $(c,p_{obs})$ are uniformly
distributed. We determine the optimal strategies for the player using complete
knowledge about the rMAB. We show whether or not social or asocial learning is
more optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence
effect. We conduct a laboratory experiment (67 subjects) and observe the swarm
intelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning
is far more optimal than asocial learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03974</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03974</id><created>2015-03-13</created><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Posenato</keyname><forenames>Roberto</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Hyper Temporal Networks</title><categories>cs.DS cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple Temporal Networks (STNs) provide a powerful and general tool for
representing conjunctions of maximum delay constraints over ordered pairs of
temporal variables. In this paper we introduce Hyper Temporal Networks (HyTNs),
a strict generalization of STNs, to overcome the limitation of considering only
conjunctions of constraints but maintaining a practical efficiency in the
consistency check of the instances. In a Hyper Temporal Network a single
temporal hyperarc constraint may be defined as a set of two or more maximum
delay constraints which is satisfied when at least one of these delay
constraints is satisfied. HyTNs are meant as a light generalization of STNs
offering an interesting compromise. On one side, there exist practical
pseudo-polynomial time algorithms for checking consistency and computing
feasible schedules for HyTNs. On the other side, HyTNs offer a more powerful
model accommodating natural constraints that cannot be expressed by STNs like
Trigger off exactly delta min before (after) the occurrence of the first (last)
event in a set., which are used to represent synchronization events in some
process aware information systems/workflow models proposed in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03989</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03989</id><created>2015-03-13</created><authors><author><keyname>Rahman</keyname><forenames>Mirzanur</forenames></author><author><keyname>Sarma</keyname><forenames>Shikhar Kumar</forenames></author></authors><title>An implementation of Apertium based Assamese morphological analyzer</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphological Analysis is an important branch of linguistics for any Natural
Language Processing Technology. Morphology studies the word structure and
formation of word of a language. In current scenario of NLP research,
morphological analysis techniques have become more popular day by day. For
processing any language, morphology of the word should be first analyzed.
Assamese language contains very complex morphological structure. In our work we
have used Apertium based Finite-State-Transducers for developing morphological
analyzer for Assamese Language with some limited domain and we get 72.7%
accuracy
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.03997</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.03997</id><created>2015-03-13</created><authors><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Raviteja</keyname><forenames>P.</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Generalized Spatial Modulation in Large-Scale Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><comments>IEEE Trans. on Wireless Communications, accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized spatial modulation (GSM) uses $n_t$ transmit antenna elements but
fewer transmit radio frequency (RF) chains, $n_{rf}$. Spatial modulation (SM)
and spatial multiplexing are special cases of GSM with $n_{rf}=1$ and
$n_{rf}=n_t$, respectively. In GSM, in addition to conveying information bits
through $n_{rf}$ conventional modulation symbols (for example, QAM), the
indices of the $n_{rf}$ active transmit antennas also convey information bits.
In this paper, we investigate {\em GSM for large-scale multiuser MIMO
communications on the uplink}. Our contributions in this paper include: ($i$)
an average bit error probability (ABEP) analysis for maximum-likelihood
detection in multiuser GSM-MIMO on the uplink, where we derive an upper bound
on the ABEP, and ($ii$) low-complexity algorithms for GSM-MIMO signal detection
and channel estimation at the base station receiver based on message passing.
The analytical upper bounds on the ABEP are found to be tight at moderate to
high signal-to-noise ratios (SNR). The proposed receiver algorithms are found
to scale very well in complexity while achieving near-optimal performance in
large dimensions. Simulation results show that, for the same spectral
efficiency, multiuser GSM-MIMO can outperform multiuser SM-MIMO as well as
conventional multiuser MIMO, by about 2 to 9 dB at a bit error rate of
$10^{-3}$. Such SNR gains in GSM-MIMO compared to SM-MIMO and conventional MIMO
can be attributed to the fact that, because of a larger number of spatial index
bits, GSM-MIMO can use a lower-order QAM alphabet which is more power
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04003</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04003</id><created>2015-03-13</created><updated>2015-04-01</updated><authors><author><keyname>Krivulin</keyname><forenames>N.</forenames></author></authors><title>A tropical optimization approach in the analysis of pairwise comparison
  matrices</title><categories>math.OC cs.SY</categories><comments>16 pages</comments><msc-class>65K10 (Primary), 15A80, 65K05, 41A50, 90B50 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach to solve the problem of rating alternatives based
on their pairwise comparison. The problem is formulated in terms of tropical
algebra, and then reduced to the approximation of pairwise comparison matrices
by reciprocal matrices of unit rank. We represent the approximation problem in
a common form for both multiplicative and additive comparison scales. To solve
the problem obtained, tropical optimization techniques are applied, providing
new complete direct solutions to the rating problems in a compact vector, which
extend known solutions and involve less computational efforts. The results are
illustrated with numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04005</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04005</id><created>2015-03-12</created><updated>2015-06-15</updated><authors><author><keyname>Brijder</keyname><forenames>Robert</forenames></author></authors><title>Dominance and Deficiency for Petri Nets and Chemical Reaction Networks</title><categories>cs.DC</categories><comments>16 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by Anderson et al. [J. R. Soc. Interface, 2014] we study the
long-term behavior of discrete chemical reaction networks (CRNs). In
particular, using techniques from both Petri net theory and CRN theory, we
provide a powerful sufficient condition for a structurally-bounded CRN to have
the property that none of the non-terminal reactions can fire for all its
recurrent configurations. We compare this result and its proof with a related
result of Anderson et al. and show its consequences for the case of CRNs with
deficiency one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04006</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04006</id><created>2015-03-12</created><authors><author><keyname>Bhaumik</keyname><forenames>Jaydeb</forenames></author></authors><title>Synthesis of all Maximum Length Cellular Automata of Cell Size up to 12</title><categories>cs.OH</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum length CA has wide range of applications in design of linear block
code, cryptographic primitives and VLSI testing particularly in
Built-In-Self-Test. In this paper, an algorithm to compute all $n$-cell maximum
length CA-rule vectors is proposed. Also rule vectors for each primitive
polynomial in GF(2^2) to GF(2^{12} have been computed by simulation and they
have been listed.Programmable rule vectors based maximum length CA can be used
to design cryptographic primitives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04018</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04018</id><created>2015-03-13</created><updated>2015-04-29</updated><authors><author><keyname>Leroux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Sutre</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>On the Coverability Problem for Pushdown Vector Addition Systems in One
  Dimension</title><categories>cs.FL</categories><acm-class>F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Does the trace language of a given vector addition system (VAS) intersect
with a given context-free language? This question lies at the heart of several
verification questions involving recursive programs with integer parameters. In
particular, it is equivalent to the coverability problem for VAS that operate
on a pushdown stack. We show decidability in dimension one, based on an
analysis of a new model called grammar-controlled vector addition systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04030</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04030</id><created>2015-03-13</created><updated>2015-06-30</updated><authors><author><keyname>Pan</keyname><forenames>Cunhua</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Jiangzhou</forenames></author><author><keyname>Ren</keyname><forenames>Hong</forenames></author><author><keyname>Zhang</keyname><forenames>Wence</forenames></author><author><keyname>Huang</keyname><forenames>Nuo</forenames></author><author><keyname>Chen</keyname><forenames>Ming</forenames></author></authors><title>Totally Distributed Energy-Efficient Transmission in MIMO Interference
  Channels</title><categories>cs.IT cs.GT cs.NI math.IT</categories><comments>42 pages, 8 figures, accepted in TWC</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we consider the problem of maximizing the energy efficiency
(EE) for multi-input multi-output (MIMO) interference channels, subject to the
per-link power constraint. To avoid extensive information exchange among all
links, the optimization problem is formulated as a noncooperative game, where
each link maximizes its own EE. We show that this game always admits a Nash
equilibrium (NE) and the sufficient condition for the uniqueness of the NE is
derived for the case of arbitrary channel matrices, which can be checked in
practice. To reach the NE of this game, we develop a totally distributed EE
algorithm, in which each link updates its own transmit covariance matrix in a
completely distributed and asynchronous way: Some players may update their
solutions more frequently than others or even use the outdated interference
information. The sufficient conditions that guarantee the global convergence of
the proposed algorithm to the NE of the game have been given as well. We also
study the impact of the circuit power consumption on the sum-EE performance of
the proposed algorithm in the case when the links are separated sufficiently
far away. Moreover, the tradeoff between the sum-EE and the sum-spectral
efficiency (SE) is investigated with the proposed algorithm under two special
cases: 1) low transmit power constraint regime; 2) high transmit power
constraint regime. Finally, extensive simulations are conducted to evaluate the
impact of various system parameters on the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04034</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04034</id><created>2015-03-13</created><updated>2015-06-09</updated><authors><author><keyname>Clairambault</keyname><forenames>Pierre</forenames><affiliation>CNRS and ENS Lyon</affiliation></author></authors><title>Bounding linear head reduction and visible interaction through skeletons</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:6) 2015</journal-ref><doi>10.2168/LMCS-11(2:6)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the complexity of execution in higher-order
programming languages. Our study has two facets: on the one hand we give an
upper bound to the length of interactions between bounded P-visible strategies
in Hyland-Ong game semantics. This result covers models of programming
languages with access to computational effects like non-determinism, state or
control operators, but its semantic formulation causes a loose connection to
syntax. On the other hand we give a syntactic counterpart of our semantic
study: a non-elementary upper bound to the length of the linear head reduction
sequence (a low-level notion of reduction, close to the actual implementation
of the reduction of higher-order programs by abstract machines) of simply-typed
lambda-terms. In both cases our upper bounds are proved optimal by giving
matching lower bounds. These two results, although different in scope, are
proved using the same method: we introduce a simple reduction on finite trees
of natural numbers, hereby called interaction skeletons. We study this
reduction and give upper bounds to its complexity. We then apply this study by
giving two simulation results: a semantic one measuring progress in
game-theoretic interaction via interaction skeletons, and a syntactic one
establishing a correspondence between linear head reduction of terms satisfying
a locality condition called local scope and the reduction of interaction
skeletons. This result is then generalized to arbitrary terms by a local
scopization transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04036</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04036</id><created>2015-03-13</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author><author><keyname>Jayagopi</keyname><forenames>Dinesh Babu</forenames></author></authors><title>Characterizing driving behavior using automatic visual analysis</title><categories>cs.CV</categories><comments>4 pages,7 figures, IBM-ICARE2014</comments><acm-class>H.4.3</acm-class><doi>10.1145/2662117.2662126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present the problem of rash driving detection algorithm
using a single wide angle camera sensor, particularly useful in the Indian
context. To our knowledge this rash driving problem has not been addressed
using Image processing techniques (existing works use other sensors such as
accelerometer). Car Image processing literature, though rich and mature, does
not address the rash driving problem. In this work-in-progress paper, we
present the need to address this problem, our approach and our future plans to
build a rash driving detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04045</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04045</id><created>2015-03-13</created><updated>2015-04-13</updated><authors><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Karkkainen</keyname><forenames>Juha</forenames></author><author><keyname>Kempa</keyname><forenames>Dominik</forenames></author><author><keyname>Piatkowski</keyname><forenames>Marcin</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Sugimoto</keyname><forenames>Shiho</forenames></author></authors><title>Diverse Palindromic Factorization is NP-Complete</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that it is NP-complete to decide whether a given string can be
factored into palindromes that are each unique in the factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04055</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04055</id><created>2015-03-13</created><authors><author><keyname>Jansen</keyname><forenames>Bas</forenames></author></authors><title>Enron versus EUSES: A Comparison of Two Spreadsheet Corpora</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spreadsheets are widely used within companies and often form the basis for
business decisions. Numerous cases are known where incorrect information in
spreadsheets has lead to incorrect decisions. Such cases underline the
relevance of research on the professional use of spreadsheets.
  Recently a new dataset became available for research, containing over 15.000
business spreadsheets that were extracted from the Enron E-mail Archive. With
this dataset, we 1) aim to obtain a thorough understanding of the
characteristics of spreadsheets used within companies, and 2) compare the
characteristics of the Enron spreadsheets with the EUSES corpus which is the
existing state of the art set of spreadsheets that is frequently used in
spreadsheet studies.
  Our analysis shows that 1) the majority of spreadsheets are not large in
terms of worksheets and formulas, do not have a high degree of coupling, and
their formulas are relatively simple; 2) the spreadsheets from the EUSES corpus
are, with respect to the measured characteristics, quite similar to the Enron
spreadsheets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04058</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04058</id><created>2015-03-13</created><authors><author><keyname>Krause</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Danziger</keyname><forenames>Michael M.</forenames></author><author><keyname>Zlati&#x107;</keyname><forenames>Vinko</forenames></author></authors><title>Optimal redundancy against disjoint vulnerabilities in networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.CR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Redundancy is commonly used to guarantee continued functionality in networked
systems. However, often many nodes are vulnerable to the same failure or
adversary. A &quot;backup&quot; path is not sufficient if both paths depend on nodes
which share a vulnerability.For example, if two nodes of the Internet cannot be
connected without using routers belonging to a given untrusted entity, then all
of their communication-regardless of the specific paths utilized-will be
intercepted by the controlling entity.In this and many other cases, the
vulnerabilities affecting the network are disjoint: each node has exactly one
vulnerability but the same vulnerability can affect many nodes. To discover
optimal redundancy in this scenario, we describe each vulnerability as a color
and develop a &quot;color-avoiding percolation&quot; which uncovers a hidden
color-avoiding connectivity. We present algorithms for color-avoiding
percolation of general networks and an analytic theory for random graphs with
uniformly distributed colors including critical phenomena. We demonstrate our
theory by uncovering the hidden color-avoiding connectivity of the Internet. We
find that less well-connected countries are more likely able to communicate
securely through optimally redundant paths than highly connected countries like
the US. Our results reveal a new layer of hidden structure in complex systems
and can enhance security and robustness through optimal redundancy in a wide
range of systems including biological, economic and communications networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04063</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04063</id><created>2015-03-13</created><authors><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Modenini</keyname><forenames>Andrea</forenames></author><author><keyname>Piemontese</keyname><forenames>Amina</forenames></author><author><keyname>Ugolini</keyname><forenames>Alessandro</forenames></author></authors><title>On the Application of Multiuser Detection in Multibeam Satellite Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the achievable rates by a single user in multibeam satellite
scenarios. We show alternatives to the conventional symbol-by-symbol detection
applied at user terminals. Single user detection is known to suffer from strong
degradation when the terminal is located near the edge of the coverage area,
and when aggressive frequency reuse is adopted. For this reason, we consider
multiuser detection, and take into account the strongest interfering signal.
Moreover, we analyze a different transmission strategy, where the signals from
two adjacent beams jointly serve two users in a time division multiplexing
fashion. We describe an information-theoretic framework to compare different
transmission/detection strategies by computing the information rate of the user
in the reference beam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04065</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04065</id><created>2015-03-13</created><authors><author><keyname>Kulkarni</keyname><forenames>Praveen</forenames></author><author><keyname>Zepeda</keyname><forenames>Joaquin</forenames></author><author><keyname>Jurie</keyname><forenames>Frederic</forenames></author><author><keyname>Perez</keyname><forenames>Patrick</forenames></author><author><keyname>Chevallier</keyname><forenames>Louis</forenames></author></authors><title>Hybrid multi-layer Deep CNN/Aggregator feature for image classification</title><categories>cs.CV</categories><comments>Accepted in ICASSP 2015 conference, 5 pages including reference, 4
  figures and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (DCNN) have established a remarkable
performance benchmark in the field of image classification, displacing
classical approaches based on hand-tailored aggregations of local descriptors.
Yet DCNNs impose high computational burdens both at training and at testing
time, and training them requires collecting and annotating large amounts of
training data. Supervised adaptation methods have been proposed in the
literature that partially re-learn a transferred DCNN structure from a new
target dataset. Yet these require expensive bounding-box annotations and are
still computationally expensive to learn. In this paper, we address these
shortcomings of DCNN adaptation schemes by proposing a hybrid approach that
combines conventional, unsupervised aggregators such as Bag-of-Words (BoW),
with the DCNN pipeline by treating the output of intermediate layers as densely
extracted local descriptors.
  We test a variant of our approach that uses only intermediate DCNN layers on
the standard PASCAL VOC 2007 dataset and show performance significantly higher
than the standard BoW model and comparable to Fisher vector aggregation but
with a feature that is 150 times smaller. A second variant of our approach that
includes the fully connected DCNN layers significantly outperforms Fisher
vector schemes and performs comparably to DCNN approaches adapted to Pascal VOC
2007, yet at only a small fraction of the training and testing cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04066</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04066</id><created>2015-03-13</created><updated>2015-11-18</updated><authors><author><keyname>G&#xe9;nois</keyname><forenames>Mathieu</forenames></author><author><keyname>Vestergaard</keyname><forenames>Christian L.</forenames></author><author><keyname>Cattuto</keyname><forenames>Ciro</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author></authors><title>Compensating for population sampling in simulations of epidemic spread
  on temporal contact networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Nature Communications 6:8860 (2015)</journal-ref><doi>10.1038/ncomms9860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data describing human interactions often suffer from incomplete sampling of
the underlying population. As a consequence, the study of contagion processes
using data-driven models can lead to a severe underestimation of the epidemic
risk. Here we present a systematic method to alleviate this issue and obtain a
better estimation of the risk in the context of epidemic models informed by
high-resolution time-resolved contact data. We consider several such data sets
collected in various contexts and perform controlled resampling experiments. We
show how the statistical information contained in the resampled data can be
used to build a series of surrogate versions of the unknown contacts. We
simulate epidemic processes on the resulting reconstructed data sets and show
that it is possible to obtain good estimates of the outcome of simulations
performed using the complete data set. We discuss limitations and potential
improvements of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04067</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04067</id><created>2015-03-13</created><authors><author><keyname>TALL</keyname><forenames>Abdoulaye</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Altman</keyname><forenames>Zwi</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Altman</keyname><forenames>Eitan</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Virtual sectorization: design and self-optimization</title><categories>cs.NI</categories><comments>VTC2015-Spring, 5th International Workshop on Self-Organizing
  Networks (IWSON), May 2015, Glasgow, United Kingdom</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtual Sectorization (ViSn) aims at covering a confined area such as a
traffic hot-spot using a narrow beam. The beam is generated by a remote antenna
array located at-or close to the Base Station (BS). This paper develops the
ViSn model and provides the guidelines for designing the Virtual Sector (ViS)
antenna. In order to mitigate interference between the ViS and the traditional
macro sector covering the rest of the area, a Dynamic Spectrum Allocation (DSA)
algorithm that self-optimizes the frequency bandwidth split between the macro
cell and the ViS is also proposed. The Self-Organizing Network (SON) algorithm
is constructed to maximize the proportional fair utility of all the users
throughputs. Numerical simulations show the interest in deploying ViSn, and the
significant capacity gain brought about by the self-optimized bandwidth sharing
with respect to a full reuse of the bandwidth by the ViS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04069</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04069</id><created>2015-03-13</created><authors><author><keyname>Greff</keyname><forenames>Klaus</forenames></author><author><keyname>Srivastava</keyname><forenames>Rupesh Kumar</forenames></author><author><keyname>Koutn&#xed;k</keyname><forenames>Jan</forenames></author><author><keyname>Steunebrink</keyname><forenames>Bas R.</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>LSTM: A Search Space Odyssey</title><categories>cs.NE cs.LG</categories><comments>10 pages, 5 figures plus 8 pages, 6 figures supplementary</comments><msc-class>68T10</msc-class><acm-class>I.2.6; I.2.7; I.5.1; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several variants of the Long Short-Term Memory (LSTM) architecture for
recurrent neural networks have been proposed since its inception in 1995. In
recent years, these networks have become the state-of-the-art models for a
variety of machine learning problems. This has led to a renewed interest in
understanding the role and utility of various computational components of
typical LSTM variants. In this paper, we present the first large-scale analysis
of eight LSTM variants on three representative tasks: speech recognition,
handwriting recognition, and polyphonic music modeling. The hyperparameters of
all LSTM variants for each task were optimized separately using random search
and their importance was assessed using the powerful fANOVA framework. In
total, we summarize the results of 5400 experimental runs (about 15 years of
CPU time), which makes our study the largest of its kind on LSTM networks. Our
results show that none of the variants can improve upon the standard LSTM
architecture significantly, and demonstrate the forget gate and the output
activation function to be its most critical components. We further observe that
the studied hyperparameters are virtually independent and derive guidelines for
their efficient adjustment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04085</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04085</id><created>2015-03-11</created><updated>2015-12-16</updated><authors><author><keyname>Dianati</keyname><forenames>Navid</forenames></author></authors><title>Unwinding the hairball graph: pruning algorithms for weighted complex
  networks</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Phys. Rev. E 93, 012304 (2016)</journal-ref><doi>10.1103/PhysRevE.93.012304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical networks of weighted dyadic relations often contain noisy edges
that alter the global characteristics of the network and obfuscate the most
important structures therein. Graph pruning is the process of identifying the
most significant edges according to a generative null model, and extracting the
subgraph consisting of those edges. Here, we focus on integer-weighted graphs
commonly arising when weights count the occurrences of an &quot;event&quot; relating the
nodes. We introduce a simple and intuitive null model related to the
configuration model of network generation, and derive two significance filters
from it: the Marginal Likelihood Filter (MLF) and the Global Likelihood Filter
(GLF). The former is a fast algorithm assigning a significance score to each
edge based on the marginal distribution of edge weights whereas the latter is
an ensemble approach which takes into account the correlations among edges. We
apply these filters to the network of air traffic volume between US airports
and recover a geographically faithful representation of the graph. Furthermore,
compared with thresholding based on edge weight, we show that our filters
extract a larger and significantly sparser giant component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04099</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04099</id><created>2015-03-13</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Maria</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Spreer</keyname><forenames>Jonathan</forenames></author></authors><title>Algorithms and complexity for Turaev-Viro invariants</title><categories>math.GT cs.CC cs.DS cs.MS</categories><comments>17 pages, 5 figures</comments><msc-class>57M27, 57Q15, 68Q17</msc-class><acm-class>F.2.2; G.2.1; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Turaev-Viro invariants are a powerful family of topological invariants
for distinguishing between different 3-manifolds. They are invaluable for
mathematical software, but current algorithms to compute them require
exponential time.
  The invariants are parameterised by an integer $r \geq 3$. We resolve the
question of complexity for $r=3$ and $r=4$, giving simple proofs that computing
Turaev-Viro invariants for $r=3$ is polynomial time, but for $r=4$ is \#P-hard.
Moreover, we give an explicit fixed-parameter tractable algorithm for arbitrary
$r$, and show through concrete implementation and experimentation that this
algorithm is practical---and indeed preferable---to the prior state of the art
for real computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04108</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04108</id><created>2015-03-13</created><updated>2015-04-18</updated><authors><author><keyname>Sutter</keyname><forenames>Tobias</forenames></author><author><keyname>Sutter</keyname><forenames>David</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Asymptotic Capacity of a Random Channel</title><categories>cs.IT math.IT math.OC</categories><comments>20 pages, 2 figures</comments><msc-class>94A15, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider discrete memoryless channels with input alphabet size $n$ and
output alphabet size $m$, where $m=$ceil$(\gamma n)$ for some constant
$\gamma&gt;0$. The channel transition matrix consists of entries that, before
being normalised, are independent and identically distributed nonnegative
random variables $V$ and such that $E[(V \log V)^2]&lt;\infty$. We prove that in
the limit as $n\to \infty$ the capacity of such a channel converges to $Ent(V)
/ E[V]$ almost surely and in $L^2$, where $Ent(V):= E[V\log V]-E[V] E[\log V]$
denotes the entropy of $V$. We further show that the capacity of these random
channels converges to this asymptotic value exponentially in $n$. Finally, we
present an application in the context of Bayesian optimal experiment design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04115</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04115</id><created>2015-03-13</created><authors><author><keyname>Le</keyname><forenames>Nam Do-Hoang</forenames></author></authors><title>Sparse Code Formation with Linear Inhibition</title><categories>cs.CV</categories><comments>Technical report, 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse code formation in the primary visual cortex (V1) has been inspiration
for many state-of-the-art visual recognition systems. To stimulate this
behavior, networks are trained networks under mathematical constraint of
sparsity or selectivity. In this paper, the authors exploit another approach
which uses lateral interconnections in feature learning networks. However,
instead of adding direct lateral interconnections among neurons, we introduce
an inhibitory layer placed right after normal encoding layer. This idea
overcomes the challenge of computational cost and complexity on lateral
networks while preserving crucial objective of sparse code formation. To
demonstrate this idea, we use sparse autoencoder as normal encoding layer and
apply inhibitory layer. Early experiments in visual recognition show relative
improvements over traditional approach on CIFAR-10 dataset. Moreover, simple
installment and training process using Hebbian rule allow inhibitory layer to
be integrated into existing networks, which enables further analysis in the
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04118</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04118</id><created>2015-03-13</created><authors><author><keyname>Etienne</keyname><forenames>L.</forenames></author><author><keyname>Di Gennaro</keyname><forenames>S.</forenames></author><author><keyname>Barbot</keyname><forenames>J. -P.</forenames></author></authors><title>Event-Triggered Observers and Observer-Based Controllers for a Class of
  Nonlinear Systems</title><categories>cs.SY</categories><comments>Proceedings of the 2015 American Control Conference - ACC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the stabilization of a nonlinear plant subject
to network constraints, under the assumption of partial knowledge of the plant
state. The event triggered paradigm is used for the observation and the control
of the system. Necessary conditions, making use of the ISS property, are given
to guarantee the existence of a triggering mechanism, leading to asymptotic
convergence of the observer and system states. The proposed triggering
mechanism is illustrated in the stabilization of a robot with a flexible link
robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04127</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04127</id><created>2015-03-13</created><updated>2015-12-14</updated><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Li</keyname><forenames>Jimmy J.</forenames></author><author><keyname>Delouille</keyname><forenames>Veronique</forenames></author><author><keyname>De Visscher</keyname><forenames>Ruben</forenames></author><author><keyname>Watson</keyname><forenames>Fraser</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Image patch analysis of sunspots and active regions. I. Intrinsic
  dimension and correlation analysis</title><categories>astro-ph.SR cs.CV</categories><comments>Accepted for publication in the Journal of Space Weather and Space
  Climate (SWSC). 23 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The flare-productivity of an active region is observed to be related to its
spatial complexity. Mount Wilson or McIntosh sunspot classifications measure
such complexity but in a categorical way, and may therefore not use all the
information present in the observations. Moreover, such categorical schemes
hinder a systematic study of an active region's evolution for example. We
propose fine-scale quantitative descriptors for an active region's complexity
and relate them to the Mount Wilson classification. We analyze the local
correlation structure within continuum and magnetogram data, as well as the
cross-correlation between continuum and magnetogram data. We compute the
intrinsic dimension, partial correlation, and canonical correlation analysis
(CCA) of image patches of continuum and magnetogram active region images taken
from the SOHO-MDI instrument. We use masks of sunspots derived from continuum
as well as larger masks of magnetic active regions derived from the magnetogram
to analyze separately the core part of an active region from its surrounding
part. We find the relationship between complexity of an active region as
measured by Mount Wilson and the intrinsic dimension of its image patches.
Partial correlation patterns exhibit approximately a third-order Markov
structure. CCA reveals different patterns of correlation between continuum and
magnetogram within the sunspots and in the region surrounding the sunspots.
These results also pave the way for patch-based dictionary learning with a view
towards automatic clustering of active regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04135</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04135</id><created>2015-03-13</created><authors><author><keyname>Gilio</keyname><forenames>Angelo</forenames></author><author><keyname>Pfeifer</keyname><forenames>Niki</forenames></author><author><keyname>Sanfilippo</keyname><forenames>Giuseppe</forenames></author></authors><title>Transitive reasoning with imprecise probabilities</title><categories>math.PR cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study probabilistically informative (weak) versions of transitivity, by
using suitable definitions of defaults and negated defaults, in the setting of
coherence and imprecise probabilities. We represent p-consistent sequences of
defaults and/or negated defaults by g-coherent imprecise probability
assessments on the respective sequences of conditional events. Finally, we
prove the coherent probability propagation rules for Weak Transitivity and the
validity of selected inference patterns by proving the p-entailment for the
associated knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04144</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04144</id><created>2015-03-13</created><updated>2015-05-07</updated><authors><author><keyname>Zha</keyname><forenames>Shengxin</forenames></author><author><keyname>Luisier</keyname><forenames>Florian</forenames></author><author><keyname>Andrews</keyname><forenames>Walter</forenames></author><author><keyname>Srivastava</keyname><forenames>Nitish</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Exploiting Image-trained CNN Architectures for Unconstrained Video
  Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct an in-depth exploration of different strategies for doing event
detection in videos using convolutional neural networks (CNNs) trained for
image classification. We study different ways of performing spatial and
temporal pooling, feature normalization, choice of CNN layers as well as choice
of classifiers. Making judicious choices along these dimensions led to a very
significant increase in performance over more naive approaches that have been
used till now. We evaluate our approach on the challenging TRECVID MED'14
dataset with two popular CNN architectures pretrained on ImageNet. On this
MED'14 dataset, our methods, based entirely on image-trained CNN features, can
outperform several state-of-the-art non-CNN models. Our proposed late fusion of
CNN- and motion-based features can further increase the mean average precision
(mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves the
state-of-the-art classification performance on the challenging UCF-101 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04169</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04169</id><created>2015-03-13</created><updated>2015-03-17</updated><authors><author><keyname>Nguyen</keyname><forenames>Dung</forenames></author><author><keyname>Aref</keyname><forenames>Molham</forenames></author><author><keyname>Bravenboer</keyname><forenames>Martin</forenames></author><author><keyname>Kollias</keyname><forenames>George</forenames></author><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>Join Processing for Graph Patterns: An Old Dog with New Tricks</title><categories>cs.DB cs.DS</categories><acm-class>H.2; E.1; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Join optimization has been dominated by Selinger-style, pairwise optimizers
for decades. But, Selinger-style algorithms are asymptotically suboptimal for
applications in graphic analytics. This suboptimality is one of the reasons
that many have advocated supplementing relational engines with specialized
graph processing engines. Recently, new join algorithms have been discovered
that achieve optimal worst-case run times for any join or even so-called beyond
worst-case (or instance optimal) run time guarantees for specialized classes of
joins. These new algorithms match or improve on those used in specialized
graph-processing systems. This paper asks can these new join algorithms allow
relational engines to close the performance gap with graph engines?
  We examine this question for graph-pattern queries or join queries. We find
that classical relational databases like Postgres and MonetDB or newer graph
databases/stores like Virtuoso and Neo4j may be orders of magnitude slower than
these new approaches compared to a fully featured RDBMS, LogicBlox, using these
new ideas. Our results demonstrate that an RDBMS with such new algorithms can
perform as well as specialized engines like GraphLab -- while retaining a
high-level interface. We hope this adds to the ongoing debate of the role of
graph accelerators, new graph systems, and relational systems in modern
workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04177</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04177</id><created>2015-03-13</created><authors><author><keyname>Weyland</keyname><forenames>Dennis</forenames></author></authors><title>Some Comments on the Stochastic Eulerian Tour Problem</title><categories>cs.DS</categories><comments>research commentary, 4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Stochastic Eulerian Tour Problem was introduced in 2008 as a stochastic
variant of the well-known Eulerian Tour Problem. In a follow-up paper the same
authors investigated some heuristics for solving the Stochastic Eulerian Tour
Problem. After a thorough study of these two publications a few issues emerged.
In this short research commentary we would like to discuss these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04187</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04187</id><created>2015-03-13</created><authors><author><keyname>McGregor</keyname><forenames>Simon</forenames></author><author><keyname>Baltieri</keyname><forenames>Manuel</forenames></author><author><keyname>Buckley</keyname><forenames>Christopher L.</forenames></author></authors><title>A Minimal Active Inference Agent</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on the so-called &quot;free-energy principle'' (FEP) in cognitive
neuroscience is becoming increasingly high-profile. To date, introductions to
this theory have proved difficult for many readers to follow, but it depends
mainly upon two relatively simple ideas: firstly that normative or teleological
values can be expressed as probability distributions (active inference), and
secondly that approximate Bayesian reasoning can be effectively performed by
gradient descent on model parameters (the free-energy principle). The notion of
active inference is of great interest for a number of disciplines including
cognitive science and artificial intelligence, as well as cognitive
neuroscience, and deserves to be more widely known.
  This paper attempts to provide an accessible introduction to active inference
and informational free-energy, for readers from a range of scientific
backgrounds. In this work introduce an agent-based model with an agent trying
to make predictions about its position in a one-dimensional discretized world
using methods from the FEP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04193</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04193</id><created>2015-03-13</created><updated>2015-09-04</updated><authors><author><keyname>Porello</keyname><forenames>Daniele</forenames></author><author><keyname>Troquard</keyname><forenames>Nicolas</forenames></author></authors><title>Non-normal modalities in variants of Linear Logic</title><categories>cs.LO cs.AI</categories><doi>10.1080/11663081.2015.1080422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents modal versions of resource-conscious logics. We
concentrate on extensions of variants of Linear Logic with one minimal
non-normal modality. In earlier work, where we investigated agency in
multi-agent systems, we have shown that the results scale up to logics with
multiple non-minimal modalities. Here, we start with the language of
propositional intuitionistic Linear Logic without the additive disjunction, to
which we add a modality. We provide an interpretation of this language on a
class of Kripke resource models extended with a neighbourhood function: modal
Kripke resource models. We propose a Hilbert-style axiomatization and a
Gentzen-style sequent calculus. We show that the proof theories are sound and
complete with respect to the class of modal Kripke resource models. We show
that the sequent calculus admits cut elimination and that proof-search is in
PSPACE. We then show how to extend the results when non-commutative connectives
are added to the language. Finally, we put the logical framework to use by
instantiating it as logics of agency. In particular, we propose a logic to
reason about the resource-sensitive use of artefacts and illustrate it with a
variety of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04194</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04194</id><created>2015-03-13</created><authors><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Chyla</keyname><forenames>Roman</forenames></author><author><keyname>Luker</keyname><forenames>James</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn S.</forenames></author><author><keyname>Thompson</keyname><forenames>Donna M.</forenames></author><author><keyname>Holachek</keyname><forenames>Alexandra</forenames></author><author><keyname>Dave</keyname><forenames>Rahul</forenames></author><author><keyname>Murray</keyname><forenames>Stephen S.</forenames></author></authors><title>ADS: The Next Generation Search Platform</title><categories>astro-ph.IM cs.DL</categories><comments>Submitted to Library and Information Services in Astronomy VII,
  Naples, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four years after the last LISA meeting, the NASA Astrophysics Data System
(ADS) finds itself in the middle of major changes to the infrastructure and
contents of its database. In this paper we highlight a number of features of
great importance to librarians and discuss the additional functionality that we
are currently developing. Starting in 2011, the ADS started to systematically
collect, parse and index full-text documents for all the major publications in
Physics and Astronomy as well as many smaller Astronomy journals and arXiv
e-prints, for a total of over 3.5 million papers. Our citation coverage has
doubled since 2010 and now consists of over 70 million citations. We are
normalizing the affiliation information in our records and, in collaboration
with the CfA library and NASA, we have started collecting and linking funding
sources with papers in our system. At the same time, we are undergoing major
technology changes in the ADS platform which affect all aspects of the system
and its operations. We have rolled out and are now enhancing a new
high-performance search engine capable of performing full-text as well as
metadata searches using an intuitive query language which supports fielded,
unfielded and functional searches. We are currently able to index
acknowledgments, affiliations, citations, funding sources, and to the extent
that these metadata are available to us they are now searchable under our new
platform. The ADS private library system is being enhanced to support reading
groups, collaborative editing of lists of papers, tagging, and a variety of
privacy settings when managing one's paper collection. While this effort is
still ongoing, some of its benefits are already available through the ADS Labs
user interface and API at http://adslabs.org/adsabs/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04208</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04208</id><created>2015-03-13</created><authors><author><keyname>West</keyname><forenames>Robert</forenames></author><author><keyname>Paranjape</keyname><forenames>Ashwin</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Mining Missing Hyperlinks from Human Navigation Traces: A Case Study of
  Wikipedia</title><categories>cs.SI cs.HC</categories><doi>10.1145/2736277.2741666</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperlinks are an essential feature of the World Wide Web. They are
especially important for online encyclopedias such as Wikipedia: an article can
often only be understood in the context of related articles, and hyperlinks
make it easy to explore this context. But important links are often missing,
and several methods have been proposed to alleviate this problem by learning a
linking model based on the structure of the existing links. Here we propose a
novel approach to identifying missing links in Wikipedia. We build on the fact
that the ultimate purpose of Wikipedia links is to aid navigation. Rather than
merely suggesting new links that are in tune with the structure of existing
links, our method finds missing links that would immediately enhance
Wikipedia's navigability. We leverage data sets of navigation paths collected
through a Wikipedia-based human-computation game in which users must find a
short path from a start to a target article by only clicking links encountered
along the way. We harness human navigational traces to identify a set of
candidates for missing links and then rank these candidates. Experiments show
that our procedure identifies missing links of high quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04213</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04213</id><created>2015-03-13</created><updated>2015-09-24</updated><authors><author><keyname>Audenaert</keyname><forenames>Koenraad</forenames></author><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author><author><keyname>Ozols</keyname><forenames>Maris</forenames></author></authors><title>Entropy power inequalities for qudits</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>33 pages, 8 figures, 1 table, minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon's entropy power inequality (EPI) can be viewed as a statement of
concavity of an entropic function of a continuous random variable under a
scaled addition rule: $$f(\sqrt{a}\,X + \sqrt{1-a}\,Y) \geq a f(X) + (1-a) f(Y)
\quad \forall \, a \in [0,1].$$ Here, $X$ and $Y$ are continuous random
variables and the function $f$ is either the differential entropy or, for
$a=1/2$, the entropy power. Konig and Smith [IEEE Trans. Inf. Theory.
60(3):1536--1548, 2014] obtained quantum analogues of these inequalities for
continuous-variable quantum systems, where $X$ and $Y$ are replaced by bosonic
fields and the addition rule is the action of a beamsplitter with
transmissivity $a$ on those fields. In this paper, we similarly establish a
class of EPI analogues for $d$-level quantum systems (i.e. qudits). The
underlying addition rule for which these inequalities hold is given by a
quantum channel that depends on the parameter $a \in [0,1]$ and acts like a
finite-dimensional analogue of a beamsplitter with transmissivity $a$,
converting a two-qudit product state into a single qudit state. We refer to
this channel as a partial swap channel because of the particular way its output
interpolates between the states of the two qudits in the input as $a$ is
changed from zero to one. We obtain analogues of Shannon's EPI, not only for
the von Neumann entropy and the entropy power for the output of such channels,
but for a much larger class of functions as well. This class includes the Renyi
entropies and the subentropy. We also prove a qudit analogue of the entropy
photon number inequality (EPnI). Finally, for the subclass of partial swap
channels for which one of the qudit states in the input is fixed, our EPIs and
EPnI yield lower bounds on the minimum output entropy and upper bounds on the
Holevo capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04215</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04215</id><created>2015-03-13</created><authors><author><keyname>Hirzel</keyname><forenames>Martin</forenames></author><author><keyname>Rabbah</keyname><forenames>Rodric</forenames></author><author><keyname>Suter</keyname><forenames>Philippe</forenames></author><author><keyname>Tardieu</keyname><forenames>Olivier</forenames></author><author><keyname>Vaziri</keyname><forenames>Mandana</forenames></author></authors><title>Spreadsheets for Stream Partitions and Windows</title><categories>cs.SE</categories><comments>In Proceedings of the 2nd Workshop on Software Engineering Methods in
  Spreadsheets (http://spreadsheetlab.org/sems15/)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We discuss the suitability of spreadsheet processors as tools for programming
streaming systems. We argue that, while spreadsheets can function as powerful
models for stream operators, their fundamental boundedness limits their scope
of application. We propose two extensions to the spreadsheet model and argue
their utility in the context of programming streaming systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04220</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04220</id><created>2015-03-13</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>Chatterjee</keyname><forenames>Dipak</forenames></author></authors><title>Fuzzy Mixed Integer Optimization Model for Regression Approach</title><categories>cs.AI</categories><comments>Conference Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed Integer Optimization has been a topic of active research in past
decades. It has been used to solve Statistical problems of classification and
regression involving massive data. However, there is an inherent degree of
vagueness present in huge real life data. This impreciseness is handled by
Fuzzy Sets. In this Paper, Fuzzy Mixed Integer Optimization Method (FMIOM) is
used to find solution to Regression problem. The methodology exploits discrete
character of problem. In this way large scale problems are solved within
practical limits. The data points are separated into different polyhedral
regions and each region has its own distinct regression coefficients. In this
attempt, an attention is drawn to Statistics and Data Mining community that
Integer Optimization can be significantly used to revisit different Statistical
problems. Computational experimentations with generated and real data sets show
that FMIOM is comparable to and often outperforms current leading methods. The
results illustrate potential for significant impact of Fuzzy Integer
Optimization methods on Computational Statistics and Data Mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04222</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04222</id><created>2015-03-13</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author><author><keyname>Chatterjee</keyname><forenames>Dipak</forenames></author><author><keyname>Rajput</keyname><forenames>Ritesh</forenames></author></authors><title>Fuzzy Mixed Integer Linear Programming for Air Vehicles Operations
  Optimization</title><categories>cs.AI</categories><comments>Conference Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Air Vehicles (AVs) to prosecute geographically dispersed targets is
an important optimization problem. Associated multiple tasks viz., target
classification, attack and verification are successively performed on each
target. The optimal minimum time performance of these tasks requires
cooperation among vehicles such that critical time constraints are satisfied
i.e. target must be classified before it can be attacked and AV is sent to
target area to verify its destruction after target has been attacked. Here,
optimal task scheduling problem from Indian Air Force is formulated as Fuzzy
Mixed Integer Linear Programming (FMILP) problem. The solution assigns all
tasks to vehicles and performs scheduling in an optimal manner including
scheduled staged departure times. Coupled tasks involving time and task order
constraints are addressed. When AVs have sufficient endurance, existence of
optimal solution is guaranteed. The solution developed can serve as an
effective heuristic for different categories of AV optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04238</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04238</id><created>2015-03-13</created><authors><author><keyname>Hamlin</keyname><forenames>Nathan</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Bala</forenames></author><author><keyname>Webb</keyname><forenames>William</forenames></author></authors><title>A Knapsack-Like Code Using Recurrence Sequence Representations</title><categories>math.NT cs.CR cs.IT math.IT</categories><comments>9 pages</comments><msc-class>11B37, 11Y55, 94A60</msc-class><journal-ref>Fibonacci Quarterly. 1(53), 24-33 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We had recently shown that every positive integer can be represented uniquely
using a recurrence sequence, when certain restrictions on the digit strings are
satisfied. We present the details of how such representations can be used to
build a knapsack-like public key cryptosystem. We also present new disguising
methods, and provide arguments for the security of the code against known
methods of attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04244</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04244</id><created>2015-03-13</created><authors><author><keyname>Agarwal</keyname><forenames>Abhishek</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>Security in Locally Repairable Storage</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the notion of {\em locally repairable} codes to {\em
secret sharing} schemes. The main problem that we consider is to find optimal
ways to distribute shares of a secret among a set of storage-nodes
(participants) such that the content of each node (share) can be recovered by
using contents of only few other nodes, and at the same time the secret can be
reconstructed by only some allowable subsets of nodes. As a special case, an
eavesdropper observing some set of specific nodes (such as less than certain
number of nodes) does not get any information. In other words, we propose to
study a locally repairable distributed storage system that is secure against a
{\em passive eavesdropper} that can observe some subsets of nodes.
  We provide a number of results related to such systems including upper-bounds
and achievability results on the number of bits that can be securely stored
with these constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04250</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04250</id><created>2015-03-13</created><authors><author><keyname>Bernd</keyname><forenames>Julia</forenames></author><author><keyname>Borth</keyname><forenames>Damian</forenames></author><author><keyname>Elizalde</keyname><forenames>Benjamin</forenames></author><author><keyname>Friedland</keyname><forenames>Gerald</forenames></author><author><keyname>Gallagher</keyname><forenames>Heather</forenames></author><author><keyname>Gottlieb</keyname><forenames>Luke</forenames></author><author><keyname>Janin</keyname><forenames>Adam</forenames></author><author><keyname>Karabashlieva</keyname><forenames>Sara</forenames></author><author><keyname>Takahashi</keyname><forenames>Jocelyn</forenames></author><author><keyname>Won</keyname><forenames>Jennifer</forenames></author></authors><title>The YLI-MED Corpus: Characteristics, Procedures, and Plans</title><categories>cs.MM cs.CL</categories><comments>47 pages; 3 figures; 25 tables. Also published as ICSI Technical
  Report TR-15-001</comments><report-no>TR-15-001</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The YLI Multimedia Event Detection corpus is a public-domain index of videos
with annotations and computed features, specialized for research in multimedia
event detection (MED), i.e., automatically identifying what's happening in a
video by analyzing the audio and visual content. The videos indexed in the
YLI-MED corpus are a subset of the larger YLI feature corpus, which is being
developed by the International Computer Science Institute and Lawrence
Livermore National Laboratory based on the Yahoo Flickr Creative Commons 100
Million (YFCC100M) dataset. The videos in YLI-MED are categorized as depicting
one of ten target events, or no target event, and are annotated for additional
attributes like language spoken and whether the video has a musical score. The
annotations also include degree of annotator agreement and average annotator
confidence scores for the event categorization of each video. Version 1.0 of
YLI-MED includes 1823 &quot;positive&quot; videos that depict the target events and
48,138 &quot;negative&quot; videos, as well as 177 supplementary videos that are similar
to event videos but are not positive examples. Our goal in producing YLI-MED is
to be as open about our data and procedures as possible. This report describes
the procedures used to collect the corpus; gives detailed descriptive
statistics about the corpus makeup (and how video attributes affected
annotators' judgments); discusses possible biases in the corpus introduced by
our procedural choices and compares it with the most similar existing dataset,
TRECVID MED's HAVIC corpus; and gives an overview of our future plans for
expanding the annotation effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04251</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04251</id><created>2015-03-13</created><updated>2015-12-13</updated><authors><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author></authors><title>Performance Impact of LoS and NLoS Transmissions in Dense Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>32 pages, 8 figures, IEEE TWC [J]</comments><doi>10.1109/TWC.2015.2503391</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a sophisticated path loss model incorporating
both line-of-sight (LoS) and non-line-of-sight (NLoS) transmissions to study
their impact on the performance of dense small cell networks (SCNs). Analytical
results are obtained for the coverage probability and the area spectral
efficiency (ASE), assuming both a general path loss model and a special case
with a linear LoS probability function. The performance impact of LoS and NLoS
transmissions in dense SCNs in terms of the coverage probability and the ASE is
significant, both quantitatively and qualitatively, compared with the previous
work that does not differentiate LoS and NLoS transmissions. Our analysis
demonstrates that the network coverage probability first increases with the
increase of the base station (BS) density, and then decreases as the SCN
becomes denser. This decrease further makes the ASE suffer from a slow growth
or even a decrease with network densification. The ASE will grow almost
linearly as the BS density goes ultra dense. For practical regime of the BS
density, the performance results derived from our analysis are distinctively
different from previous results, and thus shed new insights on the design and
deployment of future dense SCNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04253</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04253</id><created>2015-03-13</created><updated>2015-06-17</updated><authors><author><keyname>Yong-Rim</keyname><forenames>Kang</forenames></author><author><keyname>Yong-Jin</keyname><forenames>Kim</forenames></author></authors><title>Novel Super-Resolution Method Based on High Order Nonlocal-Means</title><categories>cs.IT cs.CV math.IT</categories><comments>1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Super-resolution without explicit sub-pixel motion estimation is a very
active subject of image reconstruction containing general motion. The Non-Local
Means (NLM) method is a simple image reconstruction method without explicit
motion estimation. In this paper we generalize NLM method to higher orders
using kernel regression can apply to super-resolution reconstruction. The
performance of the generalized method is compared with other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04254</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04254</id><created>2015-03-13</created><updated>2015-03-22</updated><authors><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>GreenDelivery: Proactive Content Caching and Push with
  Energy-Harvesting-based Small Cells</title><categories>cs.IT math.IT</categories><comments>15 pages, 5 figures, accepted by IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth of mobile multimedia traffic calls for scalable wireless
access with high quality of service and low energy cost. Motivated by the
emerging energy harvesting communications, and the trend of caching multimedia
contents at the access edge and user terminals, we propose a paradigm-shift
framework, namely GreenDelivery, enabling efficient content delivery with
energy harvesting based small cells. To resolve the two-dimensional randomness
of energy harvesting and content request arrivals, proactive caching and push
are jointly optimized, with respect to the content popularity distribution and
battery states. We thus develop a novel way of understanding the interplay
between content and energy over time and space. Case studies are provided to
show the substantial reduction of macro BS activities, and thus the related
energy consumption from the power grid is reduced. Research issues of the
proposed GreenDelivery framework are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04255</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04255</id><created>2015-03-13</created><authors><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Chen</keyname><forenames>Tingjun</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Outage Minimization for a Fading Wireless Link with Energy Harvesting
  Transmitter and Receiver</title><categories>cs.IT math.IT</categories><comments>15 pages, 8 figures, accepted by IEEE Journal on Selected Areas in
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies online power control policies for outage minimization in a
fading wireless link with energy harvesting transmitter and receiver. The
outage occurs when either the transmitter or the receiver does not have enough
energy, or the channel is in outage, where the transmitter only has the channel
distribution information. Under infinite battery capacity and without
retransmission, we prove that threshold-based power control policies are
optimal. We thus propose disjoint/joint threshold-based policies with and
without battery state sharing between the transmitter and receiver,
respectively. We also analyze the impact of practical receiver detection and
processing on the outage performance. When retransmission is considered, policy
with linear power levels is adopted to adapt the power thresholds per
retransmission. With finite battery capacity, a three dimensional finite state
Markov chain is formulated to calculate the optimal parameters and
corresponding performance of proposed policies. The energy arrival correlation
between the transmitter and receiver is addressed for both finite and infinite
battery cases. Numerical results show the impact of battery capacity, energy
arrival correlation and detection cost on the outage performance of the
proposed policies, as well as the tradeoff between the outage probability and
the average transmission times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04256</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04256</id><created>2015-03-13</created><authors><author><keyname>Mahal</keyname><forenames>Jasmin A.</forenames></author><author><keyname>Khawar</keyname><forenames>Awais</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Radar Precoder Design for Spectral Coexistence with Coordinated
  Multi-point (CoMP) System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper details the design of precoders for a MIMO radar spectrally
coexistent with a MIMO cellular network. We focus on a coordinated multi-point
(CoMP) system where a cluster of base stations (BSs) coordinate their
transmissions to the intended user. The radar operates in two modes,
interference-mitigation mode when it avoids interference with the CoMP system
and cooperation mode when it exchanges information with it. Using either the
conventional Switched Null Space Projection (SNSP) or the newly proposed
Switched Small Singular Value Space Projection (SSSVSP), the radar beam sweeps
across the BS clusters focusing on the optimal ones, optimal in either nullity
or difference between the precoded and original radar signal. Taking the
channel estimation error into account, the design of precoder is pivoted on the
minimal radar interference at the BS clusters during interference-mitigation
mode and minimal bit-error-rate at the BSs during cooperation mode while
interfering minimally with the radar target detection capability. Our
investigation shows that loss in radar performance can be compensated using
SSSVSP instead of SNSP to some extent but increasing the number of radar
antenna elements goes a long way to serve the purpose. Simulations verify our
theoretical predictions about the proposed SSSVSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04260</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04260</id><created>2015-03-13</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author></authors><title>Quantum Structure of Negation and Conjunction in Human Thought</title><categories>cs.AI quant-ph</categories><comments>44 pages. arXiv admin note: text overlap with arXiv:1406.2358</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse in this paper the data collected in a set of experiments performed
on human subjects on the combination of natural concepts. We investigate the
mutual influence of conceptual conjunction and negation by measuring the
membership weights of a list of exemplars with respect to two concepts, e.g.,
'Fruits' and 'Vegetables', and their conjunction 'Fruits And Vegetables', but
also their conjunction when one or both concepts are negated, namely, 'Fruits
And Not Vegetables', 'Not Fruits And Vegetables' and 'Not Fruits And Not
Vegetables'. Our findings sharpen existing analysis on conceptual combinations,
revealing systematic and remarkable deviations from classical (fuzzy set) logic
and probability theory. And, more important, our results give further
considerable evidence to the validity of our quantum-theoretic framework for
the combination of two concepts. Indeed, the representation of conceptual
negation naturally arises from the general assumptions of our two-sector Fock
space model, and this representation faithfully agrees with the collected data.
In addition, we find a further significant deviation and a priori unexpected
from classicality, which can exactly be explained by assuming that human
reasoning is the superposition of an 'emergent reasoning' and a 'logical
reasoning', and that these two processes can be successfully represented in a
Fock space algebraic structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04263</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04263</id><created>2015-03-13</created><authors><author><keyname>Jeon</keyname><forenames>Seung Hyun</forenames></author><author><keyname>An</keyname><forenames>Sanghong</forenames></author><author><keyname>Yoon</keyname><forenames>Changwoo</forenames></author><author><keyname>Lee</keyname><forenames>Hyun-woo</forenames></author><author><keyname>Choi</keyname><forenames>Junkyun</forenames></author></authors><title>User Centric Content Management System for Open IPTV Over SNS (ICTC2012)</title><categories>cs.MM</categories><comments>10 pages, 17 figures, An earlier version of this paper was awarded as
  best paper at the IEEE International Conference on ICT Convergence (ICTC),
  Jeju, Korea, October 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coupled schemes between service-oriented architecture (SOA) and Web 2.0 have
recently been researched. Web-based content providers and telecommunications
company (Telecom) based Internet protocol television (IPTV) providers have
struggled against each other to accommodate more three-screen service
subscribers. Since the advent of Web 2.0, more abundant reproduced content can
be circulated. However, because according to increasing device's resolution and
content formats IPTV providers transcode content in advance, network bandwidth,
storage and operation costs for content management systems (CMSs) are wasted.
In this paper, we present a user centric CMS for open IPTV, which integrates
SOA and Web 2.0. Considering content popularity based on a Zipf-like
distribution to solve these problems, we analyze the performance between the
user centric CMS and the conventional Web syndication system for normalized
costs. Based on the user centric CMS, we implement a social Web TV with
device-aware function, which can aggregate, transcode, and deploy content over
social networking service (SNS) independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04265</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04265</id><created>2015-03-14</created><authors><author><keyname>Hui</keyname><forenames>Zhuo</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author></authors><title>A Dictionary-based Approach for Estimating Shape and Spatially-Varying
  Reflectance</title><categories>cs.CV</categories><comments>IEEE Intl. Conf. Computational Photography, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for estimating the shape and reflectance of an object
in terms of its surface normals and spatially-varying BRDF. We assume that
multiple images of the object are obtained under fixed view-point and varying
illumination, i.e, the setting of photometric stereo. Assuming that the BRDF at
each pixel lies in the non-negative span of a known BRDF dictionary, we derive
a per-pixel surface normal and BRDF estimation framework that requires neither
iterative optimization techniques nor careful initialization, both of which are
endemic to most state-of-the-art techniques. We showcase the performance of our
technique on a wide range of simulated and real scenes where we outperform
competing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04267</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04267</id><created>2015-03-14</created><authors><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Gupta</keyname><forenames>Mohit</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author></authors><title>LiSens --- A Scalable Architecture for Video Compressive Sensing</title><categories>cs.CV</categories><comments>IEEE Intl. Conf. Computational Photography, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement rate of cameras that take spatially multiplexed measurements
by using spatial light modulators (SLM) is often limited by the switching speed
of the SLMs. This is especially true for single-pixel cameras where the
photodetector operates at a rate that is many orders-of-magnitude greater than
the SLM. We study the factors that determine the measurement rate for such
spatial multiplexing cameras (SMC) and show that increasing the number of
pixels in the device improves the measurement rate, but there is an optimum
number of pixels (typically, few thousands) beyond which the measurement rate
does not increase. This motivates the design of LiSens, a novel imaging
architecture, that replaces the photodetector in the single-pixel camera with a
1D linear array or a line-sensor. We illustrate the optical architecture
underlying LiSens, build a prototype, and demonstrate results of a range of
indoor and outdoor scenes. LiSens delivers on the promise of SMCs: imaging at a
megapixel resolution, at video rate, using an inexpensive low-resolution
sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04269</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04269</id><created>2015-03-14</created><updated>2015-04-20</updated><authors><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author><author><keyname>Mahmood</keyname><forenames>A. Rupam</forenames></author><author><keyname>White</keyname><forenames>Martha</forenames></author></authors><title>An Emphatic Approach to the Problem of Off-policy Temporal-Difference
  Learning</title><categories>cs.LG</categories><comments>29 pages This is a significant revision based on the first set of
  reviews. The most important change was to signal early that the main result
  is about stability, not convergence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the idea of improving the performance of
parametric temporal-difference (TD) learning algorithms by selectively
emphasizing or de-emphasizing their updates on different time steps. In
particular, we show that varying the emphasis of linear TD($\lambda$)'s updates
in a particular way causes its expected update to become stable under
off-policy training. The only prior model-free TD methods to achieve this with
per-step computation linear in the number of function approximation parameters
are the gradient-TD family of methods including TDC, GTD($\lambda$), and
GQ($\lambda$). Compared to these methods, our _emphatic TD($\lambda$)_ is
simpler and easier to use; it has only one learned parameter vector and one
step-size parameter. Our treatment includes general state-dependent discounting
and bootstrapping functions, and a way of specifying varying degrees of
interest in accurately valuing different states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04277</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04277</id><created>2015-03-14</created><authors><author><keyname>Changling</keyname><forenames>Zhou</forenames></author><author><keyname>Jianguo</keyname><forenames>Xiao</forenames></author><author><keyname>Jian</keyname><forenames>Cui</forenames></author><author><keyname>Bei</keyname><forenames>Zhang</forenames></author><author><keyname>Feng</keyname><forenames>Li</forenames></author></authors><title>Approximate Discovery of Service Nodes by Duplicate Detection in Flows</title><categories>cs.NI</categories><comments>15 pages</comments><journal-ref>China Communications, 2012, 9(5): 75-89</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge about which nodes provide services is of critical importance for
network administrators. Discovery of service nodes can be done by making full
use of duplicate element detection in flows. Because the amount of traffic
across network is massive, especially in large ISPs or campus networks, we
propose an approximate algorithm with Round-robin Buddy Bloom Filters(RBBF) for
service detection using NetFlow data solely. The properties and analysis of
RBBF data structure are also given. Our method has better time/space efficiency
than conventional algorithm with a small false positive rate.%portion of false
positive. We also demonstrate the contributions through a prototype system by
real world case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04286</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04286</id><created>2015-03-14</created><authors><author><keyname>Turcu</keyname><forenames>Cristina</forenames></author><author><keyname>Turcu</keyname><forenames>Cornel</forenames></author><author><keyname>Popa</keyname><forenames>Valentin</forenames></author><author><keyname>Gaitan</keyname><forenames>Vasile</forenames></author></authors><title>ICT and RFID in Education: Some Practical Aspects in Campus Life</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper summarizes our preliminary findings regarding the development and
implementation of a newly proposed system based on ICT and RFID (Radio
Frequency Identification) technologies for campus access and facility usage. It
is generally acknowledged that any educational environment is highly dependent
upon a wide range of resources or variables such as teaching staff, research
and study areas, meeting and accommodation facilities, library services,
restaurant and leisure facilities, etc. The system we have devised using ICT
and RFID technologies supports not only authentic transactions among all
university departments, but also interconnects all levels of academic life and
activity. Thus, the utility of the system ranges from access control (student/
staff/ visitor identification), attendance tracking, library check-out services
and voting to grade book consulting, inventory, cashless vending, parking,
laundry and copying services. Physically, the system consists of several RFID
gates/readers, a data server and some network stations, all of them requiring
specific structuring and integration solutions. The system is quite different
from already existing ones in that it proposes an innovative access solution.
Thus, the search of the ID card holder in a database has been replaced by local
processing. Since one and the same card is employed to perform a variety of
operations, the system has immediate and numerous utilizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04287</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04287</id><created>2015-03-14</created><updated>2015-04-16</updated><authors><author><keyname>Agarwal</keyname><forenames>Pratik</forenames></author><author><keyname>Burgard</keyname><forenames>Wolfram</forenames></author><author><keyname>Spinello</keyname><forenames>Luciano</forenames></author></authors><title>Metric Localization using Google Street View</title><categories>cs.RO cs.CV</categories><comments>8 pages, 11 figures. 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate metrical localization is one of the central challenges in mobile
robotics. Many existing methods aim at localizing after building a map with the
robot. In this paper, we present a novel approach that instead uses geotagged
panoramas from the Google Street View as a source of global positioning. We
model the problem of localization as a non-linear least squares estimation in
two phases. The first estimates the 3D position of tracked feature points from
short monocular camera sequences. The second computes the rigid body
transformation between the Street View panoramas and the estimated points. The
only input of this approach is a stream of monocular camera images and odometry
estimates. We quantified the accuracy of the method by running the approach on
a robotic platform in a parking lot by using visual fiducials as ground truth.
Additionally, we applied the approach in the context of personal localization
in a real urban scenario by using data from a Google Tango tablet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04288</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04288</id><created>2015-03-14</created><authors><author><keyname>Turcu</keyname><forenames>Cristina</forenames></author><author><keyname>Turcu</keyname><forenames>Cornel</forenames></author><author><keyname>Graur</keyname><forenames>Evelina</forenames></author></authors><title>A Proposal for a Nationwide Student Gradebook Information Network</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Todays students are encouraged to study and develop expertise in more than
one national academic environment. As a matter of fact, their educational
activities inevitably occur in a variety of academic settings and even span
several years. Consequently students academic results and progress are expected
to be easily monitored and accessed nationally. The authors of the present
paper have devised a student gradebook information network to be nationally
employed by all public and private universities and colleges. The papers deals
with the architectural principles underlying the system and discusses aspects
related to data collection, data analysis and data storage across multiple
machines while providing a seamless view of entire infrastructure and service
delivery system from a single Web access point. The utility of the
architectural system is discussed in relationship with its major advantages:
user-friendliness, security access, flexibility, transparency, distributional
power, and scalability. The major beneficiary of the system is the Romanian
higher education system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04304</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04304</id><created>2015-03-14</created><authors><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author></authors><title>Laplace's rule of succession in information geometry</title><categories>cs.IT math.IT math.ST stat.TH</categories><msc-class>62b10, 94a29, 62f15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laplace's &quot;add-one&quot; rule of succession modifies the observed frequencies in a
sequence of heads and tails by adding one to the observed counts. This improves
prediction by avoiding zero probabilities and corresponds to a uniform Bayesian
prior on the parameter. The canonical Jeffreys prior corresponds to the
&quot;add-one-half&quot; rule. We prove that, for exponential families of distributions,
such Bayesian predictors can be approximated by taking the average of the
maximum likelihood predictor and the \emph{sequential normalized maximum
likelihood} predictor from information theory. Thus in this case it is possible
to approximate Bayesian predictors without the cost of integrating or sampling
in parameter space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04315</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04315</id><created>2015-03-14</created><authors><author><keyname>Papachristou</keyname><forenames>Marios</forenames></author></authors><title>Designing and Building a Three-dimensional Projective Scanner for
  Smartphones</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the frustrating things in the digital fabrication era is that its
media are neither affordable nor easily accessible and usable.
Three-dimensional (3D) fabrication media (DFM) such as 3D Printers and 3D
Scanners have experienced an upsurge in popularity, while the latter remain
expensive and hard to function. With this paper, we aim to present you the
RhoScanner Project - a an affordable and efficient Three-dimensional Projective
Scanner for Smart-phones, hence shedding light on the extended capabilities of
digital fabrication media on popular use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04317</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04317</id><created>2015-03-14</created><authors><author><keyname>Wette</keyname><forenames>Philip</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>HybridTE: Traffic Engineering for Very Low-Cost Software-Defined
  Data-Center Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The size of modern data centers is constantly increasing. As it is not
economic to interconnect all machines in the data center using a
full-bisection-bandwidth network, techniques have to be developed to increase
the efficiency of data-center networks. The Software-Defined Network paradigm
opened the door for centralized traffic engineering (TE) in such environments.
Up to now, there were already a number of TE proposals for SDN-controlled data
centers that all work very well. However, these techniques either use a high
amount of flow table entries or a high flow installation rate that overwhelms
available switching hardware, or they require custom or very expensive
end-of-line equipment to be usable in practice.
  We present HybridTE, a TE technique that uses (uncertain) information about
large flows. Using this extra information, our technique has very low hardware
requirements while maintaining better performance than existing TE techniques.
This enables us to build very low-cost, high performance data-center networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04320</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04320</id><created>2015-03-14</created><updated>2015-06-10</updated><authors><author><keyname>Salvati</keyname><forenames>Sylvain</forenames><affiliation>INRIA, LaBRI, universit&#xe9; de Bordeaux</affiliation></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames><affiliation>CNRS, LaBRI, universit&#xe9; de Bordeaux</affiliation></author></authors><title>Using models to model-check recursive schemes</title><categories>cs.LO</categories><comments>Long version of a paper presented at TLCA 2013</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:7) 2015</journal-ref><doi>10.2168/LMCS-11(2:7)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a model-based approach to the model checking problem for recursive
schemes. Since simply typed lambda calculus with the fixpoint operator,
lambda-Y-calculus, is equivalent to schemes, we propose the use of a model of
lambda-Y-calculus to discriminate the terms that satisfy a given property. If a
model is finite in every type, this gives a decision procedure. We provide a
construction of such a model for every property expressed by automata with
trivial acceptance conditions and divergence testing. Such properties pose
already interesting challenges for model construction. Moreover, we argue that
having models capturing some class of properties has several other virtues in
addition to providing decidability of the model-checking problem. As an
illustration, we show a very simple construction transforming a scheme to a
scheme reflecting a property captured by a given model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04333</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04333</id><created>2015-03-14</created><updated>2015-06-28</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>Dynamic Move Chains and Move Tables in Computer Chess</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of dynamic move chains has been described in a preceding paper [8].
It allows the search tree to be forward-pruned, which is known to be dangerous,
because it can remove important moves that would only be evaluated through a
more exhaustive search process. This paper has added to the forward-pruning
technique, through the use of 'Move Tables' that can act in the same way as
Transposition Tables, but for moves not positions. They use an efficient memory
structure and have put the design into the context of short or long-term
memories. The forward-pruning technique can also be fortified, to help to
remove some of the error. For example, a minimum beam width can guarantee x
dynamic moves are searched, making the result more reliable. Or a minimum
window for the evaluation difference can determine if a full search is
subsequently required. Therefore, with some configuration, dynamic move chains
can be reliably used and relatively independently of the position. This has
advanced some of the future work theory of the earlier paper as well and made
more explicit where logical plans or more knowledge-based approaches might be
applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04334</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04334</id><created>2015-03-14</created><authors><author><keyname>Barros</keyname><forenames>C. M. F.</forenames></author><author><keyname>de Assis</keyname><forenames>Francisco Marcos</forenames></author><author><keyname>de Oliveira</keyname><forenames>H. M.</forenames></author></authors><title>Quantum Decoding with Venn Diagrams</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quantum error correction theory is as a rule formulated in a rather
convoluted way, in comparison to classical algebraic theory. This work revisits
the error correction in a noisy quantum channel so as to make it intelligible
to engineers. An illustrative example is presented of a naive perfect quantum
code (Hamming-like code) with five-qubits for transmitting a single qubit of
information. Also the (9,1)-Shor codes is addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04337</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04337</id><created>2015-03-14</created><updated>2015-08-11</updated><authors><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Sun</keyname><forenames>Yuekai</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Taylor</keyname><forenames>Jonathan E.</forenames></author></authors><title>Communication-efficient sparse regression: a one-shot approach</title><categories>stat.ML cs.LG</categories><comments>29 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a one-shot approach to distributed sparse regression in the
high-dimensional setting. The key idea is to average &quot;debiased&quot; or
&quot;desparsified&quot; lasso estimators. We show the approach converges at the same
rate as the lasso as long as the dataset is not split across too many machines.
We also extend the approach to generalized linear models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04338</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04338</id><created>2015-03-14</created><updated>2015-04-11</updated><authors><author><keyname>Petschow</keyname><forenames>Matthias</forenames></author></authors><title>Towards radio astronomical imaging using an arbitrary basis</title><categories>astro-ph.IM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new generation of radio telescopes, such as the Square Kilometer Array
(SKA), requires dramatic advances in computer hardware and software, in order
to process the large amounts of produced data efficiently. In this document, we
explore a new approach to wide-field imaging. By generalizing the image
reconstruction, which is performed by an inverse Fourier transform, to
arbitrary transformations, we gain enormous new possibilities. In particular,
we outline an approach that might allow to obtain a sky image of size P times Q
in (optimal) O(PQ) time. This could be a step in the direction of real-time,
wide-field sky imaging for future telescopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04344</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04344</id><created>2015-03-14</created><authors><author><keyname>Abbas</keyname><forenames>Safia</forenames></author></authors><title>Deposit subscribe Prediction using Data Mining Techniques based Real
  Marketing Dataset</title><categories>cs.DB</categories><doi>10.5120/19293-0725</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, economic depression, which scoured all over the world, affects
business organizations and banking sectors. Such economic pose causes a severe
attrition for banks and customer retention becomes impossible. Accordingly,
marketing managers are in need to increase marketing campaigns, whereas
organizations evade both expenses and business expansion. In order to solve
such riddle, data mining techniques is used as an uttermost factor in data
analysis, data summarizations, hidden pattern discovery, and data
interpretation. In this paper, rough set theory and decision tree mining
techniques have been implemented, using a real marketing data obtained from
Portuguese marketing campaign related to bank deposit subscription [Moro et
al., 2011]. The paper aims to improve the efficiency of the marketing campaigns
and helping the decision makers by reducing the number of features, that
describes the dataset and spotting on the most significant ones, and predict
the deposit customer retention criteria based on potential predictive rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04347</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04347</id><created>2015-03-14</created><updated>2015-07-01</updated><authors><author><keyname>Di Luna</keyname><forenames>G. A.</forenames></author><author><keyname>Flocchini</keyname><forenames>P.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>S. Gan</forenames></author><author><keyname>Poloni</keyname><forenames>F.</forenames></author><author><keyname>Santoro</keyname><forenames>N.</forenames></author><author><keyname>Viglietta</keyname><forenames>G.</forenames></author></authors><title>Mutual Visibility by Luminous Robots Without Collisions</title><categories>cs.DC cs.CG cs.RO</categories><comments>60 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a finite set of identical computational entities that can move
freely in the Euclidean plane operating in Look-Compute-Move cycles. Let p(t)
denote the location of entity p at time t; entity p can see entity q at time t
if at that time no other entity lies in the line segment p(t)q(t). We consider
the basic problem called Mutual Visibility: starting from arbitrary distinct
locations, within finite time the entities must reach, without collisions, a
configuration where they all see each other. This problem must be solved by
each entity autonomously executing the same algorithm. We study this problem in
the &quot;luminous robots&quot; model; in this generalization of the standard model of
oblivious robots, each entity, called &quot;robot&quot;, has an externally visible
persistent light which can assume colors from a fixed set. The case where the
number of colors is c=1 corresponds to the classical model without lights.
  In this paper we investigate under what conditions luminous robots can solve
Mutual Visibility without collisions and at what cost (i.e., with how many
colors). We establish a spectrum of results, depending on the power of the
adversary, on the number c of colors, and on the a-priori knowledge the robots
have about the system. Among such results, we prove that Mutual Visibility can
always be solved without collisions in SSynch with c=2 colors and in ASynch
with c=3 colors. If an adversary can interrupt and stop a robot moving to its
computed destination, Mutual Visibility is still always solvable without
collisions in SSynch with c=3 colors, and, if the robots agree on the direction
of one axis, also in ASynch. All the results are obtained constructively by
means of novel protocols.
  As a byproduct of our solutions, we provide the first obstructed-visibility
solutions to two classical problems for oblivious robots: Collision-less
Convergence to a point and Circle Formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04358</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04358</id><created>2015-03-14</created><authors><author><keyname>Koopman</keyname><forenames>Rob</forenames></author><author><keyname>Wang</keyname><forenames>Shenghui</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Englebienne</keyname><forenames>Gwenn</forenames></author></authors><title>Ariadne's Thread - Interactive Navigation in a World of Networked
  Information</title><categories>cs.DL</categories><comments>CHI'15 Extended Abstracts, April 18-23, 2015, Seoul, Republic of
  Korea. ACM 978-1-4503-3146-3/15/04</comments><acm-class>H.5.2; H.3.3</acm-class><doi>10.1145/2702613.2732781</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work-in-progress paper introduces an interface for the interactive
visual exploration of the context of queries using the ArticleFirst database, a
product of OCLC. We describe a workflow which allows the user to browse live
entities associated with 65 million articles. In the on-line interface, each
query leads to a specific network representation of the most prevailing
entities: topics (words), authors, journals and Dewey decimal classes linked to
the set of terms in the query. This network represents the context of a query.
Each of the network nodes is clickable: by clicking through, a user traverses a
large space of articles along dimensions of authors, journals, Dewey classes
and words simultaneously. We present different use cases of such an interface.
This paper provides a link between the quest for maps of science and on-going
debates in HCI about the use of interactive information visualisation to
empower users in their search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04359</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04359</id><created>2015-03-14</created><updated>2015-10-02</updated><authors><author><keyname>Sallinen</keyname><forenames>Scott</forenames></author><author><keyname>Gharaibeh</keyname><forenames>Abdullah</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>Accelerating Direction-Optimized Breadth First Search on Hybrid
  Architectures</title><categories>cs.DC</categories><comments>As appeared in HeteroPar 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale-free graphs are famously difficult to process efficiently: the
skewed vertex degree distribution makes it difficult to obtain balanced
partitioning. Our research instead aims to turn this into an advantage by
partitioning the workload to match the strength of the individual computing
elements in a Hybrid, GPU-accelerated architecture. As a proof of concept we
focus on the direction-optimized breadth first search algorithm. We present the
key graph partitioning, workload allocation, and communication strategies
required for massive concurrency and good overall performance. We show that
exploiting specialization enables gains as high as 2.4x in terms of
time-to-solution and 2.0x in terms of energy efficiency by adding 2 GPUs to a 2
CPU-only baseline, for synthetic graphs with up to 16 Billion undirected edges
as well as for large real-world graphs. We also show that, for a capped energy
envelope, it is more efficient to add a GPU than an additional CPU. Finally,
our performance would place us at the top of today's [Green]Graph500 challenges
for Scale29 graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04360</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04360</id><created>2015-03-14</created><updated>2015-10-07</updated><authors><author><keyname>Sar&#x131;ta&#x15f;</keyname><forenames>Serkan</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author><author><keyname>Gezici</keyname><forenames>Sinan</forenames></author></authors><title>Quadratic Multi-Dimensional Signaling Games and Affine Equilibria</title><categories>math.OC cs.IT math.IT</categories><comments>32 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the decentralized quadratic cheap talk and signaling game
problems when an encoder and a decoder, viewed as two decision makers, have
misaligned objective functions. The main contributions of this study are the
extension of Crawford and Sobel's cheap talk formulation to multi-dimensional
sources and to noisy channel setups. We consider both Nash equilibria and
(sequential) Stackelberg equilibria. We show that for arbitrary scalar sources,
in the presence of misalignment, the quantized nature of all equilibrium
policies holds for Nash equilibria in the sense that all Nash equilibria are
equivalent to those achieved by quantized encoder policies. On the other hand,
all Stackelberg equilibria policies are fully informative. For
multi-dimensional setups, unlike the scalar case, Nash equilibrium policies may
be of non-quantized nature, and even linear. In the noisy setup, a Gaussian
source is to be transmitted over an additive Gaussian channel. The goals of the
encoder and the decoder are misaligned by a bias term and encoder's cost also
includes a penalty term on signal power. Conditions for the existence of affine
Nash equilibria as well as general informative equilibria are presented. For
the noisy setup, the only Stackelberg equilibrium is the linear equilibrium
when the variables are scalar. Our findings provide further conditions on when
affine policies may be optimal in decentralized multi-criteria control problems
and lead to conditions for the presence of active information transmission in
strategic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04371</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04371</id><created>2015-03-14</created><updated>2016-02-02</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Uniform Random Number Generation from Markov Chains: Non-Asymptotic and
  Asymptotic Analyses</title><categories>cs.IT cs.CR math.IT</categories><comments>There is no technical overlap with the latest version of
  arXiv:1309.7528</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive non-asymptotic achievability and converse bounds on
the random number generation with/without side-information. Our bounds are
efficiently computable in the sense that the computational complexity does not
depend on the block length. We also characterize the asymptotic behaviors of
the large deviation regime and the moderate deviation regime by using our
bounds, which implies that our bounds are asymptotically tight in those
regimes. We also show the second order rates of those problems, and derive
single letter forms of the variances characterizing the second order rates.
Further, we address the equivocation rates for these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04374</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04374</id><created>2015-03-14</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author></authors><title>Science Bots: a Model for the Future of Scientific Computation?</title><categories>cs.CY cs.DL cs.MA</categories><comments>WWW 2015 Companion, May 18-22, 2015, Florence, Italy</comments><acm-class>K.4.2</acm-class><doi>10.1145/2740908.2742014</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As a response to the trends of the increasing importance of computational
approaches and the accelerating pace in science, I propose in this position
paper to establish the concept of &quot;science bots&quot; that autonomously perform
programmed tasks on input data they encounter and immediately publish the
results. We can let such bots participate in a reputation system together with
human users, meaning that bots and humans get positive or negative feedback by
other participants. Positive reputation given to these bots would also shine on
their owners, motivating them to contribute to this system, while negative
reputation will allow us to filter out low-quality data, which is inevitable in
an open and decentralized system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04375</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04375</id><created>2015-03-14</created><updated>2015-03-16</updated><authors><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author><author><keyname>Fang</keyname><forenames>Kan</forenames></author><author><keyname>Francis</keyname><forenames>Gregory</forenames></author></authors><title>Optimization of Switch Keyboards</title><categories>cs.HC</categories><comments>In Proceedings of the 15th International ACM SIGACCESS Conference on
  Computers and Accessibility 2013</comments><doi>10.1145/2513383.2513394</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patients with motor control difficulties often &quot;type&quot; on a computer using a
switch keyboard to guide a scanning cursor to text elements. We show how to
optimize some parts of the design of switch keyboards by casting the design
problem as mixed integer programming. A new algorithm to find an optimized
design solution is approximately 3600 times faster than a previous algorithm,
which was also susceptible to finding a non-optimal solution. The optimization
requires a model of the probability of an entry error, and we show how to build
such a model from experimental data. Example optimized keyboards are
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04377</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04377</id><created>2015-03-14</created><authors><author><keyname>Rehof</keyname><forenames>Jakob</forenames><affiliation>TU-Dortmund</affiliation></author></authors><title>Proceedings Seventh Workshop on Intersection Types and Related Systems</title><categories>cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 177, 2015</journal-ref><doi>10.4204/EPTCS.177</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains a final and revised selection of papers presented at the
Seventh Workshop on Intersection Types and Related Systems (ITRS 2014), held in
Vienna (Austria) on July 18th, affiliated with TLCA 2014, Typed Lambda Calculi
and Applications (held jointly with RTA, Rewriting Techniques and Applications)
as part of FLoC and the Vienna Summer of Logic (VSL) 2014. Intersection types
have been introduced in the late 1970s as a language for describing properties
of lambda calculus which were not captured by all previous type systems. They
provided the first characterisation of strongly normalising lambda terms and
have become a powerful syntactic and semantic tool for analysing various
normalisation properties as well as lambda models. Over the years the scope of
research on intersection types has broadened. Recently, there have been a
number of breakthroughs in the use of intersection types and similar technology
for practical purposes such as program analysis, verification and concurrency,
and program synthesis. The aim of the ITRS workshop series is to bring together
researchers working on both the theory and practical applications of systems
based on intersection types and related approaches (e.g., union types,
refinement types, behavioral types).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04378</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04378</id><created>2015-03-14</created><authors><author><keyname>Buhnova</keyname><forenames>Bara</forenames></author><author><keyname>Happe</keyname><forenames>Lucia</forenames></author><author><keyname>Kofro&#x148;</keyname><forenames>Jan</forenames></author></authors><title>Proceedings 12th International Workshop on Formal Engineering approaches
  to Software Components and Architectures</title><categories>cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 178, 2015</journal-ref><doi>10.4204/EPTCS.178</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the FESCA workshop is to bring together junior researchers from
formal methods, software engineering, and industry interested in the
development and application of formal modelling approaches as well as
associated analysis and reasoning techniques with practical benefits for
software engineering.
  In recent years, the growing importance of functional correctness and the
increased relevance of system quality properties (e.g. performance,
reliability, security) have stimulated the emergence of analytical and
modelling techniques for the design and development of software systems. With
the increasing complexity of today's software systems, FESCA aims at addressing
two research questions: (1) what role the software architecture can play in
systematic addressing of the analytical and modelling challenges, and (2) how
formal and semi-formal techniques can be applied effectively to make the issues
easier to address automatically, with lower human intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04380</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04380</id><created>2015-03-14</created><authors><author><keyname>Zhu</keyname><forenames>Wei</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author></authors><title>A Triangular Decomposition Algorithm for Differential Polynomial Systems
  with Elementary Computation Complexity</title><categories>cs.SC</categories><msc-class>12H05, 14Q99</msc-class><acm-class>I.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new triangular decomposition algorithm is proposed for
ordinary differential polynomial systems, which has triple exponential
computational complexity. The key idea is to eliminate one algebraic variable
from a set of polynomials in one step using the theory of multivariate
resultant. This seems to be the first differential triangular decomposition
algorithm with elementary computation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04381</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04381</id><created>2015-03-14</created><updated>2015-11-01</updated><authors><author><keyname>Liu</keyname><forenames>Hongwu</forenames></author><author><keyname>Kim</keyname><forenames>Kyeong Jin</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>Relay Control for Full-Duplex Relaying with Wireless Information and
  Energy Transfer</title><categories>cs.IT math.IT</categories><comments>16 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates wireless information and energy transfer for dual-hop
amplify-and-forward full-duplex relaying systems. By forming energy efficiency
(EE) maximization problem into a concave fractional program of transmission
power, three relay control schemes are separately designed to enable energy
harvesting and full-duplex information relaying. With Rician fading modeled
residual self-interference channel, analytical expressions of outage
probability and ergodic capacity are presented for the maximum relay,
signal-to-interference-plus-noise-ratio (SINR) relay, and target relay. It has
shown that EE maximization problem of the maximum relay is concave for time
switching factor, so that bisection method has been applied to obtain the
optimized value. By incorporating instantaneous channel information, the SINR
relay with collateral time switching factor achieves an improved EE over the
maximum relay in delay-limited and delay-tolerant transmissions. Without
requiring channel information for the second-hop, the target relay ensures a
competitive performance for outage probability, ergodic capacity, and EE.
Comparing to the direct source-destination transmission, numerical results show
that the proposed relaying scheme is beneficial in achieving a comparable EE
for low-rate delay-limited transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04385</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04385</id><created>2015-03-15</created><authors><author><keyname>Ray</keyname><forenames>Kisor</forenames></author></authors><title>Design and Implementation of Database Independent Auto Sequence Numbers</title><categories>cs.DB</categories><comments>03 pages, 02 figures</comments><acm-class>K.8.1</acm-class><journal-ref>International Journal of Computer Trends and Technology,Volume-20
  Number-2,2015</journal-ref><doi>10.14445/22312803/IJCTT-V20P111</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Developers across the world use autonumber or auto sequences field of the
backend databases for developing both the desktop and web based data centric
applications which is easier to use at the development and deployment purpose
but can create a lot of problems under varied situations. This paper examines
how a database independent autonumber could be developed and reused solving all
the problems as well as providing the same degree of easy to use features of
autonumber offered by modern Relational Database Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04400</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04400</id><created>2015-03-15</created><authors><author><keyname>Miszczak</keyname><forenames>Jaros&#x142;aw Adam</forenames></author></authors><title>Separable and non-separable data representation for pattern
  discrimination</title><categories>quant-ph cs.CV cs.LG</categories><comments>11 pages, 2 figures</comments><acm-class>I.5.2; I.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a complete work-flow, based on the language of quantum information
theory, suitable for processing data for the purpose of pattern recognition.
The main advantage of the introduced scheme is that it can be easily
implemented and applied to process real-world data using modest computation
resources. At the same time it can be used to investigate the difference in the
pattern recognition resulting from the utilization of the tensor product
structure of the space of quantum states. We illustrate this difference by
providing a simple example based on the classification of 2D data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04404</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04404</id><created>2015-03-15</created><updated>2015-09-16</updated><authors><author><keyname>Liebig</keyname><forenames>J.</forenames></author><author><keyname>Rao</keyname><forenames>A.</forenames></author></authors><title>Predicting Item Popularity: Analysing Local Clustering Behaviour of
  Users</title><categories>cs.SI</categories><comments>25 pages, 11 figures</comments><doi>10.1016/j.physa.2015.08.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the popularity of items in rating networks is an interesting but
challenging problem. This is especially so when an item has first appeared and
has received very few ratings. In this paper, we propose a novel approach to
predicting the future popularity of new items in rating networks, defining a
new bipartite clustering coefficient to predict the popularity of movies and
stories in the MovieLens and Digg networks respectively. We show that the
clustering behaviour of the first user who rates a new item gives insight into
the future popularity of that item. Our method predicts, with a success rate of
over 65% for the MovieLens network and over 50% for the Digg network, the
future popularity of an item. This is a major improvement on current results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04422</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04422</id><created>2015-03-15</created><authors><author><keyname>Chen</keyname><forenames>Pengfei</forenames></author><author><keyname>Qi</keyname><forenames>Yong</forenames></author><author><keyname>Wang</keyname><forenames>Peipei</forenames></author><author><keyname>Su</keyname><forenames>Li</forenames></author><author><keyname>Li</keyname><forenames>Xinyi</forenames></author></authors><title>Making Availability as a Service in the Clouds</title><categories>cs.DC</categories><comments>5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has achieved great success in modern IT industry as an
excellent computing paradigm due to its flexible management and elastic
resource sharing. To date, cloud computing takes an irrepalceable position in
our socioeconomic system and influences almost every aspect of our daily life.
However, it is still in its infancy, many problems still exist.Besides the
hotly-debated security problem, availability is also an urgent issue.With the
limited power of availability mechanisms provided in present cloud platform, we
can hardly get detailed availability information of current applications such
as the root causes of availability problem,mean time to failure, etc. Thus a
new mechanism based on deep avaliability analysis is neccessary and
benificial.Following the prevalent terminology 'XaaS',this paper proposes a new
win-win concept for cloud users and providers in term of 'Availability as a
Service' (abbreviated as 'AaaS').The aim of 'AaaS' is to provide comprehensive
and aimspecific runtime avaliabilty analysis services for cloud users by
integrating plent of data-driven and modeldriven approaches. To illustrate this
concept, we realize a prototype named 'EagleEye' with all features of 'AaaS'.
By subscribing corresponding services in 'EagleEye', cloud users could get
specific availability information of their applications deployed in cloud
platform. We envision this new kind of service will be merged into the cloud
management mechanism in the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04424</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04424</id><created>2015-03-15</created><authors><author><keyname>Magdy</keyname><forenames>Walid</forenames></author><author><keyname>Sajjad</keyname><forenames>Hassan</forenames></author><author><keyname>El-Ganainy</keyname><forenames>Tarek</forenames></author><author><keyname>Sebastiani</keyname><forenames>Fabrizio</forenames></author></authors><title>Bridging Social Media via Distant Supervision</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Microblog classification has received a lot of attention in recent years.
Different classification tasks have been investigated, most of them focusing on
classifying microblogs into a small number of classes (five or less) using a
training set of manually annotated tweets. Unfortunately, labelling data is
tedious and expensive, and finding tweets that cover all the classes of
interest is not always straightforward, especially when some of the classes do
not frequently arise in practice. In this paper we study an approach to tweet
classification based on distant supervision, whereby we automatically transfer
labels from one social medium to another for a single-label multi-class
classification task. In particular, we apply YouTube video classes to tweets
linking to these videos. This provides for free a virtually unlimited number of
labelled instances that can be used as training data. The classification
experiments we have run show that training a tweet classifier via these
automatically labelled data achieves substantially better performance than
training the same classifier with a limited amount of manually labelled data;
this is advantageous, given that the automatically labelled data come at no
cost. Further investigation of our approach shows its robustness when applied
with different numbers of classes and across different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04426</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04426</id><created>2015-03-15</created><updated>2015-12-23</updated><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>An Improved Pseudo-Polynomial Upper Bound for the Value Problem and
  Optimal Strategy Synthesis in Mean Payoff Games</title><categories>cs.DS cs.CC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we offer an $O(|V|^2 |E|\, W)$ pseudo-polynomial time
deterministic algorithm for solving the Value Problem and Optimal Strategy
Synthesis in Mean Payoff Games. This improves by a factor $\log(|V|\, W)$ the
best previously known pseudo-polynomial time upper bound due to Brim,~\etal The
improvement hinges on a suitable characterization of values, and a description
of optimal positional strategies, in terms of reweighted Energy Games and Small
Energy-Progress Measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04444</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04444</id><created>2015-03-15</created><updated>2015-11-30</updated><authors><author><keyname>Tahir</keyname><forenames>Muhammad Masood</forenames></author><author><keyname>Hussain</keyname><forenames>Ayyaz</forenames></author></authors><title>Pattern Recognition of Bearing Faults using Smoother Statistical
  Features</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a crucial errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pattern recognition (PR) based diagnostic scheme is presented to identify
bearing faults, using time domain features. Vibration data is acquired from
faulty bearings using a test rig. The features are extracted from the data, and
processed prior to utilize in the PR process. The processing involves smoothing
of feature distributions. This reduces the undesired impact of vibration
randomness on the PR process, and thus enhances the diagnostic accuracy of the
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04468</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04468</id><created>2015-03-15</created><authors><author><keyname>Puerto</keyname><forenames>J.</forenames></author><author><keyname>Ramos</keyname><forenames>A. B.</forenames></author><author><keyname>Rodriguez-Chia</keyname><forenames>A. M.</forenames></author><author><keyname>Sanchez-Gil</keyname><forenames>M. C.</forenames></author></authors><title>Ordered Median Hub Location Problems with Capacity Constraints</title><categories>math.OC cs.DS</categories><msc-class>90B80</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Single Allocation Ordered Median Hub Location problem is a recent hub
model introduced in Puerto et al. (2011) that provides a unifying analysis of a
wide class of hub location mod- els. In this paper, we deal with the
capacitated version of this problem, presenting two formulations as well as
some preprocessing phases for fixing variables. In addition, a strengthening of
one of these formulations is also studied through the use of some fami- lies of
valid inequalities. A battery of test problems with data taken from the AP
library are solved where it is shown that the running times have been
significantly reduced with the improvements presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04473</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04473</id><created>2015-03-15</created><authors><author><keyname>Abbas</keyname><forenames>Waseem</forenames></author><author><keyname>Bhatia</keyname><forenames>Sajal</forenames></author><author><keyname>Koutsoukos</keyname><forenames>Xenofon</forenames></author></authors><title>Guarding Networks Through Heterogeneous Mobile Guards</title><categories>cs.DM cs.CR cs.SY</categories><comments>American Control Conference, Chicago, IL, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the issue of guarding multi-agent systems against a sequence
of intruder attacks through mobile heterogeneous guards (guards with different
ranges) is discussed. The article makes use of graph theoretic abstractions of
such systems in which agents are the nodes of a graph and edges represent
interconnections between agents. Guards represent specialized mobile agents on
specific nodes with capabilities to successfully detect and respond to an
attack within their guarding range. Using this abstraction, the article
addresses the problem in the context of eternal security problem in graphs.
Eternal security refers to securing all the nodes in a graph against an
infinite sequence of intruder attacks by a certain minimum number of guards.
This paper makes use of heterogeneous guards and addresses all the components
of the eternal security problem including the number of guards, their
deployment and movement strategies. In the proposed solution, a graph is
decomposed into clusters and a guard with appropriate range is then assigned to
each cluster. These guards ensure that all nodes within their corresponding
cluster are being protected at all times, thereby achieving the eternal
security in the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04475</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04475</id><created>2015-03-15</created><authors><author><keyname>Lienert</keyname><forenames>Eric</forenames></author></authors><title>Simulation of Genetic Algorithm: Traffic Light Efficiency</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic is a problem in many urban areas worldwide. Traffic flow is dictated
by certain devices such as traffic lights. The traffic lights signal when each
lane is able to pass through the intersection. Often, static schedules
interfere with ideal traffic flow. The purpose of this project was to find a
way to make intersections controlled with traffic lights more efficient. This
goal was accomplished through the creation of a genetic algorithm, which
enhances an input algorithm through genetic principles to produce the fittest
algorithm. The program was comprised of two major elements: coding in Java and
coding in Simulation of Urban Mobility (SUMO), which is an environment that
simulates real traffic. The Java code called upon the SUMO simulation via a
command prompt which ran the simulation, received the output, altered the
algorithm, and looped. The SUMO component initialized a simulation in which a 1
x 1 street layout was created, each intersection with its own traffic light.
Each loop enhanced the input algorithm by altering the scheduling string
(dictates the light changes). After the looped simulations were executed, the
data was then analyzed. This was accomplished by creating an algorithm based
upon regular practice, timed traffic lights, and comparing the output which was
comprised of the total time it took for all vehicles to exit the system and the
average time it took each individual vehicle to exit the system. These
different variables: the time it took the average vehicle to exit the system
and total time for all vehicles to exit the system, where then graphed together
to provide a visual aid. The genetic algorithm did improve traffic light and
traffic flow efficiency in comparison to traditional scheduling methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04476</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04476</id><created>2015-03-15</created><authors><author><keyname>Torrents</keyname><forenames>Jordi</forenames></author><author><keyname>Ferraro</keyname><forenames>Fabrizio</forenames></author></authors><title>Structural Cohesion: Visualization and Heuristics for Fast Computation</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structural cohesion model is a powerful theoretical conception of
cohesion in social groups, but its diffusion in empirical literature has been
hampered by operationalization and computational problems. In this paper we
start from the classic definition of structural cohesion as the minimum number
of actors who need to be removed in a network in order to disconnect it, and
extend it by using average node connectivity as a finer grained measure of
cohesion. We present useful heuristics for computing structural cohesion that
allow a speed-up of one order of magnitude over the algorithms currently
available. We analyze three large collaboration networks (co-maintenance of
Debian packages, co-authorship in Nuclear Theory and High-Energy Theory) and
show how our approach can help researchers measure structural cohesion in
relatively large networks. We also introduce a novel graphical representation
of the structural cohesion analysis to quickly spot differences across
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04486</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04486</id><created>2015-03-15</created><updated>2015-05-14</updated><authors><author><keyname>Bhangale</keyname><forenames>Amey</forenames></author><author><keyname>Kopparty</keyname><forenames>Swastik</forenames></author></authors><title>The complexity of computing the minimum rank of a sign pattern matrix</title><categories>cs.CC math.CO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that computing the minimum rank of a sign pattern matrix is NP hard.
Our proof is based on a simple but useful connection between minimum ranks of
sign pattern matrices and the stretchability problem for pseudolines
arrangements. In fact, our hardness result shows that it is already hard to
determine if the minimum rank of a sign pattern matrix is $\leq 3$. We
complement this by giving a polynomial time algorithm for determining if a
given sign pattern matrix has minimum rank $\leq 2$.
  Our result answers one of the open problems from Linial et al.
[Combinatorica, 27(4):439--463, 2007].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04500</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04500</id><created>2015-03-15</created><updated>2015-12-20</updated><authors><author><keyname>Jia</keyname><forenames>Zhongxiao</forenames></author><author><keyname>Kang</keyname><forenames>Wenjie</forenames></author></authors><title>A Residual Based Sparse Approximate Inverse Preconditioning Procedure
  for Large Sparse Linear Systems</title><categories>math.NA cs.NA</categories><comments>18 pages, 1 figure</comments><msc-class>65F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SPAI algorithm, a sparse approximate inverse preconditioning technique
for large sparse linear systems, proposed by Grote and Huckle [SIAM J. Sci.
Comput., 18 (1997), pp.~838--853.], is based on the F-norm minimization and
computes a sparse approximate inverse $M$ of a large sparse matrix $A$
adaptively. However, SPAI may be costly to seek the most profitable indices at
each loop and $M$ may be ineffective for preconditioning. In this paper, we
propose a residual based sparse approximate inverse preconditioning procedure
(RSAI), which, unlike SPAI, is based on only the {\em dominant} rather than all
information on the current residual and augments sparsity patterns adaptively
during the loops. RSAI is less costly to seek indices and is more effective to
capture a good approximate sparsity pattern of $A^{-1}$ than SPAI. To control
the sparsity of $M$ and reduce computational cost, we develop a practical
RSAI($tol$) algorithm that drops small nonzero entries adaptively during the
process. Numerical experiments are reported to demonstrate that RSAI($tol$) is
at least competitive with SPAI and can be considerably more efficient and
effective than SPAI. They also indicate that RSAI($tol$) is comparable to the
PSAI($tol$) algorithm proposed by one of the authors in 2009.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04501</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04501</id><created>2015-03-15</created><authors><author><keyname>Kawata</keyname><forenames>Shigeo</forenames></author></authors><title>Computer Assisted Parallel Program Generation</title><categories>physics.comp-ph cs.MS cs.SE</categories><comments>10 pages and 8 figures. Prepared for a book entitled &quot;Encyclopedia of
  Information Science and Technology&quot;, IGI global</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel computation is widely employed in scientific researches, engineering
activities and product development. Parallel program writing itself is not
always a simple task depending on problems solved. Large-scale scientific
computing, huge data analyses and precise visualizations, for example, would
require parallel computations, and the parallel computing needs the
parallelization techniques. In this Chapter a parallel program generation
support is discussed, and a computer-assisted parallel program generation
system P-NCAS is introduced. Computer assisted problem solving is one of key
methods to promote innovations in science and engineering, and contributes to
enrich our society and our life toward a programming-free environment in
computing science. Problem solving environments (PSE) research activities had
started to enhance the programming power in 1970's. The P-NCAS is one of the
PSEs; The PSE concept provides an integrated human-friendly computational
software and hardware system to solve a target class of problems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04502</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04502</id><created>2015-03-15</created><authors><author><keyname>Hendricks</keyname><forenames>Jacob</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Rogers</keyname><forenames>Trent A.</forenames></author></authors><title>The Simulation Powers and Limitations of Higher Temperature Hierarchical
  Self-Assembly Systems</title><categories>cs.CG cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend existing results about simulation and intrinsic
universality in a model of tile-based self-assembly. Namely, we work within the
2-Handed Assembly Model (2HAM), which is a model of self-assembly in which
assemblies are formed by square tiles that are allowed to combine, using glues
along their edges, individually or as pairs of arbitrarily large assemblies in
a hierarchical manner, and we explore the abilities of these systems to
simulate each other when the simulating systems have a higher &quot;temperature&quot;
parameter, which is a system wide threshold dictating how many glue bonds must
be formed between two assemblies to allow them to combine. It has previously
been shown that systems with lower temperatures cannot simulate arbitrary
systems with higher temperatures, and also that systems at some higher
temperatures can simulate those at particular lower temperatures, creating an
infinite set of infinite hierarchies of 2HAM systems with strictly increasing
simulation power within each hierarchy. These previous results relied on two
different definitions of simulation, one (strong simulation) seemingly more
restrictive than the other (standard simulation), but which have previously not
been proven to be distinct. Here we prove distinctions between them by first
fully characterizing the set of pairs of temperatures such that the high
temperature systems are intrinsically universal for the lower temperature
systems (i.e. one tile set at the higher temperature can simulate any at the
lower) using strong simulation. This includes the first impossibility result
for simulation downward in temperature. We then show that lower temperature
systems which cannot be simulated by higher temperature systems using the
strong definition, can in fact be simulated using the standard definition,
proving the distinction between the types of simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04522</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04522</id><created>2015-03-16</created><authors><author><keyname>de Amorim</keyname><forenames>Arthur Azevedo</forenames></author><author><keyname>Arias</keyname><forenames>Emilio Jes&#xfa;s Gallego</forenames></author><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author></authors><title>Really Natural Linear Indexed Type Checking</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works have shown the power of linear indexed type systems for
enforcing complex program properties. These systems combine linear types with a
language of type-level indices, allowing more fine-grained analyses. Such
systems have been fruitfully applied in diverse domains, including implicit
complexity and differential privacy. A natural way to enhance the
expressiveness of this approach is by allowing the indices to depend on runtime
information, in the spirit of dependent types. This approach is used in DFuzz,
a language for differential privacy. The DFuzz type system relies on an index
language supporting real and natural number arithmetic over constants and
variables. Moreover, DFuzz uses a subtyping mechanism to make types more
flexible. By themselves, linearity, dependency, and subtyping each require
delicate handling when performing type checking or type inference; their
combination increases this challenge substantially, as the features can
interact in non-trivial ways. In this paper, we study the type-checking problem
for DFuzz. We show how we can reduce type checking for (a simple extension of)
DFuzz to constraint solving over a first-order theory of naturals and real
numbers which, although undecidable, can often be handled in practice by
standard numeric solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04533</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04533</id><created>2015-03-16</created><updated>2015-07-17</updated><authors><author><keyname>Kala</keyname><forenames>Srikant Manas</forenames></author><author><keyname>Reddy</keyname><forenames>M. Pavan Kumar</forenames></author><author><keyname>Musham</keyname><forenames>Ranadheer</forenames></author><author><keyname>Tamma</keyname><forenames>Bheemarjuna Reddy</forenames></author></authors><title>Radio Co-location Aware Channel Assignments for Interference Mitigation
  in Wireless Mesh Networks</title><categories>cs.NI</categories><comments>Accepted @ ICACCI-2015</comments><journal-ref>ICACCI, Aug. 2015, 28 - 37</journal-ref><doi>10.1109/ICACCI.2015.7275580</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing high performance channel assignment schemes to harness the
potential of multi-radio multi-channel deployments in wireless mesh networks
(WMNs) is an active research domain. A pragmatic channel assignment approach
strives to maximize network capacity by restraining the endemic interference
and mitigating its adverse impact on network performance. Interference
prevalent in WMNs is multi-faceted, radio co-location interference (RCI) being
a crucial aspect that is seldom addressed in research endeavors. In this
effort, we propose a set of intelligent channel assignment algorithms, which
focus primarily on alleviating the RCI. These graph theoretic schemes are
structurally inspired by the spatio-statistical characteristics of
interference. We present the theoretical design foundations for each of the
proposed algorithms, and demonstrate their potential to significantly enhance
network capacity in comparison to some well-known existing schemes. We also
demonstrate the adverse impact of radio co- location interference on the
network, and the efficacy of the proposed schemes in successfully mitigating
it. The experimental results to validate the proposed theoretical notions were
obtained by running an exhaustive set of ns-3 simulations in IEEE 802.11g/n
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04566</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04566</id><created>2015-03-16</created><updated>2015-07-27</updated><authors><author><keyname>Rad</keyname><forenames>Tudor-Dan</forenames></author><author><keyname>Schr&#xf6;cker</keyname><forenames>Hans-Peter</forenames></author></authors><title>The Kinematic Image of 2R Dyads and Exact Synthesis of 5R Linkages</title><categories>math.MG cs.RO math.RA</categories><comments>Accepted for publication in the proceedings of the IMA Conference on
  Mathematics of Robotics, Oxford, 2015</comments><msc-class>70B15, 70B10, 12D05, 51N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterise the kinematic image of the constraint variety of a 2R dyad as
a regular ruled quadric in a 3-space that contains a &quot;null quadrilateral&quot;.
Three prescribed poses determine, in general, two such quadrics. This allows us
to modify a recent algorithm for the synthesis of 6R linkages in such a way
that two consecutive revolute axes coincide, thus producing a 5R linkage. Using
the classical geometry of twisted cubics on a quadric, we explain some of the
peculiar properties of the the resulting synthesis procedure for 5R linkages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04567</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04567</id><created>2015-03-16</created><updated>2015-04-22</updated><authors><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author></authors><title>Learning Mixed Membership Community Models in Social Tagging Networks
  through Tensor Methods</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in graphs has been extensively studied both in theory and
in applications. However, detecting communities in hypergraphs is more
challenging. In this paper, we propose a tensor decomposition approach for
guaranteed learning of communities in a special class of hypergraphs modeling
social tagging systems or folksonomies. A folksonomy is a tripartite 3-uniform
hypergraph consisting of (user, tag, resource) hyperedges. We posit a
probabilistic mixed membership community model, and prove that the tensor
method consistently learns the communities under efficient sample complexity
and separation requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04575</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04575</id><created>2015-03-16</created><authors><author><keyname>Fang</keyname><forenames>Qizhi</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Shan</keyname><forenames>Xiaohan</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author></authors><title>The Least-core and Nucleolus of Path Cooperative Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative games provide an appropriate framework for fair and stable profit
distribution in multiagent systems. In this paper, we study the algorithmic
issues on path cooperative games that arise from the situations where some
commodity flows through a network. In these games, a coalition of edges or
vertices is successful if it enables a path from the source to the sink in the
network, and lose otherwise. Based on dual theory of linear programming and the
relationship with flow games, we provide the characterizations on the CS-core,
least-core and nucleolus of path cooperative games. Furthermore, we show that
the least-core and nucleolus are polynomially solvable for path cooperative
games defined on both directed and undirected network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04576</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04576</id><created>2015-03-16</created><authors><author><keyname>Mijumbi</keyname><forenames>Rashid</forenames></author><author><keyname>Serrat</keyname><forenames>Joan</forenames></author><author><keyname>Gorricho</keyname><forenames>Juan-Luis</forenames></author></authors><title>Autonomic Resource Management in Virtual Networks</title><categories>cs.NI</categories><comments>Short Paper, 4 Pages, Summer School, PhD Work In Progress Workshop.
  Scalable and Adaptive Internet Solutions (SAIL). June 2012</comments><journal-ref>Scalable and Adaptive Internet Solutions (SAIL), 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization enables the building of multiple virtual networks over a
shared substrate. One of the challenges to virtualisation is efficient resource
allocation. This problem has been found to be NP hard. Therefore, most
approaches to it have not only proposed static solutions, but have also made
many assumptions to simplify it. In this paper, we propose a distributed,
autonomic and artificial intelligence based solution to resource allocation.
Our aim is to obtain self-configuring, selfoptimizing, self-healing and context
aware virtual networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04585</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04585</id><created>2015-03-16</created><updated>2015-09-13</updated><authors><author><keyname>Yasuda</keyname><forenames>Muneki</forenames></author><author><keyname>Kataoka</keyname><forenames>Shun</forenames></author><author><keyname>Tanaka</keyname><forenames>Kazuyuki</forenames></author></authors><title>Statistical Analysis of Loopy Belief Propagation in Random Fields</title><categories>stat.ML cond-mat.dis-nn cs.CV</categories><journal-ref>Phys. Rev. E 92, 042120 (2015)</journal-ref><doi>10.1103/PhysRevE.92.042120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loopy belief propagation (LBP), which is equivalent to the Bethe
approximation in statistical mechanics, is a message-passing-type inference
method that is widely used to analyze systems based on Markov random fields
(MRFs). In this paper, we propose a message-passing-type method to analytically
evaluate the quenched average of LBP in random fields by using the replica
cluster variation method. The proposed analytical method is applicable to
general pair-wise MRFs with random fields whose distributions differ from each
other and can give the quenched averages of the Bethe free energies over random
fields, which are consistent with numerical results. The order of its
computational cost is equivalent to that of standard LBP. In the latter part of
this paper, we describe the application of the proposed method to Bayesian
image restoration, in which we observed that our theoretical results are in
good agreement with the numerical results for natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04593</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04593</id><created>2015-03-16</created><authors><author><keyname>Avoine</keyname><forenames>Gildas</forenames></author><author><keyname>Mauw</keyname><forenames>Sjouke</forenames></author><author><keyname>Trujillo-Rasua</keyname><forenames>Rolando</forenames></author></authors><title>Comparing Distance Bounding Protocols: a Critical Mission Supported by
  Decision Theory</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance bounding protocols are security countermeasures designed to thwart
relay attacks. Such attacks consist in relaying messages exchanged between two
parties, making them believe they communicate directly with each other.
Although distance bounding protocols have existed since the early nineties,
this research topic resurrected with the deployment of contactless systems,
against which relay attacks are particularly impactful. Given the impressive
number of distance bounding protocols that are designed every year, it becomes
urgent to provide researchers and engineers with a methodology to fairly
compare the protocols in spite of their various properties. This paper
introduces such a methodology based on concepts from the decision making field.
The methodology allows for a multi-criteria comparison of distance bounding
protocols, thereby identifying the most appropriate protocols once the context
is provided. As a side effect, this paper clearly identifies the protocols that
should no longer be considered, regardless of the considered scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04596</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04596</id><created>2015-03-16</created><updated>2015-08-15</updated><authors><author><keyname>McDonnell</keyname><forenames>Mark D.</forenames></author><author><keyname>Vladusich</keyname><forenames>Tony</forenames></author></authors><title>Enhanced Image Classification With a Fast-Learning Shallow Convolutional
  Neural Network</title><categories>cs.NE cs.CV cs.LG</categories><comments>7 pages, 2 figures, Paper at IJCNN 2015 (International Joint
  Conference on Neural Networks, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a neural network architecture and training method designed to
enable very rapid training and low implementation complexity. Due to its
training speed and very few tunable parameters, the method has strong potential
for applications requiring frequent retraining or online training. The approach
is characterized by (a) convolutional filters based on biologically inspired
visual processing filters, (b) randomly-valued classifier-stage input weights,
(c) use of least squares regression to train the classifier output weights in a
single batch, and (d) linear classifier-stage output units. We demonstrate the
efficacy of the method by applying it to image classification. Our results
match existing state-of-the-art results on the MNIST (0.37% error) and
NORB-small (2.2% error) image classification databases, but with very fast
training times compared to standard deep network approaches. The network's
performance on the Google Street View House Number (SVHN) (4% error) database
is also competitive with state-of-the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04598</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04598</id><created>2015-03-16</created><updated>2015-03-17</updated><authors><author><keyname>Sabzevari</keyname><forenames>Reza</forenames></author><author><keyname>Murino</keyname><forenames>Vittori</forenames></author><author><keyname>Del Bue</keyname><forenames>Alessio</forenames></author></authors><title>PiMPeR: Piecewise Dense 3D Reconstruction from Multi-View and
  Multi-Illumination Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of dense 3D reconstruction from
multiple view images subject to strong lighting variations. In this regard, a
new piecewise framework is proposed to explicitly take into account the change
of illumination across several wide-baseline images. Unlike multi-view stereo
and multi-view photometric stereo methods, this pipeline deals with
wide-baseline images that are uncalibrated, in terms of both camera parameters
and lighting conditions. Such a scenario is meant to avoid use of any specific
imaging setup and provide a tool for normal users without any expertise. To the
best of our knowledge, this paper presents the first work that deals with such
unconstrained setting. We propose a coarse-to-fine approach, in which a coarse
mesh is first created using a set of geometric constraints and, then, fine
details are recovered by exploiting photometric properties of the scene.
Augmenting the fine details on the coarse mesh is done via a final optimization
step. Note that the method does not provide a generic solution for multi-view
photometric stereo problem but it relaxes several common assumptions of this
problem. The approach scales very well in size given its piecewise nature,
dealing with large scale optimization and with severe missing data. Experiments
on a benchmark dataset Robot data-set show the method performance against 3D
ground truth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04599</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04599</id><created>2015-03-16</created><authors><author><keyname>Dijkman</keyname><forenames>Remco</forenames></author><author><keyname>Ipeirotis</keyname><forenames>Panagiotis</forenames></author><author><keyname>Aertsen</keyname><forenames>Freek</forenames></author><author><keyname>van Helden</keyname><forenames>Roy</forenames></author></authors><title>Using Twitter to Predict Sales: A Case Study</title><categories>cs.SI</categories><report-no>BETA Working Paper WP-471</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the relation between activity on Twitter and sales. While
research exists into the relation between Tweets and movie and book sales, this
paper shows that the same relations do not hold for products that receive less
attention on social media. For such products, classification of Tweets is far
more important to determine a relation. Also, for such products advanced
statistical relations, in addition to correlation, are required to relate
Twitter activity and sales. In a case study that involves Tweets and sales from
a company in four countries, the paper shows how, by classifying Tweets, such
relations can be identified. In particular, the paper shows evidence that
positive Tweets by persons (as opposed to companies) can be used to forecast
sales and that peaks in positive Tweets by persons are strongly related to an
increase in sales. These results can be used to improve sales forecasts and to
increase sales in marketing campaigns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04604</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04604</id><created>2015-03-16</created><authors><author><keyname>Yang</keyname><forenames>Gang</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Multi-Antenna Wireless Energy Transfer for Backscatter Communication
  Systems</title><categories>cs.IT math.IT</categories><comments>30 pages, 5 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study RF-enabled wireless energy transfer (WET) via energy beamforming,
from a multi-antenna energy transmitter (ET) to multiple energy receivers (ERs)
in a backscatter communication system, such as RFID, where each ER (or RFID
tag) reflects back a portion of the incident signal to the ET (or RFID reader).
For such a system, the acquisition of the forward-channel (i.e., ET-to-ER)
state information (F-CSI) at the ET is challenging, since the ERs are typically
too energy-and-hardware-constrained to estimate or feed back the F-CSI. The ET
leverages its observed backscatter signals to estimate the backscatter-channel
(i.e., ET-to-ER-to-ET) state information (BS-CSI) directly. We first analyze
the harvested energy obtained by using the estimated BS-CSI. Furthermore, we
optimize the channel-training energy and the energy allocation weights for
different energy beams, for weighted-sum-energy (WSE) maximization and
proportional-fair-energy (PFE) maximization. For WET to single ER, we obtain
the optimal channel-training energy in a semi-closed form. For WET to multiple
ERs, the optimal WET scheme for WSE maximization is shown to use only one
energy beam. For PFE maximization, we show it is a biconvex problem, and
propose a block-coordinate-descent based algorithm to find the close-to-optimal
solution. Numerical results show that with the optimized solutions, the
harvested energy suffers slight reduction of less than 10%, compared to that
obtained by using the perfect F-CSI. Hence, energy beamforming by using the
estimated BS-CSI is promising, as the complexity and energy requirement is
shifted from the ERs to the ET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04608</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04608</id><created>2015-03-16</created><authors><author><keyname>Dimovski</keyname><forenames>Aleksandar S.</forenames></author><author><keyname>Brabrand</keyname><forenames>Claus</forenames></author><author><keyname>W&#x105;sowski</keyname><forenames>Andrzej</forenames></author></authors><title>Variability Abstractions: Trading Precision for Speed in Family-Based
  Analyses (Extended Version)</title><categories>cs.PL</categories><comments>50 pages, 10 figures</comments><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Family-based (lifted) data-flow analysis for Software Product Lines (SPLs) is
capable of analyzing all valid products (variants) without generating any of
them explicitly. It takes as input only the common code base, which encodes all
variants of a SPL, and produces analysis results corresponding to all variants.
However, the computational cost of the lifted analysis still depends inherently
on the number of variants (which is exponential in the number of features, in
the worst case). For a large number of features, the lifted analysis may be too
costly or even infeasible. In this paper, we introduce variability abstractions
defined as Galois connections and use abstract interpretation as a formal
method for the calculational-based derivation of approximate (abstracted)
lifted analyses of SPL programs, which are sound by construction. Moreover,
given an abstraction we define a syntactic transformation that translates any
SPL program into an abstracted version of it, such that the analysis of the
abstracted SPL coincides with the corresponding abstracted analysis of the
original SPL. We implement the transformation in a tool, reconfigurator that
works on Object-Oriented Java program families, and evaluate the practicality
of this approach on three Java SPL benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04609</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04609</id><created>2015-03-16</created><updated>2015-11-05</updated><authors><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Bacci</keyname><forenames>Giacomo</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Energy-Efficient Power Control: A Look at 5G Wireless Technologies</title><categories>cs.IT math.IT math.OC</categories><comments>Accepted for Publication in the IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work develops power control algorithms for energy efficiency (EE)
maximization (measured in bit/Joule) in wireless networks. Unlike previous
related works, minimum-rate constraints are imposed and the
signal-to-interference-plus-noise ratio takes a more general expression, which
allows one to encompass some of the most promising 5G candidate technologies.
Both network-centric and user-centric EE maximizations are considered. In the
network-centric scenario, the maximization of the global EE and the minimum EE
of the network are performed. Unlike previous contributions, we develop
centralized algorithms that are guaranteed to converge, with affordable
computational complexity, to a Karush-Kuhn-Tucker point of the considered
non-convex optimization problems. Moreover, closed-form feasibility conditions
are derived. In the user-centric scenario, game theory is used to study the
equilibria of the network and to derive convergent power control algorithms,
which can be implemented in a fully decentralized fashion. Both scenarios above
are studied under the assumption that single or multiple resource blocks are
employed for data transmission. Numerical results assess the performance of the
proposed solutions, analyzing the impact of minimum-rate constraints, and
comparing the network-centric and user-centric approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04610</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04610</id><created>2015-03-16</created><authors><author><keyname>Birget</keyname><forenames>J. C.</forenames></author></authors><title>Infinitely generated semigroups and polynomial complexity</title><categories>math.GR cs.CC</categories><comments>16 pages</comments><msc-class>20M05, 20M17, 68Q17, 68Q15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues the functional approach to the P-versus-NP problem,
begun in [1]. Here we focus on the monoid RM_2^P of right-ideal morphisms of
the free monoid, that have polynomial input balance and polynomial
time-complexity. We construct a machine model for the functions in RM_2^P, and
evaluation functions. We prove that RM_2^P is not finitely generated, and use
this to show separation results for time-complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04624</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04624</id><created>2015-03-16</created><authors><author><keyname>Barbier</keyname><forenames>Morgan</forenames></author><author><keyname>Bars</keyname><forenames>Jean-Marie Le</forenames></author><author><keyname>Rosenberger</keyname><forenames>Christophe</forenames></author></authors><title>Image Watermaking With Biometric Data For Copyright Protection</title><categories>cs.CR cs.MM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we deal with the proof of ownership or legitimate usage of a
digital content, such as an image, in order to tackle the illegitimate copy.
The proposed scheme based on the combination of the watermark-ing and
cancelable biometrics does not require a trusted third party, all the exchanges
are between the provider and the customer. The use of cancelable biometrics
permits to provide a privacy compliant proof of identity. We illustrate the
robustness of this method against intentional and unintentional attacks of the
watermarked content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04628</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04628</id><created>2015-03-16</created><authors><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author><author><keyname>Dubrova</keyname><forenames>Elena</forenames></author><author><keyname>Petersen</keyname><forenames>Kim</forenames></author></authors><title>Logic BIST: State-of-the-Art and Open Problems</title><categories>cs.AR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many believe that in-field hardware faults are too rare in practice to
justify the need for Logic Built-In Self-Test (LBIST) in a design. Until now,
LBIST was primarily used in safety-critical applications. However, this may
change soon. First, even if costly methods like burn-in are applied, it is no
longer possible to get rid of all latent defects in devices at leading-edge
technology. Second, demands for high reliability spread to consumer electronics
as smartphones replace our wallets and IDs. However, today many ASIC vendors
are reluctant to use LBIST. In this paper, we describe the needs for successful
deployment of LBIST in the industrial practice and discuss how these needs can
be addressed. Our work is hoped to attract a wider attention to this important
research topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04643</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04643</id><created>2015-03-16</created><updated>2015-05-06</updated><authors><author><keyname>Ngo</keyname><forenames>Dat Tien</forenames></author><author><keyname>Ostlund</keyname><forenames>Jonas</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Template-based Monocular 3D Shape Recovery using Laplacian Meshes</title><categories>cs.CV</categories><comments>Article</comments><doi>10.1109/TPAMI.2015.2435739</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that by extending the Laplacian formalism, which was first introduced
in the Graphics community to regularize 3D meshes, we can turn the monocular 3D
shape reconstruction of a deformable surface given correspondences with a
reference image into a much better-posed problem. This allows us to quickly and
reliably eliminate outliers by simply solving a linear least squares problem.
This yields an initial 3D shape estimate, which is not necessarily accurate,
but whose 2D projections are. The initial shape is then refined by a
constrained optimization problem to output the final surface reconstruction.
  Our approach allows us to reduce the dimensionality of the surface
reconstruction problem without sacrificing accuracy, thus allowing for
real-time implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04645</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04645</id><created>2015-03-16</created><updated>2015-08-03</updated><authors><author><keyname>Latu</keyname><forenames>G.</forenames></author><author><keyname>Haefele</keyname><forenames>M.</forenames></author><author><keyname>Bigot</keyname><forenames>J.</forenames></author><author><keyname>Grandgirard</keyname><forenames>V.</forenames></author><author><keyname>Cartier-Michaud</keyname><forenames>T.</forenames></author><author><keyname>Rozar</keyname><forenames>F.</forenames></author></authors><title>Evaluating kernels on Xeon Phi to accelerate Gysela application</title><categories>physics.comp-ph cs.DC cs.PF</categories><comments>submitted to ESAIM proceedings for CEMRACS 2014 summer school version
  reviewed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work describes the challenges presented by porting parts ofthe Gysela
code to the Intel Xeon Phi coprocessor, as well as techniques used for
optimization, vectorization and tuning that can be applied to other
applications. We evaluate the performance of somegeneric micro-benchmark on Phi
versus Intel Sandy Bridge. Several interpolation kernels useful for the Gysela
application are analyzed and the performance are shown. Some memory-bound and
compute-bound kernels are accelerated by a factor 2 on the Phi device compared
to Sandy architecture. Nevertheless, it is hard, if not impossible, to reach a
large fraction of the peek performance on the Phi device,especially for
real-life applications as Gysela. A collateral benefit of this optimization and
tuning work is that the execution time of Gysela (using 4D advections) has
decreased on a standard architecture such as Intel Sandy Bridge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04668</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04668</id><created>2015-03-16</created><authors><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author></authors><title>Mode Selection in MU-MIMO Downlink Networks: A Physical Layer Security
  Perspective</title><categories>cs.IT math.IT</categories><comments>9 pages, 5 figures and 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we consider a homogenous multi-antenna downlink network where
a passive eavesdropper intends to intercept the communication between a base
station (BS) and multiple secure users (SU) over Rayleigh fading channels. In
order to guarantee the security of information transfer, physical layer
security is employed accordingly. For such a multiple user (MU) secure network,
the number of accessing SUs, namely transmission mode, has a great impact on
the secrecy performance. Specifically, on the one hand, a large number of
accessing SUs will arise high inter-user interference at SUs, resulting in a
reduction of the capacity of the legitimate channel. On the other hand, high
inter-user interference will interfere with the eavesdropper and thus degrades
the performance of the eavesdropper channel. Generally speaking, the harmful
inter-user interference may be transformed as a useful tool of
anti-eavesdropping. The focus of this paper is on selecting the optimal
transmission mode according to channel conditions and system parameters, so as
to maximize the sum secrecy outage capacity. Moreover, through asymptotic
analysis, we present several simple mode selection schemes in some extreme
cases. Finally, simulation results validate the effectiveness of the proposed
mode selection schemes in MU secure communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04673</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04673</id><created>2015-03-16</created><authors><author><keyname>Pershin</keyname><forenames>Y. V.</forenames></author><author><keyname>Castelano</keyname><forenames>L. K.</forenames></author><author><keyname>Hartmann</keyname><forenames>F.</forenames></author><author><keyname>Lopez-Richard</keyname><forenames>V.</forenames></author><author><keyname>Di Ventra</keyname><forenames>M.</forenames></author></authors><title>A Memcomputing Pascaline</title><categories>cs.ET cond-mat.mes-hall cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original Pascaline was a mechanical calculator able to sum and subtract
integers. It encodes information in the angles of mechanical wheels and through
a set of gears, and aided by gravity, could perform the calculations. Here, we
show that such a concept can be realized in electronics using memory elements
such as memristive systems. By using memristive emulators we have demonstrated
experimentally the memcomputing version of the mechanical Pascaline, capable of
processing and storing the numerical results in the multiple levels of each
memristive element. Our result is the first experimental demonstration of
multidigit arithmetics with multi-level memory devices that further emphasizes
the versatility and potential of memristive systems for future
massively-parallel high-density computing architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04682</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04682</id><created>2015-03-16</created><authors><author><keyname>Banks</keyname><forenames>H. T.</forenames><affiliation>CRSC</affiliation></author><author><keyname>Doumic</keyname><forenames>M</forenames><affiliation>INRIA-Paris-Rocquencourt, LJLL</affiliation></author><author><keyname>Kruse</keyname><forenames>C</forenames><affiliation>INRIA-Paris-Rocquencourt, LJLL</affiliation></author><author><keyname>Prigent</keyname><forenames>S</forenames><affiliation>INRIA-Paris-Rocquencourt, LJLL</affiliation></author><author><keyname>Rezaei</keyname><forenames>H</forenames><affiliation>VIM</affiliation></author></authors><title>Information Content in Data Sets for a Nucleated-Polymerization Model</title><categories>math.AP cs.CE stat.AP</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We illustrate the use of tools (asymptotic theories of standard error
quantification using appropriate statistical models, bootstrapping, model
comparison techniques) in addition to sensitivity that may be employed to
determine the information content in data sets. We do this in the context of
recent models [23] for nucleated polymerization in proteins, about which very
little is known regarding the underlying mechanisms; thus the methodology we
develop here may be of great help to experimentalists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04688</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04688</id><created>2015-03-16</created><updated>2016-03-08</updated><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author><author><keyname>Richard</keyname><forenames>Adrien</forenames></author></authors><title>Simple dynamics on graphs</title><categories>cs.DM math.CO</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Does the interaction graph of a finite dynamical system can force this system
to have a &quot;complex&quot; dynamics ? In other words, given a finite interval of
integers $A$, which are the signed digraphs $G$ such that every finite
dynamical system $f:A^n\to A^n$ with $G$ as interaction graph has a &quot;complex&quot;
dynamics ? If $|A|\geq 3$ we prove that no such signed digraph exists. More
precisely, we prove that for every signed digraph $G$ there exists a system
$f:A^n\to A^n$ with $G$ as interaction graph that converges toward a unique
fixed point in at most $\lfloor\log_2 n\rfloor+2$ steps. The boolean case
$|A|=2$ is more difficult, and we provide partial answers instead. We exhibit
large classes of unsigned digraphs which admit boolean dynamical systems which
converge toward a unique fixed point in polynomial, linear or constant time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04693</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04693</id><created>2015-03-16</created><authors><author><keyname>Tzoumas</keyname><forenames>Vasileios</forenames></author><author><keyname>Rahimian</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Minimal Actuator Placement with Optimal Control Constraints</title><categories>cs.SY cs.SI math.OC</categories><comments>This version includes all the omitted proofs from the one to appear
  in the American Control Conference (ACC) 2015 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the problem of minimal actuator placement in a linear control
system so that a bound on the minimum control effort for a given state transfer
is satisfied while controllability is ensured. We first show that this is an
NP-hard problem following the recent work of Olshevsky. Next, we prove that
this problem has a supermodular structure. Afterwards, we provide an efficient
algorithm that approximates up to a multiplicative factor of O(logn), where n
is the size of the multi-agent network, any optimal actuator set that meets the
specified energy criterion. Moreover, we show that this is the best
approximation factor one can achieve in polynomial-time for the worst case.
Finally, we test this algorithm over large Erdos-Renyi random networks to
further demonstrate its efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04694</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04694</id><created>2015-03-16</created><authors><author><keyname>Rezaei</keyname><forenames>Aria</forenames></author><author><keyname>Far</keyname><forenames>Saeed Mahlouji</forenames></author><author><keyname>Soleymani</keyname><forenames>Mahdieh</forenames></author></authors><title>Controlled Label Propagation: Preventing Over-Propagation through
  Gradual Expansion</title><categories>cs.SI</categories><comments>8 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying communities has always been a fundamental task in analysis of
complex networks. Many methods have been devised over the last decade for
detection of communities. Amongst them, the label propagation algorithm brings
great scalability together with high accuracy. However, it has one major flaw;
when the community structure in the network is not clear enough, it will assign
every node the same label, thus detecting the whole graph as one giant
community. We have addressed this issue by setting a capacity for communities,
starting from a small value and gradually increasing it over time. Preliminary
results show that not only our extension improves the detection capability of
classic label propagation algorithm when communities are not clearly
detectable, but also improves the overall quality of the identified clusters in
complex networks with a clear community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04702</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04702</id><created>2015-03-16</created><authors><author><keyname>Cattan&#xe9;o</keyname><forenames>David</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>Minimum Degree up to Local Complementation: Bounds, Parameterized
  Complexity, and Exact Algorithms</title><categories>cs.DM math.CO quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local minimum degree of a graph is the minimum degree that can be reached
by means of local complementation. For any n, there exist graphs of order n
which have a local minimum degree at least 0.189n, or at least 0.110n when
restricted to bipartite graphs. Regarding the upper bound, we show that for any
graph of order n, its local minimum degree is at most 3n/8+o(n) and n/4+o(n)
for bipartite graphs, improving the known n/2 upper bound. We also prove that
the local minimum degree is smaller than half of the vertex cover number (up to
a logarithmic term). The local minimum degree problem is NP-Complete and hard
to approximate. We show that this problem, even when restricted to bipartite
graphs, is in W[2] and FPT-equivalent to the EvenSet problem, which
W[1]-hardness is a long standing open question. Finally, we show that the local
minimum degree is computed by a O*(1.938^n)-algorithm, and a
O*(1.466^n)-algorithm for the bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04706</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04706</id><created>2015-03-16</created><authors><author><keyname>Marc</keyname><forenames>Tilen</forenames></author></authors><title>There are no finite partial cubes of girth more than six and minimum
  degree at least three</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial cubes are graphs isometrically embeddable into hypercubes. We analyze
how isometric cycles in partial cubes behave and derive that every partial cube
of girth more than six must have vertices of degree less than three. As a
direct corollary we get that every regular partial cube of girth more than six
is an even cycle. Along the way we prove that every partial cube $G$ with girth
more than six is the so-called zone graph and therefore
$2n(G)-m(G)-i(G)+ce(G)=2$ holds, where $i(G)$ is the isometric dimension of $G$
and $ce(G)$ its convex excess.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04717</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04717</id><created>2015-03-16</created><authors><author><keyname>Faenza</keyname><forenames>Yuri</forenames></author><author><keyname>Sanit&#xe0;</keyname><forenames>Laura</forenames></author></authors><title>On the existence of compact {\epsilon}-approximated formulations for
  knapsack in the original space</title><categories>math.OC cs.DM math.CO</categories><msc-class>90C05</msc-class><acm-class>G.1.6; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exists a family of Knapsack polytopes such that, for each
polytope P from this family and each {\epsilon} &gt; 0, any
{\epsilon}-approximated formulation of P in the original space R^n requires a
number of inequalities that is super-polynomial in n. This answers a question
by Bienstock and McClosky (2012). We also prove that, for any down-monotone
polytope, an {\epsilon}-approximated formulation in the original space can be
obtained with inequalities using at most O(min{log(n/{\epsilon}),n}/{\epsilon})
different coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04718</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04718</id><created>2015-03-16</created><updated>2015-03-16</updated><authors><author><keyname>Li</keyname><forenames>Haodong</forenames></author><author><keyname>Luo</keyname><forenames>Weiqi</forenames></author><author><keyname>Qiu</keyname><forenames>Xiaoqing</forenames></author><author><keyname>Huang</keyname><forenames>Jiwu</forenames></author></authors><title>Identification of Image Operations Based on Steganalytic Features</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image forensics have attracted wide attention during the past decade. Though
many forensic methods have been proposed to identify image forgeries, most of
them are targeted ones, since their proposed features are highly dependent on
the image operation under investigation. The performance of the well-designed
features for detecting the targeted operation usually degrades significantly
for other operations. On the other hand, a wise attacker can perform
anti-forensics to fool the existing forensic methods, making countering
anti-forensics become an urgent need. In this paper, we try to find a universal
feature to detect various image processing and anti-forensic operations. Based
on our extensive experiments and analysis, we find that any image
processing/anti-forensic operations would inevitably modify many image pixels.
This would change some inherent statistics within original images, which is
similar to the case of steganography. Therefore, we model image
processing/anti-forensic operations as steganography problems, and propose a
detection strategy by applying steganalytic features. With some advanced
steganalytic features, we are able to detect various image operations and
further identify their types. In our experiments, we have tested several
steganalytic features on 11 different kinds of typical image processing
operations and 4 kinds of anti-forensic operations. The experimental results
have shown that the proposed strategy significantly outperforms the existing
forensic methods in both effectiveness and universality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04723</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04723</id><created>2015-03-16</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Staiano</keyname><forenames>Jacopo</forenames></author></authors><title>Deep Feelings: A Massive Cross-Lingual Study on the Relation between
  Emotions and Virality</title><categories>cs.SI cs.CL cs.CY</categories><comments>preprint version of WWW 2015 'Web Science Track' paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a comprehensive investigation on the relations between
virality of news articles and the emotions they are found to evoke. Virality,
in our view, is a phenomenon with many facets, i.e. under this generic term
several different effects of persuasive communication are comprised. By
exploiting a high-coverage and bilingual corpus of documents containing metrics
of their spread on social networks as well as a massive affective annotation
provided by readers, we present a thorough analysis of the interplay between
evoked emotions and viral facets. We highlight and discuss our findings in
light of a cross-lingual approach: while we discover differences in evoked
emotions and corresponding viral effects, we provide preliminary evidence of a
generalized explanatory model rooted in the deep structure of emotions: the
Valence-Arousal-Dominance (VAD) circumplex. We find that viral facets appear to
be consistently affected by particular VAD configurations, and these
configurations indicate a clear connection with distinct phenomena underlying
persuasive communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04729</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04729</id><created>2015-03-16</created><authors><author><keyname>Gottschlich</keyname><forenames>Carsten</forenames></author></authors><title>Skilled Impostor Attacks Against Fingerprint Verification Systems And
  Its Remedy</title><categories>cs.CV cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprint verification systems are becoming ubiquitous in everyday life.
This trend is propelled especially by the proliferation of mobile devices with
fingerprint sensors such as smartphones and tablet computers, and fingerprint
verification is increasingly applied for authenticating financial transactions.
In this study we describe a novel attack vector against fingerprint
verification systems which we coin skilled impostor attack. We show that
existing protocols for performance evaluation of fingerprint verification
systems are flawed and as a consequence of this, the system's real
vulnerability is systematically underestimated. We examine a scenario in which
a fingerprint verification system is tuned to operate at false acceptance rate
of 0.1% using the traditional verification protocols with random impostors
(zero-effort attacks). We demonstrate that an active and intelligent attacker
can achieve a chance of success in the area of 89% or more against this system
by performing skilled impostor attacks. We describe a new protocol for
evaluating fingerprint verification performance in order to improve the
assessment of potential and limitations of fingerprint recognition systems.
This new evaluation protocol enables a more informed decision concerning the
operating threshold in practical applications and the respective trade-off
between security (low false acceptance rates) and usability (low false
rejection rates). The skilled impostor attack is a general attack concept which
is independent of specific databases or comparison algorithms. The proposed
protocol relying on skilled impostor attacks can directly be applied for
evaluating the verification performance of other biometric modalities such as
e.g. iris, face, ear, finger vein, gait or speaker recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04748</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04748</id><created>2015-03-16</created><authors><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Asymmetric coloring games on incomparability graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following game on a graph $G$: Alice and Bob take turns coloring
the vertices of $G$ properly from a fixed set of colors; Alice wins when the
entire graph has been colored, while Bob wins when some uncolored vertices have
been left. The game chromatic number of $G$ is the minimum number of colors
that allows Alice to win the game. The game Grundy number of $G$ is defined
similarly except that the players color the vertices according to the first-fit
rule and they only decide on the order in which it is applied. The $(a,b)$-game
chromatic and Grundy numbers are defined likewise except that Alice colors $a$
vertices and Bob colors $b$ vertices in each round. We study the behavior of
these parameters for incomparability graphs of posets with bounded width. We
conjecture a complete characterization of the pairs $(a,b)$ for which the
$(a,b)$-game chromatic and Grundy numbers are bounded in terms of the width of
the poset; we prove that it gives a necessary condition and provide some
evidence for its sufficiency. We also show that the game chromatic number is
not bounded in terms of the Grundy number, which answers a question of Havet
and Zhu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04752</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04752</id><created>2015-03-16</created><authors><author><keyname>Meric</keyname><forenames>Hugo</forenames></author></authors><title>Approaching the Gaussian channel capacity with APSK constellations</title><categories>cs.IT math.IT</categories><comments>Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Gaussian channel with power constraint P. A gap exists
between the channel capacity and the highest achievable rate of equiprobable
uniformly spaced signal. Several approaches enable to overcome this limitation
such as constellations with non-uniform probability or constellation shaping.
In this letter, we focus on constellation shaping. We give a construction of
amplitude and phase-shift keying (APSK) constellations with equiprobable
signaling that achieve the Gaussian capacity as the number of constellation
points goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04753</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04753</id><created>2015-03-13</created><authors><author><keyname>Sou</keyname><forenames>Kin Cheong</forenames></author></authors><title>Minimum Equivalent Precedence Relation Systems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper two related simplification problems for systems of linear
inequalities describing precedence relation systems are considered. Given a
precedence relation system, the first problem seeks a minimum subset of the
precedence relations (i.e., inequalities) which has the same solution set as
that of the original system. The second problem is the same as the first one
except that the ``subset restriction'' in the first problem is removed. This
paper establishes that the first problem is NP-hard. However, a sufficient
condition is provided under which the first problem is solvable in
polynomial-time. In addition, a decomposition of the first problem into
independent tractable and intractable subproblems is derived. The second
problem is shown to be solvable in polynomial-time, with a full
parameterization of all solutions described. The results in this paper
generalize those in [Moyles and Thompson 1969, Aho, Garey, and Ullman 1972] for
the minimum equivalent graph problem and transitive reduction problem, which
are applicable to unweighted directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04755</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04755</id><created>2015-03-16</created><updated>2015-04-02</updated><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>The Price of Anarchy in Large Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game-theoretic models relevant for computer science applications usually
feature a large number of players. The goal of this paper is to develop an
analytical framework for bounding the price of anarchy in such models. We
demonstrate the wide applicability of our framework through instantiations for
several well-studied models, including simultaneous single-item auctions,
greedy combinatorial auctions, and routing games. In all cases, we identify
conditions under which the POA of large games is much better than that of
worst-case instances. Our results also give new senses in which simple auctions
can perform almost as well as optimal ones in realistic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04768</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04768</id><created>2015-03-16</created><updated>2015-08-12</updated><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>Van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Self-organizing Networks of Information Gathering Cognitive Agents</title><categories>cs.SI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many scenarios, networks emerge endogenously as cognitive agents establish
links in order to exchange information. Network formation has been widely
studied in economics, but only on the basis of simplistic models that assume
that the value of each additional piece of information is constant. In this
paper we present a first model and associated analysis for network formation
under the much more realistic assumption that the value of each additional
piece of information depends on the type of that piece of information and on
the information already possessed: information may be complementary or
redundant. We model the formation of a network as a non-cooperative game in
which the actions are the formation of links and the benefit of forming a link
is the value of the information exchanged minus the cost of forming the link.
We characterize the topologies of the networks emerging at a Nash equilibrium
(NE) of this game and compare the efficiency of equilibrium networks with the
efficiency of centrally designed networks. To quantify the impact of
information redundancy and linking cost on social information loss, we provide
estimates for the Price of Anarchy (PoA); to quantify the impact on individual
information loss we introduce and provide estimates for a measure we call
Maximum Information Loss (MIL). Finally, we consider the setting in which
agents are not endowed with information, but must produce it. We show that the
validity of the well-known &quot;law of the few&quot; depends on how information
aggregates; in particular, the &quot;law of the few&quot; fails when information displays
complementarities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04776</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04776</id><created>2015-03-16</created><authors><author><keyname>Tofighi</keyname><forenames>Mohammad</forenames></author><author><keyname>Yorulmaz</keyname><forenames>Onur</forenames></author><author><keyname>Cetin</keyname><forenames>A. Enis</forenames></author></authors><title>Phase and TV Based Convex Sets for Blind Deconvolution of Microscopic
  Images</title><categories>math.OC cs.CV</categories><comments>Submitted to IEEE Selected Topics in Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, two closed and convex sets for blind deconvolution problem
are proposed. Most blurring functions in microscopy are symmetric with respect
to the origin. Therefore, they do not modify the phase of the Fourier transform
(FT) of the original image. As a result blurred image and the original image
have the same FT phase. Therefore, the set of images with a prescribed FT phase
can be used as a constraint set in blind deconvolution problems. Another convex
set that can be used during the image reconstruction process is the epigraph
set of Total Variation (TV) function. This set does not need a prescribed upper
bound on the total variation of the image. The upper bound is automatically
adjusted according to the current image of the restoration process. Both of
these two closed and convex sets can be used as a part of any blind
deconvolution algorithm. Simulation examples are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04779</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04779</id><created>2015-03-16</created><authors><author><keyname>Eftekhari</keyname><forenames>Mohammad</forenames><affiliation>LAMFA</affiliation></author></authors><title>Cryptanalysis of some protocols using matrices over group rings</title><categories>cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a cryptanalysis of two protocols based on the supposed difficulty
of discrete logarithm problem on (semi) groups of matrices over a group ring.
We can find the secret key and break entirely the protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04784</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04784</id><created>2015-03-16</created><authors><author><keyname>Ram</keyname><forenames>Yoav</forenames></author><author><keyname>Moshaioff</keyname><forenames>Ofer</forenames></author><author><keyname>Cohen</keyname><forenames>Idan</forenames></author><author><keyname>Dor</keyname><forenames>Omri</forenames></author></authors><title>Forecasting the Israeli 2015 elections using a smartphone application</title><categories>stat.AP cs.SI</categories><comments>5 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed a smartphone application, Ha'Midgam, to poll and forecast the
results of the 2015 Israeli elections. The application was downloaded by over
7,500 people. We present the method used to control bias in our sample and our
forecasts. We discuss limitations of our approach and suggest possible
solutions to control bias in similar applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04794</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04794</id><created>2015-03-10</created><authors><author><keyname>LaPlante</keyname><forenames>Michael</forenames></author></authors><title>A Polynomial Time Algorithm For Solving Clique Problems</title><categories>cs.DS</categories><comments>23 pages, 35 figures and some blocks of pseudo-code, but the
  algorithm is explained by page 7 and 11 figures</comments><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a single algorithm which solves the clique problems, &quot;What is the
largest size clique?&quot;, &quot;What are all the maximal cliques?&quot; and the decision
problem, &quot;Does a clique of size k exist?&quot; for any given graph in polynomial
time. The existence of this algorithm proves that P = NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04796</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04796</id><created>2015-03-14</created><authors><author><keyname>Jasim</keyname><forenames>Omer K.</forenames></author><author><keyname>Abbas</keyname><forenames>Safia</forenames></author><author><keyname>Horbaty</keyname><forenames>El-Sayed M.</forenames></author><author><keyname>Salem</keyname><forenames>Abdel-Badeeh M.</forenames></author></authors><title>Evolution of an Emerging Symmetric Quantum Cryptographic Algorithm</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid evolution of data exchange in network environments,
information security has been the most important process for data storage and
communication. In order to provide such information security, the
confidentiality, data integrity, and data origin authentication must be
verified based on cryptographic encryption algorithms. This paper presents a
new emerging trend of modern symmetric encryption algorithm by development of
the advanced encryption standard (AES) algorithm. The new development focuses
on the integration between Quantum Key Distribution (QKD) and an enhanced
version of AES. A new quantum symmetric encryption algorithm, which is
abbreviated as Quantum-AES (QAES), is the output of such integration. QAES
depends on generation of dynamic quantum S-Boxes (DQS-Boxes) based quantum
cipher key, instead of the ordinary used static S-Boxes. Furthermore, QAES
exploits the specific selected secret key generated from the QKD cipher using
two different modes (online and off-line).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04831</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04831</id><created>2015-03-16</created><authors><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author><author><keyname>Pirro</keyname><forenames>Giuseppe</forenames></author></authors><title>A Context-Based Semantics for SPARQL Property Paths over the Web
  (Extended Version)</title><categories>cs.DB</categories><comments>29 pages, Extended version of a paper published in ESWC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As of today, there exists no standard language for querying Linked Data on
the Web, where navigation across distributed data sources is a key feature. A
natural candidate seems to be SPARQL, which recently has been enhanced with
navigational capabilities thanks to the introduction of property paths (PPs).
However, the semantics of SPARQL restricts the scope of navigation via PPs to
single RDF graphs. This restriction limits the applicability of PPs on the Web.
To fill this gap, in this paper we provide formal foundations for evaluating
PPs on the Web, thus contributing to the definition of a query language for
Linked Data. In particular, we introduce a query semantics for PPs that couples
navigation at the data level with navigation on the Web graph. Given this
semantics we find that for some PP-based SPARQL queries a complete evaluation
on the Web is not feasible. To enable systems to identify queries that can be
evaluated completely, we establish a decidable syntactic property of such
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04837</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04837</id><created>2015-03-16</created><authors><author><keyname>Ray</keyname><forenames>Kisor</forenames></author></authors><title>Use of Effective Audio in E-learning Courseware</title><categories>cs.CY</categories><comments>05 pages, 09 Tables, International Journal of Advanced Research in
  Computer Science and Software Engineering, Volume 4, Issue 9, September 2014
  ISSN: 2277 128X</comments><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  E-Learning uses electronic media, information &amp; communication technologies to
provide education to the masses. E-learning deliver hypertext, text, audio,
images, animation and videos using desktop standalone computer, local area
network based intranet and internet based contents. While producing an
e-learning content or course-ware, a major decision making factor is whether to
use audio for the benefit of the end users. Generally, three types of audio can
be used in e-learning: narration, music and sound effect. This paper shows that
the use of proper audio based on contents type and subject can make the content
more interesting as well as help the end users to better understand the
contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04843</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04843</id><created>2015-03-16</created><updated>2015-11-09</updated><authors><author><keyname>Bassily</keyname><forenames>Raef</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>More General Queries and Less Generalization Error in Adaptive Data
  Analysis</title><categories>cs.LG cs.DS</categories><comments>This paper was merged with another manuscript and is now subsumed by
  arXiv:1511.02513</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptivity is an important feature of data analysis---typically the choice of
questions asked about a dataset depends on previous interactions with the same
dataset. However, generalization error is typically bounded in a non-adaptive
model, where all questions are specified before the dataset is drawn. Recent
work by Dwork et al. (STOC '15) and Hardt and Ullman (FOCS '14) initiated the
formal study of this problem, and gave the first upper and lower bounds on the
achievable generalization error for adaptive data analysis.
  Specifically, suppose there is an unknown distribution $\mathcal{P}$ and a
set of $n$ independent samples $x$ is drawn from $\mathcal{P}$. We seek an
algorithm that, given $x$ as input, &quot;accurately&quot; answers a sequence of
adaptively chosen &quot;queries&quot; about the unknown distribution $\mathcal{P}$. How
many samples $n$ must we draw from the distribution, as a function of the type
of queries, the number of queries, and the desired level of accuracy?
  In this work we make two new contributions towards resolving this question:
  *We give upper bounds on the number of samples $n$ that are needed to answer
statistical queries that improve over the bounds of Dwork et al.
  *We prove the first upper bounds on the number of samples required to answer
more general families of queries. These include arbitrary low-sensitivity
queries and the important class of convex risk minimization queries.
  As in Dwork et al., our algorithms are based on a connection between
differential privacy and generalization error, but we feel that our analysis is
simpler and more modular, which may be useful for studying these questions in
the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04864</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04864</id><created>2015-03-16</created><authors><author><keyname>Hamdi</keyname><forenames>Fay&#xe7;al</forenames></author><author><keyname>Abadie</keyname><forenames>Nathalie</forenames></author><author><keyname>Bucher</keyname><forenames>B&#xe9;n&#xe9;dicte</forenames></author><author><keyname>Feliachi</keyname><forenames>Abdelfettah</forenames></author></authors><title>GeomRDF: A Geodata Converter with a Fine-Grained Structured
  Representation of Geometry in the Web</title><categories>cs.DB cs.AI</categories><comments>12 pages, 2 figures, the 1st International Workshop on Geospatial
  Linked Data (GeoLD 2014) - SEMANTiCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, with the advent of the web of data, a growing number of
national mapping agencies tend to publish their geospatial data as Linked Data.
However, differences between traditional GIS data models and Linked Data model
can make the publication process more complicated. Besides, it may require, to
be done, the setting of several parameters and some expertise in the semantic
web technologies. In addition, the use of standards like GeoSPARQL (or ad hoc
predicates) is mandatory to perform spatial queries on published geospatial
data. In this paper, we present GeomRDF, a tool that helps users to convert
spatial data from traditional GIS formats to RDF model easily. It generates
geometries represented as GeoSPARQL WKT literal but also as structured
geometries that can be exploited by using only the RDF query language, SPARQL.
GeomRDF was implemented as a module in the RDF publication platform Datalift. A
validation of GeomRDF has been realized against the French administrative units
dataset (provided by IGN France).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04867</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04867</id><created>2015-03-16</created><updated>2015-03-17</updated><authors><author><keyname>Brooks</keyname><forenames>Martin</forenames></author></authors><title>Varlets: Additive Decomposition, Topological Total Variation, and
  Filtering of Scalar Fields</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous interpolation of real-valued data is characterized by piecewise
monotone functions on a compact metric space. Topological total variation of
piecewise monotone function f:X-&gt;R is a homeomorphism-invariant generalization
of 1D total variation. A varlet basis is a collection of piecewise monotone
functions { $g_i$ |i = 1...n}, called varlets, such that every linear
combination $\sum a_ig_i$ has topological total variation $\sum |a_i|$. A
varlet transform for $f$ is a varlet basis for which $f =\sum \alpha_ig_i$.
Filtered versions of $f$ result from altering the coefficients $\alpha_i$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04871</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04871</id><created>2015-03-16</created><authors><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Strong Matching of Points with Geometric Shapes</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in general position in the plane. Given a
convex geometric shape $S$, a geometric graph $G_S(P)$ on $P$ is defined to
have an edge between two points if and only if there exists an empty homothet
of $S$ having the two points on its boundary. A matching in $G_S(P)$ is said to
be $strong$, if the homothests of $S$ representing the edges of the matching,
are pairwise disjoint, i.e., do not share any point in the plane. We consider
the problem of computing a strong matching in $G_S(P)$, where $S$ is a
diametral-disk, an equilateral-triangle, or a square. We present an algorithm
which computes a strong matching in $G_S(P)$; if $S$ is a diametral-disk, then
it computes a strong matching of size at least $\lceil \frac{n-1}{17} \rceil$,
and if $S$ is an equilateral-triangle, then it computes a strong matching of
size at least $\lceil \frac{n-1}{9} \rceil$. If $S$ can be a downward or an
upward equilateral-triangle, we compute a strong matching of size at least
$\lceil \frac{n-1}{4} \rceil$ in $G_S(P)$. When $S$ is an axis-aligned square
we compute a strong matching of size $\lceil \frac{n-1}{4} \rceil$ in $G_S(P)$,
which improves the previous lower bound of $\lceil \frac{n}{5} \rceil$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04877</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04877</id><created>2015-03-16</created><authors><author><keyname>Muhammad</keyname><forenames>Syed Agha</forenames></author><author><keyname>Van Laerhoven</keyname><forenames>Kristof</forenames></author></authors><title>An Automated System for Discovering Neighborhood Patterns in Ego
  Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generally, social network analysis has often focused on the topology of the
network without considering the characteristics of individuals involved in
them. Less attention is given to study the behavior of individuals, considering
they are the basic entity of a graph. Given a mobile social network graph, what
are good features to extract key information from the nodes?How many distinct
neighborhood patterns exist for ego nodes? What clues does such information
provide to study nodes over a long period of time?
  In this report, we develop an automated system in order to discover the
occurrences of prototypical ego-centric patterns from data. We aim to provide a
data-driven instrument to be used in behavioral sciences for graph
interpretations. We analyze social networks derived from real-world data
collected with smart-phones. We select 13 well-known network measures,
especially those concerned with ego graphs. We form eight feature subsets and
then assess their performance using unsupervised clustering techniques to
discover distinguishing ego-centric patterns. From clustering analysis, we
discover that eight distinct neighborhood patterns have emerged. This
categorization allows concise analysis of users' data as they change over time.
The results provide a fine-grained analysis for the contribution of different
feature sets to detect unique clustering patterns. Last, as a case study, two
datasets are studied over long periods to demonstrate the utility of this
method. The study shows the effectiveness of the proposed approach in
discovering important trends from data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04881</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04881</id><created>2015-03-16</created><authors><author><keyname>Zhu</keyname><forenames>Xiaodan</forenames></author><author><keyname>Sobhani</keyname><forenames>Parinaz</forenames></author><author><keyname>Guo</keyname><forenames>Hongyu</forenames></author></authors><title>Long Short-Term Memory Over Tree Structures</title><categories>cs.CL cs.LG cs.NE</categories><comments>On February 6th, 2015, this work was submitted to the International
  Conference on Machine Learning (ICML)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The chain-structured long short-term memory (LSTM) has showed to be effective
in a wide range of problems such as speech recognition and machine translation.
In this paper, we propose to extend it to tree structures, in which a memory
cell can reflect the history memories of multiple child cells or multiple
descendant cells in a recursive process. We call the model S-LSTM, which
provides a principled way of considering long-distance interaction over
hierarchies, e.g., language or image parse structures. We leverage the models
for semantic composition to understand the meaning of text, a fundamental
problem in natural language understanding, and show that it outperforms a
state-of-the-art recursive model by replacing its composition layers with the
S-LSTM memory blocks. We also show that utilizing the given structures is
helpful in achieving a performance better than that without considering the
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04885</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04885</id><created>2015-03-16</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Optimal control of the state statistics for a linear stochastic system</title><categories>math.OC cs.SY</categories><comments>7 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1410.3447</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variant of the classical linear quadratic Gaussian regulator
(LQG) in which penalties on the endpoint state are replaced by the
specification of the terminal state distribution. The resulting theory
considerably differs from LQG as well as from formulations that bound the
probability of violating state constraints. We develop results for optimal
state-feedback control in the two cases where i) steering of the state
distribution is to take place over a finite window of time with minimum energy,
and ii) the goal is to maintain the state at a stationary distribution over an
infinite horizon with minimum power. For both problems the distribution of
noise and state are Gaussian. In the first case, we show that provided the
system is controllable, the state can be steered to any terminal Gaussian
distribution over any specified finite time-interval. In the second case, we
characterize explicitly the covariance of admissible stationary state
distributions that can be maintained with constant state-feedback control. The
conditions for optimality are expressed in terms of a system of dynamically
coupled Riccati equations in the finite horizon case and in terms of algebraic
conditions for the stationary case. In the case where the noise and control
share identical input channels, the Riccati equations for finite-horizon
steering become homogeneous and can be solved in closed form. The present paper
is largely based on our recent work in arxiv.org/abs/1408.2222,
arxiv.org/abs/1410.3447 and presents an overview of certain key results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04894</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04894</id><created>2015-03-16</created><authors><author><keyname>Halder</keyname><forenames>Udit</forenames></author><author><keyname>Dey</keyname><forenames>Biswadip</forenames></author></authors><title>Biomimetic Algorithms for Coordinated Motion: Theory and Implementation</title><categories>cs.RO cs.MA cs.SY</categories><msc-class>68T40, 93C85</msc-class><acm-class>I.2.9; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing inspiration from flight behavior in biological settings (e.g.
territorial battles in dragonflies, and flocking in starlings), this paper
demonstrates two strategies for coverage and flocking. Using earlier
theoretical studies on mutual motion camouflage, an appropriate steering
control law for area coverage has been implemented in a laboratory test-bed
equipped with wheeled mobile robots and a Vicon high speed motion capture
system. The same test-bed is also used to demonstrate another strategy (based
on local information), termed topological velocity alignment, which serves to
make agents move in the same direction. The present work illustrates the
applicability of biological inspiration in the design of multi-agent robotic
collectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04896</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04896</id><created>2015-03-16</created><authors><author><keyname>Magalingam</keyname><forenames>Pritheega</forenames></author><author><keyname>Rao</keyname><forenames>Asha</forenames></author><author><keyname>Davis</keyname><forenames>Stephen</forenames></author></authors><title>Identifying a Criminal's Network of Trust</title><categories>cs.SI physics.soc-ph</categories><comments>2014 Tenth International Conference on Signal-Image Technology &amp;
  Internet-Based Systems (Presented at Third International Workshop on Complex
  Networks and their Applications,SITIS 2014, Marrakesh, Morocco, 23-27,
  November 2014)</comments><doi>10.1109/SITIS.2014.64</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracing criminal ties and mining evidence from a large network to begin a
crime case analysis has been difficult for criminal investigators due to large
numbers of nodes and their complex relationships. In this paper, trust networks
using blind carbon copy (BCC) emails were formed. We show that our new shortest
paths network search algorithm combining shortest paths and network centrality
measures can isolate and identify criminals' connections within a trust
network. A group of BCC emails out of 1,887,305 Enron email transactions were
isolated for this purpose. The algorithm uses two central nodes, most
influential and middle man, to extract a shortest paths trust network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04899</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04899</id><created>2015-03-16</created><updated>2015-10-05</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>A Multi-Tier Wireless Spectrum Sharing System Leveraging Secure Spectrum
  Auctions</title><categories>cs.NI cs.GT</categories><comments>(c) 2015 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, including
  reprinting/republishing this material for advertising or promotional
  purposes, collecting new collected works for resale or redistribution to
  servers or lists, or reuse of any copyrighted component of this work in other
  works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure spectrum auctions can revolutionize the spectrum utilization of
cellular networks and satisfy the ever increasing demand for resources. In this
paper, a multi-tier dynamic spectrum sharing system is studied for efficient
sharing of spectrum with commercial wireless system providers (WSPs), with an
emphasis on federal spectrum sharing. The proposed spectrum sharing system
optimizes usage of spectrum resources, manages intra-WSP and inter-WSP
interference and provides essential level of security, privacy, and obfuscation
to enable the most efficient and reliable usage of the shared spectrum. It
features an intermediate spectrum auctioneer responsible for allocating
resources to commercial WSPs by running secure spectrum auctions. The proposed
secure spectrum auction, MTSSA, leverages Paillier cryptosystem to avoid
possible fraud and bid-rigging. Numerical simulations are provided to compare
the performance of MTSSA, in the considered spectrum sharing system, with other
spectrum auction mechanisms for realistic cellular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04903</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04903</id><created>2015-03-16</created><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author></authors><title>Decision-theoretic rough sets based on time-dependent loss function</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental notion of decision-theoretic rough sets is the concept of loss
functions, which provides a powerful tool of calculating a pair of thresholds
for making a decision with a minimum cost. In this paper, time-dependent loss
functions which are variations of the time are of interest because such
functions are frequently encountered in practical situations, we present the
relationship between the pair of thresholds and loss functions satisfying
time-dependent uniform distributions and normal processes in light of bayesian
decision procedure. Subsequently, with the aid of bayesian decision procedure,
we provide the relationship between the pair of thresholds and loss functions
which are time-dependent interval sets and fuzzy numbers. Finally, we employ
several examples to illustrate that how to calculate the thresholds for making
a decision by using time-dependent loss functions-based decision-theoretic
rough sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04904</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04904</id><created>2015-03-16</created><updated>2016-02-03</updated><authors><author><keyname>Lou</keyname><forenames>Youcheng</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author><author><keyname>Wang</keyname><forenames>Shouyang</forenames></author></authors><title>Distributed Continuous-time Approximate Projection Protocols for
  Shortest Distance Optimization Problems</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the distributed shortest distance optimization
problem for a multi-agent network to cooperatively minimize the sum of the
quadratic distances from some convex sets, where each set is only associated
with one agent. To deal with the optimization problem with projection
uncertainties, we propose a distributed continuous-time dynamical protocol
based on a new concept of approximate projection. Here each agent can only
obtain an approximate projection point on the boundary of its convex set, and
communicate with its neighbors over a time-varying communication graph. First,
we show that no matter how large the approximate angle is, the system states
are always bounded for any initial condition, and uniformly bounded with
respect to all initial conditions if the inferior limit of the stepsize is
greater than zero. Then, in the two cases, nonempty intersection and empty
intersection of convex sets, we provide stepsize and approximate angle
conditions to ensure the optimal convergence, respectively. Moreover, we give
some characterizations about the optimal solutions for the empty intersection
case and also present the convergence error between agents' estimates and the
optimal point in the case of constant stepsizes and approximate angles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04906</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04906</id><created>2015-03-16</created><authors><author><keyname>Statman</keyname><forenames>Rick</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>A Finite Model Property for Intersection Types</title><categories>cs.PL cs.LO</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 177, 2015, pp. 1-9</journal-ref><doi>10.4204/EPTCS.177.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the relational theory of intersection types known as BCD has the
finite model property; that is, BCD is complete for its finite models. Our
proof uses rewriting techniques which have as an immediate by-product the
polynomial time decidability of the preorder &lt;= (although this also follows
from the so called beta soundness of BCD).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04907</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04907</id><created>2015-03-16</created><authors><author><keyname>Kikuchi</keyname><forenames>Kentaro</forenames><affiliation>RIEC, Tohoku University, Japan</affiliation></author></authors><title>Uniform Proofs of Normalisation and Approximation for Intersection Types</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 177, 2015, pp. 10-23</journal-ref><doi>10.4204/EPTCS.177.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present intersection type systems in the style of sequent calculus,
modifying the systems that Valentini introduced to prove normalisation
properties without using the reducibility method. Our systems are more natural
than Valentini's ones and equivalent to the usual natural deduction style
systems. We prove the characterisation theorems of strong and weak
normalisation through the proposed systems, and, moreover, the approximation
theorem by means of direct inductive arguments. This provides in a uniform way
proofs of the normalisation and approximation theorems via type systems in
sequent calculus style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04908</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04908</id><created>2015-03-16</created><authors><author><keyname>Pereira</keyname><forenames>M&#xe1;rio</forenames><affiliation>University of Porto, Department of Computer Science &amp; LIACC</affiliation></author><author><keyname>Alves</keyname><forenames>Sandra</forenames><affiliation>University of Porto, Department of Computer Science &amp; LIACC</affiliation></author><author><keyname>Florido</keyname><forenames>M&#xe1;rio</forenames><affiliation>University of Porto, Department of Computer Science &amp; LIACC</affiliation></author></authors><title>Liquid Intersection Types</title><categories>cs.PL</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 177, 2015, pp. 24-42</journal-ref><doi>10.4204/EPTCS.177.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new type system combining refinement types and the
expressiveness of intersection type discipline. The use of such features makes
it possible to derive more precise types than in the original refinement
system. We have been able to prove several interesting properties for our
system (including subject reduction) and developed an inference algorithm,
which we proved to be sound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04909</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04909</id><created>2015-03-16</created><authors><author><keyname>Grellois</keyname><forenames>Charles</forenames><affiliation>Laboratoires PPS and LIAFA, Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Melli&#xe8;s</keyname><forenames>Paul-Andr&#xe9;</forenames><affiliation>Laboratoire PPS, CNRS and Universit&#xe9; Paris Diderot</affiliation></author></authors><title>Indexed linear logic and higher-order model checking</title><categories>cs.LO</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 177, 2015, pp. 43-52</journal-ref><doi>10.4204/EPTCS.177.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, Kobayashi observed that the acceptance by an alternating tree
automaton A of an infinite tree T generated by a higher-order recursion scheme
G may be formulated as the typability of the recursion scheme G in an
appropriate intersection type system associated to the automaton A. The purpose
of this article is to establish a clean connection between this line of work
and Bucciarelli and Ehrhard's indexed linear logic. This is achieved in two
steps. First, we recast Kobayashi's result in an equivalent infinitary
intersection type system where intersection is not idempotent anymore. Then, we
show that the resulting type system is a fragment of an infinitary version of
Bucciarelli and Ehrhard's indexed linear logic. While this work is very
preliminary and does not integrate key ingredients of higher-order
model-checking like priorities, it reveals an interesting and promising
connection between higher-order model-checking and linear logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04910</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04910</id><created>2015-03-16</created><authors><author><keyname>Coppo</keyname><forenames>Mario</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Margaria</keyname><forenames>Ines</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Zacchi</keyname><forenames>Maddalena</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author></authors><title>On Isomorphism of &quot;Functional&quot; Intersection and Union Types</title><categories>cs.LO</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.3.3</acm-class><journal-ref>EPTCS 177, 2015, pp. 53-64</journal-ref><doi>10.4204/EPTCS.177.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Type isomorphism is useful for retrieving library components, since a
function in a library can have a type different from, but isomorphic to, the
one expected by the user. Moreover type isomorphism gives for free the coercion
required to include the function in the user program with the right type. The
present paper faces the problem of type isomorphism in a system with
intersection and union types. In the presence of intersection and union,
isomorphism is not a congruence and cannot be characterised in an equational
way. A characterisation can still be given, quite complicated by the
interference between functional and non functional types. This drawback is
faced in the paper by interpreting each atomic type as the set of functions
mapping any argument into the interpretation of the type itself. This choice
has been suggested by the initial projection of Scott's inverse limit
lambda-model. The main result of this paper is a condition assuring type
isomorphism, based on an isomorphism preserving reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04911</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04911</id><created>2015-03-16</created><authors><author><keyname>Bessai</keyname><forenames>Jan</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>D&#xfc;dder</keyname><forenames>Boris</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>Dudenhefner</keyname><forenames>Andrej</forenames><affiliation>Technical University of Dortmund</affiliation></author><author><keyname>Chen</keyname><forenames>Tzu-Chun</forenames><affiliation>Technical University of Darmstadt</affiliation></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames><affiliation>University of Torino</affiliation></author></authors><title>Typing Classes and Mixins with Intersection Types</title><categories>cs.PL cs.LO cs.SE</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><acm-class>D.3.3; F.4.1</acm-class><journal-ref>EPTCS 177, 2015, pp. 79-93</journal-ref><doi>10.4204/EPTCS.177.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an assignment system of intersection types for a lambda-calculus
with records and a record-merge operator, where types are preserved both under
subject reduction and expansion. The calculus is expressive enough to naturally
represent mixins as functions over recursively defined classes, whose fixed
points, the objects, are recursive records. In spite of the double recursion
that is involved in their definition, classes and mixins can be meaningfully
typed without resorting to neither recursive nor F-bounded polymorphic types.
  We then adapt mixin construct and composition to Java and C#, relying solely
on existing features in such a way that the resulting code remains typable in
the respective type systems. We exhibit some example code, and study its
typings in the intersection type system via interpretation into the
lambda-calculus with records we have proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04912</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04912</id><created>2015-03-16</created><authors><author><keyname>Cassar</keyname><forenames>Ian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Francalanza</keyname><forenames>Adrian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Said</keyname><forenames>Simon</forenames><affiliation>University of Malta</affiliation></author></authors><title>Improving Runtime Overheads for detectEr</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2015, arXiv:1503.04378</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 178, 2015, pp. 1-8</journal-ref><doi>10.4204/EPTCS.178.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design monitor optimisations for detectEr, a runtime-verification tool
synthesising systems of concurrent monitors from correctness properties for
Erlang programs. We implement these optimisations as part of the existing tool
and show that they yield considerably lower runtime overheads when compared to
the unoptimised monitor synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04913</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04913</id><created>2015-03-16</created><authors><author><keyname>J&#xe4;hnig</keyname><forenames>Nils</forenames><affiliation>TU Berlin</affiliation></author><author><keyname>G&#xf6;thel</keyname><forenames>Thomas</forenames><affiliation>TU Berlin</affiliation></author><author><keyname>Glesner</keyname><forenames>Sabine</forenames><affiliation>TU Berlin</affiliation></author></authors><title>A Denotational Semantics for Communicating Unstructured Code</title><categories>cs.PL cs.LO</categories><comments>In Proceedings FESCA 2015, arXiv:1503.04378</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 178, 2015, pp. 9-21</journal-ref><doi>10.4204/EPTCS.178.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important property of programming language semantics is that they should
be compositional. However, unstructured low-level code contains goto-like
commands making it hard to define a semantics that is compositional. In this
paper, we follow the ideas of Saabas and Uustalu to structure low-level code.
This gives us the possibility to define a compositional denotational semantics
based on least fixed points to allow for the use of inductive verification
methods. We capture the semantics of communication using finite traces similar
to the denotations of CSP. In addition, we examine properties of this semantics
and give an example that demonstrates reasoning about communication and jumps.
With this semantics, we lay the foundations for a proof calculus that captures
both, the semantics of unstructured low-level code and communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04914</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04914</id><created>2015-03-16</created><authors><author><keyname>Riener</keyname><forenames>Heinz</forenames></author><author><keyname>Ehlers</keyname><forenames>R&#xfc;diger</forenames></author><author><keyname>Fey</keyname><forenames>G&#xf6;rschwin</forenames></author></authors><title>Path-Based Program Repair</title><categories>cs.PL cs.SE</categories><comments>In Proceedings FESCA 2015, arXiv:1503.04378</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 178, 2015, pp. 22-32</journal-ref><doi>10.4204/EPTCS.178.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a path-based approach to program repair for imperative programs.
Our repair framework takes as input a faulty program, a logic specification
that is refuted, and a hint where the fault may be located. An iterative
abstraction refinement loop is then used to repair the program: in each
iteration, the faulty program part is re-synthesized considering a symbolic
counterexample, where the control-flow is kept concrete but the data-flow is
symbolic. The appeal of the idea is two-fold: 1) the approach lazily considers
candidate repairs and 2) the repairs are directly derived from the logic
specification. In contrast to prior work, our approach is complete for programs
with finitely many control-flow paths, i.e., the program is repaired if and
only if it can be repaired at the specified fault location. Initial results for
small programs indicate that the approach is useful for debugging programs in
practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04915</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04915</id><created>2015-03-17</created><authors><author><keyname>Hufflen</keyname><forenames>Jean-Michel</forenames><affiliation>FEMTO-ST &amp; University of Franche-Comt&#xe9;</affiliation></author></authors><title>Using Model-Checking Techniques for Component-Based Systems with
  Reconfigurations</title><categories>cs.SE cs.DS cs.FL</categories><comments>In Proceedings FESCA 2015, arXiv:1503.04378</comments><proxy>EPTCS</proxy><acm-class>C.1.3; D.2.4; D.2.11</acm-class><journal-ref>EPTCS 178, 2015, pp. 33-46</journal-ref><doi>10.4204/EPTCS.178.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within a component-based approach allowing dynamic reconfigurations,
sequences of successive reconfiguration operations are expressed by means of
reconfiguration paths, possibly infinite. We show that a subclass of such paths
can be modelled by finite state automata. This feature allows us to use
techniques related to model-checking to prove some architectural, event, and
temporal properties related to dynamic reconfiguration. Our method is proved
correct w.r.t. these properties' definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04916</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04916</id><created>2015-03-17</created><authors><author><keyname>Marmsoler</keyname><forenames>Diego</forenames><affiliation>Technische Universitaet Muenchen</affiliation></author><author><keyname>Malkis</keyname><forenames>Alexander</forenames><affiliation>Technische Universitaet Muenchen</affiliation></author><author><keyname>Eckhardt</keyname><forenames>Jonas</forenames><affiliation>Technische Universitaet Muenchen</affiliation></author></authors><title>A Model of Layered Architectures</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2015, arXiv:1503.04378</comments><proxy>EPTCS</proxy><acm-class>D.2.11 [Software Engineering]: Software Architectures | Patterns</acm-class><journal-ref>EPTCS 178, 2015, pp. 47-61</journal-ref><doi>10.4204/EPTCS.178.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Architectural styles and patterns play an important role in software
engineering. One of the most known ones is the layered architecture style.
However, this style is usually only stated informally, which may cause problems
such as ambiguity, wrong conclusions, and difficulty when checking the
conformance of a system to the style. We address these problems by providing a
formal, denotational semantics of the layered architecture style. Mainly, we
present a sufficiently abstract and rigorous description of layered
architectures. Loosely speaking, a layered architecture consists of a hierarchy
of layers, in which services communicate via ports. A layer is modeled as a
relation between used and provided services, and layer composition is defined
by means of relational composition. Furthermore, we provide a formal definition
for the notions of syntactic and semantic dependency between the layers. We
show that these dependencies are not comparable in general. Moreover, we
identify sufficient conditions under which, in an intuitive sense which we make
precise in our treatment, the semantic dependency implies, is implied by, or
even coincides with the reflexive-transitive closure of the syntactic
dependency. Our results provide a technology-independent characterization of
the layered architecture style, which may be used by software architects to
ensure that a system is indeed built according to that style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04917</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04917</id><created>2015-03-17</created><authors><author><keyname>Koutsoumpas</keyname><forenames>Vasileios</forenames><affiliation>TUM</affiliation></author></authors><title>A Formal Approach based on Fuzzy Logic for the Specification of
  Component-Based Interactive Systems</title><categories>cs.SE cs.LO</categories><comments>In Proceedings FESCA 2015, arXiv:1503.04378</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 178, 2015, pp. 62-76</journal-ref><doi>10.4204/EPTCS.178.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal methods are widely recognized as a powerful engineering method for the
specification, simulation, development, and verification of distributed
interactive systems. However, most formal methods rely on a two-valued logic,
and are therefore limited to the axioms of that logic: a specification is valid
or invalid, component behavior is realizable or not, safety properties hold or
are violated, systems are available or unavailable. Especially when the problem
domain entails uncertainty, impreciseness, and vagueness, the appliance of such
methods becomes a challenging task. In order to overcome the limitations
resulting from the strict modus operandi of formal methods, the main objective
of this work is to relax the boolean notion of formal specifications by using
fuzzy logic. The present approach is based on Focus theory, a model-based and
strictly formal method for componentbased interactive systems. The contribution
of this work is twofold: i) we introduce a specification technique based on
fuzzy logic which can be used on top of Focus to develop formal specifications
in a qualitative fashion; ii) we partially extend Focus theory to a fuzzy one
which allows the specification of fuzzy components and fuzzy interactions.
While the former provides a methodology for approximating I/O behaviors under
imprecision, the latter enables to capture a more quantitative view of
specification properties such as realizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04918</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04918</id><created>2015-03-17</created><authors><author><keyname>Benke</keyname><forenames>Marcin</forenames></author><author><keyname>Bono</keyname><forenames>Viviana</forenames></author><author><keyname>Schubert</keyname><forenames>Aleksy</forenames></author></authors><title>Lucretia - intersection type polymorphism for scripting languages</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ITRS 2014, arXiv:1503.04377</comments><proxy>EPTCS</proxy><acm-class>F.3.3; F.4.1</acm-class><journal-ref>EPTCS 177, 2015, pp. 65-78</journal-ref><doi>10.4204/EPTCS.177.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scripting code may present maintenance problems in the long run. There is,
then, the call for methodologies that make it possible to control the
properties of programs written in dynamic languages in an automatic fashion. We
introduce Lucretia, a core language with an introspection primitive. Lucretia
is equipped with a (retrofitted) static type system based on local updates of
types that describe the structure of objects being used. In this way, we deal
with one of the most dynamic features of scripting languages, that is, the
runtime modification of object interfaces. Judgements in our systems have a
Hoare-like shape, as they have a precondition and a postcondition part.
Preconditions describe static approximations of the interfaces of visible
objects before a certain expression has been executed and postconditions
describe them after its execution. The field update operation complicates the
issue of aliasing in the system. We cope with it by introducing intersection
types in method signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04921</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04921</id><created>2015-03-17</created><updated>2015-03-19</updated><authors><author><keyname>Lee</keyname><forenames>Changmin</forenames></author><author><keyname>Koo</keyname><forenames>Bonhong</forenames></author><author><keyname>Kim</keyname><forenames>Na-Rae</forenames></author><author><keyname>Yilmaz</keyname><forenames>Birkan</forenames></author><author><keyname>Farsard</keyname><forenames>Nariman</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author></authors><title>Molecular MIMO Communication Link</title><categories>cs.ET</categories><comments>2 pages, 3 figures, accepted to present on 2015 INFOCOM conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this demonstration, we will present the world's first molecular
multiple-input multiple-output (MIMO) communication link to deliver two data
streams in a spatial domain. We show that chemical signals such as
concentration gradients could be used in MIMO fashion to transfer sequential
data. Until now it was unclear whether MIMO techniques, which are used
extensively in modern radio communication, could be applied to molecular
communication. In the demonstration, using our devised MIMO apparatus and
carefully designed detection algorithm, we will show that we can achieve about
1.7 times higher data rate than single input single output (SISO) molecular
communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04927</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04927</id><created>2015-03-17</created><authors><author><keyname>Hu</keyname><forenames>Qingbo</forenames></author><author><keyname>Xie</keyname><forenames>Sihong</forenames></author><author><keyname>Lin</keyname><forenames>Shuyang</forenames></author><author><keyname>Wang</keyname><forenames>Senzhang</forenames></author><author><keyname>Yu</keyname><forenames>Philip</forenames></author></authors><title>CENI: a Hybrid Framework for Efficiently Inferring Information Networks</title><categories>cs.SI</categories><comments>Full-length version of the paper with the same title published in
  ICWSM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the message diffusion links among users or websites drive the
development of countless innovative applications. However, in reality, it is
easier for us to observe the timestamps when different nodes in the network
react on a message, while the connections empowering the diffusion of the
message remain hidden. This motivates recent extensive studies on the network
inference problem: unveiling the edges from the records of messages
disseminated through them. Existing solutions are computationally expensive,
which motivates us to develop an efficient two-step general framework,
Clustering Embedded Network Inference (CENI). CENI integrates clustering
strategies to improve the efficiency of network inference. By clustering nodes
directly on the timelines of messages, we propose two naive implementations of
CENI: Infection-centric CENI and Cascade-centric CENI. Additionally, we point
out the critical dimension problem of CENI: instead of one-dimensional
timelines, we need to first project the nodes to an Euclidean space of certain
dimension before clustering. A CENI adopting clustering method on the projected
space can better preserve the structure hidden in the cascades, and generate
more accurately inferred links. This insight sheds light on other related work
attempting to discover or utilize the latent cluster structure in the
disseminated messages. By addressing the critical dimension problem, we propose
the third implementation of the CENI framework: Projection-based CENI. Through
extensive experiments on two real datasets, we show that the three CENI models
only need around 20% $\sim$ 50% of the running time of state-of-the-art
methods. Moreover, the inferred edges of Projection-based CENI preserves or
even outperforms the effectiveness of state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04928</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04928</id><created>2015-03-17</created><authors><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames></author></authors><title>Hybrid Automata for Formal Modeling and Verification of Cyber-Physical
  Systems</title><categories>cs.LO cs.FL</categories><comments>17 pages</comments><journal-ref>Journal of Indian Institute of Science, Special Issue on Cyber
  Physical Systems, vol. 93 (3), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of a tight integration between the discrete control (the
&quot;cyber&quot;) and the analog environment (the &quot;physical&quot;)---via sensors and
actuators over wired or wireless communication networks---is the defining
feature of cyber-physical systems. Hence, the functional correctness of a
cyber- physical system is crucially dependent not only on the dynamics of the
analog physical environment, but also on the decisions taken by the discrete
control that alter the dynamics of the environment. The framework of Hybrid
automata---introduced by Alur, Courcoubetis, Henzinger, and Ho---provides a
formal modeling and specification environment to analyze the interaction
between the discrete and continuous parts of a cyber-physical system. Hybrid
automata can be considered as generalizations of finite state automata
augmented with a finite set of real-valued variables whose dynamics in each
state is governed by a system of ordinary differential equations. Moreover, the
discrete transitions of hybrid automata are guarded by constraints over the
values of these real-valued variables, and enable discontinuous jumps in the
evolution of these variables. Considering the richness of the dynamics in a
hybrid automaton, it is perhaps not surprising that the fundamental
verification questions, like reachability and schedulability, for the general
model are undecidable. In this article we present a review of hybrid automata
as modeling and verification framework for cyber-physical systems, and survey
some of the key results related to practical verification questions related to
hybrid automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04937</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04937</id><created>2015-03-17</created><authors><author><keyname>Ray</keyname><forenames>Kisor</forenames></author><author><keyname>Sarkar</keyname><forenames>Saumen</forenames></author></authors><title>Interactive MCQs as a tool for Knowledge Acquisition</title><categories>cs.CY</categories><comments>04 pages, 02 tables, 02 figures, International Journal of Advanced
  Research in Computer Science and Software Engineering, Volume 5, Issue 1,
  January 2015 ISSN: 2277 128X</comments><acm-class>K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Choice Questions or MCQs are very important for e-learning.
Generally, MCQs are used as a tool for the assessment of student performance at
the end of their learning sessions. Can MCQs become an important tool in the
process of knowledge acquisition while attending a course? This paper intends
to find out how MCQs could be used as a tool for the better understanding,
coverage as well as knowledge acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04941</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04941</id><created>2015-03-17</created><authors><author><keyname>van Hateren</keyname><forenames>J. H.</forenames></author></authors><title>How the symbol grounding of living organisms can be realized in
  artificial agents</title><categories>cs.AI cs.NE cs.RO</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system with artificial intelligence usually relies on symbol manipulation,
at least partly and implicitly. However, the interpretation of the symbols -
what they represent and what they are about - is ultimately left to humans, as
designers and users of the system. How symbols can acquire meaning for the
system itself, independent of external interpretation, is an unsolved problem.
Some grounding of symbols can be obtained by embodiment, that is, by causally
connecting symbols (or sub-symbolic variables) to the physical environment,
such as in a robot with sensors and effectors. However, a causal connection as
such does not produce representation and aboutness of the kind that symbols
have for humans. Here I present a theory that explains how humans and other
living organisms have acquired the capability to have symbols and sub-symbolic
variables that represent, refer to, and are about something else. The theory
shows how reference can be to physical objects, but also to abstract objects,
and even how it can be misguided (errors in reference) or be about non-existing
objects. I subsequently abstract the primary components of the theory from
their biological context, and discuss how and under what conditions the theory
could be implemented in artificial agents. A major component of the theory is
the strong nonlinearity associated with (potentially unlimited)
self-reproduction. The latter is likely not acceptable in artificial systems.
It remains unclear if goals other than those inherently serving
self-reproduction can have aboutness and if such goals could be stabilized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04949</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04949</id><created>2015-03-17</created><updated>2015-11-25</updated><authors><author><keyname>Jampani</keyname><forenames>Varun</forenames></author><author><keyname>Kiefel</keyname><forenames>Martin</forenames></author><author><keyname>Gehler</keyname><forenames>Peter V.</forenames></author></authors><title>Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs
  and Bilateral Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilateral filters have wide spread use due to their edge-preserving
properties. The common use case is to manually choose a parametric filter type,
usually a Gaussian filter. In this paper, we will generalize the
parametrization and in particular derive a gradient descent algorithm so the
filter parameters can be learned from data. This derivation allows to learn
high dimensional linear filters that operate in sparsely populated feature
spaces. We build on the permutohedral lattice construction for efficient
filtering. The ability to learn more general forms of high-dimensional filters
can be used in several diverse applications. First, we demonstrate the use in
applications where single filter applications are desired for runtime reasons.
Further, we show how this algorithm can be used to learn the pairwise
potentials in densely connected conditional random fields and apply these to
different image segmentation tasks. Finally, we introduce layers of bilateral
filters in CNNs and propose bilateral neural networks for the use of
high-dimensional sparse data. This view provides new ways to encode model
structure into network architectures. A diverse set of experiments empirically
validates the usage of general forms of filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04955</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04955</id><created>2015-03-17</created><authors><author><keyname>L&#xfc;ders</keyname><forenames>Christoph</forenames></author></authors><title>Fast Multiplication of Large Integers: Implementation and Analysis of
  the DKSS Algorithm</title><categories>cs.MS</categories><comments>Diploma Thesis, Universit\&quot;at Bonn</comments><acm-class>G.1.0; G.4; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sch\&quot;onhage-Strassen algorithm (SSA) is the de-facto standard for
multiplication of large integers. For $N$-bit numbers it has a time bound of
$O(N \cdot \log N \cdot \log \log N)$. De, Kurur, Saha and Saptharishi (DKSS)
presented an asymptotically faster algorithm with a better time bound of $N
\cdot \log N \cdot 2^{O(\log^* N)}$. In this diploma thesis, results of an
implementation of DKSS multiplication are presented: run-time is about 30 times
larger than SSA, while memory requirements are about 3.75 times higher than
SSA. A possible crossover point is estimated to be out of reach even if we
utilized the whole universe for computer memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04957</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04957</id><created>2015-03-17</created><authors><author><keyname>Burattin</keyname><forenames>Andrea</forenames></author><author><keyname>Maggi</keyname><forenames>Fabrizio Maria</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author></authors><title>Conformance Checking Based on Multi-Perspective Declarative Process
  Models</title><categories>cs.SE cs.DB cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process mining is a family of techniques that aim at analyzing business
process execution data recorded in event logs. Conformance checking is a branch
of this discipline embracing approaches for verifying whether the behavior of a
process, as recorded in a log, is in line with some expected behaviors provided
in the form of a process model. The majority of these approaches require the
input process model to be procedural (e.g., a Petri net). However, in turbulent
environments, characterized by high variability, the process behavior is less
stable and predictable. In these environments, procedural process models are
less suitable to describe a business process. Declarative specifications,
working in an open world assumption, allow the modeler to express several
possible execution paths as a compact set of constraints. Any process execution
that does not contradict these constraints is allowed. One of the open
challenges in the context of conformance checking with declarative models is
the capability of supporting multi-perspective specifications. In this paper,
we close this gap by providing a framework for conformance checking based on
MP-Declare, a multi-perspective version of the declarative process modeling
language Declare. The approach has been implemented in the process mining tool
ProM and has been experimented in three real life case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04958</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04958</id><created>2015-03-17</created><authors><author><keyname>Gorbachev</keyname><forenames>V. N.</forenames></author><author><keyname>Kaynarova</keyname><forenames>E. M.</forenames></author><author><keyname>Metelev</keyname><forenames>I. K.</forenames></author><author><keyname>Pavlovskaya</keyname><forenames>O. V.</forenames></author></authors><title>The blind detection for palette image watermarking without changing the
  color</title><categories>cs.MM</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To hide a binary pattern in the palette image a steganographic scheme with
blind detection is considered. The embedding algorithm uses the Lehmer code by
palette color permutations for which the cover image palette is generally
required. The found transformation between the palette and RGB images allows to
extract the hidden data without any cover work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04963</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04963</id><created>2015-03-17</created><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Korhonen</keyname><forenames>Janne H.</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Paz</keyname><forenames>Ami</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Algebraic Methods in the Congested Clique</title><categories>cs.DC cs.DS</categories><comments>This is work is a merger of arxiv:1412.2109 and arxiv:1412.2667</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we use algebraic methods for studying distance computation and
subgraph detection tasks in the congested clique model. Specifically, we adapt
parallel matrix multiplication implementations to the congested clique,
obtaining an $O(n^{1-2/\omega})$ round matrix multiplication algorithm, where
$\omega &lt; 2.3728639$ is the exponent of matrix multiplication. In conjunction
with known techniques from centralised algorithmics, this gives significant
improvements over previous best upper bounds in the congested clique model. The
highlight results include:
  -- triangle and 4-cycle counting in $O(n^{0.158})$ rounds, improving upon the
$O(n^{1/3})$ triangle detection algorithm of Dolev et al. [DISC 2012],
  -- a $(1 + o(1))$-approximation of all-pairs shortest paths in $O(n^{0.158})$
rounds, improving upon the $\tilde{O} (n^{1/2})$-round $(2 +
o(1))$-approximation algorithm of Nanongkai [STOC 2014], and
  -- computing the girth in $O(n^{0.158})$ rounds, which is the first
non-trivial solution in this model.
  In addition, we present a novel constant-round combinatorial algorithm for
detecting 4-cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04964</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04964</id><created>2015-03-17</created><authors><author><keyname>Padakandla</keyname><forenames>Sindhu</forenames></author><author><keyname>J</keyname><forenames>Prabuchandran K.</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Energy Sharing for Multiple Sensor Nodes with Finite Buffers</title><categories>cs.NI cs.LG</categories><comments>38 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding optimal energy sharing policies that
maximize the network performance of a system comprising of multiple sensor
nodes and a single energy harvesting (EH) source. Sensor nodes periodically
sense the random field and generate data, which is stored in the corresponding
data queues. The EH source harnesses energy from ambient energy sources and the
generated energy is stored in an energy buffer. Sensor nodes receive energy for
data transmission from the EH source. The EH source has to efficiently share
the stored energy among the nodes in order to minimize the long-run average
delay in data transmission. We formulate the problem of energy sharing between
the nodes in the framework of average cost infinite-horizon Markov decision
processes (MDPs). We develop efficient energy sharing algorithms, namely
Q-learning algorithm with exploration mechanisms based on the $\epsilon$-greedy
method as well as upper confidence bound (UCB). We extend these algorithms by
incorporating state and action space aggregation to tackle state-action space
explosion in the MDP. We also develop a cross entropy based method that
incorporates policy parameterization in order to find near optimal energy
sharing policies. Through simulations, we show that our algorithms yield energy
sharing policies that outperform the heuristic greedy method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04967</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04967</id><created>2015-03-17</created><authors><author><keyname>Profanter</keyname><forenames>Stefan</forenames></author></authors><title>Implementation and Evaluation of multimodal input/output channels for
  task-based industrial robot programming</title><categories>cs.RO cs.HC</categories><comments>Master Thesis in Robotics, Cognition, Intelligence</comments><doi>10.13140/2.1.1044.0008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming industrial robots is not very intuitive, and the programmer has
to be a domain expert for e.g. welding and programming to know how the task is
optimally executed. For SMEs such employees are not affordable, nor
cost-effective. Therefore a new system is needed where domain experts from a
specific area, like welding or assembly, can easily program a robot without
knowing anything about programming languages or how to use TeachPads. Such a
system needs to be flexible to adapt to new tasks and functions. These
requirements can be met by using a task based programming approach where the
robot program is built up using a hierarchical structure of process, tasks and
skills. It also needs to be intuitive so that domain experts don't need much
training time on handling the system. Intuitive interaction is achieved by
using different input and output modalities like gesture input, speech input,
or touch input which are suitable for the current task.
  This master thesis focuses on the implementation of a user interface (GUI)
for task based industrial robot programming and evaluates different input
modalities (gesture, speech, touch, pen input) for the interaction with the
system. The evaluation is based on a user study conducted with 30 participants
as a Wizard-Of-Oz experiment, where non expert users had to program assembly
and welding tasks to an industrial robot, using the previously developed GUI
and various input and output modalities.
  The findings of the task analysis and user study are then used for creating a
semantic description which will be used in the cognitive robotics-worker cell
for automatically inferring required system components, and to provide the best
suited input modality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04973</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04973</id><created>2015-03-17</created><authors><author><keyname>Sigg</keyname><forenames>Stephan</forenames></author><author><keyname>Kunze</keyname><forenames>Kai</forenames></author><author><keyname>Fu</keyname><forenames>Xiaoming</forenames></author></authors><title>Recent Advances and Challenges in Ubiquitous Sensing</title><categories>cs.HC</categories><comments>Submitted to PIEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous sensing is tightly coupled with activity recognition. This survey
reviews recent advances in Ubiquitous sensing and looks ahead on promising
future directions. In particular, Ubiquitous sensing crosses new barriers
giving us new ways to interact with the environment or to inspect our psyche.
Through sensing paradigms that parasitically utilise stimuli from the noise of
environmental, third-party pre-installed systems, sensing leaves the boundaries
of the personal domain. Compared to previous environmental sensing approaches,
these new systems mitigate high installation and placement cost by providing a
robustness towards process noise. On the other hand, sensing focuses inward and
attempts to capture mental activities such as cognitive load, fatigue or
emotion through advances in, for instance, eye-gaze sensing systems or
interpretation of body gesture or pose. This survey summarises these
developments and discusses current research questions and promising future
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04988</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04988</id><created>2015-03-17</created><updated>2015-03-18</updated><authors><author><keyname>Sackman</keyname><forenames>Matthew</forenames></author></authors><title>Perfect Consistent Hashing</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Consistent Hashing functions are widely used for load balancing across a
variety of applications. However, the original presentation and typical
implementations of Consistent Hashing rely on randomised allocation of hash
codes to keys which results in a flawed and approximately-uniform allocation of
keys to hash codes. We analyse the desired properties and present an algorithm
that perfectly achieves them without resorting to any random distributions. The
algorithm is simple and adds to our understanding of what is necessary to
create a consistent hash function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04990</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04990</id><created>2015-03-17</created><updated>2015-03-30</updated><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Bruckdorfer</keyname><forenames>Till</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Raftopoulou</keyname><forenames>Chrysanthi N.</forenames></author></authors><title>The Book Thickness of 1-Planar Graphs is Constant</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a book embedding, the vertices of a graph are placed on the spine of a
book and the edges are assigned to pages, so that edges on the same page do not
cross. In this paper, we prove that every $1$-planar graph (that is, a graph
that can be drawn on the plane such that no edge is crossed more than once)
admits an embedding in a book with constant number of pages. To the best of our
knowledge, the best non-trivial previous upper-bound is $O(\sqrt{n})$, where
$n$ is the number of vertices of the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04991</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04991</id><created>2015-03-17</created><authors><author><keyname>Ferrari</keyname><forenames>Luca</forenames></author></authors><title>Dyck algebras, interval temporal logic and posets of intervals</title><categories>math.CO cs.DM cs.LO math.LO</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a natural Heyting algebra structure on the set of Dyck paths
of the same length. We provide a geometrical description of the operations of
pseudocomplement and relative pseudocomplement, as well as of regular elements.
We also find a logic-theoretic interpretation of such Heyting algebras, which
we call Dyck algebras, by showing that they are the algebraic counterpart of a
certain fragment of a classical interval temporal logic (also known as
Halpern-Shoham logic). Finally, we propose a generalization of our approach,
suggesting a similar study of the Heyting algebra arising from the poset of
intervals of a finite poset using Birkh\&quot;off duality. In order to illustrate
this, we show how several combinatorial parameters of Dyck paths can be
expressed in terms of the Heyting algebra structure of Dyck algebras together
with a certain total order on the set of atoms of each Dyck algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04994</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04994</id><created>2015-03-17</created><authors><author><keyname>Teslenko</keyname><forenames>Maxim</forenames></author><author><keyname>Dubrova</keyname><forenames>Elena</forenames></author></authors><title>A Linear-Time Algorithm for Finding All Double-Vertex Dominators of a
  Given Vertex</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dominators provide a general mechanism for identifying reconverging paths in
graphs. This is useful for a number of applications in Computer-Aided Design
(CAD) including signal probability computation in biased random simulation,
switching activity estimation in power and noise analysis, and cut points
identification in equivalence checking. However, traditional single-vertex
dominators are too rare in circuit graphs. In order to handle reconverging
paths more efficiently, we consider the case of double-vertex dominators which
occur more frequently. First, we derive a number of specific properties of
double-vertex dominators. Then, we describe a data structure for representing
all double-vertex dominators of a given vertex in linear space. Finally, we
present an algorithm for finding all double-vertex dominators of a given vertex
in linear time. Our results provide an efficient systematic way of partitioning
large graphs along the reconverging points of the signal flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04996</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04996</id><created>2015-03-17</created><authors><author><keyname>Fawagreh</keyname><forenames>Khaled</forenames></author><author><keyname>Gaber</keyname><forenames>Mohamad Medhat</forenames></author><author><keyname>Elyan</keyname><forenames>Eyad</forenames></author></authors><title>On Extreme Pruning of Random Forest Ensembles for Real-time Predictive
  Applications</title><categories>cs.LG</categories><comments>10 pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Forest (RF) is an ensemble supervised machine learning technique that
was developed by Breiman over a decade ago. Compared with other ensemble
techniques, it has proved its accuracy and superiority. Many researchers,
however, believe that there is still room for enhancing and improving its
performance accuracy. This explains why, over the past decade, there have been
many extensions of RF where each extension employed a variety of techniques and
strategies to improve certain aspect(s) of RF. Since it has been proven
empiricallthat ensembles tend to yield better results when there is a
significant diversity among the constituent models, the objective of this paper
is twofold. First, it investigates how data clustering (a well known diversity
technique) can be applied to identify groups of similar decision trees in an RF
in order to eliminate redundant trees by selecting a representative from each
group (cluster). Second, these likely diverse representatives are then used to
produce an extension of RF termed CLUB-DRF that is much smaller in size than
RF, and yet performs at least as good as RF, and mostly exhibits higher
performance in terms of accuracy. The latter refers to a known technique called
ensemble pruning. Experimental results on 15 real datasets from the UCI
repository prove the superiority of our proposed extension over the traditional
RF. Most of our experiments achieved at least 95% or above pruning level while
retaining or outperforming the RF accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.04999</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.04999</id><created>2015-03-17</created><authors><author><keyname>Ren</keyname><forenames>Xiaoqiang</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Shi</keyname><forenames>Dawei</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Quickest Change Detection in Adaptive Censoring Sensor Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of quickest change detection with communication rate constraints
is studied. A network of wireless sensors with limited computation capability
monitors the environment and sends observations to a fusion center via wireless
channels. At an unknown time instant, the distributions of observations at all
the sensor nodes change simultaneously. Due to limited communication bandwidth,
the sensors cannot transmit at all the time instants. The objective is to
detect the change at the fusion center as quickly as possible, subject to
constraints on false detection and average communication rate between the
sensors and the fusion center. Two minimax formulations are proposed. The
cumulative sum (CuSum) algorithm is used at the fusion center and censoring
strategies adaptive to the CuSum statistic are used at the sensor nodes. The
sensors only send observations that fall into prescribed sets to the fusion
center. This CuSum adaptive censoring (CuSum-AC) algorithm is proved to be an
equalizer rule for Lorden's criterion and to be globally asymptotically optimal
for any positive communication rate constraint for both formulations we
propose, as the average run length to false alarm goes to infinity. It is also
shown, by numerical examples, that the CuSum-AC algorithm has a good trade-off
between the detection performance and the communication rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05018</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05018</id><created>2015-03-17</created><authors><author><keyname>Wistuba</keyname><forenames>Martin</forenames></author><author><keyname>Grabocka</keyname><forenames>Josif</forenames></author><author><keyname>Schmidt-Thieme</keyname><forenames>Lars</forenames></author></authors><title>Ultra-Fast Shapelets for Time Series Classification</title><categories>cs.LG</categories><comments>Preprint submitted to Journal of Data &amp; Knowledge Engineering January
  24, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series shapelets are discriminative subsequences and their similarity to
a time series can be used for time series classification. Since the discovery
of time series shapelets is costly in terms of time, the applicability on long
or multivariate time series is difficult. In this work we propose Ultra-Fast
Shapelets that uses a number of random shapelets. It is shown that Ultra-Fast
Shapelets yield the same prediction quality as current state-of-the-art
shapelet-based time series classifiers that carefully select the shapelets by
being by up to three orders of magnitudes. Since this method allows a
ultra-fast shapelet discovery, using shapelets for long multivariate time
series classification becomes feasible.
  A method for using shapelets for multivariate time series is proposed and
Ultra-Fast Shapelets is proven to be successful in comparison to
state-of-the-art multivariate time series classifiers on 15 multivariate time
series datasets from various domains. Finally, time series derivatives that
have proven to be useful for other time series classifiers are investigated for
the shapelet-based classifiers. It is shown that they have a positive impact
and that they are easy to integrate with a simple preprocessing step, without
the need of adapting the shapelet discovery algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05025</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05025</id><created>2015-03-17</created><authors><author><keyname>Hoyrup</keyname><forenames>Mathieu</forenames></author></authors><title>A Rice-like theorem for primitive recursive functions</title><categories>cs.LO</categories><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an explicit characterization of the properties of primitive
recursive functions that are decidable or semi-decidable, given a primitive
recursive index for the function. The result is much more general as it applies
to any c.e. class of total computable functions. This is an analog of Rice and
Rice-Shapiro theorem, for restricted classes of total computable functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05032</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05032</id><created>2015-03-17</created><updated>2015-04-09</updated><authors><author><keyname>Liu</keyname><forenames>Weifeng</forenames></author><author><keyname>Vinter</keyname><forenames>Brian</forenames></author></authors><title>CSR5: An Efficient Storage Format for Cross-Platform Sparse
  Matrix-Vector Multiplication</title><categories>cs.MS cs.DC math.NA</categories><comments>12 pages, 10 figures, In Proceedings of the 29th ACM International
  Conference on Supercomputing (ICS '15)</comments><msc-class>65F50</msc-class><acm-class>G.4; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse matrix-vector multiplication (SpMV) is a fundamental building block
for numerous applications. In this paper, we propose CSR5 (Compressed Sparse
Row 5), a new storage format, which offers high-throughput SpMV on various
platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is
insensitive to the sparsity structure of the input matrix. Thus the single
format can support an SpMV algorithm that is efficient both for regular
matrices and for irregular matrices. Furthermore, we show that the overhead of
the format conversion from the CSR to the CSR5 can be as low as the cost of a
few SpMV operations. We compare the CSR5-based SpMV algorithm with 11
state-of-the-art formats and algorithms on four mainstream processors using 14
regular and 10 irregular matrices as a benchmark suite. For the 14 regular
matrices in the suite, we achieve comparable or better performance over the
previous work. For the 10 irregular matrices, the CSR5 obtains average
performance improvement of 17.6\%, 28.5\%, 173.0\% and 293.3\% (up to 213.3\%,
153.6\%, 405.1\% and 943.3\%) over the best existing work on dual-socket Intel
CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For
real-world applications such as a solver with only tens of iterations, the CSR5
format can be more practical because of its low-overhead for format conversion.
The source code of this work is downloadable at
https://github.com/bhSPARSE/Benchmark_SpMV_using_CSR5
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05034</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05034</id><created>2015-03-17</created><updated>2015-04-24</updated><authors><author><keyname>Wang</keyname><forenames>Mingxuan</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Jiang</keyname><forenames>Wenbin</forenames></author><author><keyname>Liu</keyname><forenames>Qun</forenames></author></authors><title>$gen$CNN: A Convolutional Architecture for Word Sequence Prediction</title><categories>cs.CL</categories><comments>Accepted by ACL as full paper(oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel convolutional architecture, named $gen$CNN, for word
sequence prediction. Different from previous work on neural network-based
language modeling and generation (e.g., RNN or LSTM), we choose not to greedily
summarize the history of words as a fixed length vector. Instead, we use a
convolutional neural network to predict the next word with the history of words
of variable length. Also different from the existing feedforward networks for
language modeling, our model can effectively fuse the local correlation and
global correlation in the word sequence, with a convolution-gating strategy
specifically designed for the task. We argue that our model can give adequate
representation of the history, and therefore can naturally exploit both the
short and long range dependencies. Our model is fast, easy to train, and
readily parallelized. Our extensive experiments on text generation and $n$-best
re-ranking in machine translation show that $gen$CNN outperforms the
state-of-the-arts with big margins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05038</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05038</id><created>2015-03-17</created><authors><author><keyname>Pepik</keyname><forenames>Bojan</forenames></author><author><keyname>Stark</keyname><forenames>Michael</forenames></author><author><keyname>Gehler</keyname><forenames>Peter</forenames></author><author><keyname>Ritschel</keyname><forenames>Tobias</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>3D Object Class Detection in the Wild</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object class detection has been a synonym for 2D bounding box localization
for the longest time, fueled by the success of powerful statistical learning
techniques, combined with robust image representations. Only recently, there
has been a growing interest in revisiting the promise of computer vision from
the early days: to precisely delineate the contents of a visual scene, object
by object, in 3D. In this paper, we draw from recent advances in object
detection and 2D-3D object lifting in order to design an object class detector
that is particularly tailored towards 3D object class detection. Our 3D object
class detection method consists of several stages gradually enriching the
object detection output with object viewpoint, keypoints and 3D shape
estimates. Following careful design, in each stage it constantly improves the
performance and achieves state-ofthe-art performance in simultaneous 2D
bounding box and viewpoint estimation on the challenging Pascal3D+ dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05055</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05055</id><created>2015-03-17</created><authors><author><keyname>Chebbah</keyname><forenames>Mouna</forenames><affiliation>IRISA</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>IRISA</affiliation></author><author><keyname>Yaghlane</keyname><forenames>Boutheina Ben</forenames></author></authors><title>Combining partially independent belief functions</title><categories>cs.AI</categories><comments>Decision Support Systems, Elsevier, 2015</comments><proxy>ccsd</proxy><doi>10.1016/j.dss.2015.02.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of belief functions manages uncertainty and also proposes a set of
combination rules to aggregate opinions of several sources. Some combination
rules mix evidential information where sources are independent; other rules are
suited to combine evidential information held by dependent sources. In this
paper we have two main contributions: First we suggest a method to quantify
sources' degree of independence that may guide the choice of the more
appropriate set of combination rules. Second, we propose a new combination rule
that takes consideration of sources' degree of independence. The proposed
method is illustrated on generated mass functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05062</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05062</id><created>2015-03-17</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Flores-Pe&#xf1;aloza</keyname><forenames>David</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>Modem Illumination of Monotone Polygons</title><categories>cs.CG</categories><comments>full version, 21 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a generalization of the classical problem of the illumination of
polygons. Instead of modeling a light source we model a wireless device whose
radio signal can penetrate a given number $k$ of walls. We call these objects
$k$-modems and study the minimum number of $k$-modems sufficient and sometimes
necessary to illuminate monotone and monotone orthogonal polygons. We show that
every monotone polygon with $n$ vertices can be illuminated with $\big\lceil
\frac{n-2}{2k+3} \big\rceil$ $k$-modems. In addition, we exhibit examples of
monotone polygons requiring at least $\lceil \frac {n-2} {2k+3}\rceil$
$k$-modems to be illuminated.
  For monotone orthogonal polygons with $n$ vertices we show that for $k=1$ and
for even $k$, every such polygon can be illuminated with $\big\lceil
\frac{n-2}{2k+4} \big\rceil$ $k$-modems, while for odd $k\geq3$, $\big\lceil
\frac{n-2}{2k+6} \big\rceil$ $k$-modems are always sufficient. Further, by
presenting according examples of monotone orthogonal polygons, we show that
both bounds are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05067</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05067</id><created>2015-03-17</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author></authors><title>Low Autocorrelation Binary Sequences: Number Theory-based Analysis for
  Minimum Energy Level, Barker codes</title><categories>cs.IT math.IT</categories><comments>34 pages</comments><journal-ref>Digital Signal Processing, Elsevier, vol. 20, issue 2, pp.
  483-495, 2010</journal-ref><doi>10.1016/j.dsp.2009.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low autocorrelation binary sequences (LABS) are very important for
communication applications. And it is a notoriously difficult computational
problem to find binary sequences with low aperiodic autocorrelations. The
problem can also be stated in terms of finding binary sequences with minimum
energy levels or maximum merit factor defined by M.J.E. Golay, F=N^2/2E, N and
E being the sequence length and energy respectively. Conjectured asymptotic
value of F is 12.32 for very long sequences. In this paper, a theorem has been
proved to show that there are finite number of possible energy levels, spaced
at an equal interval of 4, for the binary sequence of a particular length. Two
more theorems are proved to derive the theoretical minimum energy level of a
binary sequence of even and odd length of N to be N/2, and N-1/2 respectively,
making the merit factor equal to N and N^2/N-1 respectively. The derived
theoretical minimum energy level successfully explains the case of N =13, for
which the merit factor (F =14.083) is higher than the conjectured value.
Sequence of lengths 4, 5, 7, 11, 13 are also found to be following the
theoretical minimum energy level. These sequences are exactly the Barker
sequences which are widely used in direct-sequence spread spectrum and pulse
compression radar systems because of their low autocorrelation properties.
Further analysis shows physical reasoning in support of the conjecture that
Barker sequences exists only when N &lt;= 13 (this has been proven for all odd N).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05070</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05070</id><created>2015-03-17</created><authors><author><keyname>Wang</keyname><forenames>Zijian</forenames></author><author><keyname>Stupia</keyname><forenames>Ivan</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Energy Efficient Precoder Design for MIMO-OFDM with Rate-dependent
  Circuit Power</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies an energy efficient design of precoders for point-to-point
multiple-input-multiple-output (MIMO) orthogonal frequency-division
multiplexing (OFDM) systems. Differently from traditional approaches, the
optimal power allocation strategy is studied by modelling the circuit power as
a rate-dependent function. We show that if the circuit power is a constant plus
an increasing and convex function of the transmission rate, the problem of
minimizing the consumed energy per bit received can be reformulated as a convex
fractional program and solved by means of a bisection algorithm. The impact of
the some system parameters is investigated either analytically or by means of
computational results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05079</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05079</id><created>2015-03-17</created><authors><author><keyname>Hemachandra</keyname><forenames>Sachithra</forenames></author><author><keyname>Duvallet</keyname><forenames>Felix</forenames></author><author><keyname>Howard</keyname><forenames>Thomas M.</forenames></author><author><keyname>Roy</keyname><forenames>Nicholas</forenames></author><author><keyname>Stentz</keyname><forenames>Anthony</forenames></author><author><keyname>Walter</keyname><forenames>Matthew R.</forenames></author></authors><title>Learning Models for Following Natural Language Directions in Unknown
  Environments</title><categories>cs.RO</categories><comments>ICRA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural language offers an intuitive and flexible means for humans to
communicate with the robots that we will increasingly work alongside in our
homes and workplaces. Recent advancements have given rise to robots that are
able to interpret natural language manipulation and navigation commands, but
these methods require a prior map of the robot's environment. In this paper, we
propose a novel learning framework that enables robots to successfully follow
natural language route directions without any previous knowledge of the
environment. The algorithm utilizes spatial and semantic information that the
human conveys through the command to learn a distribution over the metric and
semantic properties of spatially extended environments. Our method uses this
distribution in place of the latent world model and interprets the natural
language instruction as a distribution over the intended behavior. A novel
belief space planner reasons directly over the map and behavior distributions
to solve for a policy using imitation learning. We evaluate our framework on a
voice-commandable wheelchair. The results demonstrate that by learning and
performing inference over a latent environment model, the algorithm is able to
successfully follow natural language route directions within novel, extended
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05081</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05081</id><created>2015-03-17</created><authors><author><keyname>Spricer</keyname><forenames>Kristoffer</forenames></author><author><keyname>Britton</keyname><forenames>Tom</forenames></author></authors><title>The Configuration Model for Partially Directed Graphs</title><categories>math.PR cs.SI physics.soc-ph</categories><comments>19 pages, 3 figures, 2 tables</comments><doi>10.1007/s10955-015-1360-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The configuration model was originally defined for undirected networks and
has recently been extended to directed networks. Many empirical networks are
however neither undirected nor completely directed, but instead usually
partially directed meaning that certain edges are directed and others are
undirected. In the paper we define a configuration model for such networks
where nodes have in-, out-, and undirected degrees that may be dependent. We
prove conditions under which the resulting degree distributions converge to the
intended degree distributions. The new model is shown to better approximate
several empirical networks compared to undirected and completely directed
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05085</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05085</id><created>2015-03-17</created><updated>2015-04-14</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Chiranjib</forenames></author><author><keyname>Shukla</keyname><forenames>Namrata</forenames></author><author><keyname>Pati</keyname><forenames>Arun Kumar</forenames></author></authors><title>Stronger Error Disturbance Relations for Incompatible Quantum
  Measurements</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>6+epsilon pages, 4 figures ; RevTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate three new error disturbance relations, one of which is free from
explicit dependence upon intrinsic fluctuations of observables. The first
error-disturbance relation is tighter than the one provided by the Branciard
inequality and the Ozawa inequality for some initial states. Other two error
disturbance relations provide a tighter bound to Ozawa's error disturbance
relation and one of them is in fact tighter than the bound provided by
Branciard's inequality for a small number of states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05087</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05087</id><created>2015-03-17</created><authors><author><keyname>Neu</keyname><forenames>Gergely</forenames></author><author><keyname>Bart&#xf3;k</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Importance weighting without importance weights: An efficient algorithm
  for combinatorial semi-bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a sample-efficient alternative for importance weighting for
situations where one only has sample access to the probability distribution
that generates the observations. Our new method, called Recurrence Weighting
(RW), is described and analyzed in the context of online combinatorial
optimization under semi-bandit feedback, where a learner sequentially selects
its actions from a combinatorial decision set so as to minimize its cumulative
loss. In particular, we show that the well-known Follow-the-Perturbed-Leader
(FPL) prediction method coupled with Recurrence Weighting yields the first
computationally efficient reduction from offline to online optimization in this
setting. We provide a thorough theoretical analysis for the resulting
algorithm, showing that its performance is on par with previous, inefficient
solutions. Our main contribution is showing that, despite the relatively large
variance induced by the RW procedure, our performance guarantees hold with high
probability rather than only in expectation. As a side result, we also improve
the best known regret bounds for FPL in online combinatorial optimization with
full feedback, closing the perceived performance gap between FPL and
exponential weights in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05096</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05096</id><created>2015-03-17</created><authors><author><keyname>Kraker</keyname><forenames>Peter</forenames></author><author><keyname>Enkhbayar</keyname><forenames>Asura</forenames></author><author><keyname>Lex</keyname><forenames>Elisabeth</forenames></author></authors><title>Exploring Coverage and Distribution of Identifiers on the Scholarly Web</title><categories>cs.DL</categories><comments>Accepted for publication at the 14th International Symposium of
  Information Science (ISI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a scientific publishing environment that is increasingly moving online,
identifiers of scholarly work are gaining in importance. In this paper, we
analysed identifier distribution and coverage of articles from the discipline
of quantitative biology using arXiv, Mendeley and CrossRef as data sources. The
results show that when retrieving arXiv articles from Mendeley, we were able to
find more papers using the DOI than the arXiv ID. This indicates that DOI may
be a better identifier with respect to findability. We also find that coverage
of articles on Mendeley decreases in the most recent years, whereas the
coverage of DOIs does not decrease in the same order of magnitude. This hints
at the fact that there is a certain time lag involved, before articles are
covered in crowd-sourced services on the scholarly web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05110</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05110</id><created>2015-03-17</created><updated>2015-10-28</updated><authors><author><keyname>Bonnet</keyname><forenames>&#xc9;douard</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>The Graph Motif problem parameterized by the structure of the input
  graph</title><categories>cs.DS</categories><comments>conference version in IPEC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Graph Motif problem was introduced in 2006 in the context of biological
networks. It consists of deciding whether or not a multiset of colors occurs in
a connected subgraph of a vertex-colored graph. Graph Motif has been mostly
analyzed from the standpoint of parameterized complexity. The main parameters
which came into consideration were the size of the multiset and the number of
colors. Though, in the many applications of Graph Motif, the input graph
originates from real-life and has structure. Motivated by this prosaic
observation, we systematically study its complexity relatively to graph
structural parameters. For a wide range of parameters, we give new or improved
FPT algorithms, or show that the problem remains intractable. For the FPT
cases, we also give some kernelization lower bounds as well as some ETH-based
lower bounds on the worst case running time. Interestingly, we establish that
Graph Motif is W[1]-hard (while in W[P]) for parameter max leaf number, which
is, to the best of our knowledge, the first problem to behave this way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05113</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05113</id><created>2015-03-17</created><authors><author><keyname>Ghazi-Zahedi</keyname><forenames>Keyan</forenames></author><author><keyname>Rauh</keyname><forenames>Johannes</forenames></author></authors><title>Quantifying Morphological Computation based on an Information
  Decomposition of the Sensorimotor Loop</title><categories>cs.AI cs.IT math.IT</categories><comments>8 pages, 4 figures</comments><msc-class>68T01, 94A15, 94A17</msc-class><acm-class>I.2.0; H.1.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question how an agent is affected by its embodiment has attracted growing
attention in recent years. A new field of artificial intelligence has emerged,
which is based on the idea that intelligence cannot be understood without
taking into account embodiment. We believe that a formal approach to
quantifying the embodiment's effect on the agent's behaviour is beneficial to
the fields of artificial life and artificial intelligence. The contribution of
an agent's body and environment to its behaviour is also known as morphological
computation. Therefore, in this work, we propose a quantification of
morphological computation, which is based on an information decomposition of
the sensorimotor loop into shared, unique and synergistic information. In
numerical simulation based on a formal representation of the sensorimotor loop,
we show that the unique information of the body and environment is a good
measure for morphological computation. The results are compared to our
previously derived quantification of morphological computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05123</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05123</id><created>2015-03-17</created><authors><author><keyname>Amunategui</keyname><forenames>Manuel</forenames></author><author><keyname>Markwell</keyname><forenames>Tristan</forenames></author><author><keyname>Rozenfeld</keyname><forenames>Yelena</forenames></author></authors><title>Prediction Using Note Text: Synthetic Feature Creation with word2vec</title><categories>cs.CL</categories><comments>13 pages including appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  word2vec affords a simple yet powerful approach of extracting quantitative
variables from unstructured textual data. Over half of healthcare data is
unstructured and therefore hard to model without involved expertise in data
engineering and natural language processing. word2vec can serve as a bridge to
quickly gather intelligence from such data sources.
  In this study, we ran 650 megabytes of unstructured, medical chart notes from
the Providence Health &amp; Services electronic medical record through word2vec. We
used two different approaches in creating predictive variables and tested them
on the risk of readmission for patients with COPD (Chronic Obstructive Lung
Disease). As a comparative benchmark, we ran the same test using the LACE risk
model (a single score based on length of stay, acuity, comorbid conditions, and
emergency department visits).
  Using only free text and mathematical might, we found word2vec comparable to
LACE in predicting the risk of readmission of COPD patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05124</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05124</id><created>2015-03-17</created><updated>2016-03-03</updated><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author></authors><title>A representation theorem for stratified complete lattices</title><categories>cs.LO</categories><msc-class>06B23, 68Q55</msc-class><acm-class>F.3.2; D.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider complete lattices equipped with preorderings indexed by the
ordinals less than a given (limit) ordinal subject to certain axioms. These
structures, called stratified complete lattices, and weakly monotone functions
over them, provide a framework for solving fixed point equations involving
non-monotone operations such as negation or complement, and have been used to
give semantics to logic programs with negation.
  More precisely, we consider stratified complete lattices subject to two
slightly different systems of axioms defining `models' and `strong models'. We
prove that a stratified complete lattice is a model iff it is isomorphic to the
stratified complete lattice determined by the limit of an inverse system of
complete lattices with `locally completely additive' projections. Moreover, we
prove that a stratified complete lattice is a strong model iff it is isomorphic
to the stratified complete lattice determined by the limit of an inverse system
of complete lattices with completely additive projections.
  We use the inverse limit representation to give alternative proofs of some
recent results and to derive some new ones for models and strong models. In
particular, we use the representation theorem to prove that every model gives
rise to another complete lattice structure, which in limit models corresponds
to the lexicographic order. Moreover, we prove that the set of all fixed points
of a weakly monotone function over a model, equipped with the new ordering, is
a complete lattice. We also consider symmetric models that satisfy, together
with each axiom, the dual axiom, and use the inverse limit representation to
prove that every strong model is symmetric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05133</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05133</id><created>2015-03-17</created><authors><author><keyname>Schulte</keyname><forenames>Patrick</forenames></author><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author></authors><title>Constant Composition Distribution Matching</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distribution matching transforms independent and Bernoulli(1/2) distributed
input bits into a sequence of output symbols with a desired distribution.
Fixed-to-fixed length, invertible, and low complexity encoders and decoders
based on constant composition and arithmetic coding are presented.
Asymptotically in the blocklength, the encoder achieves the maximum rate,
namely the entropy of the desired distribution. Furthermore, the normalized
divergence of the encoder output and the desired distribution goes to zero in
the blocklength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05140</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05140</id><created>2015-03-17</created><authors><author><keyname>Asgari</keyname><forenames>Ehsaneddin</forenames></author><author><keyname>Mofrad</keyname><forenames>Mohammad R. K.</forenames></author></authors><title>ProtVec: A Continuous Distributed Representation of Biological Sequences</title><categories>q-bio.QM cs.AI cs.LG q-bio.GN</categories><journal-ref>PLoS ONE 10(11): e0141287, 2015</journal-ref><doi>10.1371/journal.pone.0141287</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach for representing biological sequences. This method,
named protein-vectors or ProtVec for short, can be utilized in bioinformatics
applications such as family classification, protein visualization, structure
prediction, disordered protein identification, and protein-protein interaction
prediction. Using the Skip-gram neural networks, protein sequences are
represented with a single dense n-dimensional vector. This method was evaluated
by classifying protein sequences obtained from Swiss-Prot belonging to 7,027
protein families where an average family classification accuracy of $94\%\pm
0.03\%$ was obtained, outperforming existing family classification methods. In
addition, our model was used to predict disordered proteins from structured
proteins. Two databases of disordered sequences were used: the DisProt database
as well as a database featuring the disordered regions of nucleoporins rich
with phenylalanine-glycine repeats (FG-Nups). Using support vector machine
classifiers, FG-Nup sequences were distinguished from structured Protein Data
Bank (PDB) sequences with 99.81\% accuracy, and unstructured DisProt sequences
from structured DisProt sequences with 100.0\% accuracy. These results indicate
that by only providing sequence data for various proteins into this model,
information about protein structure can be determined with high accuracy. This
so-called embedding model needs to be trained only once and can then be used to
ascertain a diverse set of information regarding the proteins of interest. In
addition, this representation can be considered as pre-training for various
applications of deep learning in bioinformatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05141</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05141</id><created>2015-03-17</created><authors><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Urgaonkar</keyname><forenames>Rahul</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author><author><keyname>Zafer</keyname><forenames>Murtaza</forenames></author><author><keyname>Chan</keyname><forenames>Kevin</forenames></author><author><keyname>Leung</keyname><forenames>Kin K.</forenames></author></authors><title>Mobility-Induced Service Migration in Mobile Micro-Clouds</title><categories>cs.DC cs.NI math.OC</categories><comments>in Proc. of IEEE MILCOM 2014, Oct. 2014</comments><doi>10.1109/MILCOM.2014.145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile micro-cloud is an emerging technology in distributed computing, which
is aimed at providing seamless computing/data access to the edge of the network
when a centralized service may suffer from poor connectivity and long latency.
Different from the traditional cloud, a mobile micro-cloud is smaller and
deployed closer to users, typically attached to a cellular basestation or
wireless network access point. Due to the relatively small coverage area of
each basestation or access point, when a user moves across areas covered by
different basestations or access points which are attached to different
micro-clouds, issues of service performance and service migration become
important. In this paper, we consider such migration issues. We model the
general problem as a Markov decision process (MDP), and show that, in the
special case where the mobile user follows a one-dimensional asymmetric random
walk mobility model, the optimal policy for service migration is a threshold
policy. We obtain the analytical solution for the cost resulting from arbitrary
thresholds, and then propose an algorithm for finding the optimal thresholds.
The proposed algorithm is more efficient than standard mechanisms for solving
MDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05144</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05144</id><created>2015-03-17</created><authors><author><keyname>Lazzeretti</keyname><forenames>Riccardo</forenames></author><author><keyname>Pignata</keyname><forenames>Tommaso</forenames></author><author><keyname>Barni</keyname><forenames>Mauro</forenames></author></authors><title>Piecewise Function Approximation with Private Data</title><categories>cs.CR</categories><comments>Draft of paper that will be soon submitted to IEEE Transaction on
  Information Forensic and Security</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two Secure Two Party Computation (STPC) protocols for piecewise
function approximation on private data. The protocols rely on a piecewise
approximation of the to-be-computed function easing the implementation in a
STPC setting. The first protocol relies entirely on Garbled Circuit (GC)
theory, while the second one exploits a hybrid construction where GC and
Homomorphic Encryption (HE) are used together. In addition to piecewise
constant and linear approximation, polynomial interpolation is also considered.
From a communication complexity perspective, the full-GC implementation is
preferable when the input and output variables can be represented with a small
number of bits, while the hybrid solution is preferable otherwise. With regard
to computational complexity, the full-GC solution is generally more convenient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05146</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05146</id><created>2015-03-17</created><authors><author><keyname>Patel</keyname><forenames>Utkarsh R.</forenames></author><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>Accurate Impedance Calculation for Underground and Submarine Power
  Cables using MoM-SO and a Multilayer Ground Model</title><categories>cs.CE</categories><comments>Submitted to IEEE Transactions on Power Delivery on March 16, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate knowledge of the per-unit length impedance of power cables is
necessary to correctly predict electromagnetic transients in power systems. In
particular, skin, proximity, and ground return effects must be properly
estimated. In many applications, the medium that surrounds the cable is not
uniform and can consist of multiple layers of different conductivity, such as
dry and wet soil, water, or air. We introduce a multilayer ground model for the
recently-proposed MoM-SO method, suitable to accurately predict ground return
effects in such scenarios. The proposed technique precisely accounts for skin,
proximity, ground and tunnel effects, and is applicable to a variety of cable
configurations, including underground and submarine cables. Numerical results
show that the proposed method is more accurate than analytic formulas typically
employed for transient analyses, and delivers an accuracy comparable to the
finite element method (FEM). With respect to FEM, however, MoM-SO is over 1000
times faster, and can calculate the impedance of a submarine cable inside a
three-layer medium in 0.10~s per frequency point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05157</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05157</id><created>2015-03-17</created><authors><author><keyname>Debattista</keyname><forenames>Jeremy</forenames></author><author><keyname>Londo&#xf1;o</keyname><forenames>Santiago</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Quality Assessment of Linked Datasets using Probabilistic Approximation</title><categories>cs.DB cs.DS</categories><comments>15 pages, 2 figures, To appear in ESWC 2015 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing application of Linked Open Data, assessing the quality of
datasets by computing quality metrics becomes an issue of crucial importance.
For large and evolving datasets, an exact, deterministic computation of the
quality metrics is too time consuming or expensive. We employ probabilistic
techniques such as Reservoir Sampling, Bloom Filters and Clustering Coefficient
estimation for implementing a broad set of data quality metrics in an
approximate but sufficiently accurate way. Our implementation is integrated in
the comprehensive data quality assessment framework Luzzu. We evaluated its
performance and accuracy on Linked Open Datasets of broad relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05171</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05171</id><created>2015-03-17</created><authors><author><keyname>Abdeen</keyname><forenames>Hani</forenames></author><author><keyname>Sahraoui</keyname><forenames>Houari</forenames></author></authors><title>Modeling and Analyzing Release Trajectory based on the Process of Issue
  Tracking</title><categories>cs.SE</categories><comments>DIRO technical report, draft, undersubmission</comments><report-no>1374</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software release development process, that we refer to as &quot;release
trajectory&quot;, involves development activities that are usually sorted in
different categories, such as incorporating new features, improving software,
or fixing bugs, and associated to &quot;issues&quot;. Release trajectory management is a
difficult and crucial task. Managers must be aware of every aspect of the
development process for managing the software-related issues. Issue Tracking
Systems (ITS) play a central role in supporting the management of release
trajectory. These systems, which support reporting and tracking issues of
different kinds (such as &quot;bug&quot;, &quot;feature&quot;, &quot;improvement&quot;, etc.), record rich
data about the software development process. Yet, recorded historical data in
ITS are still not well-modeled for supporting practical needs of release
trajectory management.
  In this paper, we describe a sequence analysis approach for modeling and
analyzing releases' trajectories, using the tracking process of reported
issues. Release trajectory analysis is based on the categories of tracked
issues and their temporal changing, and aims to address important questions
regarding the co-habitation of unresolved issues, the transitions between
different statuses in release trajectory, the recurrent patterns of release
trajectories, and the properties of a release trajectory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05172</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05172</id><created>2015-03-17</created><authors><author><keyname>AlGhamdi</keyname><forenames>Rayed</forenames></author><author><keyname>Alfarraj</keyname><forenames>Osama Abdulaziz</forenames></author><author><keyname>Bahaddad</keyname><forenames>Adel A.</forenames></author></authors><title>How Retailers at different Stages of E-Commerce Maturity Evaluate Their
  Entry to E-Commerce Activities?</title><categories>cs.CY</categories><journal-ref>Journal of Computer Science and Information Technology, June 2014,
  Vol. 2, No. 2, pp. 37-71</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates how retailers at different stages of e-commerce
maturity evaluate their entry to e-commerce activities. The study was conducted
using qualitative approach interviewing 16 retailers in Saudi Arabia. It comes
up with 22 factors that are believed the most influencing factors for retailers
in Saudi Arabia. Interestingly, there seem to be differences between retailers
in companies at different maturity stages in terms of having different
attitudes regarding the issues of using e-commerce. The businesses that have
reached a high stage of e-commerce maturity provide practical evidence of
positive and optimistic attitudes and practices regarding use of e-commerce,
whereas the businesses that have not reached higher levels of maturity provide
practical evidence of more negative and pessimistic attitudes and practices.
The study, therefore, should contribute to efforts leading to greater
e-commerce development in Saudi Arabia and other countries with similar
context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05180</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05180</id><created>2015-03-17</created><authors><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Orsini</keyname><forenames>Chiara</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Hyperbolic Graph Generator</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 2 figures</comments><journal-ref>Computer Physics Communications 196, 492 (2015)</journal-ref><doi>10.1016/j.cpc.2015.05.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks representing many complex systems in nature and society share some
common structural properties like heterogeneous degree distributions and strong
clustering. Recent research on network geometry has shown that those real
networks can be adequately modeled as random geometric graphs in hyperbolic
spaces. In this paper, we present a computer program to generate such graphs.
Besides real-world-like networks, the program can generate random graphs from
other well-known graph ensembles, such as the soft configuration model, random
geometric graphs on a circle, or Erd\H{o}s-R\'enyi random graphs. The
simulations show a good match between the expected values of different network
structural properties and the corresponding empirical values measured in
generated graphs, confirming the accurate behavior of the program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05187</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05187</id><created>2015-03-17</created><authors><author><keyname>Fawagreh</keyname><forenames>Khaled</forenames></author><author><keyname>Gaber</keyname><forenames>Mohamad Medhat</forenames></author><author><keyname>Elyan</keyname><forenames>Eyad</forenames></author></authors><title>An Outlier Detection-based Tree Selection Approach to Extreme Pruning of
  Random Forests</title><categories>cs.LG</categories><comments>21 pages, 4 Figures. arXiv admin note: substantial text overlap with
  arXiv:1503.04996</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Forest (RF) is an ensemble classification technique that was developed
by Breiman over a decade ago. Compared with other ensemble techniques, it has
proved its accuracy and superiority. Many researchers, however, believe that
there is still room for enhancing and improving its performance in terms of
predictive accuracy. This explains why, over the past decade, there have been
many extensions of RF where each extension employed a variety of techniques and
strategies to improve certain aspect(s) of RF. Since it has been proven
empirically that ensembles tend to yield better results when there is a
significant diversity among the constituent models, the objective of this paper
is twofolds. First, it investigates how an unsupervised learning technique,
namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the
RF. Second, trees with the highest LOF scores are then used to produce an
extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet
performs at least as good as RF, but mostly exhibits higher performance in
terms of accuracy. The latter refers to a known technique called ensemble
pruning. Experimental results on 10 real datasets prove the superiority of our
proposed extension over the traditional RF. Unprecedented pruning levels
reaching 99% have been achieved at the time of boosting the predictive accuracy
of the ensemble. The notably high pruning level makes the technique a good
candidate for real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05214</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05214</id><created>2015-03-17</created><updated>2015-05-13</updated><authors><author><keyname>Elgamal</keyname><forenames>Tarek</forenames></author><author><keyname>Hefeeda</keyname><forenames>Mohamed</forenames></author></authors><title>Analysis of PCA Algorithms in Distributed Environments</title><categories>cs.DC cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical machine learning algorithms often face scalability bottlenecks when
they are applied to large-scale data. Such algorithms were designed to work
with small data that is assumed to fit in the memory of one machine. In this
report, we analyze different methods for computing an important machine learing
algorithm, namely Principal Component Analysis (PCA), and we comment on its
limitations in supporting large datasets. The methods are analyzed and compared
across two important metrics: time complexity and communication complexity. We
consider the worst-case scenarios for both metrics, and we identify the
software libraries that implement each method. The analysis in this report
helps researchers and engineers in (i) understanding the main bottlenecks for
scalability in different PCA algorithms, (ii) choosing the most appropriate
method and software library for a given application and data set
characteristics, and (iii) designing new scalable PCA algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05224</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05224</id><created>2015-03-17</created><authors><author><keyname>Mogharbel</keyname><forenames>Bander</forenames></author><author><keyname>Fan</keyname><forenames>Lingling</forenames></author><author><keyname>Miao</keyname><forenames>Zhixin</forenames></author></authors><title>Least Squares Estimation-Based Synchronous Generator Parameter
  Estimation Using PMU Data</title><categories>cs.SY</categories><comments>5 pages, 6 figures, accepted by IEEE PESGM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, least square estimation (LSE)-based dynamic generator model
parameter identification is investigated. Electromechanical dynamics related
parameters such as inertia constant and primary frequency control droop for a
synchronous generator are estimated using Phasor Measurement Unit (PMU) data
obtained at the generator terminal bus. The key idea of applying LSE for
dynamic parameter estimation is to have a discrete
\underline{a}uto\underline{r}egression with e\underline{x}ogenous input (ARX)
model. With an ARX model, a linear estimation problem can be formulated and the
parameters of the ARX model can be found. This paper gives the detailed
derivation of converting a generator model with primary frequency control into
an ARX model. The generator parameters will be recovered from the estimated ARX
model parameters afterwards. Two types of conversion methods are presented:
zero-order hold (ZOH) method and Tustin method. Numerical results are presented
to illustrate the proposed LSE application in dynamic system parameter
identification using PMU data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05225</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05225</id><created>2015-03-17</created><authors><author><keyname>Abdullah</keyname><forenames>Amirali</forenames></author><author><keyname>Kumar</keyname><forenames>Ravi</forenames></author><author><keyname>McGregor</keyname><forenames>Andrew</forenames></author><author><keyname>Vassilvitskii</keyname><forenames>Sergei</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Sketching, Embedding, and Dimensionality Reduction for Information
  Spaces</title><categories>cs.DS cs.CG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information distances like the Hellinger distance and the Jensen-Shannon
divergence have deep roots in information theory and machine learning. They are
used extensively in data analysis especially when the objects being compared
are high dimensional empirical probability distributions built from data.
However, we lack common tools needed to actually use information distances in
applications efficiently and at scale with any kind of provable guarantees. We
can't sketch these distances easily, or embed them in better behaved spaces, or
even reduce the dimensionality of the space while maintaining the probability
structure of the data.
  In this paper, we build these tools for information distances---both for the
Hellinger distance and Jensen--Shannon divergence, as well as related measures,
like the $\chi^2$ divergence. We first show that they can be sketched
efficiently (i.e. up to multiplicative error in sublinear space) in the
aggregate streaming model. This result is exponentially stronger than known
upper bounds for sketching these distances in the strict turnstile streaming
model. Second, we show a finite dimensionality embedding result for the
Jensen-Shannon and $\chi^2$ divergences that preserves pair wise distances.
Finally we prove a dimensionality reduction result for the Hellinger,
Jensen--Shannon, and $\chi^2$ divergences that preserves the information
geometry of the distributions (specifically, by retaining the simplex structure
of the space). While our second result above already implies that these
divergences can be explicitly embedded in Euclidean space, retaining the
simplex structure is important because it allows us to continue doing inference
in the reduced space. In essence, we preserve not just the distance structure
but the underlying geometry of the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05241</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05241</id><created>2015-03-17</created><authors><author><keyname>Zhu</keyname><forenames>Dengkui</forenames></author><author><keyname>Li</keyname><forenames>Boyu</forenames></author><author><keyname>Liang</keyname><forenames>Ping</forenames></author></authors><title>On the Matrix Inversion Approximation Based on Neumann Series in Massive
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>accepted to conference; Proc. IEEE ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-Forcing (ZF) has been considered as one of the potential practical
precoding and detection method for massive MIMO systems. One of the most
important advantages of massive MIMO is the capability of supporting a large
number of users in the same time-frequency resource, which requires much larger
dimensions of matrix inversion for ZF than conventional multi-user MIMO
systems. In this case, Neumann Series (NS) has been considered for the Matrix
Inversion Approximation (MIA), because of its suitability for massive MIMO
systems and its advantages in hardware implementation. The
performance-complexity trade-off and the hardware implementation of NS-based
MIA in massive MIMO systems have been discussed. In this paper, we analyze the
effects of the ratio of the number of massive MIMO antennas to the number of
users on the performance of NS-based MIA. In addition, we derive the
approximation error estimation formulas for different practical numbers of
terms of NS-based MIA. These results could offer useful guidelines for
practical massive MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05252</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05252</id><created>2015-03-17</created><authors><author><keyname>Stephen</keyname><forenames>Tamon</forenames></author><author><keyname>Yusun</keyname><forenames>Timothy</forenames></author></authors><title>Circuit diameter and Klee-Walkup constructions</title><categories>math.CO cs.CG</categories><msc-class>52B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a variant of the graph diameter of a polyhedron where each step in a
walk between two vertices travels maximally in a circuit direction instead of
along incident edges. Here circuit directions are non-trivial solutions to
minimally-dependent subsystems of the presentation of the polyhedron. These can
be understood as the set of all possible edge directions, including edges that
may arise from translation of the facets.
  It is appealing to consider a circuit analogue of the Hirsch conjecture for
graph diameter, as suggested by Borgwardt et al. [BFH15]. They ask whether the
known counterexamples to the Hirsch conjecture give rise to counterexamples for
this relaxed notion of circuit diameter. We show that the most basic
counterexample to the unbounded Hirsch conjecture, the Klee-Walkup polyhedron,
does have a circuit diameter that satisfies the Hirsch bound, regardless of
representation. We also examine the circuit diameter of the bounded Klee-Walkup
polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05265</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05265</id><created>2015-03-17</created><authors><author><keyname>MacCartney</keyname><forenames>George R.</forenames><suffix>Jr.</suffix></author><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>Exploiting Directionality for Millimeter-Wave Wireless System
  Improvement</title><categories>cs.IT math.IT</categories><comments>To appear in ICC 2015 (London, UK), 7 Figures, 6 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents directional and omnidirectional RMS delay spread
statistics obtained from 28 GHz and 73 GHz ultrawideband propagation
measurements carried out in New York City using a 400 Megachips per second
broadband sliding correlator channel sounder and highly directional steerable
horn antennas. The 28 GHz measurements did not systematically seek the optimum
antenna pointing angles and resulted in 33% outage for 39 T-R separation
distances within 200 m. The 73 GHz measurements systematically found the best
antenna pointing angles and resulted in 14.3% outage for 35 T-R separation
distances within 200 m, all for mobile height receivers. Pointing the antennas
to yield the strongest received power is shown to significantly reduce RMS
delay spreads in line-of-sight (LOS) environments. A new term, distance
extension exponent (DEE) is defined, and used to mathematically describe the
increase in coverage distance that results by combining beams from angles with
the strongest received power at a given location. These results suggest that
employing directionality in millimeter-wave communications systems will reduce
inter-symbol interference, improve link margin at cell edges, and enhance
overall system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05269</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05269</id><created>2015-03-17</created><authors><author><keyname>Maamari</keyname><forenames>Diana</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author></authors><title>Coverage in mmWave Cellular Networks with Base station Cooperation</title><categories>cs.IT cs.NI math.IT</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of signal outage, due to shadowing and blockage, is expected to
be the main bottleneck in millimeter wave (mmWave) networks. Moreover, with the
anticipated vision that mmWave networks would have a dense deployment of base
stations, interference from strong line-of-sight base stations increases too,
thus further increasing the probability of outage. To address the issue of
reducing outage, this paper explores the possibility of base station
cooperation in the downlink of a mmWave heterogenous network. The main focus of
this work is showing that, in a stochastic geometry framework, cooperation from
randomly located base stations decreases outage probability. With the presumed
vision that less severe fading will be experienced due to highly directional
transmissions, one might expect that cooperation would increase the coverage
probability; our numerical examples suggest that is in fact the case. Coverage
probabilities are derived accounting for: different fading distributions,
antenna directionality and blockage. Numerical results suggest that coverage
with base station cooperation in dense mmWave systems and with no small scale
fading considerably exceeds coverage with no cooperation. In contrast, an
insignificant increase is reported when mmWave networks are less dense with a
high probability of signal blockage and with Rayleigh fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05271</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05271</id><created>2015-03-17</created><authors><author><keyname>Chen</keyname><forenames>Jiadi</forenames></author><author><keyname>Long</keyname><forenames>Hang</forenames></author><author><keyname>Zheng</keyname><forenames>Qiang</forenames></author><author><keyname>Xing</keyname><forenames>Minyao</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author></authors><title>An SMDP-based Resource Management Scheme for Distributed Cloud Systems</title><categories>cs.NI cs.DC</categories><comments>5 pages, 5 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the resource management problem in geographically distributed
cloud systems is considered. The Follow Me Cloud concept which enables service
migration across federated data centers (DCs) is adopted. Therefore, there are
two types of service requests to the DC, i.e., new requests (NRs) initiated in
the local service area and migration requests (MRs) generated when mobile users
move across service areas. A novel resource management scheme is proposed to
help the resource manager decide whether to accept the service requests (NRs or
MRs) or not and determine how much resources should be allocated to each
service (if accepted). The optimization objective is to maximize the average
system reward and keep the rejection probability of service requests under a
certain threshold. Numerical results indicate that the proposed scheme can
significantly improve the overall system utility as well as the user experience
compared with other resource management schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05272</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05272</id><created>2015-03-17</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author><author><keyname>Bernasconi</keyname><forenames>J.</forenames></author><author><keyname>Braendle</keyname><forenames>H.</forenames></author><author><keyname>Buijs</keyname><forenames>H.</forenames></author><author><keyname>Bonenfant</keyname><forenames>S.</forenames></author></authors><title>Improved Calibration of Near-Infrared Spectra by Using Ensembles of
  Neural Network Models</title><categories>cs.NE</categories><comments>7 pages</comments><journal-ref>IEEE Sensors Journal, vol. 10, no. 3, pp. 578-584, 2010</journal-ref><doi>10.1109/JSEN.2009.2038124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IR or near-infrared (NIR) spectroscopy is a method used to identify a
compound or to analyze the composition of a material. Calibration of NIR
spectra refers to the use of the spectra as multivariate descriptors to predict
concentrations of the constituents. To build a calibration model,
state-of-the-art software predominantly uses linear regression techniques. For
nonlinear calibration problems, neural network-based models have proved to be
an interesting alternative. In this paper, we propose a novel extension of the
conventional neural network-based approach, the use of an ensemble of neural
network models. The individual neural networks are obtained by resampling the
available training data with bootstrapping or cross-validation techniques. The
results obtained for a realistic calibration example show that the
ensemble-based approach produces a significantly more accurate and robust
calibration model than conventional regression methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05273</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05273</id><created>2015-03-17</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author><author><keyname>Siti</keyname><forenames>W.</forenames></author></authors><title>Feeder Load Balancing using Fuzzy Logic and Combinatorial
  Optimization-based Implementation</title><categories>cs.OH</categories><comments>11 pages</comments><journal-ref>Electric Power Systems Research, Elsevier, vol. 78, issue 11, pp.
  1922-1932, 2008</journal-ref><doi>10.1016/j.epsr.2008.03.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes a novel
reconfiguration of the phase balancing using the fuzzy logic and the
combinatorial optimization-based implementation step back to back. Input to the
fuzzy step is the total load per phase of the feeders. Output of the fuzzy step
is the load change values, negative value for load releasing and positive value
for load receiving. The output of the fuzzy step is the input to the load
changing system. The load changing system uses combinatorial optimization
techniques to translate the change values (kW) into number of load points and
then selects the specific load points. It also performs the inter-changing of
the load points between the releasing and the receiving phases in an optimal
fashion. Application results using the distribution feeder network of South
Africa are presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05275</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05275</id><created>2015-03-17</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author><author><keyname>Zivanovic</keyname><forenames>R.</forenames></author></authors><title>Abrupt Change Detection in Power System Fault Analysis using Adaptive
  Whitening Filter and Wavelet Transform</title><categories>cs.CE</categories><comments>9 pages in final print</comments><journal-ref>Electric Power Systems Research, Elsevier, vol. 76, issues 9-10,
  pp. 815-823, 2006</journal-ref><doi>10.1016/j.epsr.2005.10.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the application of the adaptive whitening filter and the
wavelet transform used to detect the abrupt changes in the signals recorded
during disturbances in the electrical power network in South Africa. Main focus
has been to estimate exactly the time-instants of the changes in the signal
model parameters during the pre-fault condition and following events like
initiation of fault, circuit-breaker opening, auto-reclosure of the
circuit-breakers. The key idea is to decompose the fault signals, de-noised
using the adaptive whitening filter, into effective detailed and smoothed
version using the multiresolution signal decomposition technique based on
discrete wavelet transform. Then we apply the threshold method on the
decomposed signals to estimate the change time-instants, segmenting the fault
signals into the event-specific sections for further signal processing and
analysis. This paper presents application on the recorded signals in the power
transmission network of South Africa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05276</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05276</id><created>2015-03-17</created><authors><author><keyname>Ray</keyname><forenames>Kisor</forenames></author><author><keyname>deb</keyname><forenames>Partha Pratim</forenames></author></authors><title>Design and Implementation of a GUI based Offline GIFT Tool to exchange
  data between different systems</title><categories>cs.CY</categories><comments>04 pages, 04 figures, 01 tables, International Journal of Computer
  Applications, Volume 114, No.3, March 2015, ISSN 0975 8887</comments><acm-class>K.3.1</acm-class><doi>10.5120/19959-1793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Choice Questions or MCQs are very important for e-learning. Many MCQ
Tools allow us to generate MCQs very easily. However, in most of the cases they
are not portable. That means MCQs generated for one system cannot be used for
other unless a common format is used. So, collaboration and/or up gradation
becomes a time consuming tedious task. In this paper, we will examine how tool
could be designed which can produce portable MCQs and that too generating in
the laptop and/or desktop without any need for going online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05280</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05280</id><created>2015-03-18</created><authors><author><keyname>Mouradian</keyname><forenames>Carla</forenames></author><author><keyname>Saha</keyname><forenames>Tonmoy</forenames></author><author><keyname>Sahoo</keyname><forenames>Jagruti</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author><author><keyname>Morrow</keyname><forenames>Monique</forenames></author><author><keyname>Polakos</keyname><forenames>Paul</forenames></author></authors><title>NFV Based Gateways for Virtualized Wireless Sensors Networks: A Case
  Study</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Virtualization enables the sharing of a same wireless sensor network (WSN) by
multiple applications. However, in heterogeneous environments, virtualized
wireless sensor networks (VWSN) raises new challenges such as the need for
on-the-fly, dynamic, elastic and scalable provisioning of gateways. Network
Functions Virtualization (NFV) is an emerging paradigm that can certainly aid
in tackling these new challenges. It leverages standard virtualization
technology to consolidate special-purpose network elements on top of commodity
hardware. This article presents a case study on NFV based gateways for VWSNs.
In the study, a VWSN gateway provider, operates and manages an NFV based
infrastructure. We use two different brands of wireless sensors. The NFV
infrastructure makes possible the dynamic, elastic and scalable deployment of
gateway modules in this heterogeneous VWSN environment. The prototype built
with Openstack as platform is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05287</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05287</id><created>2015-03-18</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author><author><keyname>Zivanovic</keyname><forenames>R.</forenames></author></authors><title>Adjusted Haar Wavelet for Application in the Power Systems Disturbance
  Analysis</title><categories>cs.OH</categories><comments>13 pages in final printed version</comments><journal-ref>Digital Signal Processing, Elsevier, vol. 18, issue 2, pp.
  103-115, 2008</journal-ref><doi>10.1016/j.dsp.2007.04.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abrupt change detection based on the wavelet transform and threshold method
is very effective in detecting the abrupt changes and hence segmenting the
signals recorded during disturbances in the electrical power network. The
wavelet method estimates the time-instants of the changes in the signal model
parameters during the pre-fault condition, after initiation of fault, after
circuit-breaker opening and auto-reclosure. Certain kinds of disturbance
signals do not show distinct abrupt changes in the signal parameters. In those
cases, the standard mother wavelets fail to achieve correct event-specific
segmentations. A new adjustment technique to the standard Haar wavelet is
proposed in this paper, by introducing 2n adjusting zeros in the Haar wavelet
scaling filter, n being a positive integer. This technique is quite effective
in segmenting those fault signals into pre- and post-fault segments, and it is
an improvement over the standard mother wavelets for this application. This
paper presents many practical examples where recorded signals from the power
network in South Africa have been used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05294</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05294</id><created>2015-03-18</created><authors><author><keyname>Ray</keyname><forenames>Kisor</forenames></author><author><keyname>Bag</keyname><forenames>Sourav</forenames></author><author><keyname>Sarkar</keyname><forenames>Saumen</forenames></author></authors><title>Easy and Fast Design and Implementation of PostgreSQL based image
  handling application</title><categories>cs.DB</categories><comments>05 pages, 04 figures, 02 tables, International Journal of Advanced
  Research in Computer Science and Software Engineering, Volume 5, Issue 2,
  February 2015, ISSN 2277 128X</comments><acm-class>K.8.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern computing, RDBMS are great to store different types of data. To a
developer, one of the major objectives is to provide a very low cost and easy
to use solution to an existing problem. While commercial databases are more
easy to use along with their new as well as documented features come with
complicated licensing cost, free open source databases are not that
straightforward under many situations. This paper shows how a completely free
advanced open source RDBMS like PostgreSQL could be designed and modified to
store and retrieve high quality images in order to use them along with a
frontend application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05296</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05296</id><created>2015-03-18</created><authors><author><keyname>Al-Jarrah</keyname><forenames>O. Y.</forenames></author><author><keyname>Yoo</keyname><forenames>P. D.</forenames></author><author><keyname>Muhaidat</keyname><forenames>S</forenames></author><author><keyname>Karagiannidis</keyname><forenames>G. K.</forenames></author><author><keyname>Taha</keyname><forenames>K.</forenames></author></authors><title>Efficient Machine Learning for Big Data: A Review</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emerging technologies and all associated devices, it is predicted
that massive amount of data will be created in the next few years, in fact, as
much as 90% of current data were created in the last couple of years,a trend
that will continue for the foreseeable future. Sustainable computing studies
the process by which computer engineer/scientist designs computers and
associated subsystems efficiently and effectively with minimal impact on the
environment. However, current intelligent machine-learning systems are
performance driven, the focus is on the predictive/classification accuracy,
based on known properties learned from the training samples. For instance, most
machine-learning-based nonparametric models are known to require high
computational cost in order to find the global optima. With the learning task
in a large dataset, the number of hidden nodes within the network will
therefore increase significantly, which eventually leads to an exponential rise
in computational complexity. This paper thus reviews the theoretical and
experimental data-modeling literature, in large-scale data-intensive fields,
relating to: (1) model efficiency, including computational requirements in
learning, and data-intensive areas structure and design, and introduces (2) new
algorithmic approaches with the least memory requirements and processing to
minimize computational cost, while maintaining/improving its
predictive/classification accuracy and stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05297</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05297</id><created>2015-03-18</created><updated>2015-05-20</updated><authors><author><keyname>Ben-Yishai</keyname><forenames>Assaf</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>The AWGN BC with MAC Feedback: A Reduction to Noiseless Feedback via
  Interaction</title><categories>cs.IT math.IT</categories><comments>Invited talk ITW 2015, Jerusalem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communication over a two-user Additive White
Gaussian Noise Broadcast Channel (AWGN-BC) with an AWGN Multiple Access (MAC)
active feedback. We describe a constructive reduction from this setup to the
well-studied setup of linear-feedback coding over the AWGN-BC with noiseless
feedback (and different parameters). This reduction facilitates the design of
linear-feedback coding schemes in the (passive) noiseless feedback regime,
which can then be easily and constructively transformed into coding schemes in
the MAC feedback regime that attain the exact same rates. Our construction
introduces an element of interaction into the coding protocol, and is based on
modulo-lattice operations. As an example, we apply our method to the
Ozarow-Leung scheme, and demonstrate how MAC feedback can be used to increase
the capacity region of the AWGN-BC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05298</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05298</id><created>2015-03-18</created><authors><author><keyname>Morral</keyname><forenames>Gemma</forenames></author><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author></authors><title>Distributed on-line multidimensional scaling for self-localization in
  wireless sensor networks</title><categories>cs.DC cs.IT math.IT</categories><comments>32 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work considers the localization problem in wireless sensor
networks formed by fixed nodes. Each node seeks to estimate its own position
based on noisy measurements of the relative distance to other nodes. In a
centralized batch mode, positions can be retrieved (up to a rigid
transformation) by applying Principal Component Analysis (PCA) on a so-called
similarity matrix built from the relative distances. In this paper, we propose
a distributed on-line algorithm allowing each node to estimate its own position
based on limited exchange of information in the network. Our framework
encompasses the case of sporadic measurements and random link failures. We
prove the consistency of our algorithm in the case of fixed sensors. Finally,
we provide numerical and experimental results from both simulated and real
data. Simulations issued to real data are conducted on a wireless sensor
network testbed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05299</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05299</id><created>2015-03-18</created><authors><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Discrete Signal Reconstruction by Sum of Absolute Values</title><categories>cs.IT math.IT math.OC</categories><comments>IEEE Signal Processing Letters (to appear)</comments><doi>10.1109/LSP.2015.2414932</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider a problem of reconstructing an unknown discrete
signal taking values in a finite alphabet from incomplete linear measurements.
The difficulty of this problem is that the computational complexity of the
reconstruction is exponential as it is. To overcome this difficulty, we extend
the idea of compressed sensing, and propose to solve the problem by minimizing
the sum of weighted absolute values. We assume that the probability
distribution defined on an alphabet is known, and formulate the reconstruction
problem as linear programming. Examples are shown to illustrate that the
proposed method is effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05314</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05314</id><created>2015-03-18</created><authors><author><keyname>Ma</keyname><forenames>Junjie</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Ping</keyname><forenames>Li</forenames></author></authors><title>On the Performance of Turbo Signal Recovery with Partial DFT Sensing
  Matrices</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter is on the performance of the turbo signal recovery (TSR)
algorithm for partial discrete Fourier transform (DFT) matrices based
compressed sensing. Based on state evolution analysis, we prove that TSR with a
partial DFT sensing matrix outperforms the well-known approximate message
passing (AMP) algorithm with an independent identically distributed (IID)
sensing matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05317</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05317</id><created>2015-03-18</created><authors><author><keyname>Jensen</keyname><forenames>Andreas Schmidt</forenames></author></authors><title>Model Checking AORTA: Verification of Organization-Aware Agents</title><categories>cs.MA</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As agent systems grow larger and more complex, there is an increasing need to
formally verify them. Furthermore, it is often suggested that complex systems
can be regulated using organizational models, imposing constraints on the
agents in the systems. Agents that can understand the organizational model and
constraints in a system is said to be organization-aware. This paper is
concerned with verification of organization-aware agents. We show how agents
using AORTA, a framework for making agents organization-aware, can be formally
verified using an extended version of the Agent Java PathFinder (AJPF), a model
checking system designed specifically for agent programming languages. We
integrate AORTA with the Agent Infrastructure Layer (AIL), which is an
intermediate layer on top of which APLs can be implemented, and use our
extension of AJPF to verify a system of agents aiming to write a paper together
by using an organization for coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05338</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05338</id><created>2015-03-18</created><authors><author><keyname>Vamanan</keyname><forenames>Balajee</forenames></author><author><keyname>Sohail</keyname><forenames>Hamza Bin</forenames></author><author><keyname>Hasan</keyname><forenames>Jahangir</forenames></author><author><keyname>Vijaykumar</keyname><forenames>T. N.</forenames></author></authors><title>TimeTrader: Exploiting Latency Tail to Save Datacenter Energy for
  On-line Data-Intensive Applications</title><categories>cs.DC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Datacenters running on-line, data-intensive applications (OLDIs) consume
significant amounts of energy. However, reducing their energy is challenging
due to their tight response time requirements. A key aspect of OLDIs is that
each user query goes to all or many of the nodes in the cluster, so that the
overall time budget is dictated by the tail of the replies' latency
distribution; replies see latency variations both in the network and compute.
Previous work proposes to achieve load-proportional energy by slowing down the
computation at lower datacenter loads based directly on response times (i.e.,
at lower loads, the proposal exploits the average slack in the time budget
provisioned for the peak load). In contrast, we propose TimeTrader to reduce
energy by exploiting the latency slack in the sub- critical replies which
arrive before the deadline (e.g., 80% of replies are 3-4x faster than the
tail). This slack is present at all loads and subsumes the previous work's
load-related slack. While the previous work shifts the leaves' response time
distribution to consume the slack at lower loads, TimeTrader reshapes the
distribution at all loads by slowing down individual sub-critical nodes without
increasing missed deadlines. TimeTrader exploits slack in both the network and
compute budgets. Further, TimeTrader leverages Earliest Deadline First
scheduling to largely decouple critical requests from the queuing delays of
sub- critical requests which can then be slowed down without hurting critical
requests. A combination of real-system measurements and at-scale simulations
shows that without adding to missed deadlines, TimeTrader saves 15-19% and
41-49% energy at 90% and 30% loading, respectively, in a datacenter with 512
nodes, whereas previous work saves 0% and 31-37%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05352</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05352</id><created>2015-03-18</created><authors><author><keyname>Liu</keyname><forenames>Xiao-Lan</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Chen</keyname><forenames>Gui-Lin</forenames></author></authors><title>Barrier Coverage in Mobile Camera Sensor Networks with Grid-Based
  Deployment</title><categories>cs.NI</categories><comments>15 pages and 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barrier coverage is a critical issue in wireless sensor networks for many
practical applications,e.g., national border monitoring, security surveillance
and intruder detection, etc. Its aim is to detect intruders that attempt to
cross the protected region. In this paper, we study how to efficiently improve
barrier coverage using mobile camera sensors, where camera sensors are deployed
by a grid-based strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05358</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05358</id><created>2015-03-18</created><updated>2015-04-22</updated><authors><author><keyname>Shi</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author></authors><title>A Volume Correlation Subspace Detector for signals buried in unknown
  clutter</title><categories>cs.IT math.IT</categories><comments>10 pages version, accepted by ISIT'2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting the presence of target subspace signals with unknown clutters is a
well-known hard problem encountered in various signal processing applications.
Traditional methods fails to solve this problem because prior knowledge of
clutter subspace is required, which can not be obtained when target and clutter
are intimately mixed. In this paper, we propose a novel subspace detector that
can detect target signal buried in clutter without knowledge of clutter
subspace. This detector makes use of the geometrical relation between target
and clutter subspaces and is derived based upon the calculation of volume of
high dimensional geometrical objects. Moreover, the proposed detector can
accomplish the detection simultaneously with the learning processes of clutter,
a property called &quot;detecting while learning&quot;. The performance of detector was
showed by theoretical analysis and numerical simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05365</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05365</id><created>2015-03-18</created><authors><author><keyname>Perabathini</keyname><forenames>Bhanukiran</forenames></author><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author><author><keyname>Conte</keyname><forenames>Alberto</forenames></author></authors><title>Caching at the Edge: a Green Perspective for 5G Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>to be presented at IEEE International Conference on Communications
  (ICC), London, UK, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Endowed with context-awareness and proactive capabilities, caching users'
content locally at the edge of the network is able to cope with increasing data
traffic demand in 5G wireless networks. In this work, we focus on the energy
consumption aspects of cache-enabled wireless cellular networks, specifically
in terms of area power consumption (APC) and energy efficiency (EE). We assume
that both base stations (BSs) and mobile users are distributed according to
homogeneous Poisson point processes (PPPs) and we introduce a detailed power
model that takes into account caching. We study the conditions under which the
area power consumption is minimized with respect to BS transmit power, while
ensuring a certain quality of service (QoS) in terms of coverage probability.
Furthermore, we provide the optimal BS transmit power that maximizes the area
spectral efficiency per unit total power spent. The main takeaway of this paper
is that caching seems to be an energy efficient solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05366</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05366</id><created>2015-03-18</created><authors><author><keyname>BENAIJA</keyname><forenames>Khadija</forenames></author><author><keyname>KJIRI</keyname><forenames>Laila</forenames></author></authors><title>Project portfolio selection: Multi-criteria analysis and interactions
  between projects</title><categories>cs.SE</categories><journal-ref>K. Benaija, and L. Kjiri, Project portfolio selection:
  Multi-criteria analysis and interactions between projects. International
  Journal of Computer Science Issues, Vol. 11, Issue 6, 2014, pp. 134-143</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the project portfolio management, the project selection phase presents the
greatest interest. In this article, we focus on this important phase by
proposing a new method of projects selection consisting of several steps. We
propose as a first step, a classification of projects based on the three most
important criteria namely the value maximization, risk minimization and
strategic alignment. The second step is building alternatives portfolio by the
portfolio managers taking into account the classification of projects already
completed in the first step. The third and final step enables the
identification of the alternative portfolio to consider the contribution of
projects to achieve the organization objectives as well as interactions between
projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05367</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05367</id><created>2015-03-18</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>The Application of MIMO to Non-Orthogonal Multiple Access</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the application of multiple-input multiple-output (MIMO)
techniques to non-orthogonal multiple access (NOMA) systems. A new design of
precoding and detection matrices for MIMO-NOMA is proposed and its performance
is analyzed for the case with a fixed set of power allocation coefficients. To
further improve the performance gap between MIMO-NOMA and conventional
orthogonal multiple access schemes, user pairing is applied to NOMA and its
impact on the system performance is characterized. More sophisticated choices
of power allocation coefficients are also proposed to meet various quality of
service requirements. Finally computer simulation results are provided to
facilitate the performance evaluation of MIMO-NOMA and also demonstrate the
accuracy of the developed analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05377</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05377</id><created>2015-03-18</created><authors><author><keyname>Miyoshi</keyname><forenames>Naoto</forenames></author><author><keyname>Shirai</keyname><forenames>Tomoyuki</forenames></author></authors><title>Downlink Coverage Probability in a Cellular Network with Ginibre
  Deployed Base Stations and Nakagami-m Fading Channels</title><categories>cs.IT math.IT</categories><comments>WiOpt2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, spatial stochastic models based on determinantal point processes
(DPP) are studied as promising models for analysis of cellular wireless
networks. Indeed, the DPPs can express the repulsive nature of the macro base
station (BS) configuration observed in a real cellular network and have many
desirable mathematical properties to analyze the network performance. However,
almost all the prior works on the DPP based models assume the Rayleigh fading
while the spatial models based on Poisson point processes have been developed
to allow arbitrary distributions of fading/shadowing propagation effects. In
order for the DPP based model to be more promising, it is essential to extend
it to allow non-Rayleigh propagation effects. In the present paper, we propose
the downlink cellular network model where the BSs are deployed according to the
Ginibre point process, which is one of the main examples of the DPPs, over
Nakagami-m fading. For the proposed model, we derive a numerically computable
form of the coverage probability and reveal some properties of it numerically
and theoretically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05414</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05414</id><created>2015-03-18</created><updated>2015-03-30</updated><authors><author><keyname>Lin</keyname><forenames>Xinye</forenames></author><author><keyname>Xia</keyname><forenames>Mingyuan</forenames></author><author><keyname>Liu</keyname><forenames>Xue</forenames></author></authors><title>Does &quot;Like&quot; Really Mean Like? A Study of the Facebook Fake Like
  Phenomenon and an Efficient Countermeasure</title><categories>cs.SI</categories><comments>10 pages; updated the flaw details according to new Facebook API</comments><acm-class>H.4; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks help to bond people who share similar interests all over the
world. As a complement, the Facebook &quot;Like&quot; button is an efficient tool that
bonds people with the online information. People click on the &quot;Like&quot; button to
express their fondness of a particular piece of information and in turn tend to
visit webpages with high &quot;Like&quot; count. The important fact of the Like count is
that it reflects the number of actual users who &quot;liked&quot; this information.
However, according to our study, one can easily exploit the defects of the
&quot;Like&quot; button to counterfeit a high &quot;Like&quot; count. We provide a proof-of-concept
implementation of these exploits, and manage to generate 100 fake Likes in 5
minutes with a single account. We also reveal existing counterfeiting
techniques used by some online sellers to achieve unfair advantage for
promoting their products. To address this fake Like problem, we study the
varying patterns of Like count and propose an innovative fake Like detection
method based on clustering. To evaluate the effectiveness of our algorithm, we
collect the Like count history of more than 9,000 websites. Our experiments
successfully uncover 16 suspicious fake Like buyers that show abnormal Like
count increase patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05423</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05423</id><created>2015-03-18</created><authors><author><keyname>Gr&#xe4;del</keyname><forenames>Erich</forenames></author><author><keyname>Pakusa</keyname><forenames>Wied</forenames></author></authors><title>Rank logic is dead, long live rank logic!</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the search for a logic for polynomial time, we study rank logic
(FPR) which extends fixed-point logic with counting (FPC) by operators that
determine the rank of matrices over finite fields. While FPR can express most
of the known queries that separate FPC from PTIME, nearly nothing was known
about the limitations of its expressive power.
  In our first main result we show that the extensions of FPC by rank operators
over different prime fields are incomparable. This solves an open question
posed by Dawar and Holm and also implies that rank logic, in its original
definition with a distinct rank operator for every field, fails to capture
polynomial time. In particular we show that the variant of rank logic FPR* with
an operator that uniformly expresses the matrix rank over finite fields is more
expressive than FPR.
  One important step in our proof is to consider solvability logic FPS which is
the analogous extension of FPC by quantifiers which express the solvability
problem for linear equation systems over finite fields. Solvability logic can
easily be embedded into rank logic, but it is open whether it is a strict
fragment. In our second main result we give a partial answer to this question:
in the absence of counting, rank operators are strictly more expressive than
solvability quantifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05426</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05426</id><created>2015-03-18</created><authors><author><keyname>Giordano</keyname><forenames>Danilo</forenames></author><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author><author><keyname>Grimaudo</keyname><forenames>Luigi</forenames></author><author><keyname>Mellia</keyname><forenames>Marco</forenames></author><author><keyname>Baralis</keyname><forenames>Elena</forenames></author><author><keyname>Tongaonkar</keyname><forenames>Alok</forenames></author><author><keyname>Saha</keyname><forenames>Sabyasachi</forenames></author></authors><title>YouLighter: An Unsupervised Methodology to Unveil YouTube CDN Changes</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  YouTube relies on a massively distributed Content Delivery Network (CDN) to
stream the billions of videos in its catalogue. Unfortunately, very little
information about the design of such CDN is available. This, combined with the
pervasiveness of YouTube, poses a big challenge for Internet Service Providers
(ISPs), which are compelled to optimize end-users' Quality of Experience (QoE)
while having no control on the CDN decisions.
  This paper presents YouLighter, an unsupervised technique to identify changes
in the YouTube CDN. YouLighter leverages only passive measurements to cluster
co-located identical caches into edge-nodes. This automatically unveils the
structure of YouTube's CDN. Further, we propose a new metric, called
Constellation Distance, that compares the clustering obtained from two
different time snapshots, to pinpoint sudden changes. While several approaches
allow comparison between the clustering results from the same dataset, no
technique allows to measure the similarity of clusters from different datasets.
Hence, we develop a novel methodology, based on the Constellation Distance, to
solve this problem.
  By running YouLighter over 10-month long traces obtained from two ISPs in
different countries, we pinpoint both sudden changes in edge-node allocation,
and small alterations to the cache allocation policies which actually impair
the QoE that the end-users perceive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05430</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05430</id><created>2015-03-18</created><updated>2015-09-28</updated><authors><author><keyname>Parag</keyname><forenames>Toufiq</forenames></author></authors><title>What Properties are Desirable from an Electron Microscopy Segmentation
  Algorithm</title><categories>cs.CV</categories><comments>Extended version of the ICCV 2015 paper: Efficient Classifier
  Training to Minimize False Merges in Electron Microscopy Segmentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prospect of neural reconstruction from Electron Microscopy (EM) images
has been elucidated by the automatic segmentation algorithms. Although
segmentation algorithms eliminate the necessity of tracing the neurons by hand,
significant manual effort is still essential for correcting the mistakes they
make. A considerable amount of human labor is also required for annotating
groundtruth volumes for training the classifiers of a segmentation framework.
It is critically important to diminish the dependence on human interaction in
the overall reconstruction system. This study proposes a novel classifier
training algorithm for EM segmentation aimed to reduce the amount of manual
effort demanded by the groundtruth annotation and error refinement tasks.
Instead of using an exhaustive pixel level groundtruth, an active learning
algorithm is proposed for sparse labeling of pixel and boundaries of
superpixels. Because over-segmentation errors are in general more tolerable and
easier to correct than the under-segmentation errors, our algorithm is designed
to prioritize minimization of false-merges over false-split mistakes. Our
experiments on both 2D and 3D data suggest that the proposed method yields
segmentation outputs that are more amenable to neural reconstruction than those
of existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05432</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05432</id><created>2015-03-03</created><updated>2015-08-08</updated><authors><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Varma</keyname><forenames>Rohan</forenames></author><author><keyname>Sandryhaila</keyname><forenames>Aliaksei</forenames></author><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Discrete Signal Processing on Graphs: Sampling Theory</title><categories>cs.IT cs.SI math.IT</categories><comments>To appear in IEEE T-SP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a sampling theory for signals that are supported on either
directed or undirected graphs. The theory follows the same paradigm as
classical sampling theory. We show that perfect recovery is possible for graph
signals bandlimited under the graph Fourier transform. The sampled signal
coefficients form a new graph signal, whose corresponding graph structure
preserves the first-order difference of the original graph signal. For general
graphs, an optimal sampling operator based on experimentally designed sampling
is proposed to guarantee perfect recovery and robustness to noise; for graphs
whose graph Fourier transforms are frames with maximal robustness to erasures
as well as for Erd\H{o}s-R\'enyi graphs, random sampling leads to perfect
recovery with high probability. We further establish the connection to the
sampling theory of finite discrete-time signal processing and previous work on
signal recovery on graphs. To handle full-band graph signals, we propose a
graph filter bank based on sampling theory on graphs. Finally, we apply the
proposed sampling theory to semi-supervised classification on online blogs and
digit images, where we achieve similar or better performance with fewer labeled
samples compared to previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05434</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05434</id><created>2015-03-17</created><authors><author><keyname>Harshan</keyname><forenames>J.</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>Compressed Differential Erasure Codes for Efficient Archival of
  Versioned Data</title><categories>cs.IT cs.DC math.IT</categories><comments>16 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of storing an archive of versioned data
in a reliable and efficient manner in distributed storage systems. We propose a
new storage technique called differential erasure coding (DEC) where the
differences (deltas) between subsequent versions are stored rather than the
whole objects, akin to a typical delta encoding technique. However, unlike
delta encoding techniques, DEC opportunistically exploits the sparsity (i.e.,
when the differences between two successive versions have few non-zero entries)
in the updates to store the deltas using compressed sensing techniques applied
with erasure coding. We first show that DEC provides significant savings in the
storage size for versioned data whenever the update patterns are characterized
by in-place alterations. Subsequently, we propose a practical DEC framework so
as to reap storage size benefits against not just in-place alterations but also
real-world update patterns such as insertions and deletions that alter the
overall data sizes. We conduct experiments with several synthetic workloads to
demonstrate that the practical variant of DEC provides significant reductions
in storage overhead (up to 60\% depending on the workload) compared to baseline
storage system which incorporates concepts from Rsync, a delta encoding
technique to store and synchronize data across a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05443</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05443</id><created>2015-03-18</created><updated>2015-03-19</updated><authors><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Noyons</keyname><forenames>Ed</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author></authors><title>Can we track the geography of surnames based on bibliographic data?</title><categories>cs.DL</categories><comments>Paper accepted for oral presentation at the ISSI 2015 Congress.
  Version 2 corrects the reference to the paper by Waltman &amp; van Eck (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the possibility of using bibliographic databases for
tracking the geographic origin of surnames. Surnames are used as a proxy to
determine the ethnic, genetic or geographic origin of individuals in many
fields such as Genetics or Demography; however they could also be used for
bibliometric purposes such as the analysis of scientific migration flows. Here
we present two relevant methodologies for determining the most probable country
to which a surname could be assigned. The first methodology assigns surnames
based on the most common country that can be assigned to a surname and the
Kullback-Liebler divergence measure. The second method uses the Gini Index to
evaluate the assignment of surnames to countries. We test both methodologies
with control groups and conclude that, despite needing further analysis on its
validity; these methodologies already show promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05445</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05445</id><created>2015-03-18</created><authors><author><keyname>David</keyname><forenames>Cristina</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Matt</forenames></author></authors><title>Danger Invariants</title><categories>cs.PL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static analysers search for overapproximating proofs of safety commonly known
as safety invariants. Fundamentally, such analysers summarise traces into sets
of states, thus trading the ability to distinguish traces for computational
tractability. Conversely, static bug finders (e.g. Bounded Model Checking) give
evidence for the failure of an assertion in the form of a counterexample, which
can be inspected by the user. However, static bug finders fail to scale when
analysing programs with bugs that require many iterations of a loop as the
computational effort grows exponentially with the depth of the bug. We propose
a novel approach for finding bugs, which delivers the performance of abstract
interpretation together with the concrete precision of BMC. To do this, we
introduce the concept of danger invariants -- the dual to safety invariants.
Danger invariants summarise sets of traces that are guaranteed to reach an
error state. This summarisation allows us to find deep bugs without false
alarms and without explicitly unwinding loops. We present a second-order
formulation of danger invariants and use the Second-Order SAT solver described
in previous work to compute danger invariants for intricate programs taken from
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05448</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05448</id><created>2015-03-18</created><updated>2015-09-29</updated><authors><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>A Transfer Learning Approach for Cache-Enabled Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>some small fixes in notation</comments><doi>10.1109/WIOPT.2015.7151068</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locally caching contents at the network edge constitutes one of the most
disruptive approaches in $5$G wireless networks. Reaping the benefits of edge
caching hinges on solving a myriad of challenges such as how, what and when to
strategically cache contents subject to storage constraints, traffic load,
unknown spatio-temporal traffic demands and data sparsity. Motivated by this,
we propose a novel transfer learning-based caching procedure carried out at
each small cell base station. This is done by exploiting the rich contextual
information (i.e., users' content viewing history, social ties, etc.) extracted
from device-to-device (D2D) interactions, referred to as source domain. This
prior information is incorporated in the so-called target domain where the goal
is to optimally cache strategic contents at the small cells as a function of
storage, estimated content popularity, traffic load and backhaul capacity. It
is shown that the proposed approach overcomes the notorious data sparsity and
cold-start problems, yielding significant gains in terms of users'
quality-of-experience (QoE) and backhaul offloading, with gains reaching up to
$22\%$ in a setting consisting of four small cell base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05451</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05451</id><created>2015-03-18</created><updated>2015-06-07</updated><authors><author><keyname>Muelling</keyname><forenames>Katharina</forenames></author><author><keyname>Venkatraman</keyname><forenames>Arun</forenames></author><author><keyname>Valois</keyname><forenames>Jean-Sebastien</forenames></author><author><keyname>Downey</keyname><forenames>John</forenames></author><author><keyname>Weiss</keyname><forenames>Jeffrey</forenames></author><author><keyname>Javdani</keyname><forenames>Shervin</forenames></author><author><keyname>Hebert</keyname><forenames>Martial</forenames></author><author><keyname>Schwartz</keyname><forenames>Andrew B.</forenames></author><author><keyname>Collinger</keyname><forenames>Jennifer L.</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Autonomy Infused Teleoperation with Application to BCI Manipulation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robot teleoperation systems face a common set of challenges including
latency, low-dimensional user commands, and asymmetric control inputs. User
control with Brain-Computer Interfaces (BCIs) exacerbates these problems
through especially noisy and erratic low-dimensional motion commands due to the
difficulty in decoding neural activity. We introduce a general framework to
address these challenges through a combination of computer vision, user intent
inference, and arbitration between the human input and autonomous control
schemes. Adjustable levels of assistance allow the system to balance the
operator's capabilities and feelings of comfort and control while compensating
for a task's difficulty. We present experimental results demonstrating
significant performance improvement using the shared-control assistance
framework on adapted rehabilitation benchmarks with two subjects implanted with
intracortical brain-computer interfaces controlling a seven degree-of-freedom
robotic manipulator as a prosthetic. Our results further indicate that shared
assistance mitigates perceived user difficulty and even enables successful
performance on previously infeasible tasks. We showcase the extensibility of
our architecture with applications to quality-of-life tasks such as opening a
door, pouring liquids from containers, and manipulation with novel objects in
densely cluttered environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05456</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05456</id><created>2015-03-17</created><updated>2015-09-09</updated><authors><author><keyname>Cardinali</keyname><forenames>Ilaria</forenames></author><author><keyname>Giuzzi</keyname><forenames>Luca</forenames></author></authors><title>Minimum distance of Symplectic Grassmann codes</title><categories>cs.IT math.CO math.IT</categories><comments>Revised contents and biblography</comments><msc-class>94B05, 51A50</msc-class><journal-ref>Linear Algebra and its Applications 488: 124-134 (2016)</journal-ref><doi>10.1016/j.laa.2015.09.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Symplectic Grassmann codes as projective codes defined by
symplectic Grassmannians, in analogy with the orthogonal Grassmann codes
introduced in [4]. Note that the Lagrangian-Grassmannian codes are a special
class of Symplectic Grassmann codes. We describe the weight enumerator of the
Lagrangian--Grassmannian codes of rank $2$ and $3$ and we determine the minimum
distance of the line Symplectic Grassmann codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05458</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05458</id><created>2015-03-16</created><updated>2015-06-20</updated><authors><author><keyname>Pawlick</keyname><forenames>Jeffrey</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Deception by Design: Evidence-Based Signaling Games for Network Defense</title><categories>cs.CR</categories><comments>To be presented at Workshop on the Economics of Information Security
  (WEIS) 2015, Delft University of Technology, The Netherlands</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deception plays a critical role in the financial industry, online markets,
national defense, and countless other areas. Understanding and harnessing
deception - especially in cyberspace - is both crucial and difficult. Recent
work in this area has used game theory to study the roles of incentives and
rational behavior. Building upon this work, we employ a game-theoretic model
for the purpose of mechanism design. Specifically, we study a defensive use of
deception: implementation of honeypots for network defense. How does the design
problem change when an adversary develops the ability to detect honeypots? We
analyze two models: cheap-talk games and an augmented version of those games
that we call cheap-talk games with evidence, in which the receiver can detect
deception with some probability. Our first contribution is this new model for
deceptive interactions. We show that the model includes traditional signaling
games and complete information games as special cases. We also demonstrate
numerically that deception detection sometimes eliminate pure-strategy
equilibria. Finally, we present the surprising result that the utility of a
deceptive defender can sometimes increase when an adversary develops the
ability to detect deception. These results apply concretely to network defense.
They are also general enough for the large and critical body of strategic
interactions that involve deception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05464</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05464</id><created>2015-03-18</created><updated>2015-06-26</updated><authors><author><keyname>Rouet</keyname><forenames>Fran&#xe7;ois-Henry</forenames></author><author><keyname>Li</keyname><forenames>Xiaoye S.</forenames></author><author><keyname>Ghysels</keyname><forenames>Pieter</forenames></author><author><keyname>Napov</keyname><forenames>Artem</forenames></author></authors><title>A distributed-memory package for dense Hierarchically Semi-Separable
  matrix computations using randomization</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed-memory library for computations with dense
structured matrices. A matrix is considered structured if its off-diagonal
blocks can be approximated by a rank-deficient matrix with low numerical rank.
Here, we use Hierarchically Semi-Separable representations (HSS). Such matrices
appear in many applications, e.g., finite element methods, boundary element
methods, etc. Exploiting this structure allows for fast solution of linear
systems and/or fast computation of matrix-vector products, which are the two
main building blocks of matrix computations. The compression algorithm that we
use, that computes the HSS form of an input dense matrix, relies on randomized
sampling with a novel adaptive sampling mechanism. We discuss the
parallelization of this algorithm and also present the parallelization of
structured matrix-vector product, structured factorization and solution
routines. The efficiency of the approach is demonstrated on large problems from
different academic and industrial applications, on up to 8,000 cores.
  This work is part of a more global effort, the STRUMPACK (STRUctured Matrices
PACKage) software package for computations with sparse and dense structured
matrices. Hence, although useful on their own right, the routines also
represent a step in the direction of a distributed-memory sparse solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05471</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05471</id><created>2015-03-18</created><authors><author><keyname>Doroshin</keyname><forenames>Danila</forenames></author><author><keyname>Yamshinin</keyname><forenames>Alexander</forenames></author><author><keyname>Lubimov</keyname><forenames>Nikolay</forenames></author><author><keyname>Nastasenko</keyname><forenames>Marina</forenames></author><author><keyname>Kotov</keyname><forenames>Mikhail</forenames></author><author><keyname>Tkachenko</keyname><forenames>Maxim</forenames></author></authors><title>Shared latent subspace modelling within Gaussian-Binary Restricted
  Boltzmann Machines for NIST i-Vector Challenge 2014</title><categories>cs.LG cs.NE cs.SD stat.ML</categories><comments>5 pages, 3 figures, submitted to Interspeech 2015</comments><msc-class>62M45</msc-class><acm-class>I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to speaker subspace modelling based on
Gaussian-Binary Restricted Boltzmann Machines (GRBM). The proposed model is
based on the idea of shared factors as in the Probabilistic Linear Discriminant
Analysis (PLDA). GRBM hidden layer is divided into speaker and channel factors,
herein the speaker factor is shared over all vectors of the speaker. Then
Maximum Likelihood Parameter Estimation (MLE) for proposed model is introduced.
Various new scoring techniques for speaker verification using GRBM are
proposed. The results for NIST i-vector Challenge 2014 dataset are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05477</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05477</id><created>2015-03-18</created><authors><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Lavery</keyname><forenames>Domanic</forenames></author><author><keyname>Maher</keyname><forenames>Robert</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author></authors><title>Replacing the Soft FEC Limit Paradigm in the Design of Optical
  Communication Systems</title><categories>cs.IT math.IT physics.optics</categories><doi>10.1109/JLT.2015.2450537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FEC limit paradigm is the prevalent practice for designing optical
communication systems to attain a certain bit-error rate (BER) without forward
error correction (FEC). This practice assumes that there is an FEC code that
will reduce the BER after decoding to the desired level. In this paper, we
challenge this practice and show that the concept of a channel-independent FEC
limit is invalid for soft-decision bit-wise decoding. It is shown that for low
code rates and high order modulation formats, the use of the soft FEC limit
paradigm can underestimate the spectral efficiencies by up to 20%. A better
predictor for the BER after decoding is the generalized mutual information,
which is shown to give consistent post-FEC BER predictions across different
channel conditions and modulation formats. Extensive optical full-field
simulations and experiments are carried out in both the linear and nonlinear
transmission regimes to confirm the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05479</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05479</id><created>2015-03-18</created><updated>2015-10-26</updated><authors><author><keyname>Zheng</keyname><forenames>Qinqing</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author></authors><title>Interpolating Convex and Non-Convex Tensor Decompositions via the
  Subspace Norm</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a low-rank tensor from its noisy
observation. Previous work has shown a recovery guarantee with signal to noise
ratio $O(n^{\lceil K/2 \rceil /2})$ for recovering a $K$th order rank one
tensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper,
we first improve this bound to $O(n^{K/4})$ by a much simpler approach, but
with a more careful analysis. Then we propose a new norm called the subspace
norm, which is based on the Kronecker products of factors obtained by the
proposed simple estimator. The imposed Kronecker structure allows us to show a
nearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound, in which the parameter $H$
controls the blend from the non-convex estimator to mode-wise nuclear norm
minimization. Furthermore, we empirically demonstrate that the subspace norm
achieves the nearly ideal denoising performance even with $H=O(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05493</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05493</id><created>2015-03-18</created><authors><author><keyname>Abdullah</keyname><forenames>M. H. Khan</forenames></author><author><keyname>Srivastava</keyname><forenames>Reena</forenames></author></authors><title>Testability Measurement Model for Object Oriented Design (TMMOOD)</title><categories>cs.SE</categories><comments>10</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 7, No 1, February 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring testability early in the development life cycle especially at
design phase is a criterion of crucial importance to software designers,
developers, quality controllers and practitioners. However, most of the
mechanism available for testability measurement may be used in the later phases
of development life cycle. Early estimation of testability, absolutely at
design phase helps designers to improve their designs before the coding starts.
Practitioners regularly advocate that testability should be planned early in
design phase. Testability measurement early in design phase is greatly
emphasized in this study; hence, considered significant for the delivery of
quality software. As a result, it extensively reduces rework during and after
implementation, as well as facilitate for design effective test plans, better
project and resource planning in a practical manner, with a focus on the design
phase. An effort has been put forth in this paper to recognize the key factors
contributing in testability measurement at design phase. Additionally,
testability measurement model is developed to quantify software testability at
design phase. Furthermore, the relationship of Testability with these factors
has been tested and justified with the help of statistical measures. The
developed model has been validated using experimental tryout. Finally, it
incorporates the empirical validation of the testability measurement model as
the authors most important contribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05496</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05496</id><created>2015-03-18</created><authors><author><keyname>Ekici</keyname><forenames>Burak</forenames></author></authors><title>IMP with exceptions over decorated logic</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we separately design the decorated logic with respect to the
state and the exception effects. Then, we combine two logics to be able to
establish small-step semantics of IMP imperative language with exceptional
abilities, in a decorated setting. We implement the decorated framework in Coq
and certify program equivalence proofs written in that context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05501</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05501</id><created>2015-03-18</created><authors><author><keyname>Gabbay</keyname><forenames>D. M.</forenames></author><author><keyname>Rodrigues</keyname><forenames>O.</forenames></author></authors><title>Probabilistic Argumentation. An Equational Approach</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a generic way to add any new feature to a system. It involves 1)
identifying the basic units which build up the system and 2) introducing the
new feature to each of these basic units.
  In the case where the system is argumentation and the feature is
probabilistic we have the following. The basic units are: a. the nature of the
arguments involved; b. the membership relation in the set S of arguments; c.
the attack relation; and d. the choice of extensions.
  Generically to add a new aspect (probabilistic, or fuzzy, or temporal, etc)
to an argumentation network &lt;S,R&gt; can be done by adding this feature to each
component a-d. This is a brute-force method and may yield a non-intuitive or
meaningful result.
  A better way is to meaningfully translate the object system into another
target system which does have the aspect required and then let the target
system endow the aspect on the initial system. In our case we translate
argumentation into classical propositional logic and get probabilistic
argumentation from the translation.
  Of course what we get depends on how we translate.
  In fact, in this paper we introduce probabilistic semantics to abstract
argumentation theory based on the equational approach to argumentation
networks. We then compare our semantics with existing proposals in the
literature including the approaches by M. Thimm and by A. Hunter. Our
methodology in general is discussed in the conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05502</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05502</id><created>2015-03-18</created><updated>2015-05-30</updated><authors><author><keyname>Paldino</keyname><forenames>Silvia</forenames></author><author><keyname>Bojic</keyname><forenames>Iva</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author><author><keyname>Gonzalez</keyname><forenames>Marta C.</forenames></author></authors><title>Urban Magnetism Through The Lens of Geo-tagged Photography</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 10 figures, 6 tables</comments><msc-class>91D30</msc-class><journal-ref>EPJ Data Science 2015, 4:5</journal-ref><doi>10.1140/epjds/s13688-015-0043-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing trend of people leaving digital traces through social
media. This reality opens new horizons for urban studies. With this kind of
data, researchers and urban planners can detect many aspects of how people live
in cities and can also suggest how to transform cities into more efficient and
smarter places to live in. In particular, their digital trails can be used to
investigate tastes of individuals, and what attracts them to live in a
particular city or to spend their vacation there. In this paper we propose an
unconventional way to study how people experience the city, using information
from geotagged photographs that people take at different locations. We compare
the spatial behavior of residents and tourists in 10 most photographed cities
all around the world. The study was conducted on both a global and local level.
On the global scale we analyze the 10 most photographed cities and measure how
attractive each city is for people visiting it from other cities within the
same country or from abroad. For the purpose of our analysis we construct the
users mobility network and measure the strength of the links between each pair
of cities as a level of attraction of people living in one city (i.e., origin)
to the other city (i.e., destination). On the local level we study the spatial
distribution of user activity and identify the photographed hotspots inside
each city. The proposed methodology and the results of our study are a low cost
mean to characterize a touristic activity within a certain location and can
help in urban organization to strengthen their touristic potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05508</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05508</id><created>2015-03-18</created><authors><author><keyname>Bekkouche</keyname><forenames>Mohammed</forenames></author></authors><title>Exploration of the scalability of LocFaults approach for error
  localization with While-loops programs</title><categories>cs.AI cs.SE</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model checker can produce a trace of counterexample, for an erroneous
program, which is often long and difficult to understand. In general, the part
about the loops is the largest among the instructions in this trace. This makes
the location of errors in loops critical, to analyze errors in the overall
program. In this paper, we explore the scala-bility capabilities of LocFaults,
our error localization approach exploiting paths of CFG(Control Flow Graph)
from a counterexample to calculate the MCDs (Minimal Correction Deviations),
and MCSs (Minimal Correction Subsets) from each found MCD. We present the times
of our approach on programs with While-loops unfolded b times, and a number of
deviated conditions ranging from 0 to n. Our preliminary results show that the
times of our approach, constraint-based and flow-driven, are better compared to
BugAssist which is based on SAT and transforms the entire program to a Boolean
formula, and further the information provided by LocFaults is more expressive
for the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05521</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05521</id><created>2015-03-18</created><authors><author><keyname>Imbiriba</keyname><forenames>Tales</forenames><affiliation>Federal University of Santa Catarina, Florian&#xf3;polis, SC, Brazil</affiliation></author><author><keyname>Bermudez</keyname><forenames>Jos&#xe9; Carlos Moreira</forenames><affiliation>Federal University of Santa Catarina, Florian&#xf3;polis, SC, Brazil</affiliation></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames><affiliation>Universit&#xe9; de Nice Sophia-Antipolis, CNRS, Nice, France</affiliation></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames><affiliation>Universit&#xe9; de Toulouse, IRIT-ENSEEIHT, CNRS, Toulouse, France</affiliation></author></authors><title>Nonparametric Detection of Nonlinearly Mixed Pixels and Endmember
  Estimation in Hyperspectral Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixing phenomena in hyperspectral images depend on a variety of factors such
as the resolution of observation devices, the properties of materials, and how
these materials interact with incident light in the scene. Different parametric
and nonparametric models have been considered to address hyperspectral unmixing
problems. The simplest one is the linear mixing model. Nevertheless, it has
been recognized that mixing phenomena can also be nonlinear. The corresponding
nonlinear analysis techniques are necessarily more challenging and complex than
those employed for linear unmixing. Within this context, it makes sense to
detect the nonlinearly mixed pixels in an image prior to its analysis, and then
employ the simplest possible unmixing technique to analyze each pixel. In this
paper, we propose a technique for detecting nonlinearly mixed pixels. The
detection approach is based on the comparison of the reconstruction errors
using both a Gaussian process regression model and a linear regression model.
The two errors are combined into a detection statistics for which a probability
density function can be reasonably approximated. We also propose an iterative
endmember extraction algorithm to be employed in combination with the detection
algorithm. The proposed Detect-then-Unmix strategy, which consists of
extracting endmembers, detecting nonlinearly mixed pixels and unmixing, is
tested with synthetic and real images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05522</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05522</id><created>2015-03-18</created><updated>2016-01-25</updated><authors><author><keyname>Karimi</keyname><forenames>Fariba</forenames></author><author><keyname>Bohlin</keyname><forenames>Ludvig</forenames></author><author><keyname>Samoilenko</keyname><forenames>Anna</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author></authors><title>Mapping bilateral information interests using the activity of Wikipedia
  editors</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>11 pages, 3 figures in Palgrave Communications 1 (2015)</comments><doi>10.1057/palcomms.2015.41</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We live in a global village where electronic communication has eliminated the
geographical barriers of information exchange. The road is now open to
worldwide convergence of information interests, shared values, and
understanding. Nevertheless, interests still vary between countries around the
world. This raises important questions about what today's world map of in-
formation interests actually looks like and what factors cause the barriers of
information exchange between countries. To quantitatively construct a world map
of information interests, we devise a scalable statistical model that
identifies countries with similar information interests and measures the
countries' bilateral similarities. From the similarities we connect countries
in a global network and find that countries can be mapped into 18 clusters with
similar information interests. Through regression we find that language and
religion best explain the strength of the bilateral ties and formation of
clusters. Our findings provide a quantitative basis for further studies to
better understand the complex interplay between shared interests and conflict
on a global scale. The methodology can also be extended to track changes over
time and capture important trends in global information exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05526</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05526</id><created>2015-03-18</created><authors><author><keyname>Rabenoro</keyname><forenames>Tsirizo</forenames><affiliation>SAMM</affiliation></author><author><keyname>Lacaille</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Interpretable Aircraft Engine Diagnostic via Expert Indicator
  Aggregation</title><categories>stat.ML cs.LG math.ST stat.AP stat.TH</categories><comments>arXiv admin note: substantial text overlap with arXiv:1408.6214,
  arXiv:1409.4747, arXiv:1407.0880</comments><proxy>ccsd</proxy><journal-ref>Transactions on Machine Learning and Data Mining, 2014, 7 (2),
  pp.39-64</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting early signs of failures (anomalies) in complex systems is one of
the main goal of preventive maintenance. It allows in particular to avoid
actual failures by (re)scheduling maintenance operations in a way that
optimizes maintenance costs. Aircraft engine health monitoring is one
representative example of a field in which anomaly detection is crucial.
Manufacturers collect large amount of engine related data during flights which
are used, among other applications, to detect anomalies. This article
introduces and studies a generic methodology that allows one to build automatic
early signs of anomaly detection in a way that builds upon human expertise and
that remains understandable by human operators who make the final maintenance
decision. The main idea of the method is to generate a very large number of
binary indicators based on parametric anomaly scores designed by experts,
complemented by simple aggregations of those scores. A feature selection method
is used to keep only the most discriminant indicators which are used as inputs
of a Naive Bayes classifier. This give an interpretable classifier based on
interpretable anomaly detectors whose parameters have been optimized indirectly
by the selection process. The proposed methodology is evaluated on simulated
data designed to reproduce some of the anomaly types observed in real world
engines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05528</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05528</id><created>2015-03-18</created><updated>2015-06-08</updated><authors><author><keyname>Newson</keyname><forenames>Alasdair</forenames><affiliation>LTCI</affiliation></author><author><keyname>Almansa</keyname><forenames>Andr&#xe9;s</forenames><affiliation>LTCI</affiliation></author><author><keyname>Fradet</keyname><forenames>Matthieu</forenames></author><author><keyname>Gousseau</keyname><forenames>Yann</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Patrick</forenames></author></authors><title>Video Inpainting of Complex Scenes</title><categories>cs.CV cs.MM math.NA</categories><proxy>ccsd</proxy><journal-ref>SIAM Journal on Imaging Sciences, Society for Industrial and
  Applied Mathematics, 2014, 7 (4), pp.1993-2019</journal-ref><doi>10.1137/140954933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an automatic video inpainting algorithm which relies on the
optimisation of a global, patch-based functional. Our algorithm is able to deal
with a variety of challenging situations which naturally arise in video
inpainting, such as the correct reconstruction of dynamic textures, multiple
moving objects and moving background. Furthermore, we achieve this in an order
of magnitude less execution time with respect to the state-of-the-art. We are
also able to achieve good quality results on high definition videos. Finally,
we provide specific algorithmic details to make implementation of our algorithm
as easy as possible. The resulting algorithm requires no segmentation or manual
input other than the definition of the inpainting mask, and can deal with a
wider variety of situations than is handled by previous work. 1. Introduction.
Advanced image and video editing techniques are increasingly common in the
image processing and computer vision world, and are also starting to be used in
media entertainment. One common and difficult task closely linked to the world
of video editing is image and video &quot; inpainting &quot;. Generally speaking, this is
the task of replacing the content of an image or video with some other content
which is visually pleasing. This subject has been extensively studied in the
case of images, to such an extent that commercial image inpainting products
destined for the general public are available, such as Photoshop's &quot; Content
Aware fill &quot; [1]. However, while some impressive results have been obtained in
the case of videos, the subject has been studied far less extensively than
image inpainting. This relative lack of research can largely be attributed to
high time complexity due to the added temporal dimension. Indeed, it has only
very recently become possible to produce good quality inpainting results on
high definition videos, and this only in a semi-automatic manner. Nevertheless,
high-quality video inpainting has many important and useful applications such
as film restoration, professional post-production in cinema and video editing
for personal use. For this reason, we believe that an automatic, generic video
inpainting algorithm would be extremely useful for both academic and
professional communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05530</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05530</id><created>2015-03-18</created><authors><author><keyname>Bekkouche</keyname><forenames>Mohammed</forenames></author></authors><title>Exploration of the scalability of LocFaults</title><categories>cs.AI cs.SE</categories><comments>in French</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model checker can produce a trace of counterexample, for an erroneous
program, which is often long and difficult to understand. In general, the part
about the loops is the largest among the instructions in this trace. This makes
the location of errors in loops critical, to analyze errors in the overall
program. In this paper, we explore the scalability capabilities of LocFaults,
our error localization approach exploiting paths of CFG(Control Flow Graph)
from a counterexample to calculate the MCDs (Minimal Correction Deviations),
and MCSs (Minimal Correction Subsets) from each found MCD. We present the times
of our approach on programs with While-loops unfolded b times, and a number of
deviated conditions ranging from 0 to n. Our preliminary results show that the
times of our approach, constraint-based and flow-driven, are better compared to
BugAssist which is based on SAT and transforms the entire program to a Boolean
formula, and further the information provided by LocFaults is more expressive
for the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05533</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05533</id><created>2015-03-18</created><authors><author><keyname>Kelemen</keyname><forenames>Z&#xe1;dor D&#xe1;niel</forenames></author></authors><title>Fundamental Analysis of a Developer Support Chat Log for Identifying
  Process Improvement Opportunities</title><categories>cs.SE</categories><comments>13 pages</comments><report-no>NNGTR-CTUQM-1501</report-no><doi>10.13140/RG.2.1.1465.0407</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report analysis of a support chat log of a development team is shown.
Developer support chat is used to provide internal support to other development
teams. The report shows how a fundamental data analysis helped to identify gaps
and action items to boost performance of a development team by reducing time
spent on developer support chat and minimizing interrupts from other developer
teams. The report also shows an example of how a root cause analysis can be
supported by simple data analysis in finding process improvement opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05543</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05543</id><created>2015-03-18</created><authors><author><keyname>Alemi</keyname><forenames>Alexander A</forenames></author><author><keyname>Ginsparg</keyname><forenames>Paul</forenames></author></authors><title>Text Segmentation based on Semantic Word Embeddings</title><categories>cs.CL cs.IR</categories><comments>10 pages, 4 figures. KDD2015 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the use of semantic word embeddings in text segmentation
algorithms, including the C99 segmentation algorithm and new algorithms
inspired by the distributed word vector representation. By developing a general
framework for discussing a class of segmentation objectives, we study the
effectiveness of greedy versus exact optimization approaches and suggest a new
iterative refinement technique for improving the performance of greedy
strategies. We compare our results to known benchmarks, using known metrics. We
demonstrate state-of-the-art performance for an untrained method with our
Content Vector Segmentation (CVS) on the Choi test set. Finally, we apply the
segmentation procedure to an in-the-wild dataset consisting of text extracted
from scholarly articles in the arXiv.org database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05567</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05567</id><created>2015-03-18</created><authors><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author><author><keyname>Powell</keyname><forenames>Warren</forenames></author></authors><title>The Knowledge Gradient Policy Using A Sparse Additive Belief Model</title><categories>stat.ML cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a sequential learning policy for noisy discrete global
optimization and ranking and selection (R\&amp;S) problems with high dimensional
sparse belief functions, where there are hundreds or even thousands of
features, but only a small portion of these features contain explanatory power.
We aim to identify the sparsity pattern and select the best alternative before
the finite budget is exhausted. We derive a knowledge gradient policy for
sparse linear models (KGSpLin) with group Lasso penalty. This policy is a
unique and novel hybrid of Bayesian R\&amp;S with frequentist learning.
Particularly, our method naturally combines B-spline basis expansion and
generalizes to the nonparametric additive model (KGSpAM) and functional ANOVA
model. Theoretically, we provide the estimation error bounds of the posterior
mean estimate and the functional estimate. Controlled experiments show that the
algorithm efficiently learns the correct set of nonzero parameters even when
the model is imbedded with hundreds of dummy parameters. Also it outperforms
the knowledge gradient for a linear model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05570</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05570</id><created>2015-03-18</created><authors><author><keyname>Baumer</keyname><forenames>Ben</forenames></author></authors><title>A Data Science Course for Undergraduates: Thinking with Data</title><categories>stat.OT cs.CY stat.CO</categories><comments>21 pages total including supplementary materials</comments><msc-class>62-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data science is an emerging interdisciplinary field that combines elements of
mathematics, statistics, computer science, and knowledge in a particular
application domain for the purpose of extracting meaningful information from
the increasingly sophisticated array of data available in many settings. These
data tend to be non-traditional, in the sense that they are often live, large,
complex, and/or messy. A first course in statistics at the undergraduate level
typically introduces students with a variety of techniques to analyze small,
neat, and clean data sets. However, whether they pursue more formal training in
statistics or not, many of these students will end up working with data that is
considerably more complex, and will need facility with statistical computing
techniques. More importantly, these students require a framework for thinking
structurally about data. We describe an undergraduate course in a liberal arts
environment that provides students with the tools necessary to apply data
science. The course emphasizes modern, practical, and useful skills that cover
the full data analysis spectrum, from asking an interesting question to
acquiring, managing, manipulating, processing, querying, analyzing, and
visualizing data, as well communicating findings in written, graphical, and
oral forms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05571</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05571</id><created>2015-03-18</created><updated>2015-03-23</updated><authors><author><keyname>Alain</keyname><forenames>Guillaume</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author><author><keyname>Thibodeau-Laufer</keyname><forenames>Eric</forenames></author><author><keyname>Zhang</keyname><forenames>Saizheng</forenames></author><author><keyname>Vincent</keyname><forenames>Pascal</forenames></author></authors><title>GSNs : Generative Stochastic Networks</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.1091</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel training principle for probabilistic models that is an
alternative to maximum likelihood. The proposed Generative Stochastic Networks
(GSN) framework is based on learning the transition operator of a Markov chain
whose stationary distribution estimates the data distribution. Because the
transition distribution is a conditional distribution generally involving a
small move, it has fewer dominant modes, being unimodal in the limit of small
moves. Thus, it is easier to learn, more like learning to perform supervised
function approximation, with gradients that can be obtained by
back-propagation. The theorems provided here generalize recent work on the
probabilistic interpretation of denoising auto-encoders and provide an
interesting justification for dependency networks and generalized
pseudolikelihood (along with defining an appropriate joint distribution and
sampling mechanism, even when the conditionals are not consistent). We study
how GSNs can be used with missing inputs and can be used to sample subsets of
variables given the rest. Successful experiments are conducted, validating
these theoretical results, on two image datasets and with a particular
architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows
training to proceed with backprop, without the need for layerwise pretraining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05608</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05608</id><created>2015-03-18</created><authors><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Greedy Algorithms make Efficient Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study mechanisms that use greedy allocation rules and pay-your-bid pricing
to allocate resources subject to a matroid constraint. We show that all such
mechanisms obtain a constant fraction of the optimal welfare at any equilibrium
of bidder behavior, via a smoothness argument. This unifies numerous recent
results on the price of anarchy of simple auctions. Our results extend to
polymatroid and matching constraints, and we discuss extensions to more general
matroid intersections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05615</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05615</id><created>2015-03-18</created><updated>2015-05-07</updated><authors><author><keyname>Chang</keyname><forenames>Kai-Wei</forenames></author><author><keyname>He</keyname><forenames>He</forenames></author><author><keyname>Daum&#xe9;</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Learning to Search for Dependencies</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that a dependency parser can be built using a credit
assignment compiler which removes the burden of worrying about low-level
machine learning details from the parser implementation. The result is a simple
parser which robustly applies to many languages that provides similar
statistical and computational performance with best-to-date transition-based
parsing approaches, while avoiding various downsides including randomization,
extra feature requirements, and custom learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05619</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05619</id><created>2015-03-18</created><authors><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>3-D Statistical Channel Model for Millimeter-Wave Outdoor Mobile
  Broadband Communications</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures, ICC 2015 (London, UK, to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an omnidirectional spatial and temporal 3-dimensional
statistical channel model for 28 GHz dense urban non-line of sight
environments. The channel model is developed from 28 GHz ultrawideband
propagation measurements obtained with a 400 megachips per second broadband
sliding correlator channel sounder and highly directional, steerable horn
antennas in New York City. A 3GPP-like statistical channel model that is easy
to implement in software or hardware is developed from measured power delay
profiles and a synthesized method for providing absolute propagation delays
recovered from 3-D ray-tracing, as well as measured angle of departure and
angle of arrival power spectra. The extracted statistics are used to implement
a MATLAB-based statistical simulator that generates 3-D millimeter-wave
temporal and spatial channel coefficients that reproduce realistic impulse
responses of measured urban channels. The methods and model presented here can
be used for millimeter-wave system-wide simulations, and air interface design
and capacity analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05626</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05626</id><created>2015-03-18</created><authors><author><keyname>Pak</keyname><forenames>Myong-Chol</forenames></author></authors><title>Phrase database Approach to structural and semantic disambiguation in
  English-Korean Machine Translation</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In machine translation it is common phenomenon that machine-readable
dictionaries and standard parsing rules are not enough to ensure accuracy in
parsing and translating English phrases into Korean language, which is revealed
in misleading translation results due to consequent structural and semantic
ambiguities. This paper aims to suggest a solution to structural and semantic
ambiguities due to the idiomaticity and non-grammaticalness of phrases commonly
used in English language by applying bilingual phrase database in
English-Korean Machine Translation (EKMT). This paper firstly clarifies what
the phrase unit in EKMT is based on the definition of the English phrase,
secondly clarifies what kind of language unit can be the target of the phrase
database for EKMT, thirdly suggests a way to build the phrase database by
presenting the format of the phrase database with examples, and finally
discusses briefly the method to apply this bilingual phrase database to the
EKMT for structural and semantic disambiguation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05637</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05637</id><created>2015-03-18</created><authors><author><keyname>Tan</keyname><forenames>Yihua</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author></authors><title>Compute-Compress-and-Forward: Exploiting Asymmetry of Wireless Relay
  Networks</title><categories>cs.IT math.IT</categories><comments>23 pages, 4 figures, journal version</comments><doi>10.1109/TSP.2015.2481876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compute-and-forward (CF) harnesses interference in a wireless networkby
allowing relays to compute combinations of source messages. The computed
message combinations at relays are correlated, and so directly forwarding these
combinations to a destination generally incurs information redundancy and
spectrum inefficiency. To address this issue, we propose a novel relay
strategy, termed compute-compress-and-forward (CCF). In CCF, source messages
are encoded using nested lattice codes constructed on a chain of nested coding
and shaping lattices. A key difference of CCF from CF is an extra compressing
stage inserted in between the computing and forwarding stages of a relay, so as
to reduce the forwarding information rate of the relay. The compressing stage
at each relay consists of two operations: first to quantize the computed
message combination on an appropriately chosen lattice (referred to as a
quantization lattice), and then to take modulo on another lattice (referred to
as a modulo lattice). We study the design of the quantization and modulo
lattices and propose successive recovering algorithms to ensure the
recoverability of source messages at destination. Based on that, we formulate a
sum-rate maximization problem that is in general an NP-hard mixed integer
program. A low-complexity algorithm is proposed to give a suboptimal solution.
Numerical results are presented to demonstrate the superiority of CCF over the
existing CF schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05638</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05638</id><created>2015-03-18</created><updated>2015-09-21</updated><authors><author><keyname>Yu</keyname><forenames>Y. William</forenames></author><author><keyname>Daniels</keyname><forenames>Noah M.</forenames></author><author><keyname>Danko</keyname><forenames>David Christian</forenames></author><author><keyname>Berger</keyname><forenames>Bonnie</forenames></author></authors><title>Entropy-scaling search of massive biological data</title><categories>cs.DS q-bio.GN</categories><comments>Including supplement: 41 pages, 6 figures, 4 tables, 1 box</comments><journal-ref>Cell Systems, Volume 1, Issue 2, 130-140, 2015</journal-ref><doi>10.1016/j.cels.2015.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many datasets exhibit a well-defined structure that can be exploited to
design faster search tools, but it is not always clear when such acceleration
is possible. Here, we introduce a framework for similarity search based on
characterizing a dataset's entropy and fractal dimension. We prove that
searching scales in time with metric entropy (number of covering hyperspheres),
if the fractal dimension of the dataset is low, and scales in space with the
sum of metric entropy and information-theoretic entropy (randomness of the
data). Using these ideas, we present accelerated versions of standard tools,
with no loss in specificity and little loss in sensitivity, for use in three
domains---high-throughput drug screening (Ammolite, 150x speedup), metagenomics
(MICA, 3.5x speedup of DIAMOND [3,700x BLASTX]), and protein structure search
(esFragBag, 10x speedup of FragBag). Our framework can be used to achieve
&quot;compressive omics,&quot; and the general theory can be readily applied to data
science problems outside of biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05641</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05641</id><created>2015-03-18</created><updated>2015-08-12</updated><authors><author><keyname>Speidel</keyname><forenames>Leo</forenames></author><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>Community detection in directed acyclic graphs</title><categories>physics.soc-ph cs.SI</categories><comments>2 figures, 7 tables</comments><journal-ref>European Physical Journal B, 88, 203 (2015)</journal-ref><doi>10.1140/epjb/e2015-60226-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some temporal networks, most notably citation networks, are naturally
represented as directed acyclic graphs (DAGs). To detect communities in DAGs,
we propose a modularity for DAGs by defining an appropriate null model (i.e.,
randomized network) respecting the order of nodes. We implement a spectral
method to approximately maximize the proposed modularity measure and test the
method on citation networks and other DAGs. We find that the attained values of
the modularity for DAGs are similar for partitions that we obtain by maximizing
the proposed modularity (designed for DAGs), the modularity for undirected
networks and that for general directed networks. In other words, if we neglect
the order imposed on nodes (and the direction of links) in a given DAG and
maximize the conventional modularity measure, the obtained partition is close
to the optimal one in the sense of the modularity for DAGs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05642</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05642</id><created>2015-03-18</created><authors><author><keyname>Chigozie</keyname><forenames>O.</forenames></author><author><keyname>Williams</keyname><forenames>P.</forenames></author><author><keyname>Osegi</keyname><forenames>N. E.</forenames></author></authors><title>Hybrid Social Networking Application for a University Community</title><categories>cs.SI</categories><comments>7 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A hybrid social network for building social communities for a university
community is presented. The system employed the semantic ontology for an
offline/online social network site (SNS). It captures the core features of an
SNS including profile creation, friend invite/search, group formation,
chatting/messaging, blogging and voting. Three core frameworks - the peer2me
framework, SMSN semantic mobile social network and Peoplepods framework were
considered in the implementation phase. The results show remarkable matching
performance for prosumers with similar interests with relevance close to unity.
The social network was able to capture the needs of the university students by
serving as a handy direction to popular locations within the campus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05646</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05646</id><created>2015-03-19</created><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Jiang</keyname><forenames>Changjun</forenames></author><author><keyname>Yang</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Zhong</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaobo</forenames></author></authors><title>Rule Optimization for Real-Time Query Service in Software-Defined
  Internet of Vehicles</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Vehicles (IoV) has recently gained considerable attentions from
both industry and research communities since the development of communication
technology and smart city. However, a proprietary and closed way of operating
hardwares in network equipments slows down the progress of new services
deployment and extension in IoV. Moreover, the tightly coupled control and data
planes in traditional networks significantly increase the complexity and cost
of network management. By proposing a novel architecture, called
Software-Defined Internet of Vehicles (SDIV), we adopt the software-defined
network (SDN) architecture to address these problems by leveraging its
separation of the control plane from the data plane and a uniform way to
configure heterogeneous switches. However, the characteristics of IoV introduce
the very challenges in rule installation due to the limited size of Flow Tables
at OpenFlow-enabled switches which are the main component of SDN. It is
necessary to build compact Flow Tables for the scalability of IoV. Accordingly,
we develop a rule optimization approach for real-time query service in SDIV.
Specifically, we separate wired data plane from wireless data plane and use
multicast address in wireless data plane. Furthermore, we introduce a
destination-driven model in wired data plane for reducing the number of rules
at switches. Experiments show that our rule optimization strategy reduces the
number of rules while keeping the performance of data transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05650</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05650</id><created>2015-03-19</created><authors><author><keyname>Luo</keyname><forenames>Jinquan</forenames></author></authors><title>Binary sequences with three-valued cross correlations of different
  lengths</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new pairs of binary sequences with three cross correlation
values are presented. The cross correlation values are shown to be low. Finally
we present some numerical results and some open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05656</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05656</id><created>2015-03-19</created><authors><author><keyname>Vakilian</keyname><forenames>Ali</forenames></author><author><keyname>Chodpathumwan</keyname><forenames>Yodsawalai</forenames></author><author><keyname>Termehchy</keyname><forenames>Arash</forenames></author><author><keyname>Nayyeri</keyname><forenames>Amir</forenames></author></authors><title>Cost-Effective Conceptual Design Using Taxonomies</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that annotating named entities in unstructured and
semi-structured data sets by their concepts improves the effectiveness of
answering queries over these data sets. As every enterprise has a limited
budget of time or computational resources, it has to annotate a subset of
concepts in a given domain whose costs of annotation do not exceed the budget.
We call such a subset of concepts a {\it conceptual design} for the annotated
data set. We focus on finding a conceptual design that provides the most
effective answers to queries over the annotated data set, i.e., a {\it
cost-effective conceptual design}. Since, it is often less time-consuming and
costly to annotate general concepts than specific concepts, we use information
on superclass/subclass relationships between concepts in taxonomies to find a
cost-effective conceptual design. We quantify the amount by which a conceptual
design with concepts from a taxonomy improves the effectiveness of answering
queries over an annotated data set. If the taxonomy is a tree, we prove that
the problem is NP-hard and propose an efficient approximation and
pseudo-polynomial time algorithms for the problem. We further prove that if the
taxonomy is a directed acyclic graph, given some generally accepted hypothesis,
it is not possible to find any approximation algorithm with reasonably small
approximation ratio for the problem. Our empirical study using real-world data
sets, taxonomies, and query workloads shows that our framework effectively
quantifies the amount by which a conceptual design improves the effectiveness
of answering queries. It also indicates that our algorithms are efficient for a
design-time task with pseudo-polynomial algorithm being generally more
effective than the approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05667</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05667</id><created>2015-03-19</created><authors><author><keyname>Dasgupta</keyname><forenames>Sourish</forenames></author><author><keyname>Maheshwari</keyname><forenames>Gaurav</forenames></author><author><keyname>Trivedi</keyname><forenames>Priyansh</forenames></author></authors><title>BitSim: An Algebraic Similarity Measure for Description Logics Concepts</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose an algebraic similarity measure {\sigma}BS (BS
stands for BitSim) for assigning semantic similarity score to concept
definitions in ALCH+ an expressive fragment of Description Logics (DL). We
define an algebraic interpretation function, I_B, that maps a concept
definition to a unique string ({\omega}_B) called bit-code) over an alphabet
{\Sigma}_B of 11 symbols belonging to L_B - the language over P B. IB has
semantic correspondence with conventional model-theoretic interpretation of DL.
We then define {\sigma}_BS on L_B. A detailed analysis of I_B and {\sigma}_BS
has been given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05670</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05670</id><created>2015-03-19</created><updated>2015-03-20</updated><authors><author><keyname>Song</keyname><forenames>Yinglei</forenames></author></authors><title>Time and Space Efficient Algorithms for RNA Folding with the
  Four-Russians Technique</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop new algorithms for the basic RNA folding problem.
Given an RNA sequence that contains $n$ nucleotides, the goal of the problem is
to compute a pseudoknot-free secondary structure that maximizes the number of
base pairs in the sequence. We show that there exists a dynamic programming
algorithm that can solve the problem in time $O(\frac{n^{3}}{\log_{2}{n}})$
while using only $O(\frac{n^{2}}{\log_{2}{n}})$ memory space. In addition, we
show that the time complexity of this algorithm can be further improved to
$O(\frac{n^{3}}{\log_{2}^{2}{n}})$ at the expense of a slightly increased space
complexity. To the best of our knowledge, this is the first algorithm that can
solve the problem with traditional dynamic programming techniques in time
$O(\frac{n^{3}}{\log_{2}^{2}{n}})$. In addition, our results improve the best
known upper bound of the space complexity for efficiently solving both this
problem and the context-free language recognition problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05671</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05671</id><created>2015-03-19</created><updated>2015-07-23</updated><authors><author><keyname>Martens</keyname><forenames>James</forenames></author><author><keyname>Grosse</keyname><forenames>Roger</forenames></author></authors><title>Optimizing Neural Networks with Kronecker-factored Approximate Curvature</title><categories>cs.LG cs.NE stat.ML</categories><comments>Added some diagrams of neural networks (original and transformed
  versions), and added a conclusions section. Made a few other minor tweaks and
  fixes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient method for approximating natural gradient descent in
neural networks which we call Kronecker-factored Approximate Curvature (K-FAC).
K-FAC is based on an efficiently invertible approximation of a neural network's
Fisher information matrix which is neither diagonal nor low-rank, and in some
cases is completely non-sparse. It is derived by approximating various large
blocks of the Fisher (corresponding to entire layers) as being the Kronecker
product of two much smaller matrices. While only several times more expensive
to compute than the plain stochastic gradient, the updates produced by K-FAC
make much more progress optimizing the objective, which results in an algorithm
that can be much faster than stochastic gradient descent with momentum in
practice. And unlike some previously proposed approximate
natural-gradient/Newton methods which use high-quality non-diagonal curvature
matrices (such as Hessian-free optimization), K-FAC works very well in highly
stochastic optimization regimes. This is because the cost of storing and
inverting K-FAC's approximation to the curvature matrix does not depend on the
amount of data used to estimate it, which is a feature typically associated
only with diagonal or low-rank approximations to the curvature matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05681</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05681</id><created>2015-03-19</created><authors><author><keyname>Cannon</keyname><forenames>Sarah</forenames></author><author><keyname>Fai</keyname><forenames>Thomas G.</forenames></author><author><keyname>Iwerks</keyname><forenames>Justin</forenames></author><author><keyname>Leopold</keyname><forenames>Undine</forenames></author><author><keyname>Schmidt</keyname><forenames>Christiane</forenames></author></authors><title>Combinatorics and complexity of guarding polygons with edge and point
  2-transmitters</title><categories>cs.CG</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalization of the classical Art Gallery Problem, where
instead of a light source, the guards, called $k$-transmitters, model a
wireless device with a signal that can pass through at most $k$ walls. We show
it is NP-hard to compute a minimum cover of point 2-transmitters, point
$k$-transmitters, and edge 2-transmitters in a simple polygon. The point
2-transmitter result extends to orthogonal polygons. In addition, we give
necessity and sufficiency results for the number of edge 2-transmitters in
general, monotone, orthogonal monotone, and orthogonal polygons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05689</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05689</id><created>2015-03-19</created><authors><author><keyname>Sadiq</keyname><forenames>B. O</forenames></author><author><keyname>Sani</keyname><forenames>S. M</forenames></author><author><keyname>Garba</keyname><forenames>S</forenames></author></authors><title>Edge Detection: A Collection of Pixel based Approach for Colored Images</title><categories>cs.CV</categories><comments>5 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing traditional edge detection algorithms process a single pixel on
an image at a time, thereby calculating a value which shows the edge magnitude
of the pixel and the edge orientation. Most of these existing algorithms
convert the coloured images into gray scale before detection of edges. However,
this process leads to inaccurate precision of recognized edges, thus producing
false and broken edges in the image. This paper presents a profile modelling
scheme for collection of pixels based on the step and ramp edges, with a view
to reducing the false and broken edges present in the image. The collection of
pixel scheme generated is used with the Vector Order Statistics to reduce the
imprecision of recognized edges when converting from coloured to gray scale
images. The Pratt Figure of Merit (PFOM) is used as a quantitative comparison
between the existing traditional edge detection algorithm and the developed
algorithm as a means of validation. The PFOM value obtained for the developed
algorithm is 0.8480, which showed an improvement over the existing traditional
edge detection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05692</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05692</id><created>2015-03-19</created><authors><author><keyname>Sadiq</keyname><forenames>B O.</forenames></author><author><keyname>Sani</keyname><forenames>S. M.</forenames></author><author><keyname>Garba</keyname><forenames>S.</forenames></author></authors><title>An approach to improving edge detection for facial and remotely sensed
  images using vector order statistics</title><categories>cs.CV</categories><comments>9 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an improved edge detection algorithm for facial and
remotely sensed images using vector order statistics. The developed algorithm
processes colored images directly without been converted to gray scale. A
number of the existing algorithms converts the colored images into gray scale
before detection of edges. But this process leads to inaccurate precision of
recognized edges, thus producing false and broken edges in the output edge map.
Facial and remotely sensed images consist of curved edge lines which have to be
detected continuously to prevent broken edges. In order to deal with this, a
collection of pixel approach is introduced with a view to minimizing the false
and broken edges that exists in the generated output edge map of facial and
remotely sensed images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05694</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05694</id><created>2015-03-19</created><updated>2015-06-05</updated><authors><author><keyname>Jatala</keyname><forenames>Vishwesh</forenames></author><author><keyname>Anantpur</keyname><forenames>Jayvant</forenames></author><author><keyname>Karkare</keyname><forenames>Amey</forenames></author></authors><title>Improving GPU Performance Through Resource Sharing</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphics Processing Units (GPUs) consisting of Streaming Multiprocessors
(SMs) achieve high throughput by running a large number of threads and context
switching among them to hide execution latencies. The number of thread blocks,
and hence the number of threads that can be launched on an SM, depends on the
resource usage--e.g. number of registers, amount of shared memory--of the
thread blocks. Since the allocation of threads to an SM is at the thread block
granularity, some of the resources may not be used up completely and hence will
be wasted.
  We propose an approach that shares the resources of SM to utilize the wasted
resources by launching more thread blocks. We show the effectiveness of our
approach for two resources: register sharing, and scratchpad (shared memory)
sharing. We further propose optimizations to hide long execution latencies,
thus reducing the number of stall cycles. We implemented our approach in
GPGPU-Sim simulator and experimentally validated it on several applications
from 4 different benchmark suites: GPGPU-Sim, Rodinia, CUDA-SDK, and Parboil.
We observed that with register sharing, applications show maximum improvement
of 24%, and average improvement of 11%. With scratchpad sharing, we observed a
maximum improvement of 30% and an average improvement of 12.5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05696</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05696</id><created>2015-03-19</created><authors><author><keyname>Khan</keyname><forenames>Amjad Saeed</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author></authors><title>Performance Analysis of Random Linear Network Coding in Two-Source
  Single-Relay Networks</title><categories>cs.IT cs.NI cs.PF math.IT</categories><comments>Proc. ICC 2015, Workshop on Cooperative and Cognitive Mobile Networks
  (CoCoNet), to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the multiple-access relay channel in a setting where two
source nodes transmit packets to a destination node, both directly and via a
relay node, over packet erasure channels. Intra-session network coding is used
at the source nodes and inter-session network coding is employed at the relay
node to combine the recovered source packets of both source nodes. In this
work, we investigate the performance of the network-coded system in terms of
the probability that the destination node will successfully recover the source
packets of the two source nodes. We build our analysis on fundamental
probability expressions for random matrices over finite fields and we derive
upper bounds on the system performance for the case of systematic and
non-systematic network coding. Simulation results show that the upper bounds
are very tight and accurately predict the decoding probability at the
destination node. Our analysis also exposes the clear benefits of systematic
network coding at the source nodes compared to non-systematic transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05698</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05698</id><created>2015-03-19</created><authors><author><keyname>Wimmer</keyname><forenames>Martin</forenames></author><author><keyname>Gruber</keyname><forenames>Jakob</forenames></author><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author><author><keyname>Tsigas</keyname><forenames>Philippas</forenames></author></authors><title>The Lock-free $k$-LSM Relaxed Priority Queue</title><categories>cs.DS</categories><comments>Short version as ACM PPoPP'15 poster</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priority queues are data structures which store keys in an ordered fashion to
allow efficient access to the minimal (maximal) key. Priority queues are
essential for many applications, e.g., Dijkstra's single-source shortest path
algorithm, branch-and-bound algorithms, and prioritized schedulers.
  Efficient multiprocessor computing requires implementations of basic data
structures that can be used concurrently and scale to large numbers of threads
and cores. Lock-free data structures promise superior scalability by avoiding
blocking synchronization primitives, but the \emph{delete-min} operation is an
inherent scalability bottleneck in concurrent priority queues. Recent work has
focused on alleviating this obstacle either by batching operations, or by
relaxing the requirements to the \emph{delete-min} operation.
  We present a new, lock-free priority queue that relaxes the \emph{delete-min}
operation so that it is allowed to delete \emph{any} of the $\rho+1$ smallest
keys, where $\rho$ is a runtime configurable parameter. Additionally, the
behavior is identical to a non-relaxed priority queue for items added and
removed by the same thread. The priority queue is built from a logarithmic
number of sorted arrays in a way similar to log-structured merge-trees. We
experimentally compare our priority queue to recent state-of-the-art lock-free
priority queues, both with relaxed and non-relaxed semantics, showing high
performance and good scalability of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05702</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05702</id><created>2015-03-19</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Liu</keyname><forenames>Chen</forenames></author><author><keyname>Mao</keyname><forenames>Wenli</forenames></author><author><keyname>Fang</keyname><forenames>Zhichao</forenames></author></authors><title>The Open Access Advantage Considering Citation, Article Usage and Social
  Media Attention</title><categories>cs.DL cs.IR</categories><comments>12 pages, 6 figures</comments><doi>10.1007/s11192-015-1547-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we compare the difference in the impact between open access
(OA) and non-open access (non-OA) articles. 1761 Nature Communications articles
published from 1 Jan. 2012 to 31 Aug. 2013 are selected as our research
objects, including 587 OA articles and 1174 non-OA articles. Citation data and
daily updated article-level metrics data are harvested directly from the
platform of nature.com. Data is analyzed from the static versus
temporal-dynamic perspectives. The OA citation advantage is confirmed, and the
OA advantage is also applicable when extending the comparing from citation to
article views and social media attention. More important, we find that OA
papers not only have the great advantage of total downloads, but also have the
feature of keeping sustained and steady downloads for a long time. For article
downloads, non-OA papers only have a short period of attention, when the
advantage of OA papers exists for a much longer time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05704</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05704</id><created>2015-03-19</created><authors><author><keyname>Pandian</keyname><forenames>P. Chella</forenames></author><author><keyname>Durairajan</keyname><forenames>C.</forenames></author></authors><title>On Various Parameters of $Z_q$-Simplex codes for an even integer q</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we defined the $Z_q$-linear codes and discussed its various
parameters. We constructed $Z_q$-Simplex code and $Z_q$-MacDonald code and
found its parameters. We have given a lower and an upper bounds of its covering
radius for q is an even integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05724</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05724</id><created>2015-03-19</created><updated>2015-06-01</updated><authors><author><keyname>Urban</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>A Neural Transfer Function for a Smooth and Differentiable Transition
  Between Additive and Multiplicative Interactions</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing approaches to combine both additive and multiplicative neural units
either use a fixed assignment of operations or require discrete optimization to
determine what function a neuron should perform. This leads either to an
inefficient distribution of computational resources or an extensive increase in
the computational complexity of the training procedure.
  We present a novel, parameterizable transfer function based on the
mathematical concept of non-integer functional iteration that allows the
operation each neuron performs to be smoothly and, most importantly,
differentiablely adjusted between addition and multiplication. This allows the
decision between addition and multiplication to be integrated into the standard
backpropagation training procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05733</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05733</id><created>2015-03-19</created><authors><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>A software controlled voltage tuning system using multi-purpose ring
  oscillators</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel software driven voltage tuning method that
utilises multi-purpose Ring Oscillators (ROs) to provide process variation and
environment sensitive energy reductions. The proposed technique enables voltage
tuning based on the observed frequency of the ROs, taken as a representation of
the device speed and used to estimate a safe minimum operating voltage at a
given core frequency. A conservative linear relationship between RO frequency
and silicon speed is used to approximate the critical path of the processor.
  Using a multi-purpose RO not specifically implemented for critical path
characterisation is a unique approach to voltage tuning. The parameters
governing the relationship between RO and silicon speed are obtained through
the testing of a sample of processors from different wafer regions. These
parameters can then be used on all devices of that model. The tuning method and
software control framework is demonstrated on a sample of XMOS XS1-U8A-64
embedded microprocessors, yielding a dynamic power saving of up to 25% with no
performance reduction and no negative impact on the real-time constraints of
the embedded software running on the processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05743</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05743</id><created>2015-03-19</created><authors><author><keyname>Miura</keyname><forenames>Ken</forenames></author><author><keyname>Harada</keyname><forenames>Tatsuya</forenames></author></authors><title>Implementation of a Practical Distributed Calculation System with
  Browsers and JavaScript, and Application to Distributed Deep Learning</title><categories>cs.DC cs.LG cs.MS cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning can achieve outstanding results in various fields. However, it
requires so significant computational power that graphics processing units
(GPUs) and/or numerous computers are often required for the practical
application. We have developed a new distributed calculation framework called
&quot;Sashimi&quot; that allows any computer to be used as a distribution node only by
accessing a website. We have also developed a new JavaScript neural network
framework called &quot;Sukiyaki&quot; that uses general purpose GPUs with web browsers.
Sukiyaki performs 30 times faster than a conventional JavaScript library for
deep convolutional neural networks (deep CNNs) learning. The combination of
Sashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the
distributed deep learning of deep CNNs only with web browsers on various
devices. The libraries that comprise the proposed methods are available under
MIT license at http://mil-tokyo.github.io/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05761</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05761</id><created>2015-03-19</created><updated>2015-04-04</updated><authors><author><keyname>Lin</keyname><forenames>Sian-Jheng</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Han</keyname><forenames>Yunghsiang S.</forenames></author></authors><title>Efficient Frequency-Domain Decoding Algorithms for Reed-Solomon Codes</title><categories>cs.IT cs.DS math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This work develops frequency-domain decoding algorithms for $(n=2^m,k)$
systematic Reed-Solomon (RS) codes over fields $\mathbb{F}_{2^m},m\in
\mathbb{Z}^+$, where $n-k$ is a power of two. The proposed algorithms are based
on a new polynomial basis with a fast Fourier transform with computational
complexity of order $\mathcal{O}(n\lg(n))$. First, the basis of syndrome
polynomials is reformulated in the decoding procedure so that the new
transforms can be applied to the decoding procedure. A fast extended Euclidean
algorithm is developed to determine the error locator polynomial. The
computational complexity of the proposed decoding algorithm is
$\mathcal{O}(n\lg(n-k)+(n-k)\lg^2(n-k))$, improving upon the best currently
available decoding complexity of $\mathcal{O}(n\lg^2(n)\lg\lg(n))$ and reaching
the best known complexity bound that was established by Justesen in 1976, whose
approach is for RS codes that operate only on some specified finite fields. As
revealed by the computer simulations, the proposed decoding algorithm is $50$
times faster than the conventional one for the $(2^{16},2^{15})$ RS code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05767</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05767</id><created>2015-03-19</created><authors><author><keyname>Chung</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Tom&#xe1;s</forenames></author></authors><title>Automatic Pollen Grain and Exine Segmentation from Microscope Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose an automatic method for the segmentation of
pollen grains from microscope images, followed by the automatic segmentation of
their exine. The objective of exine segmentation is to separate the pollen
grain in two regions of interest: exine and inner part. A coarse-to-fine
approach ensures a smooth and accurate segmentation of both structures. As a
rough stage, grain segmentation is performed by a procedure involving
clustering and morphological operations, while the exine is approximated by an
iterative procedure consisting in consecutive cropping steps of the pollen
grain. A snake-based segmentation is performed to refine the segmentation of
both structures. Results have shown that our segmentation method is able to
deal with different pollen types, as well as with different types of exine and
inner part appearance. The proposed segmentation method aims to be generic and
has been designed as one of the core steps of an automatic pollen
classification framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05768</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05768</id><created>2015-03-19</created><updated>2015-03-25</updated><authors><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>On learning optimized reaction diffusion processes for effective image
  restoration</title><categories>cs.CV</categories><comments>9 pages, 3 figures, 3 tables. CVPR2015 oral presentation together
  with the supplemental material of 13 pages, 8 pages (Notes on diffusion
  networks)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several decades, image restoration remains an active research topic in
low-level computer vision and hence new approaches are constantly emerging.
However, many recently proposed algorithms achieve state-of-the-art performance
only at the expense of very high computation time, which clearly limits their
practical relevance. In this work, we propose a simple but effective approach
with both high computational efficiency and high restoration quality. We extend
conventional nonlinear reaction diffusion models by several parametrized linear
filters as well as several parametrized influence functions. We propose to
train the parameters of the filters and the influence functions through a loss
based approach. Experiments show that our trained nonlinear reaction diffusion
models largely benefit from the training of the parameters and finally lead to
the best reported performance on common test datasets for image restoration.
Due to their structural simplicity, our trained models are highly efficient and
are also well-suited for parallel computation on GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05781</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05781</id><created>2015-03-19</created><authors><author><keyname>Yavlinsky</keyname><forenames>Alexei</forenames></author></authors><title>Memantic: A Medical Knowledge Discovery Engine</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system that constructs and maintains an up-to-date co-occurrence
network of medical concepts based on continuously mining the latest biomedical
literature. Users can explore this network visually via a concise online
interface to quickly discover important and novel relationships between medical
entities. This enables users to rapidly gain contextual understanding of their
medical topics of interest, and we believe this constitutes a significant user
experience improvement over contemporary search engines operating in the
biomedical literature domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05782</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05782</id><created>2015-03-19</created><authors><author><keyname>Huang</keyname><forenames>Sheng</forenames></author><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author></authors><title>Learning Hypergraph-regularized Attribute Predictors</title><categories>cs.CV cs.LG</categories><comments>This is an attribute learning paper accepted by CVPR 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a novel attribute learning framework named Hypergraph-based
Attribute Predictor (HAP). In HAP, a hypergraph is leveraged to depict the
attribute relations in the data. Then the attribute prediction problem is
casted as a regularized hypergraph cut problem in which HAP jointly learns a
collection of attribute projections from the feature space to a hypergraph
embedding space aligned with the attribute space. The learned projections
directly act as attribute classifiers (linear and kernelized). This formulation
leads to a very efficient approach. By considering our model as a multi-graph
cut task, our framework can flexibly incorporate other available information,
in particular class label. We apply our approach to attribute prediction,
Zero-shot and $N$-shot learning tasks. The results on AWA, USAA and CUB
databases demonstrate the value of our methods in comparison with the
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05784</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05784</id><created>2015-03-18</created><authors><author><keyname>Cobo</keyname><forenames>Alfredo</forenames></author><author><keyname>Parra</keyname><forenames>Denis</forenames></author><author><keyname>Nav&#xf3;n</keyname><forenames>Jaime</forenames></author></authors><title>Identifying Relevant Messages in a Twitter-based Citizen Channel for
  Natural Disaster Situations</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During recent years the online social networks (in particular Twitter) have
become an important alternative information channel to traditional media during
natural disasters, but the amount and diversity of messages poses the challenge
of information overload to end users. The goal of our research is to develop an
automatic classifier of tweets to feed a mobile application that reduces the
difficulties that citizens face to get relevant information during natural
disasters. In this paper, we present in detail the process to build a
classifier that filters tweets relevant and non-relevant to an earthquake. By
using a dataset from the Chilean earthquake of 2010, we first build and
validate a ground truth, and then we contribute by presenting in detail the
effect of class imbalance and dimensionality reduction over 5 classifiers. We
show how the performance of these models is affected by these variables,
providing important considerations at the moment of building these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05786</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05786</id><created>2015-03-19</created><authors><author><keyname>Chung</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Tom&#xe1;s</forenames></author></authors><title>A General Framework for Multi-focal Image Classification and
  Authentication: Application to Microscope Pollen Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a general framework for multi-focal image
classification and authentication, the methodology being demonstrated on
microscope pollen images. The framework is meant to be generic and based on a
brute force-like approach aimed to be efficient not only on any kind, and any
number, of pollen images (regardless of the pollen type), but also on any kind
of multi-focal images. All stages of the framework's pipeline are designed to
be used in an automatic fashion. First, the optimal focus is selected using the
absolute gradient method. Then, pollen grains are extracted using a
coarse-to-fine approach involving both clustering and morphological techniques
(coarse stage), and a snake-based segmentation (fine stage). Finally, features
are extracted and selected using a generalized approach, and their
classification is tested with four classifiers: Weighted Neighbor Distance,
Neural Network, Decision Tree and Random Forest. The latter method, which has
shown the best and more robust classification accuracy results (above 97\% for
any number of pollen types), is finally used for the authentication stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05787</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05787</id><created>2015-03-19</created><updated>2015-04-17</updated><authors><author><keyname>Everts</keyname><forenames>Maarten H.</forenames></author><author><keyname>Bekker</keyname><forenames>Henk</forenames></author><author><keyname>Roerdink</keyname><forenames>Jos B. T. M.</forenames></author><author><keyname>Isenberg</keyname><forenames>Tobias</forenames></author></authors><title>Interactive Illustrative Line Styles and Line Style Transfer Functions
  for Flow Visualization</title><categories>cs.GR</categories><comments>Extended version of a short paper at Pacific Graphics 2011
  (http://dx.doi.org/10.2312/PE/PG/PG2011short/105-110)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a flexible illustrative line style model for the visualization of
streamline data. Our model partitions view-oriented line strips into parallel
bands whose basic visual properties can be controlled independently. We thus
extend previous line stylization techniques specifically for visualization
purposes by allowing the parametrization of these bands based on the local line
data attributes. Moreover, our approach supports emphasis and abstraction by
introducing line style transfer functions that map local line attribute values
to complete line styles. With a flexible GPU implementation of this line style
model we enable the interactive exploration of visual representations of
streamlines. We demonstrate the effectiveness of our model by applying it to 3D
flow field datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05793</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05793</id><created>2015-03-19</created><authors><author><keyname>Chan</keyname><forenames>Kam Wai Clifford</forenames></author><author><keyname>Rifai</keyname><forenames>Mayssaa El</forenames></author><author><keyname>Verma</keyname><forenames>Pramode K.</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author><author><keyname>Chen</keyname><forenames>Yuhua</forenames></author></authors><title>Multi-Photon Quantum Key Distribution Based on Double-Lock Encryption</title><categories>quant-ph cs.CR</categories><journal-ref>International Journal on Cryptography and Information Security,
  Vol. 5, No. 3/4, pp. 1-13 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a multi-stage, multi-photon quantum key distribution
protocol based on the double-lock cryptography. It exploits the asymmetry in
the detection strategies between the legitimate users and the eavesdropper. The
security analysis of the protocol is presented with coherent states under the
intercept-resend attack, the photon number splitting attack, and the
man-in-the-middle attack. It is found that the mean photon number can be much
larger than one. This complements the recent interest in multi-photon quantum
communication protocols that require a pre-shared key between the legitimate
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05807</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05807</id><created>2015-03-19</created><updated>2015-06-15</updated><authors><author><keyname>Baudry</keyname><forenames>Benoit</forenames></author><author><keyname>Allier</keyname><forenames>Simon</forenames></author><author><keyname>Rodriguez-Cancio</keyname><forenames>Marcelino</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>DSpot: Test Amplification for Automatic Assessment of Computational
  Diversity</title><categories>cs.SE</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Computational diversity, i.e., the presence of a set of programs
that all perform compatible services but that exhibit behavioral differences
under certain conditions, is essential for fault tolerance and security.
Objective: We aim at proposing an approach for automatically assessing the
presence of computational diversity. In this work, computationally diverse
variants are defined as (i) sharing the same API, (ii) behaving the same
according to an input-output based specification (a test-suite) and (iii)
exhibiting observable differences when they run outside the specified input
space. Method: Our technique relies on test amplification. We propose source
code transformations on test cases to explore the input domain and
systematically sense the observation domain. We quantify computational
diversity as the dissimilarity between observations on inputs that are outside
the specified domain. Results: We run our experiments on 472 variants of 7
classes from open-source, large and thoroughly tested Java classes. Our test
amplification multiplies by ten the number of input points in the test suite
and is effective at detecting software diversity. Conclusion: The key insights
of this study are: the systematic exploration of the observable output space of
a class provides new insights about its degree of encapsulation; the behavioral
diversity that we observe originates from areas of the code that are
characterized by their flexibility (caching, checking, formatting, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05812</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05812</id><created>2015-03-19</created><updated>2015-07-16</updated><authors><author><keyname>Yin</keyname><forenames>Yitong</forenames></author><author><keyname>Zhao</keyname><forenames>Jinman</forenames></author></authors><title>Counting hypergraph matchings up to uniqueness threshold</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of approximately counting hypergraph matchings with an
activity parameter $\lambda$ in hypergraphs of bounded maximum degree and
bounded maximum size of hyperedges. This problem unifies two important
statistical physics models in approximate counting: the hardcore model (graph
independent sets) and the monomer-dimer model (graph matchings).
  We show for this model the critical activity $\lambda_c= \frac{d^d}{k
(d-1)^{d+1}}$ is the threshold for the uniqueness of Gibbs measures on the
infinite $(d+1)$-uniform $(k+1)$-regular hypertree. And we show that when
$\lambda&lt;\lambda_c$ the model exhibits strong spatial mixing at an exponential
rate and there is an FPTAS for the partition function of the model on all
hypergraphs of maximum degree at most $k+1$ and maximum edge size at most
$d+1$. Assuming NP$\neq$RP, there is no FPRAS for the partition function of the
model when $\lambda &gt; 2\lambda_c$ on above family of hypergraphs .
  Towards closing this gap and obtaining a tight transition of approximability,
we study the local weak convergence from an infinite sequence of random finite
hypergraphs to the infinite uniform regular hypertree with specified symmetry,
and prove a surprising result: the existence of such local convergence is fully
characterized by the reversibility of the uniform random walk on the infinite
hypertree projected on the symmetry classes. We also give an explicit
construction for the sequence of random finite hypergraphs with proper local
convergence property when the reversibility condition is satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05816</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05816</id><created>2015-03-19</created><updated>2015-06-15</updated><authors><author><keyname>Kolarijani</keyname><forenames>Arman Sharifi</forenames></author><author><keyname>Mazo</keyname><forenames>Manuel</forenames><suffix>Jr</suffix></author></authors><title>A Formal Traffic Characterization of LTI Event-triggered Control Systems</title><categories>cs.SY</categories><comments>10 pages, 7 figures</comments><msc-class>93C30, 93C57, 93C10, 93C95</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unnecessary communication and computation in the periodic execution of
control tasks lead to over-provisioning in hardware design (or
underexploitation in hardware utilization) in control applications, such as
networked control systems. To address these issues, researchers have proposed a
new class of strategies, named event-driven strategies. Despite of their
beneficiary effects, matters like task scheduling and appropriate dimensioning
of communication components have become more complicated with respect to
traditional periodic strategies. In this paper, we present a formal approach to
derive an abstracted system that captures the sampling behavior of a family of
event-triggered strategies for the case of LTI systems. This structure
approximately simulates the sampling behavior of the aperiodic control system.
Furthermore, the resulting quotient system is equivalent to a timed automaton.
In the construction of the abstraction, the state space is confined to a finite
number of convex regions, each of which represents a mode in the quotient
system. An LMI-based technique is deployed to derive a sampling time interval
associated to each region. Finally, reachability analysis is leveraged to find
the transitions of the quotient system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05819</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05819</id><created>2015-03-19</created><authors><author><keyname>Shafique</keyname><forenames>Taniya</forenames></author><author><keyname>Zia</keyname><forenames>Muhammad</forenames></author><author><keyname>Han</keyname><forenames>Huy Dung</forenames></author></authors><title>Analysis and Throughput Optimization of Selective Chase Combining for
  OFDM Systems</title><categories>cs.IT cs.SY math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present throughput analysis and optimization of bandwidth
efficient selective retransmission method at modulation layer for conventional
Chase Combining (CC) method under orthogonal frequency division multiplexing
(OFDM) signaling. Most of the times, there are fewer errors in a failed packet
and receiver can recover from errors receiving partial copy of original frame.
The proposed selective retransmission method at modulation layer for OFDM
modulation requests retransmission of information corresponding to the poor
quality subcarriers. In this work, we propose cross-layer multiple selective
Chase combining (MSCC) method and Chase combining with selective retransmission
(CCWS) at modulation level. We also present bit-error rate (BER) and throughput
analysis of the proposed MSCC and CCWS methods. In order to maximize throughput
of the proposed methods under OFDM signaling, we formulate optimization problem
with respect to amount of information to be retransmitted in selective
retransmission in the event of packet failure. We present tight BER upper
bounds and tight throughput lower bounds for the proposed selective Chase
combining methods. The simulation results demonstrate significant throughput
gain of the optimized selective retransmission methods over conventional
retransmission methods. The throughput gain of the proposed selective
retransmission atmodulation layer are also holds for conventional for hybrid
automatic repeat request (HARQ) methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05826</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05826</id><created>2015-03-19</created><authors><author><keyname>Rocha</keyname><forenames>Luis Enrique Correa</forenames></author><author><keyname>Thorson</keyname><forenames>Anna Ekeus</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author></authors><title>Respondent-driven sampling bias induced by clustering and community
  structure in social networks</title><categories>stat.AP cs.SI physics.data-an physics.soc-ph</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling hidden populations is particularly challenging using standard
sampling methods mainly because of the lack of a sampling frame.
Respondent-driven sampling (RDS) is an alternative methodology that exploits
the social contacts between peers to reach and weight individuals in these
hard-to-reach populations. It is a snowball sampling procedure where the weight
of the respondents is adjusted for the likelihood of being sampled due to
differences in the number of contacts. In RDS, the structure of the social
contacts thus defines the sampling process and affects its coverage, for
instance by constraining the sampling within a sub-region of the network. In
this paper we study the bias induced by network structures such as social
triangles, community structure, and heterogeneities in the number of contacts,
in the recruitment trees and in the RDS estimator. We simulate different
scenarios of network structures and response-rates to study the potential
biases one may expect in real settings. We find that the prevalence of the
estimated variable is associated with the size of the network community to
which the individual belongs. Furthermore, we observe that low-degree nodes may
be under-sampled in certain situations if the sample and the network are of
similar size. Finally, we also show that low response-rates lead to reasonably
accurate average estimates of the prevalence but generate relatively large
biases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05829</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05829</id><created>2015-03-19</created><authors><author><keyname>Abrardo</keyname><forenames>Andrea</forenames></author><author><keyname>Barni</keyname><forenames>Mauro</forenames></author><author><keyname>Kallas</keyname><forenames>Kassem</forenames></author><author><keyname>Tondi</keyname><forenames>Benedetta</forenames></author></authors><title>Optimum Fusion of Possibly Corrupted Reports for Distributed Detection
  in Multi-Sensor Networks</title><categories>cs.SY cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most common approach to mitigate the impact that the presence of
malicious nodes has on the accuracy of decision fusion schemes consists in
observing the behavior of the nodes over a time interval T and then removing
the reports of suspect nodes from the fusion process. By assuming that some
a-priori information about the presence of malicious nodes and their behavior
is available, we show that the information stemming from the suspect nodes can
be exploited to further improve the decision fusion accuracy. Specifically, we
derive the optimum fusion rule and analyze the achievable performance for two
specific cases. In the first case, the states of the nodes (corrupted or
honest) are independent of each other and the fusion center knows only the
probability that a node is malicious. In the second case, the exact number of
corrupted nodes is fixed and known to the fusion center. We also investigate
the optimum corruption strategy for the malicious nodes, showing that always
reverting the local decision does not necessarily maximize the loss of
performance at the fusion center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05830</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05830</id><created>2015-03-19</created><authors><author><keyname>Rioux-Maldague</keyname><forenames>Lucas</forenames></author><author><keyname>Gigu&#xe8;re</keyname><forenames>Philippe</forenames></author></authors><title>Sign Language Fingerspelling Classification from Depth and Color Images
  using a Deep Belief Network</title><categories>cs.CV</categories><comments>Published in 2014 Canadian Conference on Computer and Robot Vision</comments><doi>10.1109/CRV.2014.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic sign language recognition is an open problem that has received a
lot of attention recently, not only because of its usefulness to signers, but
also due to the numerous applications a sign classifier can have. In this
article, we present a new feature extraction technique for hand pose
recognition using depth and intensity images captured from a Microsoft Kinect
sensor. We applied our technique to American Sign Language fingerspelling
classification using a Deep Belief Network, for which our feature extraction
technique is tailored. We evaluated our results on a multi-user data set with
two scenarios: one with all known users and one with an unseen user. We
achieved 99% recall and precision on the first, and 77% recall and 79%
precision on the second. Our method is also capable of real-time sign
classification and is adaptive to any environment or lightning intensity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05831</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05831</id><created>2015-03-19</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author><author><keyname>Bernasconi</keyname><forenames>J.</forenames></author></authors><title>Neural Network-Based Active Learning in Multivariate Calibration</title><categories>cs.NE cs.CE cs.LG</categories><comments>9 pages in final printed version</comments><journal-ref>IEEE Transactions on Systems, Man, Cybernetics-Part C, vol. 42,
  issue 6, pp. 1763-1771, 2012</journal-ref><doi>10.1109/TSMCC.2012.2220963</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In chemometrics, data from infrared or near-infrared (NIR) spectroscopy are
often used to identify a compound or to analyze the composition of amaterial.
This involves the calibration of models that predict the concentration
ofmaterial constituents from the measured NIR spectrum. An interesting aspect
of multivariate calibration is to achieve a particular accuracy level with a
minimum number of training samples, as this reduces the number of laboratory
tests and thus the cost of model building. In these chemometric models, the
input refers to a proper representation of the spectra and the output to the
concentrations of the sample constituents. The search for a most informative
new calibration sample thus has to be performed in the output space of the
model, rather than in the input space as in conventionalmodeling problems. In
this paper, we propose to solve the corresponding inversion problem by
utilizing the disagreements of an ensemble of neural networks to represent the
prediction error in the unexplored component space. The next calibration sample
is then chosen at a composition where the individual models of the ensemble
disagree most. The results obtained for a realistic chemometric calibration
example show that the proposed active learning can achieve a given calibration
accuracy with less training samples than random sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05832</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05832</id><created>2015-03-19</created><authors><author><keyname>Sanft</keyname><forenames>Kevin R.</forenames></author><author><keyname>Othmer</keyname><forenames>Hans G.</forenames></author></authors><title>Constant-complexity Stochastic Simulation Algorithm with Optimal Binning</title><categories>cs.DS</categories><comments>The following article has been submitted to The Journal of Chemical
  Physics. After it is published, it will be found at
  http://scitation.aip.org/content/aip/journal/jcp</comments><doi>10.1063/1.4928635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the cellular scale, biochemical processes are governed by random
interactions between reactant molecules with small copy counts, leading to
behavior that is inherently stochastic. Such systems are often modeled as
continuous-time Markov jump processes that can be described by the Chemical
Master Equation. Gillespie's Stochastic Simulation Algorithm (SSA) generates
exact trajectories of these systems. The amount of computational work required
for each step of the original SSA is proportional to the number of reaction
channels, leading to computational complexity that scales linearly as the
problem size increases. The original SSA is therefore inefficient for large
problems, which has prompted the development of several alternative
formulations with improved scaling properties. We describe an exact SSA that
uses a table data structure with event time binning to achieve constant
computational complexity. Optimal algorithm parameters and binning strategies
are discussed. We compare the computational efficiency of the algorithm to
existing methods and demonstrate excellent scaling for large problems. This
method is well suited for generating exact trajectories of large models that
can be described by the Reaction-Diffusion Master Equation arising from
spatially discretized reaction-diffusion processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05847</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05847</id><created>2015-03-19</created><authors><author><keyname>Bose-Kolanu</keyname><forenames>Abhishek</forenames></author></authors><title>Hypercomputation, Frege, Deleuze: Solving Thomson's Lamp</title><categories>cs.LO cs.CC</categories><comments>2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first known solution to the original supertask, the Thomson
Lamp Paradox.
  We also offer preliminary resources for classifying computational complexity
of various supertasks. In so doing we consider a newly apparent paradox between
the metrical limit and the ordinal limit. We use this distinction between the
metrical and ordinal limits to explain the shortcomings both of Thomson's
original formulation of the Lamp Paradox and Benacerraf's consequent critique.
  We resolve this paradox through a careful consideration of transfinite
ordinals and locate its ambiguity as inherent to the identity relation under
logic with a close reading of Frege's Begriffsschrift. With this close reading
in hand we expose how the identity relation is counter-intuitively polyvalent
and, with supertasks, how the logico-mathematical field operates on the basis
of Deleuzian point-folds. Our results combine resources from philosophy,
mathematics, and computer science to ground the field of hypercomputation for
logically rigorous study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05849</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05849</id><created>2015-03-19</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Deep Transform: Time-Domain Audio Error Correction via Probabilistic
  Re-Synthesis</title><categories>cs.SD cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the process of recording, storage and transmission of time-domain audio
signals, errors may be introduced that are difficult to correct in an
unsupervised way. Here, we train a convolutional deep neural network to
re-synthesize input time-domain speech signals at its output layer. We then use
this abstract transformation, which we call a deep transform (DT), to perform
probabilistic re-synthesis on further speech (of the same speaker) which has
been degraded. Using the convolutive DT, we demonstrate the recovery of speech
audio that has been subject to extreme degradation. This approach may be useful
for correction of errors in communications devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05857</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05857</id><created>2015-03-19</created><updated>2015-07-19</updated><authors><author><keyname>Zeng</keyname><forenames>William</forenames></author></authors><title>Models of Quantum Algorithms in Sets and Relations</title><categories>quant-ph cs.LO math.CT math.QA</categories><comments>fixed classical relation errors, and clarified terminology/defns</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct abstract models of blackbox quantum algorithms using a model of
quantum computation in sets and relations, a setting that is usually considered
for nondeterministic classical computation. This alternative model of quantum
computation (QCRel), though unphysical, nevertheless faithfully models its
computational structure. Our main results are models of the Deutsch-Jozsa,
single-shot Grovers, and GroupHomID algorithms in QCRel. These results provide
new tools to analyze the semantics of quantum computation and improve our
understanding of the relationship between computational speedups and the
structure of physical theories. They also exemplify a method of extending
physical/computational intuition into new mathematical settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05858</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05858</id><created>2015-03-19</created><updated>2016-02-11</updated><authors><author><keyname>G&#xfc;nther</keyname><forenames>Christian</forenames></author><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author></authors><title>Merit factors of polynomials derived from difference sets</title><categories>math.CO cs.IT math.IT</categories><comments>22 pages, this revision contains a more general version of Thm. 2.1</comments><msc-class>05B10, 11B83, 11T22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of constructing polynomials with all coefficients $1$ or $-1$ and
large merit factor (equivalently with small $L^4$ norm on the unit circle)
arises naturally in complex analysis, condensed matter physics, and digital
communications engineering. Most known constructions arise (sometimes in a
subtle way) from difference sets, in particular from Paley and Singer
difference sets. We consider the asymptotic merit factor of polynomials
constructed from other difference sets, providing the first essentially new
examples since 1991. In particular we prove a general theorem on the asymptotic
merit factor of polynomials arising from cyclotomy, which includes results on
Hall and Paley difference sets as special cases. In addition, we establish the
asymptotic merit factor of polynomials derived from Gordon-Mills-Welch
difference sets and Sidelnikov almost difference sets, proving two recent
conjectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05860</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05860</id><created>2015-03-19</created><authors><author><keyname>Pishchulin</keyname><forenames>Leonid</forenames></author><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author><author><keyname>Helten</keyname><forenames>Thomas</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Building Statistical Shape Spaces for 3D Human Modeling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical models of 3D human shape and pose learned from scan databases
have developed into valuable tools to solve a variety of vision and graphics
problems. Unfortunately, most publicly available models are of limited
expressiveness as they were learned on very small databases that hardly reflect
the true variety in human body shapes. In this paper, we contribute by
rebuilding a widely used statistical body representation from the largest
commercially available scan database, and making the resulting model available
to the community (visit http://humanshape.mpi-inf.mpg.de). As preprocessing
several thousand scans for learning the model is a challenge in itself, we
contribute by developing robust best practice solutions for scan alignment that
quantitatively lead to the best learned models. We make implementations of
these preprocessing steps also publicly available. We extensively evaluate the
improved accuracy and generality of our new model, and show its improved
performance for human body reconstruction from sparse input data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05872</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05872</id><created>2015-03-19</created><updated>2015-06-10</updated><authors><author><keyname>Maguluri</keyname><forenames>Siva Theja</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author></authors><title>Queue Length Behavior in a Switch under the MaxWeight Algorithm</title><categories>math.PR cs.PF</categories><comments>30 pages An earlier version dealing with the special case of Uniform
  Bernoulli traffic can be found here: http://arxiv.org/abs/1503.05872</comments><msc-class>60K25, 90B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a switch operating under the MaxWeight scheduling algorithm,
under any traffic pattern such that all the ports are loaded. This system is
interesting to study since the queue lengths exhibit a multi-dimensional
state-space collapse in the heavy-traffic regime. We use a Lyapunov-type drift
technique to characterize the heavy-traffic behavior of the expectation of the
sum queue lengths in steady-state, under the assumption that all ports are
saturated and all queues receive non-zero traffic. Under these conditions, we
show that the heavy-traffic scaled queue length is given by
$\left(1-\frac{1}{2n}\right)||\sigma||^2$, where $\sigma$ is the vector of the
standard deviations of arrivals to each port in the heavy-traffic limit. In the
special case of uniform Bernoulli arrivals, the corresponding formula is given
by $\left(n-\frac{3}{2}+\frac{1}{2n}\right)$. The result shows that the
heavy-traffic scaled queue length has optimal scaling with respect to $n,$ thus
settling one version of an open conjecture; in fact, it is shown that the
heavy-traffic queue length is at most within a factor of two from the optimal.
We then consider certain asymptotic regimes where the load of the system scales
simultaneously with the number of ports. We show that the MaxWeight algorithm
has optimal queue length scaling behavior provided that the arrival rate
approaches capacity sufficiently fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05879</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05879</id><created>2015-03-19</created><authors><author><keyname>Rubtsov</keyname><forenames>Alexander A.</forenames></author></authors><title>Regular realizability problems and regular languages</title><categories>cs.FL cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate regular realizability (RR) problems, which are the problems of
verifying whether intersection of a regular language -- the input of the
problem -- and fixed language called filter is non-empty. We consider two kind
of problems depending on representation of regular language. If a regular
language on input is represented by a DFA, then we obtain (deterministic)
regular realizability problem and we show that in this case the complexity of
regular realizability problem for an arbitrary regular filter is either
L-complete or NL-complete. We also show that in case of representation regular
language on input by NFA the problem is always NL-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05881</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05881</id><created>2015-03-19</created><authors><author><keyname>Chyla</keyname><forenames>Roman</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Holachek</keyname><forenames>Alexandra</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn S.</forenames></author><author><keyname>Elliott</keyname><forenames>Jonathan</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Thompson</keyname><forenames>Donna M.</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Murray</keyname><forenames>Stephen S.</forenames></author><author><keyname>Sudilovsky</keyname><forenames>Vladimir</forenames></author></authors><title>ADS 2.0: new architecture, API and services</title><categories>cs.DL</categories><comments>ADASS Conference 2014</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The ADS platform is undergoing the biggest rewrite of its 20-year history.
While several components have been added to its architecture over the past
couple of years, this talk will concentrate on the underpinnings of ADS's
search layer and its API. To illustrate the design of the components in the new
system, we will show how the new ADS user interface is built exclusively on top
of the API using RESTful web services. Taking one step further, we will discuss
how we plan to expose the treasure trove of information hosted by ADS (10
million records and fulltext for much of the Astronomy and Physics refereed
literature) to partners interested in using this API. This will provide you
(and your intelligent applications) with access to ADS's underlying data to
enable the extraction of new knowledge and the ingestion of these results back
into the ADS. Using this framework, researchers could run controlled
experiments with content extraction, machine learning, natural language
processing, etc. In this talk, we will discuss what is already implemented,
what will be available soon, and where we are going next.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05882</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05882</id><created>2015-03-17</created><authors><author><keyname>Chen</keyname><forenames>Jiadi</forenames></author><author><keyname>Zheng</keyname><forenames>Qiang</forenames></author><author><keyname>Long</keyname><forenames>Hang</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author></authors><title>Divisible Load Scheduling in Mobile Grid based on Stackelberg Pricing
  Game</title><categories>cs.DC cs.GT</categories><comments>5 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, it has become feasible to use mobile nodes as contributing entities
in computing systems. In this paper, we consider a computational grid in which
the mobile devices can share their idle resources to realize parallel
processing. The overall computing task can be arbitrarily partitioned into
multiple subtasks to be distributed to mobile resource providers (RPs). In this
process, the computation load scheduling problem is highlighted. Based on the
optimization objective, i.e., minimizing the task makespan, a buyer-seller
model in which the task sponsor can inspire the SPs to share their computing
resources by paying certain profits, is proposed. The Stackelberg Pricing Game
(SPG) is employed to obtain the optimal price and shared resource amount of
each SP. Finally, we evaluate the performance of the proposed algorithm by
system simulation and the results indicate that the SPG-based load scheduling
algorithm can significantly improve the time gain in mobile grid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05897</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05897</id><created>2015-03-19</created><authors><author><keyname>Ho</keyname><forenames>Chien-Ju</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Suri</keyname><forenames>Siddharth</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>Incentivizing High Quality Crowdwork</title><categories>cs.GT</categories><comments>This is a preprint of an Article accepted for publication in WWW
  \c{opyright} 2015 International World Wide Web Conference Committee</comments><doi>10.1145/2736277.2741102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the causal effects of financial incentives on the quality of
crowdwork. We focus on performance-based payments (PBPs), bonus payments
awarded to workers for producing high quality work. We design and run
randomized behavioral experiments on the popular crowdsourcing platform Amazon
Mechanical Turk with the goal of understanding when, where, and why PBPs help,
identifying properties of the payment, payment structure, and the task itself
that make them most effective. We provide examples of tasks for which PBPs do
improve quality. For such tasks, the effectiveness of PBPs is not too sensitive
to the threshold for quality required to receive the bonus, while the magnitude
of the bonus must be large enough to make the reward salient. We also present
examples of tasks for which PBPs do not improve quality. Our results suggest
that for PBPs to improve quality, the task must be effort-responsive: the task
must allow workers to produce higher quality work by exerting more effort. We
also give a simple method to determine if a task is effort-responsive a priori.
Furthermore, our experiments suggest that all payments on Mechanical Turk are,
to some degree, implicitly performance-based in that workers believe their work
may be rejected if their performance is sufficiently poor. Finally, we propose
a new model of worker behavior that extends the standard principal-agent model
from economics to include a worker's subjective beliefs about his likelihood of
being paid, and show that the predictions of this model are in line with our
experimental findings. This model may be useful as a foundation for theoretical
studies of incentives in crowdsourcing markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05904</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05904</id><created>2015-03-19</created><updated>2015-05-19</updated><authors><author><keyname>Hahn</keyname><forenames>Bridger</forenames></author><author><keyname>Nithyanand</keyname><forenames>Rishab</forenames></author><author><keyname>Gill</keyname><forenames>Phillipa</forenames></author><author><keyname>Johnson</keyname><forenames>Rob</forenames></author></authors><title>Games Without Frontiers: Investigating Video Games as a Covert Channel</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has become a critical communication infrastructure for citizens
to organize protests and express dissatisfaction with their governments. This
fact has not gone unnoticed, with governments clamping down on this medium via
censorship, and circumvention researchers working to stay one step ahead.
  In this paper, we explore a promising new avenue for covert channels:
real-time strategy-video games. Video games have two key features that make
them attractive cover protocols for censorship circumvention. First, due to the
popularity of gaming platforms such as Steam, there are a lot of different
video games, each with their own protocols and server infrastructure. Users of
video-game-based censorship-circumvention tools can therefore diversify across
many games, making it difficult for the censor to respond by simply blocking a
single cover protocol. Second, games in the same genre have many common
features and concepts. As a result, the same covert channel framework can be
easily adapted to work with many different games. This means that circumvention
tool developers can stay ahead of the censor by creating a diverse set of tools
and by quickly adapting to blockades created by the censor.
  We demonstrate the feasibility of this approach by implementing our coding
scheme over two real-time strategy-games (including a very popular
closed-source game). We evaluate the security of our system prototype -- Castle
-- by quantifying its resilience to a censor-adversary, its similarity to real
game traffic, and its ability to avoid common pitfalls in covert channel
design. We use our prototype to demonstrate that our approach can provide
throughput which is amenable to transfer of textual data, such at e-mail, SMS
messages, and tweets, which are commonly used to organize political actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05907</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05907</id><created>2015-03-19</created><authors><author><keyname>Christen</keyname><forenames>Daniel</forenames></author></authors><title>Syntagma Lexical Database</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the structure of Syntagma's Lexical Database (focused on
Italian). The basic database consists in four tables. Table Forms contains word
inflections, used by the POS-tagger for the identification of input-words.
Forms is related to Lemma. Table Lemma stores all kinds of grammatical features
of words, word-level semantic data and restrictions. In the table Meanings
meaning-related data are stored: definition, examples, domain, and semantic
information. Table Valency contains the argument structure of each meaning,
with syntactic and semantic features for each argument. The extended version of
SLD contains the links to Syntagma's Semantic Net and to the WordNet synsets of
other languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05908</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05908</id><created>2015-03-15</created><authors><author><keyname>Bindewald</keyname><forenames>Eckart</forenames></author></authors><title>Achieving Multiple Goals via Voluntary Efforts and Motivation Asymmetry</title><categories>cs.GT</categories><msc-class>91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The achievement of common goals through voluntary efforts of members of a
group can be challenged by the high temptation of individual defection. Here,
two-person one-goal assurance games are generalized to N-person, M-goal
achievement games in which group members can have different motivations with
respect to the achievement of the different goals. The theoretical performance
of groups faced with the challenge of multiple simultaneous goals is analyzed
mathematically and computationally. For two-goal scenarios one finds that
&quot;polarized&quot; as well as &quot;biased&quot; groups perform well in the presence of
defectors. A special case, called individual purpose games (N-person, N-goal
achievements games where there is a one-to-one mapping between actors and goals
for which they have a high achievement motivation) is analyzed in more detail
in form of the &quot;importance of being different theorem&quot;. It is shown that in
some individual purpose games, groups of size N can successfully accomplish N
goals, such that each group member is highly motivated towards the achievement
of one unique goal. The game-theoretic results suggest that multiple goals as
well as differences in motivations can, in some cases, correspond to highly
effective groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05911</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05911</id><created>2015-03-19</created><updated>2015-06-05</updated><authors><author><keyname>Sch&#xfc;rmann</keyname><forenames>Thomas</forenames></author></authors><title>A note on entropy estimation</title><categories>physics.data-an cs.IT math.IT math.ST stat.TH</categories><comments>7 pages, including 4 figures; two references added</comments><journal-ref>Neural Computation, October 2015, Vol. 27, No. 10 , Pages
  2097-2106</journal-ref><doi>10.1162/NECO_a_00775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare an entropy estimator $H_z$ recently discussed in [10] with two
estimators $H_1$ and $H_2$ introduced in [6][7]. We prove the identity $H_z
\equiv H_1$, which has not been taken into account in [10]. Then, we prove that
the statistical bias of $H_1$ is less than the bias of the ordinary likelihood
estimator of entropy. Finally, by numerical simulation we verify that for the
most interesting regime of small sample estimation and large event spaces, the
estimator $H_2$ has a significant smaller statistical error than $H_z$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05913</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05913</id><created>2015-03-18</created><updated>2015-06-15</updated><authors><author><keyname>Zhao</keyname><forenames>Bin</forenames></author><author><keyname>Guan</keyname><forenames>Yongqiang</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Leader selection and weight adjustment problems for multi-agent systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For an uncontrollable system, adding leaders and adjusting edge weights are
two methods to improve controllability. In this paper, controllability of
multi-agent systems under directed topologies is studied, especially on leader
selection problem and weight adjustment problem. For a given system, necessary
and sufficient algebraic conditions for controllability with fewest leaders are
proposed. From another perspective, when leaders are fixed, controllability
could be improved by adjusting edge weights, and therefore the system is
supposed to be structurally controllable, which holds if and only if the
communication topology contains a spanning tree. It is also proved that the
number of fewest edges needed to be assigned on new weights equals the rank
deficiency of controllability matrix. An algorithm on how to perform weight
adjustment is presented. Simulation examples are provided to illustrate the
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05937</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05937</id><created>2015-03-18</created><authors><author><keyname>Boscain</keyname><forenames>Ugo</forenames><affiliation>CMAP, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Mason</keyname><forenames>Paolo</forenames><affiliation>CMAP, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Panati</keyname><forenames>Gianluca</forenames><affiliation>CMAP, INRIA Saclay - Ile de France</affiliation></author><author><keyname>Sigalotti</keyname><forenames>Mario</forenames><affiliation>CMAP, INRIA Saclay - Ile de France</affiliation></author></authors><title>Controllability of spin-boson systems</title><categories>quant-ph cs.SY math-ph math.MP math.OC</categories><proxy>ccsd</proxy><report-no>Roma01.Math.MP</report-no><journal-ref>Journal of Mathematical Physics 56, 092101 (2015)</journal-ref><doi>10.1063/1.4929543</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the so-called spin-boson system, namely {a two-level
system} in interaction with a distinguished mode of a quantized bosonic field.
We give a brief description of the controlled Rabi and Jaynes--Cummings models
and we discuss their appearance in the mathematics and physics literature. We
then study the controllability of the Rabi model when the control is an
external field acting on the bosonic part. Applying geometric control
techniques to the Galerkin approximation and using perturbation theory to
guarantee non-resonance of the spectrum of the drift operator, we prove
approximate controllability of the system, for almost every value of the
interaction parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05938</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05938</id><created>2015-03-19</created><authors><author><keyname>Anselmi</keyname><forenames>Fabio</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>On Invariance and Selectivity in Representation Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss data representation which can be learned automatically from data,
are invariant to transformations, and at the same time selective, in the sense
that two points have the same representation only if they are one the
transformation of the other. The mathematical results here sharpen some of the
key claims of i-theory -- a recent theory of feedforward processing in sensory
cortex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05944</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05944</id><created>2015-03-19</created><updated>2015-08-19</updated><authors><author><keyname>Wu</keyname><forenames>Ting</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Collins</keyname><forenames>Christopher M.</forenames></author></authors><title>The Human Body and Millimeter-Wave Wireless Communication Systems:
  Interactions and Implications</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increasing interest in millimeter wave wireless communications,
investigations on interactions between the human body and millimeter wave
devices are becoming important. This paper gives examples of current regulatory
requirements, and provides an example for a 60 GHz transceiver. Also, the
propagation characteristics of millimeter-waves in the presence of the human
body are studied, and four models representing different body parts are
considered to evaluate thermal effects of millimeter-wave radiation on the
body. Simulation results show that about 34% to 42% of the incident power is
reflected at the skin surface at 60 GHz. This paper shows that power density is
not suitable to determine exposure compliance when millimeter wave devices are
used very close to the body. A temperature-based technique for the evaluation
of safety compliance is proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05947</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05947</id><created>2015-03-19</created><authors><author><keyname>Chen</keyname><forenames>Yanlai</forenames></author></authors><title>Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression
  Algorithm</title><categories>math.NA cs.AI cs.CV cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimension reduction is often needed in the area of data mining. The goal of
these methods is to map the given high-dimensional data into a low-dimensional
space preserving certain properties of the initial data. There are two kinds of
techniques for this purpose. The first, projective methods, builds an explicit
linear projection from the high-dimensional space to the low-dimensional one.
On the other hand, the nonlinear methods utilizes nonlinear and implicit
mapping between the two spaces. In both cases, the methods considered in
literature have usually relied on computationally very intensive matrix
factorizations, frequently the Singular Value Decomposition (SVD). The
computational burden of SVD quickly renders these dimension reduction methods
infeasible thanks to the ever-increasing sizes of the practical datasets.
  In this paper, we present a new decomposition strategy, Reduced Basis
Decomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given
$X$ the high-dimensional data, the method approximates it by $Y \, T (\approx
X)$ with $Y$ being the low-dimensional surrogate and $T$ the transformation
matrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. In
fact, it is significantly faster than SVD with comparable accuracy. $T$ can be
computed on the fly. Moreover, unlike many compression algorithms, it easily
finds the mapping for an arbitrary ``out-of-sample'' vector and it comes with
an ``error indicator'' certifying the accuracy of the compression. Numerical
results are shown validating these claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05951</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05951</id><created>2015-03-19</created><authors><author><keyname>Li</keyname><forenames>Kai</forenames></author><author><keyname>Qi</keyname><forenames>Guojun</forenames></author><author><keyname>Ye</keyname><forenames>Jun</forenames></author><author><keyname>Hua</keyname><forenames>Kien A.</forenames></author></authors><title>Rank Subspace Learning for Compact Hash Codes</title><categories>cs.LG cs.IR</categories><comments>10 pages</comments><acm-class>I.2.6; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The era of Big Data has spawned unprecedented interests in developing hashing
algorithms for efficient storage and fast nearest neighbor search. Most
existing work learn hash functions that are numeric quantizations of feature
values in projected feature space. In this work, we propose a novel hash
learning framework that encodes feature's rank orders instead of numeric values
in a number of optimal low-dimensional ranking subspaces. We formulate the
ranking subspace learning problem as the optimization of a piece-wise linear
convex-concave function and present two versions of our algorithm: one with
independent optimization of each hash bit and the other exploiting a sequential
learning framework. Our work is a generalization of the Winner-Take-All (WTA)
hash family and naturally enjoys all the numeric stability benefits of rank
correlation measures while being optimized to achieve high precision at very
short code length. We compare with several state-of-the-art hashing algorithms
in both supervised and unsupervised domain, showing superior performance in a
number of data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05959</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05959</id><created>2015-03-19</created><updated>2015-05-06</updated><authors><author><keyname>Michelucci</keyname><forenames>Pietro</forenames></author></authors><title>Human Computation and Convergence</title><categories>cs.HC cs.CY nlin.AO</categories><comments>Pre-publication draft of chapter. 24 pages, 3 figures; added
  references to page 1 and 3, and corrected typo</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans are the most effective integrators and producers of information,
directly and through the use of information-processing inventions. As these
inventions become increasingly sophisticated, the substantive role of humans in
processing information will tend toward capabilities that derive from our most
complex cognitive processes, e.g., abstraction, creativity, and applied world
knowledge. Through the advancement of human computation - methods that leverage
the respective strengths of humans and machines in distributed
information-processing systems - formerly discrete processes will combine
synergistically into increasingly integrated and complex information processing
systems. These new, collective systems will exhibit an unprecedented degree of
predictive accuracy in modeling physical and techno-social processes, and may
ultimately coalesce into a single unified predictive organism, with the
capacity to address societies most wicked problems and achieve planetary
homeostasis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05960</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05960</id><created>2015-03-19</created><updated>2015-09-22</updated><authors><author><keyname>Kazemian</keyname><forenames>Iman</forenames></author><author><keyname>Aref</keyname><forenames>Samin</forenames></author></authors><title>Hub Location under Uncertainty: a Minimax Regret Model for the
  Capacitated Problem with Multiple Allocations</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the capacitated hub location problem is formulated by a minimax
regret model, which takes into account uncertain setup cost and demand. We
focus on capacitated hub location with multiple allocations as a strategic
problem requiring one definite solution. Investigating how deterministic models
may lead to sub-optimal solutions, we provide an efficient formulation method
for the problem. A computational analysis is performed to investigate the
impact of uncertainty on the location of hubs. The suggested model is also
compared with an alternative method, seasonal optimization, in terms of
efficiency and practicability. The results indicate the importance of
incorporating stochasticity and variability of parameters in solving practical
hub location problems. Applying our method to a case study derived from an
industrial food production company, we solve a logistical problem involving
seasonal demand and uncertainty. The solution yields a definite hub network
configuration to be implemented throughout the planning horizon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05968</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05968</id><created>2015-03-19</created><updated>2015-07-06</updated><authors><author><keyname>Belabbas</keyname><forenames>M. -A.</forenames></author></authors><title>Geometric methods for optimal sensor design</title><categories>math.OC cs.SY math.ST stat.TH</categories><doi>10.1098/rspa.2015.0312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An observer is an estimator of the state of a dynamical system from noisy
sensor measurements. The need for observers is ubiquitous, with applications in
fields ranging from engineering to biology to economics. The most widely used
observer is the Kalman filter, which is known to be the optimal estimator of
the state when the noise is additive and Gaussian. Because its performance is
limited by the sensors to which it is paired, it is natural to seek an optimal
sensor for the Kalman filter. The problem is however not convex and, as a
consequence, many ad hoc methods have been used over the years to design
sensors. We show in this paper how to characterize and obtain the optimal
sensor for the Kalman filter. Precisely, we exhibit a positive definite
operator which optimal sensors have to commute with. We furthermore provide a
gradient flow to find optimal sensors, and prove the convergence of this
gradient flow to the unique minimum in a broad range of applications. This
optimal sensor yields the lowest possible estimation error for measurements
with a fixed signal to noise ratio. The results presented here also apply to
the dual problem of optimal actuator design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05971</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05971</id><created>2015-03-19</created><updated>2015-06-30</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Barcelo</keyname><forenames>H&#xe9;l&#xe8;ne</forenames></author></authors><title>Group size effect on cooperation in one-shot social dilemmas II.
  Curvilinear effect</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><comments>Forthcoming in PLoS ONE</comments><doi>10.1371/journal.pone.0131419</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a world in which many pressing global issues require large scale
cooperation, understanding the group size effect on cooperative behavior is a
topic of central importance. Yet, the nature of this effect remains largely
unknown, with lab experiments insisting that it is either positive or negative
or null, and field experiments suggesting that it is instead curvilinear. Here
we shed light on this apparent contradiction by considering a novel class of
public goods games inspired to the realistic scenario in which the natural
output limits of the public good imply that the benefit of cooperation
increases fast for early contributions and then decelerates. We report on a
large lab experiment providing evidence that, in this case, group size has a
curvilinear effect on cooperation, according to which intermediate-size groups
cooperate more than smaller groups and more than larger groups. In doing so,
our findings help fill the gap between lab experiments and field experiments
and suggest concrete ways to promote large scale cooperation among people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05972</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05972</id><created>2015-03-19</created><authors><author><keyname>Du</keyname><forenames>Jing</forenames></author></authors><title>Serious Game for Human Environmental Consciousness Education in
  Residents Daily Life</title><categories>cs.CY cs.HC</categories><acm-class>H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been challenging to find ways to educate people to have better
environmental consciousness. In some cases, people do not know what the right
behaviors are to protect the environment. Game engine has been used in the AEC
industry for visualization. However, it has barely been used in environmental
consciousness education, for example, what operation can reduce building energy
consumption, what items are recyclables. As social psychology studies show that
video game can influence human behavior, a good designed game should provide
the game player with right incentives and guide the users to make wiser choices
for better environmental protection. This paper discussed a method to use
serious game engines to educate the players the right actions that should be
taken under in different scenarios. These actions in real life will results in
a better environmental protection. The game proposed in this study is for
residential home operation. Other scenarios such as restaurant operation,
grocery store operations are discussed as expansion of this study. The game
players points will be calculated based on their performance on different
choices and when they surpass a certain level, different rewards will be gained
in order for them to adjust their current living style. The purpose of the game
is to raise the environmental consciousness among the game players and educate
them the right actions they can make to better protect the environment while
they are spending time on games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05977</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05977</id><created>2015-03-19</created><authors><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Vitter</keyname><forenames>Jeffrey Scott</forenames></author></authors><title>Dynamic Data Structures for Document Collections and Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the dynamic indexing problem, we must maintain a changing collection of
text documents so that we can efficiently support insertions, deletions, and
pattern matching queries. We are especially interested in developing efficient
data structures that store and query the documents in compressed form. All
previous compressed solutions to this problem rely on answering rank and select
queries on a dynamic sequence of symbols. Because of the lower bound in
[Fredman and Saks, 1989], answering rank queries presents a bottleneck in
compressed dynamic indexing. In this paper we show how this lower bound can be
circumvented using our new framework. We demonstrate that the gap between
static and dynamic variants of the indexing problem can be almost closed. Our
method is based on a novel framework for adding dynamism to static compressed
data structures. Our framework also applies more generally to dynamizing other
problems. We show, for example, how our framework can be applied to develop
compressed representations of dynamic graphs and binary relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05980</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05980</id><created>2015-03-19</created><authors><author><keyname>Li</keyname><forenames>Jie</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author></authors><title>Optimal Exact Repair Strategy for the Parity Nodes of the $(k+2,k)$
  Zigzag Code</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we reinterprets the $(k+2,k)$ Zigzag code in coding matrix and
then propose an optimal exact repair strategy for its parity nodes, whose
repair disk I/O approaches a lower bound derived in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05988</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05988</id><created>2015-03-20</created><updated>2016-02-14</updated><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author><author><keyname>Xu</keyname><forenames>Haifeng</forenames></author></authors><title>Algorithmic Bayesian Persuasion</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persuasion, defined as the act of exploiting an informational advantage in
order to effect the decisions of others, is ubiquitous. Indeed, persuasive
communication has been estimated to account for almost a third of all economic
activity in the US. This paper examines persuasion through a computational
lens, focusing on what is perhaps the most basic and fundamental model in this
space: the celebrated Bayesian persuasion model of Kamenica and Gentzkow. Here
there are two players, a sender and a receiver. The receiver must take one of a
number of actions with a-priori unknown payoff, and the sender has access to
additional information regarding the payoffs. The sender can commit to
revealing a noisy signal regarding the realization of the payoffs of various
actions, and would like to do so as to maximize her own payoff assuming a
perfectly rational receiver.
  We examine the sender's optimization task in three of the most natural input
models for this problem, and essentially pin down its computational complexity
in each. When the payoff distributions of the different actions are i.i.d. and
given explicitly, we exhibit a polynomial-time (exact) algorithm, and a
&quot;simple&quot; $(1-1/e)$-approximation algorithm. Our optimal scheme for the i.i.d.
setting involves an analogy to auction theory, and makes use of Border's
characterization of the space of reduced-forms for single-item auctions. When
action payoffs are independent but non-identical with marginal distributions
given explicitly, we show that it is #P-hard to compute the optimal expected
sender utility. Finally, we consider a general (possibly correlated) joint
distribution of action payoffs presented by a black box sampling oracle, and
exhibit a fully polynomial-time approximation scheme (FPTAS) with a bi-criteria
guarantee. We show that this result is the best possible in the black-box model
for information-theoretic reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.05992</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.05992</id><created>2015-03-20</created><authors><author><keyname>Chakraborti</keyname><forenames>Subhamoy</forenames></author><author><keyname>Acharjya</keyname><forenames>D. P.</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Application Security framework for Mobile App Development in Enterprise
  setup</title><categories>cs.CR</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enterprise Mobility has been increasing the reach over the years. Initially
Mobile devices were adopted as consumer devices. However, the enterprises world
over have rightly taken the leap and started using the ubiquitous technology
for managing its employees as well as to reach out to the customers. While the
Mobile ecosystem has been evolving over the years, the increased exposure of
mobility in Enterprise framework have caused major focus on the security
aspects of it. While a significant focus have been put on network security,
this paper discusses on the approach that can be taken at Mobile application
layer, which would reduce the risk to the enterprises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06004</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06004</id><created>2015-03-20</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author><author><keyname>Siti</keyname><forenames>W.</forenames></author><author><keyname>Jordaan</keyname><forenames>J.</forenames></author></authors><title>Feeder Load Balancing using Neural Network</title><categories>cs.NE</categories><comments>6 pages in final published version</comments><journal-ref>Lecture Notes in Computer Science, Springer, vol. 3972, pp.
  1311-1316, 2006</journal-ref><doi>10.1007/11760023_190</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes optimal
reconfiguration of the phase balancing using the neural network, to switch on
and off the different switches, allowing the three phases supply by the
transformer to the end-users to be balanced. This paper presents the
application examples of the proposed method using the real and simulated test
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06007</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06007</id><created>2015-03-20</created><authors><author><keyname>Cheung</keyname><forenames>Man Hon</forenames></author><author><keyname>Southwell</keyname><forenames>Richard</forenames></author><author><keyname>Hou</keyname><forenames>Fen</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Distributed Time-Sensitive Task Selection in Mobile Crowdsensing</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rich set of embedded sensors installed in smartphones and the large
number of mobile users, we witness the emergence of many innovative commercial
mobile crowdsensing applications that combine the power of mobile technology
with crowdsourcing to deliver time-sensitive and location-dependent information
to their customers. Motivated by these real-world applications, we consider the
task selection problem for heterogeneous users with different initial
locations, movement costs, movement speeds, and reputation levels. Computing
the social surplus maximization task allocation turns out to be an NP-hard
problem. Hence we focus on the distributed case, and propose an asynchronous
and distributed task selection (ADTS) algorithm to help the users plan their
task selections on their own. We prove the convergence of the algorithm, and
further characterize the computation time for users' updates in the algorithm.
Simulation results suggest that the ADTS scheme achieves the highest Jain's
fairness index and coverage comparing with several benchmark algorithms, while
yielding similar user payoff to a greedy centralized benchmark. Finally, we
illustrate how mobile users coordinate under the ADTS scheme based on some
practical movement time data derived from Google Maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06009</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06009</id><created>2015-03-20</created><updated>2015-08-11</updated><authors><author><keyname>Chhabra</keyname><forenames>Anamika</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author><author><keyname>Saini</keyname><forenames>Poonam</forenames></author><author><keyname>Bhat</keyname><forenames>Rajesh Shreedhar</forenames></author></authors><title>A Framework for Textbook Enhancement and Learning using Crowdsourced
  Annotations</title><categories>cs.CY cs.HC</categories><comments>11 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite a significant improvement in the educational aids in terms of
effective teaching-learning process, most of the educational content available
to the students is less than optimal in the context of being up-to-date,
exhaustive and easy-to-understand. There is a need to iteratively improve the
educational material based on the feedback collected from the students'
learning experience. This can be achieved by observing the students'
interactions with the content, and then having the authors modify it based on
this feedback. Hence, we aim to facilitate and promote communication between
the communities of authors, instructors and students in order to gradually
improve the educational material. Such a system will also help in students'
learning process by encouraging student-to-student teaching. Underpinning these
objectives, we provide the framework of a platform named Crowdsourced
Annotation System (CAS) where the people from these communities can collaborate
and benefit from each other. We use the concept of in-context annotations,
through which, the students can add their comments about the given text while
learning it. An experiment was conducted on 60 students who try to learn an
article of a textbook by annotating it for four days. According to the result
of the experiment, most of the students were highly satisfied with the use of
CAS. They stated that the system is extremely useful for learning and they
would like to use it for learning other concepts in future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06014</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06014</id><created>2015-03-20</created><updated>2015-08-18</updated><authors><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author><author><keyname>Lindquist</keyname><forenames>Anders</forenames></author></authors><title>Optimal estimation with missing observations via balanced time-symmetric
  stochastic models</title><categories>math.OC cs.SY</categories><comments>15 pages, 8 figures</comments><msc-class>93E11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider data fusion for the purpose of smoothing and interpolation based
on observation records with missing data. Stochastic processes are generated by
linear stochastic models. The paper begins by drawing a connection between time
reversal in stochastic systems and all-pass extensions. A particular
normalization (choice of basis) between the two time-directions allows the two
to share the same orthonormalized state process and simplifies the mathematics
of data fusion. In this framework we derive symmetric and balanced
Mayne-Fraser-like formulas that apply simultaneously to smoothing and
interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06021</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06021</id><created>2015-03-20</created><updated>2015-08-15</updated><authors><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Objective Optimization for Power Efficient Full-Duplex Wireless
  Communication Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at the IEEE Globecom 2015, San Diego, CA,
  USA, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate power efficient resource allocation algorithm
design for multiuser wireless communication systems employing a full-duplex
(FD) radio base station for serving multiple half-duplex (HD) downlink and
uplink users simultaneously. We propose a multi-objective optimization
framework for achieving two conflicting yet desirable system design objectives,
i.e., total downlink transmit power minimization and total uplink transmit
power minimization, while guaranteeing the quality-of-service of all users. To
this end, the weighted Tchebycheff method is adopted to formulate a
multi-objective optimization problem (MOOP). Although the considered MOOP is
non-convex, we solve it optimally by semidefinite programming relaxation.
Simulation results not only unveil the trade-off between the total downlink and
the total uplink transmit power, but also confirm that the proposed FD system
provides substantial power savings over traditional HD systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06022</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06022</id><created>2015-03-20</created><updated>2015-06-23</updated><authors><author><keyname>Danos</keyname><forenames>Vincent</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Harmer</keyname><forenames>Russell</forenames><affiliation>CNRS &amp; ENS Lyon</affiliation></author><author><keyname>Honorato-Zimmer</keyname><forenames>Ricardo</forenames><affiliation>University of Edinburgh</affiliation></author></authors><title>Thermodynamic graph-rewriting</title><categories>cs.LO q-bio.MN</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:13) 2015</journal-ref><doi>10.2168/LMCS-11(2:13)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new thermodynamic approach to stochastic graph-rewriting. The
ingredients are a finite set of reversible graph-rewriting rules called
generating rules, a finite set of connected graphs P called energy patterns and
an energy cost function. The idea is that the generators define the qualitative
dynamics, by showing which transformations are possible, while the energy
patterns and cost function specify the long-term probability $\pi$ of any
reachable graph. Given the generators and energy patterns, we construct a
finite set of rules which (i) has the same qualitative transition system as the
generators; and (ii) when equipped with suitable rates, defines a
continuous-time Markov chain of which $\pi$ is the unique fixed point. The
construction relies on the use of site graphs and a technique of `growth
policy' for quantitative rule refinement which is of independent interest. This
division of labour between the qualitative and long-term quantitative aspects
of the dynamics leads to intuitive and concise descriptions for realistic
models (see the examples in S4 and S5). It also guarantees thermodynamical
consistency (AKA detailed balance), otherwise known to be undecidable, which is
important for some applications. Finally, it leads to parsimonious
parameterizations of models, again an important point in some applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06025</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06025</id><created>2015-03-20</created><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author><author><keyname>Karthick</keyname><forenames>T.</forenames></author></authors><title>Weighted Efficient Domination in Classes of $P_6$-free Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a graph $G$, an efficient dominating set is a subset $D$ of vertices such
that $D$ is an independent set and each vertex outside $D$ has exactly one
neighbor in $D$. The Minimum Weight Efficient Dominating Set (Min-WED) problem
asks for an efficient dominating set of total minimum weight in a given
vertex-weighted graph; the Maximum Weight Efficient Dominating Set (Max-WED)
problem is defined similarly. The Min-WED/Max-WED is known to be $NP$-complete
for $P_7$-free graphs, and is known to be polynomial time solvable for
$P_5$-free graphs. However, the computational complexity of the Min-WED/Max-WED
problem is unknown for $P_6$-free graphs. In this paper, we show that the
Min-WED/Max-WED problem can be solved in polynomial time for two subclasses of
$P_6$-free graphs, namely for ($P_6,S_{1,1,3}$)-free graphs, and for ($P_6$,
bull)-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06029</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06029</id><created>2015-03-20</created><authors><author><keyname>Kaczmarski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Rz&#x105;&#x17c;ewski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Wolant</keyname><forenames>Albert</forenames></author></authors><title>Massively Parallel Construction of the Cell Graph</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion planning is an important and well-studied field of robotics. A typical
approach to finding a route is to construct a {\em cell graph} representing a
scene and then to find a path in such a graph. In this paper we present and
analyze parallel algorithms for constructing the cell graph on a SIMD-like GPU
processor.
  Additionally, we present a new implementation of the dictionary data type on
a GPU device. In the contrary to hash tables, which are common in GPU
algorithms, it uses a search tree in which all values are kept in leaves. With
such a structure we can effectively perform dictionary operations on a set of
long vectors over a limited alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06046</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06046</id><created>2015-03-20</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Deep Transform: Cocktail Party Source Separation via Probabilistic
  Re-Synthesis</title><categories>cs.SD cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cocktail party listening scenarios, the human brain is able to separate
competing speech signals. However, the signal processing implemented by the
brain to perform cocktail party listening is not well understood. Here, we
trained two separate convolutive autoencoder deep neural networks (DNN) to
separate monaural and binaural mixtures of two concurrent speech streams. We
then used these DNNs as convolutive deep transform (CDT) devices to perform
probabilistic re-synthesis. The CDTs operated directly in the time-domain. Our
simulations demonstrate that very simple neural networks are capable of
exploiting monaural and binaural information available in a cocktail party
listening scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06052</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06052</id><created>2015-03-20</created><updated>2015-03-24</updated><authors><author><keyname>Molinero</keyname><forenames>X.</forenames></author><author><keyname>Olsen</keyname><forenames>M.</forenames></author><author><keyname>Serna</keyname><forenames>M.</forenames></author></authors><title>On the Complexity of Exchanging</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the computational complexity of the problem of deciding whether,
for a given simple game, there exists the possibility of rearranging the
participants in a set of $j$ given losing coalitions into a set of $j$ winning
coalitions. We also look at the problem of turning winning coalitions into
losing coalitions. We analyze the problem when the simple game is represented
by a list of wining, losing, minimal winning or maximal loosing coalitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06060</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06060</id><created>2015-03-20</created><authors><author><keyname>Guigour&#xe8;s</keyname><forenames>Romain</forenames></author><author><keyname>Gay</keyname><forenames>Dominique</forenames></author><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames></author><author><keyname>Cl&#xe9;rot</keyname><forenames>Fabrice</forenames></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames></author></authors><title>Country-scale Exploratory Analysis of Call Detail Records through the
  Lens of Data Grid Models</title><categories>cs.DB stat.ML</categories><comments>Submitted to Industrial Track of ECML/PKDD 2015</comments><msc-class>stat.ML - Machine Learning</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Call Detail Records (CDRs) are data recorded by telecommunications companies,
consisting of basic informations related to several dimensions of the calls
made through the network: the source, destination, date and time of calls. CDRs
data analysis has received much attention in the recent years since it might
reveal valuable information about human behavior. It has shown high added value
in many application domains like e.g., communities analysis or network
planning. In this paper, we suggest a generic methodology for summarizing
information contained in CDRs data. The method is based on a parameter-free
estimation of the joint distribution of the variables that describe the calls.
We also suggest several well-founded criteria that allows one to browse the
summary at various granularities and to explore the summary by means of
insightful visualizations. The method handles network graph data, temporal
sequence data as well as user mobility data stemming from original CDRs data.
We show the relevance of our methodology for various case studies on real-world
CDRs data from Ivory Coast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06061</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06061</id><created>2015-03-20</created><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author></authors><title>The selection monad as a CPS transformation</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computation in the continuation monad returns a final result given a
continuation, ie. it is a function with type $(X \to R) \to R$. If we instead
return the intermediate result at $X$ then our computation is called a
selection function. Selection functions appear in diverse areas of mathematics
and computer science (especially game theory, proof theory and topology) but
the existing literature does not heavily emphasise the fact that the selection
monad is a CPS translation. In particular it has so far gone unnoticed that the
selection monad has a call/cc-like operator with interesting similarities and
differences to the usual call/cc, which we explore experimentally using
Haskell.
  Selection functions can be used whenever we find the intermediate result more
interesting than the final result. For example a SAT solver computes an
assignment to a boolean function, and then its continuation decides whether it
is a satisfying assignment, and we find the assignment itself more interesting
than the fact that it is or is not satisfying. In game theory we find the move
chosen by a player more interesting than the outcome that results from that
move. The author and collaborators are developing a theory of games in which
selection functions are viewed as generalised notions of rationality, used to
model players. By realising that strategic contexts in game theory are examples
of continuations we can see that classical game theory narrowly misses being in
CPS, and that a small change of viewpoint yields a theory of games that is
better behaved, and especially more compositional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06063</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06063</id><created>2015-03-20</created><authors><author><keyname>Papoutsakis</keyname><forenames>Ioannis</forenames></author></authors><title>Tree spanners of small diameter</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph that contains a spanning tree of diameter at most $t$ clearly admits
a tree $t$-spanner, since a tree $t$-spanner of a graph $G$ is a sub tree of
$G$ such that the distance between pairs of vertices in the tree is at most $t$
times their distance in $G$. In this paper, graphs that admit a tree
$t$-spanner of diameter at most $t+1$ are studied. For $t$ equal to 1 or 2 the
problem has been solved. For $t=3$ we present an algorithm that determines if a
graph admits a tree 3-spanner of diameter at most 4. For $t\geq4$ it is proved
that it is an NP-complete problem to decide whether a graph admits a tree
$t$-spanner of diameter at most $t+1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06072</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06072</id><created>2015-03-20</created><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author></authors><title>String diagrams for game theory</title><categories>cs.GT cs.LO math.CT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a monoidal category whose morphisms are games (in the
sense of game theory, not game semantics) and an associated diagrammatic
language. The two basic operations of a monoidal category, namely categorical
composition and tensor product, correspond roughly to sequential and
simultaneous composition of games. This leads to a compositional theory in
which we can reason about properties of games in terms of corresponding
properties of the component parts. In particular, we give a definition of Nash
equilibrium which is recursive on the causal structure of the game.
  The key technical idea in this paper is the use of continuation passing style
for reasoning about the future consequences of players' choices, closely based
on applications of selection functions in game theory. Additionally, the clean
categorical foundation gives many opportunities for generalisation, for example
to learning agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06081</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06081</id><created>2015-03-20</created><authors><author><keyname>Dolce</keyname><forenames>Francesco</forenames></author><author><keyname>Perrin</keyname><forenames>Dominique</forenames></author></authors><title>Enumeration formul{\ae} in neutral sets</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several enumeration results holding in sets of words called
neutral and which satisfy restrictive conditions on the set of possible
extensions of nonempty words. These formulae concern return words and bifix
codes. They generalize formulae previously known for Sturmian sets or more
generally for tree sets. We also give a geometric example of this class of
sets, namely the natural coding of some interval exchange transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06087</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06087</id><created>2015-03-20</created><updated>2015-07-30</updated><authors><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Schon</keyname><forenames>Claudia</forenames></author><author><keyname>Stolzenburg</keyname><forenames>Frieder</forenames></author><author><keyname>Weis</keyname><forenames>Karl-Heinz</forenames></author><author><keyname>Wirth</keyname><forenames>Claus-Peter</forenames></author></authors><title>The RatioLog Project: Rational Extensions of Logical Reasoning</title><categories>cs.AI</categories><comments>7 pages, 3 figures</comments><journal-ref>KI, 29(3):271-277, 2015</journal-ref><doi>10.1007/s13218-015-0377-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-level cognition includes logical reasoning and the ability of question
answering with common sense. The RatioLog project addresses the problem of
rational reasoning in deep question answering by methods from automated
deduction and cognitive computing. In a first phase, we combine techniques from
information retrieval and machine learning to find appropriate answer
candidates from the huge amount of text in the German version of the free
encyclopedia &quot;Wikipedia&quot;. In a second phase, an automated theorem prover tries
to verify the answer candidates on the basis of their logical representations.
In a third phase - because the knowledge may be incomplete and inconsistent -,
we consider extensions of logical reasoning to improve the results. In this
context, we work toward the application of techniques from human reasoning: We
employ defeasible reasoning to compare the answers w.r.t. specificity, deontic
logic, normative reasoning, and model construction. Moreover, we use integrated
case-based reasoning and machine learning techniques on the basis of the
semantic structure of the questions and answer candidates to learn giving the
right answers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06091</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06091</id><created>2015-03-03</created><updated>2015-03-28</updated><authors><author><keyname>Ma</keyname><forenames>Ding</forenames></author><author><keyname>Sandberg</keyname><forenames>Mats</forenames></author><author><keyname>Jiang</keyname><forenames>Bin</forenames></author></authors><title>Characterizing the Heterogeneity of the OpenStreetMap Data and Community</title><categories>cs.SI nlin.AO physics.data-an</categories><comments>13 pages, 6 figures, and 8 tables</comments><journal-ref>ISPRS International Journal of Geo-Information, 4(2), 535-550,
  2015</journal-ref><doi>10.3390/ijgi4020535</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenStreetMap (OSM) constitutes an unprecedented, free, geographic
information source contributed by millions of individuals, resulting in a
database of great volume and heterogeneity. In this study, we characterize the
heterogeneity of the entire OSM database and historical archive in the context
of big data. We consider all users, geographic elements, and user contributions
from an eight-year data archive, at a size of 692 GB. We rely on some nonlinear
methods such as power-law statistics and head/tail breaks to uncover and
illustrate the underlying scaling properties. All three aspects (users,
elements, and contributions) demonstrate striking power laws or heavy-tailed
distributions. The heavy-tailed distributions imply that there are far more
small elements than large ones, far more inactive users than active ones, and
far more lightly edited elements than heavily edited ones. Furthermore, about
500 users in the core group of the OSM are highly networked in terms of
collaboration.
  Keywords: OpenStreetMap, big data, power laws, head/tail breaks, ht-index
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06095</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06095</id><created>2015-03-20</created><authors><author><keyname>Felty</keyname><forenames>Amy P.</forenames></author><author><keyname>Momigliano</keyname><forenames>Alberto</forenames></author><author><keyname>Pientka</keyname><forenames>Brigitte</forenames></author></authors><title>The Next 700 Challenge Problems for Reasoning with Higher-Order Abstract
  Syntax Representations: Part 1-A Common Infrastructure for Benchmarks</title><categories>cs.LO</categories><comments>42 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of logical frameworks support the use of higher-order abstract
syntax (HOAS) in representing formal systems. Although these systems seem
superficially the same, they differ in a variety of ways; for example, how they
handle a context of assumptions and which theorems about a given formal system
can be concisely expressed and proved. Our contributions in this paper are
three-fold: 1) we develop a common infrastructure for representing benchmarks
for systems supporting reasoning with binders, 2) we present several concrete
benchmarks, which highlight a variety of different aspects of reasoning within
a context of assumptions, and 3) we design an open repository ORBI, (Open
challenge problem Repository for systems supporting reasoning with BInders).
Our work sets the stage for providing a basis for qualitative comparison of
different systems. This allows us to review and survey the state of the art,
which we do in great detail for four systems in Part 2 of this paper (Felty et
al, 2015). It also allows us to outline future fundamental research questions
regarding the design and implementation of meta-reasoning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06101</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06101</id><created>2015-03-20</created><authors><author><keyname>Al-Shatri</keyname><forenames>Hussein</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Ganesan</keyname><forenames>Rakash SivaSiva</forenames></author><author><keyname>Klein</keyname><forenames>Anja</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author></authors><title>Maximizing the Sum Rate in Cellular Networks Using Multi-Convex
  Optimization</title><categories>cs.IT math.IT</categories><comments>24 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel algorithm to maximize the sum rate in
interference-limited scenarios where each user decodes its own message with the
presence of unknown interferences and noise considering the
signal-to-interference-plus-noise-ratio. It is known that the problem of
adapting the transmit and receive filters of the users to maximize the sum rate
with a sum transmit power constraint is non-convex. Our novel approach is to
formulate the sum rate maximization problem as an equivalent multi-convex
optimization problem by adding two sets of auxiliary variables. An iterative
algorithm which alternatingly adjusts the system variables and the auxiliary
variables is proposed to solve the multi-convex optimization problem. The
proposed algorithm is applied to a downlink cellular scenario consisting of
several cells each of which contains a base station serving several mobile
stations. We examine the two cases, with or without several half-duplex
amplify-and-forward relays assisting the transmission. A sum power constraint
at the base stations and a sum power constraint at the relays are assumed.
Finally, we show that the proposed multi-convex formulation of the sum rate
maximization problem is applicable to many other wireless systems in which the
estimated data symbols are multi-affine functions of the system variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06115</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06115</id><created>2015-03-20</created><updated>2015-06-29</updated><authors><author><keyname>Corrigan-Gibbs</keyname><forenames>Henry</forenames></author><author><keyname>Boneh</keyname><forenames>Dan</forenames></author><author><keyname>Mazi&#xe8;res</keyname><forenames>David</forenames></author></authors><title>Riposte: An Anonymous Messaging System Handling Millions of Users</title><categories>cs.CR</categories><comments>This is the full version of a paper published at the IEEE Symposium
  on Security and Privacy 2015 (&quot;Oakland 2015&quot;)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Riposte, a new system for anonymous broadcast messaging.
Riposte is the first such system, to our knowledge, that simultaneously
protects against traffic-analysis attacks, prevents anonymous denial-of-service
by malicious clients, and scales to million-user anonymity sets. To achieve
these properties, Riposte makes novel use of techniques used in systems for
private information retrieval and secure multi-party computation. For
latency-tolerant workloads with many more readers than writers (e.g. Twitter,
Wikileaks), we demonstrate that a three-server Riposte cluster can build an
anonymity set of 2,895,216 users in 32 hours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06124</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06124</id><created>2015-03-20</created><authors><author><keyname>Liu</keyname><forenames>Rui</forenames></author></authors><title>A Multi-Agent System of Project Bidding Management Simulation</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simulation model based on the general framework of
Multi-Agent System (MAS) that can be used to investigate construction project
bidding process. Specifically, it can be used to investigate different
strategies in project bidding management from the general contractors'
perspective. The effectiveness of the studied management strategies is
evaluated by the quality, time and cost of bidding activities. As an
implementation of MAS theory, this work is expected to test the suitability of
MAS in studying construction management related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06126</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06126</id><created>2015-03-20</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author></authors><title>Polynomial complexity recognizing a tropical linear variety</title><categories>cs.SC math.AG</categories><msc-class>15T05</msc-class><acm-class>I.1.2</acm-class><journal-ref>Lect. Notes Comput. Sci., 2015, vol. 9301, p. 152-157</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polynomial complexity algorithm is designed which tests whether a point
belongs to a given tropical linear variety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06133</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06133</id><created>2015-03-20</created><updated>2015-08-31</updated><authors><author><keyname>Khazaeni</keyname><forenames>Yasaman</forenames></author><author><keyname>Cassandras</keyname><forenames>Christos G.</forenames></author></authors><title>An Optimal Control Approach for the Data Harvesting Problem</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for trajectory planning to solve the data harvesting
problem. In a two-dimensional mission space, $N$ mobile agents are tasked with
the collection of data generated at $M$ stationary sources and delivery to a
base aiming at minimizing expected delays. An optimal control formulation of
this problem provides some initial insights regarding its solution, but it is
computationally intractable, especially in the case where the data generating
processes are stochastic. We propose an agent trajectory parameterization in
terms of general function families which can be subsequently optimized on line
through the use of Infinitesimal Perturbation Analysis (IPA). Explicit results
are provided for the case of elliptical and Fourier series trajectories and
some properties of the solution are identified, including robustness with
respect to the data generation processes and scalability in the size of an
event set characterizing the underlying hybrid dynamic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06144</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06144</id><created>2015-03-20</created><updated>2015-09-30</updated><authors><author><keyname>Mendoza-Armenta</keyname><forenames>Sarai</forenames></author><author><keyname>Dobson</keyname><forenames>Ian</forenames></author></authors><title>Applying a formula for generator redispatch to damp interarea
  oscillations using synchrophasors</title><categories>cs.SY math.OC</categories><comments>To appear in IEEE Transactions on Power Systems, accepted September
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If an interarea oscillatory mode has insufficient damping, generator
redispatch can be used to improve its damping. We explain and apply a new
analytic formula for the modal sensitivity to rank the best pairs of generators
to redispatch. The formula requires some dynamic power system data and we show
how to obtain that data from synchrophasor measurements. The application of the
formula to damp interarea modes is explained and illustrated with interarea
modes of the New England 10-machine power system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06151</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06151</id><created>2015-03-20</created><authors><author><keyname>Litvak</keyname><forenames>Maxim</forenames></author></authors><title>On measuring linguistic intelligence</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the problem of measuring how many languages a person
&quot;effectively&quot; speaks given that some of the languages are close to each other.
In other words, to assign a meaningful number to her language portfolio.
Intuition says that someone who speaks fluently Spanish and Portuguese is
linguistically less proficient compared to someone who speaks fluently Spanish
and Chinese since it takes more effort for a native Spanish speaker to learn
Chinese than Portuguese. As the number of languages grows and their proficiency
levels vary, it gets even more complicated to assign a score to a language
portfolio. In this article we propose such a measure (&quot;linguistic quotient&quot; -
LQ) that can account for these effects.
  We define the properties that such a measure should have. They are based on
the idea of coherent risk measures from the mathematical finance. Having laid
down the foundation, we propose one such a measure together with the algorithm
that works on languages classification tree as input.
  The algorithm together with the input is available online at lingvometer.com
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06154</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06154</id><created>2015-03-20</created><authors><author><keyname>Rizvi</keyname><forenames>Syed Zain</forenames></author><author><keyname>Fong</keyname><forenames>Philip W. L.</forenames></author><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Sellwood</keyname><forenames>James</forenames></author></authors><title>Relationship-Based Access Control for OpenMRS</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the access control models of social network systems,
Relationship-Based Access Control (ReBAC) was recently proposed as a
general-purpose access control paradigm for application domains in which
authorization must take into account the relationship between the access
requestor and the resource owner. The healthcare domain is envisioned to be an
archetypical application domain in which ReBAC is sorely needed: e.g., my
patient record should be accessible only by my family doctor, but not by all
doctors.
  In this work, we demonstrate for the first time that ReBAC can be
incorporated into a production-scale medical records system, OpenMRS, with
backward compatibility to the legacy RBAC mechanism. Specifically, we extend
the access control mechanism of OpenMRS to enforce ReBAC policies. Our
extensions incorporate and extend advanced ReBAC features recently proposed by
Crampton and Sellwood. In addition, we designed and implemented the first
administrative model for ReBAC. In this paper, we describe our ReBAC
implementation, discuss the system engineering lessons learnt as a result, and
evaluate the experimental work we have undertaken. In particular, we compare
the performance of the various authorization schemes we implemented, thereby
demonstrating the feasibility of ReBAC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06169</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06169</id><created>2015-03-20</created><authors><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Zhou</keyname><forenames>Yaqin</forenames></author></authors><title>Networked Stochastic Multi-Armed Bandits with Combinatorial Strategies</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate a largely extended version of classical MAB
problem, called networked combinatorial bandit problems. In particular, we
consider the setting of a decision maker over a networked bandits as follows:
each time a combinatorial strategy, e.g., a group of arms, is chosen, and the
decision maker receives a reward resulting from her strategy and also receives
a side bonus resulting from that strategy for each arm's neighbor. This is
motivated by many real applications such as on-line social networks where
friends can provide their feedback on shared content, therefore if we promote a
product to a user, we can also collect feedback from her friends on that
product. To this end, we consider two types of side bonus in this study: side
observation and side reward. Upon the number of arms pulled at each time slot,
we study two cases: single-play and combinatorial-play. Consequently, this
leaves us four scenarios to investigate in the presence of side bonus:
Single-play with Side Observation, Combinatorial-play with Side Observation,
Single-play with Side Reward, and Combinatorial-play with Side Reward. For each
case, we present and analyze a series of \emph{zero regret} polices where the
expect of regret over time approaches zero as time goes to infinity. Extensive
simulations validate the effectiveness of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06171</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06171</id><created>2015-03-20</created><authors><author><keyname>Ji</keyname><forenames>Yuting</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author><author><keyname>Thomas</keyname><forenames>Robert J.</forenames></author></authors><title>Probabilistic Forecast of Real-Time LMP and Network Congestion</title><categories>stat.AP cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of short-term forecast of real-time locational marginal price
(LMP) and network congestion is considered from a system operator perspective.
A new probabilistic forecast technique is proposed based on a multiparametric
programming formulation that partitions the uncertainty parameter space into
critical regions from which the conditional probability distribution of the
real-time LMP/congestion is obtained. The proposed method incorporates
load/generation forecast, time varying operation constraints, and probabilistic
contingency models. By shifting the computation cost associated with
multiparametric program offline, the online computation cost is significantly
reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06182</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06182</id><created>2015-03-20</created><authors><author><keyname>Campbell</keyname><forenames>John M.</forenames></author><author><keyname>Ellis</keyname><forenames>R. Keith</forenames></author><author><keyname>Giele</keyname><forenames>Walter T.</forenames></author></authors><title>A Multi-Threaded Version of MCFM</title><categories>physics.comp-ph cs.DC cs.MS hep-ph</categories><comments>7 pages, 3 figures, MCFM-7.0 which runs under the OpenMP protocol as
  described in this paper can be downloaded from http://mcfm.fnal.gov</comments><report-no>Fermilab-PUB-15-043-T</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on our findings modifying MCFM using OpenMP to implement
multi-threading. By using OpenMP, the modified MCFM will execute on any
processor, automatically adjusting to the number of available threads. We
modified the integration routine VEGAS to distribute the event evaluation over
the threads, while combining all events at the end of every iteration to
optimize the numerical integration. Special care has been taken that the
results of the Monte Carlo integration are independent of the number of threads
used, to facilitate the validation of the OpenMP version of MCFM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06188</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06188</id><created>2015-03-20</created><updated>2015-12-07</updated><authors><author><keyname>Avgustinovich</keyname><forenames>Sergey V.</forenames></author><author><keyname>Frid</keyname><forenames>Anna E.</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author></authors><title>Ergodic infinite permutations of minimal complexity</title><categories>math.CO cs.DM math.DS</categories><comments>An old (weaker) version presented at DLT 2015. The current version
  submitted to Ergodic Theory and Dynamical Systems</comments><msc-class>68R15, 05A05, 37E10, 37E99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An infinite permutation can be defined as a linear ordering of the set of
natural numbers. Similarly to infinite words, a complexity $p(n)$ of an
infinite permutation is defined as a function counting the number of its
factors of length $n$. For infinite words, a classical result of Morse and
Hedlind, 1940, states that if the complexity of an infinite word satisfies
$p(n)\leq n$ for some $n$, then the word is ultimately periodic. Hence minimal
complexity of aperiodic words is equal to $n+1$, and words with such complexity
are called Sturmian. For infinite permutations this does not hold: There exist
aperiodic permutations with complexity functions of arbitrarily slow growth,
and hence there are no permutations of minimal complexity.
  In the paper we introduce a new notion of ergodic permutation, i.e., a
permutation which can be defined by a sequence of numbers from $[0, 1]$, such
that the frequency of its elements in any interval is equal to the length of
that interval. We show that the minimal complexity of an ergodic permutation is
$p(n)=n$, and that the class of ergodic permutations of minimal complexity
coincides with the class of so-called Sturmian permutations, directly related
to Sturmian words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06201</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06201</id><created>2015-03-20</created><authors><author><keyname>Osman</keyname><forenames>Akin</forenames><affiliation>CGS</affiliation></author><author><keyname>Mines</keyname><forenames>Kazak&#xe7;i</forenames><affiliation>CGS</affiliation></author></authors><title>Data Science as a New Frontier for Design</title><categories>cs.AI stat.OT</categories><comments>International Conference on Engineering Design, Jul 2015, Milan,
  Italy</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to contribute to the challenge of transferring
know-how, theories and methods from design research to the design processes in
information science and technologies. More specifically, we shall consider a
domain, namely data-science, that is becoming rapidly a globally invested
research and development axis with strong imperatives for innovation given the
data deluge we are currently facing. We argue that, in order to rise to the
data-related challenges that the society is facing, data-science initiatives
should ensure a renewal of traditional research methodologies that are still
largely based on trial-error processes depending on the talent and insights of
a single (or a restricted group of) researchers. It is our claim that design
theories and methods can provide, at least to some extent, the much-needed
framework. We will use a worldwide data-science challenge organized to study a
technical problem in physics, namely the detection of Higgs boson, as a use
case to demonstrate some of the ways in which design theory and methods can
help in analyzing and shaping the innovation dynamics in such projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06239</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06239</id><created>2015-03-20</created><authors><author><keyname>Zhang</keyname><forenames>Jinye</forenames></author><author><keyname>Ou</keyname><forenames>Zhijian</forenames></author></authors><title>Block-Wise MAP Inference for Determinantal Point Processes with
  Application to Change-Point Detection</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Existing MAP inference algorithms for determinantal point processes (DPPs)
need to calculate determinants or conduct eigenvalue decomposition generally at
the scale of the full kernel, which presents a great challenge for real-world
applications. In this paper, we introduce a class of DPPs, called BwDPPs, that
are characterized by an almost block diagonal kernel matrix and thus can allow
efficient block-wise MAP inference. Furthermore, BwDPPs are successfully
applied to address the difficulty of selecting change-points in the problem of
change-point detection (CPD), which results in a new BwDPP-based CPD method,
named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is
first created based on existing well-studied metrics. Then, these change-point
candidates are treated as DPP items, and DPP-based subset selection is
conducted to give the final estimate of the change-points that favours both
quality and diversity. The effectiveness of BwDppCpd is demonstrated through
extensive experiments on five real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06242</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06242</id><created>2015-03-20</created><authors><author><keyname>Parzysz</keyname><forenames>Fanny</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author><author><keyname>Gagnon</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Modeling and Analysis of Energy Efficiency and Interference for Cellular
  Relay Deployment</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Journal on Selected Areas in Communications,
  Series on Green Communications and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By relying on a wireless backhaul link, relay stations enhance the
performance of cellular networks at low infrastructure cost, but at the same
time, they can aggravate the interference issue. In this paper, we analyze for
several relay coding schemes the maximum energy gain provided by a relay,
taking into account the additional relay-generated interference to neighboring
cells. First, we define spatial areas for relaying efficiency in log-normal
shadowing environments and propose three easily-computable and tractable
models. These models allow the prediction of 1) the probability of
energy-efficient relaying, 2) the spatial distribution of energy consumption
within a cell and 3) the average interference generated by relays. Second, we
define a new performance metric that jointly captures both aspects of energy
and interference, and characterize the optimal number and location of relays.
These results are obtainable with significantly lower complexity and execution
time when applying the proposed models as compared to system simulations.We
highlight that energy-efficient relay deployment does not necessarily lead to
interference reduction and conversely, an interference-aware deployment is
suboptimal in the energy consumption. We then propose a map showing the optimal
utilization of relay coding schemes across a cell. This map combines two-hop
relaying and energy-optimized partial decode-forward as a function of their
respective circuitry consumption. Such a combination not only alleviates the
interference issue, but also leads to a reduction in the number of relays
required for the same performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06244</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06244</id><created>2015-03-20</created><authors><author><keyname>Lubin</keyname><forenames>Benjamin</forenames></author></authors><title>Games and Meta-Games: Pricing Rules for Combinatorial Mechanisms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In settings where full incentive-compatibility is not available, such as
core-constraint combinatorial auctions and budget-balanced combinatorial
exchanges, we may wish to design mechanisms that are as incentive-compatible as
possible. This paper offers a new characterization of approximate
incentive-compatibility by casting the pricing problem as a meta-game between
the center and the participating agents. Through a suitable set of
simplifications, we describe the equilibrium of this game as a variational
problem. We use this to characterize the space of optimal prices, enabling
closed-form solutions in restricted cases, and numerically-determined prices in
the general case. We offer theory motivating this approach, and numerical
experiments showing its application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06250</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06250</id><created>2015-03-20</created><authors><author><keyname>Razzaghi</keyname><forenames>Talayeh</forenames></author><author><keyname>Roderick</keyname><forenames>Oleg</forenames></author><author><keyname>Safro</keyname><forenames>Ilya</forenames></author><author><keyname>Marko</keyname><forenames>Nick</forenames></author></authors><title>Fast Imbalanced Classification of Healthcare Data with Missing Values</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In medical domain, data features often contain missing values. This can
create serious bias in the predictive modeling. Typical standard data mining
methods often produce poor performance measures. In this paper, we propose a
new method to simultaneously classify large datasets and reduce the effects of
missing values. The proposed method is based on a multilevel framework of the
cost-sensitive SVM and the expected maximization imputation method for missing
values, which relies on iterated regression analyses. We compare classification
results of multilevel SVM-based algorithms on public benchmark datasets with
imbalanced classes and missing values as well as real data in health
applications, and show that our multilevel SVM-based method produces fast, and
more accurate and robust classification results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06264</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06264</id><created>2015-03-21</created><authors><author><keyname>Zhao</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Liumeng</forenames></author><author><keyname>Zheng</keyname><forenames>Xi</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>HyCell: Enabling GREEN Base Station Operations in Software-Defined Radio
  Access Networks</title><categories>cs.NI</categories><comments>6 pages, 4 figures, accepted by IEEE ICC 2015 Workshop on Next
  Generation Green ICT</comments><doi>10.1109/ICCW.2015.7247614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radio access networks (RANs) need to support massive and diverse data
traffic with limited spectrum and energy. To cope with this challenge,
software-defined radio access network (SDRAN) architectures have been proposed
to renovate the RANs. However, current researches lack the design and
evaluation of network protocols. In this paper, we address this problem by
presenting the protocol design and evaluation of hyper-cellular networks
(HyCell), an SDRAN framework making base station (BS) operations globally
resource-optimized and energy-efficient (GREEN). Specifically, we first propose
a separation scheme to realize the decoupled air interface in HyCell. Then we
design a BS dispatching protocol which determines and assigns the optimal BS
for serving mobile users, and a BS sleeping protocol to improve the network
energy efficiency. Finally, we evaluate the proposed design in our HyCell
testbed. Our evaluation validates the feasibility of the proposed separation
scheme, demonstrates the effectiveness of BS dispatching, and shows great
potential in energy saving through BS sleeping control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06268</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06268</id><created>2015-03-21</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Kumar</keyname><forenames>Suhansanu</forenames></author><author><keyname>Goyal</keyname><forenames>Pawan</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>On the categorization of scientific citation profiles in computer
  sciences</title><categories>cs.DL</categories><comments>11 pages, 10 figures, Accepted in Communications of the ACM (CACM),
  2015. arXiv admin note: text overlap with arXiv:1206.0108 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common consensus in the literature is that the citation profile of
published articles in general follows a universal pattern - an initial growth
in the number of citations within the first two to three years after
publication followed by a steady peak of one to two years and then a final
decline over the rest of the lifetime of the article. This observation has long
been the underlying heuristic in determining major bibliometric factors such as
the quality of a publication, the growth of scientific communities, impact
factor of publication venues etc. In this paper, we gather and analyze a
massive dataset of scientific papers from the computer science domain and
notice that the citation count of the articles over the years follows a
remarkably diverse set of patterns - a profile with an initial peak (PeakInit),
with distinct multiple peaks (PeakMul), with a peak late in time (PeakLate),
that is monotonically decreasing (MonDec), that is monotonically increasing
(MonIncr) and that can not be categorized into any of the above (Oth). We
conduct a thorough experiment to investigate several important characteristics
of these categories such as how individual categories attract citations, how
the categorization is influenced by the year and the venue of publication of
papers, how each category is affected by self-citations, the stability of the
categories over time, and how much each of these categories contribute to the
core of the network. Further, we show that the traditional preferential
attachment models fail to explain these citation profiles. Therefore, we
propose a novel dynamic growth model that takes both the preferential
attachment and the aging factor into account in order to replicate the
real-world behavior of various citation profiles. We believe that this paper
opens the scope for a serious re-investigation of the existing bibliometric
indices for scientific research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06271</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06271</id><created>2015-03-21</created><authors><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Abdullah</keyname><forenames>Amirali</forenames></author></authors><title>Binary Coding in Stream</title><categories>cs.DS</categories><comments>5 figures, 9 pages</comments><msc-class>68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data is becoming ever more ubiquitous, ranging over massive video
repositories, document corpuses, image sets and Internet routing history.
Proximity search and clustering are two algorithmic primitives fundamental to
data analysis, but suffer from the &quot;curse of dimensionality&quot; on these gigantic
datasets. A popular attack for this problem is to convert object
representations into short binary codewords, while approximately preserving
near neighbor structure. However, there has been limited research on
constructing codewords in the &quot;streaming&quot; or &quot;online&quot; settings often applicable
to this scale of data, where one may only make a single pass over data too
massive to fit in local memory.
  In this paper, we apply recent advances in matrix sketching techniques to
construct binary codewords in both streaming and online setting. Our
experimental results compete outperform several of the most popularly used
algorithms, and we prove theoretical guarantees on performance in the streaming
setting under mild assumptions on the data and randomness of the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06273</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06273</id><created>2015-03-21</created><authors><author><keyname>Kasthurirathna</keyname><forenames>Dharshana</forenames></author><author><keyname>Piraveenan</keyname><forenames>Mahendra</forenames></author><author><keyname>Uddin</keyname><forenames>Shahadat</forenames></author></authors><title>Evolutionary stable strategies in networked games: the influence of
  topology</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary game theory is used to model the evolution of competing
strategies in a population of players. Evolutionary stability of a strategy is
a dynamic equilibrium, in which any competing mutated strategy would be wiped
out from a population. If a strategy is weak evolutionarily stable, the
competing strategy may manage to survive within the network. Understanding the
network-related factors that affect the evolutionary stability of a strategy
would be critical in making accurate predictions about the behaviour of a
strategy in a real-world strategic decision making environment. In this work,
we evaluate the effect of network topology on the evolutionary stability of a
strategy. We focus on two well-known strategies known as the Zero-determinant
strategy and the Pavlov strategy. Zero-determinant strategies have been shown
to be evolutionarily unstable in a well-mixed population of players. We
identify that the Zero-determinant strategy may survive, and may even dominate
in a population of players connected through a non-homogeneous network. We
introduce the concept of `topological stability' to denote this phenomenon. We
argue that not only the network topology, but also the evolutionary process
applied and the initial distribution of strategies are critical in determining
the evolutionary stability of strategies. Further, we observe that topological
stability could affect other well-known strategies as well, such as the general
cooperator strategy and the cooperator strategy. Our observations suggest that
the variation of evolutionary stability due to topological stability of
strategies may be more prevalent in the social context of strategic evolution,
in comparison to the biological context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06275</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06275</id><created>2015-03-21</created><authors><author><keyname>Siddiqui</keyname><forenames>Kazi Tanvir Ahmed</forenames></author><author><keyname>Wasif</keyname><forenames>Abu</forenames></author></authors><title>Skin Detection of Animation Characters</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing popularity of animes makes it vulnerable to unwanted usages
like copyright violations and pornography. That is why, we need to develop a
method to detect and recognize animation characters. Skin detection is one of
the most important steps in this way. Though there are some methods to detect
human skin color, but those methods do not work properly for anime characters.
Anime skin varies greatly from human skin in color, texture, tone and in
different kinds of lighting. They also vary greatly among themselves. Moreover,
many other things (for example leather, shirt, hair etc.), which are not skin,
can have color similar to skin. In this paper, we have proposed three methods
that can identify an anime character skin more successfully as compared with
Kovac, Swift, Saleh and Osman methods, which are primarily designed for human
skin detection. Our methods are based on RGB values and their comparative
relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06277</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06277</id><created>2015-03-21</created><authors><author><keyname>Saha</keyname><forenames>Biswajit</forenames></author><author><keyname>Mandal</keyname><forenames>Amitabha</forenames></author><author><keyname>Tripathy</keyname><forenames>Soumendu Bikas</forenames></author><author><keyname>Mukherjee</keyname><forenames>Debaprasad</forenames></author></authors><title>Complex Networks, Communities and Clustering: A survey</title><categories>cs.SI</categories><comments>15 pages, Review article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an extensive survey of literature on complex network
communities and clustering. Complex networks describe a widespread variety of
systems in nature and society especially systems composed by a large number of
highly interconnected dynamical entities. Complex networks like real networks
can also have community structure. There are several types of methods and
algorithms for detection and identification of communities in complex networks.
Several complex networks have the property of clustering or network
transitivity. Some of the important concepts in the field of complex networks
are small-world and scale-robustness, degree distributions, clustering, network
correlations, random graph models, models of network growth, dynamical
processes on networks, etc. Some current areas of research on complex network
communities are those on community evolution, overlapping communities,
communities in directed networks, community characterization and
interpretation, etc. Many of the algorithms or methods proposed for network
community detection through clustering are modified versions of or inspired
from the concepts of minimum-cut based algorithms, hierarchical connectivity
based algorithms, the original GirvanNewman algorithm, concepts of modularity
maximization, algorithms utilizing metrics from information and coding theory,
and clique based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06286</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06286</id><created>2015-03-21</created><authors><author><keyname>Cioab&#x103;</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Koolen</keyname><forenames>Jack H.</forenames></author><author><keyname>Nozaki</keyname><forenames>Hiroshi</forenames></author><author><keyname>Vermette</keyname><forenames>Jason R.</forenames></author></authors><title>Large regular graphs with given valency and second eigenvalue</title><categories>math.CO cs.DM</categories><comments>21 pages, 3 tables, 5 figures</comments><msc-class>05C50, 05E30, 15A18, 68R10, 90C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From Alon and Boppana, and Serre, we know that for any given integer $k\geq
3$ and real number $\lambda&lt;2\sqrt{k-1}$, there are finitely many $k$-regular
graphs whose second largest eigenvalue is at most $\lambda$. In this paper, we
investigate the largest number of vertices of such graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06289</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06289</id><created>2015-03-21</created><authors><author><keyname>Magalingam</keyname><forenames>Pritheega</forenames></author><author><keyname>Davis</keyname><forenames>Stephen</forenames></author><author><keyname>Rao</keyname><forenames>Asha</forenames></author></authors><title>Using shortest path to discover criminal community</title><categories>cs.SI physics.soc-ph</categories><journal-ref>DIIN584 2015</journal-ref><doi>10.1016/j.diin.2015.08.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting communities using existing community detection algorithms yields
dense sub-networks that are difficult to analyse. Extracting a smaller sample
that embodies the relationships of a list of suspects is an important part of
the beginning of an investigation. In this paper, we present the efficacy of
our shortest paths network search algorithm (SPNSA) that begins with an
&quot;algorithm feed&quot;, a small subset of nodes of particular interest, and builds an
investigative sub-network. The algorithm feed may consist of known criminals or
suspects, or persons of influence. This sets our approach apart from existing
community detection algorithms. We apply the SPNSA on the Enron Dataset of
e-mail communications starting with those convicted of money laundering in
relation to the collapse of Enron as the algorithm feed. The algorithm produces
sparse and small sub-networks that could feasibly identify a list of persons
and relationships to be further investigated. In contrast, we show that
identifying sub-networks of interest using either community detection
algorithms or a k-Neighbourhood approach produces sub-networks of much larger
size and complexity. When the 18 top managers of Enron were used as the
algorithm feed, the resulting sub-network identified 4 convicted criminals that
were not managers and so not part of the algorithm feed. We also directly
tested the SPNSA by removing one of the convicted criminals from the algorithm
feed and re-running the algorithm; in 5 out of 9 cases the left out criminal
occurred in the resulting sub-network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06292</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06292</id><created>2015-03-21</created><updated>2015-03-24</updated><authors><author><keyname>Tucci</keyname><forenames>Michele</forenames></author><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Vasquez</keyname><forenames>Juan C.</forenames></author><author><keyname>Guerrero</keyname><forenames>Josep M.</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author></authors><title>A decentralized scalable approach to voltage control of DC islanded
  microgrids</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1405.2421</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new decentralized control scheme for DC Islanded microGrids
(ImGs) composed by several Distributed Generation Units (DGUs) with a general
interconnection topology. Each local controller regulates to a reference value
the voltage of the Point of Common Coupling (PCC) of the corresponding DGU.
Notably, off-line control design is conducted in a Plug-and-Play (PnP) fashion
meaning that (i) the possibility of adding/removing a DGU without spoiling
stability of the overall ImG is checked through an optimization problem; (ii)
when a DGU is plugged in or out at most neighbouring DGUs have to update their
controllers and (iii) the synthesis of a local controller uses only information
on the corresponding DGU and lines connected to it. This guarantee total
scalability of control synthesis as the ImG size grows or DGU gets replaced.
Yes, under mild approximations of line dynamics, we formally guarantee
stability of the overall closed-loop ImG. The performance of the proposed
controllers is analyzed simulating different scenarios in PSCAD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06300</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06300</id><created>2015-03-21</created><authors><author><keyname>Conway</keyname><forenames>Rylan T.</forenames></author><author><keyname>Sangaline</keyname><forenames>Evan W.</forenames></author></authors><title>Optimizing Touchscreen Keyboard Layouts to Minimize Swipe Input Errors</title><categories>cs.HC</categories><comments>8 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swiping is a technique used for text entry on touchscreen devices which
involves sliding a finger between each successive letter in a word. The word is
then probabilistically reconstructed from this ambiguous input. This method of
text input is ergonomically suited for the form factor of touchscreen devices
and results in a much faster input method than tapping each letter
individually. Unfortunately, there are many cases where different words have
extremely similar swipe patterns which results in a high frequency of errors.
In this paper, we describe a method to determine a keyboard layout that
minimizes swipe errors in order to further increase the speed of text input on
a touchscreen device. Finally, we present the results of running this analysis
on an English language corpus and standard keyboard geometry leading to a
reduction in the frequency of swipe errors by over 50% relative to the standard
QWERTY keyboard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06301</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06301</id><created>2015-03-21</created><authors><author><keyname>Gupta</keyname><forenames>Yash</forenames></author><author><keyname>Karlapalem</keyname><forenames>Kamalakar</forenames></author></authors><title>Effective Handling of Urgent Jobs - Speed Up Scheduling for Computing
  Applications</title><categories>cs.PF</categories><comments>Paper covering main contributions from MS Thesis of Yash Gupta
  http://web2py.iiit.ac.in/research_centres/publications/view_publication/mastersthesis/247
  - presented in ACM format</comments><report-no>MS Thesis Number IIIT/TH/2014/7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A queue is required when a service provider is not able to handle jobs
arriving over the time. In a highly flexible and dynamic environment, some jobs
might demand for faster execution at run-time especially when the resources are
limited and the jobs are competing for acquiring resources. A user might demand
for speed up (reduced wait time) for some of the jobs present in the queue at
run time. In such cases, it is required to accelerate (directly sending the job
to the server) urgent jobs (requesting for speed up) ahead of other jobs
present in the queue for an earlier completion of urgent jobs. Under the
assumption of no additional resources, such acceleration of jobs would result
in slowing down of other jobs present in the queue. In this paper, we formulate
the problem of Speed Up Scheduling without acquiring any additional resources
for the scheduling of on-line speed up requests posed by a user at run-time and
present algorithms for the same. We apply the idea of Speed Up Scheduling to
two different domains -Web Scheduling and CPU Scheduling. We demonstrate our
results with a simulation based model using trace driven workload and synthetic
datasets to show the usefulness of Speed Up scheduling. Speed Up provides a new
way of addressing urgent jobs, provides a different evaluation criteria for
comparing scheduling algorithms and has practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06316</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06316</id><created>2015-03-21</created><authors><author><keyname>Tirunagari</keyname><forenames>Santosh</forenames></author><author><keyname>Poh</keyname><forenames>Norman</forenames></author><author><keyname>Hu</keyname><forenames>Guosheng</forenames></author><author><keyname>Windridge</keyname><forenames>David</forenames></author></authors><title>Identifying Similar Patients Using Self-Organising Maps: A Case Study on
  Type-1 Diabetes Self-care Survey Responses</title><categories>cs.CE cs.AI</categories><comments>01-05 pages</comments><report-no>TR-DoC-02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diabetes is considered a lifestyle disease and a well managed self-care plays
an important role in the treatment. Clinicians often conduct surveys to
understand the self-care behaviors in their patients. In this context, we
propose to use Self-Organising Maps (SOM) to explore the survey data for
assessing the self-care behaviors in Type-1 diabetic patients. Specifically,
SOM is used to visualize high dimensional similar patient profiles, which is
rarely discussed. Experiments demonstrate that our findings through SOM
analysis corresponds well to the expectations of the clinicians. In addition,
our findings inspire the experts to improve their understanding of the
self-care behaviors for their patients. The principle findings in our study
show: 1) patients who take correct dose of insulin, inject insulin at the right
time, 2) patients who take correct food portions undertake regular physical
activity and 3) patients who eat on time take correct food portions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06321</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06321</id><created>2015-03-21</created><updated>2015-06-28</updated><authors><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author><author><keyname>Kaspi</keyname><forenames>Moshe</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author><author><keyname>Navon</keyname><forenames>Barak</forenames></author></authors><title>Parameterized Complexity of Critical Node Cuts</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following natural graph cut problem called Critical Node Cut
(CNC): Given a graph $G$ on $n$ vertices, and two positive integers $k$ and
$x$, determine whether $G$ has a set of $k$ vertices whose removal leaves $G$
with at most $x$ connected pairs of vertices. We analyze this problem in the
framework of parameterized complexity. That is, we are interested in whether or
not this problem is solvable in $f(\kappa) \cdot n^{O(1)}$ time (i.e., whether
or not it is fixed-parameter tractable), for various natural parameters
$\kappa$. We consider four such parameters:
  - The size $k$ of the required cut.
  - The upper bound $x$ on the number of remaining connected pairs.
  - The lower bound $y$ on the number of connected pairs to be removed.
  - The treewidth $w$ of $G$.
  We determine whether or not CNC is fixed-parameter tractable for each of
these parameters. We determine this also for all possible aggregations of these
four parameters, apart from $w+k$. Moreover, we also determine whether or not
CNC admits a polynomial kernel for all these parameterizations. That is,
whether or not there is an algorithm that reduces each instance of CNC in
polynomial time to an equivalent instance of size $\kappa^{O(1)}$, where
$\kappa$ is the given parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06323</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06323</id><created>2015-03-21</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sabyasachi</forenames></author><author><keyname>Das</keyname><forenames>Nandan K.</forenames></author><author><keyname>Mandal</keyname><forenames>Soham</forenames></author><author><keyname>Pratiher</keyname><forenames>Sawon</forenames></author><author><keyname>Mitra</keyname><forenames>Asish</forenames></author><author><keyname>Pradhan</keyname><forenames>Asima</forenames></author><author><keyname>Ghosh</keyname><forenames>Nirmalya</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>Wavelet based approach for tissue fractal parameter measurement: Pre
  cancer detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have carried out the detail studies of pre-cancer by
wavelet coherency and multifractal based detrended fluctuation analysis (MFDFA)
on differential interference contrast (DIC) images of stromal region among
different grades of pre-cancer tissues. Discrete wavelet transform (DWT)
through Daubechies basis has been performed for identifying fluctuations over
polynomial trends for clear characterization and differentiation of tissues.
Wavelet coherence plots are performed for identifying the level of correlation
in time scale plane between normal and various grades of DIC samples. Applying
MFDFA on refractive index variations of cervical tissues, we have observed that
the values of Hurst exponent (correlation) decreases from healthy (normal) to
pre-cancer tissues. The width of singularity spectrum has a sudden degradation
at grade-I in comparison of healthy (normal) tissue but later on it increases
as cancer progresses from grade-II to grade-III.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06326</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06326</id><created>2015-03-21</created><updated>2015-07-21</updated><authors><author><keyname>Pereira</keyname><forenames>Pedro O.</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Family of Controllers for Attitude Synchronization on the Sphere</title><categories>cs.SY math.OC</categories><comments>A preliminary version of this work was submitted to the 2015 IEEE
  Conference on Decision and Control, &quot;Family of Controllers for Attitude
  Synchronization in S2&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a family of controllers that guarantees attitude
synchronization for a network of elements in the unit sphere domain, i.e.
$\mathcal{S}^2$. We propose distributed continuous controllers for elements
whose dynamics are controllable (i.e. control with torque as command), and
which can be implemented by each individual agent without the need of a common
global orientation frame among the network, i.e. it requires only local
information that can be measured by each individual agent from its own
orientation frame. The controllers are specified according to arbitrary
distance functions in $\mathcal{S}^2$, and we provide conditions on those
distance functions that guarantee that i) a synchronized network of agents is
locally asymptotically stable for an arbitrary network topology; ii) a
synchronized network can be achieved for almost all initial conditions in a
tree graph network. We also study the equilibria configurations that come with
specific types of network graphs. The proposed strategies can be used in
attitude synchronization of swarms of fully actuated rigid bodies, such as
satellites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06331</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06331</id><created>2015-03-21</created><authors><author><keyname>Tirunagari</keyname><forenames>Santosh</forenames></author></authors><title>Exploratory Data Analysis of The KelvinHelmholtz instability in Jets</title><categories>cs.CE physics.flu-dyn</categories><report-no>DNS-Report-2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The KelvinHelmholtz (KH) instability is a fundamental wave instability that
is frequently observed in all kinds of shear layer (jets, wakes, atmospheric
air currents etc). The study of KH-instability, coherent flow structures has a
major impact in understanding the fundamentals of fluid dynamics. Therefore
there is a need for methods that can identify and analyse these structures. In
this Final assignment, we use machine-learning methods such as Proper
Orthogonal Decomposition (POD) and Dynamic Mode Decomposition (DMD) to analyse
the coherent flow structures. We used a 2D co-axial jet as our data, with
Reynolds number corresponding to Re: 10,000. Results for POD modes and DMD
modes are discussed and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06333</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06333</id><created>2015-03-21</created><updated>2015-10-27</updated><authors><author><keyname>Awan</keyname><forenames>Zohaib Hassan</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>On SDoF of Multi-Receiver Wiretap Channel With Alternating CSIT</title><categories>cs.IT math.IT</categories><comments>Under Submission--Dec. 2014. This is an extended version of the
  conference paper appeared in ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of secure transmission over a Gaussian multi-input
single-output (MISO) two receiver channel with an external eavesdropper, under
the assumption that the state of the channel which is available to each
receiver is conveyed either perfectly ($P$) or with delay ($D$) to the
transmitter. Denoting by $S_1$, $S_2$, and $S_3$ the channel state information
at the transmitter (CSIT) of user 1, user 2, and eavesdropper, respectively,
the overall CSIT can then alternate between eight possible states, i.e.,
$(S_1,S_2,S_3) \in \{P,D\}^3$. We denote by $\lambda_{S_1 S_2 S_3}$ the
fraction of time during which the state $S_1S_2S_3$ occurs. Under these
assumptions, we first consider the Gaussian MISO wiretap channel and
characterize the secure degrees of freedom (SDoF). Next, we consider the
general multi-receiver setup and characterize the SDoF region of fixed hybrid
states $PPD$, $PDP$, and $DDP$. We then focus our attention on the symmetric
case in which $\lambda_{PDD}=\lambda_{DPD}$. For this case, we establish bounds
on SDoF region. The analysis reveals that alternating CSIT allows synergistic
gains in terms of SDoF; and shows that, by opposition to encoding separately
over different states, joint encoding across the states enables strictly better
secure rates. Furthermore, we specialize our results for the two receivers
channel with an external eavesdropper to the two-user broadcast channel. We
show that, the synergistic gains in terms of SDoF by alternating CSIT is not
restricted to multi-receiver wiretap channels; and, can also be harnessed under
broadcast setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06342</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06342</id><created>2015-03-21</created><authors><author><keyname>Overbury</keyname><forenames>Peter</forenames></author><author><keyname>Berthouze</keyname><forenames>Luc</forenames></author></authors><title>Using novelty-biased GA to sample diversity in graphs satisfying
  constraints</title><categories>physics.soc-ph cs.NE cs.SI math.CO</categories><comments>Extended version of a short paper accepted for publication in
  Proceedings of Genetic and Evolutionary Computation Conference (GECCO'15)</comments><msc-class>05C85, 68R10, 90B15, 90C35</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structure of the network underlying many complex systems, whether
artificial or natural, plays a significant role in how these systems operate.
As a result, much emphasis has been placed on accurately describing networks
using network theoretic metrics. When it comes to generating networks with
similar properties, however, the set of available techniques and properties
that can be controlled for remains limited. Further, whilst it is becoming
clear that some of the metrics currently used to control the generation of such
networks are not very prescriptive so that networks could potentially exhibit
very different higher-order structure within those constraints, network
generating algorithms typically produce fairly contrived networks and lack
mechanisms by which to systematically explore the space of network solutions.
In this paper, we explore the potential of a multi-objective novelty-biased GA
to provide a viable alternative to these algorithms. We believe our results
provide the first proof of principle that (i) it is possible to use GAs to
generate graphs satisfying set levels of key classical graph theoretic
properties and (ii) it is possible to generate diverse solutions within these
constraints. The paper is only a preliminary step, however, and we identify key
avenues for further development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06350</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06350</id><created>2015-03-21</created><authors><author><keyname>Karianakis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Fuchs</keyname><forenames>Thomas J.</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author></authors><title>Boosting Convolutional Features for Robust Object Proposals</title><categories>cs.CV cs.AI cs.LG</categories><comments>9 pages, 4 figures, 2 tables, 42 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Networks (CNNs) have demonstrated excellent
performance in image classification, but still show room for improvement in
object-detection tasks with many categories, in particular for cluttered scenes
and occlusion. Modern detection algorithms like Regions with CNNs (Girshick et
al., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions
which with high probability represent objects, where in turn CNNs are deployed
for classification. Selective Search represents a family of sophisticated
algorithms that are engineered with multiple segmentation, appearance and
saliency cues, typically coming with a significant run-time overhead.
Furthermore, (Hosang et al., 2014) have shown that most methods suffer from low
reproducibility due to unstable superpixels, even for slight image
perturbations. Although CNNs are subsequently used for classification in
top-performing object-detection pipelines, current proposal methods are
agnostic to how these models parse objects and their rich learned
representations. As a result they may propose regions which may not resemble
high-level objects or totally miss some of them. To overcome these drawbacks we
propose a boosting approach which directly takes advantage of hierarchical CNN
features for detecting regions of interest fast. We demonstrate its performance
on ImageNet 2013 detection benchmark and compare it with state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06358</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06358</id><created>2015-03-21</created><authors><author><keyname>Ruan</keyname><forenames>Liangzhong</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>Generalized Interference Alignment --- Part I: Theoretical Framework</title><categories>cs.IT math.IT</categories><comments>Minor Revision at IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) has attracted enormous research interest as it
achieves optimal capacity scaling with respect to signal to noise ratio on
interference networks. IA has also recently emerged as an effective tool in
engineering interference for secrecy protection on wireless wiretap networks.
However, despite the numerous works dedicated to IA, two of its fundamental
issues, i.e., feasibility conditions and transceiver design, are not completely
addressed in the literature. In this two part paper, a generalised interference
alignment (GIA) technique is proposed to enhance the IA's capability in secrecy
protection. A theoretical framework is established to analyze the two
fundamental issues of GIA in Part I and then the performance of GIA in
large-scale stochastic networks is characterized to illustrate how GIA benefits
secrecy protection in Part II. The theoretical framework for GIA adopts
methodologies from algebraic geometry, determines the necessary and sufficient
feasibility conditions of GIA, and generates a set of algorithms that can solve
the GIA problem. This framework sets up a foundation for the development and
implementation of GIA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06361</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06361</id><created>2015-03-21</created><authors><author><keyname>Ruan</keyname><forenames>Liangzhong</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>Generalized Interference Alignment --- Part II: Application to Wireless
  Secrecy</title><categories>cs.IT math.IT</categories><comments>minor revision at IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to its wired counterpart, wireless communication is highly
susceptible to eavesdropping due to the broadcast nature of the wireless
propagation medium. Recent works have proposed the use of interference to
reduce eavesdropping capabilities in wireless wiretap networks. However, the
concurrent effect of interference on both eavesdropping receivers (ERs) and
legitimate receivers (LRs) has not been thoroughly investigated, and carefully
engineering the network interference is required to harness the full potential
of interference for wireless secrecy. This two part paper addresses this issue
by proposing a generalized interference alignment (GIA) technique, which
jointly designs the transceivers at the legitimate partners to impede the ERs
without interfering with LRs. In Part I, we have established a theoretical
framework for the GIA technique. In Part II, we will first propose an efficient
GIA algorithm that is applicable to large-scale networks and then evaluate the
performance of this algorithm in stochastic wireless wiretap network via both
analysis and simulation. These results reveal insights into when and how GIA
contributes to wireless secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06364</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06364</id><created>2015-03-21</created><authors><author><keyname>Jonathan</keyname><forenames>Laporte</forenames></author><author><keyname>Antoine</keyname><forenames>Chaillet</forenames></author><author><keyname>Yacine</keyname><forenames>Chitour</forenames></author></authors><title>Global stabilization of multiple integrators by a bounded feedback with
  constraints on its successive derivatives</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the global stabilization of chains of integrators
by means of a bounded static feedback law whose p first time derivatives are
bounded. Our construction is based on the technique of nested saturations
introduced by Teel. We show that the control amplitude and the maximum value of
its p first derivatives can be imposed below any prescribed values. Our results
are illustrated by the stabilization of the third order integrator on the
feedback and its first two derivatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06365</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06365</id><created>2015-03-21</created><authors><author><keyname>Bell</keyname><forenames>Paul</forenames></author><author><keyname>Reidenbach</keyname><forenames>Daniel</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Factorization in Formal Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider several novel aspects of unique factorization in formal
languages. We reprove the familiar fact that the set uf(L) of words having
unique factorization into elements of L is regular if L is regular, and from
this deduce an quadratic upper and lower bound on the length of the shortest
word not in uf(L). We observe that uf(L) need not be context-free if L is
context-free.
  Next, we consider variations on unique factorization. We define a notion of
&quot;semi-unique&quot; factorization, where every factorization has the same number of
terms, and show that, if L is regular or even finite, the set of words having
such a factorization need not be context-free. Finally, we consider additional
variations, such as unique factorization &quot;up to permutation&quot; and &quot;up to
subset&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06375</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06375</id><created>2015-03-21</created><updated>2015-05-08</updated><authors><author><keyname>Sankaran</keyname><forenames>Bharath</forenames></author><author><keyname>Bohg</keyname><forenames>Jeannette</forenames></author><author><keyname>Ratliff</keyname><forenames>Nathan</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>Policy Learning with Hypothesis based Local Action Selection</title><categories>cs.RO</categories><comments>RLDM abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For robots to be able to manipulate in unknown and unstructured environments
the robot should be capable of operating under partial observability of the
environment. Object occlusions and unmodeled environments are some of the
factors that result in partial observability. A common scenario where this is
encountered is manipulation in clutter. In the case that the robot needs to
locate an object of interest and manipulate it, it needs to perform a series of
decluttering actions to accurately detect the object of interest. To perform
such a series of actions, the robot also needs to account for the dynamics of
objects in the environment and how they react to contact. This is a non trivial
problem since one needs to reason not only about robot-object interactions but
also object-object interactions in the presence of contact. In the example
scenario of manipulation in clutter, the state vector would have to account for
the pose of the object of interest and the structure of the surrounding
environment. The process model would have to account for all the aforementioned
robot-object, object-object interactions. The complexity of the process model
grows exponentially as the number of objects in the scene increases. This is
commonly the case in unstructured environments. Hence it is not reasonable to
attempt to model all object-object and robot-object interactions explicitly.
Under this setting we propose a hypothesis based action selection algorithm
where we construct a hypothesis set of the possible poses of an object of
interest given the current evidence in the scene and select actions based on
our current set of hypothesis. This hypothesis set tends to represent the
belief about the structure of the environment and the number of poses the
object of interest can take. The agent's only stopping criterion is when the
uncertainty regarding the pose of the object is fully resolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06377</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06377</id><created>2015-03-21</created><updated>2015-03-25</updated><authors><author><keyname>Bari</keyname><forenames>Md. Faizul</forenames></author><author><keyname>Chowdhury</keyname><forenames>Shihabur Rahman</forenames></author><author><keyname>Ahmed</keyname><forenames>Reaz</forenames></author><author><keyname>Boutaba</keyname><forenames>Raouf</forenames></author></authors><title>On Orchestrating Virtual Network Functions in NFV</title><categories>cs.NI</categories><comments>NFV, VNF. Orchestration, Service Chaining, Network Function</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Middleboxes or network appliances like firewalls, proxies and WAN optimizers
have become an integral part of today's ISP and enterprise networks. Middlebox
functionalities are usually deployed on expensive and proprietary hardware that
require trained personnel for deployment and maintenance. Middleboxes
contribute significantly to a network's capital and operational costs. In
addition, organizations often require their traffic to pass through a specific
sequence of middleboxes for compliance with security and performance policies.
This makes the middlebox deployment and maintenance tasks even more
complicated. Network Function Virtualization (NFV) is an emerging and promising
technology that is envisioned to overcome these challenges. It proposes to move
packet processing from dedicated hardware middleboxes to software running on
commodity servers. In NFV terminology, software middleboxes are referred to as
Virtualized Network Functions (VNFs). It is a challenging problem to determine
the required number and placement of VNFs that optimizes network operational
costs and utilization, without violating service level agreements. We call this
the VNF Orchestration Problem (VNF-OP) and provide an Integer Linear
Programming (ILP) formulation with implementation in CPLEX. We also provide a
dynamic programming based heuristic to solve larger instances of VNF-OP. Trace
driven simulations on real-world network topologies demonstrate that the
heuristic can provide solutions that are within 1.3 times of the optimal
solution. Our experiments suggest that a VNF based approach can provide more
than 4x reduction in the operational cost of a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06379</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06379</id><created>2015-03-21</created><updated>2015-03-31</updated><authors><author><keyname>Kundu</keyname><forenames>Abhisek</forenames></author></authors><title>Relaxed Leverage Sampling for Low-rank Matrix Completion</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exact recovery of any $m\times n$ matrix of rank
$\varrho$ from a small number of observed entries via the standard nuclear norm
minimization framework. Such low-rank matrices have degrees of freedom
$(m+n)\varrho - \varrho^2$. We show that such arbitrary low-rank matrices can
be recovered exactly from a small subset of $\Theta\left(((m+n)\varrho -
\varrho^2)\log^2(m+n)\right)$ randomly sampled entries, thus matching the lower
bound on the required number of entries (in terms of degrees of freedom), with
an additional factor of $O(\log^2(m+n))$. The above bound on sample size is
achieved if each entry is observed according to probabilities proportional to
the sum of corresponding row and column leverage scores, minus their product.
We show that this relaxation in sampling probabilities (as opposed to sum of
leverage scores in Chen et al, 2014) gives us an additive improvement on the
(best known) sample size obtained by Chen et al, 2014, for the nuclear norm
minimization. Further, exact recovery of $(a)$ incoherent matrices (with
restricted leverage scores), and $(b)$ matrices with only one of the row or
column spaces to be incoherent, can be performed using our relaxed leverage
score sampling, via nuclear norm minimization, without knowing the leverage
scores a priori. In such settings also we achieve additive improvement on
sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06381</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06381</id><created>2015-03-21</created><authors><author><keyname>Lewko</keyname><forenames>Allison</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author></authors><title>Balancing Communication for Multi-party Interactive Coding</title><categories>cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider interactive coding in a setting where $n$ parties wish to compute
a joint function of their inputs via an interactive protocol over imperfect
channels. We assume that adversarial errors can comprise a
$\mathcal{O}(\frac{1}{n})$ fraction of the total communication, occurring
anywhere on the communication network. Our goal is to maintain a constant
multiplicative overhead in the total communication required, as compared to the
error-free setting, and also to balance the workload over the different
parties. We build upon the prior protocol of Jain, Kalai, and Lewko, but while
that protocol relies on a single coordinator to shoulder a heavy burden
throughout the protocol, we design a mechanism to pass the coordination duties
from party to party, resulting in a more even distribution of communication
over the course of the computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06382</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06382</id><created>2015-03-22</created><authors><author><keyname>Sukumaran</keyname><forenames>Vineeth Bala</forenames></author></authors><title>On the tradeoff of average delay, average service cost, and average
  utility for single server queues with monotone policies</title><categories>cs.PF</categories><comments>Ph.D. thesis, Department of Electrical Communication Engineering,
  Indian Institute of Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we study the optimal tradeoff of average delay, average
service cost, and average utility for single server queueing models, with and
without admission control. The continuous time and discrete time queueing
models that we consider are motivated by cross-layer models for noisy
point-to-point links, with random packet arrivals. We study the above tradeoff
problem for a class of admissible policies, which are monotone and stationary
and obtain an asymptotic characterization of the minimum average delay as a
function of the average service cost and average utility constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06383</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06383</id><created>2015-03-22</created><authors><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author></authors><title>Real-time Dynamic MRI Reconstruction using Stacked Denoising Autoencoder</title><categories>cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of real-time dynamic MRI reconstruction.
There are a handful of studies on this topic; these techniques are either based
on compressed sensing or employ Kalman Filtering. These techniques cannot
achieve the reconstruction speed necessary for real-time reconstruction. In
this work, we propose a new approach to MRI reconstruction. We learn a
non-linear mapping from the unstructured aliased images to the corresponding
clean images using a stacked denoising autoencoder (SDAE). The training for
SDAE is slow, but the reconstruction is very fast - only requiring a few matrix
vector multiplications. In this work, we have shown that using SDAE one can
reconstruct the MRI frame faster than the data acquisition rate, thereby
achieving real-time reconstruction. The quality of reconstruction is of the
same order as a previous compressed sensing based online reconstruction
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06384</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06384</id><created>2015-03-22</created><authors><author><keyname>Boehm</keyname><forenames>Matthias</forenames></author></authors><title>Costing Generated Runtime Execution Plans for Large-Scale Machine
  Learning Programs</title><categories>cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Declarative large-scale machine learning (ML) aims at the specification of ML
algorithms in a high-level language and automatic generation of hybrid runtime
execution plans ranging from single node, in-memory computations to distributed
computations on MapReduce (MR) or similar frameworks like Spark. The
compilation of large-scale ML programs exhibits many opportunities for
automatic optimization. Advanced cost-based optimization techniques
require---as a fundamental precondition---an accurate cost model for evaluating
the impact of optimization decisions. In this paper, we share insights into a
simple and robust yet accurate technique for costing alternative runtime
execution plans of ML programs. Our cost model relies on generating and costing
runtime plans in order to automatically reflect all successive optimization
phases. Costing runtime plans also captures control flow structures such as
loops and branches, and a variety of cost factors like IO, latency, and
computation costs. Finally, we linearize all these cost factors into a single
measure of expected execution time. Within SystemML, this cost model is
leveraged by several advanced optimizers like resource optimization and global
data flow optimization. We share our lessons learned in order to provide
foundations for the optimization of ML programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06391</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06391</id><created>2015-03-22</created><authors><author><keyname>Sakka</keyname><forenames>Sophie</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Ma</keyname><forenames>Ruina</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Bennis</keyname><forenames>Fouad</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Predictive model of the human muscle fatigue: application to repetitive
  push-pull tasks with light external load</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>International Journal of Human Factors Modelling and Simulation,
  Inderscience, 2015, 5 (1), pp.81-97</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repetitive tasks in industrial works may contribute to health problems among
operators, such as musculo-skeletal disorders, in part due to insufficient
control of muscle fatigue. In this paper, a predictive model of fatigue is
proposed for repetitive push/pull operations. Assumptions generally accepted in
the literature are first explicitly set in this framework. Then, an earlier
static fatigue model is recalled and extended to quasi-static situations.
Specifically, the maximal torque that can be generated at a joint is not
considered as constant, but instead varies over time accordingly to the
operator's changing posture. The fatigue model is implemented with this new
consideration and evaluated in a simulation of push/pull operation. Reference
to this paper should be made as follows: Sakka, S., Chablat, D., Ma, R. and
Bennis, F. (2015) 'Predictive model of the human muscle fatigue: application to
repetitive push-pull tasks with light external load', Int.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06394</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06394</id><created>2015-03-22</created><authors><author><keyname>Han</keyname><forenames>Insu</forenames></author><author><keyname>Malioutov</keyname><forenames>Dmitry</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author></authors><title>Large-scale Log-determinant Computation through Stochastic Chebyshev
  Expansions</title><categories>cs.DS cs.LG cs.NA</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Logarithms of determinants of large positive definite matrices appear
ubiquitously in machine learning applications including Gaussian graphical and
Gaussian process models, partition functions of discrete graphical models,
minimum-volume ellipsoids, metric learning and kernel learning. Log-determinant
computation involves the Cholesky decomposition at the cost cubic in the number
of variables, i.e., the matrix dimension, which makes it prohibitive for
large-scale applications. We propose a linear-time randomized algorithm to
approximate log-determinants for very large-scale positive definite and general
non-singular matrices using a stochastic trace approximation, called the
Hutchinson method, coupled with Chebyshev polynomial expansions that both rely
on efficient matrix-vector multiplications. We establish rigorous additive and
multiplicative approximation error bounds depending on the condition number of
the input matrix. In our experiments, the proposed algorithm can provide very
high accuracy solutions at orders of magnitude faster time than the Cholesky
decomposition and Schur completion, and enables us to compute log-determinants
of matrices involving tens of millions of variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06408</identifier>
 <datestamp>2015-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06408</id><created>2015-03-22</created><updated>2015-06-08</updated><authors><author><keyname>Taniguchi</keyname><forenames>Tadahiro</forenames></author><author><keyname>Kawasaki</keyname><forenames>Koki</forenames></author><author><keyname>Fukui</keyname><forenames>Yoshiro</forenames></author><author><keyname>Takata</keyname><forenames>Tomohiro</forenames></author><author><keyname>Yano</keyname><forenames>Shiro</forenames></author></authors><title>Automated Linear Function Submission-based Double Auction as Bottom-up
  Real-Time Pricing in a Regional Prosumers' Electricity Network</title><categories>cs.SY cs.MA</categories><journal-ref>Energies 2015, 8(7), 7381-7406</journal-ref><doi>10.3390/en8077381</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A linear function submission-based double-auction (LFS-DA) mechanism for a
regional electricity network is proposed in this paper. Each agent in the
network is equipped with a battery and a generator. Each agent simultaneously
becomes a producer and consumer of electricity, i.e., a prosumer and trades
electricity in the regional market at a variable price. In the LFS-DA, each
agent uses linear demand and supply functions when they submit bids and asks to
an auctioneer in the regional market.The LFS-DA can achieve an exact balance
between electricity demand and supply for each time slot throughout the
learning phase and was shown capable of solving the primal problem of
maximizing the social welfare of the network without any central price setter,
e.g., a utility or a large electricity company, in contrast with conventional
real-time pricing (RTP). This paper presents a clarification of the
relationship between the RTP algorithm derived on the basis of a dual
decomposition framework and LFS-DA. Specifically, we proved that the changes in
the price profile of the LFS-DA mechanism are equal to those achieved by the
RTP mechanism derived from the dual decomposition framework except for a
constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06410</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06410</id><created>2015-03-22</created><authors><author><keyname>Powers</keyname><forenames>David M. W.</forenames></author></authors><title>What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes</title><categories>cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML</categories><comments>19 pages</comments><report-no>KIT-14-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The F-measure or F-score is one of the most commonly used single number
measures in Information Retrieval, Natural Language Processing and Machine
Learning, but it is based on a mistake, and the flawed assumptions render it
unsuitable for use in most contexts! Fortunately, there are better
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06419</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06419</id><created>2015-03-22</created><updated>2015-08-30</updated><authors><author><keyname>Fontanari</keyname><forenames>Jos&#xe9; F.</forenames></author></authors><title>Exploring NK Fitness Landscapes Using Imitative Learning</title><categories>cs.MA physics.soc-ph</categories><journal-ref>Eur. Phys. J. B 88 (2015) 251</journal-ref><doi>10.1140/epjb/e2015-60608-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea that a group of cooperating agents can solve problems more
efficiently than when those agents work independently is hardly controversial,
despite our obliviousness of the conditions that make cooperation a successful
problem solving strategy. Here we investigate the performance of a group of
agents in locating the global maxima of NK fitness landscapes with varying
degrees of ruggedness. Cooperation is taken into account through imitative
learning and the broadcasting of messages informing on the fitness of each
agent. We find a trade-off between the group size and the frequency of
imitation: for rugged landscapes, too much imitation or too large a group yield
a performance poorer than that of independent agents. By decreasing the
diversity of the group, imitative learning may lead to duplication of work and
hence to a decrease of its effective size. However, when the parameters are set
to optimal values the cooperative group substantially outperforms the
independent agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06424</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06424</id><created>2015-03-22</created><authors><author><keyname>Merelo-Guerv&#xf3;s</keyname><forenames>Juan Juli&#xe1;n</forenames></author><author><keyname>Garc&#xed;a-S&#xe1;nchez</keyname><forenames>Pablo</forenames></author></authors><title>Modeling browser-based distributed evolutionary computation systems</title><categories>cs.DC cs.NE cs.NI</categories><comments>Technical report</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  From the era of big science we are back to the &quot;do it yourself&quot;, where you do
not have any money to buy clusters or subscribe to grids but still have
algorithms that crave many computing nodes and need them to measure
scalability. Fortunately, this coincides with the era of big data, cloud
computing, and browsers that include JavaScript virtual machines. Those are the
reasons why this paper will focus on two different aspects of volunteer or
freeriding computing: first, the pragmatic: where to find those resources,
which ones can be used, what kind of support you have to give them; and then,
the theoretical: how evolutionary algorithms can be adapted to an environment
in which nodes come and go, have different computing capabilities and operate
in complete asynchrony of each other. We will examine the setup needed to
create a very simple distributed evolutionary algorithm using JavaScript and
then find a model of how users react to it by collecting data from several
experiments featuring different classical benchmark functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06429</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06429</id><created>2015-03-22</created><authors><author><keyname>Miranda</keyname><forenames>Conrado S.</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando J.</forenames></author></authors><title>Asymmetric Distributions from Constrained Mixtures</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces constrained mixtures for continuous distributions,
characterized by a mixture of distributions where each distribution has a shape
similar to the base distribution and disjoint domains. This new concept is used
to create generalized asymmetric versions of the Laplace and normal
distributions, which are shown to define exponential families, with known
conjugate priors, and to have maximum likelihood estimates for the original
parameters, with known closed-form expressions. The asymmetric and symmetric
normal distributions are compared in a linear regression example, showing that
the asymmetric version performs at least as well as the symmetric one, and in a
real world time-series problem, where a hidden Markov model is used to fit a
stock index, indicating that the asymmetric version provides higher likelihood
and may learn distribution models over states and transition distributions with
considerably less entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06437</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06437</id><created>2015-03-22</created><authors><author><keyname>Chu</keyname><forenames>Zheng</forenames></author><author><keyname>Johnston</keyname><forenames>Martin</forenames></author><author><keyname>Goff</keyname><forenames>Stephane Le</forenames></author></authors><title>Simultaneous Wireless Information Power Transfer for MISO Secrecy
  Channel</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates simultaneous wireless information and power transfer
(SWIPT) for multiuser multiple-input-single-output (MISO) secrecy channel.
First, transmit beamfoming without artificial noise (AN) design is considered,
where two secrecy rate optimization frameworks (i.e., secrecy rate maximization
and harvested energy maximization) are investigated. These two optimization
problems are not convex, and cannot be solved directly. For secrecy rate
maximization problem, we employ bisection method to optimize the associated
power minimization problem, and first-order Taylor series expansion is consider
to approximate the energy harvesting (EH) constraint and the harvested energy
maximization problem. Moreover, we extend our proposed algorithm to the
associated robust schemes by incorporating with channel uncertainties, where
two-level method is proposed for the harvested energy maximization problem.
Then, transmit beamforming with AN design is studied for the same secrecy rate
maximization problem, which are reformulated into semidefinite programming
(SDP) based on one-dimensional search and successive convex approximation
(SCA), respectively. Moreover, tightness analysis of rank relaxation is
provided to show the optimal transmit covariance matrix exactly returns
rank-one. Simulation results is provided to validate the performance of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06438</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06438</id><created>2015-03-22</created><authors><author><keyname>Zehavi</keyname><forenames>Meirav</forenames></author></authors><title>Maximization Problems Parameterized Using Their Minimization Versions:
  The Case of Vertex Cover</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parameterized complexity of problems is often studied with respect to the
size of their optimal solutions. However, for a maximization problem, the size
of the optimal solution can be very large, rendering algorithms parameterized
by it inefficient. Therefore, we suggest to study the parameterized complexity
of maximization problems with respect to the size of the optimal solutions to
their minimization versions. We examine this suggestion by considering the
Maximal Minimal Vertex Cover (MMVC) problem, whose minimization version, Vertex
Cover, is one of the most studied problems in the field of Parameterized
Complexity. Our main contribution is a parameterized approximation algorithm
for MMVC, including its weighted variant. We also give conditional lower bounds
for the running times of algorithms for MMVC and its weighted variant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06447</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06447</id><created>2015-03-22</created><authors><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Potechin</keyname><forenames>Aaron</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Sum-of-squares lower bounds for planted clique</title><categories>cs.CC cs.DS math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding cliques in random graphs and the closely related &quot;planted&quot; clique
variant, where a clique of size k is planted in a random G(n, 1/2) graph, have
been the focus of substantial study in algorithm design. Despite much effort,
the best known polynomial-time algorithms only solve the problem for k ~
sqrt(n).
  In this paper we study the complexity of the planted clique problem under
algorithms from the Sum-of-squares hierarchy. We prove the first average case
lower bound for this model: for almost all graphs in G(n,1/2), r rounds of the
SOS hierarchy cannot find a planted k-clique unless k &gt; n^{1/2r} (up to
logarithmic factors). Thus, for any constant number of rounds planted cliques
of size n^{o(1)} cannot be found by this powerful class of algorithms. This is
shown via an integrability gap for the natural formulation of maximum clique
problem on random graphs for SOS and Lasserre hierarchies, which in turn follow
from degree lower bounds for the Positivestellensatz proof system.
  We follow the usual recipe for such proofs. First, we introduce a natural
&quot;dual certificate&quot; (also known as a &quot;vector-solution&quot; or &quot;pseudo-expectation&quot;)
for the given system of polynomial equations representing the problem for every
fixed input graph. Then we show that the matrix associated with this dual
certificate is PSD (positive semi-definite) with high probability over the
choice of the input graph.This requires the use of certain tools. One is the
theory of association schemes, and in particular the eigenspaces and
eigenvalues of the Johnson scheme. Another is a combinatorial method we develop
to compute (via traces) norm bounds for certain random matrices whose entries
are highly dependent; we hope this method will be useful elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06450</identifier>
 <datestamp>2015-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06450</id><created>2015-03-22</created><updated>2015-06-05</updated><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>Kumar</keyname><forenames>Shankar</forenames></author></authors><title>Multilingual Open Relation Extraction Using Cross-lingual Projection</title><categories>cs.CL</categories><comments>Proceedings of NAACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open domain relation extraction systems identify relation and argument
phrases in a sentence without relying on any underlying schema. However,
current state-of-the-art relation extraction systems are available only for
English because of their heavy reliance on linguistic tools such as
part-of-speech taggers and dependency parsers. We present a cross-lingual
annotation projection method for language independent relation extraction. We
evaluate our method on a manually annotated test set and present results on
three typologically different languages. We release these manual annotations
and extracted relations in 61 languages from Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06452</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06452</id><created>2015-03-22</created><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Unsupervised model compression for multilayer bootstrap networks</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, multilayer bootstrap network (MBN) has demonstrated promising
performance in unsupervised dimensionality reduction. It can learn compact
representations in standard data sets, i.e. MNIST and RCV1. However, as a
bootstrap method, the prediction complexity of MBN is high. In this paper, we
propose an unsupervised model compression framework for this general problem of
unsupervised bootstrap methods. The framework compresses a large unsupervised
bootstrap model into a small model by taking the bootstrap model and its
application together as a black box and learning a mapping function from the
input of the bootstrap model to the output of the application by a supervised
learner. To specialize the framework, we propose a new technique, named
compressive MBN. It takes MBN as the unsupervised bootstrap model and deep
neural network (DNN) as the supervised learner. Our initial result on MNIST
showed that compressive MBN not only maintains the high prediction accuracy of
MBN but also is over thousands of times faster than MBN at the prediction
stage. Our result suggests that the new technique integrates the effectiveness
of MBN on unsupervised learning and the effectiveness and efficiency of DNN on
supervised learning together for the effectiveness and efficiency of
compressive MBN on unsupervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06453</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06453</id><created>2015-03-22</created><authors><author><keyname>Zick</keyname><forenames>Kenneth M.</forenames></author><author><keyname>Shehab</keyname><forenames>Omar</forenames></author><author><keyname>French</keyname><forenames>Matthew</forenames></author></authors><title>Experimental quantum annealing: case study involving the graph
  isomorphism problem</title><categories>quant-ph cs.ET</categories><comments>15 pages, 6 figures</comments><journal-ref>Sci. Rep. 5, 11168 (2015)</journal-ref><doi>10.1038/srep11168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum annealing is a proposed combinatorial optimization technique meant to
exploit quantum mechanical effects such as tunneling and entanglement.
Real-world quantum annealing-based solvers require a combination of annealing
and classical pre- and post-processing; at this early stage, little is known
about how to partition and optimize the processing. This article presents an
experimental case study of quantum annealing and some of the factors involved
in real-world solvers, using a 504-qubit D-Wave Two machine and the graph
isomorphism problem. To illustrate the role of classical pre-processing, a
compact Hamiltonian is presented that enables a reduced Ising model for each
problem instance. On random N-vertex graphs, the median number of variables is
reduced from N^2 to fewer than N lg N and solvable graph sizes increase from N
= 5 to N = 13. Additionally, a type of classical post-processing error
correction is evaluated. While the solution times are not competitive with
classical approaches to graph isomorphism, the enhanced solver ultimately
classified correctly every problem that was mapped to the processor and
demonstrated clear advantages over the baseline approach. The results shed some
light on the nature of real-world quantum annealing and the associated hybrid
classical-quantum solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06456</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06456</id><created>2015-03-22</created><authors><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Mancini</keyname><forenames>Simone</forenames></author><author><keyname>Sarzo</keyname><forenames>Fabio</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author></authors><title>Model predictive controllers for reduction of mechanical fatigue in wind
  farms</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of dispatching WindFarm (WF) power demand to
individual Wind Turbines (WT) with the goal of minimizing mechanical stresses.
We assume wind is strong enough to let each WTs to produce the required power
and propose different closed-loop Model Predictive Control (MPC) dispatching
algorithms. Similarly to existing approaches based on MPC, our methods do not
require changes in WT hardware but only software changes in the SCADA system of
the WF. However, differently from previous MPC schemes, we augment the model of
a WT with an ARMA predictor of the wind turbulence, which reduces uncertainty
in wind predictions over the MPC control horizon. This allows us to develop
both stochastic and deterministic MPC algorithms. In order to compare different
MPC schemes and demonstrate improvements with respect to classic open-loop
schedulers, we performed simulations using the SimWindFarm toolbox for MatLab.
We demonstrate that MPC controllers allow to achieve reduction of stresses even
in the case of large installations such as the 100-WTs Thanet offshore WF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06462</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06462</id><created>2015-03-19</created><authors><author><keyname>Patro</keyname><forenames>S. Gopal Krishna</forenames></author><author><keyname>Sahu</keyname><forenames>Kishore Kumar</forenames></author></authors><title>Normalization: A Preprocessing Stage</title><categories>cs.OH</categories><comments>4 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we know that the normalization is a pre-processing stage of any type
problem statement. Especially normalization takes important role in the field
of soft computing, cloud computing etc. for manipulation of data like scale
down or scale up the range of data before it becomes used for further stage.
There are so many normalization techniques are there namely Min-Max
normalization, Z-score normalization and Decimal scaling normalization. So by
referring these normalization techniques we are going to propose one new
normalization technique namely, Integer Scaling Normalization. And we are going
to show our proposed normalization technique using various data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06465</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06465</id><created>2015-03-22</created><authors><author><keyname>Carreira</keyname><forenames>Joao</forenames></author><author><keyname>Vicente</keyname><forenames>Sara</forenames></author><author><keyname>Agapito</keyname><forenames>Lourdes</forenames></author><author><keyname>Batista</keyname><forenames>Jorge</forenames></author></authors><title>Lifting Object Detection Datasets into 3D</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While data has certainly taken the center stage in computer vision in recent
years, it can still be difficult to obtain in certain scenarios. In particular,
acquiring ground truth 3D shapes of objects pictured in 2D images remains a
challenging feat and this has hampered progress in recognition-based object
reconstruction from a single image. Here we propose to bypass previous
solutions such as 3D scanning or manual design, that scale poorly, and instead
populate object category detection datasets semi-automatically with dense,
per-object 3D reconstructions, bootstrapped from:(i) class labels, (ii) ground
truth figure-ground segmentations and (iii) a small set of keypoint
annotations. Our proposed algorithm first estimates camera viewpoint using
rigid structure-from-motion and then reconstructs object shapes by optimizing
over visual hull proposals guided by loose within-class shape similarity
assumptions. The visual hull sampling process attempts to intersect an object's
projection cone with the cones of minimal subsets of other similar objects
among those pictured from certain vantage points. We show that our method is
able to produce convincing per-object 3D reconstructions and to accurately
estimate cameras viewpoints on one of the most challenging existing
object-category detection datasets, PASCAL VOC. We hope that our results will
re-stimulate interest on joint object recognition and 3D reconstruction from a
single image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06466</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06466</id><created>2015-03-22</created><updated>2015-04-17</updated><authors><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Xu</keyname><forenames>Xinyi</forenames></author><author><keyname>Alanis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Ng</keyname><forenames>Soon Xin</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Is the Low-Complexity Mobile-Relay-Aided FFR-DAS Capable of
  Outperforming the High-Complexity CoMP?</title><categories>cs.IT math.IT</categories><comments>16 pages, 17 figures, 2 tables, accepted to appear on IEEE
  Transactions on Vehicular Technology, March 2015</comments><doi>10.1109/TVT.2015.2416333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coordinated multi-point transmission/reception aided collocated antenna
system (CoMP-CAS) and mobile relay assisted fractional frequency reuse
distributed antenna system (MR-FFR-DAS) constitute a pair of virtual-MIMO based
technical options for achieving high spectral efficiency in
interference-limited cellular networks. In practice both techniques have their
respective pros and cons, which are studied in this paper by evaluating the
achievable cell-edge performance on the uplink of multicell systems. We show
that assuming the same antenna configuration in both networks, the maximum
available cooperative spatial diversity inherent in the MR-FFR-DAS is lower
than that of the CoMP-CAS. However, when the cell-edge MSs have a low
transmission power, the lower-complexity MR-FFR-DAS relying on the simple
single-cell processing may outperform the CoMP-CAS by using the proposed
soft-combining based probabilistic data association (SC-PDA) receiver, despite
the fact that the latter scheme is more complex and incurs a higher cooperation
overhead. Furthermore, the benefits of the SC-PDA receiver may be enhanced by
properly selecting the MRs' positions. Additionally, we show that the
performance of the cell-edge MSs roaming near the angular direction halfway
between two adjacent RAs (i.e. the &quot;worst-case direction&quot;) of the MR-FFR-DAS
may be more significantly improved than that of the cell-edge MSs of other
directions by using multiuser power control, which also improves the fairness
amongst cell-edge MSs. Our simulation results show that given a moderate MS
transmit power, the proposed MR-FFR-DAS architecture employing the SC-PDA
receiver is capable of achieving significantly better bit-error rate (BER) and
effective throughput across the entire cell-edge area, including even the
&quot;worst-case direction&quot; and the cell-edge boundary, than the CoMP-CAS
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06468</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06468</id><created>2015-03-22</created><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Esnaola</keyname><forenames>Inaki</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sanjeev R.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Machine Learning Methods for Attack Detection in the Smart Grid</title><categories>cs.LG cs.CR cs.SY</categories><comments>14 pages, 11 Figures</comments><journal-ref>A version of the manuscript was published in IEEE Transactions on
  Neural Networks and Learning Systems, 19 March 2015</journal-ref><doi>10.1109/TNNLS.2015.2404803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attack detection problems in the smart grid are posed as statistical learning
problems for different attack scenarios in which the measurements are observed
in batch or online settings. In this approach, machine learning algorithms are
used to classify measurements as being either secure or attacked. An attack
detection framework is provided to exploit any available prior knowledge about
the system and surmount constraints arising from the sparse structure of the
problem in the proposed approach. Well-known batch and online learning
algorithms (supervised and semi-supervised) are employed with decision and
feature level fusion to model the attack detection problem. The relationships
between statistical and geometric properties of attack vectors employed in the
attack scenarios and learning algorithms are analyzed to detect unobservable
attacks using statistical learning methods. The proposed algorithms are
examined on various IEEE test systems. Experimental analyses show that machine
learning algorithms can detect attacks with performances higher than the attack
detection algorithms which employ state vector estimation methods in the
proposed attack detection framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06479</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06479</id><created>2015-03-22</created><authors><author><keyname>Khabbazian</keyname><forenames>Majid</forenames></author></authors><title>Multi-Version Coding</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a simple lower bound for the multi-version coding problem
formulated in [1]. We also propose simple algorithms that almost match the
lower bound derived. Another lower bound is proven for an extended version of
the multi-version coding problem introduced in [2].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06480</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06480</id><created>2015-03-22</created><authors><author><keyname>Islam</keyname><forenames>Md. Ariful</forenames></author><author><keyname>DeFrancisco</keyname><forenames>Richard</forenames></author><author><keyname>Fan</keyname><forenames>Chuchu</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author><author><keyname>Smolka</keyname><forenames>Scott A.</forenames></author></authors><title>Model Checking Tap Withdrawal in C. Elegans</title><categories>cs.LO cs.CE cs.SY q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present what we believe to be the first formal verification of a
biologically realistic (nonlinear ODE) model of a neural circuit in a
multicellular organism: Tap Withdrawal (TW) in \emph{C. Elegans}, the common
roundworm. TW is a reflexive behavior exhibited by \emph{C. Elegans} in
response to vibrating the surface on which it is moving; the neural circuit
underlying this response is the subject of this investigation. Specifically, we
perform reachability analysis on the TW circuit model of Wicks et al. (1996),
which enables us to estimate key circuit parameters. Underlying our approach is
the use of Fan and Mitra's recently developed technique for automatically
computing local discrepancy (convergence and divergence rates) of general
nonlinear systems. We show that the results we obtain are in agreement with the
experimental results of Wicks et al. (1995). As opposed to the fixed parameters
found in most biological models, which can only produce the predominant
behavior, our techniques characterize ranges of parameters that produce (and do
not produce) all three observed behaviors: reversal of movement, acceleration,
and lack of response.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06483</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06483</id><created>2015-03-22</created><authors><author><keyname>Kowsari</keyname><forenames>Kamran</forenames></author><author><keyname>Yammahi</keyname><forenames>Maryam</forenames></author><author><keyname>Bari</keyname><forenames>Nima</forenames></author><author><keyname>Vichr</keyname><forenames>Roman</forenames></author><author><keyname>Alsaby</keyname><forenames>Faisal</forenames></author><author><keyname>Berkovich</keyname><forenames>Simon Y.</forenames></author></authors><title>Construction of FuzzyFind Dictionary using Golay Coding Transformation
  for Searching Applications</title><categories>cs.DB</categories><doi>10.14569/IJACSA.2015.060313</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Searching through a large volume of data is very critical for companies,
scientists, and searching engines applications due to time complexity and
memory complexity. In this paper, a new technique of generating FuzzyFind
Dictionary for text mining was introduced. We simply mapped the 23 bits of the
English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more
FuzzyFind Dictionary, and reflecting the presence or absence of particular
letters. This representation preserves closeness of word distortions in terms
of closeness of the created binary vectors within Hamming distance of 2
deviations. This paper talks about the Golay Coding Transformation Hash Table
and how it can be used on a FuzzyFind Dictionary as a new technology for using
in searching through big data. This method is introduced by linear time
complexity for generating the dictionary and constant time complexity to access
the data and update by new data sets, also updating for new data sets is linear
time depends on new data points. This technique is based on searching only for
letters of English that each segment has 23 bits, and also we have more than
23-bit and also it could work with more segments as reference table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06485</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06485</id><created>2015-03-22</created><updated>2015-06-21</updated><authors><author><keyname>Hashim</keyname><forenames>Hanaan</forenames></author><author><keyname>Srikanth</keyname><forenames>R.</forenames></author></authors><title>The concept of free will as an infinite metatheoretic recursion</title><categories>physics.hist-ph cs.AI</categories><comments>Accepted in INDECS (close to the accepted version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is argued that the concept of free will, like the concept of truth in
formal languages, requires a separation between an object level and a
meta-level for being consistently defined. The Jamesian two-stage model, which
deconstructs free will into the causally open &quot;free&quot; stage with its closure in
the &quot;will&quot; stage, is implicitly a move in this direction. However, to avoid the
dilemma of determinism, free will additionally requires an infinite regress of
causal meta-stages, making free choice a hypertask. We use this model to define
free will of the rationalist-compatibilist type. This is shown to provide a
natural three-way distinction between quantum indeterminism, freedom and free
will, applicable respectively to artificial intelligence (AI), animal agents
and human agents. We propose that the causal hierarchy in our model corresponds
to a hierarchy of Turing uncomputability. Possible neurobiological and
behavioral tests to demonstrate free will experimentally are suggested.
Ramifications of the model for physics, evolutionary biology, neuroscience,
neuropathological medicine and moral philosophy are briefly outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06489</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06489</id><created>2015-03-22</created><updated>2015-10-03</updated><authors><author><keyname>Brinton</keyname><forenames>Christopher G.</forenames></author><author><keyname>Buccapatnam</keyname><forenames>Swapna</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Mining MOOC Clickstreams: On the Relationship Between Learner Behavior
  and Performance</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study student behavior and performance in two Massive Open Online Courses
(MOOCs). In doing so, we present two frameworks by which video-watching
clickstreams can be represented: one based on the sequence of events created,
and another on the sequence of positions visited. With the event-based
framework, we extract recurring subsequences of student behavior, which contain
fundamental characteris- tics such as reflecting (i.e., repeatedly playing and
pausing) and revising (i.e., plays and skip backs). We find that some of these
behaviors are significantly associated with whether a user will be Correct on
First Attempt (CFA) or not in answering quiz questions. With the position-based
framework, we then devise models for performance. In evaluating these through
CFA prediction, we find that three of them can substantially improve prediction
quality in terms of accuracy and F1, which underlines the ability to relate
behavior to performance. Since our prediction considers videos individually,
these benefits also suggest that our models are useful in situations where
there is limited training data, e.g., for early detection or in short courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06497</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06497</id><created>2015-03-22</created><authors><author><keyname>Rossi</keyname><forenames>Luca</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author><author><keyname>Torsello</keyname><forenames>Andrea</forenames></author></authors><title>On the k-Anonymization of Time-varying and Multi-layer Social Graphs</title><categories>cs.CR cs.SI</categories><comments>In Proceedings of 9th AAAI International Conference on Weblogs and
  Social Media (ICWSM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popularity of online social media platforms provides an unprecedented
opportunity to study real-world complex networks of interactions. However,
releasing this data to researchers and the public comes at the cost of
potentially exposing private and sensitive user information. It has been shown
that a naive anonymization of a network by removing the identity of the nodes
is not sufficient to preserve users' privacy. In order to deal with malicious
attacks, k-anonymity solutions have been proposed to partially obfuscate
topological information that can be used to infer nodes' identity.
  In this paper, we study the problem of ensuring k-anonymity in time-varying
graphs, i.e., graphs with a structure that changes over time, and multi-layer
graphs, i.e., graphs with multiple types of links. More specifically, we
examine the case in which the attacker has access to the degree of the nodes.
The goal is to generate a new graph where, given the degree of a node in each
(temporal) layer of the graph, such a node remains indistinguishable from other
k-1 nodes in the graph. In order to achieve this, we find the optimal
partitioning of the graph nodes such that the cost of anonymizing the degree
information within each group is minimum. We show that this reduces to a
special case of a Generalized Assignment Problem, and we propose a simple yet
effective algorithm to solve it. Finally, we introduce an iterated linear
programming approach to enforce the realizability of the anonymized degree
sequences. The efficacy of the method is assessed through an extensive set of
experiments on synthetic and real-world graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06499</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06499</id><created>2015-03-22</created><authors><author><keyname>Rossi</keyname><forenames>Luca</forenames></author><author><keyname>Williams</keyname><forenames>Matthew J.</forenames></author><author><keyname>Stich</keyname><forenames>Christoph</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Privacy and the City: User Identification and Location Semantics in
  Location-Based Social Networks</title><categories>cs.CR cs.CY cs.SI</categories><comments>In Proceedings of 9th AAAI International Conference on Weblogs and
  Social Media (ICWSM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of GPS enabled smartphones, an increasing number of users is
actively sharing their location through a variety of applications and services.
Along with the continuing growth of Location-Based Social Networks (LBSNs),
security experts have increasingly warned the public of the dangers of exposing
sensitive information such as personal location data. Most importantly, in
addition to the geographical coordinates of the user's location, LBSNs allow
easy access to an additional set of characteristics of that location, such as
the venue type or popularity.
  In this paper, we investigate the role of location semantics in the
identification of LBSN users. We simulate a scenario in which the attacker's
goal is to reveal the identity of a set of LBSN users by observing their
check-in activity. We then propose to answer the following question: what are
the types of venues that a malicious user has to monitor to maximize the
probability of success? Conversely, when should a user decide whether to make
his/her check-in to a location public or not? We perform our study on more than
1 million check-ins distributed over 17 urban regions of the United States. Our
analysis shows that different types of venues display different discriminative
power in terms of user identity, with most of the venues in the &quot;Residence&quot;
category providing the highest re-identification success across the urban
regions. Interestingly, we also find that users with a high entropy of their
check-ins distribution are not necessarily the hardest to identify, suggesting
that it is the collective behaviour of the users' population that determines
the complexity of the identification task, rather than the individual
behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06506</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06506</id><created>2015-03-22</created><updated>2015-09-09</updated><authors><author><keyname>Chen</keyname><forenames>Xudong</forenames></author></authors><title>A Remark on Formation Control with Triangulated Laman Graphs: Genericity
  of Equivariant Morse Functions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper, as a continuing work of [1], focus on establishing the fact that
if we equip a reciprocal multi-agent (RMA) system with a triangulated Laman
graph (TLG), then the associated potential function is generically an
equivariant Morse function, i.e, there are only finitely many critical orbits
each of which is nondegenerate. Though this assumption on the potential
function of being an equivariant Morse function has been used, and in fact
indispensable, in several occasions. But it is actually still an open question
whether it is true for a given RMA system. Thus, in this paper we will provide
a confirmative answer to the question for the class of RMA systems with TLGs.
The main result, as well as the analysis of this paper, has many implications
for other difficult problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06511</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06511</id><created>2015-03-22</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>Linear Codes from Some 2-Designs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A classical method of constructing a linear code over $\gf(q)$ with a
$t$-design is to use the incidence matrix of the $t$-design as a generator
matrix over $\gf(q)$ of the code. This approach has been extensively
investigated in the literature. In this paper, a different method of
constructing linear codes using specific classes of $2$-designs is studied, and
linear codes with a few weights are obtained from almost difference sets,
difference sets, and a type of $2$-designs associated to semibent functions.
Two families of the codes obtained in this paper are optimal. The linear codes
presented in this paper have applications in secret sharing and authentication
schemes, in addition to their applications in consumer electronics,
communication and data storage systems. A coding-theory approach to the
characterisation of highly nonlinear Boolean functions is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06512</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06512</id><created>2015-03-22</created><authors><author><keyname>Ding</keyname><forenames>Kelan</forenames></author><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>A Class of Two-Weight and Three-Weight Codes and Their Applications in
  Secret Sharing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a class of two-weight and three-weight linear codes over
$\gf(p)$ is constructed, and their application in secret sharing is
investigated. Some of the linear codes obtained are optimal in the sense that
they meet certain bounds on linear codes. These codes have applications also in
authentication codes, association schemes, and strongly regular graphs, in
addition to their applications in consumer electronics, communication and data
storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06514</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06514</id><created>2015-03-22</created><updated>2015-07-27</updated><authors><author><keyname>Lin</keyname><forenames>Haoxiang</forenames></author></authors><title>On the Well Extension of Partial Well Orderings</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the well extension of strict(irreflective) partial
well orderings. We first prove that any partially well-ordered structure &lt;A, R&gt;
can be extended to a well-ordered one. Then we prove that every linear
extension of &lt;A, R&gt; is well-ordered if and only if A has no infinite totally
unordered subset under R.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06515</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06515</id><created>2015-03-22</created><authors><author><keyname>Singh</keyname><forenames>Vaibhav</forenames></author><author><keyname>Prasad</keyname><forenames>Narayan</forenames></author><author><keyname>Arslan</keyname><forenames>Mustafa Y.</forenames></author><author><keyname>Rangarajan</keyname><forenames>Sampath</forenames></author></authors><title>Optimizing User Association and Activation Fractions in Heterogeneous
  Wireless Networks</title><categories>cs.NI</categories><comments>To appear in part, WiOpt 2015; Tech. Report, NECLA, Aug. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of maximizing the alpha-fairness utility over the
downlink of a heterogeneous wireless network (HetNet) by jointly optimizing the
association of users to transmission points (TPs) and the activation fractions
of all TPs. Activation fraction of each TP is the fraction of the frame
duration for which it is active, and together these fractions influence the
interference seen in the network. To address this joint optimization problem we
adopt an approach wherein the activation fractions and the user associations
are optimized in an alternating manner. The sub-problem of determining the
optimal activation fractions is solved using an auxiliary function method that
we show is provably convergent and is amenable to distributed implementation.
On the other hand, the sub-problem of determining the user association is
solved via a simple combinatorial algorithm. Meaningful performance guarantees
are derived and a distributed variant offering identical guarantees is also
proposed. The significant benefits of using the proposed algorithms are then
demonstrated via realistic simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06532</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06532</id><created>2015-03-23</created><authors><author><keyname>Karimi</keyname><forenames>Kamran</forenames></author></authors><title>The Feasibility of Using OpenCL Instead of OpenMP for Parallel CPU
  Programming</title><categories>cs.DC cs.PF</categories><comments>8 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenCL, along with CUDA, is one of the main tools used to program GPGPUs.
However, it allows running the same code on multi-core CPUs too, making it a
rival for the long-established OpenMP. In this paper we compare OpenCL and
OpenMP when developing and running compute-heavy code on a CPU. Both ease of
programming and performance aspects are considered. Since, unlike a GPU, no
memory copy operation is involved, our comparisons measure the code generation
quality, as well as thread management efficiency of OpenCL and OpenMP. We
evaluate the performance of these development tools under two conditions: a
large number of short-running compute-heavy parallel code executions, when more
thread management is performed, and a small number of long-running parallel
code executions, when less thread management is required. The results show that
OpenCL and OpenMP each win in one of the two conditions. We argue that while
using OpenMP requires less setup, OpenCL can be a viable substitute for OpenMP
from a performance point of view, especially when a high number of thread
invocations is required. We also provide a number of potential pitfalls to
watch for when moving from OpenMP to OpenCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06536</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06536</id><created>2015-03-23</created><authors><author><keyname>Liu</keyname><forenames>Qipeng</forenames></author><author><keyname>Liu</keyname><forenames>Yicheng</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author></authors><title>Mechanism design for resource allocation with applications to
  centralized multi-commodity routing</title><categories>cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and study the algorithmic mechanism design problem for a general
class of resource allocation settings, where the center redistributes the
private resources brought by individuals. Money transfer is forbidden. Distinct
from the standard literature, which assumes the amount of resources brought by
an individual to be public information, we consider this amount as an agent's
private, possibly multi-dimensional type. Our goal is to design truthful
mechanisms that achieve two objectives: max-min and Pareto efficiency. For each
objective, we provide a reduction that converts any optimal algorithm into a
strategy-proof mechanism that achieves the same objective. Our reductions do
not inspect the input algorithms but only query these algorithms as oracles.
Applying the reductions, we produce strategy-proof mechanisms in a non-trivial
application: network route allocation. Our models and result in the application
are valuable on their own rights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06544</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06544</id><created>2015-03-23</created><updated>2015-03-23</updated><authors><author><keyname>Choi</keyname><forenames>Sou-Cheng T.</forenames></author><author><keyname>Ding</keyname><forenames>Yuhan</forenames></author><author><keyname>Hickernell</keyname><forenames>Fred J.</forenames></author><author><keyname>Jiang</keyname><forenames>Lan</forenames></author><author><keyname>Rugama</keyname><forenames>Llu&#xed;s Antoni Jim&#xe9;nez</forenames></author><author><keyname>Tong</keyname><forenames>Xin</forenames></author><author><keyname>Zhang</keyname><forenames>Yizhi</forenames></author><author><keyname>Zhou</keyname><forenames>Xuan</forenames></author></authors><title>GAIL---Guaranteed Automatic Integration Library in MATLAB: Documentation
  for Version 2.1</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic and adaptive approximation, optimization, or integration of
functions in a cone with guarantee of accuracy is a relatively new paradigm.
Our purpose is to create an open-source MATLAB package, Guaranteed Automatic
Integration Library (GAIL), following the philosophy of reproducible research
and sustainable practices of robust scientific software development. For our
conviction that true scholarship in computational sciences are characterized by
reliable reproducibility, we employ the best practices in mathematical research
and software engineering known to us and available in MATLAB. This document
describes the key features of functions in GAIL, which includes one-dimensional
function approximation and minimization using linear splines, one-dimensional
numerical integration using trapezoidal rule, and last but not least, mean
estimation and multidimensional integration by Monte Carlo methods or Quasi
Monte Carlo methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06548</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06548</id><created>2015-03-23</created><authors><author><keyname>Kanoje</keyname><forenames>Sumitkumar</forenames></author><author><keyname>Powar</keyname><forenames>Varsha</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Using MongoDB for Social Networking Website</title><categories>cs.DB</categories><comments>3 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media is a biggest successful buzzword used in the recent time. Its
success opened various opportunities for the developers. Developing any
application requires storage of large data into databases. Many databases are
available for the developers, Choosing the right one make development easier.
MongoDB is a cross platform document oriented, schema-less database eschewed
the traditional table based relational database structure in favor of JSON like
documents. This article discusses various pros and cons encountered with the
use of the MongoDB so that developers would be helped while choosing it wisely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06549</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06549</id><created>2015-03-23</created><authors><author><keyname>Fischer</keyname><forenames>Lydia</forenames></author><author><keyname>Hammer</keyname><forenames>Barbara</forenames></author><author><keyname>Wersing</keyname><forenames>Heiko</forenames></author></authors><title>Optimum Reject Options for Prototype-based Classification</title><categories>cs.LG</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse optimum reject strategies for prototype-based classifiers and
real-valued rejection measures, using the distance of a data point to the
closest prototype or probabilistic counterparts. We compare reject schemes with
global thresholds, and local thresholds for the Voronoi cells of the
classifier. For the latter, we develop a polynomial-time algorithm to compute
optimum thresholds based on a dynamic programming scheme, and we propose an
intuitive linear time, memory efficient approximation thereof with competitive
accuracy. Evaluating the performance in various benchmarks, we conclude that
local reject options are beneficial in particular for simple prototype-based
classifiers, while the improvement is less pronounced for advanced models. For
the latter, an accuracy-reject curve which is comparable to support vector
machine classifiers with state of the art reject options can be reached.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06555</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06555</id><created>2015-03-23</created><authors><author><keyname>Kanoje</keyname><forenames>Sumitkumar</forenames></author><author><keyname>Girase</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>User Profiling for Recommendation System</title><categories>cs.IR cs.HC</categories><comments>5 pages, 5 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation system is a type of information filtering systems that
recommend various objects from a vast variety and quantity of items which are
of the user interest. This results in guiding an individual in personalized way
to interesting or useful objects in a large space of possible options. Such
systems also help many businesses to achieve more profits to sustain in their
filed against their rivals. But looking at the amount of information which a
business holds it becomes difficult to identify the items of user interest.
Therefore personalization or user profiling is one of the challenging tasks
that give access to user relevant information which can be used in solving the
difficult task of classification and ranking items according to an individuals
interest. Profiling can be done in various ways such assupervised or
unsupervised, individual or group profiling, distributive or and non
distributive profiling. Our focus in this paper will be on the dataset which we
will use, we identify some interesting facts by using Weka Tool that can be
used for recommending the items from dataset. Our aim is to present a novel
technique to achieve user profiling in recommendation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06558</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06558</id><created>2015-03-23</created><authors><author><keyname>Raje</keyname><forenames>Manali</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>Algorithm for Back-up and Authentication of Data Stored on Cloud</title><categories>cs.DC cs.CR</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Everyday a huge amount of data is generated in Cloud Computing. The
maintenance of this electronic data needs some extremely efficient services.
There is a need to properly collect this data, check for its authenticity and
develop proper backups is needed. The Objective of this paper is to provide
Response Server, some solution for the backup of data and its restoration,
using the Cloud. Thecollection of the data is to be done from the client and
then the data should be sent to a central location. This process is a platform
independent one. The data can then be used as required. The Remote Backup
Server facilitates the collection of information from any remote location and
provides services to recover the data in case of loss. The authentication of
the user is done by using the Asymmetric key algorithm which will in turn leads
to the authentication of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06561</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06561</id><created>2015-03-23</created><authors><author><keyname>Gupta</keyname><forenames>Ankit</forenames></author><author><keyname>Oberoi</keyname><forenames>Ashish</forenames></author></authors><title>A Comparative Analysis of Tensor Decomposition Models Using Hyper
  Spectral Image</title><categories>cs.NA cs.CV</categories><comments>7 pages, 3 figures,1 table</comments><journal-ref>International Journal of Computer Science Trends and Technology
  (IJCST) V3(2): Page(5-11) Mar-Apr 2015. ISSN: 2347-8578</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyper spectral imaging is a remote sensing technology, providing variety of
applications such as material identification, space object identification,
planetary exploitation etc. It deals with capturing continuum of images of the
earth surface from different angles. Due to the multidimensional nature of the
image, multi-way arrays are one of the possible solutions for analyzing hyper
spectral data. This multi-way array is called tensor. Our approach deals with
implementing three decomposition models LMLRA, BTD and CPD to the sample data
for choosing the best decomposition of the data set. The results have proved
that Block Term Decomposition (BTD) is the best tensor model for decomposing
the hyper spectral image in to resultant factor matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06562</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06562</id><created>2015-03-23</created><authors><author><keyname>Bokde</keyname><forenames>Dheeraj kumar</forenames></author><author><keyname>Girase</keyname><forenames>Sheetal</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author></authors><title>An Item-Based Collaborative Filtering using Dimensionality Reduction
  Techniques on Mahout Framework</title><categories>cs.IR</categories><comments>6 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative Filtering is the most widely used prediction technique in
Recommendation System. Most of the current CF recommender systems maintains
single criteria user rating in user item matrix. However, recent studies
indicate that recommender system depending on multi criteria can improve
prediction and accuracy levels of recommendation by considering the user
preferences in multi aspects of items. This gives birth to Multi Criteria
Collaborative Filtering. In MC CF users provide the rating on multiple aspects
of an item in new dimensions,thereby increasing the size of rating matrix,
sparsity and scalability problem. Appropriate dimensionality reduction
techniques are thus needed to take care of these challenges to reduce the
dimension of user item rating matrix to improve the prediction accuracy and
efficiency of CF recommender system. The process of dimensionality reduction
maps the high dimensional input space into lower dimensional space. Thus, the
objective of this paper is to propose an efficient MC CF algorithm using
dimensionality reduction technique to improve the recommendation quality and
prediction accuracy. Dimensionality reduction techniques such as Singular Value
Decomposition and Principal Component Analysis are used to solve the
scalability and alleviate the sparsity problems in overall rating. The proposed
MC CF approach will be implemented using Apache Mahout, which allows processing
of massive dataset stored in distributed/non-distributed file system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06567</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06567</id><created>2015-03-23</created><updated>2015-08-22</updated><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>On some provably correct cases of variational inference for topic models</title><categories>cs.LG cs.DS stat.ML</categories><comments>46 pages, Compared to previous version: clarified notation, a number
  of typos fixed throughout paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational inference is a very efficient and popular heuristic used in
various forms in the context of latent variable models. It's closely related to
Expectation Maximization (EM), and is applied when exact EM is computationally
infeasible. Despite being immensely popular, current theoretical understanding
of the effectiveness of variaitonal inference based algorithms is very limited.
In this work we provide the first analysis of instances where variational
inference algorithms converge to the global optimum, in the setting of topic
models.
  More specifically, we show that variational inference provably learns the
optimal parameters of a topic model under natural assumptions on the topic-word
matrix and the topic priors. The properties that the topic word matrix must
satisfy in our setting are related to the topic expansion assumption introduced
in (Anandkumar et al., 2013), as well as the anchor words assumption in (Arora
et al., 2012c). The assumptions on the topic priors are related to the well
known Dirichlet prior, introduced to the area of topic modeling by (Blei et
al., 2003).
  It is well known that initialization plays a crucial role in how well
variational based algorithms perform in practice. The initializations that we
use are fairly natural. One of them is similar to what is currently used in
LDA-c, the most popular implementation of variational inference for topic
models. The other one is an overlapping clustering algorithm, inspired by a
work by (Arora et al., 2014) on dictionary learning, which is very simple and
efficient.
  While our primary goal is to provide insights into when variational inference
might work in practice, the multiplicative, rather than the additive nature of
the variational inference updates forces us to use fairly non-standard proof
arguments, which we believe will be of general interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06571</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06571</id><created>2015-03-23</created><authors><author><keyname>Kaushik</keyname><forenames>Ankit</forenames></author><author><keyname>Sharma</keyname><forenames>Shree Krishna</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich</forenames></author></authors><title>Estimation-Throughput Tradeoff for Underlay Cognitive Radio Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, to appear in Proceedings of IEEE International
  Conference on Communications (ICC) - Cognitive Radio and Networks Symposium,
  June 2015, London, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the performance of cognitive radio systems is of great
interest. To perform dynamic spectrum access, different paradigms are
conceptualized in the literature. Of these, Underlay System (US) has caught
much attention in the recent past. According to US, a power control mechanism
is employed at the Secondary Transmitter (ST) to constrain the interference at
the Primary Receiver (PR) below a certain threshold. However, it requires the
knowledge of channel towards PR at the ST. This knowledge can be obtained by
estimating the received power, assuming a beacon or a pilot channel
transmission by the PR. This estimation is never perfect, hence the induced
error may distort the true performance of the US. Motivated by this fact, we
propose a novel model that captures the effect of channel estimation errors on
the performance of the system. More specifically, we characterize the
performance of the US in terms of the estimation-throughput tradeoff.
Furthermore, we determine the maximum achievable throughput for the secondary
link. Based on numerical analysis, it is shown that the conventional model
overestimates the performance of the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06572</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06572</id><created>2015-03-23</created><authors><author><keyname>Shi</keyname><forenames>Bichen</forenames></author><author><keyname>Schellekens</keyname><forenames>Michel</forenames></author><author><keyname>Ifrim</keyname><forenames>Georgiana</forenames></author></authors><title>A Machine Learning Approach to Predicting the Smoothed Complexity of
  Sorting Algorithms</title><categories>cs.LG cs.AI cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smoothed analysis is a framework for analyzing the complexity of an
algorithm, acting as a bridge between average and worst-case behaviour. For
example, Quicksort and the Simplex algorithm are widely used in practical
applications, despite their heavy worst-case complexity. Smoothed complexity
aims to better characterize such algorithms. Existing theoretical bounds for
the smoothed complexity of sorting algorithms are still quite weak.
Furthermore, empirically computing the smoothed complexity via its original
definition is computationally infeasible, even for modest input sizes. In this
paper, we focus on accurately predicting the smoothed complexity of sorting
algorithms, using machine learning techniques. We propose two regression models
that take into account various properties of sorting algorithms and some of the
known theoretical results in smoothed analysis to improve prediction quality.
We show experimental results for predicting the smoothed complexity of
Quicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore
filling the gap between known theoretical and empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06574</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06574</id><created>2015-03-23</created><authors><author><keyname>Hu</keyname><forenames>Lansheng</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author></authors><title>Dynamic Power Splitting Policies for AF Relay Networks with Wireless
  Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE ICC 2015 - Workshop on Green Communications and
  Networks with Energy Harvesting, Smart Grids, and Renewable Energies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless energy harvesting (WEH) provides an exciting way to supply energy
for relay nodes to forward information for the source-destination pairs. In
this paper, we investigate the problem on how the relay node dynamically
adjusts the power splitting ratio of information transmission (IT) and energy
harvesting (EH) in order to achieve the optimal outage performance. According
to the knowledge of channel state information (CSI) at the relay, optimal
dynamic power splitting policy with full CSI and partial CSI are both provided.
Finally, through simulations, the proposed power splitting policies can improve
the outage performances and the policy with full CSI achieves the best
performance. It is also shown that the policy with partial CSI can approach the
policy with full CSI closely and incurs far less system overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06575</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06575</id><created>2015-03-23</created><authors><author><keyname>Brdar</keyname><forenames>Sanja</forenames></author><author><keyname>Gavric</keyname><forenames>Katarina</forenames></author><author><keyname>Culibrk</keyname><forenames>Dubravko</forenames></author><author><keyname>Crnojevic</keyname><forenames>Vladimir</forenames></author></authors><title>Unveiling Spatial Epidemiology of HIV with Mobile Phone Data</title><categories>stat.AP cs.CY cs.SI</categories><comments>13 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing amount of geo-referenced mobile phone data enables the
identification of behavioral patterns, habits and movements of people. With
this data, we can extract the knowledge potentially useful for many
applications including the one tackled in this study - understanding spatial
variation of epidemics. We explored the datasets collected by a cell phone
service provider and linked them to spatial HIV prevalence rates estimated from
publicly available surveys. For that purpose, 224 features were extracted from
mobility and connectivity traces and related to the level of HIV epidemic in 50
Ivory Coast departments. By means of regression models, we evaluated predictive
ability of extracted features. Several models predicted HIV prevalence that are
highly correlated (&gt;0.7) with actual values. Through contribution analysis we
identified key elements that impact the rate of infections. Our findings
indicate that night connectivity and activity, spatial area covered by users
and overall migrations are strongly linked to HIV. By visualizing the
communication and mobility flows, we strived to explain the spatial structure
of epidemics. We discovered that strong ties and hubs in communication and
mobility align with HIV hot spots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06579</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06579</id><created>2015-03-23</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author></authors><title>The Emergence and Dynamical Evolution of Complex Transport Networks from
  Simple Low-Level Behaviours</title><categories>cs.ET nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The true slime mould Physarum polycephalum is a recent well studied example
of how complex transport networks emerge from simple auto-catalytic and self-
organising local interactions, adapting structure and function against changing
environmental conditions and external perturbation. Physarum networks also
exhibit computationally desirable measures of transport efficiency in terms of
overall path length, minimal connectivity and network resilience. Although
significant progress has been made in mathematically modelling the behaviour of
Physarum networks (and other biological transport networks) based on observed
features in experimental settings, their initial emergence - and in particular
their long-term persistence and evolution - is still poorly understood. We
present a low-level, bottom-up, approach to the modelling of emergent transport
networks. A population of simple particle-like agents coupled with paracrine
chemotaxis behaviours in a dissipative environment results in the spontaneous
emergence of persistent, complex structures. Second order emergent behaviours,
in the form of network surface minimisation, are also observed contributing to
the long term evolution and dynamics of the networks. The framework is extended
to allow data presentation and the population is used to perform a direct
(spatial) approximation of network minimisation problems. Three methods are
employed, loosely relating to behaviours of Physarum under different
environmental conditions. Finally, the low-level approach is summarised with a
view to further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06583</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06583</id><created>2015-03-23</created><authors><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Information content of contact-pattern representations and
  predictability of epidemic outbreaks</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><comments>Supplementary material not included</comments><journal-ref>Sci. Rep. 5, 14462 (2015)</journal-ref><doi>10.1038/srep14462</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To understand the contact patterns of a population -- who is in contact with
whom, and when the contacts happen -- is crucial for modeling outbreaks of
infectious disease. Traditional theoretical epidemiology assumes that any
individual can meet any with equal probability. A more modern approach, network
epidemiology, assumes people are connected into a static network over which the
disease spreads. Newer yet, temporal network epidemiology, includes the time in
the contact representations. In this paper, we investigate the effect of these
successive inclusions of more information. Using empirical proximity data, we
study both outbreak sizes from unknown sources, and from known states of
ongoing outbreaks. In the first case, there are large differences going from a
fully mixed simulation to a network, and from a network to a temporal network.
In the second case, differences are smaller. We interpret these observations in
terms of the temporal network structure of the data sets. For example, a fast
overturn of nodes and links seem to make the temporal information more
important.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06584</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06584</id><created>2015-03-23</created><authors><author><keyname>Loe</keyname><forenames>Chuan Wen</forenames></author><author><keyname>Jensen</keyname><forenames>Henrik Jeldtoft</forenames></author></authors><title>Citation Analysis with Mark-and-Recapture</title><categories>cs.DL physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mark-and-Recapture is a methodology from Population Biology to estimate the
number of a species without counting every individual. This is done by multiple
samplings of the species using traps and discounting the instances that were
caught repeated. In this paper we show that this methodology is applicable for
citation analysis as it is also not feasible to count all the relevant
publications of a research topic. In addition this estimation also allows us to
propose a stopping rule for researchers to decide how far one should extend
their search for relevant literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06590</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06590</id><created>2015-03-23</created><updated>2015-12-06</updated><authors><author><keyname>Boban</keyname><forenames>Mate</forenames></author><author><keyname>d'Orey</keyname><forenames>Pedro M.</forenames></author></authors><title>Exploring the Practical Limits of Cooperative Awareness in Vehicular
  Communications</title><categories>cs.NI</categories><comments>Accepted to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform an extensive study of cooperative awareness in vehicular
communication based on periodic message exchange. We start by analyzing
measurements collected on four test sites across Europe. To measure cooperative
awareness, we use three metrics: 1) neighborhood awareness ratio; 2) ratio of
neighbors above range; and 3) packet delivery rate. Using the collected data,
we define a simple model for calculating neighborhood awareness given packet
delivery ratio for a given environment. Finally, we perform realistic,
large-scale simulations to explore the achievable performance of cooperative
awareness under realistic transmit power and transmit rate constraints. Our
measurements and simulation results show that: i) above a certain threshold,
there is little benefit in increasing cooperative message rate to improve the
awareness; higher transmit power and fewer messages transmissions are a better
approach, since message delivery is dominated by shadowing. ii) the efficacy of
cooperative awareness varies greatly in different environments on both large
scale (e.g., 90% awareness is achievable up to 200 m in urban and over 500 m in
highway) and small scale (e.g., vehicles in nearby streets can have
significantly different awareness); iii) V2V and V2I communication have
distinct awareness patterns; iv) each location has a distinct transmit power
that achieves high awareness; and v) achieving high awareness levels results in
increased reception of potentially unwanted messages; therefore, a balance
needs to be found between awareness and interference, depending on the specific
context. We hope our results will serve as a starting point for designing more
effective periodic message exchange services for cooperative awareness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06598</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06598</id><created>2015-03-23</created><authors><author><keyname>Galkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Mouromtsev</keyname><forenames>Dmitry</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author></authors><title>Identifying Web Tables - Supporting a Neglected Type of Content on the
  Web</title><categories>cs.IR</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abundance of the data in the Internet facilitates the improvement of
extraction and processing tools. The trend in the open data publishing
encourages the adoption of structured formats like CSV and RDF. However, there
is still a plethora of unstructured data on the Web which we assume contain
semantics. For this reason, we propose an approach to derive semantics from web
tables which are still the most popular publishing tool on the Web. The paper
also discusses methods and services of unstructured data extraction and
processing as well as machine learning techniques to enhance such a workflow.
The eventual result is a framework to process, publish and visualize linked
open data. The software enables tables extraction from various open data
sources in the HTML format and an automatic export to the RDF format making the
data linked. The paper also gives the evaluation of machine learning techniques
in conjunction with string similarity functions to be applied in a tables
recognition task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06599</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06599</id><created>2015-03-23</created><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author></authors><title>An implementation of Sub-CAD in Maple</title><categories>cs.SC</categories><comments>9 pages</comments><report-no>University of Bath, Dept. Computer Science Technical Report Series,
  2015-01, 2015</report-no><msc-class>68W30</msc-class><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets, with applications in algebraic geometry
and beyond. We have previously reported on an implementation of CAD in Maple
which offers the original projection and lifting algorithm of Collins along
with subsequent improvements.
  Here we report on new functionality: specifically the ability to build
cylindrical algebraic sub-decompositions (sub-CADs) where only certain cells
are returned. We have implemented algorithms to return cells of a prescribed
dimensions or higher (layered {\scad}s), and an algorithm to return only those
cells on which given polynomials are zero (variety {\scad}s). These offer
substantial savings in output size and computation time.
  The code described and an introductory Maple worksheet / pdf demonstrating
the full functionality of the package are freely available online at
http://opus.bath.ac.uk/43911/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06600</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06600</id><created>2015-03-23</created><authors><author><keyname>Shakil</keyname><forenames>Kashish Ara</forenames><affiliation>Member, IAENG</affiliation></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames><affiliation>Member, IAENG</affiliation></author><author><keyname>Sethi</keyname><forenames>Shuchi</forenames></author></authors><title>Exploring Non-Homogeneity and Dynamicity of High Scale Cloud through
  Hive and Pig</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing deals with heterogeneity and dynamicity at all levels and
therefore there is a need to manage resources in such an environment and
properly allocate them. Resource planning and scheduling requires a proper
understanding of arrival patterns and scheduling of resources. Study of
workloads can aid in proper understanding of their associated environment.
Google has released its latest version of cluster trace, trace version 2.1 in
November 2014.The trace consists of cell information of about 29 days spanning
across 700k jobs. This paper deals with statistical analysis of this cluster
trace. Since the size of trace is very large, Hive which is a Hadoop
distributed file system (HDFS) based platform for querying and analysis of Big
data, has been used. Hive was accessed through its Beeswax interface. The data
was imported into HDFS through HCatalog. Apart from Hive, Pig which is a
scripting language and provides abstraction on top of Hadoop was used. To the
best of our knowledge the analytical method adopted by us is novel and has
helped in gaining several useful insights. Clustering of jobs and arrival time
has been done in this paper using K-means++ clustering followed by analysis of
distribution of arrival time of jobs which revealed weibull distribution while
resource usage was close to zip-f like distribution and process runtimes
revealed heavy tailed distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06606</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06606</id><created>2015-03-23</created><updated>2015-05-22</updated><authors><author><keyname>Nurminen</keyname><forenames>Henri</forenames></author><author><keyname>Ardeshiri</keyname><forenames>Tohid</forenames></author><author><keyname>Pich&#xe9;</keyname><forenames>Robert</forenames></author><author><keyname>Gustafsson</keyname><forenames>Fredrik</forenames></author></authors><title>Robust Inference for State-Space Models with Skewed Measurement Noise</title><categories>cs.SY stat.CO</categories><comments>5 pages, 7 figures. Accepted for publication in IEEE Signal
  Processing Letters</comments><journal-ref>IEEE Signal Processing Letters 22(11) (2015) 1898-1902</journal-ref><doi>10.1109/LSP.2015.2437456</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filtering and smoothing algorithms for linear discrete-time state-space
models with skewed and heavy-tailed measurement noise are presented. The
algorithms use a variational Bayes approximation of the posterior distribution
of models that have normal prior and skew-t-distributed measurement noise. The
proposed filter and smoother are compared with conventional low-complexity
alternatives in a simulated pseudorange positioning scenario. In the
simulations the proposed methods achieve better accuracy than the alternative
methods, the computational complexity of the filter being roughly 5 to 10 times
that of the Kalman filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06608</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06608</id><created>2015-03-23</created><authors><author><keyname>C</keyname><forenames>Lakshmi Devasena</forenames></author></authors><title>Proficiency Comparison of LADTree and REPTree Classifiers for Credit
  Risk Forecast</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1310.5963 by other authors</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vol.5, No.1, February 2015, pp. 39 - 50</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the Credit Defaulter is a perilous task of Financial Industries
like Banks. Ascertaining non-payer before giving loan is a significant and
conflict-ridden task of the Banker. Classification techniques are the better
choice for predictive analysis like finding the claimant, whether he/she is an
unpretentious customer or a cheat. Defining the outstanding classifier is a
risky assignment for any industrialist like a banker. This allow computer
science researchers to drill down efficient research works through evaluating
different classifiers and finding out the best classifier for such predictive
problems. This research work investigates the productivity of LADTree
Classifier and REPTree Classifier for the credit risk prediction and compares
their fitness through various measures. German credit dataset has been taken
and used to predict the credit risk with a help of open source machine learning
tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06609</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06609</id><created>2015-03-23</created><authors><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author><author><keyname>Sadaf</keyname><forenames>Kishwar</forenames></author></authors><title>Web Search Result Clustering based on Cuckoo Search and Consensus
  Clustering</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering of web search result document has emerged as a promising tool for
improving retrieval performance of an Information Retrieval (IR) system. Search
results often plagued by problems like synonymy, polysemy, high volume etc.
Clustering other than resolving these problems also provides the user the
easiness to locate his/her desired information. In this paper, a method, called
WSRDC-CSCC, is introduced to cluster web search result using cuckoo search
meta-heuristic method and Consensus clustering. Cuckoo search provides a solid
foundation for consensus clustering. As a local clustering function, k-means
technique is used. The final number of cluster is not depended on this k.
Consensus clustering finds the natural grouping of the objects. The proposed
algorithm is compared to another clustering method which is based on cuckoo
search and Bayesian Information Criterion. The experimental results show that
proposed algorithm finds the actual number of clusters with great value of
precision, recall and F-measure as compared to the other method
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06610</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06610</id><created>2015-03-23</created><authors><author><keyname>Barth</keyname><forenames>Dominique</forenames></author><author><keyname>David</keyname><forenames>Olivier</forenames></author><author><keyname>Quessette</keyname><forenames>Franck</forenames></author><author><keyname>Reinhard</keyname><forenames>Vincent</forenames></author><author><keyname>Strozecki</keyname><forenames>Yann</forenames></author><author><keyname>Vial</keyname><forenames>Sandrine</forenames></author></authors><title>Efficient Generation of Stable Planar Cages for Chemistry</title><categories>cs.DS</categories><comments>17 pages, 7 figures. Accepted at the 14th International Symposium on
  Experimental Algorithms (SEA 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe an algorithm which generates all colored planar
maps with a good minimum sparsity from simple motifs and rules to connect them.
An implementation of this algorithm is available and is used by chemists who
want to quickly generate all sound molecules they can obtain by mixing some
basic components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06614</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06614</id><created>2015-03-23</created><authors><author><keyname>Li</keyname><forenames>Yongzhe</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>Ambiguity Function of the Transmit Beamspace-Based MIMO Radar</title><categories>cs.IT math.IT</categories><comments>28 pages, 4 figures, Submitted to IEEE Trans. Signal Processing on
  July 2014</comments><journal-ref>IEEE Trans. Signal Processing, vol. 63, no. 17, pp.4445-4457,
  Sept. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive an ambiguity function (AF) for the transmit
beamspace (TB)-based multipleinput multiple-output (MIMO) radar for the case of
far-field targets and narrow-band waveforms. The effects of transmit coherent
processing gain and waveform diversity are incorporated into the AF definition.
To cover all the phase information conveyed by different factors, we introduce
the equivalent transmit phase centers. The newly defined AF serves as a
generalized AF form for which the phased-array (PA) and traditional MIMO radar
AFs are important special cases. We establish relationships among the defined
TB-based MIMO radar AF and the existing AF results including the Woodward's AF,
the AFs defined for the traditional colocated MIMO radar, and also the PA radar
AF, respectively. Moreover, we compare the TB-based MIMO radar AF with the
square-summation-form AF definition and identify two limiting cases to bound
its 'clear region' in Doppler-delay domain that is free of sidelobes.
Corresponding bounds for these two cases are derived, and it is shown that the
bound for the worst case is inversely proportional to the number of transmitted
waveforms K, whereas the bound for the best case is independent of K. The
actual 'clear region' of the TB-based MIMO radar AF depends on the array
configuration and is in between of the worst- and best-case bounds. We propose
a TB design strategy to reduce the levels of the AF sidelobes, and show in
simulations that proper design of the TB matrix leads to reduction of the
relative sidelobe levels of the TB-based MIMO radar AF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06618</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06618</id><created>2015-03-23</created><authors><author><keyname>Ukil</keyname><forenames>A.</forenames></author></authors><title>Practical Denoising of MEG Data using Wavelet Transform</title><categories>cs.OH</categories><comments>8 pages. arXiv admin note: text overlap with arXiv:1503.05821</comments><journal-ref>Lecture Notes in Computer Science, Springer, vol. 4233, pp.
  578-585, 2006</journal-ref><doi>10.1007/11893257_65</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetoencephalography (MEG) is an important noninvasive, nonhazardous
technology for functional brain mapping, measuring the magnetic fields due to
the intracellular neuronal current flow in the brain. However, the inherent
level of noise in the data collection process is large enough to obscure the
signal(s) of interest most often. In this paper, a practical denoising
technique based on the wavelet transform and the multiresolution signal
decomposition technique is presented. The proposed technique is substantiated
by the application results using three different mother wavelets on the
recorded MEG signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06619</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06619</id><created>2015-03-23</created><updated>2015-06-13</updated><authors><author><keyname>Zhu</keyname><forenames>Tingting</forenames></author><author><keyname>Dunkley</keyname><forenames>Nic</forenames></author><author><keyname>Behar</keyname><forenames>Joachim</forenames></author><author><keyname>Clifton</keyname><forenames>David A.</forenames></author><author><keyname>Clifford</keyname><forenames>Gari D.</forenames></author></authors><title>Fusing Continuous-valued Medical Labels using a Bayesian Model</title><categories>cs.LG</categories><doi>10.1007/s10439-015-1344-1</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the rapid increase in volume of time series medical data available
through wearable devices, there is a need to employ automated algorithms to
label data. Examples of labels include interventions, changes in activity (e.g.
sleep) and changes in physiology (e.g. arrhythmias). However, automated
algorithms tend to be unreliable resulting in lower quality care. Expert
annotations are scarce, expensive, and prone to significant inter- and
intra-observer variance. To address these problems, a Bayesian
Continuous-valued Label Aggregator(BCLA) is proposed to provide a reliable
estimation of label aggregation while accurately infer the precision and bias
of each algorithm. The BCLA was applied to QT interval (pro-arrhythmic
indicator) estimation from the electrocardiogram using labels from the 2006
PhysioNet/Computing in Cardiology Challenge database. It was compared to the
mean, median, and a previously proposed Expectation Maximization (EM) label
aggregation approaches. While accurately predicting each labelling algorithm's
bias and precision, the root-mean-square error of the BCLA was
11.78$\pm$0.63ms, significantly outperforming the best Challenge entry
(15.37$\pm$2.13ms) as well as the EM, mean, and median voting strategies
(14.76$\pm$0.52ms, 17.61$\pm$0.55ms, and 14.43$\pm$0.57ms respectively with
$p&lt;0.0001$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06629</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06629</id><created>2015-03-23</created><authors><author><keyname>Gadde</keyname><forenames>Akshay</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>A Probabilistic Interpretation of Sampling Theory of Graph Signals</title><categories>cs.LG</categories><comments>5 pages, 2 figures, To appear in International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a probabilistic interpretation of sampling theory of graph signals.
To do this, we first define a generative model for the data using a pairwise
Gaussian random field (GRF) which depends on the graph. We show that, under
certain conditions, reconstructing a graph signal from a subset of its samples
by least squares is equivalent to performing MAP inference on an approximation
of this GRF which has a low rank covariance matrix. We then show that a
sampling set of given size with the largest associated cut-off frequency, which
is optimal from a sampling theoretic point of view, minimizes the worst case
predictive covariance of the MAP estimate on the GRF. This interpretation also
gives an intuitive explanation for the superior performance of the sampling
theoretic approach to active semi-supervised classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06630</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06630</id><created>2015-03-23</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author></authors><title>Metadata in SDN API</title><categories>cs.NI cs.SE</categories><comments>submitted to EuCNC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the system aspects of development of applied programming
interfaces in Software-Defined Networking (SDN). Almost all existing SDN
interfaces use so-called Representational State Transfer (REST) services as a
basic model. This model is simple and straightforward for developers, but often
does not support the information (metadata) necessary for programming
automation. In this article, we cover the issues of representation of metadata
in the SDN API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06632</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06632</id><created>2015-03-23</created><authors><author><keyname>Teslenko</keyname><forenames>Maxim</forenames></author><author><keyname>Dubrova</keyname><forenames>Elena</forenames></author></authors><title>A Fast Heuristic Algorithm for Redundancy Removal</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Redundancy identification is an important step of the design flow that
typically follows logic synthesis and optimization. In addition to reducing
circuit area, power consumption, and delay, redundancy removal also improves
testability. All commercially available synthesis tools include a redundancy
removal engine which is often run multiple times on the same netlist during
optimization. This paper presents a fast heuristic algorithm for redundancy
removal in combinational circuits. Our idea is to provide a quick partial
solution which can be used for the intermediate redundancy removal runs instead
of exact ATPG or SAT-based approaches. The presented approach has a higher
implication power than the traditional heuristic algorithms, such as FIRE, e.g.
on average it removes 37% more redundancies than FIRE with no penalty in
runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06638</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06638</id><created>2015-03-23</created><authors><author><keyname>Liu</keyname><forenames>Yuansheng</forenames></author><author><keyname>Zhang</keyname><forenames>Leo Yu</forenames></author><author><keyname>Wang</keyname><forenames>Jia</forenames></author><author><keyname>Zhang</keyname><forenames>Yushu</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-wo</forenames></author></authors><title>Chosen-plaintext attack of an image encryption scheme based on modified
  permutation-diffusion structure</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the first appearance in Fridrich's design, the usage of
permutation-diffusion structure for designing digital image cryptosystem has
been receiving increasing research attention in the field of chaos-based
cryptography. Recently, a novel chaotic Image Cipher using one round Modified
Permutation-Diffusion pattern (ICMPD) was proposed. Unlike traditional
permutation-diffusion structure, the permutation is operated on bit level
instead of pixel level and the diffusion is operated on masked pixels, which
are obtained by carrying out the classical affine cipher, instead of plain
pixels in ICMPD. Following a \textit{divide-and-conquer strategy}, this paper
reports that ICMPD can be compromised by a chosen-plaintext attack efficiently
and the involved data complexity is linear to the size of the plain-image.
Moreover, the relationship between the cryptographic kernel at the diffusion
stage of ICMPD and modulo addition then XORing is explored thoroughly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06641</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06641</id><created>2015-03-23</created><updated>2015-03-29</updated><authors><author><keyname>Abbasi</keyname><forenames>Alireza</forenames></author><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author><author><keyname>Kapucu</keyname><forenames>Naim</forenames></author></authors><title>A longitudinal study of emerging networks during natural disaster</title><categories>cs.SI physics.soc-ph</categories><comments>30 pages, 2 figures, 11 tables, journal This paper has been withdrawn
  by the author due to a crucial sign error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present longitudinal analysis of the evolution of inter-organizational
disaster coordination networks during natural disasters. We suggest that social
networks are a useful paradigm for exploring this complex phenomenon from both
theoretical and methodological perspective aiming to develop a quantitative
assessment framework which could aid in developing a better understanding of
the optimal functioning of these emerging inter-organizational networks during
natural disasters. We highlight the importance of network metrics in order to
investigate disaster response coordination networks. Results suggest that in
disasters, rate of communication increases and creates the conditions where
organizational structures need to move at that same pace to exchange new
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06642</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06642</id><created>2015-03-23</created><authors><author><keyname>Wang</keyname><forenames>Junyan</forenames></author><author><keyname>Yeung</keyname><forenames>Sai-Kit</forenames></author></authors><title>Superpixelizing Binary MRF for Image Labeling Problems</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Superpixels have become prevalent in computer vision. They have been used to
achieve satisfactory performance at a significantly smaller computational cost
for various tasks. People have also combined superpixels with Markov random
field (MRF) models. However, it often takes additional effort to formulate MRF
on superpixel-level, and to the best of our knowledge there exists no
principled approach to obtain this formulation. In this paper, we show how
generic pixel-level binary MRF model can be solved in the superpixel space. As
the main contribution of this paper, we show that a superpixel-level MRF can be
derived from the pixel-level MRF by substituting the superpixel representation
of the pixelwise label into the original pixel-level MRF energy. The resultant
superpixel-level MRF energy also remains submodular for a submodular
pixel-level MRF. The derived formula hence gives us a handy way to formulate
MRF energy in superpixel-level. In the experiments, we demonstrate the efficacy
of our approach on several computer vision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06643</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06643</id><created>2015-03-23</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author></authors><title>A novel pLSA based Traffic Signs Classification System</title><categories>cs.CV</categories><comments>APMediaCast-2015, Bali, Indonesia</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we developed a novel and fast traffic sign recognition system, a
very important part for advanced driver assistance system and for autonomous
driving. Traffic signs play a very vital role in safe driving and avoiding
accident. We have used image processing and topic discovery model pLSA to
tackle this challenging multiclass classification problem. Our algorithm is
consist of two parts, shape classification and sign classification for improved
accuracy. For processing and representation of image we have used bag of
features model with SIFT local descriptor. Where a visual vocabulary of size
300 words are formed using k-means codebook formation algorithm. We exploited
the concept that every image is a collection of visual topics and images having
same topics will belong to same category. Our algorithm is tested on German
traffic sign recognition benchmark (GTSRB) and gives very promising result near
to existing state of the art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06647</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06647</id><created>2015-03-23</created><updated>2016-01-01</updated><authors><author><keyname>Shen</keyname><forenames>Lili</forenames></author><author><keyname>Tao</keyname><forenames>Yuanye</forenames></author><author><keyname>Zhang</keyname><forenames>Dexue</forenames></author></authors><title>Chu connections and back diagonals between $\mathcal{Q}$-distributors</title><categories>math.CT cs.LO</categories><comments>39 pages, final version. License updated</comments><msc-class>18D20, 18A40, 06B23</msc-class><journal-ref>Journal of Pure and Applied Algebra, 220(5):1858-1901, 2016</journal-ref><doi>10.1016/j.jpaa.2015.10.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chu connections and back diagonals are introduced as morphisms for
distributors between categories enriched in a small quantaloid $\mathcal{Q}$.
These notions, meaningful for closed bicategories, dualize the constructions of
arrow categories and the Freyd completion of categories. It is shown that, for
a small quantaloid $\mathcal{Q}$, the category of complete
$\mathcal{Q}$-categories and left adjoints is a retract of the dual of the
category of $\mathcal{Q}$-distributors and Chu connections, and it is dually
equivalent to the category of $\mathcal{Q}$-distributors and back diagonals. As
an application of Chu connections, a postulation of the intuitive idea of
reduction of formal contexts in the theory of formal concept analysis is
presented, and a characterization of reducts of formal contexts is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06648</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06648</id><created>2015-03-23</created><authors><author><keyname>Haloi</keyname><forenames>Mrinal</forenames></author><author><keyname>Jayagopi</keyname><forenames>Dinesh Babu</forenames></author></authors><title>Vehicle Local Position Estimation System</title><categories>cs.CV</categories><comments>Accepted in ICVES-2014, Hyderabad, India</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a robust vehicle local position estimation with the help of
single camera sensor and GPS is presented. A modified Inverse Perspective
Mapping, illuminant Invariant techniques and object detection based approach is
used to localize the vehicle in the road. Vehicles current lane, its position
from road boundary and other cars are used to define its local position. For
this purpose Lane markings are detected using a Laplacian edge feature, robust
to shadowing. Effect of shadowing and extra sun light are removed using Lab
color space and illuminant invariant techniques. Lanes are assumed to be as
parabolic model and fitted using robust RANSAC. This method can reliably detect
all lanes of the road, estimate lane departure angle and local position of
vehicle relative to lanes, road boundary and other cars. Different type of
obstacle like pedestrians, vehicles are detected using HOG feature based
deformable part model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06652</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06652</id><created>2015-03-23</created><authors><author><keyname>Abbasi</keyname><forenames>Alireza</forenames></author><author><keyname>Hossain</keyname><forenames>Liaquat</forenames></author><author><keyname>Wigand</keyname><forenames>Rolf T</forenames></author></authors><title>Evolutionary Dynamics of Complex Networks: Theory, Methods and
  Applications</title><categories>cs.SI physics.soc-ph</categories><comments>24 pages, 7 figures, 2 tables, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new direction to understanding evolutionary dynamics of complex
networks using two different types of collaboration networks: academic
collaboration networks; and, disaster collaboration networks. The results show
that academic collaboration network has all properties of small-world networks
and can be considered a real small-world network with the result of its
structural analysis maybe being extendable for other real-networks who share
the common grounds of small-world properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06665</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06665</id><created>2015-03-23</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Frederiksen</keyname><forenames>S&#xf8;ren Kristoffer Stiil</forenames></author></authors><title>The Adjusted Winner Procedure: Characterizations and Equilibria</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Adjusted Winner procedure is an important mechanism proposed by Brams and
Taylor for fairly allocating goods between two agents. It has been used in
practice for divorce settlements and analyzing political disputes. Assuming
truthful declaration of the valuations, it computes an allocation that is
envy-free, equitable and Pareto optimal. We show that Adjusted Winner admits
several elegant characterizations, which further shed light on the outcomes
reached with strategic agents. We find that the procedure may not admit pure
Nash equilibria in either the discrete or continuous variants, but is
guaranteed to have $\epsilon$-Nash equilibria for each $\epsilon$ &gt; 0.
Moreover, under informed tie-breaking, exact pure Nash equilibria always exist,
are Pareto optimal, and their social welfare is at least 3/4 of the optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06666</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06666</id><created>2015-03-23</created><updated>2015-12-03</updated><authors><author><keyname>Raposo</keyname><forenames>Francisco</forenames></author><author><keyname>Ribeiro</keyname><forenames>Ricardo</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author></authors><title>Using Generic Summarization to Improve Music Information Retrieval Tasks</title><categories>cs.IR cs.LG cs.SD</categories><comments>24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio,
  Speech and Language Processing</comments><acm-class>H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to satisfy processing time constraints, many MIR tasks process only
a segment of the whole music signal. This practice may lead to decreasing
performance, since the most important information for the tasks may not be in
those processed segments. In this paper, we leverage generic summarization
algorithms, previously applied to text and speech summarization, to summarize
items in music datasets. These algorithms build summaries, that are both
concise and diverse, by selecting appropriate segments from the input signal
which makes them good candidates to summarize music as well. We evaluate the
summarization process on binary and multiclass music genre classification
tasks, by comparing the performance obtained using summarized datasets against
the performances obtained using continuous segments (which is the traditional
method used for addressing the previously mentioned time constraints) and full
songs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,
MMR, and a Support Sets-based Centrality model improve classification
performance when compared to selected 30-second baselines. We also show that
summarized datasets lead to a classification performance whose difference is
not statistically significant from using full songs. Furthermore, we make an
argument stating the advantages of sharing summarized datasets for future MIR
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06675</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06675</id><created>2015-02-26</created><updated>2015-08-31</updated><authors><author><keyname>Singh</keyname><forenames>Pushpendra</forenames></author><author><keyname>Joshi</keyname><forenames>Shiv Dutt</forenames></author><author><keyname>Patney</keyname><forenames>Rakesh Kumar</forenames></author><author><keyname>Saha</keyname><forenames>Kaushik</forenames></author></authors><title>The Fourier Decomposition Method for nonlinear and nonstationary time
  series analysis</title><categories>stat.ME cs.IT math.IT math.NA</categories><comments>14 Pages, 18 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since many decades, there is a general perception in literature that the
Fourier methods are not suitable for the analysis of nonlinear and
nonstationary data. In this paper, we propose a Fourier Decomposition Method
(FDM) and demonstrate its efficacy for the analysis of nonlinear (i.e. data
generated by nonlinear systems) and nonstationary time series. The proposed FDM
decomposes any data into a small number of `Fourier intrinsic band functions'
(FIBFs). The FDM presents a generalized Fourier expansion with variable
amplitudes and frequencies of a time series by the Fourier method itself. We
propose an idea of zero-phase filter bank based multivariate FDM (MFDM)
algorithm, for the analysis of multivariate nonlinear and nonstationary time
series, from the FDM. We also present an algorithm to obtain cutoff frequencies
for MFDM. The MFDM algorithm is generating finite number of band limited
multivariate FIBFs (MFIBFs). The MFDM preserves some intrinsic physical
properties of the multivariate data, such as scale alignment, trend and
instantaneous frequency. The proposed methods produce the results in a
time-frequency-energy distribution that reveal the intrinsic structures of a
data. Simulations have been carried out and comparison is made with the
Empirical Mode Decomposition (EMD) methods in the analysis of various simulated
as well as real life time series, and results show that the proposed methods
are powerful tools for analyzing and obtaining the time-frequency-energy
representation of any data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06679</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06679</id><created>2015-03-23</created><updated>2015-03-25</updated><authors><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author><author><keyname>Kim</keyname><forenames>Jong Min</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Improving M-SBL for Joint Sparse Recovery using a Subspace Penalty</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2477049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiple measurement vector problem (MMV) is a generalization of the
compressed sensing problem that addresses the recovery of a set of jointly
sparse signal vectors. One of the important contributions of this paper is to
reveal that the seemingly least related state-of-art MMV joint sparse recovery
algorithms - M-SBL (multiple sparse Bayesian learning) and subspace-based
hybrid greedy algorithms - have a very important link. More specifically, we
show that replacing the $\log\det(\cdot)$ term in M-SBL by a rank proxy that
exploits the spark reduction property discovered in subspace-based joint sparse
recovery algorithms, provides significant improvements. In particular, if we
use the Schatten-$p$ quasi-norm as the corresponding rank proxy, the global
minimiser of the proposed algorithm becomes identical to the true solution as
$p \rightarrow 0$. Furthermore, under the same regularity conditions, we show
that the convergence to a local minimiser is guaranteed using an alternating
minimization algorithm that has closed form expressions for each of the
minimization steps, which are convex. Numerical simulations under a variety of
scenarios in terms of SNR, and condition number of the signal amplitude matrix
demonstrate that the proposed algorithm consistently outperforms M-SBL and
other state-of-the art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06680</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06680</id><created>2015-01-29</created><updated>2015-05-24</updated><authors><author><keyname>Larkin</keyname><forenames>Kieran Gerard</forenames></author></authors><title>Structural Similarity Index SSIMplified: Is there really a simpler
  concept at the heart of image quality measurement?</title><categories>cs.CV</categories><comments>Updated abstract and references. 4 pages total, main analysis 2
  pages, notes and minimal references 1 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Structural Similarity Index (SSIM) is generally considered to be a
milestone in the recent history of Image Quality Assessment (IQA). Alas, SSIM's
accepted development from the product of three heuristic factors continues to
obscure it's real underlying simplicity. Starting instead from a
symmetric-antisymmetric reformulation we first show SSIM to be a contrast or
visibility function in the classic sense. Furthermore, the previously enigmatic
structural covariance is revealed to be the difference of variances. The second
step, eliminating the intrinsic quadratic nature of SSIM, allows a near linear
correlation with human observer scores, and without invoking the usual, but
arbitrary, sigmoid model fitting. We conclude that SSIM can be re-interpreted
in terms of perceptual masking: it is essentially equivalent to a normalised
error or noise visibility function (NVF), and, furthermore, the NVF alone
explains it success in modelling perceptual image quality. We use the term
Dissimilarity Quotient (DQ) for the specifically anti/symmetric SSIM derived
NVF. It seems that IQA researchers may now have two choices: 1) Continue to use
the complex SSIM formula, but noting that SSIM only works coincidentally since
the covariance term is actually the mean square error (MSE) in disguise. 2) Use
the simplest of all perceptually-masked image quality metrics, namely NVF or
DQ. On this choice Occam is clear: in the absence of differences in predictive
ability, the fewer assumptions that are made, the better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06683</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06683</id><created>2015-03-23</created><authors><author><keyname>Carevic</keyname><forenames>Zeljko</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Extending search facilities via bibliometric-enhanced stratagems</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces simple bibliometric-enhanced search facilities which are
derived from the famous stratagems by Bates. Moves, tactics and stratagems are
revisited from a Digital Library perspective. The potential of extended
versions of &quot;journal run&quot; or &quot;citation search&quot; for interactive information
retrieval is outlined. The authors elaborate on the future implementation and
evaluation of new bibliometric-enhanced search services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06687</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06687</id><created>2015-03-23</created><updated>2015-06-18</updated><authors><author><keyname>Marshall</keyname><forenames>Andrew M</forenames><affiliation>The University of Mary Washington</affiliation></author><author><keyname>Meadows</keyname><forenames>Catherine</forenames><affiliation>U.S. Naval Research Laboratory</affiliation></author><author><keyname>Narendran</keyname><forenames>Paliath</forenames><affiliation>University at Albany, SUNY</affiliation></author></authors><title>On Unification Modulo One-Sided Distributivity: Algorithms, Variants and
  Asymmetry</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:11) 2015</journal-ref><doi>10.2168/LMCS-11(2:11)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for unification modulo one-sided distributivity is an early
result by Tid\'en and Arnborg. More recently this theory has been of interest
in cryptographic protocol analysis due to the fact that many cryptographic
operators satisfy this property. Unfortunately the algorithm presented in the
paper, although correct, has recently been shown not to be polynomial time
bounded as claimed. In addition, for some instances, there exist most general
unifiers that are exponentially large with respect to the input size. In this
paper we first present a new polynomial time algorithm that solves the decision
problem for a non-trivial subcase, based on a typed theory, of unification
modulo one-sided distributivity. Next we present a new polynomial algorithm
that solves the decision problem for unification modulo one-sided
distributivity. A construction, employing string compression, is used to
achieve the polynomial bound. Lastly, we examine the one-sided distributivity
problem in the new asymmetric unification paradigm. We give the first
asymmetric unification algorithm for one-sided distributivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06692</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06692</id><created>2015-03-23</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>Toni</forenames></author><author><keyname>Fern&#xe1;ndez-Gracia</keyname><forenames>Juan</forenames></author><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author><author><keyname>Egu&#xed;luz</keyname><forenames>V&#xed;ctor M.</forenames></author></authors><title>Persistence in voting behavior: stronghold dynamics in elections</title><categories>physics.soc-ph cs.SI</categories><comments>Lecture Notes in Computer Science: Social Computing,
  Behavioral-Cultural Modeling, and Prediction (2015)</comments><doi>10.1007/978-3-319-16268-3_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence among individuals is at the core of collective social phenomena
such as the dissemination of ideas, beliefs or behaviors, social learning and
the diffusion of innovations. Different mechanisms have been proposed to
implement inter-agent influence in social models from the voter model, to
majority rules, to the Granoveter model. Here we advance in this direction by
confronting the recently introduced Social Influence and Recurrent Mobility
(SIRM) model, that reproduces generic features of vote-shares at different
geographical levels, with data in the US presidential elections. Our approach
incorporates spatial and population diversity as inputs for the opinion
dynamics while individuals' mobility provides a proxy for social context, and
peer imitation accounts for social influence. The model captures the observed
stationary background fluctuations in the vote-shares across counties. We study
the so-called political strongholds, i.e., locations where the votes-shares for
a party are systematically higher than average. A quantitative definition of a
stronghold by means of persistence in time of fluctuations in the voting
spatial distribution is introduced, and results from the US Presidential
Elections during the period 1980-2012 are analyzed within this framework. We
compare electoral results with simulations obtained with the SIRM model finding
a good agreement both in terms of the number and the location of strongholds.
The strongholds duration is also systematically characterized in the SIRM
model. The results compare well with the electoral results data revealing an
exponential decay in the persistence of the strongholds with time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06699</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06699</id><created>2015-03-23</created><updated>2015-04-09</updated><authors><author><keyname>Zhang</keyname><forenames>Zhengwu</forenames></author><author><keyname>Su</keyname><forenames>Jingyong</forenames></author><author><keyname>Klassen</keyname><forenames>Eric</forenames></author><author><keyname>Le</keyname><forenames>Huiling</forenames></author><author><keyname>Srivastava</keyname><forenames>Anuj</forenames></author></authors><title>Video-Based Action Recognition Using Rate-Invariant Analysis of
  Covariance Trajectories</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical classification of actions in videos is mostly performed by
extracting relevant features, particularly covariance features, from image
frames and studying time series associated with temporal evolutions of these
features. A natural mathematical representation of activity videos is in form
of parameterized trajectories on the covariance manifold, i.e. the set of
symmetric, positive-definite matrices (SPDMs). The variable execution-rates of
actions implies variable parameterizations of the resulting trajectories, and
complicates their classification. Since action classes are invariant to
execution rates, one requires rate-invariant metrics for comparing
trajectories. A recent paper represented trajectories using their transported
square-root vector fields (TSRVFs), defined by parallel translating
scaled-velocity vectors of trajectories to a reference tangent space on the
manifold. To avoid arbitrariness of selecting the reference and to reduce
distortion introduced during this mapping, we develop a purely intrinsic
approach where SPDM trajectories are represented by redefining their TSRVFs at
the starting points of the trajectories, and analyzed as elements of a vector
bundle on the manifold. Using a natural Riemannain metric on vector bundles of
SPDMs, we compute geodesic paths and geodesic distances between trajectories in
the quotient space of this vector bundle, with respect to the
re-parameterization group. This makes the resulting comparison of trajectories
invariant to their re-parameterization. We demonstrate this framework on two
applications involving video classification: visual speech recognition or
lip-reading and hand-gesture recognition. In both cases we achieve results
either comparable to or better than the current literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06702</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06702</id><created>2015-03-23</created><authors><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Rybicki</keyname><forenames>Joel</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Towards Optimal Synchronous Counting</title><categories>cs.DC</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a complete communication network of $n$ nodes, where the nodes
receive a common clock pulse. We study the synchronous $c$-counting problem:
given any starting state and up to $f$ faulty nodes with arbitrary behaviour,
the task is to eventually have all correct nodes counting modulo $c$ in
agreement. Thus, we are considering algorithms that are self-stabilizing
despite Byzantine failures. In this work, we give new algorithms for the
synchronous counting problem that (1) are deterministic, (2) have linear
stabilisation time in $f$, (3) use a small number of states, and (4) achieve
almost-optimal resilience. Prior algorithms either resort to randomisation, use
a large number of states, or have poor resilience. In particular, we achieve an
exponential improvement in the space complexity of deterministic algorithms,
while still achieving linear stabilisation time and almost-linear resilience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06714</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06714</id><created>2015-03-23</created><authors><author><keyname>Wu</keyname><forenames>Junfeng</forenames></author><author><keyname>Meng</keyname><forenames>Ziyang</forenames></author><author><keyname>Yang</keyname><forenames>Tao</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Sampled-Data Consensus over Random Networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the consensus problem for a network of nodes with random
interactions and sampled-data control actions. We first show that consensus in
expectation, in mean square, and almost surely are equivalent for a general
random network model when the inter-sampling interval and network size satisfy
a simple relation. The three types of consensus are shown to be simultaneously
achieved over an independent or a Markovian random network defined on an
underlying graph with a directed spanning tree. For both independent and
Markovian random network models, necessary and sufficient conditions for
mean-square consensus are derived in terms of the spectral radius of the
corresponding state transition matrix. These conditions are then interpreted as
the existence of critical value on the inter-sampling interval, below which
global mean-square consensus is achieved and above which the system diverges in
mean-square sense for some initial states. Finally, we establish an upper bound
on the inter-sampling interval below which almost sure consensus is reached,
and a lower bound on the inter-sampling interval above which almost sure
divergence is reached. Some numerical simulations are given to validate the
theoretical results and some discussions on the critical value of the
inter-sampling intervals for the mean-square consensus are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06725</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06725</id><created>2015-03-23</created><updated>2015-06-30</updated><authors><author><keyname>Bassler</keyname><forenames>Kevin E.</forenames></author><author><keyname>Del Genio</keyname><forenames>Charo I.</forenames></author><author><keyname>Erd&#x151;s</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>Exact sampling of graphs with prescribed degree correlations</title><categories>cs.DM cond-mat.stat-mech cs.DS math.CO physics.soc-ph</categories><comments>25 pages, 7 figures</comments><doi>10.1088/1367-2630/17/8/083052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world networks exhibit correlations between the node degrees. For
instance, in social networks nodes tend to connect to nodes of similar degree.
Conversely, in biological and technological networks, high-degree nodes tend to
be linked with low-degree nodes. Degree correlations also affect the dynamics
of processes supported by a network structure, such as the spread of opinions
or epidemics. The proper modelling of these systems, i.e., without uncontrolled
biases, requires the sampling of networks with a specified set of constraints.
We present a solution to the sampling problem when the constraints imposed are
the degree correlations. In particular, we develop an efficient and exact
method to construct and sample graphs with a specified joint-degree matrix,
which is a matrix providing the number of edges between all the sets of nodes
of a given degree, for all degrees, thus completely specifying all pairwise
degree correlations, and additionally, the degree sequence itself. Our
algorithm always produces independent samples without backtracking. The
complexity of the graph construction algorithm is O(NM) where N is the number
of nodes and M is the number of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06729</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06729</id><created>2015-03-20</created><authors><author><keyname>Bouhaya</keyname><forenames>Lina</forenames><affiliation>NAVIER UMR 8205</affiliation></author><author><keyname>Baverel</keyname><forenames>Olivier</forenames><affiliation>NAVIER UMR 8205</affiliation></author><author><keyname>Caron</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>NAVIER UMR 8205</affiliation></author></authors><title>Optimization of gridshell bar orientation using a simplified genetic
  approach</title><categories>cs.OH</categories><proxy>ccsd</proxy><journal-ref>Structural and Multidisciplinary Optimization, Springer Verlag
  (Germany), 2014, 50 (5), pp. 839-848</journal-ref><doi>10.1007/s00158-014-1088-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gridshells are defined as structures that have the shape and rigidity of a
double curvature shell but consist of a grid instead of a continuous surface.
This study concerns those obtained by elastic deformation of an initially flat
two-way grid. This paper presents a novel approach to generate gridshells on an
imposed shape under imposed boundary conditions. A numerical tool based on a
geometrical method, the compass method, is developed. It is coupled with
genetic algorithms to optimize the orientation of gridshell bars in order to
minimize the stresses and therefore to avoid bar breakage during the
construction phase. Examples of application are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06733</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06733</id><created>2015-03-23</created><updated>2015-03-24</updated><authors><author><keyname>Rasooli</keyname><forenames>Mohammad Sadegh</forenames></author><author><keyname>Tetreault</keyname><forenames>Joel</forenames></author></authors><title>Yara Parser: A Fast and Accurate Dependency Parser</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependency parsers are among the most crucial tools in natural language
processing as they have many important applications in downstream tasks such as
information retrieval, machine translation and knowledge acquisition. We
introduce the Yara Parser, a fast and accurate open-source dependency parser
based on the arc-eager algorithm and beam search. It achieves an unlabeled
accuracy of 93.32 on the standard WSJ test set which ranks it among the top
dependency parsers. At its fastest, Yara can parse about 4000 sentences per
second when in greedy mode (1 beam). When optimizing for accuracy (using 64
beams and Brown cluster features), Yara can parse 45 sentences per second. The
parser can be trained on any syntactic dependency treebank and different
options are provided in order to make it more flexible and tunable for specific
tasks. It is released with the Apache version 2.0 license and can be used for
both commercial and academic purposes. The parser can be found at
https://github.com/yahoo/YaraParser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06745</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06745</id><created>2015-03-23</created><authors><author><keyname>Zhang</keyname><forenames>Junlin</forenames></author><author><keyname>Garcia</keyname><forenames>Jose</forenames></author></authors><title>Online classifier adaptation for cost-sensitive learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the problem of online cost-sensitive clas- sifier
adaptation and the first algorithm to solve it. We assume we have a base
classifier for a cost-sensitive classification problem, but it is trained with
respect to a cost setting different to the desired one. Moreover, we also have
some training data samples streaming to the algorithm one by one. The prob- lem
is to adapt the given base classifier to the desired cost setting using the
steaming training samples online. To solve this problem, we propose to learn a
new classifier by adding an adaptation function to the base classifier, and
update the adaptation function parameter according to the streaming data
samples. Given a input data sample and the cost of misclassifying it, we up-
date the adaptation function parameter by minimizing cost weighted hinge loss
and respecting previous learned parameter simultaneously. The proposed
algorithm is compared to both online and off-line cost-sensitive algorithms on
two cost-sensitive classification problems, and the experiments show that it
not only outperforms them one classification performances, but also requires
significantly less running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06746</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06746</id><created>2015-03-23</created><authors><author><keyname>Boccardi</keyname><forenames>Federico</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey</forenames></author><author><keyname>Elshaer</keyname><forenames>Hisham</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Parkvall</keyname><forenames>Stefan</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author></authors><title>Why to Decouple the Uplink and Downlink in Cellular Networks and How To
  Do It</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ever since the inception of mobile telephony, the downlink and uplink of
cellular networks have been coupled, i.e. mobile terminals have been
constrained to associate with the same base station (BS) in both the downlink
and uplink directions. New trends in network densification and mobile data
usage increase the drawbacks of this constraint, and suggest that it should be
revisited. In this paper we identify and explain five key arguments in favor of
Downlink/Uplink Decoupling (DUDe) based on a blend of theoretical,
experimental, and logical arguments. We then overview the changes needed in
current (LTE-A) mobile systems to enable this decoupling, and then look ahead
to fifth generation (5G) cellular standards. We believe the introduced paradigm
will lead to significant gains in network throughput, outage and power
consumption at a much lower cost compared to other solutions providing
comparable or lower gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06751</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06751</id><created>2015-03-23</created><updated>2015-08-14</updated><authors><author><keyname>Jakubisin</keyname><forenames>Daniel J.</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>Approximate Joint MAP Detection of Co-Channel Signals</title><categories>cs.IT math.IT</categories><comments>Proc. 2015 IEEE MILCOM, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider joint detection of co-channel signals---specifically, signals
which do not possess a natural separability due to, for example, the multiple
access technique or the use of multiple antennas. Iterative joint detection and
decoding is a well known approach for utilizing the error correction code to
improve detection performance. However, the joint maximum a posteriori
probability (MAP) detector may be prohibitively complex, especially in a
multipath channel. In this paper, we present an approximation to the joint MAP
detector motivated by a factor graph model of the received signal. The proposed
algorithm is designed to approximate the joint MAP detector as closely as
possible within the computational capability of the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06760</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06760</id><created>2015-03-23</created><authors><author><keyname>Lin</keyname><forenames>Chu-Cheng</forenames></author><author><keyname>Ammar</keyname><forenames>Waleed</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Levin</keyname><forenames>Lori</forenames></author></authors><title>Unsupervised POS Induction with Word Embeddings</title><categories>cs.CL</categories><comments>NAACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised word embeddings have been shown to be valuable as features in
supervised learning problems; however, their role in unsupervised problems has
been less thoroughly explored. In this paper, we show that embeddings can
likewise add value to the problem of unsupervised POS induction. In two
representative models of POS induction, we replace multinomial distributions
over the vocabulary with multivariate Gaussian distributions over word
embeddings and observe consistent improvements in eight languages. We also
analyze the effect of various choices while inducing word embeddings on
&quot;downstream&quot; POS induction results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06772</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06772</id><created>2015-03-23</created><updated>2015-05-31</updated><authors><author><keyname>Jacobs</keyname><forenames>Abigail Z.</forenames></author><author><keyname>Way</keyname><forenames>Samuel F.</forenames></author><author><keyname>Ugander</keyname><forenames>Johan</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Assembling thefacebook: Using heterogeneity to understand online social
  network assembly</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 11 figures, Proceedings of the 7th Annual ACM Web Science
  Conference (WebSci), 2015</comments><doi>10.1145/2786451.2786477</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks represent a popular and diverse class of social media
systems. Despite this variety, each of these systems undergoes a general
process of online social network assembly, which represents the complicated and
heterogeneous changes that transform newly born systems into mature platforms.
However, little is known about this process. For example, how much of a
network's assembly is driven by simple growth? How does a network's structure
change as it matures? How does network structure vary with adoption rates and
user heterogeneity, and do these properties play different roles at different
points in the assembly? We investigate these and other questions using a unique
dataset of online connections among the roughly one million users at the first
100 colleges admitted to Facebook, captured just 20 months after its launch. We
first show that different vintages and adoption rates across this population of
networks reveal temporal dynamics of the assembly process, and that assembly is
only loosely related to network growth. We then exploit natural experiments
embedded in this dataset and complementary data obtained via Internet
archaeology to show that different subnetworks matured at different rates
toward similar end states. These results shed light on the processes and
patterns of online social network assembly, and may facilitate more effective
design for online social systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06775</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06775</id><created>2015-03-23</created><authors><author><keyname>Amelard</keyname><forenames>Robert</forenames></author><author><keyname>Scharfenberger</keyname><forenames>Christian</forenames></author><author><keyname>Kazemzadeh</keyname><forenames>Farnoud</forenames></author><author><keyname>Pfisterer</keyname><forenames>Kaylen J.</forenames></author><author><keyname>Lin</keyname><forenames>Bill S.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author><author><keyname>Clausi</keyname><forenames>David A.</forenames></author></authors><title>Non-contact transmittance photoplethysmographic imaging (PPGI) for
  long-distance cardiovascular monitoring</title><categories>physics.optics cs.CV</categories><comments>13 pages, 6 figures, submitted to Nature Scientific Reports, for
  associated video files see
  http://vip.uwaterloo.ca/publications/non-contact-transmittance-photoplethysmographic-imaging-ppgi-long-distance</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photoplethysmography (PPG) devices are widely used for monitoring
cardiovascular function. However, these devices require skin contact, which
restrict their use to at-rest short-term monitoring using single-point
measurements. Photoplethysmographic imaging (PPGI) has been recently proposed
as a non-contact monitoring alternative by measuring blood pulse signals across
a spatial region of interest. Existing systems operate in reflectance mode, of
which many are limited to short-distance monitoring and are prone to temporal
changes in ambient illumination. This paper is the first study to investigate
the feasibility of long-distance non-contact cardiovascular monitoring at the
supermeter level using transmittance PPGI. For this purpose, a novel PPGI
system was designed at the hardware and software level using ambient correction
via temporally coded illumination (TCI) and signal processing for PPGI signal
extraction. Experimental results show that the processing steps yield a
substantially more pulsatile PPGI signal than the raw acquired signal,
resulting in statistically significant increases in correlation to ground-truth
PPG in both short- ($p \in [&lt;0.0001, 0.040]$) and long-distance ($p \in
[&lt;0.0001, 0.056]$) monitoring. The results support the hypothesis that
long-distance heart rate monitoring is feasible using transmittance PPGI,
allowing for new possibilities of monitoring cardiovascular function in a
non-contact manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06776</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06776</id><created>2015-01-20</created><authors><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Walking through a library remotely - Why we need maps for collections
  and how KnoweScape can help us to make them?</title><categories>cs.DL physics.soc-ph</categories><comments>preprint with figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is no escape from the expansion of information, so that structuring and
locating meaningful knowledge becomes ever more difficult. The question of how
to order our knowledge is as old as the systematic acquisition, circulation,
and storage of knowledge. Classification systems have been known since ancient
times. On the Internet, one finds both classifications and taxonomies designed
by information professionals and folksonomies based on social tagging.
Nevertheless, a user navigating through large information spaces is still
confronted with a text based search interface and a list of hits as outcome.
There is still an obvious gap between a physical encounter with, for example, a
librarys collection and browsing its content through an on-line catalogue. This
paper starts from the need of digital scholarship for effective knowledge
inquiry, revisits traditional ways to support knowledge ordering and
information retrieval, and introduces into a newly funded research network
where five different communities from all corners of the scientific landscape
join forces in a quest for knowledge maps. It can be read as a manifesto for a
newly funded specific research network KnoweScape. At the same time it is a
general reflection about what one has to take into account when representing
structure and evolution of data, information and knowledge and designing
instruments to help scholars and others to navigate across the lands and oceans
of knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06782</identifier>
 <datestamp>2015-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06782</id><created>2015-03-23</created><authors><author><keyname>Zhang</keyname><forenames>Changchun</forenames></author><author><keyname>Qiu</keyname><forenames>Robert Caiming</forenames></author></authors><title>Massive MIMO as a Big Data System: Random Matrix Models and Testbed</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1402.6419 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper has two parts. The first one deals with how to use large random
matrices as building blocks to model the massive data arising from the massive
(or large-scale) MIMO system. As a result, we apply this model for distributed
spectrum sensing and network monitoring. The part boils down to the streaming,
distributed massive data, for which a new algorithm is obtained and its
performance is derived using the central limit theorem that is recently
obtained in the literature. The second part deals with the large-scale testbed
using software-defined radios (particularly USRP) that takes us more than four
years to develop this 70-node network testbed. To demonstrate the power of the
software defined radio, we reconfigure our testbed quickly into a testbed for
massive MIMO. The massive data of this testbed is of central interest in this
paper. It is for the first time for us to model the experimental data arising
from this testbed. To our best knowledge, we are not aware of other similar
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06809</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06809</id><created>2015-03-23</created><updated>2015-07-20</updated><authors><author><keyname>Christakou</keyname><forenames>Konstantina</forenames></author><author><keyname>Tomozei</keyname><forenames>Dan-Cristian</forenames></author><author><keyname>Boudec</keyname><forenames>Jean-Yves Le</forenames></author><author><keyname>Paolone</keyname><forenames>Mario</forenames></author></authors><title>AC OPF in Radial Distribution Networks - Parts I,II</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal power-flow problem (OPF) has played a key role in the planning
and operation of power systems. Due to the non-linear nature of the AC
power-flow equations, the OPF problem is known to be non-convex, therefore hard
to solve. Most proposed methods for solving the OPF rely on approximations that
render the problem convex, but that may yield inexact solutions. Recently,
Farivar and Low proposed a method that is claimed to be exact for radial
distribution systems, despite no apparent approximations. In our work, we show
that it is, in fact, not exact. On one hand, there is a misinterpretation of
the physical network model related to the ampacity constraint of the lines'
current flows. On the other hand, the proof of the exactness of the proposed
relaxation requires unrealistic assumptions related to the unboundedness of
specific control variables. We also show that the extension of this approach to
account for exact line models might provide physically infeasible solutions.
Recently, several contributions have proposed OPF algorithms that rely on the
use of the alternating-direction method of multipliers (ADMM). However, as we
show in this work, there are cases for which the ADMM-based solution of the
non-relaxed OPF problem fails to converge. To overcome the aforementioned
limitations, we propose an algorithm for the solution of a non-approximated,
non-convex OPF problem in radial distribution systems that is based on the
method of multipliers, and on a primal decomposition of the OPF. This work is
divided in two parts. In Part I, we specifically discuss the limitations of BFM
and ADMM to solve the OPF problem. In Part II, we provide a centralized version
and a distributed asynchronous version of the proposed OPF algorithm and we
evaluate its performances using both small-scale electrical networks, as well
as a modified IEEE 13-node test feeder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06813</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06813</id><created>2015-03-23</created><updated>2015-04-12</updated><authors><author><keyname>Zhang</keyname><forenames>Haopeng</forenames></author><author><keyname>El-Gaaly</keyname><forenames>Tarek</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Jiang</keyname><forenames>Zhiguo</forenames></author></authors><title>Factorization of View-Object Manifolds for Joint Object Recognition and
  Pose Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to large variations in shape, appearance, and viewing conditions, object
recognition is a key precursory challenge in the fields of object manipulation
and robotic/AI visual reasoning in general. Recognizing object categories,
particular instances of objects and viewpoints/poses of objects are three
critical subproblems robots must solve in order to accurately grasp/manipulate
objects and reason about their environments. Multi-view images of the same
object lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g.
visual/depth descriptor spaces). These object manifolds share the same topology
despite being geometrically different. Each object manifold can be represented
as a deformed version of a unified manifold. The object manifolds can thus be
parameterized by its homeomorphic mapping/reconstruction from the unified
manifold. In this work, we develop a novel framework to jointly solve the three
challenging recognition sub-problems, by explicitly modeling the deformations
of object manifolds and factorizing it in a view-invariant space for
recognition. We perform extensive experiments on several challenging datasets
and achieve state-of-the-art results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06819</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06819</id><created>2015-03-23</created><authors><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author><author><keyname>Liu</keyname><forenames>Benyuan</forenames></author><author><keyname>Susanto</keyname><forenames>Hengky</forenames></author><author><keyname>Xue</keyname><forenames>Guoliang</forenames></author></authors><title>Auction-based Incentive Mechanisms for Dynamic Mobile Ad-Hoc Crowd
  Service</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a type of emerging user-assisted mobile applications or
services, referred to as Dynamic Mobile Ad-hoc Crowd Service (DMACS), such as
collaborative streaming via smartphones or location privacy protection through
a crowd of smartphone users. Such services are provided and consumed by users
carrying smart mobile devices (e.g., smartphones) who are in close proximity of
each other (e.g., within Bluetooth range). Users in a DMACS system dynamically
arrive and depart over time, and are divided into multiple possibly overlapping
groups according to radio range constraints. Crucial to the success of such
systems is a mechanism that incentivizes users' participation and ensures fair
trading. In this paper, we design a multi-market, dynamic double auction
mechanism, referred to as M-CHAIN, and show that it is truthful, feasible,
individual-rational, no-deficit, and computationally efficient. The novelty and
significance of M-CHAIN is that it addresses and solves the fair trading
problem in a multi-group or multi-market dynamic double auction problem which
naturally occurs in a mobile wireless environment. We demonstrate its
efficiency via simulations based on generated user patterns (stochastic
arrivals, random market clustering of users) and real-world traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06822</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06822</id><created>2015-03-23</created><updated>2015-10-12</updated><authors><author><keyname>Papoutsakis</keyname><forenames>Ioannis</forenames></author></authors><title>Tree spanners of bounded degree graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tree $t$-spanner of a graph $G$ is a spanning tree of $G$ such that the
distance between pairs of vertices in the tree is at most $t$ times their
distance in $G$. Deciding tree $t$-spanner admissible graphs has been proved to
be tractable for $t&lt;3$ and NP-complete for $t&gt;3$, while the complexity status
of this problem is unresolved when $t=3$. For every $t&gt;2$ and $b&gt;0$, an
efficient dynamic programming algorithm to decide tree $t$-spanner
admissibility of graphs with vertex degrees less than $b$ is presented. Only
for $t=3$, the algorithm remains efficient, when graphs $G$ with degrees less
than $b\log |V(G)|$ are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06826</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06826</id><created>2015-03-23</created><updated>2015-06-18</updated><authors><author><keyname>Bouyer</keyname><forenames>Patricia</forenames><affiliation>LSV -- ENS Cachan &amp; CNRS</affiliation></author><author><keyname>Brenguier</keyname><forenames>Romain</forenames><affiliation>LSV -- ENS Cachan &amp; CNRS</affiliation></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames><affiliation>LSV -- ENS Cachan &amp; CNRS</affiliation></author><author><keyname>Ummels</keyname><forenames>Michael</forenames><affiliation>TU Dresden</affiliation></author></authors><title>Pure Nash Equilibria in Concurrent Deterministic Games</title><categories>cs.LO cs.GT</categories><comments>72 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:9) 2015</journal-ref><doi>10.2168/LMCS-11(2:9)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study pure-strategy Nash equilibria in multi-player concurrent
deterministic games, for a variety of preference relations. We provide a novel
construction, called the suspect game, which transforms a multi-player
concurrent game into a two-player turn-based game which turns Nash equilibria
into winning strategies (for some objective that depends on the preference
relations of the players in the original game). We use that transformation to
design algorithms for computing Nash equilibria in finite games, which in most
cases have optimal worst-case complexity, for large classes of preference
relations. This includes the purely qualitative framework, where each player
has a single omega-regular objective that she wants to satisfy, but also the
larger class of semi-quantitative objectives, where each player has several
omega-regular objectives equipped with a preorder (for instance, a player may
want to satisfy all her objectives, or to maximise the number of objectives
that she achieves.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06833</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06833</id><created>2015-03-23</created><authors><author><keyname>Arjevani</keyname><forenames>Yossi</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>On Lower and Upper Bounds for Smooth and Strongly Convex Optimization
  Problems</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel framework to study smooth and strongly convex optimization
algorithms, both deterministic and stochastic. Focusing on quadratic functions
we are able to examine optimization algorithms as a recursive application of
linear operators. This, in turn, reveals a powerful connection between a class
of optimization algorithms and the analytic theory of polynomials whereby new
lower and upper bounds are derived. Whereas existing lower bounds for this
setting are only valid when the dimensionality scales with the number of
iterations, our lower bound holds in the natural regime where the
dimensionality is fixed. Lastly, expressing it as an optimal solution for the
corresponding optimization problem over polynomials, as formulated by our
framework, we present a novel systematic derivation of Nesterov's well-known
Accelerated Gradient Descent method. This rather natural interpretation of AGD
contrasts with earlier ones which lacked a simple, yet solid, motivation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06839</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06839</id><created>2015-03-23</created><updated>2015-09-22</updated><authors><author><keyname>Borr&#xe0;s</keyname><forenames>J&#xfa;lia</forenames></author><author><keyname>Asfour</keyname><forenames>Tamim</forenames></author></authors><title>A Whole-Body Pose Taxonomy for Loco-Manipulation Tasks</title><categories>cs.RO</categories><comments>8 pages, 7 figures, 1 table with full page figure that appears in
  landscape page, 2015 IEEE/RSJ International Conference on Intelligent Robots
  and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting interaction with the environment is a promising and powerful way
to enhance stability of humanoid robots and robustness while executing
locomotion and manipulation tasks. Recently some works have started to show
advances in this direction considering humanoid locomotion with multi-contacts,
but to be able to fully develop such abilities in a more autonomous way, we
need to first understand and classify the variety of possible poses a humanoid
robot can achieve to balance. To this end, we propose the adaptation of a
successful idea widely used in the field of robot grasping to the field of
humanoid balance with multi-contacts: a whole-body pose taxonomy classifying
the set of whole-body robot configurations that use the environment to enhance
stability. We have revised criteria of classification used to develop grasping
taxonomies, focusing on structuring and simplifying the large number of
possible poses the human body can adopt. We propose a taxonomy with 46 poses,
containing three main categories, considering number and type of supports as
well as possible transitions between poses. The taxonomy induces a
classification of motion primitives based on the pose used for support, and a
set of rules to store and generate new motions. We present preliminary results
that apply known segmentation techniques to motion data from the KIT whole-body
motion database. Using motion capture data with multi-contacts, we can identify
support poses providing a segmentation that can distinguish between locomotion
and manipulation parts of an action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06842</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06842</id><created>2015-03-23</created><updated>2015-04-01</updated><authors><author><keyname>Lopez-Pablos</keyname><forenames>Rodrigo</forenames><affiliation>Universidad Nacional de La Matanza y Universidad Tecnol&#xf3;gica Nacional</affiliation></author></authors><title>Apuntes sobre teor\'ia del comportamiento corrupto: nociones
  cibern\'eticas e inform\'aticas para una actualizaci\'on de la ecuaci\'on de
  Klitgaard</title><categories>cs.IT math.IT</categories><comments>21 pages, 2 figures, written in castilian</comments><acm-class>J.4; K.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This essay presents an exploration of elements from information theory and
cibernetics on the struggle against corruption behavior in public sector and
beyond; the existence of an exemplary or corrupt ethical equilibriums are
explored by updating Klitgaard corruption formula along with the presence of
information pressure, entropy and cibernetics servomechanisms in digital
societies, including alternatives and sistemics approaches for further
anti-corruption policies implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06851</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06851</id><created>2015-03-23</created><updated>2015-07-01</updated><authors><author><keyname>Taylor</keyname><forenames>Josh A.</forenames></author><author><keyname>Mathieu</keyname><forenames>Johanna L.</forenames></author><author><keyname>Callaway</keyname><forenames>Duncan S.</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author></authors><title>Price and capacity competition in balancing markets with energy storage</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy storage can absorb variability from the rising number of wind and
solar power producers. Storage is different from the conventional generators
that have traditionally balanced supply and demand on fast time scales due to
its hard energy capacity constraints, dynamic coupling, and low marginal costs.
These differences are leading system operators to propose new mechanisms for
enabling storage to participate in reserve and real-time energy markets. The
persistence of market power and gaming in electricity markets suggests that
these changes will expose new vulnerabilities.
  We develop a new model of strategic behavior among storages in energy
balancing markets. Our model is a two-stage game that generalizes a classic
model of capacity followed by Bertrand-Edgeworth price competition by
explicitly modeling storage dynamics and uncertainty in the pricing stage. By
applying the model to balancing markets with storage, we are able to compare
capacity and energy-based pricing schemes, and to analyze the dynamic effects
of the market horizon and energy losses due to leakage. Our first key finding
is that capacity pricing leads to higher prices and higher capacity
commitments, and that energy pricing leads to lower, randomized prices and
lower capacity commitments. Second, we find that a longer market horizon and
higher physical efficiencies lead to lower prices by inducing the storage to
compete to have their states of charge cycled more frequently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06854</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06854</id><created>2015-03-23</created><updated>2015-08-18</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author></authors><title>Massive MIMO: Ten Myths and One Critical Question</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Communications Magazine, 10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communications is one of the most successful technologies in modern
years, given that an exponential growth rate in wireless traffic has been
sustained for over a century (known as Cooper's law). This trend will certainly
continue driven by new innovative applications; for example, augmented reality
and internet-of-things.
  Massive MIMO (multiple-input multiple-output) has been identified as a key
technology to handle orders of magnitude more data traffic. Despite the
attention it is receiving from the communication community, we have personally
witnessed that Massive MIMO is subject to several widespread misunderstandings,
as epitomized by following (fictional) abstract:
  &quot;The Massive MIMO technology uses a nearly infinite number of high-quality
antennas at the base stations. By having at least an order of magnitude more
antennas than active terminals, one can exploit asymptotic behaviors that some
special kinds of wireless channels have. This technology looks great at first
sight, but unfortunately the signal processing complexity is off the charts and
the antenna arrays would be so huge that it can only be implemented in
millimeter wave bands.&quot;
  The statements above are, in fact, completely false. In this overview
article, we identify ten myths and explain why they are not true. We also ask a
question that is critical for the practical adoption of the technology and
which will require intense future research activities to answer properly. We
provide references to key technical papers that support our claims, while a
further list of related overview and technical papers can be found at the
Massive MIMO Info Point: http://massivemimo.eu
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06855</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06855</id><created>2015-03-23</created><authors><author><keyname>Br&#xe2;nzei</keyname><forenames>Simina</forenames></author><author><keyname>Hosseini</keyname><forenames>Hadi</forenames></author><author><keyname>Miltersen</keyname><forenames>Peter Bro</forenames></author></authors><title>Characterization and Computation of Equilibria for Indivisible Goods</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of allocating indivisible goods using the leading
notion of fairness in economics: the competitive equilibrium from equal
incomes. Focusing on two major classes of valuations, namely perfect
substitutes and perfect complements, we establish the computational properties
of algorithms operating in this framework. For the class of valuations with
perfect complements, our algorithm yields a surprisingly succinct
characterization of instances that admit a competitive equilibrium from equal
incomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06857</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06857</id><created>2015-03-23</created><authors><author><keyname>Rai</keyname><forenames>Anurag</forenames></author><author><keyname>Li</keyname><forenames>Chih-ping</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Loop-Free Backpressure Routing Using Link-Reversal Algorithms</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The backpressure routing policy is known to be a throughput optimal policy
that supports any feasible traffic demand in data networks, but may have poor
delay performance when packets traverse loops in the network. In this paper, we
study loop-free backpressure routing policies that forward packets along
directed acyclic graphs (DAGs) to avoid the looping problem. These policies use
link reversal algorithms to improve the DAGs in order to support any achievable
traffic demand.
  For a network with a single commodity, we show that a DAG that supports a
given traffic demand can be found after a finite number of iterations of the
link-reversal process. We use this to develop a joint link-reversal and
backpressure routing policy, called the loop free backpressure (LFBP)
algorithm. This algorithm forwards packets on the DAG, while the DAG is
dynamically updated based on the growth of the queue backlogs. We show by
simulations that such a DAG-based policy improves the delay over the classical
backpressure routing policy. We also propose a multicommodity version of the
LFBP algorithm, and via simulation we show that its delay performance is better
than that of backpressure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06858</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06858</id><created>2015-03-23</created><updated>2016-02-13</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author><author><keyname>Woodruff</keyname><forenames>David</forenames></author><author><keyname>Xie</keyname><forenames>Bo</forenames></author></authors><title>Communication Efficient Distributed Kernel Principal Component Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel Principal Component Analysis (KPCA) is a key machine learning
algorithm for extracting nonlinear features from data. In the presence of a
large volume of high dimensional data collected in a distributed fashion, it
becomes very costly to communicate all of this data to a single data center and
then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a
distributed and communication efficient fashion while maintaining provable and
strong guarantees in solution quality?
  In this paper, we give an affirmative answer to the question by developing a
communication efficient algorithm to perform kernel PCA in the distributed
setting. The algorithm is a clever combination of subspace embedding and
adaptive sampling techniques, and we show that the algorithm can take as input
an arbitrary configuration of distributed datasets, and compute a set of global
kernel principal components with relative error guarantees independent of the
dimension of the feature space or the total number of data points. In
particular, computing $k$ principal components with relative error $\epsilon$
over $s$ workers has communication cost $\tilde{O}(s \rho k/\epsilon+s
k^2/\epsilon^3)$ words, where $\rho$ is the average number of nonzero entries
in each data point. Furthermore, we experimented the algorithm with large-scale
real world datasets and showed that the algorithm produces a high quality
kernel PCA solution while using significantly less communication than
alternative approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06866</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06866</id><created>2015-03-23</created><updated>2016-02-20</updated><authors><author><keyname>Eb&#xe9;l&#xe9;</keyname><forenames>Serge Alain</forenames></author><author><keyname>Ndoundam</keyname><forenames>Ren&#xe8;</forenames></author></authors><title>Study of all the periods of a Neuronal Recurrence Equation</title><categories>cs.NE</categories><comments>22 pages</comments><msc-class>92B20</msc-class><acm-class>F.1.1</acm-class><journal-ref>Complex Systems, 24, 2015</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We characterize the structure of the periods of a neuronal recurrence
equation. Firstly, we give a characterization of k-chains in 0-1 periodic
sequences. Secondly, we characterize the periods of all cycles of some neuronal
recurrence equation. Thirdly, we explain how these results can be used to
deduce the existence of the generalized period-halving bifurcation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06869</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06869</id><created>2015-03-23</created><authors><author><keyname>Bartuschat</keyname><forenames>Dominik</forenames></author><author><keyname>Fischermeier</keyname><forenames>Ellen</forenames></author><author><keyname>Gustavsson</keyname><forenames>Katarina</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Two Computational Models for Simulating the Tumbling Motion of Elongated
  Particles in Fluids</title><categories>cs.CE physics.comp-ph physics.flu-dyn</categories><comments>Submitted to the Journal Computers &amp; Fluids (Elsevier)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suspensions with fiber-like particles in the low Reynolds number regime are
modeled by two different approaches that both use a Lagrangian representation
of individual particles. The first method is the well-established formulation
based on Stokes flow that is formulated as integral equations. It uses a
slender body approximation for the fibers to represent the interaction between
them directly without explicitly computing the flow field. The second is a new
technique using the 3D lattice Boltzmann method on parallel supercomputers.
Here the flow computation is coupled to a computational model of the dynamics
of rigid bodies using fluid-structure interaction techniques. Both methods can
be applied to simulate fibers in fluid flow. They are carefully validated and
compared against each other, exposing systematically their strengths and
weaknesses regarding their accuracy, the computational cost, and possible model
extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06870</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06870</id><created>2015-03-23</created><authors><author><keyname>Kloumann</keyname><forenames>Isabel</forenames></author><author><keyname>Adamic</keyname><forenames>Lada</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Wu</keyname><forenames>Shaomei</forenames></author></authors><title>The Lifecycles of Apps in a Social Ecosystem</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 10 figures, 3 tables, International World Wide Web
  Conference</comments><acm-class>H.2.8</acm-class><doi>10.1145/2736277.2741684</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apps are emerging as an important form of on-line content, and they combine
aspects of Web usage in interesting ways --- they exhibit a rich temporal
structure of user adoption and long-term engagement, and they exist in a
broader social ecosystem that helps drive these patterns of adoption and
engagement. It has been difficult, however, to study apps in their natural
setting since this requires a simultaneous analysis of a large set of popular
apps and the underlying social network they inhabit.
  In this work we address this challenge through an analysis of the collection
of apps on Facebook Login, developing a novel framework for analyzing both
temporal and social properties. At the temporal level, we develop a retention
model that represents a user's tendency to return to an app using a very small
parameter set. At the social level, we organize the space of apps along two
fundamental axes --- popularity and sociality --- and we show how a user's
probability of adopting an app depends both on properties of the local network
structure and on the match between the user's attributes, his or her friends'
attributes, and the dominant attributes within the app's user population. We
also develop models that show the importance of different feature sets with
strong performance in predicting app success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06871</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06871</id><created>2015-03-23</created><updated>2015-06-21</updated><authors><author><keyname>Zabolotny</keyname><forenames>Wojciech M.</forenames></author></authors><title>Low latency protocol for transmission of measurement data from FPGA to
  Linux computer via 10 Gbps Ethernet link</title><categories>cs.NI</categories><comments>Introduced further language corrections. Changed section describing
  resource consumption</comments><journal-ref>JINST 10 (2015), T07005</journal-ref><doi>10.1088/1748-0221/10/07/T07005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents FADE-10G - an integrated solution for modern multichannel
measurement systems. Its main aim is a low latency, reliable transmission of
measurement data from FPGA-based front-end electronic boards (FEBs) to a
computer-based node in the Data Acquisition System (DAQ), using a standard
Ethernet 1 Gbps or 10 Gbps link. In addition to transmission of data, the
system allows the user to send reliably simple control commands from DAQ to FEB
and to receive responses. The aim of the work is to provide a possible simple
base solution, which can be adapted by the end user to his or her particular
needs. Therefore, the emphasis is put on the minimal consumption of FPGA
resources in FEB and the minimal CPU load in the DAQ computer. The open source
implementation of the FPGA IP core and the Linux kernel driver published under
permissive license facilitates modifications and reuse of the solution. The
system has been successfully tested in real hardware, both with 1 Gbps and 10
Gbps links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06876</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06876</id><created>2015-03-23</created><updated>2016-01-30</updated><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Binary and Multi-Bit Coding for Stable Random Projections</title><categories>stat.ME cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop efficient binary (i.e., 1-bit) and multi-bit coding schemes for
estimating the scale parameter of $\alpha$-stable distributions. The work is
motivated by the recent work on one scan 1-bit compressed sensing (sparse
signal recovery) using $\alpha$-stable random projections, which requires
estimating of the scale parameter at bits-level. Our technique can be naturally
applied to data stream computations for estimating the $\alpha$-th frequency
moment. In fact, the method applies to the general scale family of
distributions, not limited to $\alpha$-stable distributions.
  Due to the heavy-tailed nature of $\alpha$-stable distributions, using
traditional estimators will potentially need many bits to store each
measurement in order to ensure sufficient accuracy. Interestingly, our paper
demonstrates that, using a simple closed-form estimator with merely 1-bit
information does not result in a significant loss of accuracy if the parameter
is chosen appropriately. For example, when $\alpha=0+$, 1, and 2, the
coefficients of the optimal estimation variances using full (i.e.,
infinite-bit) information are 1, 2, and 2, respectively. With the 1-bit scheme
and appropriately chosen parameters, the corresponding variance coefficients
are 1.544, $\pi^2/4$, and 3.066, respectively. Theoretical tail bounds are also
provided. Using 2 or more bits per measurements reduces the estimation variance
and importantly, stabilizes the estimate so that the variance is not sensitive
to parameters. With look-up tables, the computational cost is minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06882</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06882</id><created>2015-03-23</created><authors><author><keyname>Qiao</keyname><forenames>Deli</forenames></author><author><keyname>Qian</keyname><forenames>Haifeng</forenames></author><author><keyname>Li</keyname><forenames>Geoffrey Ye</forenames></author></authors><title>Broadbeam for Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO has been identified as one of the promising disruptive air
interface techniques to address the huge capacity requirement demanded by 5G
wireless communications. For practical deployment of such systems, the control
message need to be broadcast to all users reliably in the cell using broadbeam.
A broadbeam is expected to have the same radiated power in all directions to
cover users in any place in a cell. In this paper, we will show that there is
no perfect broadbeam. Therefore, we develop a method for generating broadbeam
that can allow tiny fluctuations in radiated power. Overall, this can serve as
an ingredient for practical deployment of the massive MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06900</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06900</id><created>2015-03-23</created><authors><author><keyname>Diao</keyname><forenames>Qiuju</forenames></author><author><keyname>Li</keyname><forenames>Juane</forenames></author><author><keyname>Lin</keyname><forenames>Shu</forenames></author><author><keyname>Blake</keyname><forenames>Ian</forenames></author></authors><title>New Classes of Partial Geometries and Their Associated LDPC Codes</title><categories>cs.IT math.IT</categories><comments>34 pages with single column, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of partial geometries to construct parity-check matrices for LDPC
codes has resulted in the design of successful codes with a probability of
error close to the Shannon capacity at bit error rates down to $10^{-15}$. Such
considerations have motivated this further investigation. A new and simple
construction of a type of partial geometries with quasi-cyclic structure is
given and their properties are investigated. The trapping sets of the partial
geometry codes were considered previously using the geometric aspects of the
underlying structure to derive information on the size of allowable trapping
sets. This topic is further considered here. Finally, there is a natural
relationship between partial geometries and strongly regular graphs. The
eigenvalues of the adjacency matrices of such graphs are well known and it is
of interest to determine if any of the Tanner graphs derived from the partial
geometries are good expanders for certain parameter sets, since it can be
argued that codes with good geometric and expansion properties might perform
well under message-passing decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06902</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06902</id><created>2015-03-23</created><authors><author><keyname>Zhou</keyname><forenames>Li</forenames></author></authors><title>A Note on Information-Directed Sampling and Thompson Sampling</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note introduce three Bayesian style Multi-armed bandit algorithms:
Information-directed sampling, Thompson Sampling and Generalized Thompson
Sampling. The goal is to give an intuitive explanation for these three
algorithms and their regret bounds, and provide some derivations that are
omitted in the original papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06911</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06911</id><created>2015-03-24</created><updated>2015-11-28</updated><authors><author><keyname>Zhao</keyname><forenames>Lin</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author></authors><title>A Stochastic Hybrid System Approach to Aggregated Load Modeling for
  Demand Response</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nondisruptive demand response programs often result in complex and fast
aggregated load dynamics, which may lead to severe impact on the distribution
and transition systems. Accurate modeling of aggregated load dynamics is of
fundamental importance for systematic analysis and design of various demand
response strategies. Existing methods mostly focus on simple first-order linear
Thermostatically Controlled Loads (TCLs). This paper develops a novel
stochastic hybrid system (SHS) framework to model individual responsive loads.
The proposed SHS has general nonlinear diffusion dynamics in each discrete mode
and has both random and deterministic state-dependent mode transitions. We make
an explicit connection between the extended generator of the corresponding SHS
process and the partial differential equations governing the evolution of the
hybrid-state probability density function. Consequently, a general coupled
Fokker-Planck equation characterizing the aggregated load dynamics is derived.
Especially, the boundary conditions due to the deterministic switchings are
uniquely determined via the accurate characterization of the extended
generator. The proposed modeling approach includes many existing methods as
special cases and provides a unified representation of a variety of responsive
loads. Realistic demand response examples are also provided to illustrate the
effectiveness of the proposed SHS framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06914</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06914</id><created>2015-03-24</created><authors><author><keyname>Kubo</keyname><forenames>Takuya</forenames></author><author><keyname>Nagaoka</keyname><forenames>Hiroshi</forenames></author></authors><title>A Fundamental Inequality for Lower-bounding the Error Probability for
  Classical and Quantum Multiple Access Channels and Its Applications</title><categories>cs.IT math.IT quant-ph</categories><comments>under submission</comments><doi>10.1587/transfun.E98.A.2376</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of the capacity problem for multiple access channels (MACs), a
lower bound on the error probability obtained by Han plays a crucial role in
the converse parts of several kinds of channel coding theorems in the
information-spectrum framework. Recently, Yagi and Oohama showed a tighter
bound than the Han bound by means of Polyanskiy's converse. In this paper, we
give a new bound which generalizes and strengthens the Yagi-Oohama bound, and
demonstrate that the bound plays a fundamental role in deriving extensions of
several known bounds. In particular, the Yagi-Oohama bound is generalized to
two different directions; i.e, to general input distributions and to general
encoders. In addition we extend these bounds to the quantum MACs and apply them
to the converse problems for several information-spectrum settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06917</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06917</id><created>2015-03-24</created><authors><author><keyname>Zhang</keyname><forenames>Qiang</forenames></author><author><keyname>Wang</keyname><forenames>Yilin</forenames></author><author><keyname>Li</keyname><forenames>Baoxin</forenames></author></authors><title>Unsupervised Video Analysis Based on a Spatiotemporal Saliency Detector</title><categories>cs.CV</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual saliency, which predicts regions in the field of view that draw the
most visual attention, has attracted a lot of interest from researchers. It has
already been used in several vision tasks, e.g., image classification, object
detection, foreground segmentation. Recently, the spectrum analysis based
visual saliency approach has attracted a lot of interest due to its simplicity
and good performance, where the phase information of the image is used to
construct the saliency map. In this paper, we propose a new approach for
detecting spatiotemporal visual saliency based on the phase spectrum of the
videos, which is easy to implement and computationally efficient. With the
proposed algorithm, we also study how the spatiotemporal saliency can be used
in two important vision task, abnormality detection and spatiotemporal interest
point detection. The proposed algorithm is evaluated on several commonly used
datasets with comparison to the state-of-art methods from the literature. The
experiments demonstrate the effectiveness of the proposed approach to
spatiotemporal visual saliency detection and its application to the above
vision tasks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06929</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06929</id><created>2015-03-24</created><authors><author><keyname>Takaoka</keyname><forenames>Asahi</forenames></author></authors><title>Graph isomorphism completeness for trapezoid graphs</title><categories>cs.DM cs.CC</categories><comments>4 pages, 3 Postscript figures</comments><doi>10.1587/transfun.E98.A.1838</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexity of the graph isomorphism problem for trapezoid graphs has been
open over a decade. This paper shows that the problem is GI-complete. More
precisely, we show that the graph isomorphism problem is GI-complete for
comparability graphs of partially ordered sets with interval dimension 2 and
height 3. In contrast, the problem is known to be solvable in polynomial time
for comparability graphs of partially ordered sets with interval dimension at
most 2 and height at most 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06934</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06934</id><created>2015-03-24</created><authors><author><keyname>Atoum</keyname><forenames>Issa</forenames></author><author><keyname>Bong</keyname><forenames>Chih How</forenames></author></authors><title>Measuring Software Quality in Use: State-of-the-Art and Research
  Challenges</title><categories>cs.SE cs.CL</categories><comments>4 Figures</comments><journal-ref>ASQ.Software Quality Professional, 17(2), 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software quality in use comprises quality from the user's perspective. It has
gained its importance in e-government applications, mobile-based applications,
embedded systems, and even business process development. User's decisions on
software acquisitions are often ad hoc or based on preference due to difficulty
in quantitatively measuring software quality in use. But, why is quality-in-use
measurement difficult? Although there are many software quality models, to the
authors' knowledge no works survey the challenges related to software
quality-in-use measurement. This article has two main contributions: 1) it
identifies and explains major issues and challenges in measuring software
quality in use in the context of the ISO SQuaRE series and related software
quality models and highlights open research areas; and 2) it sheds light on a
research direction that can be used to predict software quality in use. In
short, the quality-in-use measurement issues are related to the complexity of
the current standard models and the limitations and incompleteness of the
customized software quality models. A sentiment analysis of software reviews is
proposed to deal with these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06944</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06944</id><created>2015-03-24</created><authors><author><keyname>Germain</keyname><forenames>Pascal</forenames><affiliation>LHC</affiliation></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames><affiliation>LHC</affiliation></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LHC</affiliation></author><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LHC</affiliation></author></authors><title>PAC-Bayesian Theorems for Domain Adaptation with Specialization to
  Linear Classifiers</title><categories>stat.ML cs.LG</categories><comments>Under review</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide two main contributions in PAC-Bayesian theory for
domain adaptation where the objective is to learn, from a source distribution,
a well-performing majority vote on a different target distribution. On the one
hand, we propose an improvement of the previous approach proposed by Germain et
al. (2013), that relies on a novel distribution pseudodistance based on a
disagreement averaging, allowing us to derive a new tighter PAC-Bayesian domain
adaptation bound for the stochastic Gibbs classifier. We specialize it to
linear classifiers, and design a learning algorithm which shows interesting
results on a synthetic problem and on a popular sentiment annotation task. On
the other hand, we generalize these results to multisource domain adaptation
allowing us to take into account different source domains. This study opens the
door to tackle domain adaptation tasks by making use of all the PAC-Bayesian
tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06952</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06952</id><created>2015-03-24</created><authors><author><keyname>Metz</keyname><forenames>Jean</forenames></author><author><keyname>Spola&#xf4;r</keyname><forenames>Newton</forenames></author><author><keyname>Cherman</keyname><forenames>Everton A.</forenames></author><author><keyname>Monard</keyname><forenames>Maria C.</forenames></author></authors><title>Comparing published multi-label classifier performance measures to the
  ones obtained by a simple multi-label baseline classifier</title><categories>cs.LG</categories><comments>19 pages, 8 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In supervised learning, simple baseline classifiers can be constructed by
only looking at the class, i.e., ignoring any other information from the
dataset. The single-label learning community frequently uses as a reference the
one which always predicts the majority class. Although a classifier might
perform worse than this simple baseline classifier, this behaviour requires a
special explanation. Aiming to motivate the community to compare experimental
results with the ones provided by a multi-label baseline classifier, calling
the attention about the need of special explanations related to classifiers
which perform worse than the baseline, in this work we propose the use of
General_B, a multi-label baseline classifier. General_B was evaluated in
contrast to results published in the literature which were carefully selected
using a systematic review process. It was found that a considerable number of
published results on 10 frequently used datasets are worse than or equal to the
ones obtained by General_B, and for one dataset it reaches up to 43% of the
dataset published results. Moreover, although a simple baseline classifier was
not considered in these publications, it was observed that even for very poor
results no special explanations were provided in most of them. We hope that the
findings of this work would encourage the multi-label community to consider the
idea of using a simple baseline classifier, such that further explanations are
provided when a classifiers performs worse than a baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06958</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06958</id><created>2015-03-24</created><authors><author><keyname>Musa</keyname><forenames>Sajid</forenames></author><author><keyname>Ziatdinov</keyname><forenames>Rushan</forenames></author><author><keyname>Sozcu</keyname><forenames>Omer Faruk</forenames></author><author><keyname>Griffiths</keyname><forenames>Carol</forenames></author></authors><title>Developing Educational Computer Animation Based on Human Personality
  Types</title><categories>cs.HC cs.CY cs.GR</categories><comments>19 pages, 19 figures, 18 tables</comments><journal-ref>European Journal of Contemporary Education, 2015, Vol. 11, Issue
  1, pp. 52-71</journal-ref><doi>10.13187/ejced.2015.11.52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer animation in the past decade has become one of the most noticeable
features of technology-based learning environments. With today's high
educational demands as well as the lack of time provided for certain courses,
classical educational methods have shown deficiencies in keeping up with the
drastic changes observed in the digital era. Without taking into account
various significant factors such as gender, age, level of interest and memory
level, educational animation may turn out to be insufficient for learners or
fail to meet their needs. However, we have noticed that the applications of
animation for education have been given only inadequate attention, and
students' personality types have never been taken into account. We suggest
there is an interesting relationship here, and propose essential factors in
creating educational animations based on students' personality types.
Particularly, we investigate how information in computer animation may be
presented in a preferable way based on the fundamental elements of computer
animation. The present study believes that it is likely to have wide benefits
in the field of education. Considering the personality types in designing
educational computer animations with the aid of gathered empirical results
might be a promising avenue to enhance the learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06959</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06959</id><created>2015-03-24</created><authors><author><keyname>Baroffio</keyname><forenames>Luca</forenames></author><author><keyname>Cesana</keyname><forenames>Matteo</forenames></author><author><keyname>Redondi</keyname><forenames>Alessandro</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author></authors><title>Fast keypoint detection in video sequences</title><categories>cs.CV cs.MM</categories><comments>submitted to IEEE International Conference on Image Processing 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of computer vision tasks exploit a succinct representation of the
visual content in the form of sets of local features. Given an input image,
feature extraction algorithms identify a set of keypoints and assign to each of
them a description vector, based on the characteristics of the visual content
surrounding the interest point. Several tasks might require local features to
be extracted from a video sequence, on a frame-by-frame basis. Although
temporal downsampling has been proven to be an effective solution for mobile
augmented reality and visual search, high temporal resolution is a key
requirement for time-critical applications such as object tracking, event
recognition, pedestrian detection, surveillance. In recent years, more and more
computationally efficient visual feature detectors and decriptors have been
proposed. Nonetheless, such approaches are tailored to still images. In this
paper we propose a fast keypoint detection algorithm for video sequences, that
exploits the temporal coherence of the sequence of keypoints. According to the
proposed method, each frame is preprocessed so as to identify the parts of the
input frame for which keypoint detection and description need to be performed.
Our experiments show that it is possible to achieve a reduction in
computational time of up to 40%, without significantly affecting the task
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06960</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06960</id><created>2015-03-24</created><updated>2015-04-14</updated><authors><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Yehudayoff</keyname><forenames>Amir</forenames></author></authors><title>Sample compression schemes for VC classes</title><categories>cs.LG</categories><comments>14 pages. The previous version of this text contained an error;
  Theorem 2.1 in it is false. This error only affects the statement for
  multi-labeled classes, and the construction for binary-labeled classes still
  holds. In the new version of the text, we added a relevant discussion in
  Section 4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sample compression schemes were defined by Littlestone and Warmuth (1986) as
an abstraction of the structure underlying many learning algorithms. Roughly
speaking, a sample compression scheme of size $k$ means that given an arbitrary
list of labeled examples, one can retain only $k$ of them in a way that allows
to recover the labels of all other examples in the list. They showed that
compression implies PAC learnability for binary-labeled classes, and asked
whether the other direction holds. We answer their question and show that every
concept class $C$ with VC dimension $d$ has a sample compression scheme of size
exponential in $d$. The proof uses an approximate minimax phenomenon for binary
matrices of low VC dimension, which may be of interest in the context of game
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06962</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06962</id><created>2015-03-24</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Probabilistic Binary-Mask Cocktail-Party Source Separation in a
  Convolutional Deep Neural Network</title><categories>cs.SD cs.LG cs.NE</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation of competing speech is a key challenge in signal processing and a
feat routinely performed by the human auditory brain. A long standing benchmark
of the spectrogram approach to source separation is known as the ideal binary
mask. Here, we train a convolutional deep neural network, on a two-speaker
cocktail party problem, to make probabilistic predictions about binary masks.
Our results approach ideal binary mask performance, illustrating that
relatively simple deep neural networks are capable of robust binary mask
prediction. We also illustrate the trade-off between prediction statistics and
separation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06966</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06966</id><created>2015-03-24</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Multicast Multigroup Beamforming for Per-antenna Power Constrained
  Large-scale Arrays</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE SPAWC 2015. arXiv admin note: substantial text
  overlap with arXiv:1406.7557</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large in the number of transmit elements, multi-antenna arrays with
per-element limitations are in the focus of the present work. In this context,
physical layer multigroup multicasting under per-antenna power constrains, is
investigated herein. To address this complex optimization problem
low-complexity alternatives to semi-definite relaxation are proposed. The goal
is to optimize the per-antenna power constrained transmitter in a maximum
fairness sense, which is formulated as a non-convex quadratically constrained
quadratic problem. Therefore, the recently developed tool of feasible point
pursuit and successive convex approximation is extended to account for
practical per-antenna power constraints. Interestingly, the novel iterative
method exhibits not only superior performance in terms of approaching the
relaxed upper bound but also a significant complexity reduction, as the
dimensions of the optimization variables increase. Consequently, multicast
multigroup beamforming for large-scale array transmitters with per-antenna
dedicated amplifiers is rendered computationally efficient and accurate. A
preliminary performance evaluation in large-scale systems for which the
semi-definite relaxation constantly yields non rank-1 solutions is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06970</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06970</id><created>2015-03-24</created><authors><author><keyname>Aerts</keyname><forenames>Nieke</forenames></author><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author></authors><title>Straight Line Triangle Representations</title><categories>cs.CG math.CO</categories><comments>An extended abstract of this paper was presented at GD2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A straight line triangle representation (SLTR) of a planar graph is a
straight line drawing such that all the faces including the outer face have
triangular shape. Such a drawing can be viewed as a tiling of a triangle using
triangles with the input graph as skeletal structure. In this paper we present
a characterization of graphs that have an SLTR. The characterization is based
on flat angle assignments, i.e., selections of angles of the graph that have
size~$\pi$ in the representation. We also provide a second characterization in
terms of contact systems of pseudosegments. With the aid of discrete harmonic
functions we show that contact systems of pseudosegments that respect certain
conditions are stretchable. The stretching procedure is then used to get
straight line triangle representations. Since the discrete harmonic function
approach is quite flexible it allows further applications, we mention some of
them. The drawback of the characterization of SLTRs is that we are not able to
effectively check whether a given graph admits a flat angle assignment that
fulfills the conditions. Hence it is still open to decide whether the
recognition of graphs that admit straight line triangle representation is
polynomially tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06973</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06973</id><created>2015-03-24</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Slime Mould Inspired Generalised Voronoi Diagrams with Repulsive Fields</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The giant single-celled amoeboid organism Physarum polycephalum constructs
minimising transport networks but can also approximate the Voronoi diagram
using two different mechanisms. In the first method Voronoi bisectors are rep-
resented by deformation of a pre-existing plasmodial network by repellent
sources acting as generating points. In the second method generating points act
as inoculation sites for grow- ing plasmodia and Voronoi bisectors are
represented by vacant regions before the plasmodia fuse. To explore the
behaviour of minimising networks in the presence of repulsion fields we utilise
a computational model of Physarum as a distributed virtual computing material.
We characterise the different types of computational behaviours elicited by
attraction and repulsion stimuli and demonstrate the approximation Voronoi
diagrams using growth towards attractants, avoidance of repellents, and
combinations of both. Approximation of Voronoi diagrams for point data sources,
complex planar shapes and circle sets is demonstrated. By altering repellent
con- centration we found that partition of data sources was maintained but the
internal network connectivity was minimised by the contractile force of the
transport network. To conclude, we find that the repertoire of unconventional
computation methods is enhanced by the addition of stimuli presented by
repellent fields, suggesting novel approaches to plane-division, packing, and
minimisation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06974</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06974</id><created>2015-03-24</created><updated>2015-06-09</updated><authors><author><keyname>Abraham</keyname><forenames>Erika</forenames></author><author><keyname>Bekas</keyname><forenames>Costas</forenames></author><author><keyname>Brandic</keyname><forenames>Ivona</forenames></author><author><keyname>Genaim</keyname><forenames>Samir</forenames></author><author><keyname>Johnsen</keyname><forenames>Einar Broch</forenames></author><author><keyname>Kondov</keyname><forenames>Ivan</forenames></author><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author><author><keyname>Streit</keyname><forenames>Achim</forenames></author></authors><title>Challenges and Recommendations for Preparing HPC Applications for
  Exascale</title><categories>cs.DC</categories><comments>18th International Conference on Network-Based Information Systems
  (NBiS 2015). 2-4 September 2015 in Tamkang, Taiwan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the HPC community is working towards the development of the first
Exaflop computer (expected around 2020), after reaching the Petaflop milestone
in 2008 still only few HPC applications are able to fully exploit the
capabilities of Petaflop systems. In this paper we argue that efforts for
preparing HPC applications for Exascale should start before such systems become
available. We identify challenges that need to be addressed and recommend
solutions in key areas of interest, including formal modeling, static analysis
and optimization, runtime analysis and optimization, and autonomic computing.
Furthermore, we outline a conceptual framework for porting HPC applications to
future Exascale computing systems and propose steps for its implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06980</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06980</id><created>2015-03-24</created><authors><author><keyname>Cetinkaya</keyname><forenames>Ahmet</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author><author><keyname>Hayakawa</keyname><forenames>Tomohisa</forenames></author></authors><title>Event-Triggered Control over Unreliable Networks Subject to Jamming
  Attacks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-triggered networked control of a linear dynamical system is
investigated. Specifically, the dynamical system and the controller are assumed
to be connected through a communication channel. State and control input
information packets between the system and the controller are attempted to be
exchanged over the network only at time instants when certain triggering
conditions are satisfied. We provide a probabilistic characterization for the
link failures which allows us to model random packet losses due to
unreliability in transmissions as well as those caused by malicious jamming
attacks. We obtain conditions for the almost sure stability of the closed-loop
system, and we illustrate the efficacy of our approach with a numerical
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06981</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06981</id><created>2015-03-24</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Sharma</keyname><forenames>Shree Krishna</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Jens Krauseand Bjorn</forenames></author></authors><title>Coordinated Multibeam Satellite Co-location: The Dual Satellite Paradigm</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE wirless. Comms. Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present article, a new system architecture for the next generation of
satellite communication (SatComs) is presented. The key concept lies in the
collaboration between multibeam satellites that share one orbital position.
Multi-satellite constellations in unique orbital slots offer gradual deployment
to cover unpredictable traffic patterns and redundancy to hardware failure
advantages. They are also of high relevance during the satellite replacement
phases or necessitated by constraints in the maximum communications payload
that a single satellite can bear. In this context, the potential gains of
advanced architectures, that is architectures enabled by the general class of
cooperative and cognitive techniques, are exhibited via a simple paradigm. More
specifically, the scenario presented herein, involves two co-existing multibeam
satellites which illuminate overlapping coverage areas. Based on this scenario,
specific types of cooperative and cognitive techniques are herein considered as
candidate technologies that can boost the performance of multibeam satellite
constellations. These techniques are compared to conventional frequency
splitting configurations in terms of three different criteria, namely the
spectral efficiency, the power efficiency and the fairness. Consequently,
insightful guidelines for the design of future high throughput constellations
of multibeam satellites are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06982</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06982</id><created>2015-03-24</created><updated>2015-04-02</updated><authors><author><keyname>Gahlawat</keyname><forenames>Aditya</forenames></author><author><keyname>Peet</keyname><forenames>Matthew M.</forenames></author></authors><title>Output Feedback Control of Inhomogeneous Parabolic PDEs with Point
  Actuation and Point Measurement using SOS and Semi-Separable Kernels</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use SOS and SDP to design output feedback controllers for a
class of one-dimensional parabolic partial differential equations with point
measurements and point actuation. Our approach is based on the use of SOS to
search for positive quadratic Lyapunov functions, controllers and observers.
These Lyapunov functions, controllers and observers are parameterized by linear
operators which are defined by SOS polynomials. The main result of the paper is
the development of an improved class of observer-based controllers and evidence
which indicates that when the system is controllable and observable, these
methods will find a observer-based controller for sufficiently high polynomial
degree (similar to well-known results from backstepping).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.06995</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.06995</id><created>2015-03-24</created><authors><author><keyname>Cant&#xf3;n</keyname><forenames>A.</forenames></author><author><keyname>Fern&#xe1;ndez-Jambrina</keyname><forenames>L.</forenames></author></authors><title>Interpolation of a spline developable surface between a curve and two
  rulings</title><categories>cs.GR math.NA</categories><msc-class>65D17, 68U07</msc-class><journal-ref>Frontiers of Information Technology &amp; Electronic Engineering 16,
  173-190 (2015)</journal-ref><doi>10.1631/FITEE.14a0210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of interpolating a spline developable
patch bounded by a given spline curve and the first and the last rulings of the
developable surface. In order to complete the boundary of the patch a second
spline curve is to be given. Up to now this interpolation problem could be
solved, but without the possibility of choosing both endpoints for the rulings.
We circumvent such difficulty here by resorting to degree elevation of the
developable surface. This is useful not only to solve this problem, but also
other problems dealing with triangular developable patches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07000</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07000</id><created>2015-03-24</created><authors><author><keyname>Masti</keyname><forenames>Ramya Jayaram</forenames></author><author><keyname>Rai</keyname><forenames>Devendra</forenames></author><author><keyname>Ranganathan</keyname><forenames>Aanjhan</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Christian</forenames></author><author><keyname>Thiele</keyname><forenames>Lothar</forenames></author><author><keyname>Capkun</keyname><forenames>Srdjan</forenames></author></authors><title>Thermal Covert Channels on Multi-core Platforms</title><categories>cs.CR</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Side channels remain a challenge to information flow control and security in
modern computing platforms. Resource partitioning techniques that minimise the
number of shared resources among processes are often used to address this
challenge. In this work, we focus on multi-core platforms and we demonstrate
that even seemingly strong isolation techniques based on dedicated cores and
memory can be circumvented through the use of thermal side channels.
Specifically, we show that the processor core temperature can be used both as a
side channel as well as a covert communication channel even when the system
implements strong spatial and temporal partitioning. Our experiments on an
x86-based platform demonstrate covert thermal channels that achieve up to 12.5
bps and a weak side channel that can detect processes executed on neighbouring
cores. This work therefore shows a limitation in the isolation that can be
achieved on existing multi-core systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07001</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07001</id><created>2015-03-24</created><authors><author><keyname>Rodrigues</keyname><forenames>Eug&#xe9;nio</forenames></author><author><keyname>Amaral</keyname><forenames>Ana Rita</forenames></author><author><keyname>Gaspar</keyname><forenames>Ad&#xe9;lio Rodrigues</forenames></author><author><keyname>Gomes</keyname><forenames>&#xc1;lvaro</forenames></author><author><keyname>da Silva</keyname><forenames>Manuel Carlos Gameiro</forenames></author><author><keyname>Antunes</keyname><forenames>Carlos Henggeler</forenames></author></authors><title>GerAPlanO - A new building design tool: design generation, thermal
  assessment and performance optimization</title><categories>cs.HC</categories><comments>6 pages, 3 figures, Proceedings of Energy for Sustainability 2015
  Conference: Sustainable Cities: Designing for People and the Planet, Coimbra,
  14-15 May, 2015</comments><msc-class>68N01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building practitioners (architects, engineers, energy managers) are showing a
growing interest in the design of more energy efficient and livable buildings.
The best way to predict how a building will behave regarding energy consumption
and thermal comfort is to use a dynamic simulation tool. However, the use of
this kind of tools is difficult on a daily basis practice due to the heuristic
and exploratory nature of the architectural design process. To deal with this
difficulty, the University of Coimbra and three companies have been working on
the development of a prototype design aiding tool, specifically devoted to the
space planning phase of building design, under the project GerAPlanO (Automatic
Generation of Architecture Floor plans with Energy Optimization). This project
aims to combine the capabilities of design generation techniques, thermal
assessment programs, and design optimization methods to provide assistance to
decision makers. This paper presents the overall concept, as well as the
current status of development of this tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07015</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07015</id><created>2015-03-24</created><updated>2015-07-08</updated><authors><author><keyname>Chen</keyname><forenames>Zhangli</forenames></author><author><keyname>Hohmann</keyname><forenames>Volker</forenames></author></authors><title>Online Monaural Speech Enhancement Based on Periodicity Analysis and A
  Priori SNR Estimation</title><categories>cs.SD</categories><comments>13 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an online algorithm for enhancing monaural noisy speech.
Firstly, a novel phase-corrected low-delay gammatone filterbank is derived for
signal subband decomposition and resynthesis; the subband signals are then
analyzed frame by frame. Secondly, a novel feature named periodicity degree
(PD) is proposed to be used for detecting and estimating the fundamental period
(P0) in each frame and for estimating the signal-to-noise ratio (SNR) in each
frame-subband signal unit. The PD is calculated in each unit as the
multiplication of the normalized autocorrelation and the comb filter ratio, and
shown to be robust in various low-SNR conditions. Thirdly, the noise energy
level in each signal unit is estimated recursively based on the estimated SNR
for units with high PD and based on the noisy signal energy level for units
with low PD. Then the a priori SNR is estimated using a decision-directed
approach with the estimated noise level. Finally, a revised Wiener gain is
calculated, smoothed, and applied to each unit; the processed units are summed
across subbands and frames to form the enhanced signal. The P0 detection
accuracy of the algorithm was evaluated on two corpora and showed comparable
performance on one corpus and better performance on the other corpus when
compared to a recently published pitch detection algorithm. The speech
enhancement effect of the algorithm was evaluated on one corpus with two
objective criteria and showed better performance in one highly non-stationary
noise and comparable performance in two other noises when compared to a
state-of-the-art statistical-model based algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07016</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07016</id><created>2015-03-24</created><authors><author><keyname>Amaral</keyname><forenames>Ana Rita</forenames></author><author><keyname>Rodrigues</keyname><forenames>Eug&#xe9;nio</forenames></author><author><keyname>Gaspar</keyname><forenames>Ad&#xe9;lio Rodrigues</forenames></author><author><keyname>Gomes</keyname><forenames>&#xc1;lvaro</forenames></author></authors><title>A parametric study of window-to-floor ratio of three window types using
  dynamic simulation</title><categories>cs.OH</categories><comments>7 pages, 2 figures, Proceedings of Energy for Sustainability 2015
  Conference: Sustainable Cities: Designing for People and the Planet, Coimbra,
  14-15 May, 2015</comments><msc-class>68U20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The windows can be responsible for unnecessary energy consumption in a
building, if incorrectly designed, shadowed or oriented. Considering an annual
thermal comfort assessment of a space, if windows are over-dimensioned, they
can contribute to the increase of the heating needs due to heat losses, and
also to the increase of cooling needs due to over-exposure to solar radiation.
When under-dimensioned, the same space may benefit from reduced heat losses
through the glazing surface but does not benefit from solar radiation gains.
Therefore, it is important to find the optimum design that minimizes both the
heating and cooling needs. This paper presents a parametric study of window
type (single, double and triple glazing), orientation and opening size, located
in the city of Coimbra, Portugal. An annual and a seasonal assessment were
done, in order to obtain the set of optimum values around 360 degree
orientation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07017</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07017</id><created>2015-03-24</created><authors><author><keyname>Vouros</keyname><forenames>George</forenames></author></authors><title>The Emergence of Norms via Contextual Agreements in Open Societies</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the emergence of norms in agents' societies when agents
play multiple -even incompatible- roles in their social contexts
simultaneously, and have limited interaction ranges. Specifically, this article
proposes two reinforcement learning methods for agents to compute agreements on
strategies for using common resources to perform joint tasks. The computation
of norms by considering agents' playing multiple roles in their social contexts
has not been studied before. To make the problem even more realistic for open
societies, we do not assume that agents share knowledge on their common
resources. So, they have to compute semantic agreements towards performing
their joint actions. %The paper reports on an empirical study of whether and
how efficiently societies of agents converge to norms, exploring the proposed
social learning processes w.r.t. different society sizes, and the ways agents
are connected. The results reported are very encouraging, regarding the speed
of the learning process as well as the convergence rate, even in quite complex
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07021</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07021</id><created>2015-03-24</created><updated>2015-09-08</updated><authors><author><keyname>Tzoumas</keyname><forenames>Vasileios</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Minimal Reachability Problems</title><categories>cs.SY</categories><comments>Final version (with missing proofs) that will appear in CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address a collection of state space reachability problems,
for linear time-invariant systems, using a minimal number of actuators. In
particular, we design a zero-one diagonal input matrix B, with a minimal number
of non-zero entries, so that a specified state vector is reachable from a given
initial state. Moreover, we design a B so that a system can be steered either
into a given subspace, or sufficiently close to a desired state. This work
extends the recent results of Olshevsky and Pequito, where a zero-one diagonal
or column matrix B is constructed so that the involved system is controllable.
Specifically, we prove that the first two of our aforementioned problems are
NP-hard; these results hold for a zero-one column matrix B as well. Then, we
provide efficient polynomial time algorithms for their general solution, along
with their worst case approximation guarantees. Finally, we illustrate their
performance over large random networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07025</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07025</id><created>2015-03-24</created><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames><affiliation>Toulouse</affiliation></author><author><keyname>Garoche</keyname><forenames>Pierre-Lo&#xef;c</forenames><affiliation>Toulouse</affiliation></author><author><keyname>Magron</keyname><forenames>Victor</forenames></author></authors><title>Property-based Polynomial Invariant Generation using Sums-of-Squares
  Optimization</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.3941</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While abstract interpretation is not theoretically restricted to specific
kinds of properties, it is, in practice, mainly developed to compute linear
over-approximations of reachable sets, aka. the collecting semantics of the
program. The verification of user-provided properties is not easily compatible
with the usual forward fixpoint computation using numerical abstract domains.
We propose here to rely on sums-of-squares programming to characterize a
property-driven polynomial invariant. This invariant generation can be guided
by either boundedness, or in contrary, a given zone of the state space to
avoid. While the target property is not necessarily inductive with respect to
the program semantics, our method identifies a stronger inductive polynomial
invariant using numerical optimization. Our method applies to a wide set of
programs: a main while loop composed of a disjunction (if-then-else) of
polynomial updates e.g. piecewise polynomial controllers. It has been evaluated
on various programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07026</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07026</id><created>2015-03-24</created><authors><author><keyname>Menhour</keyname><forenames>Lghani</forenames><affiliation>CAOR</affiliation></author><author><keyname>D'Andr&#xe9;a-Novel</keyname><forenames>Brigitte</forenames><affiliation>CAOR</affiliation></author><author><keyname>Fliess</keyname><forenames>Michel</forenames><affiliation>LIX</affiliation></author><author><keyname>Gruyer</keyname><forenames>Dominique</forenames><affiliation>IFSTTAR/COSYS/LIVIC</affiliation></author><author><keyname>Mounier</keyname><forenames>Hugues</forenames></author></authors><title>A new model-free design for vehicle control and its validation through
  an advanced simulation platform</title><categories>math.OC cs.RO</categories><comments>in 14th European Control Conference, Jul 2015, Linz, Austria. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new model-free setting and the corresponding &quot;intelligent&quot; P and PD
controllers are employed for the longitudinal and lateral motions of a vehicle.
This new approach has been developed and used in order to ensure simultaneously
a best profile tracking for the longitudinal and lateral behaviors. The
longitudinal speed and the derivative of the lateral deviation, on one hand,
the driving/braking torque and the steering angle, on the other hand, are
respectively the output and the input variables. Let us emphasize that a &quot;good&quot;
mathematical modeling, which is quite difficult, if not impossible to obtain,
is not needed for such a design. An important part of this publication is
focused on the presentation of simulation results with actual and virtual data.
The actual data, used in Matlab as reference trajectories, have been obtained
from a properly instrumented car (Peugeot 406). Other virtual sets of data have
been generated through the interconnected platform SiVIC/RTMaps. It is a
dedicated virtual simulation platform for prototyping and validation of
advanced driving assistance systems. Keywords- Longitudinal and lateral vehicle
control, model-free control, intelligent P controller (i-P controller),
algebraic estimation, ADAS (Advanced Driving Assistance Systems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07027</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07027</id><created>2015-03-24</created><updated>2016-02-29</updated><authors><author><keyname>Schnass</keyname><forenames>Karin</forenames></author></authors><title>Convergence radius and sample complexity of ITKM algorithms for
  dictionary learning</title><categories>cs.LG cs.IT math.IT</categories><comments>32 pages, 1 figure, revised and corrected version including new
  numerical section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we show that iterative thresholding and K-means (ITKM)
algorithms can recover a generating dictionary with K atoms from noisy $S$
sparse signals up to an error $\tilde \varepsilon$ as long as the
initialisation is within a convergence radius, that is up to a $\log K$ factor
inversely proportional to the dynamic range of the signals, and the sample size
is proportional to $K \log K \tilde \varepsilon^{-2}$. The results are valid
for arbitrary target errors if the sparsity level is of the order of the square
of the signal dimension $d$ and for target errors down to $K^{-\ell}$ if $S$
scales as $S \leq d/(\ell \log K)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07038</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07038</id><created>2015-03-24</created><authors><author><keyname>Kumar</keyname><forenames>G Arun</forenames></author><author><keyname>Saha</keyname><forenames>Snehanshu</forenames></author><author><keyname>Sundaresan</keyname><forenames>Aravind</forenames></author><author><keyname>Goswami</keyname><forenames>Bidisha</forenames></author></authors><title>A QoS aware Novel Probabilistic strategy for Dynamic Resource Allocation</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a two player game based strategy for resource allocation
in service computing domain such as cloud, grid etc. The players are modeled as
demand/workflows for the resource and represent multiple types of qualitative
and quantitative factors. The proposed strategy will classify them in two
classes. The proposed system would forecast outcome using a priori information
available and measure/estimate existing parameters such as utilization and
delay in an optimal load-balanced paradigm.
  Keywords: Load balancing; service computing; Logistic Regression;
probabilistic estimation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07073</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07073</id><created>2015-03-24</created><updated>2015-07-30</updated><authors><author><keyname>Wickerson</keyname><forenames>John</forenames></author><author><keyname>Batty</keyname><forenames>Mark</forenames></author><author><keyname>Donaldson</keyname><forenames>Alastair F.</forenames></author></authors><title>Overhauling SC atomics in C11 and OpenCL</title><categories>cs.PL</categories><comments>12 pages + 1 page of references + 7 pages of appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the conceptual simplicity of sequential consistency (SC), the
semantics of SC atomic operations and fences in the C11 and OpenCL memory
models is subtle, leading to convoluted prose descriptions that translate to
complex axiomatic formalisations. We conduct an overhaul of SC atomics in C11,
reducing the associated axioms in both number and complexity. A consequence of
our simplification is that the SC operations in an execution no longer need to
be totally ordered. This relaxation enables, for the first time, efficient and
exhaustive simulation of litmus tests that use SC atomics. We use our improved
C11 model to present the first rigorous memory model formalisation for OpenCL
(which extends C11 with support for heterogeneous many-core programming). In
the OpenCL setting, we refine the SC axioms still further to give a sensible
semantics to SC operations that employ a 'memory scope' to restrict their
visibility to specific threads. Our overhaul requires slight strengthenings of
both the C11 and the OpenCL memory models, causing some behaviours to become
disallowed. We argue that these strengthenings are natural, and prove that all
of the formalised C11 and OpenCL compilation schemes of which we are aware
(Power and x86 for C11, AMD GPU for OpenCL) remain valid in our revised models.
Using the Herd memory model simulator, we show that our overhaul leads to an
exponential improvement in simulation time for C11 litmus tests compared with
the original model, making *exhaustive* simulation competitive, time-wise, with
the *non-exhaustive* CDSChecker tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07077</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07077</id><created>2015-03-24</created><authors><author><keyname>Dieleman</keyname><forenames>Sander</forenames></author><author><keyname>Willett</keyname><forenames>Kyle W.</forenames></author><author><keyname>Dambre</keyname><forenames>Joni</forenames></author></authors><title>Rotation-invariant convolutional neural networks for galaxy morphology
  prediction</title><categories>astro-ph.IM astro-ph.GA cs.CV cs.LG cs.NE stat.ML</categories><comments>Accepted for publication in MNRAS. 20 pages, 14 figures</comments><doi>10.1093/mnras/stv632</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the morphological parameters of galaxies is a key requirement for
studying their formation and evolution. Surveys such as the Sloan Digital Sky
Survey (SDSS) have resulted in the availability of very large collections of
images, which have permitted population-wide analyses of galaxy morphology.
Morphological analysis has traditionally been carried out mostly via visual
inspection by trained experts, which is time-consuming and does not scale to
large ($\gtrsim10^4$) numbers of images.
  Although attempts have been made to build automated classification systems,
these have not been able to achieve the desired level of accuracy. The Galaxy
Zoo project successfully applied a crowdsourcing strategy, inviting online
users to classify images by answering a series of questions. Unfortunately,
even this approach does not scale well enough to keep up with the increasing
availability of galaxy images.
  We present a deep neural network model for galaxy morphology classification
which exploits translational and rotational symmetry. It was developed in the
context of the Galaxy Challenge, an international competition to build the best
model for morphology classification based on annotated images from the Galaxy
Zoo project.
  For images with high agreement among the Galaxy Zoo participants, our model
is able to reproduce their consensus with near-perfect accuracy ($&gt; 99\%$) for
most questions. Confident model predictions are highly accurate, which makes
the model suitable for filtering large collections of images and forwarding
challenging images to experts for manual annotation. This approach greatly
reduces the experts' workload without affecting accuracy. The application of
these algorithms to larger sets of training data will be critical for analysing
results from future surveys such as the LSST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07082</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07082</id><created>2015-03-24</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Hoffmann</keyname><forenames>Udo</forenames></author></authors><title>Recognition and Complexity of Point Visibility Graphs</title><categories>cs.CG math.CO</categories><comments>16 pages, 10 figures. To appear in Proceedings of SoCG 2015</comments><acm-class>I.3.5; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A point visibility graph is a graph induced by a set of points in the plane,
where every vertex corresponds to a point, and two vertices are adjacent
whenever the two corresponding points are visible from each other, that is, the
open segment between them does not contain any other point of the set. We study
the recognition problem for point visibility graphs: given a simple undirected
graph, decide whether it is the visibility graph of some point set in the
plane. We show that the problem is complete for the existential theory of the
reals. Hence the problem is as hard as deciding the existence of a real
solution to a system of polynomial inequalities. The proof involves simple
substructures forcing collinearities in all realizations of some visibility
graphs, which are applied to the algebraic universality constructions of Mn\&quot;ev
and Richter-Gebert. This solves a longstanding open question and paves the way
for the analysis of other classes of visibility graphs. Furthermore, as a
corollary of one of our construction, we show that there exist point visibility
graphs that do not admit any geometric realization with points having integer
coordinates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07092</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07092</id><created>2015-03-24</created><authors><author><keyname>Shah</keyname><forenames>Mohak</forenames></author></authors><title>Big Data and the Internet of Things</title><categories>cs.CY</categories><comments>33 pages. draft of upcoming book chapter in Japkowicz and Stefanowski
  (eds.) Big Data Analysis: New algorithms for a new society, Springer Series
  on Studies in Big Data, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in sensing and computing capabilities are making it possible to
embed increasing computing power in small devices. This has enabled the sensing
devices not just to passively capture data at very high resolution but also to
take sophisticated actions in response. Combined with advances in
communication, this is resulting in an ecosystem of highly interconnected
devices referred to as the Internet of Things - IoT. In conjunction, the
advances in machine learning have allowed building models on this ever
increasing amounts of data. Consequently, devices all the way from heavy assets
such as aircraft engines to wearables such as health monitors can all now not
only generate massive amounts of data but can draw back on aggregate analytics
to &quot;improve&quot; their performance over time. Big data analytics has been
identified as a key enabler for the IoT. In this chapter, we discuss various
avenues of the IoT where big data analytics either is already making a
significant impact or is on the cusp of doing so. We also discuss social
implications and areas of concern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07093</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07093</id><created>2015-03-24</created><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Mark&#xf3;</keyname><forenames>Roland</forenames></author></authors><title>On the Complexity of Nondeterministically Testable Hypergraph Parameters</title><categories>cs.DS math.CO math.OC</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proves the equivalence of the notions of nondeterministic and
deterministic parameter testing for uniform dense hypergraphs of arbitrary
order. It generalizes the result previously known only for the case of simple
graphs. By a similar method we establish also the equivalence between
nondeterministic and deterministic hypergraph property testing, answering the
open problem in the area. We introduce a new notion of a cut norm for
hypergraphs of higher order, and employ regularity techniques combined with the
ultralimit method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07104</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07104</id><created>2015-03-24</created><authors><author><keyname>Azmat</keyname><forenames>Freeha</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Stocks</keyname><forenames>Nigel</forenames></author></authors><title>Analysis of Spectrum Occupancy Using Machine Learning Algorithms</title><categories>cs.NI cs.LG</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the spectrum occupancy using different machine
learning techniques. Both supervised techniques (naive Bayesian classifier
(NBC), decision trees (DT), support vector machine (SVM), linear regression
(LR)) and unsupervised algorithm (hidden markov model (HMM)) are studied to
find the best technique with the highest classification accuracy (CA). A
detailed comparison of the supervised and unsupervised algorithms in terms of
the computational time and classification accuracy is performed. The classified
occupancy status is further utilized to evaluate the probability of secondary
user outage for the future time slots, which can be used by system designers to
define spectrum allocation and spectrum sharing policies. Numerical results
show that SVM is the best algorithm among all the supervised and unsupervised
classifiers. Based on this, we proposed a new SVM algorithm by combining it
with fire fly algorithm (FFA), which is shown to outperform all other
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07118</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07118</id><created>2015-03-24</created><updated>2015-04-13</updated><authors><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>On Reverse Pinsker Inequalities</title><categories>cs.IT math.IT math.PR</categories><comments>Version 3 has been submitted to the IEEE Trans. on Information
  Theory, March 2015. Version 4 includes a refinement of the inequalities in
  Theorems 3 and 4 with a new appendix that is included for this purpose, and a
  revision of Section III-B (in respect to these latter refinements). There is
  a text overlap with arXiv:1503.03417 and 1502.06428</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New upper bounds on the relative entropy are derived as a function of the
total variation distance. One bound refines an inequality by Verd\'{u} for
general probability measures. A second bound improves the tightness of an
inequality by Csisz\'{a}r and Talata for arbitrary probability measures that
are defined on a common finite set. The latter result is further extended, for
probability measures on a finite set, leading to an upper bound on the
R\'{e}nyi divergence of an arbitrary non-negative order (including $\infty$) as
a function of the total variation distance. Another lower bound by Verd\'{u} on
the total variation distance, expressed in terms of the distribution of the
relative information, is tightened and it is attained under some conditions.
The effect of these improvements is exemplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07122</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07122</id><created>2015-03-21</created><authors><author><keyname>Jehel</keyname><forenames>Pierre</forenames><affiliation>CEEM, MSSMat</affiliation></author><author><keyname>Cottereau</keyname><forenames>R&#xe9;gis</forenames><affiliation>MSSMat</affiliation></author></authors><title>On damping created by heterogeneous yielding in the numerical analysis
  of nonlinear reinforced concrete frame elements</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Computers &amp; Structures, Elsevier, 2015, pp.2015.03.001</journal-ref><doi>10.1016/j.compstruc.2015.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the dynamic analysis of structural engineering systems, it is common
practice to introduce damping models to reproduce experimentally observed
features. These models, for instance Rayleigh damping, account for the damping
sources in the system altogether and often lack physical basis. We report on an
alternative path for reproducing damping coming from material nonlinear
response through the consideration of the heterogeneous character of material
mechanical properties. The parameterization of that heterogeneity is performed
through a stochastic model. It is shown that such a variability creates the
patterns in the concrete cyclic response that are classically regarded as
source of damping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07125</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07125</id><created>2015-03-24</created><authors><author><keyname>Chen</keyname><forenames>Yuan</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Moura</keyname><forenames>Jose' M. F.</forenames></author></authors><title>Dynamic Attack Detection in Cyber-Physical Systems with Side Initial
  State Information</title><categories>math.OC cs.CR cs.SY</categories><comments>Submitted. Initial Submission: Mar. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the impact of side initial state information on the
detectability of data deception attacks against cyber-physical systems, modeled
as linear time-invariant systems. We assume the attack detector has access to a
linear measurement of the initial system state that cannot be altered by an
attacker. We provide a necessary and sufficient condition for an attack to be
undetectable by any dynamic attack detector under each specific side
information pattern. Additionally, we relate several attack attributes with its
detectability, in particular, the time of first attack to its stealthiness, and
we characterize attacks that can be sustained for arbitrarily long periods
without being detected. Specifically, we define the zero state inducing attack,
the only type of attack that remains dynamically undetectable regardless of the
side initial state information available to the attack detector. We design a
dynamic attack detector that detects all detectable attacks. Finally, we
illustrate our results with an example of a remotely piloted aircraft subject
to data deception attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07132</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07132</id><created>2015-03-24</created><authors><author><keyname>Guimar&#xe3;es</keyname><forenames>Felipe Pontes</forenames></author><author><keyname>Rodrigues</keyname><forenames>Genaina Nunes</forenames></author><author><keyname>Ali</keyname><forenames>Raian</forenames></author><author><keyname>Batista</keyname><forenames>Daniel Mac&#xea;do</forenames></author></authors><title>Pragmatic Requirements for Adaptive Systems: a Goal-Driven Modelling and
  Analysis Approach</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goal-models (GM) have been used in adaptive systems engineering for their
ability to capture the different ways to fulfill the requirements. Contextual
GM (CGM) extend these models with the notion of context and context-dependent
applicability of goals. In this paper, we observe that the interpretation of a
goal achievement is itself context-dependent. Thus, we introduce the notion of
Pragmatic Goals which have a dynamic satisfaction criteria. We also developed
and evaluated an algorithm to decide the Pragmatic CGM's achievability.
Finally, we performed several experiments to evaluate and to compare our
algorithm against human judgment and concluded that the specification of
context-dependent goals' applicability and interpretations make it hard for
domain stakeholders to decide whether the model covers all possibilities, both
in terms of time and accuracy, thus showing the importance and contribution of
our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07133</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07133</id><created>2015-03-24</created><updated>2015-09-16</updated><authors><author><keyname>Ogura</keyname><forenames>Masaki</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Cost-Optimal Switching Protection Strategy in Adaptive Networks</title><categories>cs.SI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a model of network adaptation mechanism to control
spreading processes over switching contact networks, called adaptive
susceptible-infected-susceptible model. The edges in the network model are
randomly removed or added depending on the risk of spread through them. By
analyzing the joint evolution of the spreading dynamics &quot;in the network&quot; and
the structural dynamics &quot;of the network&quot;, we derive conditions on the
adaptation law to control the dynamics of the spread in the resulting switching
network. In contrast with the results in the literature, we allow the initial
topology of the network to be an arbitrary graph. Furthermore, assuming there
is a cost associated to switching edges in the network, we propose an
optimization framework to find the cost-optimal network adaptation law, i.e.,
the cost-optimal edge switching probabilities. Under certain conditions on the
switching costs, we show that the optimal adaptation law can be found using
convex optimization. We illustrate our results with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07139</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07139</id><created>2015-03-24</created><updated>2015-07-27</updated><authors><author><keyname>Schmuck</keyname><forenames>Anne-Kathrin</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author><author><keyname>Raisch</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Comparing Asynchronous $l$-Complete Approximations and Quotient Based
  Abstractions</title><categories>cs.SY cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with a detailed comparison of two different
abstraction techniques for the construction of finite state symbolic models for
controller synthesis of hybrid systems. Namely, we compare quotient based
abstractions (QBA), with different realizations of strongest (asynchronous)
$l$-complete approximations (SAlCA) Even though the idea behind their
construction is very similar, we show that they are generally incomparable both
in terms of behavioral inclusion and similarity relations. We therefore derive
necessary and sufficient conditions for QBA to coincide with particular
realizations of SAlCA. Depending on the original system, either QBA or SAlCA
can be a tighter abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07143</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07143</id><created>2015-03-24</created><authors><author><keyname>Boskos</keyname><forenames>Dimitris</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Robust Connectivity Analysis for Multi-Agent Systems</title><categories>cs.SY</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we provide a decentralized robust control approach, which
guarantees that connectivity of a multi-agent network is maintained when
certain bounded input terms are added to the control strategy. Our main
motivation for this framework is to determine abstractions for multi-agent
systems under coupled constraints which are further exploited for high level
plan generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07150</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07150</id><created>2015-03-24</created><updated>2015-07-09</updated><authors><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Clayton</keyname><forenames>David</forenames></author></authors><title>Acoustic event detection for multiple overlapping similar sources</title><categories>cs.SD</categories><comments>Accepted for WASPAA 2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many current paradigms for acoustic event detection (AED) are not adapted to
the organic variability of natural sounds, and/or they assume a limit on the
number of simultaneous sources: often only one source, or one source of each
type, may be active. These aspects are highly undesirable for applications such
as bird population monitoring. We introduce a simple method modelling the
onsets, durations and offsets of acoustic events to avoid intrinsic limits on
polyphony or on inter-event temporal patterns. We evaluate the method in a case
study with over 3000 zebra finch calls. In comparison against a HMM-based
method we find it more accurate at recovering acoustic events, and more robust
for estimating calling rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07158</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07158</id><created>2015-03-24</created><authors><author><keyname>Wu</keyname><forenames>Junfeng</forenames></author><author><keyname>Li</keyname><forenames>Yuzhe</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Lau</keyname><forenames>Vincent</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Data-Driven Power Control for State Estimation: A Bayesian Inference
  Approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider sensor transmission power control for state estimation, using a
Bayesian inference approach. A sensor node sends its local state estimate to a
remote estimator over an unreliable wireless communication channel with random
data packet drops. As related to packet dropout rate, transmission power is
chosen by the sensor based on the relative importance of the local state
estimate. The proposed power controller is proved to preserve Gaussianity of
local estimate innovation, which enables us to obtain a closed-form solution of
the expected state estimation error covariance. Comparisons with alternative
non data-driven controllers demonstrate performance improvement using our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07159</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07159</id><created>2015-03-24</created><authors><author><keyname>Bhargava</keyname><forenames>Preeti</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Shivsubramani</forenames></author><author><keyname>Agrawala</keyname><forenames>Ashok</forenames></author></authors><title>Modeling context and situations in pervasive computing environments</title><categories>cs.CY cs.AI cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In pervasive computing environments, various entities often have to cooperate
and integrate seamlessly in a \emph{situation} which can, thus, be considered
as an amalgamation of the context of several entities interacting and
coordinating with each other, and often performing one or more activities.
However, none of the existing context models and ontologies address situation
modeling. In this paper, we describe the design, structure and implementation
of a generic, flexible and extensible context ontology called Rover Context
Model Ontology (RoCoMO) for context and situation modeling in pervasive
computing systems and environments. We highlight several limitations of the
existing context models and ontologies, such as lack of provision for
provenance, traceability, quality of context, multiple representation of
contextual information, as well as support for security, privacy and
interoperability, and explain how we are addressing these limitations in our
approach. We also illustrate the applicability and utility of RoCoMO using a
practical and extensive case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07160</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07160</id><created>2015-03-24</created><authors><author><keyname>Nasri</keyname><forenames>Ridha</forenames></author><author><keyname>Jaziri</keyname><forenames>Aymen</forenames></author></authors><title>On the Analytical Tractability of Hexagonal Network Model with Random
  Traffic Distribution</title><categories>cs.NI</categories><comments>Preprint to be submitted to a journal, 12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explicit derivation of interferences in hexagonal wireless networks has been
widely considered intractable and requires extensive computations with system
level simulations. In this paper, we fundamentally tackle this problem and
explicitly evaluate interference factor $f$ for any mobile location $m$ in a
hexagonal network composed of omni-directional or tri-sectorized sites. The
explicit formula of $f$ is a very convergent series on $m$ and involves the use
of hypergeometric and Hurwitz Riemann zeta functions. Besides, we establish
simple identities that well approximate this convergent series and turn out
quite useful compared to other approximations in literature. The derived
expression of $f$ is used to show further that the SINR distribution is
explicit as well and it is provided for any arbitrary mobile location
distribution, reflecting the spatial traffic density in the network. Knowing
explicitly about interferences and SINR distribution is very useful information
in capacity and coverage planning of wireless cellular networks and
particularly for macro-cells' layer that forms almost a regular point pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07170</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07170</id><created>2015-03-24</created><authors><author><keyname>Lampoudi</keyname><forenames>Sotiria</forenames></author><author><keyname>Saunders</keyname><forenames>Eric</forenames></author><author><keyname>Eastman</keyname><forenames>Jason</forenames></author></authors><title>An Integer Linear Programming Solution to the Telescope Network
  Scheduling Problem</title><categories>astro-ph.IM cs.DS</categories><comments>Accepted for publication in the refereed conference proceedings of
  the International Conference on Operations Research and Enterprise Systems
  (ICORES 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Telescope networks are gaining traction due to their promise of higher
resource utilization than single telescopes and as enablers of novel
astronomical observation modes. However, as telescope network sizes increase,
the possibility of scheduling them completely or even semi-manually disappears.
In an earlier paper, a step towards software telescope scheduling was made with
the specification of the Reservation formalism, through the use of which
astronomers can express their complex observation needs and preferences. In
this paper we build on that work. We present a solution to the discretized
version of the problem of scheduling a telescope network. We derive a solvable
integer linear programming (ILP) model based on the Reservation formalism. We
show computational results verifying its correctness, and confirm that our
Gurobi-based implementation can address problems of realistic size. Finally, we
extend the ILP model to also handle the novel observation requests that can be
specified using the more advanced Compound Reservation formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07189</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07189</id><created>2015-03-24</created><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Optimal control in Markov decision processes via distributed
  optimization</title><categories>cs.SY</categories><comments>8 pages, 5 figures, submitted to CDC 2015 conference</comments><msc-class>90C40</msc-class><acm-class>G.3; G.1.6; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal control synthesis in stochastic systems with respect to quantitative
temporal logic constraints can be formulated as linear programming problems.
However, centralized synthesis algorithms do not scale to many practical
systems. To tackle this issue, we propose a decomposition-based distributed
synthesis algorithm. By decomposing a large-scale stochastic system modeled as
a Markov decision process into a collection of interacting sub-systems, the
original control problem is formulated as a linear programming problem with a
sparse constraint matrix, which can be solved through distributed optimization
methods. Additionally, we propose a decomposition algorithm which automatically
exploits, if exists, the modular structure in a given large-scale system. We
illustrate the proposed methods through robotic motion planning examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07192</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07192</id><created>2015-03-24</created><authors><author><keyname>Chapuis</keyname><forenames>Guillaume</forenames></author><author><keyname>Djidjev</keyname><forenames>Hristo</forenames></author></authors><title>Shortest-Path Queries in Planar Graphs on GPU-Accelerated Architectures</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an efficient parallel algorithm for answering shortest-path
queries in planar graphs and implement it on a multi-node CPU/GPU clusters. The
algorithm uses a divide-and-conquer approach for decomposing the input graph
into small and roughly equal subgraphs and constructs a distributed data
structure containing shortest distances within each of those subgraphs and
between their boundary vertices. For a planar graph with $n$ vertices, that
data structure needs $O(n)$ storage per processor and allows queries to be
answered in $O(n^{1/4})$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07193</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07193</id><created>2015-03-24</created><updated>2015-04-20</updated><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author></authors><title>Computational methods for stochastic control with metric interval
  temporal logic specifications</title><categories>cs.SY</categories><comments>8 pages, 6 figures, submitted to IEEE CDC 2015</comments><msc-class>93E20</msc-class><acm-class>I.2.8; G.3; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies an optimal control problem for continuous-time stochastic
systems subject to reachability objectives specified in a subclass of metric
interval temporal logic specifications, a temporal logic with real-time
constraints. We propose a probabilistic method for synthesizing an optimal
control policy that maximizes the probability of satisfying a specification
based on a discrete approximation of the underlying stochastic system. First,
we show that the original problem can be formulated as a stochastic optimal
control problem in a state space augmented with finite memory and states of
some clock variables. Second, we present a numerical method for computing an
optimal policy with which the given specification is satisfied with the maximal
probability in point-based semantics in the discrete approximation of the
underlying system. We show that the policy obtained in the discrete
approximation converges to the optimal one for satisfying the specification in
the continuous or dense-time semantics as the discretization becomes finer in
both state and time. Finally, we illustrate our approach with a robotic motion
planning example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07199</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07199</id><created>2015-03-23</created><authors><author><keyname>Loe</keyname><forenames>Chuan Wen</forenames></author><author><keyname>Jensen</keyname><forenames>Henrik Jeldtoft</forenames></author></authors><title>Revisiting Interval Graphs for Network Science</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vertices of an interval graph represent intervals over a real line where
overlapping intervals denote that their corresponding vertices are adjacent.
This implies that the vertices are measurable by a metric and there exists a
linear structure in the system. The generalization is an embedding of a graph
onto a multi-dimensional Euclidean space and it was used by scientists to study
the multi-relational complexity of ecology. However the research went out of
fashion in the 1980s and was not revisited when Network Science recently
expressed interests with multi-relational networks known as multiplexes. This
paper studies interval graphs from the perspective of Network Science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07206</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07206</id><created>2015-03-24</created><updated>2016-02-13</updated><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Ghazi-Zahedi</keyname><forenames>Keyan</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Geometry and Determinism of Optimal Stationary Control in Partially
  Observable Markov Decision Processes</title><categories>math.OC cs.AI</categories><comments>25 pages, 7 figures</comments><msc-class>93E20, 90C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that for any finite state Markov decision process (MDP)
there is a memoryless deterministic policy that maximizes the expected reward.
For partially observable Markov decision processes (POMDPs), optimal memoryless
policies are generally stochastic. We study the expected reward optimization
problem over the set of memoryless stochastic policies. We formulate this as a
constrained linear optimization problem and develop a corresponding geometric
framework. We show that any POMDP has an optimal memoryless policy of limited
stochasticity, which allows us to reduce the dimensionality of the search
space. Experiments demonstrate that this approach enables better and faster
convergence of the policy gradient on the evaluated systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07211</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07211</id><created>2015-03-24</created><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author></authors><title>Universal Approximation of Markov Kernels by Shallow Stochastic
  Feedforward Networks</title><categories>cs.LG stat.ML</categories><comments>13 pages, 3 figures</comments><msc-class>82C32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish upper bounds for the minimal number of hidden units for which a
binary stochastic feedforward network with sigmoid activation probabilities and
a single hidden layer is a universal approximator of Markov kernels. We show
that each possible probabilistic assignment of the states of $n$ output units,
given the states of $k\geq1$ input units, can be approximated arbitrarily well
by a network with $2^{k-1}(2^{n-1}-1)$ hidden units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07217</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07217</id><created>2015-03-24</created><updated>2015-03-31</updated><authors><author><keyname>Choudhary</keyname><forenames>Shauvik Roy</forenames></author><author><keyname>Gorla</keyname><forenames>Alessandra</forenames></author><author><keyname>Orso</keyname><forenames>Alessandro</forenames></author></authors><title>Automated Test Input Generation for Android: Are We There Yet?</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile applications, often simply called &quot;apps&quot;, are increasingly widespread,
and we use them daily to perform a number of activities. Like all software,
apps must be adequately tested to gain confidence that they behave correctly.
Therefore, in recent years, researchers and practitioners alike have begun to
investigate ways to automate apps testing. In particular, because of Android's
open source nature and its large share of the market, a great deal of research
has been performed on input generation techniques for apps that run on the
Android operating systems. At this point in time, there are in fact a number of
such techniques in the literature, which differ in the way they generate
inputs, the strategy they use to explore the behavior of the app under test,
and the specific heuristics they use. To better understand the strengths and
weaknesses of these existing approaches, and get general insight on ways they
could be made more effective, in this paper we perform a thorough comparison of
the main existing test input generation tools for Android. In our comparison,
we evaluate the effectiveness of these tools, and their corresponding
techniques, according to four metrics: code coverage, ability to detect faults,
ability to work on multiple platforms, and ease of use. Our results provide a
clear picture of the state of the art in input generation for Android apps and
identify future research directions that, if suitably investigated, could lead
to more effective and efficient testing tools for Android.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07218</identifier>
 <datestamp>2015-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07218</id><created>2015-03-24</created><authors><author><keyname>Alamino</keyname><forenames>Roberto C.</forenames></author></authors><title>Measuring Complexity through Average Symmetry</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.IT math.IT</categories><comments>20 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a complexity measure which addresses some conflicting
issues between existing ones by using a new principle - measuring the average
amount of symmetry broken by an object. It attributes low (although different)
complexity to either deterministic or random homogeneous densities and higher
complexity to the intermediate cases. This new measure is easily computable,
breaks the coarse graining paradigm and can be straightforwardly generalised,
including to continuous cases and general networks. By applying this measure to
a series of objects, it is shown that it can be consistently used for both
small scale structures with exact symmetry breaking and large scale patterns,
for which, differently from similar measures, it consistently discriminates
between repetitive patterns, random configurations and self-similar structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07220</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07220</id><created>2015-03-24</created><updated>2015-04-02</updated><authors><author><keyname>Sonu</keyname><forenames>Ekhlas</forenames></author><author><keyname>Chen</keyname><forenames>Yingke</forenames></author><author><keyname>Doshi</keyname><forenames>Prashant</forenames></author></authors><title>Individual Planning in Agent Populations: Exploiting Anonymity and
  Frame-Action Hypergraphs</title><categories>cs.MA cs.AI cs.GT</categories><comments>8 page article plus two page appendix containing proofs in
  Proceedings of 25th International Conference on Autonomous Planning and
  Scheduling, 2015</comments><journal-ref>In Proceedings of 25th International Conference on Automated
  Planning and Scheduling, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive partially observable Markov decision processes (I-POMDP) provide
a formal framework for planning for a self-interested agent in multiagent
settings. An agent operating in a multiagent environment must deliberate about
the actions that other agents may take and the effect these actions have on the
environment and the rewards it receives. Traditional I-POMDPs model this
dependence on the actions of other agents using joint action and model spaces.
Therefore, the solution complexity grows exponentially with the number of
agents thereby complicating scalability. In this paper, we model and extend
anonymity and context-specific independence -- problem structures often present
in agent populations -- for computational gain. We empirically demonstrate the
efficiency from exploiting these problem structures by solving a new multiagent
problem involving more than 1,000 agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1503.07222</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1503.07222</id><created>2015-03-24</created><updated>2015-10-19</updated><authors><author><keyname>Boczar</keyname><forenames>Ross</forenames></author><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Exponential Convergence Bounds using Integral Quadratic Constraints</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of integral quadratic constraints (IQCs) allows verification of
stability and gain-bound properties of systems containing nonlinear or
uncertain elements. Gain bounds often imply exponential stability, but it can
be challenging to compute useful numerical bounds on the exponential decay
rate. In this work, we present a modification of the classical IQC results of
Megretski and Rantzer that leads to a tractable computational procedure for
finding exponential rate certificates.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="74000" completeListSize="102538">1122234|75001</resumptionToken>
</ListRecords>
</OAI-PMH>
