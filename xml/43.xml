<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:09:45Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|42001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5127</identifier>
 <datestamp>2014-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5127</id><created>2013-02-20</created><updated>2014-12-24</updated><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>On the k-Independence Required by Linear Probing and Minwise
  Independence</title><categories>cs.DS</categories><comments>Short preliminary version appeared at ICALP'10. Version 3 fixes typos
  from first and second version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that linear probing requires 5-independent hash functions for
expected constant-time performance, matching an upper bound of [Pagh et al.
STOC'07]. More precisely, we construct a 4-independent hash functions yielding
expected logarithmic search time.
  For (1+{\epsilon})-approximate minwise independence, we show that \Omega(log
1/{\epsilon})-independent hash functions are required, matching an upper bound
of [Indyk, SODA'99].
  We also show that the very fast 2-independent multiply-shift scheme of
Dietzfelbinger [STACS'96] fails badly in both applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5130</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5130</id><created>2013-02-20</created><authors><author><keyname>Tolba</keyname><forenames>A. S.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author></authors><title>Quantum-inspired Huffman Coding</title><categories>cs.IT cs.ET math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Huffman Compression, also known as Huffman Coding, is one of many compression
techniques in use today. The two important features of Huffman coding are
instantaneousness that is the codes can be interpreted as soon as they are
received and variable length that is a most frequent symbol has length smaller
than a less frequent symbol. The traditional Huffman coding has two procedures:
constructing a tree in O(n^2) and then traversing it in O(n). Quantum computing
is a promising approach of computation that is based on equations from Quantum
Mechanics. Instantaneousness and variable length features are difficult to
generalize to the quantum case. The quantum coding field is pioneered by
Schumacher works on block coding scheme. To encode N signals sequentially, it
requires O(N3) computational steps. The encoding and decoding processes are far
from instantaneous. Moreover, the lengths of all the codewords are the same. A
Huffman-coding-inspired scheme for the storage of quantum information takes
O(N(log N)a) computational steps for a sequential implementation on
non-parallel machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5133</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5133</id><created>2013-02-20</created><authors><author><keyname>Tolba</keyname><forenames>A. S.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author></authors><title>Q#, a quantum computation package for the .NET platform</title><categories>cs.ET cs.MS cs.PL quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computing is a promising approach of computation that is based on
equations from Quantum Mechanics. A simulator for quantum algorithms must be
capable of performing heavy mathematical matrix transforms. The design of the
simulator itself takes one of three forms: Quantum Turing Machine, Network
Model or circuit model of connected gates or, Quantum Programming Language,
yet, some simulators are hybrid. We studied previous simulators and then we
adopt features from three simulators of different implementation languages,
different paradigms, and for different platforms. They are Quantum Computing
Language (QCL), QUASI, and Quantum Optics Toolbox for Matlab 5. Our simulator
for quantum algorithms takes the form of a package or a programming library for
Quantum computing, with a case study showing the ability of using it in the
circuit model. The .NET is a promising platform for computing. VB.NET is an
easy, high productive programming language with the full power and
functionality provided by the .NET framework. It is highly readable, writeable,
and flexible language, compared to another language such as C#.NET in many
aspects. We adopted VB.NET although its shortage in built-in mathematical
complex and matrix operations, compared to Matlab. For implementation, we first
built a mathematical core of matrix operations. Then, we built a quantum core
which contains: basic qubits and register operations, basic 1D, 2D, and 3D
quantum gates, and multi-view visualization of the quantum state, then a window
for demos to show you how to use and get the most of the package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5145</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5145</id><created>2013-02-20</created><updated>2013-03-04</updated><authors><author><keyname>Chiang</keyname><forenames>Kai-Yang</forenames></author><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author><author><keyname>Natarajan</keyname><forenames>Nagarajan</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Prediction and Clustering in Signed Networks: A Local to Global
  Perspective</title><categories>cs.SI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of social networks is a burgeoning research area. However, most
existing work deals with networks that simply encode whether relationships
exist or not. In contrast, relationships in signed networks can be positive
(&quot;like&quot;, &quot;trust&quot;) or negative (&quot;dislike&quot;, &quot;distrust&quot;). The theory of social
balance shows that signed networks tend to conform to some local patterns that,
in turn, induce certain global characteristics. In this paper, we exploit both
local as well as global aspects of social balance theory for two fundamental
problems in the analysis of signed networks: sign prediction and clustering.
Motivated by local patterns of social balance, we first propose two families of
sign prediction methods: measures of social imbalance (MOIs), and supervised
learning using high order cycles (HOCs). These methods predict signs of edges
based on triangles and \ell-cycles for relatively small values of \ell.
Interestingly, by examining measures of social imbalance, we show that the
classic Katz measure, which is used widely in unsigned link prediction,
actually has a balance theoretic interpretation when applied to signed
networks. Furthermore, motivated by the global structure of balanced networks,
we propose an effective low rank modeling approach for both sign prediction and
clustering. For the low rank modeling approach, we provide theoretical
performance guarantees via convex relaxations, scale it up to large problem
sizes using a matrix factorization based algorithm, and provide extensive
experimental validation including comparisons with local approaches. Our
experimental results indicate that, by adopting a more global viewpoint of
balance structure, we get significant performance and computational gains in
prediction and clustering tasks on signed networks. Our work therefore
highlights the usefulness of the global aspect of balance theory for the
analysis of signed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5150</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5150</id><created>2013-02-20</created><updated>2013-06-30</updated><authors><author><keyname>Matsutani</keyname><forenames>Shigeki</forenames></author><author><keyname>Shimosako</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Measuring Agglomeration of Agglomerated Particles Pictures</title><categories>cs.CE math-ph math.AT math.MP math.NA math.PR</categories><comments>agglomeration, digital image processing procedure, Euler number</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we introduce a novel geometrical index $\delta_{agg}$, which
is associated with the Euler number and is obtained by an image processing
procedure for a given digital picture of aggregated particles such that
$\delta_{agg}$ exhibits the degree of the agglomerations of the particles. In
the previous work (Matsutani, Shimosako, Wang, Appl.Math.Modeling {\bf{37}}
(2013), 4007-4022), we proposed an algorithm to construct a picture of
agglomerated particles as a Monte-Carlo simulation whose agglomeration degree
is controlled by $\gamma_{agg} \in (0,1)$. By applying the image processing
procedure to the pictures of the agglomeration particles constructed following
the algorithm, we show that $\delta_{agg}$ statistically reproduces the
agglomeration parameter $\gamma_{agg}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5153</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5153</id><created>2013-02-20</created><updated>2013-05-27</updated><authors><author><keyname>Ghayoori</keyname><forenames>Arash</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Constructing Polar Codes Using Iterative Bit-Channel Upgrading</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1105.6164 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The definition of polar codes given by Arikan is explicit, but the
construction complexity is an issue. This is due to the exponential growth in
the size of the output alphabet of the bit-channels as the codeword length
increases. Tal and Vardy recently presented a method for constructing polar
codes which controls this growth. They approximated each bit-channel with a
better channel and a worse channel while reducing the alphabet size. They
constructed a polar code based on the worse channel and used the better channel
to measure the distance from the optimal channel. This paper considers the
knowledge gained from the perspective of the better channel. A method is
presented using iterative upgrading of the bit-channels which successively
results in a channel closer to the original one. It is shown that this approach
can be used to obtain a channel arbitrarily close to the original channel, and
therefore to the optimal construction of a polar code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5158</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5158</id><created>2013-02-20</created><authors><author><keyname>Kumarasamy</keyname><forenames>Saravanan</forenames></author><author><keyname>Asokan</keyname><forenames>Dr. R.</forenames></author></authors><title>An Efficient Detection Mechanism for Distributed Denial of Service
  (DDoS) Attack</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1103.3333,
  arXiv:1101.2715 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks have
emerged as a popular means of causing collection particular overhaul
disruptions, often for total periods of instance. The relative ease and low
costs of initiation such attacks, supplemented by the present insufficient sate
of any feasible defense method, have made them one of the top threats to the
Internet centre of population nowadays. Since the rising attractiveness of
web-based applications has led to quite a lot of significant services being
provided more than the Internet, it is very important to monitor the network
transfer so as to stop hateful attackers from depleting the assets of the
network and denying services to rightful users. The most important drawbacks of
the presently existing defense mechanisms and propose a new-fangled mechanism
for defending a web-server against a DDoS attack. In the proposed mechanism,
incoming traffic to the server is always monitored and some irregular rise in
the inbound traffic is without delay detected. The detection algorithm is based
on a statistical analysis of the inbound traffic on the server and a robust
suggestion testing structure. While the detection procedure is on, the sessions
from the rightful sources are not disrupted and the load on the server is
restored to the usual level by overcrowding the traffic from the attacking
sources. The accurate modules employ multifaceted detection logic and hence
involve additional overhead for their execution. On the other hand, they have
very huge detection accuracy. Simulations approved on the proposed mechanism
have produced results that show efficiency of the proposed defense mechanism
against DDoS attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5161</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5161</id><created>2013-02-20</created><updated>2013-11-28</updated><authors><author><keyname>Wei</keyname><forenames>Hengfeng</forenames></author><author><keyname>De Biasi</keyname><forenames>Marzio</forenames></author><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Cao</keyname><forenames>Jiannong</forenames></author><author><keyname>Lu</keyname><forenames>Jian</forenames></author></authors><title>Verifying PRAM Consistency over Read/Write Traces of Data Replicas</title><categories>cs.DC</categories><comments>11 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data replication technologies enable efficient and highly-available data
access, thus gaining more and more interests in both the academia and the
industry. However, data replication introduces the problem of data consistency.
Modern commercial data replication systems often provide weak consistency for
high availability under certain failure scenarios. An important weak
consistency is Pipelined-RAM (PRAM) consistency. It allows different processes
to hold different views of data. To determine whether a data replication system
indeed provides PRAM consistency, we study the problem of Verifying PRAM
Consistency over read/write traces (or VPC, for short).
  We first identify four variants of VPC according to a) whether there are
Multiple shared variables (or one Single variable), and b) whether write
operations can assign Duplicate values (or only Unique values) for each shared
variable; the four variants are labeled VPC-SU, VPC-MU, VPC-SD, and VPC-MD.
Second, we present a simple VPC-MU algorithm, called RW-CLOSURE. It constructs
an operation graph $\mathcal{G}$ by iteratively adding edges according to three
rules. Its time complexity is $O(n^5)$, where n is the number of operations in
the trace. Third, we present an improved VPC-MU algorithm, called READ-CENTRIC,
with time complexity $O(n^4)$. Basically it attempts to construct the operation
graph $\mathcal{G}$ in an incremental and efficient way. Its correctness is
based on that of RW-CLOSURE. Finally, we prove that VPC-SD (so is VPC-MD) is
$\sf{NP}$-complete by reducing the strongly $\sf{NP}$-complete problem
3-PARTITION to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5166</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5166</id><created>2013-02-20</created><authors><author><keyname>Van Nguyen</keyname><forenames>Thuy</forenames></author><author><keyname>Nosratinia</keyname><forenames>Aria</forenames></author><author><keyname>Divsalar</keyname><forenames>Dariush</forenames></author></authors><title>Rate-Compatible Short-Length Protograph LDPC Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper produces a rate-compatible protograph LDPC code at 1k information
blocklength with superior performance in both waterfall and error floor
regions. The design of such codes has proved difficult in the past because the
constraints imposed by structured design (protographs), rate-compatibility, as
well as small block length, are not easily satisfied together. For example, as
the block length decreases, the predominance of decoding threshold as the main
parameter in coding design is reduced, thus complicating the search for good
codes. Our rate-compatible protograph codes have rates ranging from 1/3 to 4/5
and show no error floor down to $10^{-6}$ FER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5168</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5168</id><created>2013-02-20</created><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>q-ary Compressive Sensing</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce q-ary compressive sensing, an extension of 1-bit compressive
sensing. We propose a novel sensing mechanism and a corresponding recovery
procedure. The recovery properties of the proposed approach are analyzed both
theoretically and empirically. Results in 1-bit compressive sensing are
recovered as a special case. Our theoretical results suggest a tradeoff between
the quantization parameter q, and the number of measurements m in the control
of the error of the resulting recovery algorithm, as well its robustness to
noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5169</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5169</id><created>2013-02-20</created><authors><author><keyname>Colombo</keyname><forenames>Christian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Francalanza</keyname><forenames>Adrian</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Mizzi</keyname><forenames>Ruth</forenames><affiliation>University of Malta</affiliation></author><author><keyname>Pace</keyname><forenames>Gordon J.</forenames><affiliation>University of Malta</affiliation></author></authors><title>Extensible Technology-Agnostic Runtime Verification</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2013, arXiv:1302.4780</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 108, 2013, pp. 1-15</journal-ref><doi>10.4204/EPTCS.108.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With numerous specialised technologies available to industry, it has become
increasingly frequent for computer systems to be composed of heterogeneous
components built over, and using, different technologies and languages. While
this enables developers to use the appropriate technologies for specific
contexts, it becomes more challenging to ensure the correctness of the overall
system. In this paper we propose a framework to enable extensible technology
agnostic runtime verification and we present an extension of polyLarva, a
runtime-verification tool able to handle the monitoring of
heterogeneous-component systems. The approach is then applied to a case study
of a component-based artefact using different technologies, namely C and Java.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5170</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5170</id><created>2013-02-20</created><authors><author><keyname>Sieverding</keyname><forenames>Sven</forenames></author><author><keyname>Ellen</keyname><forenames>Christian</forenames></author><author><keyname>Battram</keyname><forenames>Peter</forenames></author></authors><title>Sequence Diagram Test Case Specification and Virtual Integration
  Analysis using Timed-Arc Petri Nets</title><categories>cs.SE</categories><comments>In Proceedings FESCA 2013, arXiv:1302.4780</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 108, 2013, pp. 17-31</journal-ref><doi>10.4204/EPTCS.108.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formally define Test Case Sequence Diagrams (TCSD) as an
easy-to-use means to specify test cases for components including timing
constraints. These test cases are modeled using the UML2 syntax and can be
specified by standard UML-modeling-tools. In a component-based design an early
identification of errors can be achieved by a virtual integration of components
before the actual system is build. We define such a procedure which integrates
the individual test cases of the components according to the interconnections
of a given architecture and checks if all specified communication sequences are
consistent. Therefore, we formally define the transformation of TCSD into
timed-arc Petri nets and a process for the combination of these nets. The
applicability of our approach is demonstrated on an avionic use case from the
ARP4761 standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5171</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5171</id><created>2013-02-20</created><authors><author><keyname>Arcelli</keyname><forenames>Davide</forenames><affiliation>DISIM</affiliation></author><author><keyname>Cortellessa</keyname><forenames>Vittorio</forenames><affiliation>DISIM</affiliation></author></authors><title>Software model refactoring based on performance analysis: better working
  on software or performance side?</title><categories>cs.PF cs.SE</categories><comments>In Proceedings FESCA 2013, arXiv:1302.4780</comments><proxy>EPTCS</proxy><acm-class>Performance, Experimentation</acm-class><journal-ref>EPTCS 108, 2013, pp. 33-47</journal-ref><doi>10.4204/EPTCS.108.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several approaches have been introduced in the last few years to tackle the
problem of interpreting model-based performance analysis results and
translating them into architectural feedback. Typically the interpretation can
take place by browsing either the software model or the performance model. In
this paper, we compare two approaches that we have recently introduced for this
goal: one based on the detection and solution of performance antipatterns, and
another one based on bidirectional model transformations between software and
performance models. We apply both approaches to the same example in order to
illustrate the differences in the obtained performance results. Thereafter, we
raise the level of abstraction and we discuss the pros and cons of working on
the software side and on the performance side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5173</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5173</id><created>2013-02-20</created><authors><author><keyname>Gruhn</keyname><forenames>Helena</forenames></author><author><keyname>Glesner</keyname><forenames>Sabine</forenames></author></authors><title>Towards a Formal Framework for Mobile, Service-Oriented Sensor-Actuator
  Networks</title><categories>cs.NI cs.SE</categories><comments>In Proceedings FESCA 2013, arXiv:1302.4780</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 108, 2013, pp. 49-62</journal-ref><doi>10.4204/EPTCS.108.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-oriented sensor-actuator networks (SOSANETs) are deployed in
health-critical applications like patient monitoring and have to fulfill strong
safety requirements. However, a framework for the rigorous formal modeling and
analysis of SOSANETs does not exist. In particular, there is currently no
support for the verification of correct network behavior after node failure or
loss/addition of communication links. To overcome this problem, we propose a
formal framework for SOSANETs. The main idea is to base our framework on the
\pi-calculus, a formally defined, compositional and well-established formalism.
We choose KLAIM, an existing formal language based on the \pi-calculus as the
foundation for our framework. With that, we are able to formally model SOSANETs
with possible topology changes and network failures. This provides the basis
for our future work on prediction, analysis and verification of the network
behavior of these systems. Furthermore, we illustrate the real-life
applicability of this approach by modeling and extending a use case scenario
from the medical domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5174</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5174</id><created>2013-02-20</created><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames></author><author><keyname>Terrell</keyname><forenames>Jeffrey</forenames></author></authors><title>Assembling the Proofs of Ordered Model Transformations</title><categories>cs.LO</categories><comments>In Proceedings FESCA 2013, arXiv:1302.4780</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 108, 2013, pp. 63-77</journal-ref><doi>10.4204/EPTCS.108.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In model-driven development, an ordered model transformation is a nested set
of transformations between source and target classes, in which each
transformation is governed by its own pre and post- conditions, but
structurally dependent on its parent. Following the
proofs-as-model-transformations approach, in this paper we consider a
formalisation in Constructive Type Theory of the concepts of model and model
transformation, and show how the correctness proofs of potentially large
ordered model transformations can be systematically assembled from the proofs
of the specifications of their parts, making them easier to derive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5175</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5175</id><created>2013-02-20</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames><affiliation>fortiss GmbH</affiliation></author></authors><title>Towards a Framework for Behavioral Specifications of OSGi Components</title><categories>cs.SE cs.PL</categories><comments>In Proceedings FESCA 2013, arXiv:1302.4780</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 108, 2013, pp. 79-93</journal-ref><doi>10.4204/EPTCS.108.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present work on behavioral specifications of OSGi components. Our
behavioral specifications are based on finite automata like formalisms.
Behavioral specifications can be used to find appropriate components to
interact with, detect incompatibilities between communication protocols of
components and potential problems resulting from the interplay of
non-deterministic component specifications. These operations can be carried out
during development and at runtime of a system. Furthermore, we describe work
carried out using the Eclipse based implementation of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5177</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5177</id><created>2013-02-20</created><authors><author><keyname>Davies</keyname><forenames>Todd</forenames></author><author><keyname>Chandler</keyname><forenames>Reid</forenames></author></authors><title>Online Deliberation Design: Choices, Criteria, and Evidence</title><categories>cs.HC cs.CY</categories><comments>Appeared in Tina Nabatchi, John Gastil, G. Michael Weiksner, and Matt
  Leighninger (Editors), Democracy in Motion: Evaluating the Practice and
  Impact of Deliberative Civic Engagement, Oxford University Press, October
  2012, pp. 103-131; 32 pages, 2 tables</comments><acm-class>H.5.3; K.4.0; H.4.3; H.1.2; H.5.1</acm-class><doi>10.1093/acprof:oso/9780199899265.003.0006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter reviews empirical evidence bearing on the design of online
forums for deliberative civic engagement. Dimensions of design are defined for
different aspects of the deliberation: its purpose, the target population, the
spatiotemporal distance separating participants, the communication medium, and
the deliberative process to be followed. After a brief overview of criteria for
evaluating different design options, empirical findings are organized around
design choices. Research has evolved away from treating technology for online
deliberation dichotomously (either present or not) toward nuanced findings that
differentiate between technological features, ways of using them, and cultural
settings. The effectiveness of online deliberation depends on how well the
communicative environment is matched to the deliberative task. Tradeoffs, e.g.
between rich and lean media and between anonymous and identifiable
participation, suggest different designs depending on the purpose and
participants. Findings are limited by existing technologies, and may change as
technologies and users co-evolve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5181</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5181</id><created>2013-02-20</created><authors><author><keyname>Burgin</keyname><forenames>Mark</forenames></author></authors><title>Basic Classes of Grammars with Prohibition</title><categories>cs.FL cs.CL</categories><comments>2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical tool for natural language modeling and development of
human-machine interaction is developed in the context of formal grammars and
languages. A new type of formal grammars, called grammars with prohibition, is
introduced. Grammars with prohibition provide more powerful tools for natural
language generation and better describe processes of language learning than the
conventional formal grammars. Here we study relations between languages
generated by different grammars with prohibition based on conventional types of
formal grammars such as context-free or context sensitive grammars. Besides, we
compare languages generated by different grammars with prohibition and
languages generated by conventional formal grammars. In particular, it is
demonstrated that they have essentially higher computational power and
expressive possibilities in comparison with the conventional formal grammars.
Thus, while conventional formal grammars are recursive and subrecursive
algorithms, many classes of grammars with prohibition are superrecursive
algorithms. Results presented in this work are aimed at the development of
human-machine interaction, modeling natural languages, empowerment of
programming languages, computer simulation, better software systems, and theory
of recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5186</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5186</id><created>2013-02-21</created><updated>2014-02-10</updated><authors><author><keyname>Gimenez</keyname><forenames>Javier</forenames></author><author><keyname>Martinez</keyname><forenames>Jorge</forenames></author><author><keyname>Flesia</keyname><forenames>Ana Georgina</forenames></author></authors><title>Unsupervised edge map scoring: a statistical complexity approach</title><categories>cs.CV stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new Statistical Complexity Measure (SCM) to qualify edge maps
without Ground Truth (GT) knowledge. The measure is the product of two indices,
an \emph{Equilibrium} index $\mathcal{E}$ obtained by projecting the edge map
into a family of edge patterns, and an \emph{Entropy} index $\mathcal{H}$,
defined as a function of the Kolmogorov Smirnov (KS) statistic.
  This new measure can be used for performance characterization which includes:
(i)~the specific evaluation of an algorithm (intra-technique process) in order
to identify its best parameters, and (ii)~the comparison of different
algorithms (inter-technique process) in order to classify them according to
their quality.
  Results made over images of the South Florida and Berkeley databases show
that our approach significantly improves over Pratt's Figure of Merit (PFoM)
which is the objective reference-based edge map evaluation standard, as it
takes into account more features in its evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5189</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5189</id><created>2013-02-21</created><authors><author><keyname>Prasad</keyname><forenames>Dilip K.</forenames></author></authors><title>Object Detection in Real Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection and recognition are important problems in computer vision.
Since these problems are meta-heuristic, despite a lot of research, practically
usable, intelligent, real-time, and dynamic object detection/recognition
methods are still unavailable. We propose a new object detection/recognition
method, which improves over the existing methods in every stage of the object
detection/recognition process. In addition to the usual features, we propose to
use geometric shapes, like linear cues, ellipses and quadrangles, as additional
features. The full potential of geometric cues is exploited by using them to
extract other features in a robust, computationally efficient, and less
meta-heuristic manner. We also propose a new hierarchical codebook, which
provides good generalization and discriminative properties. The codebook
enables fast multi-path inference mechanisms based on propagation of
conditional likelihoods, that make it robust to occlusion and noise. It has the
capability of dynamic learning. We also propose a new learning method that has
generative and discriminative learning capabilities, does not need large and
fully supervised training dataset, and is capable of online learning. The
preliminary work of detecting geometric shapes in real images has been
completed. This preliminary work is the focus of this report. Future path for
realizing the proposed object detection/recognition method is also discussed in
brief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5192</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5192</id><created>2013-02-21</created><updated>2013-06-26</updated><authors><author><keyname>Esmaili</keyname><forenames>Kyumars Sheykh</forenames></author><author><keyname>Pamies-Juarez</keyname><forenames>Lluis</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>The CORE Storage Primitive: Cross-Object Redundancy for Efficient Data
  Repair &amp; Access in Erasure Coded Storage</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erasure codes are an integral part of many distributed storage systems aimed
at Big Data, since they provide high fault-tolerance for low overheads.
However, traditional erasure codes are inefficient on reading stored data in
degraded environments (when nodes might be unavailable), and on replenishing
lost data (vital for long term resilience). Consequently, novel codes optimized
to cope with distributed storage system nuances are vigorously being
researched. In this paper, we take an engineering alternative, exploring the
use of simple and mature techniques -juxtaposing a standard erasure code with
RAID-4 like parity. We carry out an analytical study to determine the efficacy
of this approach over traditional as well as some novel codes. We build upon
this study to design CORE, a general storage primitive that we integrate into
HDFS. We benchmark this implementation in a proprietary cluster and in EC2. Our
experiments show that compared to traditional erasure codes, CORE uses 50% less
bandwidth and is up to 75% faster while recovering a single failed node, while
the gains are respectively 15% and 60% for double node failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5195</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5195</id><created>2013-02-21</created><authors><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Good</keyname><forenames>Alice</forenames></author></authors><title>Strategic Advantage in Web Tourism Promotion: An e-Commerce Strategy for
  Developing Countries</title><categories>cs.CY</categories><comments>Journal article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research informs the means to develop an e-commerce strategy for web
based tourism promotion of hotels located in developing countries. The study
explored the aspects related to the use of information systems in web based
tourism promotion, along with a focus on the organizational factors affecting
the use of e-commerce strategy. Interviews were conducted with the managers of
selected five sample hotels located in Sri Lanka. A structured web content
analysis was undertaken for all five sample hotels to trace process level data
on the e-commerce web content. Specific aspects of web content analysis include
interactivity, trust, information and value adding features. Instrument for web
content analysis was developed by the researcher. The outcome of research
produces an outline for developing an e-commerce strategy for hotels located in
developing countries to achieve strategic advantages in web based tourism
promotion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5196</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5196</id><created>2013-02-21</created><authors><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Good</keyname><forenames>Alice</forenames></author></authors><title>Critical Success Factors for Positive User Experience in Hotel Websites:
  Applying Herzberg's Two Factor Theory for User Experience Modeling</title><categories>cs.CY cs.HC</categories><comments>Journal article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research presents the development of a critical success factor matrix
for increasing positive user experience of hotel websites based upon user
ratings. Firstly, a number of critical success factors for web usability have
been identified through the initial literature review. Secondly, hotel websites
were surveyed in terms of critical success factors identified through the
literature review. Thirdly, Herzberg's motivation theory has been applied to
the user rating and the critical success factors were categorized into two
areas. Finally, the critical success factor matrix has been developed using the
two main sets of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5198</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5198</id><created>2013-02-21</created><authors><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Good</keyname><forenames>Alice</forenames></author></authors><title>Implications for Improving Accessibility to E-Commerce Websites in
  Developing Countries: A Subjective Study of Sri Lankan Hotel Websites</title><categories>cs.CY cs.HC</categories><comments>Conference paper. Journal version is arXiv:1302.5491</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a subjective study of Sri Lankan
hotel websites. A web survey and a web content analysis were conducted as the
methods to elicit data on web accessibility. Factors preventing accessibility
were hypothesized as an initial experiment. Hazardous design elements are
identified through web content analysis, the results of which are utilized to
develop specific implications for improving web accessibility. The hypothesis
tests show that there is no significant correlation between accessibility and
geographical or economic factors. However, physical impairments of users have a
considerable influence on the accessibility. Especially, visual and mobility
impaired users experience poor accessibility. Poor readability and less
navigable page designs are two observable issues, which pose threats to
accessibility. The lack of conformance to W3C accessibility guidelines and the
poor design process are the specific shortcomings which reduce the overall
accessibility. Guidelines aim to improve the accessibility of sites with a
strategic focus. Further enhancements are suggested with adherence to
principles and user centered design and developing customizable web portals
compatible for connections with differing speeds. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5199</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5199</id><created>2013-02-21</created><authors><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Good</keyname><forenames>Alice</forenames></author></authors><title>A Virtual World Model to Enhance Tourism Destination Accessibility in
  Developing Countries</title><categories>cs.CY cs.HC</categories><comments>Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of destination accessibility is a vital concern in the
sustainable tourism development in the emerging regions due to the increasing
numbers of tourism business growth in the recent times. Tourism is one of the
potential foreign exchange earning sectors, which place sustainability as one
of the main success metrics for benchmarking the industry's overall
development. On the other hand, there are several destinations, which are
inaccessible to tourists due to several reasons. Underutilization of potential
destinations in both pre purchase and consumption stages is a strategic
disadvantage for emerging countries on leading their tourism industry towards
sustainability. A virtual world model to increase the destination accessibility
of tourism products has been proposed. The model has to be designed with visual
and auditory experience to tourists. The model is expected to enhance the
accessibility of destinations for users of different categories. Elderly users,
users with panic disorders, users with mobility impairments also will be able
to enjoy traveling experience just same as other, through the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5200</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5200</id><created>2013-02-21</created><authors><author><keyname>Good</keyname><forenames>Alice</forenames></author><author><keyname>Wilson</keyname><forenames>Clare</forenames></author><author><keyname>Ancient</keyname><forenames>Claire</forenames></author><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author></authors><title>A Proposal To Support Wellbeing in People With Borderline Personality
  Disorder: Applying Reminiscent Theory in a Mobile App</title><categories>cs.CY cs.HC</categories><comments>Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the research draws upon reminiscence therapy, which is used in
treating dementia, as an applied theory to promote well being in people who
experience low moods. The application proposed here aims to promote wellbeing
for people suffering from mood disorders and dementia but could potentially be
used to enhance wellbeing for many types of users. Use of the application is
anticipated to improve mood in a group of users where severe emotional problems
are prevalent. The research aims to evaluate the effectiveness of a
reminiscence based application in promoting well being in people specifically
with Borderline Personality Disorder (BPD). The long term objective of this
research is to establish the effectiveness of reminiscence theory on user
groups aside from dementia, particularly other mental illnesses. The research
advocates involving end users within the design process both to inform and
evaluate the development of a mobile and tablet application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5205</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5205</id><created>2013-02-21</created><authors><author><keyname>Naudts</keyname><forenames>Jan</forenames></author><author><keyname>Anthonis</keyname><forenames>Ben</forenames></author></authors><title>The exponential family in abstract information theory</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce generalized notions of a divergence function and a Fisher
information matrix. We propose to generalize the notion of an exponential
family of models by reformulating it in terms of the Fisher information matrix.
Our methods are those of information geometry. The context is general enough to
include applications from outside statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5215</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5215</id><created>2013-02-21</created><authors><author><keyname>Anandaraj</keyname><forenames>A.</forenames></author><author><keyname>Kalaivani</keyname><forenames>P.</forenames></author><author><keyname>Rameshkumar</keyname><forenames>V.</forenames></author></authors><title>Development Of Ontology-Based Intelligent System For Software Testing</title><categories>cs.AI cs.SE</categories><comments>International Journal of Communication, Computation and Innovation. /
  Volume 2, Issue 2, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing is a prime factor in software industry. Besides knowing the
importance of testing, only limited time is allocated for teaching it. It will
be more efficient if testing is taught simultaneously with programming
foundations. This integrated learning of testing techniques and programming
allows the programmers to perform in a better way and this leads to the
improvement of the performance of the industry progress. In this paper, a
technique named ontology is introduced, it first defines the various testing
process in hierarchy and define relationships among them, to share and reuse
the knowledge that is captured, secondly metadata is created by natural
language processing and finally, the application use ontologies to support test
management, it act as knowledge base for multiple environment with the
integrated teaching of programming foundation and testing concepts. Keywords:
Meta Data, Ontology, Software Testing, Integration, Programming Foundations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5226</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5226</id><created>2013-02-21</created><updated>2014-11-24</updated><authors><author><keyname>Gaubert</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Qu</keyname><forenames>Zheng</forenames></author></authors><title>Dobrushin ergodicity coefficient for Markov operators on cones, and
  beyond</title><categories>math.OA cs.MA math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of classical consensus algorithms relies on contraction
properties of adjoints of Markov operators, with respect to Hilbert's
projective metric or to a related family of seminorms (Hopf's oscillation or
Hilbert's seminorm). We generalize these properties to abstract consensus
operators over normal cones, which include the unital completely positive maps
(Kraus operators) arising in quantum information theory. In particular, we show
that the contraction rate of such operators, with respect to the Hopf
oscillation seminorm, is given by an analogue of Dobrushin's ergodicity
coefficient. We derive from this result a characterization of the contraction
rate of a non-linear flow, with respect to Hopf's oscillation seminorm and to
Hilbert's projective metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5235</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5235</id><created>2013-02-21</created><updated>2013-03-01</updated><authors><author><keyname>Guille</keyname><forenames>Adrien</forenames></author><author><keyname>Hacid</keyname><forenames>Hakim</forenames></author><author><keyname>Favre</keyname><forenames>C&#xe9;cile</forenames></author></authors><title>Predicting the Temporal Dynamics of Information Diffusion in Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages; (corrected typos)</comments><report-no>ERIC Laboratory Report RI-ERIC-13/001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks play a major role in the spread of information at very
large scale and it becomes essential to provide means to analyse this
phenomenon. In this paper we address the issue of predicting the temporal
dynamics of the information diffusion process. We develop a graph-based
approach built on the assumption that the macroscopic dynamics of the spreading
process are explained by the topology of the network and the interactions that
occur through it, between pairs of users, on the basis of properties at the
microscopic level. We introduce a generic model, called T-BaSIC, and describe
how to estimate its parameters from users behaviours using machine learning
techniques. Contrary to classical approaches where the parameters are fixed in
advance, T-BaSIC's parameters are functions depending of time, which permit to
better approximate and adapt to the diffusion phenomenon observed in online
social networks. Our proposal has been validated on real Twitter datasets.
Experiments show that our approach is able to capture the particular patterns
of diffusion depending of the studied sub-networks of users and topics. The
results corroborate the &quot;two-step&quot; theory (1955) that states that information
flows from media to a few &quot;opinion leaders&quot; who then transfer it to the mass
population via social networks and show that it applies in the online context.
This work also highlights interesting recommendations for future
investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5254</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5254</id><created>2013-02-21</created><authors><author><keyname>Ferrarotti</keyname><forenames>F.</forenames></author><author><keyname>Ren</keyname><forenames>W.</forenames></author><author><keyname>Torres</keyname><forenames>J. M. Turull</forenames></author></authors><title>Expressing Properties in Second and Third Order Logic: Hypercube Graphs
  and SATQBF</title><categories>cs.LO</categories><comments>Pre-print of article submitted to an special issue of the Logic
  Journal of the IGPL with selected papers from the 16th Brazilian Logic
  Conference</comments><msc-class>68Q19</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It follows from the famous Fagin's theorem that all problems in NP are
expressible in existential second-order logic (ESO), and vice versa. Indeed,
there are well-known ESO characterizations of NP-complete problems such as
3-colorability, Hamiltonicity and clique. Furthermore, the ESO sentences that
characterize those problems are simple and elegant. However, there are also NP
problems that do not seem to possess equally simple and elegant ESO
characterizations. In this work, we are mainly interested in this latter class
of problems. In particular, we characterize in second-order logic the class of
hypercube graphs and the classes SATQBF_k of satisfiable quantified Boolean
formulae with k alternations of quantifiers. We also provide detailed
descriptions of the strategies followed to obtain the corresponding nontrivial
second-order sentences. Finally, we sketch a third-order logic sentence that
defines the class SATQBF = \bigcup_{k \geq 1} SATQBF_k. The sub-formulae used
in the construction of these complex second- and third-order logic sentences,
are good candidates to form part of a library of formulae. Same as libraries of
frequently used functions simplify the writing of complex computer programs, a
library of formulae could potentially simplify the writing of complex second-
and third-order queries, minimizing the probability of error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5280</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5280</id><created>2013-02-21</created><updated>2013-06-27</updated><authors><author><keyname>Yang</keyname><forenames>Hyun Jong</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author><author><keyname>Paulraj</keyname><forenames>Arogyaswami</forenames></author></authors><title>Opportunistic Interference Alignment for MIMO Interfering
  Multiple-Access Channels</title><categories>cs.IT math.IT</categories><comments>24 pages, 9 figures, 1 table, to appear in IEEE Transactions on
  Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the $K$-cell multiple-input multiple-output (MIMO) interfering
multiple-access channel (IMAC) with time-invariant channel coefficients, where
each cell consists of a base station (BS) with $M$ antennas and $N$ users
having $L$ antennas each. In this paper, we propose two opportunistic
interference alignment (OIA) techniques utilizing multiple transmit antennas at
each user: antenna selection-based OIA and singular value decomposition
(SVD)-based OIA. Their performance is analyzed in terms of \textit{user scaling
law} required to achieve $KS$ degrees-of-freedom (DoF), where $S(\le M)$
denotes the number of simultaneously transmitting users per cell. We assume
that each selected user transmits a single data stream at each time-slot. It is
shown that the antenna selection-based OIA does not fundamentally change the
user scaling condition if $L$ is fixed, compared with the single-input
multiple-output (SIMO) IMAC case, which is given by $\text{SNR}^{(K 1)S}$,
where SNR denotes the signal-to-noise ratio. In addition, we show that the
SVD-based OIA can greatly reduce the user scaling condition to
$\text{SNR}^{(K-1)S-L+1}$ through optimizing a weight vector at each user.
Simulation results validate the derived scaling laws of the proposed OIA
techniques. The sum-rate performance of the proposed OIA techniques is compared
with the conventional techniques in MIMO IMAC channels and it is shown that the
proposed OIA techniques outperform the conventional techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5281</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5281</id><created>2013-02-21</created><authors><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author><author><keyname>Warsi</keyname><forenames>Naqueeb Ahmad</forenames></author></authors><title>Fundamental bound on the reliability of quantum information transmission</title><categories>quant-ph cs.IT math.IT</categories><comments>This paper (6 pages, no figures) is a compact version of
  arXiv:1205.1712</comments><journal-ref>Physical Review Letters, vol. 110, 080501, Feb. 2013</journal-ref><doi>10.1103/PhysRevLett.110.080501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory tells us that if the rate of sending information across a
noisy channel were above the capacity of that channel, then the transmission
would necessarily be unreliable. For classical information sent over classical
or quantum channels, one could, under certain conditions, make a stronger
statement that the reliability of the transmission shall decay exponentially to
zero with the number of channel uses and the proof of this statement typically
relies on a certain fundamental bound on the reliability of the transmission.
Such a statement or the bound has never been given for sending quantum
information. We give this bound and then use it to give the first example where
the reliability of sending quantum information at rates above the capacity
decays exponentially to zero. We also show that our framework can be used for
proving generalized bounds on the reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5302</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5302</id><created>2013-02-21</created><authors><author><keyname>Asadi</keyname><forenames>Nima</forenames></author><author><keyname>Lin</keyname><forenames>Jimmy</forenames></author><author><keyname>Busch</keyname><forenames>Michael</forenames></author></authors><title>Dynamic Memory Allocation Policies for Postings in Real-Time Twitter
  Search</title><categories>cs.IR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore a real-time Twitter search application where tweets are arriving
at a rate of several thousands per second. Real-time search demands that they
be indexed and searchable immediately, which leads to a number of
implementation challenges. In this paper, we focus on one aspect: dynamic
postings allocation policies for index structures that are completely held in
main memory. The core issue can be characterized as a &quot;Goldilocks Problem&quot;.
Because memory remains today a scare resource, an allocation policy that is too
aggressive leads to inefficient utilization, while a policy that is too
conservative is slow and leads to fragmented postings lists. We present a
dynamic postings allocation policy that allocates memory in increasingly-larger
&quot;slices&quot; from a small number of large, fixed pools of memory. Through
analytical models and experiments, we explore different settings that balance
time (query evaluation speed) and space (memory utilization).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5328</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5328</id><created>2013-02-21</created><updated>2014-01-15</updated><authors><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author></authors><title>Unions of Onions: Preprocessing Imprecise Points for Fast Onion
  Decomposition</title><categories>cs.CG</categories><comments>10 pages, 5 figures; a preliminary version appeared at WADS 2013</comments><journal-ref>Journal of Computational Geometry, 5(1), 2014, 1-13</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{D}$ be a set of $n$ pairwise disjoint unit disks in the plane.
We describe how to build a data structure for $\mathcal{D}$ so that for any
point set $P$ containing exactly one point from each disk, we can quickly find
the onion decomposition (convex layers) of $P$.
  Our data structure can be built in $O(n \log n)$ time and has linear size.
Given $P$, we can find its onion decomposition in $O(n \log k)$ time, where $k$
is the number of layers. We also provide a matching lower bound. Our solution
is based on a recursive space decomposition, combined with a fast algorithm to
compute the union of two disjoint onion
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5332</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5332</id><created>2013-02-21</created><authors><author><keyname>Guo</keyname><forenames>Mingyu</forenames></author><author><keyname>Deligkas</keyname><forenames>Argyrios</forenames></author></authors><title>Revenue Maximization via Hiding Item Attributes</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study probabilistic single-item second-price auctions where the item is
characterized by a set of attributes. The auctioneer knows the actual
instantiation of all the attributes, but he may choose to reveal only a subset
of these attributes to the bidders. Our model is an abstraction of the
following Ad auction scenario. The website (auctioneer) knows the demographic
information of its impressions, and this information is in terms of a list of
attributes (e.g., age, gender, country of location). The website may hide
certain attributes from its advertisers (bidders) in order to create thicker
market, which may lead to higher revenue. We study how to hide attributes in an
optimal way. We show that it is NP-hard to solve for the optimal attribute
hiding scheme. We then derive a polynomial-time solvable upper bound on the
optimal revenue. Finally, we propose two heuristic-based attribute hiding
schemes. Experiments show that revenue achieved by these schemes is close to
the upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5342</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5342</id><created>2013-02-21</created><authors><author><keyname>Evako</keyname><forenames>Alexander V.</forenames></author></authors><title>The Jordan-Brouwer theorem for the digital normal n-space Zn</title><categories>cs.DM math.CO</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate properties of digital spaces which are
represented by graphs. We find conditions for digital spaces to be digital
n-manifolds and n-spheres. We study properties of partitions of digital spaces
and prove a digital analog of the Jordan-Brouwer theorem for the normal digital
n-space Zn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5348</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5348</id><created>2013-02-21</created><updated>2013-05-31</updated><authors><author><keyname>London</keyname><forenames>Ben</forenames></author><author><keyname>Huang</keyname><forenames>Bert</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Graph-based Generalization Bounds for Learning Binary Relations</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the generalizability of learned binary relations: functions
that map pairs of instances to a logical indicator. This problem has
application in numerous areas of machine learning, such as ranking, entity
resolution and link prediction. Our learning framework incorporates an example
labeler that, given a sequence $X$ of $n$ instances and a desired training size
$m$, subsamples $m$ pairs from $X \times X$ without replacement. The challenge
in analyzing this learning scenario is that pairwise combinations of random
variables are inherently dependent, which prevents us from using traditional
learning-theoretic arguments. We present a unified, graph-based analysis, which
allows us to analyze this dependence using well-known graph identities. We are
then able to bound the generalization error of learned binary relations using
Rademacher complexity and algorithmic stability. The rate of uniform
convergence is partially determined by the labeler's subsampling process. We
thus examine how various assumptions about subsampling affect generalization;
under a natural random subsampling process, our bounds guarantee
$\tilde{O}(1/\sqrt{n})$ uniform convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5366</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5366</id><created>2013-02-21</created><updated>2013-09-05</updated><authors><author><keyname>Chakraborty</keyname><forenames>Sourav</forenames></author><author><keyname>Kamath</keyname><forenames>Akshay</forenames></author><author><keyname>Pratap</keyname><forenames>Rameshwar</forenames></author></authors><title>Testing Uniformity of Stationary Distribution</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random walk on a directed graph gives a Markov chain on the vertices of the
graph. An important question that arises often in the context of Markov chain
is whether the uniform distribution on the vertices of the graph is a
stationary distribution of the Markov chain. Stationary distribution of a
Markov chain is a global property of the graph. In this paper, we prove that
for a regular directed graph whether the uniform distribution on the vertices
of the graph is a stationary distribution, depends on a local property of the
graph, namely if (u,v) is an directed edge then outdegree(u) is equal to
indegree(v).
  This result also has an application to the problem of testing whether a given
distribution is uniform or &quot;far&quot; from being uniform. This is a well studied
problem in property testing and statistics. If the distribution is the
stationary distribution of the lazy random walk on a directed graph and the
graph is given as an input, then how many bits of the input graph do one need
to query in order to decide whether the distribution is uniform or &quot;far&quot; from
it? This is a problem of graph property testing and we consider this problem in
the orientation model (introduced by Halevy et al.). We reduce this problem to
test (in the orientation model) whether a directed graph is Eulerian. And using
result of Fischer et al. on query complexity of testing (in the orientation
model) whether a graph is Eulerian, we obtain bounds on the query complexity
for testing whether the stationary distribution is uniform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5371</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5371</id><created>2013-02-21</created><authors><author><keyname>Dasarathan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author><author><keyname>Banavar</keyname><forenames>Mahesh</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Non-Linear Distributed Average Consensus using Bounded Transmissions</title><categories>cs.DC cs.IT math.IT</categories><comments>24 pages, 8 figures, Submitted to IEEE Transactions on Signal
  Processing</comments><doi>10.1109/TSP.2013.2282912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed average consensus algorithm in which every sensor transmits
with bounded peak power is proposed. In the presence of communication noise, it
is shown that the nodes reach consensus asymptotically to a finite random
variable whose expectation is the desired sample average of the initial
observations with a variance that depends on the step size of the algorithm and
the variance of the communication noise. The asymptotic performance is
characterized by deriving the asymptotic covariance matrix using results from
stochastic approximation theory. It is shown that using bounded transmissions
results in slower convergence compared to the linear consensus algorithm based
on the Laplacian heuristic. Simulations corroborate our analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5374</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5374</id><created>2013-02-21</created><updated>2015-04-01</updated><authors><author><keyname>Yuan</keyname><forenames>Quan</forenames></author><author><keyname>Yang</keyname><forenames>Zhixin</forenames></author></authors><title>A Weight-coded Evolutionary Algorithm for the Multidimensional Knapsack
  Problem</title><categories>cs.NE math.OC</categories><comments>Submitted to Applied Mathematics and Computation on April 8, 2014</comments><msc-class>90B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A revised weight-coded evolutionary algorithm (RWCEA) is proposed for solving
multidimensional knapsack problems. This RWCEA uses a new decoding method and
incorporates a heuristic method in initialization. Computational results show
that the RWCEA performs better than a weight-coded evolutionary algorithm
proposed by Raidl (1999) and to some existing benchmarks, it can yield better
results than the ones reported in the OR-library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5376</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5376</id><created>2013-02-21</created><updated>2014-04-14</updated><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Spatial CSIT Allocation Policies for Network MIMO Channels</title><categories>cs.IT math.IT</categories><comments>Accepted for publications in IEEE Transactions on Information Theory,
  March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the problem of the optimal dissemination of channel
state information (CSI) among K spatially distributed transmitters (TXs)
jointly cooperating to serve K receivers (RXs). One of the particularities of
this work lies in the fact that the CSI is distributed in the sense that each
TX obtains its own estimate of the global multi-user MIMO channel with no
further exchange of information being allowed between the TXs. Although this is
well suited to model the cooperation between non-colocated TXs, e.g., in
cellular Coordinated Multipoint (CoMP) schemes, this type of setting has
received little attention so far in the information theoretic society. We study
in this work what are the CSI requirements at every TX, as a function of the
network geometry, to ensure that the maximal number of degrees-of-freedom (DoF)
is achieved, i.e., the same DoF as obtained under perfect CSI at all TXs. We
advocate the use of the generalized DoF to take into account the geometry of
the network in the analysis. Consistent with the intuition, the derived DoF
maximizing CSI allocation policy suggests that TX cooperation should be limited
to a specific finite neighborhood around each TX. This is in sharp contrast
with the conventional (uniform) CSI dissemination policy which induces CSI
requirements that grow unbounded with the network size. The proposed CSI
allocation policy suggests an alternative to clustering which overcomes
fundamental limitations such as (i) edge interference and (ii) unbounded
increase of the CSIT requirements with the cluster size. Finally, we show how
finite neighborhood CSIT exchange translates into finite neighborhood message
exchange so that finally global interference management is possible with only
local cooperation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5382</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5382</id><created>2013-02-21</created><updated>2013-03-23</updated><authors><author><keyname>Abdollahi</keyname><forenames>Afshin</forenames></author><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Reversible Logic Synthesis by Quantum Rotation Gates</title><categories>cs.ET cs.DS quant-ph</categories><comments>19 pages, 17 figures</comments><journal-ref>A. Abdollahi, M. Saeedi, and M. Pedram, &quot;Reversible Logic
  Synthesis by Quantum Rotation Gates,&quot; Quantum Information and Computation,
  Vol. 13, No. 9-10, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A rotation-based synthesis framework for reversible logic is proposed. We
develop a canonical representation based on binary decision diagrams and
introduce operators to manipulate the developed representation model.
Furthermore, a recursive functional bi-decomposition approach is proposed to
automatically synthesize a given function. While Boolean reversible logic is
particularly addressed, our framework constructs intermediate quantum states
that may be in superposition, hence we combine techniques from reversible
Boolean logic and quantum computation. The proposed approach results in
quadratic gate count for multiple-control Toffoli gates without ancillae,
linear depth for quantum carry-ripple adder, and quasilinear size for quantum
multiplexer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5383</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5383</id><created>2013-02-21</created><updated>2014-01-24</updated><authors><author><keyname>Rajan</keyname><forenames>Adithya</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>Stochastic Ordering of Fading Channels Through the Shannon Transform</title><categories>cs.IT math.IT</categories><comments>24 pages, 3 figures. Submitted to the IEEE transactions of
  Information Theory. Part of this work has appeared in the Proceedings of IEEE
  International Symposium on Information Theory, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new stochastic order between two fading distributions is introduced. A
fading channel dominates another in the ergodic capacity ordering sense, if the
Shannon transform of the first is greater than that of the second at all values
of average signal to noise ratio. It is shown that some parametric fading
models such as the Nakagami-m, Rician, and Hoyt are distributions that are
monotonic in their line of sight parameters with respect to the ergodic
capacity order. Some operations under which the ergodic capacity order is
preserved are also discussed. Through these properties of the ergodic capacity
order, it is possible to compare under two different fading scenarios, the
ergodic capacity of a composite system involving multiple fading links with
coding/decoding capabilities only at the transmitter/receiver. Such comparisons
can be made even in cases when a closed form expression for the ergodic
capacity of the composite system is not analytically tractable. Applications to
multiple access channels, and extensions to multiple-input multiple-output
(MIMO) systems are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5384</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5384</id><created>2013-02-21</created><authors><author><keyname>Chirayil</keyname><forenames>Beena Joy</forenames></author></authors><title>Facilitating Machine to Machine (M2M) Communication using GSM Network</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>Published in the Proceedings of 2012 International Conference on
  the Internet of Things (IoT), Wuxi; presented in M2M-CTS'12 Workshop</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a method to facilitate M2M communication using existing GSM
networks is proposed - as M2M devices primarily use SMS as their data bearer,
the focus is on increasing the number of devices that can use the associated
GSM signaling channels at a time. This is achieved by defining a new class of
low mobility, static M2M devices which use a modified physical layer control
frame structure. The proposal is expected to aid a quick, reliable and
cost-effective deployment of M2M devices in the existing GSM networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5391</identifier>
 <datestamp>2014-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5391</id><created>2013-02-21</created><updated>2014-03-01</updated><authors><author><keyname>Duchamp</keyname><forenames>G&#xe9;rard Henry Edmond</forenames><affiliation>LIPN</affiliation></author><author><keyname>Minh</keyname><forenames>Vincel Hoang Ngoc</forenames><affiliation>LIPN</affiliation></author><author><keyname>Tollu</keyname><forenames>Christophe</forenames><affiliation>LIPN</affiliation></author><author><keyname>Chi&#xea;n</keyname><forenames>B&#xf9;i</forenames><affiliation>LIPN</affiliation></author><author><keyname>Nghia</keyname><forenames>Nguyen Hoang</forenames><affiliation>LIPN</affiliation></author></authors><title>Combinatorics of $\phi$-deformed stuffle Hopf algebras</title><categories>math.CO cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to extend the Sch\&quot;utzenberger's factorization to general
perturbations, the combinatorial aspects of the Hopf algebra of the
$\phi$-deformed stuffle product is developed systematically in a parallel way
with those of the shuffle product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5392</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5392</id><created>2013-02-21</created><updated>2014-01-16</updated><authors><author><keyname>Milo&#x161;evi&#x107;</keyname><forenames>Nikola</forenames></author></authors><title>History of malware</title><categories>cs.CR</categories><comments>11 pages, 8 figures describing history and evolution of PC malware
  from first PC malware to Stuxnet, DoQu and Flame. This article has been
  withdrawed due some errors in text and publication in the jurnal that asked
  to withdraw article from other sources</comments><msc-class>68-03</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In past three decades almost everything has changed in the field of malware
and malware analysis. From malware created as proof of some security concept
and malware created for financial gain to malware created to sabotage
infrastructure. In this work we will focus on history and evolution of malware
and describe most important malwares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5401</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5401</id><created>2013-02-21</created><authors><author><keyname>Parter</keyname><forenames>Merav</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Sparse Fault-Tolerant BFS Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of designing a sparse {\em fault-tolerant}
BFS tree, or {\em FT-BFS tree} for short, namely, a sparse subgraph $T$ of the
given network $G$ such that subsequent to the failure of a single edge or
vertex, the surviving part $T'$ of $T$ still contains a BFS spanning tree for
(the surviving part of) $G$. Our main results are as follows. We present an
algorithm that for every $n$-vertex graph $G$ and source node $s$ constructs a
(single edge failure) FT-BFS tree rooted at $s$ with $O(n \cdot
\min\{\Depth(s), \sqrt{n}\})$ edges, where $\Depth(s)$ is the depth of the BFS
tree rooted at $s$. This result is complemented by a matching lower bound,
showing that there exist $n$-vertex graphs with a source node $s$ for which any
edge (or vertex) FT-BFS tree rooted at $s$ has $\Omega(n^{3/2})$ edges. We then
consider {\em fault-tolerant multi-source BFS trees}, or {\em FT-MBFS trees}
for short, aiming to provide (following a failure) a BFS tree rooted at each
source $s\in S$ for some subset of sources $S\subseteq V$. Again, tight bounds
are provided, showing that there exists a poly-time algorithm that for every
$n$-vertex graph and source set $S \subseteq V$ of size $\sigma$ constructs a
(single failure) FT-MBFS tree $T^*(S)$ from each source $s_i \in S$, with
$O(\sqrt{\sigma} \cdot n^{3/2})$ edges, and on the other hand there exist
$n$-vertex graphs with source sets $S \subseteq V$ of cardinality $\sigma$, on
which any FT-MBFS tree from $S$ has $\Omega(\sqrt{\sigma}\cdot n^{3/2})$ edges.
Finally, we propose an $O(\log n)$ approximation algorithm for constructing
FT-BFS and FT-MBFS structures. The latter is complemented by a hardness result
stating that there exists no $\Omega(\log n)$ approximation algorithm for these
problems under standard complexity assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5414</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5414</id><created>2013-02-21</created><updated>2013-03-03</updated><authors><author><keyname>Chiesa</keyname><forenames>Marco</forenames></author><author><keyname>Lospoto</keyname><forenames>Gabriele</forenames></author><author><keyname>Rimondini</keyname><forenames>Massimo</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author></authors><title>Intra-Domain Pathlet Routing</title><categories>cs.NI</categories><comments>13 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internal routing inside an ISP network is the foundation for lots of services
that generate revenue from the ISP's customers. A fine-grained control of paths
taken by network traffic once it enters the ISP's network is therefore a
crucial means to achieve a top-quality offer and, equally important, to enforce
SLAs. Many widespread network technologies and approaches (most notably, MPLS)
offer limited (e.g., with RSVP-TE), tricky (e.g., with OSPF metrics), or no
control on internal routing paths. On the other hand, recent advances in the
research community are a good starting point to address this shortcoming, but
miss elements that would enable their applicability in an ISP's network.
  We extend pathlet routing by introducing a new control plane for internal
routing that has the following qualities: it is designed to operate in the
internal network of an ISP; it enables fine-grained management of network paths
with suitable configuration primitives; it is scalable because routing changes
are only propagated to the network portion that is affected by the changes; it
supports independent configuration of specific network portions without the
need to know the configuration of the whole network; it is robust thanks to the
adoption of multipath routing; it supports the enforcement of QoS levels; it is
independent of the specific data plane used in the ISP's network; it can be
incrementally deployed and it can nicely coexist with other control planes.
Besides formally introducing the algorithms and messages of our control plane,
we propose an experimental validation in the simulation framework OMNeT++ that
we use to assess the effectiveness and scalability of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5417</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5417</id><created>2013-02-21</created><authors><author><keyname>Kalaivani</keyname><forenames>P.</forenames></author><author><keyname>Anandaraj</keyname><forenames>A.</forenames></author><author><keyname>Raja</keyname><forenames>K.</forenames></author></authors><title>An Ontology Construction Approach for the Domain Of Poultry Science
  Using Protege</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1302.5215</comments><journal-ref>International Journal of Information Technology and Management
  Sciences / Volume 1, Issue 2, 2011, ISSN:2231-6752</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The information retrieval systems that are present nowadays are mainly based
on full text matching of keywords or topic based classification. This matching
of keywords often returns a large number of irrelevant information and this
does not meet the users query requirement. In order to solve this problem and
to enhance the search using semantic environment, a technique named ontology is
implemented for the field of poultry in this paper. Ontology is an emerging
technique in the current field of research in semantic environment. This paper
constructs ontology using the tool named Protege version 4.0 and this also
generates Resource Description Framework schema and XML scripts for using
poultry ontology in web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5442</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5442</id><created>2013-02-21</created><updated>2013-07-02</updated><authors><author><keyname>Si</keyname><forenames>Weisheng</forenames></author></authors><title>Are Yao Graph and Theta Graph Void Free?</title><categories>cs.NI</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy Forwarding algorithm is a widely-used routing algorithm for wireless
networks. However, it can fail if network topologies (usually modeled by
geometric graphs) contain voids. Since Yao Graph and Theta Graph are two types
of geometric graphs exploited to construct wireless network topologies, this
paper studies whether these two types of graphs can contain voids.
Specifically, this paper shows that when the number of cones in a Yao Graph or
Theta Graph is less than 6, Yao Graph and Theta Graph can have voids, but when
the number of cones equals or exceeds 6, Yao Graph and Theta Graph are free of
voids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5445</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5445</id><created>2013-02-21</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author></authors><title>Thrifty Algorithms for Multistage Robust Optimization</title><categories>cs.DS</categories><comments>20 pages, full version of IPCO 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of multi-stage robust covering problems, where additional
information is revealed about the problem instance in each stage, but the cost
of taking actions increases. The dilemma for the decision-maker is whether to
wait for additional information and risk the inflation, or to take early
actions to hedge against rising costs. We study the &quot;k-robust&quot; uncertainty
model: in each stage i = 0, 1,...,T, the algorithm is shown some subset of size
k_i that completely contains the eventual demands to be covered; here k_1 &gt; k_2
&gt;...&gt; k_T which ensures increasing information over time. The goal is to
minimize the cost incurred in the worst-case possible sequence of revelations.
  For the multistage k-robust set cover problem, we give an O(log m + log
n)-approximation algorithm, nearly matching the \Omega(log n + log m/loglog m)
hardness of approximation even for T=2 stages. Moreover, our algorithm has a
useful &quot;thrifty&quot; property: it takes actions on just two stages. We show similar
thrifty algorithms for multi-stage k-robust Steiner tree, Steiner forest, and
minimum-cut. For these problems our approximation guarantees are O(min{T, log
n, log L_{max}), where L_{max} is the maximum inflation over all the stages. We
conjecture that these problems also admit O(1)-approximate thrifty algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5449</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5449</id><created>2013-02-21</created><authors><author><keyname>Bazerque</keyname><forenames>Juan Andres</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Nonparametric Basis Pursuit via Sparse Kernel-based Learning</title><categories>cs.LG cs.CV cs.IT math.IT stat.ML</categories><comments>IEEE SIGNAL PROCESSING MAGAZINE, 2013 (TO APPEAR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal processing tasks as fundamental as sampling, reconstruction, minimum
mean-square error interpolation and prediction can be viewed under the prism of
reproducing kernel Hilbert spaces. Endowing this vantage point with
contemporary advances in sparsity-aware modeling and processing, promotes the
nonparametric basis pursuit advocated in this paper as the overarching
framework for the confluence of kernel-based learning (KBL) approaches
leveraging sparse linear regression, nuclear-norm regularization, and
dictionary learning. The novel sparse KBL toolbox goes beyond translating
sparse parametric approaches to their nonparametric counterparts, to
incorporate new possibilities such as multi-kernel selection and matrix
smoothing. The impact of sparse KBL to signal processing applications is
illustrated through test cases from cognitive radio sensing, microarray data
imputation, and network traffic prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5453</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5453</id><created>2013-02-21</created><updated>2013-07-02</updated><authors><author><keyname>Linden</keyname><forenames>Noah</forenames></author><author><keyname>Mat&#xfa;&#x161;</keyname><forenames>Franti&#x161;ek</forenames></author><author><keyname>Ruskai</keyname><forenames>Mary Beth</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>The Quantum Entropy Cone of Stabiliser States</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>15 pages, uses lipics.cls
  (http://www.dagstuhl.de/en/publications/lipics). V2 is the final TQC 2013
  proceedings version, it has numerous corrections, updated references, and a
  new co-author</comments><journal-ref>in: Proc. 8th TQC Guelph, LIPICS vol. 22, pp. 270-284, 2013</journal-ref><doi>10.4230/LIPIcs.TQC.2013.270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the universal linear inequalities that hold for the von
Neumann entropies in a multi-party system, prepared in a stabiliser state. We
demonstrate here that entropy vectors for stabiliser states satisfy, in
addition to the classic inequalities, a type of linear rank inequalities
associated with the combinatorial structure of normal subgroups of certain
matrix groups.
  In the 4-party case, there is only one such inequality, the so-called
Ingleton inequality. For these systems we show that strong subadditivity, weak
monotonicity and Ingleton inequality exactly characterize the entropy cone for
stabiliser states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5454</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5454</id><created>2013-02-21</created><authors><author><keyname>Jassim</keyname><forenames>Firas</forenames></author><author><keyname>Altaani</keyname><forenames>Fawzi</forenames></author></authors><title>Statistical Approach for Predicting Factors of Mood Method for Object
  Oriented</title><categories>cs.SE</categories><comments>5 pages, 2 figures, 7 tables</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 1, No 1, January 2013</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Object oriented design is becoming more popular in software development and
object oriented design metrics which is an essential part of software
environment. The main goal in this paper is to predict factors of MOOD method
for OO using a statistical approach. Therefore, linear regression model is used
to find the relationship between factors of MOOD method and their influences on
OO software measurements. Fortunately, through this process a prediction could
be made for the line of code (LOC), number of classes (NOC), number of methods
(NOM), and number of attributes (NOA). These measurements permit designers to
access the software early in process, making changes that will reduce
complexity and improve the continuing capability of the design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5455</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5455</id><created>2013-02-21</created><updated>2013-02-25</updated><authors><author><keyname>Anshelevich</keyname><forenames>Elliot</forenames></author><author><keyname>Hate</keyname><forenames>Ameya</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author></authors><title>Seeding Influential Nodes in Non-Submodular Models of Information
  Diffusion</title><categories>cs.SI physics.soc-ph</categories><comments>corrections to contact info</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the model of information diffusion in social networks from
\cite{Hui2010a} which incorporates trust (weighted links) between actors, and
allows actors to actively participate in the spreading process, specifically
through the ability to query friends for additional information. This model
captures how social agents transmit and act upon information more realistically
as compared to the simpler threshold and cascade models. However, it is more
difficult to analyze, in particular with respect to seeding strategies. We
present efficient, scalable algorithms for determining good seed sets --
initial nodes to inject with the information. Our general approach is to reduce
our model to a class of simpler models for which provably good sets can be
constructed. By tuning this class of simpler models, we obtain a good seed set
for the original more complex model. We call this the \emph{projected greedy
approach} because you `project' your model onto a class of simpler models where
a greedy seed set selection is near-optimal. We demonstrate the effectiveness
of our seeding strategy on synthetic graphs as well as a realistic San Diego
evacuation network constructed during the 2007 fires.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5474</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5474</id><created>2013-02-21</created><updated>2013-03-08</updated><authors><author><keyname>Yuan</keyname><forenames>Quan</forenames></author><author><keyname>Yang</keyname><forenames>Zhixin</forenames></author></authors><title>On the performance of a hybrid genetic algorithm in dynamic environments</title><categories>cs.NE math.OC</categories><comments>This paper has been submitted to Applied Mathematics and Computation
  on May 22, 2012 Revised version has been submitted to Applied Mathematics and
  Computation on March 1, 2013</comments><msc-class>68T20</msc-class><journal-ref>Applied Mathematics and Computation, Volume 219, Issue 24, 15
  August 2013, Pages 11408-11413</journal-ref><doi>10.1016/j.amc.2013.06.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to track the optimum of dynamic environments is important in many
practical applications. In this paper, the capability of a hybrid genetic
algorithm (HGA) to track the optimum in some dynamic environments is
investigated for different functional dimensions, update frequencies, and
displacement strengths in different types of dynamic environments. Experimental
results are reported by using the HGA and some other existing evolutionary
algorithms in the literature. The results show that the HGA has better
capability to track the dynamic optimum than some other existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5481</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5481</id><created>2013-02-21</created><authors><author><keyname>Prajapati</keyname><forenames>Harshad B.</forenames></author><author><keyname>Dabhi</keyname><forenames>Vipul K.</forenames></author></authors><title>Classification and Characterization of Core Grid Protocols for Global
  Grid Computing</title><categories>cs.DC</categories><comments>Survey paper having 23 Pages, 9 Figures, 6 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid computing has attracted many researchers over a few years, and as a
result many new protocols have emerged and also evolved since its inception a
decade ago. Grid protocols play major role in implementing services that
facilitate coordinated resource sharing across diverse organizations. In this
paper, we provide comprehensive coverage of different core Grid protocols that
can be used in Global Grid Computing. We establish the classification of core
Grid protocols into i) Grid network communication and Grid data transfer
protocols, ii) Grid information security protocols, iii) Grid resource
information protocols, iv) Grid management protocols, and v) Grid interface
protocols, depending upon the kind of activities handled by these protocols.
All the classified protocols are also organized into layers of the Hourglass
model of Grid architecture to understand dependency among these protocols. We
also present the characteristics of each protocol. For better understanding of
these protocols, we also discuss applied protocols as examples from either
Globus toolkit or other popular Grid middleware projects. We believe that our
classification and characterization of Grid protocols will enable better
understanding of core Grid protocols and will motivate further research in the
area of Global Grid Computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5490</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5490</id><created>2013-02-22</created><authors><author><keyname>Good</keyname><forenames>Alice</forenames></author><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Panjganj</keyname><forenames>Vahid</forenames></author><author><keyname>Spettigue</keyname><forenames>Samuel</forenames></author></authors><title>Computer Interaction and the Benefits of Social Networking for People
  with Borderline Personality Disorder: Enlightening Mental Health
  Professionals</title><categories>cs.HC cs.CY</categories><comments>Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper seeks to present the findings of a focus group and questionnaire
in assessing how aware mental health professionals, who have experience with
people with Borderline Personality Disorder (BPD), are in the extent of ICT
based support for people with BPD. The methods used were both qualitative and
quantitative and used descriptive data. Content analysis was used to explore
specific themes and results were cross-examined between the two methods. The
work should be viewed as an exploratory study into the viability and likely
acceptance of a virtual support community specifically designed for people with
BPD. The long term aim is to provide additional support for people with BPD,
especially when they are in crisis and might be at a higher risk of harm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5491</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5491</id><created>2013-02-22</created><authors><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Good</keyname><forenames>Alice</forenames></author></authors><title>Implications for Improving Accessibility to E-Commerce Websites in
  Developing Countries - A Study of Hotel Websites</title><categories>cs.HC</categories><comments>Journal article. conference version is arXiv:1302.5198</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research explores the accessibility issues with regard to the e-commerce
websites in developing countries, through a study of Sri Lankan hotel websites.
A web survey and a web content analysis were conducted as the methods to elicit
data on web accessibility. Factors preventing accessibility were hypothesized
as an initial experiment. Affecting design elements are identified through web
content analysis, the results of which are utilized to develop specific
implications for improving web accessibility. The hypothesis tests show that
there is no significant correlation between accessibility and geographical or
economic factors. However, physical impairments of users have a considerable
influence on the accessibility of web page user interface if it has been
designed without full consideration of the needs of all users. Especially,
visual and mobility impaired users experience poor accessibility. Poor
readability and less navigable page designs are two observable issues, which
pose threats to accessibility. The lack of conformance to W3C accessibility
guidelines and the poor design process are the specific shortcomings which
reduce the overall accessibility. Guidelines aim to improve the accessibility
of sites with a strategic focus. Further enhancements are suggested with
adherence to principles, user centered design and developing customizable web
portals compatible for connections with differing speeds. Re-ordering search
results has been suggested as one of the finest step towards making the web
content accessible for users with differing needs. A need for developing new
design models for differencing user groups and implementing web accessibility
strategy are emphasized as vital steps towards effective information
dissemination via e-commerce websites in the developing countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5495</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5495</id><created>2013-02-22</created><authors><author><keyname>Good</keyname><forenames>Alice</forenames></author><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Panjganj</keyname><forenames>Vahid</forenames></author></authors><title>Looking back at Facebook content and the positive impact upon wellbeing:
  Exploring reminiscing as a tool for self soothing</title><categories>cs.CY cs.HC</categories><comments>Conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The premise of this paper is to explore the potential of reminiscing in
facilitating self soothing. The research presented looks at people's activities
on Facebook and whether these particular activities impact upon their perceived
sense of wellbeing, furthermore, whether specific Facebook activities enable a
self-soothing effect when feeling low in mood. A survey was distributed amongst
Facebook users. The results from the study appear to indicate that in
comparison to other Facebook activities, looking back upon photos and wall
posts in particular, could have a positive impact upon wellbeing. Additionally,
the results indicate that people who have mental health problems, experience a
more positive impact upon their wellbeing when looking at photos and wall
posts, than those who did not have a history of mental health issues. The
results from the research presented here contribute towards the viability of
developing a mobile application to facilitate positive reminiscing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5497</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5497</id><created>2013-02-22</created><authors><author><keyname>Good</keyname><forenames>Alice</forenames></author><author><keyname>Gnanayutham</keyname><forenames>Paul</forenames></author><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author><author><keyname>Panjganj</keyname><forenames>Vahid</forenames></author></authors><title>HCI considerations in Designing a Second Life Virtual Therapeutic
  Community for the Support &amp; Treatment of People with Borderline Personality
  Disorder</title><categories>cs.HC</categories><comments>Book chapter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to present the current position of the ongoing research into
developing the requirements and acceptance of a virtual therapeutic community
in Second Life, specifically for people with Borderline Personality Disorder
(BPD). The research has identified this particular user group given that people
with BPD often require high levels of support, which can result in emergency
hospital admissions, in addition to the significant economic cost of treating
people BPD in relation to other mental illnesses (NCCMH, 2009). The research is
also intended to be used as framework for other mental health conditions. This
work is a continuation of the research carried out in exploring the potential
of virtual therapeutic communities based on existing models of therapeutic
hospitals as well as virtual treatments and support, in treating people with
BPD. An interdisciplinary approach to this research features collaboration from
areas in HCI, forensic psychology and psychotherapy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5499</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5499</id><created>2013-02-22</created><authors><author><keyname>Good</keyname><forenames>Alice</forenames></author><author><keyname>Sambhanthan</keyname><forenames>Arunasalam</forenames></author></authors><title>A Review into eHealth Services and Therapies: Potential for Virtual
  Therapeutic Communities - Supporting People with Severe Personality Disorder</title><categories>cs.CY</categories><comments>Book chapter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  eHealth has expanded hugely over the last fifteen years and continues to
evolve, providing greater benefits for patients, health care professionals and
providers alike. The technologies that support these systems have become
increasingly more sophisticated and have progressed significantly from standard
databases, used for patient records, to highly advanced Virtual Reality (VR)
systems for the treatment of complex mental health illnesses. The scope of this
paper is to initially explore e-Health, particularly in relation to
technologies supporting the treatment and management of wellbeing in mental
health. It then provides a case study of how technology in e-Health can lend
itself to an application that could support and maintain the wellbeing of
people with a severe mental illness. The case study uses Borderline Personality
Disorder as an example, but could be applicable in many other areas, including
depression, anxiety, addiction and PTSD. This type of application demonstrates
how e-Health can empower the individuals using it but also potentially reducing
the impact upon health care providers and services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5502</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5502</id><created>2013-02-22</created><authors><author><keyname>Srimugunthan</keyname></author><author><keyname>Gopinath</keyname><forenames>K.</forenames></author><author><keyname>Yasa</keyname><forenames>Giridhar Appaji Nag</forenames></author></authors><title>LFTL: A multi-threaded FTL for a Parallel IO Flash Card under Linux</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New PCI-e flash cards and SSDs supporting over 100,000 IOPs are now
available, with several usecases in the design of a high performance storage
system. By using an array of flash chips, arranged in multiple banks, large
capacities are achieved. Such multi-banked architecture allow parallel read,
write and erase operations. In a raw PCI-e flash card, such parallelism is
directly available to the software layer. In addition, the devices have
restrictions such as, pages within a block can only be written sequentially.
The devices also have larger minimum write sizes (greater than 4KB). Current
flash translation layers (FTLs) in Linux are not well suited for such devices
due to the high device speeds, architectural restrictions as well as other
factors such as high lock contention. We present a FTL for Linux that takes
into account the hardware restrictions, that also exploits the parallelism to
achieve high speeds. We also consider leveraging the parallelism for garbage
collection by scheduling the garbage collection activities on idle banks. We
propose and evaluate an adaptive method to vary the amount of garbage
collection according to the current I/O load on the device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5503</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5503</id><created>2013-02-22</created><authors><author><keyname>Rautenbach</keyname><forenames>Dieter</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Sereni</keyname><forenames>Jean-S&#xe9;bastien</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Transversals of Longest Paths and Cycles</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a graph of order n. Let lpt(G) be the minimum cardinality of a set X
of vertices of G such that X intersects every longest path of G and define
lct(G) analogously for cycles instead of paths. We prove that lpt(G) \leq
ceiling(n/4-n^{2/3}/90), if G is connected, lct(G) \leq
ceiling(n/3-n^{2/3}/36), if G is 2-connected, and \lpt(G) \leq 3, if G is a
connected circular arc graph. Our bound on lct(G) improves an earlier result of
Thomassen and our bound for circular arc graphs relates to an earlier statement
of Balister \emph{et al.} the argument of which contains a gap. Furthermore, we
prove upper bounds on lpt(G) for planar graphs and graphs of bounded
tree-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5511</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5511</id><created>2013-02-22</created><authors><author><keyname>Aini</keyname><forenames>Nuril</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Jemakmun</keyname></author></authors><title>Perangkat lunak bantu mengenal huruf arab melayu ke bentuk huruf latin
  bahasa Indonesia</title><categories>cs.OH</categories><journal-ref>MATRIK. 8 (2006) 317-334</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of computer science has contributed greatly for increasing of
efficiency and effectively. Many areas are covered by computer science,
included education. The purpose of this research is to introduce jawi a type of
Indonesian letters. Jawis letter is one of the most popular letter in the past.
But right now few people can read and understand it. Many documents in the past
was written in Jawi. The writer develop or build the software using Pressman
method, and tools such as Microsoft Visual Basic, and Microsoft Access. This
software can introduce Jawi then people can learn it easily.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5514</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5514</id><created>2013-02-22</created><authors><author><keyname>Gabran</keyname><forenames>Wesam</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Liu</keyname><forenames>Chun-Hao</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Blind Estimation of Primary User Traffic Parameters Under Sensing Errors</title><categories>cs.PF</categories><comments>accepted to IEEE ICC 2013 [Extended version, with appendices]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the bounds on the estimation accuracy of Primary
User (PU) traffic parameters with exponentially distributed busy and idle
times. We derive closed-form expressions for the Cramer-Rao bounds on the mean
squared estimation error for the blind joint estimation of the PU traffic
parameters, specifically, the duty cycle, and the mean arrival and departure
rates. Moreover, we present the corresponding maximum-likelihood estimators for
the traffic parameters. In addition, we derive a modified likelihood function
for the joint estimation of traffic parameters when spectrum sensing errors are
considered, and we present the impact of spectrum sensing errors on the
estimation error via simulations. Finally, we consider a duty cycle estimator,
common in traffic estimation literature, that is based on averaging the traffic
samples. We derive, in closed-form, the mean squared estimation error of the
considered estimator under spectrum sensing errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5518</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5518</id><created>2013-02-22</created><updated>2013-05-15</updated><authors><author><keyname>Pamies-Juarez</keyname><forenames>Lluis</forenames></author><author><keyname>Hollmann</keyname><forenames>Henk D. L.</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>Locally Repairable Codes with Multiple Repair Alternatives</title><categories>cs.IT cs.DC math.IT</categories><comments>IEEE International Symposium on Information Theory (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems need to store data redundantly in order to
provide some fault-tolerance and guarantee system reliability. Different coding
techniques have been proposed to provide the required redundancy more
efficiently than traditional replication schemes. However, compared to
replication, coding techniques are less efficient for repairing lost
redundancy, as they require retrieval of larger amounts of data from larger
subsets of storage nodes. To mitigate these problems, several recent works have
presented locally repairable codes designed to minimize the repair traffic and
the number of nodes involved per repair. Unfortunately, existing methods often
lead to codes where there is only one subset of nodes able to repair a piece of
lost data, limiting the local repairability to the availability of the nodes in
this subset. In this paper, we present a new family of locally repairable codes
that allows different trade-offs between the number of contacted nodes per
repair, and the number of different subsets of nodes that enable this repair.
We show that slightly increasing the number of contacted nodes per repair
allows to have repair alternatives, which in turn increases the probability of
being able to perform efficient repairs. Finally, we present pg-BLRC, an
explicit construction of locally repairable codes with multiple repair
alternatives, constructed from partial geometries, in particular from
Generalized Quadrangles. We show how these codes can achieve practical lengths
and high rates, while requiring a small number of nodes per repair, and
providing multiple repair alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5521</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5521</id><created>2013-02-22</created><authors><author><keyname>Moghadam</keyname><forenames>Mikael</forenames></author><author><keyname>Christensen</keyname><forenames>David Johan</forenames></author><author><keyname>Brandt</keyname><forenames>David</forenames></author><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author></authors><title>Towards Python-based Domain-specific Languages for Self-reconfigurable
  Modular Robotics Research</title><categories>cs.RO cs.OS cs.PL cs.SE</categories><comments>Presented at DSLRob 2011 (arXiv:1212.3308)</comments><report-no>DSLRob/2011/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the role of operating system and high-level languages in
the development of software and domain-specific languages (DSLs) for
self-reconfigurable robotics. We review some of the current trends in
self-reconfigurable robotics and describe the development of a software system
for ATRON II which utilizes Linux and Python to significantly improve software
abstraction and portability while providing some basic features which could
prove useful when using Python, either stand-alone or via a DSL, on a
self-reconfigurable robot system. These features include transparent socket
communication, module identification, easy software transfer and reliable
module-to-module communication. The end result is a software platform for
modular robots that where appropriate builds on existing work in operating
systems, virtual machines, middleware and high-level languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5526</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5526</id><created>2013-02-22</created><updated>2013-05-31</updated><authors><author><keyname>Reisenauer</keyname><forenames>Rainer</forenames></author><author><keyname>Smith</keyname><forenames>Kenny</forenames></author><author><keyname>Blythe</keyname><forenames>Richard A.</forenames></author></authors><title>Stochastic dynamics of lexicon learning in an uncertain and nonuniform
  world</title><categories>physics.soc-ph cond-mat.stat-mech cs.CL q-bio.NC</categories><comments>7 pages, 3 figures. Version 2 contains additional discussion and will
  appear in Phys. Rev. Lett</comments><journal-ref>Phys Rev Lett (2013) 110 258701</journal-ref><doi>10.1103/PhysRevLett.110.258701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the time taken by a language learner to correctly identify the
meaning of all words in a lexicon under conditions where many plausible
meanings can be inferred whenever a word is uttered. We show that the most
basic form of cross-situational learning - whereby information from multiple
episodes is combined to eliminate incorrect meanings - can perform badly when
words are learned independently and meanings are drawn from a nonuniform
distribution. If learners further assume that no two words share a common
meaning, we find a phase transition between a maximally-efficient learning
regime, where the learning time is reduced to the shortest it can possibly be,
and a partially-efficient regime where incorrect candidate meanings for words
persist at late times. We obtain exact results for the word-learning process
through an equivalence to a statistical mechanical problem of enumerating loops
in the space of word-meaning mappings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5544</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5544</id><created>2013-02-22</created><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Rodriguez-S&#xe1;nchez</keyname><forenames>Rosa</forenames></author><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Fdez-Valdivia</keyname><forenames>J.</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>J. A.</forenames></author></authors><title>Mapping Citation Patterns of Book Chapters in the Book Citation Index</title><categories>cs.DL physics.soc-ph</categories><journal-ref>Torres-Salinas, D. et al. (2013). Mapping Citation Patterns of
  Book Chapters in the Book Citation Index. Journal of Informetrics, 7(2),
  412-424</journal-ref><doi>10.1016/j.joi.2013.01.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide the reader with a visual representation of
relationships among the impact of book chapters indexed in the Book Citation
Index using information gain values and published by different academic
publishers in specific disciplines. The impact of book chapters can be
characterized statistically by citations histograms. For instance, we can
compute the probability of occurrence of book chapters with a number of
citations in different intervals for each academic publisher. We predict the
similarity between two citation histograms based on the amount of relative
information between such characterizations. We observe that the citation
patterns of book chapters follow a Lotkaian distribution. This paper describes
the structure of the Book Citation Index using 'heliocentric clockwise maps'
which allow the reader not only to determine the grade of similarity of a given
academic publisher indexed in the Book Citation Index with a specific
discipline according to their citation distribution, but also to easily observe
the general structure of a discipline, identifying the publishers with higher
impact and output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5549</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5549</id><created>2013-02-22</created><authors><author><keyname>Koloniari</keyname><forenames>Georgia</forenames></author><author><keyname>Souravlias</keyname><forenames>Dimitris</forenames></author><author><keyname>Pitoura</keyname><forenames>Evaggelia</forenames></author></authors><title>On Graph Deltas for Historical Queries</title><categories>cs.DB cs.SI</categories><comments>6 pages, 1 figure, WOSS 2012, Istanbul, Turkey</comments><journal-ref>Proceedings of 1st Workshop on Online Social Systems (WOSS) 2012,
  in conjunction with VLDB 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of evaluating historical queries on
graphs. To this end, we investigate the use of graph deltas, i.e., a log of
time-annotated graph operations. Our storage model maintains the current graph
snapshot and the delta. We reconstruct past snapshots by applying appropriate
parts of the graph delta on the current snapshot. Query evaluation proceeds on
the reconstructed snapshots but we also propose algorithms based mostly on
deltas for efficiency. We introduce various techniques for improving
performance, including materializing intermediate snapshots, partial
reconstruction and indexing deltas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5554</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5554</id><created>2013-02-22</created><updated>2014-03-13</updated><authors><author><keyname>H&#xe9;as</keyname><forenames>Patrick</forenames></author><author><keyname>Lavancier</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Kadri-Harouna</keyname><forenames>Souleymane</forenames></author></authors><title>Self-similar prior and wavelet bases for hidden incompressible turbulent
  motion</title><categories>stat.AP cs.CV cs.NA physics.flu-dyn</categories><comments>SIAM Journal on Imaging Sciences, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is concerned with the ill-posed inverse problem of estimating
turbulent flows from the observation of an image sequence. From a Bayesian
perspective, a divergence-free isotropic fractional Brownian motion (fBm) is
chosen as a prior model for instantaneous turbulent velocity fields. This
self-similar prior characterizes accurately second-order statistics of velocity
fields in incompressible isotropic turbulence. Nevertheless, the associated
maximum a posteriori involves a fractional Laplacian operator which is delicate
to implement in practice. To deal with this issue, we propose to decompose the
divergent-free fBm on well-chosen wavelet bases. As a first alternative, we
propose to design wavelets as whitening filters. We show that these filters are
fractional Laplacian wavelets composed with the Leray projector. As a second
alternative, we use a divergence-free wavelet basis, which takes implicitly
into account the incompressibility constraint arising from physics. Although
the latter decomposition involves correlated wavelet coefficients, we are able
to handle this dependence in practice. Based on these two wavelet
decompositions, we finally provide effective and efficient algorithms to
approach the maximum a posteriori. An intensive numerical evaluation proves the
relevance of the proposed wavelet-based self-similar priors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5565</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5565</id><created>2013-02-22</created><authors><author><keyname>Fairbank</keyname><forenames>Michael</forenames></author></authors><title>The Importance of Clipping in Neurocontrol by Direct Gradient Descent on
  the Cost-to-Go Function and in Adaptive Dynamic Programming</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In adaptive dynamic programming, neurocontrol and reinforcement learning, the
objective is for an agent to learn to choose actions so as to minimise a total
cost function. In this paper we show that when discretized time is used to
model the motion of the agent, it can be very important to do &quot;clipping&quot; on the
motion of the agent in the final time step of the trajectory. By clipping we
mean that the final time step of the trajectory is to be truncated such that
the agent stops exactly at the first terminal state reached, and no distance
further. We demonstrate that when clipping is omitted, learning performance can
fail to reach the optimum; and when clipping is done properly, learning
performance can improve significantly.
  The clipping problem we describe affects algorithms which use explicit
derivatives of the model functions of the environment to calculate a learning
gradient. These include Backpropagation Through Time for Control, and methods
based on Dual Heuristic Dynamic Programming. However the clipping problem does
not significantly affect methods based on Heuristic Dynamic Programming,
Temporal Differences or Policy Gradient Learning algorithms. Similarly, the
clipping problem does not affect fixed-length finite-horizon problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5586</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5586</id><created>2013-02-22</created><authors><author><keyname>Baghdadi</keyname><forenames>Riyadh</forenames></author><author><keyname>Cohen</keyname><forenames>Albert</forenames></author><author><keyname>Guelton</keyname><forenames>Serge</forenames></author><author><keyname>Verdoolaege</keyname><forenames>Sven</forenames></author><author><keyname>Inoue</keyname><forenames>Jun</forenames></author><author><keyname>Grosser</keyname><forenames>Tobias</forenames></author><author><keyname>Kouveli</keyname><forenames>Georgia</forenames></author><author><keyname>Kravets</keyname><forenames>Alexey</forenames></author><author><keyname>Lokhmotov</keyname><forenames>Anton</forenames></author><author><keyname>Nugteren</keyname><forenames>Cedric</forenames></author><author><keyname>Waters</keyname><forenames>Fraser</forenames></author><author><keyname>Donaldson</keyname><forenames>Alastair F.</forenames></author></authors><title>PENCIL: Towards a Platform-Neutral Compute Intermediate Language for
  DSLs</title><categories>cs.PL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We motivate the design and implementation of a platform-neutral compute
intermediate language (PENCIL) for productive and performance-portable
accelerator programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5592</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5592</id><created>2013-02-22</created><updated>2013-04-29</updated><authors><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Seedig</keyname><forenames>Hans Georg</forenames></author></authors><title>A tournament of order 24 with two disjoint TEQ-retentive sets</title><categories>cs.MA math.CO</categories><comments>3 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brandt et al. (2013) have recently disproved a conjecture by Schwartz (1990)
by non-constructively showing the existence of a counterexample with about
10^136 alternatives. We provide a concrete counterexample for Schwartz's
conjecture with only 24 alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5607</identifier>
 <datestamp>2014-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5607</id><created>2013-02-22</created><updated>2013-06-08</updated><authors><author><keyname>Clementi</keyname><forenames>Andrea</forenames></author><author><keyname>di Ianni</keyname><forenames>Miriam</forenames></author><author><keyname>Gambosi</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Emanuele</forenames></author><author><keyname>Silvestri</keyname><forenames>Riccardo</forenames></author></authors><title>Distributed Community Detection in Dynamic Graphs</title><categories>cs.SI cs.DC math.PR</categories><comments>Version II</comments><msc-class>60, 94</msc-class><acm-class>G.3; H.3.3</acm-class><doi>10.1007/978-3-319-03578-9_1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the increasing interest in self-organizing social opportunistic
networks, we investigate the problem of distributed detection of unknown
communities in dynamic random graphs. As a formal framework, we consider the
dynamic version of the well-studied \emph{Planted Bisection Model}
$\sdG(n,p,q)$ where the node set $[n]$ of the network is partitioned into two
unknown communities and, at every time step, each possible edge $(u,v)$ is
active with probability $p$ if both nodes belong to the same community, while
it is active with probability $q$ (with $q&lt;&lt;p$) otherwise. We also consider a
time-Markovian generalization of this model.
  We propose a distributed protocol based on the popular \emph{Label
Propagation Algorithm} and prove that, when the ratio $p/q$ is larger than
$n^{b}$ (for an arbitrarily small constant $b&gt;0$), the protocol finds the right
&quot;planted&quot; partition in $O(\log n)$ time even when the snapshots of the dynamic
graph are sparse and disconnected (i.e. in the case $p=\Theta(1/n)$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5608</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5608</id><created>2013-02-22</created><authors><author><keyname>Glasmachers</keyname><forenames>Tobias</forenames></author><author><keyname>Dogan</keyname><forenames>&#xdc;r&#xfc;n</forenames></author></authors><title>Accelerated Linear SVM Training with Adaptive Variable Selection
  Frequencies</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support vector machine (SVM) training is an active research area since the
dawn of the method. In recent years there has been increasing interest in
specialized solvers for the important case of linear models. The algorithm
presented by Hsieh et al., probably best known under the name of the
&quot;liblinear&quot; implementation, marks a major breakthrough. The method is analog to
established dual decomposition algorithms for training of non-linear SVMs, but
with greatly reduced computational complexity per update step. This comes at
the cost of not keeping track of the gradient of the objective any more, which
excludes the application of highly developed working set selection algorithms.
We present an algorithmic improvement to this method. We replace uniform
working set selection with an online adaptation of selection frequencies. The
adaptation criterion is inspired by modern second order working set selection
methods. The same mechanism replaces the shrinking heuristic. This novel
technique speeds up training in some cases by more than an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5611</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5611</id><created>2013-02-22</created><authors><author><keyname>Arz</keyname><forenames>Julian</forenames></author><author><keyname>Luxen</keyname><forenames>Dennis</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author></authors><title>Transit Node Routing Reconsidered</title><categories>cs.DS</categories><comments>19 pages, submitted to SEA'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transit Node Routing (TNR) is a fast and exact distance oracle for road
networks. We show several new results for TNR. First, we give a surprisingly
simple implementation fully based on Contraction Hierarchies that speeds up
preprocessing by an order of magnitude approaching the time for just finding a
CH (which alone has two orders of magnitude larger query time). We also develop
a very effective purely graph theoretical locality filter without any
compromise in query times. Finally, we show that a specialization to the online
many-to-one (or one-to-many) shortest path further speeds up query time by an
order of magnitude. This variant even has better query time than the fastest
known previous methods which need much more space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5635</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5635</id><created>2013-02-22</created><updated>2013-02-26</updated><authors><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Seiferth</keyname><forenames>Paul</forenames></author></authors><title>Greedy is as Good as Delaunay (Almost)</title><categories>cs.CG</categories><comments>We overlooked the fact that the MST of the greedy triangulation is
  just the EMST of the point set. This makes the main technical part part of
  the paper unnecessary, and the claimed result follows readily from known
  results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let S be a planar point set. Krznaric and Levcopoulos proved that given the
Delaunay triangulation DT(S) for S, one can find the greedy triangulation GT(S)
in linear time. We provide a (partial) converse of this result: given GT(S), it
is possible to compute DT(S) in linear expected time. Thus, these structures
are basically equivalent.
  To obtain our result, we generalize another algorithm by Krznaric and
Levcopoulos to find a hierarchical clustering for S in linear time, once DT(S)
is known. We show that their algorithm remains (almost) correct for any
triangulation of bounded dilation, i.e., any triangulation in which the
shortest path distance between any two points approximates their Euclidean
distance. In general, however, the resulting running time may be superlinear.
Nonetheless, we can show that the properties of the greedy triangulation
suffice to guarantee a linear time bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5645</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5645</id><created>2013-02-18</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Role of temporal inference in the recognition of textual inference</title><categories>cs.CL</categories><comments>2008 thesis, in French</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project is a part of nature language processing and its aims to develop
a system of recognition inference text-appointed TIMINF. This type of system
can detect, given two portions of text, if a text is semantically deducted from
the other. We focused on making the inference time in this type of system. For
that we have built and analyzed a body built from questions collected through
the web. This study has enabled us to classify different types of times
inferences and for designing the architecture of TIMINF which seeks to
integrate a module inference time in a detection system inference text. We also
assess the performance of sorties TIMINF system on a test corpus with the same
strategy adopted in the challenge RTE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5646</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5646</id><created>2013-02-16</created><authors><author><keyname>Jha</keyname><forenames>Pranava K.</forenames></author></authors><title>Comments on &quot;Resource placement in Cartesian product of networks&quot;
  [Imani, Sarbazi-Azad and Zomaya, J. Parallel Distrib. Comput., 70 (2010)
  481-495]</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present note points out a number of errors, omissions, redundancies and
arbitrary deviations from the standard terminology in the paper &quot;Resource
placement in Cartesian product of networks,&quot; by N. Imani, H. Sarbazi-Azad and
A.Y. Zomaya [J. Parallel Distrib. Comput. 70 (2010) 481-495].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5657</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5657</id><created>2013-02-22</created><authors><author><keyname>Gast&#xf3;n</keyname><forenames>Bernat</forenames></author><author><keyname>Pujol</keyname><forenames>Jaume</forenames></author><author><keyname>Villanueva</keyname><forenames>Merc&#xe8;</forenames></author></authors><title>A realistic distributed storage system: the rack model</title><categories>cs.IT cs.DC math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. arXiv admin
  note: text overlap with arXiv:1301.1549</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a realistic distributed storage environment, storage nodes are usually
placed in racks, a metallic support designed to accommodate electronic
equipment. It is known that the communication (bandwidth) cost between nodes
which are in the same rack is much lower than between nodes which are in
different racks.
  In this paper, a new model, where the storage nodes are placed in two racks,
is proposed and analyzed. Moreover, the two-rack model is generalized to any
number of racks. In this model, the storage nodes have different repair costs
depending on the rack where they are placed. A threshold function, which
minimizes the amount of stored data per node and the bandwidth needed to
regenerate a failed node, is shown. This threshold function generalizes the
ones given for previous distributed storage models. The tradeoff curve obtained
from this threshold function is compared with the ones obtained from the
previous models, and it is shown that this new model outperforms the previous
ones in terms of repair cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5662</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5662</id><created>2013-02-22</created><authors><author><keyname>Maric</keyname><forenames>Ivana</forenames></author></authors><title>Low Latency Communications</title><categories>cs.IT math.IT</categories><comments>Presented at the Information Theory and Applications Workshop (ITA
  2013), Feb 10-15, 2013, San Diego, CA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous applications demand communication schemes that minimize the
transmission delay while achieving a given level of reliability. An extreme
case is high-frequency trading whereby saving a fraction of millisecond over a
route between Chicago and New York can be a game-changer. While such
communications are often carried by fiber, microwave links can reduce
transmission delays over large distances due to more direct routes and faster
wave propagation. In order to bridge large distances, information is sent over
a multihop relay network.
  Motivated by these applications, this papers present an information-theoretic
approach to the design of optimal multihop microwave networks that minimizes
end-to-end transmission delay. To characterize the delay introduced by coding,
we derive error exponents achievable in multihop networks. We formulate and
solve an optimization problem that determines optimal selection of
amplify-and-forward and decode-and-forward relays. We present the optimal
solution for several examples of networks. We prove that in high SNR the
optimum transmission scheme is for all relays to perform amplify-and-forward.
We then analyze the impact of deploying noisy feedback
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5669</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5669</id><created>2013-02-22</created><updated>2013-03-01</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>Asymmetric Quantum Codes: New Codes from Old</title><categories>quant-ph cs.IT math.IT</categories><comments>Accepted for publication Quantum Information Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend to asymmetric quantum error-correcting codes (AQECC)
the construction methods, namely: puncturing, extending, expanding, direct sum
and the (u|u + v) construction. By applying these methods, several families of
asymmetric quantum codes can be constructed. Consequently, as an example of
application of quantum code expansion developed here, new families of
asymmetric quantum codes derived from generalized Reed-Muller (GRM) codes,
quadratic residue (QR), Bose-Chaudhuri-Hocquenghem (BCH), character codes and
affine-invariant codes are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5671</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5671</id><created>2013-02-22</created><authors><author><keyname>Myasnikov</keyname><forenames>Alexei</forenames></author><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author><author><keyname>Ushakov</keyname><forenames>Alexander</forenames></author></authors><title>Knapsack Problems in Groups</title><categories>math.GR cs.CC math.CO</categories><comments>28 pages, 12 figures</comments><msc-class>03D15, 20F65, 20F10, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the classical knapsack and subset sum problems to arbitrary
groups and study the computational complexity of these new problems. We show
that these problems, as well as the bounded submonoid membership problem, are
P-time decidable in hyperbolic groups and give various examples of finitely
presented groups where the subset sum problem is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5674</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5674</id><created>2013-01-20</created><updated>2016-02-18</updated><authors><author><keyname>Heinle</keyname><forenames>Albert</forenames></author><author><keyname>Levandovskyy</keyname><forenames>Viktor</forenames></author></authors><title>Factorization of Z-homogeneous polynomials in the First (q)-Weyl Algebra</title><categories>cs.SC cs.MS</categories><comments>26 pages, Singular implementation, 2 algorithms, 1 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms to factorize weighted homogeneous elements in the first
polynomial Weyl algebra and $q$-Weyl algebra, which are both viewed as a
$\mathbb{Z}$-graded rings. We show, that factorization of homogeneous
polynomials can be almost completely reduced to commutative univariate
factorization over the same base field with some additional uncomplicated
combinatorial steps. This allows to deduce the complexity of our algorithms in
detail. Furthermore, we will show for homogeneous polynomials that
irreducibility in the polynomial first Weyl algebra also implies irreducibility
in the rational one, which is of interest for practical reasons. We report on
our implementation in the computer algebra system \textsc{Singular}. It
outperforms for homogeneous polynomials currently available implementations
dealing with factorization in the first Weyl algebra both in speed and elegancy
of the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5675</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5675</id><created>2013-02-22</created><authors><author><keyname>Bdour</keyname><forenames>Wafa N.</forenames></author><author><keyname>Gharaibeh</keyname><forenames>Natheer K.</forenames></author></authors><title>Development of Yes/No Arabic Question Answering System</title><categories>cs.CL cs.IR</categories><comments>13 pages, 4 figures, 5 tables</comments><msc-class>14J26</msc-class><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.4, No.1, January 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Developing Question Answering systems has been one of the important research
issues because it requires insights from a variety of
disciplines,including,Artificial Intelligence,Information Retrieval,
Information Extraction,Natural Language Processing, and Psychology.In this
paper we realize a formal model for a lightweight semantic based open domain
yes/no Arabic question answering system based on paragraph retrieval with
variable length. We propose a constrained semantic representation. Using an
explicit unification framework based on semantic similarities and query
expansion synonyms and antonyms.This frequently improves the precision of the
system. Employing the passage retrieval system achieves a better precision by
retrieving more paragraphs that contain relevant answers to the question; It
significantly reduces the amount of text to be processed by the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5677</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5677</id><created>2013-02-22</created><authors><author><keyname>Gharaibeh</keyname><forenames>Natheer K</forenames></author><author><keyname>Alsmadi</keyname><forenames>Mohareb A</forenames></author></authors><title>The impact of teaching two courses (electronic curriculum design,
  multimedia) on the acquisition of electronic content design skills</title><categories>cs.CY cs.MM</categories><comments>8 pages, 2 tables</comments><journal-ref>The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.4, No.6, December 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The use of Multimedia applications in Learning provides useful concepts for
Instructional Content Design. This study aimed to investigate the effect of
design electronic curriculum and multimedia applications on acquiring e-content
design skills, and improving their attitudes towards e-learning. To achieve the
objective of the study, the researchers developed a test to measure the
efficiencies of designing electronic content and the measure of attitudes
towards e-learning, The results showed that study of both courses contributed
positively to the acquisition of design skills of e-content, The results
revealed that there are statistical significant differences between the scores
of the students in the two applications (pre and post) on the total score of
the attitude measure and three areas of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5679</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5679</id><created>2013-02-22</created><authors><author><keyname>Silva</keyname><forenames>Juliana M. N.</forenames></author><author><keyname>Boeres</keyname><forenames>Cristina</forenames></author><author><keyname>Drummond</keyname><forenames>L&#xfa;cia M. A.</forenames></author><author><keyname>Pessoa</keyname><forenames>Artur A.</forenames></author></authors><title>Memory Aware Load Balance Strategy on a Parallel Branch-and-Bound
  Application</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The latest trends in high-performance computing systems show an increasing
demand on the use of a large scale multicore systems in a efficient way, so
that high compute-intensive applications can be executed reasonably well.
However, the exploitation of the degree of parallelism available at each
multicore component can be limited by the poor utilization of the memory
hierarchy available. Actually, the multicore architecture introduces some
distinct features that are already observed in shared memory and distributed
environments. One example is that subsets of cores can share different subsets
of memory. In order to achieve high performance it is imperative that a careful
allocation scheme of an application is carried out on the available cores,
based on a scheduling model that considers the main performance bottlenecks, as
for example, memory contention. In this paper, the {\em Multicore Cluster
Model} (MCM) is proposed, which captures the most relevant performance
characteristics in multicores systems such as the influence of memory hierarchy
and contention. Better performance was achieved when a load balance strategy
for a Branch-and-Bound application applied to the Partitioning Sets Problem is
based on MCM, showing its efficiency and applicability to modern systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5681</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5681</id><created>2013-02-21</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Leung</keyname><forenames>Samantha</forenames></author></authors><title>Weighted Sets of Probabilities and Minimax Weighted Expected Regret: New
  Approaches for Representing Uncertainty and Making Decisions</title><categories>cs.GT cs.AI</categories><comments>Full version of an article [arXiv:1210.4853] that appeared in UAI '12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting where an agent's uncertainty is represented by a set of
probability measures, rather than a single measure. Measure-by-measure updating
of such a set of measures upon acquiring new information is well-known to
su?ffer from problems; agents are not always able to learn appropriately. To
deal with these problems, we propose using weighted sets of probabilities: a
representation where each measure is associated with a weight, which denotes
its significance. We describe a natural approach to updating in such a
situation and a natural approach to determining the weights. We then show how
this representation can be used in decision-making, by modifying a standard
approach to decision making -- minimizing expected regret -- to obtain minimax
weighted expected regret (MWER). We provide an axiomatization that
characterizes preferences induced by MWER both in the static and dynamic case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5683</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5683</id><created>2013-02-22</created><updated>2016-01-26</updated><authors><author><keyname>Schlei</keyname><forenames>B. R.</forenames></author></authors><title>STEVE - Space-Time-Enclosing Volume Extraction</title><categories>cs.CG cs.GR</categories><comments>14 pages, 28 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The novel STEVE (i.e., Space-Time-Enclosing Volume Extraction) algorithm is
described here for the very first time. It generates iso-valued hypersurfaces
that may be implicitly contained in four-dimensional (4D) data sets, such as
temporal sequences of three-dimensional images from time-varying computed
tomography. Any final hypersurface that will be generated by STEVE is
guaranteed to be free from accidental rifts, i.e., it always fully encloses a
region in the 4D space under consideration. Furthermore, the information of the
interior/exterior of the enclosed regions is propagated to each one of the
tetrahedrons, which are embedded into 4D and which in their union represent the
final, iso-valued hypersurface(s). STEVE is usually executed in a purely
data-driven mode, and it uses lesser computational resources than other
techniques that also generate simplex-based manifolds of codimension 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5696</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5696</id><created>2013-02-22</created><authors><author><keyname>Farsani</keyname><forenames>Reza K.</forenames></author></authors><title>Capacity Bounds for Wireless Ergodic Fading Broadcast Channels with
  Partial CSIT</title><categories>cs.IT math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-user wireless ergodic fading Broadcast Channel (BC) with partial
Channel State Information at the Transmitter (CSIT) is considered. The CSIT is
given by an arbitrary deterministic function of the channel state. This
characteristic yields a full control over how much state information is
available, from perfect to no information. In literature, capacity derivations
for wireless ergodic fading channels, specifically for fading BCs, mostly rely
on the analysis of channels comprising of parallel sub-channels. This technique
is usually suitable for the cases where perfect state information is available
at the transmitters. In this paper, new arguments are proposed to directly
derive (without resorting to the analysis of parallel channels) capacity bounds
for the two-user fading BC with both common and private messages based on the
existing bounds for the discrete channel. Specifically, a novel approach is
developed to adapt and evaluate the well-known UV-outer bound for the Gaussian
fading channel using the entropy power inequality. Our approach indeed sheds
light on the role of broadcast auxiliaries in the fading channel. It is shown
that the derived outer bound is optimal for the channel with perfect CSIT as
well as for some special cases with partial CSIT. Our outer bound is also
directly applicable to the case without CSIT which has been recently considered
in several papers. Next, the approach is developed to analyze for the fading BC
with secrecy. In the case of perfect CSIT, a full characterization of the
secrecy capacity region is derived for the channel with common and confidential
messages. This result completes a gap in a previous work by Ekrem and Ulukus.
For the channel without common message, the secrecy capacity region is also
derived when the transmitter has access only to the degradedness ordering of
the channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5711</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5711</id><created>2013-02-22</created><authors><author><keyname>G&#xfc;nther</keyname><forenames>Felix</forenames></author><author><keyname>Mustata</keyname><forenames>Irina</forenames></author></authors><title>On a game on graphs</title><categories>math.CO cs.DM math.LO</categories><comments>10 pages, 5 figures</comments><msc-class>05C57</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We start with the well-known game below: Two players hold a sheet of paper to
their forehead on which a positive integer is written. The numbers are
consecutive and each player can only see the number of the other one. In each
time step, they either say nothing or tell what number they have. Both of them
will eventually figure out their number after a certain amount of time. The
game is rather cooperative than competitive, and employs the notions of common
knowledge and mutual knowledge. We generalize this game to arbitrary (directed
and non-directed) simple graphs and try to establish for which graphs one or
both of them will figure out the solution, and how long they do need to find
it. We give a complete answer for the case of two players, even if they are
both allowed to discuss before the start of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5724</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5724</id><created>2013-02-22</created><updated>2013-07-11</updated><authors><author><keyname>Horel</keyname><forenames>Thibaut</forenames></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author></authors><title>Budget Feasible Mechanisms for Experimental Design</title><categories>cs.GT</categories><journal-ref>LATIN 2014: Theoretical Informatics. Lecture Notes in Computer
  Science Volume 8392, 2014, pp 719-730</journal-ref><doi>10.1007/978-3-642-54423-1_62</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical experimental design setting, an experimenter E has access to
a population of $n$ potential experiment subjects $i\in \{1,...,n\}$, each
associated with a vector of features $x_i\in R^d$. Conducting an experiment
with subject $i$ reveals an unknown value $y_i\in R$ to E. E typically assumes
some hypothetical relationship between $x_i$'s and $y_i$'s, e.g., $y_i \approx
\beta x_i$, and estimates $\beta$ from experiments, e.g., through linear
regression. As a proxy for various practical constraints, E may select only a
subset of subjects on which to conduct the experiment.
  We initiate the study of budgeted mechanisms for experimental design. In this
setting, E has a budget $B$. Each subject $i$ declares an associated cost $c_i
&gt;0$ to be part of the experiment, and must be paid at least her cost. In
particular, the Experimental Design Problem (EDP) is to find a set $S$ of
subjects for the experiment that maximizes $V(S) = \log\det(I_d+\sum_{i\in
S}x_i\T{x_i})$ under the constraint $\sum_{i\in S}c_i\leq B$; our objective
function corresponds to the information gain in parameter $\beta$ that is
learned through linear regression methods, and is related to the so-called
$D$-optimality criterion. Further, the subjects are strategic and may lie about
their costs.
  We present a deterministic, polynomial time, budget feasible mechanism
scheme, that is approximately truthful and yields a constant factor
approximation to EDP. In particular, for any small $\delta &gt; 0$ and $\epsilon &gt;
0$, we can construct a (12.98, $\epsilon$)-approximate mechanism that is
$\delta$-truthful and runs in polynomial time in both $n$ and
$\log\log\frac{B}{\epsilon\delta}$. We also establish that no truthful,
budget-feasible algorithms is possible within a factor 2 approximation, and
show how to generalize our approach to a wide class of learning problems,
beyond linear regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5729</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5729</id><created>2013-02-22</created><updated>2014-01-03</updated><authors><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author><author><keyname>Bayram</keyname><forenames>Ilker</forenames></author></authors><title>Sparse Signal Estimation by Maximally Sparse Convex Optimization</title><categories>cs.LG stat.ML</categories><comments>13 pages, 9 figures</comments><doi>10.1109/TSP.2014.2298839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of sparsity penalized least squares for
applications in sparse signal processing, e.g. sparse deconvolution. This paper
aims to induce sparsity more strongly than L1 norm regularization, while
avoiding non-convex optimization. For this purpose, this paper describes the
design and use of non-convex penalty functions (regularizers) constrained so as
to ensure the convexity of the total cost function, F, to be minimized. The
method is based on parametric penalty functions, the parameters of which are
constrained to ensure convexity of F. It is shown that optimal parameters can
be obtained by semidefinite programming (SDP). This maximally sparse convex
(MSC) approach yields maximally non-convex sparsity-inducing penalty functions
constrained such that the total cost function, F, is convex. It is demonstrated
that iterative MSC (IMSC) can yield solutions substantially more sparse than
the standard convex sparsity-inducing approach, i.e., L1 norm minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5734</identifier>
 <datestamp>2013-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5734</id><created>2013-02-22</created><updated>2013-06-14</updated><authors><author><keyname>Gong</keyname><forenames>Xun</forenames></author><author><keyname>Rodrigues</keyname><forenames>Mavis</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Invisible Flow Watermarks for Channels with Dependent Substitution,
  Deletion, and Bursty Insertion Errors</title><categories>cs.CR cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flow watermarks efficiently link packet flows in a network in order to thwart
various attacks such as stepping stones. We study the problem of designing good
flow watermarks. Earlier flow watermarking schemes mostly considered
substitution errors, neglecting the effects of packet insertions and deletions
that commonly happen within a network. More recent schemes consider packet
deletions but often at the expense of the watermark visibility. We present an
invisible flow watermarking scheme capable of enduring a large number of packet
losses and insertions. To maintain invisibility, our scheme uses quantization
index modulation (QIM) to embed the watermark into inter-packet delays, as
opposed to time intervals including many packets. As the watermark is injected
within individual packets, packet losses and insertions may lead to watermark
desynchronization and substitution errors. To address this issue, we add a
layer of error-correction coding to our scheme. Experimental results on both
synthetic and real network traces demonstrate that our scheme is robust to
network jitter, packet drops and splits, while remaining invisible to an
attacker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5754</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5754</id><created>2013-02-22</created><authors><author><keyname>Nittoor</keyname><forenames>Vivek S</forenames></author><author><keyname>Suda</keyname><forenames>Reiji</forenames></author></authors><title>Enumeration Based Search Algorithm For Finding A Regular Bi-partite
  Graph Of Maximum Attainable Girth For Specified Degree And Number Of Vertices</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a search problem for finding a regular bi-partite graph of
maximum attainable girth for specified degree and number of vertices, by
restricting the search space using a series of mathematically rigourous
arguments from [1] and [2]. The goal of this paper is to derive the enumeration
search algorithm for finding a girth maximum (m, r) BTU, which is notation for
regular partite graph that has been introduced in [1], using the optimal
partition results from [2] as a starting point, and also understand the
structure of the search space and the computational complexity of the
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5755</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5755</id><created>2013-02-22</created><authors><author><keyname>Nittoor</keyname><forenames>Vivek S</forenames></author><author><keyname>Suda</keyname><forenames>Reiji</forenames></author></authors><title>Analysis Of The Girth For Regular Bi-partite Graphs With Degree 3</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to derive the detailed description of the
Enumeration Based Search Algorithm from the high level description provided in
[16], analyze the experimental results from our implementation of the
Enumeration Based Search Algorithm for finding a regular bi-partite graph of
degree 3, and compare it with known results from the available literature. We
show that the values of m for a given girth g for (m, 3) BTUs are within the
known mathematical bounds for regular bi-partitite graphs from the available
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5762</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5762</id><created>2013-02-22</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Tracey</keyname><forenames>Brian</forenames></author><author><keyname>Natarajan</keyname><forenames>Premkumar</forenames></author><author><keyname>Noonan</keyname><forenames>Joseph P.</forenames></author></authors><title>Probabilistic Non-Local Means</title><categories>cs.CV stat.AP stat.CO</categories><comments>11 pages, 3 figures</comments><doi>10.1109/LSP.2013.2263135</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a so-called probabilistic non-local means (PNLM)
method for image denoising. Our main contributions are: 1) we point out defects
of the weight function used in the classic NLM; 2) we successfully derive all
theoretical statistics of patch-wise differences for Gaussian noise; and 3) we
employ this prior information and formulate the probabilistic weights truly
reflecting the similarity between two noisy patches. The probabilistic nature
of the new weight function also provides a theoretical basis to choose
thresholds rejecting dissimilar patches for fast computations. Our simulation
results indicate the PNLM outperforms the classic NLM and many NLM recent
variants in terms of peak signal noise ratio (PSNR) and structural similarity
(SSIM) index. Encouraging improvements are also found when we replace the NLM
weights with the probabilistic weights in tested NLM variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5765</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5765</id><created>2013-02-23</created><updated>2013-03-27</updated><authors><author><keyname>Kimura</keyname><forenames>Daisuke</forenames><affiliation>National Institute of Informatics</affiliation></author><author><keyname>Tatsuta</keyname><forenames>Makoto</forenames><affiliation>National Institute of Informatics</affiliation></author></authors><title>Call-by-Value and Call-by-Name Dual Calculi with Inductive and
  Coinductive Types</title><categories>cs.LO cs.PL</categories><comments>The conference version of this paper has appeared in RTA 2009</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (March 29,
  2013) lmcs:1055</journal-ref><doi>10.2168/LMCS-9(1:14)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the dual calculus with inductive types and coinductive
types. The paper first introduces a non-deterministic dual calculus with
inductive and coinductive types. Besides the same duality of the original dual
calculus, it has the duality of inductive and coinductive types, that is, the
duality of terms and coterms for inductive and coinductive types, and the
duality of their reduction rules. Its strong normalization is also proved,
which is shown by translating it into a second-order dual calculus. The strong
normalization of the second-order dual calculus is proved by translating it
into the second-order symmetric lambda calculus. This paper then introduces a
call-by-value system and a call-by-name system of the dual calculus with
inductive and coinductive types, and shows the duality of call-by-value and
call-by-name, their Church-Rosser properties, and their strong normalization.
Their strong normalization is proved by translating them into the
non-deterministic dual calculus with inductive and coinductive types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5788</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5788</id><created>2013-02-23</created><authors><author><keyname>Seth</keyname><forenames>Ashish</forenames></author><author><keyname>Agarwal</keyname><forenames>Himanshu</forenames></author><author><keyname>Singla</keyname><forenames>Ashim Raj</forenames></author></authors><title>Unified Modeling Language for Describing Business Value Chain Activities</title><categories>cs.SE</categories><comments>6 pages, 11 figures</comments><journal-ref>Proceedings published by International Journal of Computer
  Applications(IJCA),2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the market competition aggravating, it becomes necessary for market
players to adopt a business model which can adopt dynamic business changes. Any
enterprise has the possibility to win in the competition only when it forms the
strategic alliance with the upstream and downstream enterprise. This paper
articulates a way of using unified modelling language (UML) to develop business
value chain activities for any enterprise to develop dynamic, adhoc and agile
business model. The results show that the UML is useful in the development of
information systems and is independent of any programming language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5793</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5793</id><created>2013-02-23</created><updated>2013-04-02</updated><authors><author><keyname>Ananichev</keyname><forenames>Dmitry S.</forenames></author><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Volkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>Primitive digraphs with large exponents and slowly synchronizing
  automata</title><categories>cs.FL</categories><comments>23 pages, 11 figures, 3 tables. This is a translation (with a
  slightly updated bibliography) of the authors' paper published in Russian in:
  Zapiski Nauchnyh Seminarov POMI [Kombinatorika i Teorija Grafov. IV], Vol.
  402, 9-39 (2012), see ftp://ftp.pdmi.ras.ru/pub/publicat/znsl/v402/p009.pdf
  Version 2: a few typos are corrected</comments><msc-class>68Q45, 68R10</msc-class><journal-ref>J. Math. Sci. 192 (2013), 263-278</journal-ref><doi>10.1007/s10958-013-1392-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several infinite series of synchronizing automata for which the
minimum length of reset words is close to the square of the number of states.
All these automata are tightly related to primitive digraphs with large
exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5794</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5794</id><created>2013-02-23</created><authors><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Srinivasan</keyname><forenames>Sriram</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Bhowmick</keyname><forenames>Sanjukta</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>Constant Communities in Complex Networks</title><categories>physics.soc-ph cs.SI</categories><comments>31 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Identifying community structure is a fundamental problem in network analysis.
Most community detection algorithms are based on optimizing a combinatorial
parameter, for example modularity. This optimization is generally NP-hard, thus
merely changing the vertex order can alter their assignments to the community.
However, there has been very less study on how vertex ordering influences the
results of the community detection algorithms. Here we identify and study the
properties of invariant groups of vertices (constant communities) whose
assignment to communities are, quite remarkably, not affected by vertex
ordering. The percentage of constant communities can vary across different
applications and based on empirical results we propose metrics to evaluate
these communities. Using constant communities as a pre-processing step, one can
significantly reduce the variation of the results. Finally, we present a case
study on phoneme network and illustrate that constant communities, quite
strikingly, form the core functional units of the larger communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5797</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5797</id><created>2013-02-23</created><authors><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Lugosi</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Neu</keyname><forenames>Gergely</forenames></author></authors><title>Prediction by Random-Walk Perturbation</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a version of the follow-the-perturbed-leader online prediction
algorithm in which the cumulative losses are perturbed by independent symmetric
random walks. The forecaster is shown to achieve an expected regret of the
optimal order O(sqrt(n log N)) where n is the time horizon and N is the number
of experts. More importantly, it is shown that the forecaster changes its
prediction at most O(sqrt(n log N)) times, in expectation. We also extend the
analysis to online combinatorial optimization and show that even in this more
general setting, the forecaster rarely switches between experts while having a
regret of near-optimal order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5798</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5798</id><created>2013-02-23</created><authors><author><keyname>Gay</keyname><forenames>Simon</forenames></author><author><keyname>Kelly</keyname><forenames>Paul</forenames></author></authors><title>Proceedings Fifth Workshop on Programming Language Approaches to
  Concurrency- and Communication-cEntric Software</title><categories>cs.PL cs.DC cs.LO cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 109, 2013</journal-ref><doi>10.4204/EPTCS.109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PLACES 2012 (full title: Programming Language Approaches to Concurrency- and
Communication-Centric Software) is the fifth edition of the PLACES workshop
series. After the first PLACES, which was affiliated to DisCoTec in 2008, the
workshop has been part of ETAPS every year since 2009 and is now an established
part of the ETAPS satellite events. PLACES 2012 was held on 31st March in
Tallinn, Estonia.
  The workshop series was started in order to promote the application of novel
programming language ideas to the increasingly important problem of developing
software for systems in which concurrency and communication are intrinsic
aspects. This includes software for both multi-core systems and large-scale
distributed and/or service-oriented systems. The scope of PLACES includes new
programming language features, whole new programming language designs, new type
systems, new semantic approaches, new program analysis techniques, and new
implementation mechanisms.
  This year's call for papers attracted 17 submissions, from which the
programme committee selected 10 papers for presentation at the workshop. After
the workshop, all of the authors were invited to produce revised versions of
their papers for inclusion in the EPTCS proceedings. The authors of six papers
accepted the invitation, and those papers constitute the present volume.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5820</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5820</id><created>2013-02-23</created><authors><author><keyname>Lu</keyname><forenames>Songjian</forenames></author><author><keyname>Lu</keyname><forenames>Xinghua</forenames></author></authors><title>An exact algorithm with the time complexity of $O^*(1.299^m)$ for the
  weighed mutually exclusive set cover problem</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we will introduce an exact algorithm with a time complexity of
$O^*(1.299^m)$ for the {\sc weighted mutually exclusive set cover} problem,
where $m$ is the number of subsets in the problem. This problem has important
applications in recognizing mutation genes that cause different cancer
diseases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5824</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5824</id><created>2013-02-23</created><authors><author><keyname>Duffy</keyname><forenames>B.</forenames></author><author><keyname>Dasgupta</keyname><forenames>A.</forenames></author><author><keyname>Kosara</keyname><forenames>R.</forenames></author><author><keyname>Walton</keyname><forenames>S.</forenames></author><author><keyname>Chen</keyname><forenames>M.</forenames></author></authors><title>Measuring Visual Complexity of Cluster-Based Visualizations</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handling visual complexity is a challenging problem in visualization owing to
the subjectiveness of its definition and the difficulty in devising
generalizable quantitative metrics. In this paper we address this challenge by
measuring the visual complexity of two common forms of cluster-based
visualizations: scatter plots and parallel coordinatess. We conceptualize
visual complexity as a form of visual uncertainty, which is a measure of the
degree of difficulty for humans to interpret a visual representation correctly.
We propose an algorithm for estimating visual complexity for the aforementioned
visualizations using Allen's interval algebra. We first establish a set of
primitive 2-cluster cases in scatter plots and another set for parallel
coordinatess based on symmetric isomorphism. We confirm that both are the
minimal sets and verify the correctness of their members computationally. We
score the uncertainty of each primitive case based on its topological
properties, including the existence of overlapping regions, splitting regions
and meeting points or edges. We compare a few optional scoring schemes against
a set of subjective scores by humans, and identify the one that is the most
consistent with the subjective scores. Finally, we extend the 2-cluster measure
to k-cluster measure as a general purpose estimator of visual complexity for
these two forms of cluster-based visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5843</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5843</id><created>2013-02-23</created><updated>2014-01-24</updated><authors><author><keyname>Lucas</keyname><forenames>Andrew</forenames></author></authors><title>Ising formulations of many NP problems</title><categories>cond-mat.stat-mech cs.CC cs.DS quant-ph</categories><comments>27 pages; v2: substantial revision to intro/conclusion, many more
  references; v3: substantial revision and extension, to-be-published version</comments><journal-ref>Frontiers in Physics 2, 5 (2014)</journal-ref><doi>10.3389/fphy.2014.00005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide Ising formulations for many NP-complete and NP-hard problems,
including all of Karp's 21 NP-complete problems. This collects and extends
mappings to the Ising model from partitioning, covering and satisfiability. In
each case, the required number of spins is at most cubic in the size of the
problem. This work may be useful in designing adiabatic quantum optimization
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5848</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5848</id><created>2013-02-23</created><updated>2013-02-26</updated><authors><author><keyname>Turner</keyname><forenames>D. Z.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author><author><keyname>Martinez</keyname><forenames>M. J.</forenames></author></authors><title>A framework for coupling flow and deformation of the porous solid</title><categories>cs.NA math.NA physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the flow of an incompressible fluid in a
deformable porous solid. We present a mathematical model using the framework
offered by the theory of interacting continua. In its most general form, this
framework provides a mechanism for capturing multiphase flow, deformation,
chemical reactions and thermal processes, as well as interactions between the
various physics in a conveniently implemented fashion. To simplify the
presentation of the framework, results are presented for a particular model
than can be seen as an extension of Darcy's equation (which assumes that the
porous solid is rigid) that takes into account elastic deformation of the
porous solid. The model also considers the effect of deformation on porosity.
We show that using this model one can recover identical results as in the
framework proposed by Biot and Terzaghi. Some salient features of the framework
are as follows: (a) It is a consistent mixture theory model, and adheres to the
laws and principles of continuum thermodynamics, (b) the model is capable of
simulating various important phenomena like consolidation and surface
subsidence, and (c) the model is amenable to several extensions. We also
present numerical coupling algorithms to obtain coupled flow-deformation
response. Several representative numerical examples are presented to illustrate
the capability of the mathematical model and the performance of the
computational framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5851</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5851</id><created>2013-02-23</created><authors><author><keyname>Pace</keyname><forenames>Matthew Felice</forenames></author><author><keyname>Tiskin</keyname><forenames>Alexander</forenames></author></authors><title>Parallel Suffix Array Construction by Accelerated Sampling</title><categories>cs.DC cs.DS</categories><comments>12 pages</comments><acm-class>F.1.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deterministic BSP algorithm for constructing the suffix array of a given
string is presented, based on a technique which we call accelerated sampling.
It runs in optimal O(n/p) local computation and communication, and requires a
near optimal O(log log p) synchronisation steps. The algorithm provides an
improvement over the synchronisation costs of existing algorithms, and
reinforces the importance of the sampling technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5860</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5860</id><created>2013-02-23</created><authors><author><keyname>Agarwal</keyname><forenames>Mukul</forenames></author><author><keyname>Mitter</keyname><forenames>Sanjoy</forenames></author><author><keyname>Sahai</keyname><forenames>Anant</forenames></author></authors><title>A universal, operational theory of unicast multi-user communication with
  fidelity criteria</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a three part paper.
  Optimality of source-channel separation for communication with a fidelity
criterion when the channel is compound as defined by Csiszar and Korner in
their book and general as defined by Verdu and Han, is proved in Part I. It is
assumed that random codes are permitted. The word &quot;universal&quot; in the title of
this paper refers to the fact that the channel model is compound. The proof
uses a layered black-box or a layered input-output view-point. In particular,
only the end-to-end description of the channel as being capable of
communicating a source to within a certain distortion level is used when
proving separation. This implies that the channel model does not play any role
for separation to hold as long as there is a source model. Further implications
of the layered black-box view-point are discussed.
  Optimality of source-medium separation for multi-user communication with
fidelity criteria over a general, compound medium in the unicast setting is
proved in Part II, thus generalizing Part I to the unicast, multi-user setting.
  Part III gets to an understanding of the question, &quot;Why is a channel which is
capable of communicating a source to within a certain distortion level, also
capable of communicating bits at any rate less than the infimum of the rates
needed to code the source to within the distortion level&quot;: this lies at the
heart of why optimality of separation for communication with a fidelity
criterion holds. The perspective taken to get to this understanding is a
randomized covering-packing perspective, and the proof is operational.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5867</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5867</id><created>2013-02-23</created><authors><author><keyname>Abbaszadeh</keyname><forenames>Masoud</forenames></author><author><keyname>Marquez</keyname><forenames>Horacio J.</forenames></author></authors><title>Design of Nonlinear State Observers for One-Sided Lipschitz Systems</title><categories>cs.SY math.DS math.OC</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control and state estimation of nonlinear systems satisfying a Lipschitz
continuity condition have been important topics in nonlinear system theory for
over three decades, resulting in a substantial amount of literature. The main
criticism behind this approach, however, has been the restrictive nature of the
Lipschitz continuity condition and the conservativeness of the related results.
This work deals with an extension to this problem by introducing a more general
family of nonlinear functions, namely one-sided Lipschitz functions. The
corresponding class of systems is a superset of its well-known Lipschitz
counterpart and possesses inherent advantages with respect to conservativeness.
In this paper, first the problem of state observer design for this class of
systems is established, the challenges are discussed and some analysis-oriented
tools are provided. Then, a solution to the observer design problem is proposed
in terms of nonlinear matrix inequalities which in turn are converted into
numerically efficiently solvable linear matrix inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5871</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5871</id><created>2013-02-24</created><authors><author><keyname>Kapoor</keyname><forenames>S.</forenames></author><author><keyname>Sarwat</keyname><forenames>M.</forenames></author></authors><title>The Budgeted Transportation Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a transportation problem with sets of sources and sinks. There are
profits and prices on the edges. The goal is to maximize the profit while
meeting the following constraints; the total flow going out of a source must
not exceed its capacity and the total price of the incoming flow on a sink must
not exceed its budget. This problem is closely related to the generalized flow
problem.
  We propose an auction based primal dual approximation algorithm to solve the
problem. The complexity is $O(\epsilon^{-1}(n^2+ n\log{m})m\log U)$ where $n$
is the number of sources, $m$ is the number of sinks, $U$ is the ratio of the
maximum profit/price to the minimum profit/price.
  We also show how to generalize the scheme to solve a more general version of
the problem, where there are edge capacities and/or the profit function is
concave and piece-wise linear. The complexity of the algorithm depends on the
number of linear segments, termed ${\cal L}$, of the profit function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5872</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5872</id><created>2013-02-24</created><authors><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>A Piggybacking Design Framework for Read-and Download-efficient
  Distributed Storage Codes</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>Extended version of ISIT 2013 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new 'piggybacking' framework for designing distributed storage
codes that are efficient in data-read and download required during node-repair.
We illustrate the power of this framework by constructing classes of explicit
codes that entail the smallest data-read and download for repair among all
existing solutions for three important settings: (a) codes meeting the
constraints of being Maximum-Distance-Separable (MDS), high-rate and having a
small number of substripes, arising out of practical considerations for
implementation in data centers, (b) binary MDS codes for all parameters where
binary MDS codes exist, (c) MDS codes with the smallest repair-locality. In
addition, we employ this framework to enable efficient repair of parity nodes
in existing codes that were originally constructed to address the repair of
only the systematic nodes. The basic idea behind our framework is to take
multiple instances of existing codes and add carefully designed functions of
the data of one instance to the other. Typical savings in data-read during
repair is 25% to 50% depending on the choice of the code parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5878</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5878</id><created>2013-02-24</created><authors><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Hu</keyname><forenames>Ke</forenames></author><author><keyname>Tang</keyname><forenames>Yi</forenames></author></authors><title>Robustness of Link-prediction Algorithm Based on Similarity and
  Application to Biological Networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 4 figues</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Many algorithms have been proposed to predict missing links in a variety of
real networks. These studies focus on mainly both accuracy and efficiency of
these algorithms. However, little attention is paid to their robustness against
either noise or irrationality of a link existing in almost all of real
networks. In this paper, we investigate the robustness of several typical
node-similarity-based algorithms and find that these algorithms are sensitive
to the strength of noise. Moreover, we find that it also depends on networks'
structure properties, especially on network efficiency, clustering coefficient
and average degree. In addition, we make an attempt to enhance the robustness
by using link weighting method to transform un-weighted network to weighted one
and then make use of weights of links to characterize their reliability. The
result shows that proper link weighting scheme can enhance both robustness and
accuracy of these algorithms significantly in biological networks while it
brings little computational effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5889</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5889</id><created>2013-02-24</created><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author></authors><title>Characterization of Minimum Cycle Basis in Weighted Partial 2-trees</title><categories>cs.DM math.CO</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a weighted outerplanar graph, the set of lex short cycles is known to be
a minimum cycle basis [Inf. Process. Lett. 110 (2010) 970-974 ]. In this work,
we show that the set of lex short cycles is a minimum cycle basis in weighted
partial 2-trees (graphs of treewidth two) which is a superclass of outerplanar
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5894</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5894</id><created>2013-02-24</created><authors><author><keyname>Eini</keyname><forenames>Sonya</forenames></author><author><keyname>Chalechale</keyname><forenames>Abdolah</forenames></author></authors><title>Four Side Distance: A New Fourier Shape Signature</title><categories>cs.CV</categories><comments>6 pages, 7 figures, International Journal of Advanced Studies in
  Computers, Science and Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shape is one of the main features in content based image retrieval (CBIR).
This paper proposes a new shape signature. In this technique, features of each
shape are extracted based on four sides of the rectangle that covers the shape.
The proposed technique is Fourier based and it is invariant to translation,
scaling and rotation. The retrieval performance between some commonly used
Fourier based signatures and the proposed four sides distance (FSD) signature
has been tested using MPEG-7 database. Experimental results are shown that the
FSD signature has better performance compared with those signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5898</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5898</id><created>2013-02-24</created><updated>2013-08-12</updated><authors><author><keyname>Chen</keyname><forenames>Shenshi</forenames></author><author><keyname>Chen</keyname><forenames>Yaqing</forenames></author><author><keyname>Yang</keyname><forenames>Quanhai</forenames></author></authors><title>Towards Randomized Testing of $q$-Monomials in Multivariate Polynomials</title><categories>cs.CC</categories><comments>21 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1007.2675, arXiv:1007.2678, arXiv:1007.2673 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given any fixed integer $q\ge 2$, a $q$-monomial is of the format
$\displaystyle x^{s_1}_{i_1}x^{s_2}_{i_2}...x_{i_t}^{s_t}$ such that $1\le s_j
\le q-1$, $1\le j \le t$. $q$-monomials are natural generalizations of
multilinear monomials. Recent research on testing multilinear monomials and
$q$-monomails for prime $q$ in multivariate polynomials relies on the property
that $Z_q$ is a field when $q\ge 2 $ is prime. When $q&gt;2$ is not prime, it
remains open whether the problem of testing $q$-monomials can be solved in some
compatible complexity. In this paper, we present a randomized $O^*(7.15^k)$
algorithm for testing $q$-monomials of degree $k$ that are found in a
multivariate polynomial that is represented by a tree-like circuit with a
polynomial size, thus giving a positive, affirming answer to the above
question. Our algorithm works regardless of the primality of $q$ and improves
upon the time complexity of the previously known algorithm for testing
$q$-monomials for prime $q&gt;7$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5903</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5903</id><created>2013-02-24</created><updated>2013-03-03</updated><authors><author><keyname>Viswanathan</keyname><forenames>Arvind</forenames></author><author><keyname>Murthy</keyname><forenames>Dr. Garimella Rama</forenames></author></authors><title>Heterogeneous Dynamic Priority Scheduling in time critical applications:
  Mobile Wireless Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the unlicensed band, the notion of primary user and secondary user (To
implement cognitive radio) is not explicit. By dynamic priority assignment we
propose to implement cognitive radio in the unlicensed band. In time critical
events, the data which is most important, has to be given the time slots.
Wireless Sensor nodes in our case are considered to be mobile, and hence make
it difficult to prioritize one over another. A node may be out of the reach of
the cluster head or base station by the time it is allotted a time slot and
hence mobility is a constraint. With the data changing dynamically and factors
such as energy and mobility, which are major constraints, assigning priority to
the nodes becomes difficult. In this paper, we have discussed about how
Wireless Sensor Networks are able to allocate priorities to nodes in the
unlicensed band with multiple parameters being posed. We have done simulations
on NS-2 and have shown the implementation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5906</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5906</id><created>2013-02-24</created><updated>2014-06-08</updated><authors><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author></authors><title>Achieving AWGN Channel Capacity With Lattice Gaussian Coding</title><categories>cs.IT math.IT</categories><comments>IEEE Trans. Information Theory, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new coding scheme using only one lattice that achieves the
$\frac{1}{2}\log(1+\SNR)$ capacity of the additive white Gaussian noise (AWGN)
channel with lattice decoding, which is provable for signal-to-noise ratio
$\SNR&gt;e$ at present. The scheme applies a discrete Gaussian distribution over
an AWGN-good lattice, but otherwise does not require a shaping lattice or
dither. Thus, it significantly simplifies the default lattice coding scheme of
Erez and Zamir which involves a quantization-good lattice as well as an
AWGN-good lattice. Using the flatness factor, we show that the error
probability of the proposed scheme under minimum mean-square error (MMSE)
lattice decoding is almost the same as that of Erez and Zamir, for any rate up
to the AWGN channel capacity. We introduce the notion of good constellations,
which carry almost the same mutual information as that of continuous Gaussian
inputs. We also address the implementation of Gaussian shaping for the proposed
lattice Gaussian coding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5909</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5909</id><created>2013-02-24</created><authors><author><keyname>Baggio</keyname><forenames>Rodolfo</forenames></author></authors><title>Studying complex tourism systems: a novel approach based on networks
  derived from a time series</title><categories>physics.soc-ph cs.SI</categories><comments>14 pages, 4 figures. Paper for the XIV April International Academic
  Conference on Economic and Social Development, Moscow, April 2-5, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tourism destination is a complex dynamic system. As such it requires
specific methods and tools to be analyzed and understood in order to better
tailor governance and policy measures for steering the destination along an
evolutionary growth path. Many proposals have been put forward for the
investigation of complex systems and some have been successfully applied to
tourism destinations. This paper uses a recent suggestion, that of transforming
a time series into a network and analyzes it with the objective of uncovering
the structural and dynamic features of a tourism destination. The algorithm,
called visibility graph, is simple and its implementation straightforward, yet
it is able to provide a number of interesting insights. An example is worked
out using data from two destinations: Italy as a country and the island of
Elba, one of its most known areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5910</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5910</id><created>2013-02-24</created><updated>2013-07-04</updated><authors><author><keyname>Yan</keyname><forenames>Yanfei</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author></authors><title>Polar Lattices: Where Ar{\i}kan Meets Forney</title><categories>cs.IT math.IT</categories><comments>ISIT 2013. This is the authors' version with the correct &quot;lattice
  formula&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the explicit construction of a new class of
lattices based on polar codes, which are provably good for the additive white
Gaussian noise (AWGN) channel. We follow the multilevel construction of Forney
\textit{et al.} (i.e., Construction D), where the code on each level is a
capacity-achieving polar code for that level. The proposed polar lattices are
efficiently decodable by using multistage decoding. Computable performance
bounds are derived to measure the gap to the generalized capacity at given
error probability. A design example is presented to demonstrate the performance
of polar lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5913</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5913</id><created>2013-02-24</created><authors><author><keyname>Gupta</keyname><forenames>Anupam</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author></authors><title>A Stochastic Probing Problem with Applications</title><categories>cs.DS</categories><comments>20 pages, full version of IPCO 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general stochastic probing problem defined on a universe V, where
each element e in V is &quot;active&quot; independently with probability p_e. Elements
have weights {w_e} and the goal is to maximize the weight of a chosen subset S
of active elements. However, we are given only the p_e values-- to determine
whether or not an element e is active, our algorithm must probe e. If element e
is probed and happens to be active, then e must irrevocably be added to the
chosen set S; if e is not active then it is not included in S. Moreover, the
following conditions must hold in every random instantiation: (1) the set Q of
probed elements satisfy an &quot;outer&quot; packing constraint, and (2) the set S of
chosen elements satisfy an &quot;inner&quot; packing constraint.
  The kinds of packing constraints we consider are intersections of matroids
and knapsacks. Our results provide a simple and unified view of results in
stochastic matching and Bayesian mechanism design, and can also handle more
general constraints. As an application, we obtain the first polynomial-time
$\Omega(1/k)$-approximate &quot;Sequential Posted Price Mechanism&quot; under k-matroid
intersection feasibility constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5914</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5914</id><created>2013-02-24</created><authors><author><keyname>Kaltiokallio</keyname><forenames>Ossi</forenames></author><author><keyname>Bocca</keyname><forenames>Maurizio</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author></authors><title>A Multi-Scale Spatial Model for RSS-based Device-Free Localization</title><categories>cs.NI cs.HC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RSS-based device-free localization (DFL) monitors changes in the received
signal strength (RSS) measured by a network of static wireless nodes to locate
people without requiring them to carry or wear any electronic device. Current
models assume that the spatial impact area, i.e., the area in which a person
affects a link's RSS, has constant size. This paper shows that the spatial
impact area varies considerably for each link. Data from extensive experiments
are used to derive a multi-scale spatial weight model that is a function of the
fade level, i.e., the difference between the predicted and measured RSS, and of
the direction of RSS change. In addition, a measurement model is proposed which
gives a probability of a person locating inside the derived spatial model for
each given RSS measurement. A real-time radio tomographic imaging system is
described which uses channel diversity and the presented models. Experiments in
an open indoor environment, in a typical one-bedroom apartment and in a
through-wall scenario are conducted to determine the accuracy of the system. We
demonstrate that the new system is capable of localizing and tracking a person
with high accuracy (&lt;0.30 m) in all the environments, without the need to
change the model parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5929</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5929</id><created>2013-02-24</created><authors><author><keyname>Mangrulkar</keyname><forenames>R. S.</forenames></author><author><keyname>Atique</keyname><forenames>Dr. Mohammad</forenames></author></authors><title>Performance Evaluation of Delay Tolerant Network in Heterogeneous Highly
  Dense Mobile Environment</title><categories>cs.NI</categories><comments>15 pages,9 figures,16 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay tolerant network (DTN) is opportunistic network where each node
searches best opportunity to deliver the message called bundle to the
destination. DTN implements a store and forward message switching system by
simply introducing another new protocol layer called the Bundle Layer on top of
the transport layer. The bundle layer is responsible for storing and forwarding
entire message in message segments called bundles between source node and
destination node. This paper evaluates the performance of delay tolerant
network layer in heterogeneous highly dense mobile node environment. The
heterogeneous network is created with the help of stationary wired node and
Base Station node by introducing dynamic dense Mobile node network. Mobile
nodes are assigned with continuous mobility. Three parameters are suggested
$\Delta$, $\Theta$ and $\lambda$ to correlate the results obtained using
rigorous simulation. Results show that after some threshold values, dense
feature about mobile node does not pretend the delay cause for delay tolerant
network packets. Also, increase in number of mobile node and number of File
Transfer connection rarely change the overall performance of the delay tolerant
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5936</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5936</id><created>2013-02-24</created><authors><author><keyname>Iwen</keyname><forenames>M. A.</forenames></author></authors><title>Compressed Sensing with Sparse Binary Matrices: Instance Optimal Error
  Guarantees in Near-Optimal Time</title><categories>cs.IT math.IT math.NA</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A compressed sensing method consists of a rectangular measurement matrix, $M
\in \mathbbm{R}^{m \times N}$ with $m \ll N$, together with an associated
recovery algorithm, $\mathcal{A}: \mathbbm{R}^m \rightarrow \mathbbm{R}^N$.
Compressed sensing methods aim to construct a high quality approximation to any
given input vector ${\bf x} \in \mathbbm{R}^N$ using only $M {\bf x} \in
\mathbbm{R}^m$ as input. In particular, we focus herein on instance optimal
nonlinear approximation error bounds for $M$ and $\mathcal{A}$ of the form $ \|
{\bf x} - \mathcal{A} (M {\bf x}) \|_p \leq \| {\bf x} - {\bf x}^{\rm opt}_k
\|_p + C k^{1/p - 1/q} \| {\bf x} - {\bf x}^{\rm opt}_k \|_q$ for ${\bf x} \in
\mathbbm{R}^N$, where ${\bf x}^{\rm opt}_k$ is the best possible $k$-term
approximation to ${\bf x}$.
  In this paper we develop a compressed sensing method whose associated
recovery algorithm, $\mathcal{A}$, runs in $O((k \log k) \log N)$-time,
matching a lower bound up to a $O(\log k)$ factor. This runtime is obtained by
using a new class of sparse binary compressed sensing matrices of near optimal
size in combination with sublinear-time recovery techniques motivated by
sketching algorithms for high-volume data streams. The new class of matrices is
constructed by randomly subsampling rows from well-chosen incoherent matrix
constructions which already have a sub-linear number of rows. As a consequence,
fewer random bits than previously required are needed in order to select the
rows utilized by the fast reconstruction algorithms considered herein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5941</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5941</id><created>2013-02-24</created><authors><author><keyname>Bojic</keyname><forenames>Milorad</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Parvedy</keyname><forenames>Alexandre Patou</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author></authors><title>Optimization of thermal comfort in building through envelope design</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>International Conference on Efficiency, Cost, Optimization,
  Simulation and Environmental Impact of Energy Systems, ECOS 2012, Perigia :
  Italy (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the current environmental situation, energy saving has become the
leading drive in modern research. Although the residential houses in tropical
climate do not use air conditioning to maintain thermal comfort in order to
avoid use of electricity. As the thermal comfort is maintained by adequate
envelope composition and natural ventilation, this paper shows that it is
possible to determine the thickness of envelope layers for which the best
thermal comfort is obtained. The building is modeled in EnergyPlus software and
HookeJeves optimization methodology. The investigated house is a typical
residential house one-storey high with five thermal zones located at Reunion
Island, France. Three optimizations are performed such as the optimization of
the thickness of the concrete block layer, of the wood layer, and that of the
thermal insulation layer. The results show optimal thickness of thermal
envelope layers that yield the maximum TC according to Fanger predicted mean
vote.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5942</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5942</id><created>2013-02-24</created><authors><author><keyname>Boji&#x107;</keyname><forenames>Milorad</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Cvetkovic</keyname><forenames>Dragan</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Skerli&#x107;</keyname><forenames>Jasmina</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Nikoli&#x107;</keyname><forenames>Danijela</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author></authors><title>Performances of Low Temperature Radiant Heating Systems</title><categories>cs.CE</categories><comments>Second International Conference on Building Energy and Environment,
  COBEE 2012, Colorado : United States (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low temperature heating panel systems offer distinctive advantages in terms
of thermal comfort and energy consumption, allowing work with low exergy
sources. The purpose of this paper is to compare floor, wall, ceiling, and
floor-ceiling panel heating systems in terms of energy, exergy and CO2
emissions. Simulation results for each of the analyzed panel system are given
by its energy (the consumption of gas for heating, electricity for pumps and
primary energy) and exergy consumption, the price of heating, and its carbon
dioxide emission. Then, the values of the air temperatures of rooms are
investigated and that of the surrounding walls and floors. It is found that the
floor-ceiling heating system has the lowest energy, exergy, CO2 emissions,
operating costs, and uses boiler of the lowest power. The worst system by all
these parameters is the classical ceiling heating
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5945</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5945</id><created>2013-02-24</created><authors><author><keyname>Ghaderi</keyname><forenames>Javad</forenames></author><author><keyname>Borst</keyname><forenames>Sem</forenames></author><author><keyname>Whiting</keyname><forenames>Phil</forenames></author></authors><title>Queue-Based Random-Access Algorithms: Fluid Limits and Stability Issues</title><categories>cs.NI cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use fluid limits to explore the (in)stability properties of wireless
networks with queue-based random-access algorithms. Queue-based random-access
schemes are simple and inherently distributed in nature, yet provide the
capability to match the optimal throughput performance of centralized
scheduling mechanisms in a wide range of scenarios. Unfortunately, the type of
activation rules for which throughput optimality has been established, may
result in excessive queue lengths and delays. The use of more
aggressive/persistent access schemes can improve the delay performance, but
does not offer any universal maximum-stability guarantees. In order to gain
qualitative insight and investigate the (in)stability properties of more
aggressive/persistent activation rules, we examine fluid limits where the
dynamics are scaled in space and time. In some situations, the fluid limits
have smooth deterministic features and maximum stability is maintained, while
in other scenarios they exhibit random oscillatory characteristics, giving rise
to major technical challenges. In the latter regime, more aggressive access
schemes continue to provide maximum stability in some networks, but may cause
instability in others. Simulation experiments are conducted to illustrate and
validate the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5955</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5955</id><created>2013-02-24</created><authors><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Multi-Feedback Successive Interference Cancellation for Multiuser MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>6 figures</comments><journal-ref>IEEE Transactions on Wireless Communications, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a low-complexity multiple feedback successive interference
cancellation (MF-SIC) strategy is proposed for the uplink of multiuser
multiple-input multiple-output (MU-MIMO) systems. In the proposed MF-SIC
{algorithm with shadow area constraints (SAC)}, an enhanced interference
cancellation is achieved by introducing {constellation points as the
candidates} to combat the error propagation in decision feedback loops. We also
combine the MF-SIC with multi-branch (MB) processing, which achieves a higher
detection diversity order. For coded systems, a low-complexity soft-input
soft-output (SISO) iterative (turbo) detector is proposed based on the MF and
the MB-MF interference suppression techniques. The computational complexity of
the MF-SIC is {comparable to} the conventional SIC algorithm {since very little
additional complexity is required}. {Simulation} results show that the
algorithms significantly outperform the conventional SIC scheme and {approach}
the optimal detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5957</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5957</id><created>2013-02-24</created><authors><author><keyname>Descombes</keyname><forenames>Xavier</forenames></author><author><keyname>Komech</keyname><forenames>Serguei</forenames></author></authors><title>Shape Characterization via Boundary Distortion</title><categories>cs.CV</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive new shape descriptors based on a directional
characterization. The main idea is to study the behavior of the shape
neighborhood under family of transformations. We obtain a description invariant
with respect to rotation, reflection, translation and scaling. A well-defined
metric is then proposed on the associated feature space. We show the continuity
of this metric. Some results on shape retrieval are provided on two databases
to show the accuracy of the proposed shape metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5958</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5958</id><created>2013-02-24</created><authors><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Decision Feedback Detection with Parallel Interference
  Cancellation and Constellation Constraints for Multi-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>10 figures</comments><journal-ref>IET Communications, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel low-complexity adaptive decision feedback detection
with parallel decision feedback and constellation constraints (P-DFCC) is
proposed for multiuser MIMO systems. We propose a constrained constellation map
which introduces a number of selected points served as the feedback candidates
for interference cancellation. By introducing a reliability checking, a higher
degree of freedom is introduced to refine the unreliable estimates. The P-DFCC
is followed by an adaptive receive filter to estimate the transmitted symbol.
In order to reduce the complexity of computing the filters with time-varying
MIMO channels, an adaptive recursive least squares (RLS) algorithm is employed
in the proposed P-DFCC scheme. An iterative detection and decoding (Turbo)
scheme is considered with the proposed P-DFCC algorithm. Simulations show that
the proposed technique has a complexity comparable to the conventional parallel
decision feedback detector while it obtains a performance close to the maximum
likelihood detector at a low to medium SNR range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5960</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5960</id><created>2013-02-24</created><authors><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Low-Complexity Variable Forgetting Factor Techniques for RLS Algorithms
  in Interference Rejection Applications</title><categories>cs.IT math.IT</categories><comments>12 figures</comments><journal-ref>IET Communications, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a low-complexity variable forgetting factor (VFF) mechanism for
recursive least square (RLS) algorithms in interference suppression
applications. The proposed VFF mechanism employs an updated component related
to the time average of the error correlation to automatically adjust the
forgetting factor in order to ensure fast convergence and good tracking of the
interference and the channel. Convergence and tracking analyses are carried out
and analytical expressions for predicting the mean squared error of the
proposed adaptation technique are obtained. Simulation results for a
direct-sequence code-division multiple access (DS-CDMA) system are presented in
nonstationary environments and show that the proposed VFF mechanism achieves
superior performance to previously reported methods at a reduced complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5973</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5973</id><created>2013-02-24</created><updated>2013-08-21</updated><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Song</keyname><forenames>Enbin</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author></authors><title>Sample Approximation-Based Deflation Approaches for Chance SINR
  Constrained Joint Power and Admission Control</title><categories>cs.IT math.IT</categories><comments>29 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an interference limited network, joint power and admission control (JPAC)
aims at supporting a maximum number of links at their specified signal to
interference plus noise ratio (SINR) targets while using a minimum total
transmission power. Most existing works on JPAC assume the perfect
instantaneous channel state information (CSI). However, in practical wireless
systems, the CSI is prone to errors, and such imperfect CSI can deteriorate the
system performance drastically.In this paper, we consider the JPAC problem with
imperfect CSI, that is, we assume that only the channel distribution
information (CDI) is available. We formulate the JPAC into a chance
(probabilistic) constrained program, where each link's SINR outage probability
is enforced to be less than or equal to a specified tolerance.
  Unfortunately, the chance SINR constraints are computationally intractable
because in general they do not have closed-form expressions and are not convex.
To circumvent such difficulty, we propose to use the sample (scenario)
approximation scheme to convert the chance constraints into finitely many
(depending on the sample size) simple linear constraints. Furthermore, we
reformulate the sample approximation of the chance SINR constrained JPAC
problem as a group sparse minimization problem and then relax it to a
second-order cone program (SOCP). The solution of the SOCP relaxation problem
can be used to check the simultaneous supportability of all links in the
network and to guide an iterative link removal procedure (deflation). Instead
of solving the SOCP with general-purpose solvers like CVX, we exploit its
special structure and custom-design an efficient algorithm for solving it.
Finally, we illustrate the effectiveness and efficiency of the proposed sample
approximation-based deflation approaches by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5974</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5974</id><created>2013-02-24</created><authors><author><keyname>Yang</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Lin</keyname><forenames>Wang</forenames></author></authors><title>Exact Safety Verification of Interval Hybrid Systems Based on
  Symbolic-Numeric Computation</title><categories>cs.SC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of safety verification of interval
hybrid systems in which the coefficients are intervals instead of explicit
numbers. A hybrid symbolic-numeric method, based on SOS relaxation and interval
arithmetic certification, is proposed to generate exact inequality invariants
for safety verification of interval hybrid systems. As an application, an
approach is provided to verify safety properties of non-polynomial hybrid
systems. Experiments on the benchmark hybrid systems are given to illustrate
the efficiency of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5975</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5975</id><created>2013-02-24</created><authors><author><keyname>Wang</keyname><forenames>Kun-Yu</forenames></author><author><keyname>Wang</keyname><forenames>Haining</forenames></author><author><keyname>Ding</keyname><forenames>Zhi</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author></authors><title>Low-Complexity Algorithm for Worst-Case Utility Maximization in
  Multiuser MISO Downlink</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers worst-case utility maximization (WCUM) problem for a
downlink wireless system where a multiantenna base station communicates with
multiple single-antenna users. Specifically, we jointly design transmit
covariance matrices for each user to robustly maximize the worst-case (i.e.,
minimum) system utility function under channel estimation errors bounded within
a spherical region. This problem has been shown to be NP-hard, and so any
algorithms for finding the optimal solution may suffer from prohibitively high
complexity. In view of this, we seek an efficient and more accurate suboptimal
solution for the WCUM problem. A low-complexity iterative WCUM algorithm is
proposed for this nonconvex problem by solving two convex problems
alternatively. We also show the convergence of the proposed algorithm, and
prove its Pareto optimality to the WCUM problem. Some simulation results are
presented to demonstrate its substantial performance gain and higher
computational efficiency over existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5978</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5978</id><created>2013-02-24</created><updated>2013-02-25</updated><authors><author><keyname>Rao</keyname><forenames>Xiongbin</forenames><affiliation>Fellow</affiliation></author><author><keyname>Ruan</keyname><forenames>Liangzhong</forenames><affiliation>Fellow</affiliation></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames><affiliation>Fellow</affiliation></author></authors><title>Limited Feedback Design for Interference Alignment on MIMO Interference
  Networks with Heterogeneous Path Loss and Spatial Correlations</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, accepted by IEEE transactions on signal
  processing in Feb. 2013</comments><doi>10.1109/TSP.2013.2252168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment is degree of freedom optimal in K -user MIMO
interference channels and many previous works have studied the transceiver
designs. However, these works predominantly focus on networks with perfect
channel state information at the transmitters and symmetrical interference
topology. In this paper, we consider a limited feedback system with
heterogeneous path loss and spatial correlations, and investigate how the
dynamics of the interference topology can be exploited to improve the feedback
efficiency. We propose a novel spatial codebook design, and perform dynamic
quantization via bit allocations to adapt to the asymmetry of the interference
topology. We bound the system throughput under the proposed dynamic scheme in
terms of the transmit SNR, feedback bits and the interference topology
parameters. It is shown that when the number of feedback bits scales with SNR
as C_{s}\cdot\log\textrm{SNR}, the sum degrees of freedom of the network are
preserved. Moreover, the value of scaling coefficient C_{s} can be
significantly reduced in networks with asymmetric interference topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5979</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5979</id><created>2013-02-24</created><authors><author><keyname>Peng</keyname><forenames>Xiao-Long</forenames></author><author><keyname>Xu</keyname><forenames>Xin-Jian</forenames></author><author><keyname>Fu</keyname><forenames>Xinchu</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Vaccination intervention on epidemic dynamics in networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 8 figures, RevTex4</comments><journal-ref>Physical Review E 87, 022813 (2013)</journal-ref><doi>10.1103/PhysRevE.87.022813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vaccination is an important measure available for preventing or reducing the
spread of infectious diseases. In this paper, an epidemic model including
susceptible, infected, and imperfectly vaccinated compartments is studied on
Watts-Strogatz small-world, Barab\'asi-Albert scale-free, and random scale-free
networks. The epidemic threshold and prevalence are analyzed. For small-world
networks, the effective vaccination intervention is suggested and its influence
on the threshold and prevalence is analyzed. For scale-free networks, the
threshold is found to be strongly dependent both on the effective vaccination
rate and on the connectivity distribution. Moreover, so long as vaccination is
effective, it can linearly decrease the epidemic prevalence in small-world
networks, whereas for scale-free networks it acts exponentially. These results
can help in adopting pragmatic treatment upon diseases in structured
populations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5985</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5985</id><created>2013-02-24</created><authors><author><keyname>Hou</keyname><forenames>Xiaodi</forenames></author><author><keyname>Yuille</keyname><forenames>Alan</forenames></author><author><keyname>Koch</keyname><forenames>Christof</forenames></author></authors><title>A Meta-Theory of Boundary Detection Benchmarks</title><categories>cs.CV</categories><comments>NIPS 2012 Workshop on Human Computation for Science and Computational
  Sustainability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human labeled datasets, along with their corresponding evaluation algorithms,
play an important role in boundary detection. We here present a psychophysical
experiment that addresses the reliability of such benchmarks. To find better
remedies to evaluate the performance of any boundary detection algorithm, we
propose a computational framework to remove inappropriate human labels and
estimate the intrinsic properties of boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5990</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5990</id><created>2013-02-24</created><authors><author><keyname>Kaynama</keyname><forenames>Shahab</forenames></author><author><keyname>Oishi</keyname><forenames>Meeko</forenames></author></authors><title>A Modified Riccati Transformation for Decentralized Computation of the
  Viability Kernel Under LTI Dynamics</title><categories>cs.SY math.OC</categories><doi>10.1109/TAC.2013.2272152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the viability kernel is key in providing guarantees of safety and
proving existence of safety-preserving controllers for constrained dynamical
systems. Current numerical techniques that approximate this construct suffer
from a complexity that is exponential in the dimension of the state. We study
conditions under which a linear time-invariant (LTI) system can be suitably
decomposed into lower-dimensional subsystems so as to admit a conservative
computation of the viability kernel in a decentralized fashion in subspaces. We
then present an isomorphism that imposes these desired conditions, particularly
on two-time-scale systems. Decentralized computations are performed in the
transformed coordinates, yielding a conservative approximation of the viability
kernel in the original state space. Significant reduction of complexity can be
achieved, allowing the previously inapplicable tools to be employed for
treatment of higher-dimensional systems. We show the results on two examples
including a 6D system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5997</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5997</id><created>2013-02-25</created><authors><author><keyname>Echahed</keyname><forenames>Rachid</forenames><affiliation>CNRS, University of Grenoble, France</affiliation></author><author><keyname>Plump</keyname><forenames>Detlef</forenames><affiliation>University of York, UK</affiliation></author></authors><title>Proceedings 7th International Workshop on Computing with Terms and
  Graphs</title><categories>cs.SC cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 110, 2013</journal-ref><doi>10.4204/EPTCS.110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Seventh International Workshop on
Computing with Terms and Graphs (TERMGRAPH 2013). The workshop took place in
Rome, Italy, on March 23rd, 2013, as part of the sixteenth edition of the
European Joint Conferences on Theory and Practice of Software (ETAPS 2013).
  Research in term and graph rewriting ranges from theoretical questions to
practical issues. Computing with graphs handles the sharing of common
subexpressions in a natural and seamless way, and improves the efficiency of
computations in space and time. Sharing is ubiquitous in several research
areas, as witnessed by the modelling of first- and higher-order term rewriting
by (acyclic or cyclic) graph rewriting, the modelling of biological or chemical
abstract machines, and the implementation techniques of programming languages:
many implementations of functional, logic, object-oriented, concurrent and
mobile calculi are based on term graphs. Term graphs are also used in automated
theorem proving and symbolic computation systems working on shared structures.
The aim of this workshop is to bring together researchers working in different
domains on term and graph transformation and to foster their interaction, to
provide a forum for presenting new ideas and work in progress, and to enable
newcomers to learn about current activities in term graph rewriting.
  These proceedings contain six accepted papers and the abstracts of two
invited talks. All submissions were subject to careful refereeing. The topics
of accepted papers range over a wide spectrum, including theoretical aspects of
term graph rewriting, concurrency, semantics as well as application issues of
term graph transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.5999</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.5999</id><created>2013-02-25</created><authors><author><keyname>Srimugunthan</keyname></author><author><keyname>Gopinath</keyname><forenames>K.</forenames></author></authors><title>Distributed Wear levelling of Flash Memories</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For large scale distributed storage systems, flash memories are an excellent
choice because flash memories consume less power, take lesser floor space for a
target throughput and provide faster access to data. In a traditional
distributed filesystem, even distribution is required to ensure load-balancing,
balanced space utilisation and failure tolerance. In the presence of flash
memories, in addition, we should also ensure that the number of writes to these
different flash storage nodes are evenly distributed, to ensure even wear of
flash storage nodes, so that unpredictable failures of storage nodes are
avoided. This requires that we distribute updates and do garbage collection,
across the flash storage nodes. We have motivated the distributed wearlevelling
problem considering the replica placement algorithm for HDFS. Viewing the
wearlevelling across flash storage nodes as a distributed co-ordination
problem, we present an alternate design, to reduce the message communication
cost across participating nodes. We demonstrate the effectiveness of our design
through simulation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6005</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6005</id><created>2013-02-25</created><authors><author><keyname>Chakraborty</keyname><forenames>Shantanav</forenames></author><author><keyname>Adhikari</keyname><forenames>Satyabrata</forenames></author></authors><title>Non-classical Correlations in the Quantum Search Algorithm</title><categories>quant-ph cs.DS</categories><comments>7 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entanglement lies at the heart of quantum mechanics and has no classical
analogue. It is central to the speed up achieved by quantum algorithms over
their classical counterparts. The Grover's search algorithm is one such
algorithm which enables us to achieve a quadratic speed up over any known
classical algorithm that searches for an element in an unstructured database.
Here, we analyse and quantify the effects of entanglement in the generalized
version of this algorithm for two qubits. By 'generalized', it is meant that
the use of any arbitrary single qubit unitary gate is permitted to create
superposed states. Our analysis has been firstly on a noise free environment
and secondly in the presence of noise. In the absence of noise, we establish a
relation between the concurrence and the amplitude of the final state thereby
showing the explicit effects of entanglement on the same. Moreover, the effects
of noisy channels, namely amplitude and phase damping channels are studied. We
investigate the amount of quantum correlation in the states obtained after the
phase inversion stage of the algorithm followed by interaction of those states
with the noisy environment. The quantum correlations are quantified by
geometric discord. It has been revealed that the states generated after the
effect of amplitude damping on the phase inverted states of the quantum search
algorithm possess non-zero quantum correlation even when entanglement is
absent. However, this is absent in the phase damping scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6009</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6009</id><created>2013-02-25</created><authors><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Weiss</keyname><forenames>Roi</forenames></author></authors><title>On learning parametric-output HMMs</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach for learning an HMM whose outputs are distributed
according to a parametric family. This is done by {\em decoupling} the learning
task into two steps: first estimating the output parameters, and then
estimating the hidden states transition probabilities. The first step is
accomplished by fitting a mixture model to the output stationary distribution.
Given the parameters of this mixture model, the second step is formulated as
the solution of an easily solvable convex quadratic program. We provide an
error analysis for the estimated transition probabilities and show they are
robust to small perturbations in the estimates of the mixture parameters.
Finally, we support our analysis with some encouraging empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6030</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6030</id><created>2013-02-25</created><authors><author><keyname>Divakaran</keyname><forenames>Srikrishnan</forenames></author><author><keyname>Mithal</keyname><forenames>Arpit</forenames></author><author><keyname>Jain</keyname><forenames>Namit</forenames></author></authors><title>A Fast Template Based Heuristic For Global Multiple Sequence Alignment</title><categories>cs.CE q-bio.QM</categories><comments>20pages, 1 Table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Advances in bio-technology have made available massive amounts of functional,
structural and genomic data for many biological sequences. This increased
availability of heterogeneous biological data has resulted in biological
applications where a multiple sequence alignment (msa) is required for aligning
similar features, where a feature is described in structural, functional or
evolutionary terms. In these applications, for a given set of sequences,
depending on the feature of interest the optimal msa is likely to be different,
and sequence similarity can only be used as a rough initial estimate on the
accuracy of an msa. This has motivated the growth in template based heuristics
that supplement the sequence information with evolutionary, structural and
functional data and exploit feature similarity instead of sequence similarity
to construct multiple sequence alignments that are biologically more accurate.
However, current frameworks for designing template based heuristics do not
allow the user to explicitly specify information that can help to classify
features into types and associate weights signifying the relative importance of
a feature with respect to other features. In this paper, we first provide a
mechanism where as a part of the template information the user can explicitly
specify for each feature, its type, and weight. The type is to classify the
features into different categories based on their characteristics and the
weight signifies the relative importance of a feature with respect to other
features in that sequence. Second, we exploit the above information to define
scoring models for pair-wise sequence alignment that assume segment
conservation as opposed to single character (residue) conservation. Finally, we
present a fast progressive alignment based heuristic framework that helps in
constructing a global msa efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6031</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6031</id><created>2013-02-25</created><authors><author><keyname>Such</keyname><forenames>Ondrej</forenames></author></authors><title>Phoneme discrimination using KS algebra I</title><categories>cs.SD cs.AI cs.NE</categories><acm-class>I.2.7; I.5.2; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6043</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6043</id><created>2013-02-25</created><updated>2015-12-11</updated><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames><affiliation>Vienna University of Technology</affiliation></author><author><keyname>Hlineny</keyname><forenames>Petr</forenames><affiliation>Masaryk University, Brno</affiliation></author><author><keyname>Kral</keyname><forenames>Daniel</forenames><affiliation>University of Warwick</affiliation></author><author><keyname>Obdrzalek</keyname><forenames>Jan</forenames><affiliation>Masaryk University, Brno</affiliation></author><author><keyname>Schwartz</keyname><forenames>Jarett</forenames><affiliation>UC Berkeley</affiliation></author><author><keyname>Teska</keyname><forenames>Jakub</forenames><affiliation>University of West Bohemia, Pilsen</affiliation></author></authors><title>FO Model Checking of Interval Graphs</title><categories>cs.DM cs.LO</categories><comments>Paper as accepted to the LMCS journal. An extended abstract of an
  earlier version of this paper has appeared at ICALP'13. Main changes to the
  previous version are mostly small improvements in presentation</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:11) 2015</journal-ref><doi>10.2168/LMCS-11(4:11)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of the FO model checking problem on
interval graphs, i.e., intersection graphs of intervals on the real line. The
main positive result is that FO model checking and successor-invariant FO model
checking can be solved in time O(n log n) for n-vertex interval graphs with
representations containing only intervals with lengths from a prescribed finite
set. We complement this result by showing that the same is not true if the
lengths are restricted to any set that is dense in an open subset, e.g., in the
set $(1, 1 + \varepsilon)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6093</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6093</id><created>2013-02-25</created><authors><author><keyname>Fradelizi</keyname><forenames>Matthieu</forenames></author><author><keyname>Marsiglietti</keyname><forenames>Arnaud</forenames></author></authors><title>On the analogue of the concavity of entropy power in the Brunn-Minkowski
  theory</title><categories>math.FA cs.IT math.IT math.MG</categories><msc-class>52A40, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elaborating on the similarity between the entropy power inequality and the
Brunn-Minkowski inequality, Costa and Cover conjectured in {\it On the
similarity of the entropy power inequality and the Brunn-Minkowski inequality}
(IEEE Trans. Inform. Theory 30 (1984), no. 6, 837-839) the
$\frac{1}{n}$-concavity of the outer parallel volume of measurable sets as an
analogue of the concavity of entropy power. We investigate this conjecture and
study its relationship with geometric inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6105</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6105</id><created>2013-02-25</created><updated>2013-05-30</updated><authors><author><keyname>Escande</keyname><forenames>Paul</forenames><affiliation>ITAV</affiliation></author><author><keyname>Weiss</keyname><forenames>Pierre</forenames><affiliation>ITAV</affiliation></author><author><keyname>Malgouyres</keyname><forenames>Francois</forenames><affiliation>IMT</affiliation></author></authors><title>Image restoration using sparse approximations of spatially varying blur
  operators in the wavelet domain</title><categories>math.OC cs.CV math.NA</categories><comments>6 pages</comments><proxy>ccsd</proxy><doi>10.1088/1742-6596/464/1/012004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restoration of images degraded by spatially varying blurs is an issue of
increasing importance in the context of photography, satellite or microscopy
imaging. One of the main difficulty to solve this problem comes from the huge
dimensions of the blur matrix. It prevents the use of naive approaches for
performing matrix-vector multiplications. In this paper, we propose to
approximate the blur operator by a matrix sparse in the wavelet domain. We
justify this approach from a mathematical point of view and investigate the
approximation quality numerically. We finish by showing that the sparsity
pattern of the matrix can be pre-defined, which is central in tasks such as
blind deconvolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6109</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6109</id><created>2013-02-25</created><authors><author><keyname>Garcia</keyname><forenames>David</forenames></author><author><keyname>Mavrodiev</keyname><forenames>Pavlin</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Social Resilience in Online Communities: The Autopsy of Friendster</title><categories>cs.SI physics.soc-ph</categories><comments>Submitted, 22 February 2013</comments><acm-class>H.1.2</acm-class><journal-ref>Proceedings of the first ACM conference on Online social networks
  (2013) Pages 39-50</journal-ref><doi>10.1145/2512938.2512946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We empirically analyze five online communities: Friendster, Livejournal,
Facebook, Orkut, Myspace, to identify causes for the decline of social
networks. We define social resilience as the ability of a community to
withstand changes. We do not argue about the cause of such changes, but
concentrate on their impact. Changes may cause users to leave, which may
trigger further leaves of others who lost connection to their friends. This may
lead to cascades of users leaving. A social network is said to be resilient if
the size of such cascades can be limited. To quantify resilience, we use the
k-core analysis, to identify subsets of the network in which all users have at
least k friends. These connections generate benefits (b) for each user, which
have to outweigh the costs (c) of being a member of the network. If this
difference is not positive, users leave. After all cascades, the remaining
network is the k-core of the original network determined by the cost-to-benefit
c/b ratio. By analysing the cumulative distribution of k-cores we are able to
calculate the number of users remaining in each community. This allows us to
infer the impact of the c/b ratio on the resilience of these online
communities. We find that the different online communities have different
k-core distributions. Consequently, similar changes in the c/b ratio have a
different impact on the amount of active users. As a case study, we focus on
the evolution of Friendster. We identify time periods when new users entering
the network observed an insufficient c/b ratio. This measure can be seen as a
precursor of the later collapse of the community. Our analysis can be applied
to estimate the impact of changes in the user interface, which may temporarily
increase the c/b ratio, thus posing a threat for the community to shrink, or
even to collapse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6123</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6123</id><created>2013-02-25</created><authors><author><keyname>Kadloor</keyname><forenames>Sachin</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author><author><keyname>Venkitasubramaniam</keyname><forenames>Parv</forenames></author></authors><title>Mitigating Timing Side Channel in Shared Schedulers</title><categories>cs.CR</categories><comments>Submitted to IEEE Transactions on Networking. A conference version of
  this paper was presented at Infocom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study information leakage in timing side channels that arise
in the context of shared event schedulers. Consider two processes, one of them
an innocuous process (referred to as Alice) and the other a malicious one
(referred to as Bob), using a common scheduler to process their jobs. Based on
when his jobs get processed, Bob wishes to learn about the pattern (size and
timing) of jobs of Alice. Depending on the context, knowledge of this pattern
could have serious implications on Alice's privacy and security. For instance,
shared routers can reveal traffic patterns, shared memory access can reveal
cloud usage patterns, and suchlike. We present a formal framework to study the
information leakage in shared resource schedulers using the pattern estimation
error as a performance metric. The first-come-first-serve (FCFS) scheduling
policy and time-division-multiple-access (TDMA) are identified as two extreme
policies on the privacy metric, FCFS has the least, and TDMA has the highest.
However, on performance based metrics, such as throughput and delay, it is well
known that FCFS significantly outperforms TDMA. We then derive two parametrized
policies, accumulate and serve, and proportional TDMA, which take two different
approaches to offer a tunable trade-off between privacy and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6149</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6149</id><created>2013-02-25</created><authors><author><keyname>Anderson</keyname><forenames>Monia</forenames></author><author><keyname>Crawford</keyname><forenames>Chris</forenames></author><author><keyname>Kilgo</keyname><forenames>Paul</forenames></author><author><keyname>Stanforth</keyname><forenames>Megan</forenames></author></authors><title>Work in Progress: Enabling robot device discovery through robot device
  descriptions</title><categories>cs.RO</categories><comments>Presented at DSLRob 2011 (arXiv:cs/1212.3308)</comments><report-no>DSLRob/2011/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is no dearth of new robots that provide both generalized and customized
platforms for learning and research. Unfortunately as we attempt to adapt
existing software components, we are faced with an explosion of device drivers
that interface each hardware platform with existing frameworks. We certainly
gain the efficiencies of reusing algorithms and tools developed across
platforms but only once the device driver is created.
  We propose a domain specific language that describes the development and
runtime interface of a robot and defines its link to existing frameworks. The
Robot Device Interface Specification (RDIS) takes advantage of the internal
firmware present on many existing devices by defining the communication
mechanism, syntax and semantics in such a way to enable the generation of
automatic interface links and resource discovery. We present the current domain
model as it relates to differential drive robots as a mechanism to use the RDIS
to link described robots to HTML5 via web sockets and ROS (Robot Operating
System).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6154</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6154</id><created>2013-02-25</created><updated>2013-06-19</updated><authors><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author><author><keyname>Z&#xe9;mor</keyname><forenames>Gilles</forenames></author></authors><title>Upper Bounds on the Size of Grain-Correcting Codes</title><categories>cs.DM cs.IT math.IT</categories><comments>This is an extended version of a paper submitted to the 2013 IEEE
  International Symposium on Information Theory (ISIT 2013). Material from
  Sections 1, 2, 3 and 6 of this version were submitted to ISIT 2013. Version
  v4 contains updated figures and a new Section 5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we re-visit the combinatorial error model of Mazumdar et al.
that models errors in high-density magnetic recording caused by lack of
knowledge of grain boundaries in the recording medium. We present new upper
bounds on the cardinality/rate of binary block codes that correct errors within
this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6156</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6156</id><created>2013-02-25</created><authors><author><keyname>Singla</keyname><forenames>Ankit</forenames></author><author><keyname>Godfrey</keyname><forenames>P. Brighten</forenames></author><author><keyname>Fall</keyname><forenames>Kevin</forenames></author><author><keyname>Iannaccone</keyname><forenames>Gianluca</forenames></author><author><keyname>Ratnasamy</keyname><forenames>Sylvia</forenames></author></authors><title>Scalable Routing on Flat Names</title><categories>cs.NI</categories><comments>13 pages</comments><journal-ref>Extends our ACM CoNEXT 2010 paper with proofs for the theoretical
  results</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a protocol which routes on flat, location-independent
identifiers with guaranteed scalability and low stretch. Our design builds on
theoretical advances in the area of compact routing, and is the first to
realize these guarantees in a dynamic distributed setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6173</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6173</id><created>2013-02-25</created><updated>2013-03-20</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author></authors><title>Robust Capon Beamforming via Shaping Beam Pattern</title><categories>cs.IT math.IT</categories><comments>12 pages, 10 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High sidelobe level and direction of arrival (DOA) estimation sensitivity are
two major disadvantages of the Capon beamforming. To deal with these problems,
this paper gives an overview of a series of robust Capon beamforming methods
via shaping beam pattern, including sparse Capon beamforming, weighted sparse
Capon beamforming, mixed norm based Capon beamforming, total variation
minimization based Capon beamforming, mainlobe-to-sidelobe power ratio
maximization based Capon beamforming. With these additional structure-inducing
constraints, the sidelobe is suppressed, and the robustness against DOA
mismatch is improved too. Simulations show that the obtained beamformers
outperform the standard Capon beamformer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6183</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6183</id><created>2013-02-25</created><updated>2013-05-16</updated><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Fijav&#x17e;</keyname><forenames>Ga&#x161;per</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author></authors><title>Threshold-Coloring and Unit-Cube Contact Representation of Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study threshold coloring of graphs, where the vertex colors
represented by integers are used to describe any spanning subgraph of the given
graph as follows. Pairs of vertices with near colors imply the edge between
them is present and pairs of vertices with far colors imply the edge is absent.
Not all planar graphs are threshold-colorable, but several subclasses, such as
trees, some planar grids, and planar graphs without short cycles can always be
threshold-colored. Using these results we obtain unit-cube contact
representation of several subclasses of planar graphs. Variants of the
threshold coloring problem are related to well-known graph coloring and other
graph-theoretic problems. Using these relations we show the NP-completeness for
two of these variants, and describe a polynomial-time algorithm for another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6189</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6189</id><created>2013-02-05</created><authors><author><keyname>Duy</keyname><forenames>Truong Vinh Truong</forenames></author><author><keyname>Ozaki</keyname><forenames>Taisuke</forenames></author></authors><title>A decomposition method with minimum communication amount for
  parallelization of multi-dimensional FFTs</title><categories>cs.NA cond-mat.mtrl-sci</categories><comments>24 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fast Fourier transform (FFT) is undoubtedly an essential primitive that
has been applied in various fields of science and engineering. In this paper,
we present a decomposition method for parallelization of multi-dimensional FFTs
with smallest communication amount for all ranges of the number of processes
compared to previously proposed methods. This is achieved by two distinguishing
features: adaptive decomposition and transpose order awareness. In the proposed
method, the FFT data are decomposed based on a row-wise basis that maps the
multi-dimensional data into one-dimensional data, and translates the
corresponding coordinates from multi-dimensions into one-dimension so that the
resultant one-dimensional data can be divided and allocated equally to the
processes. As a result, differently from previous works that have the
dimensions of decomposition pre-defined, our method can adaptively decompose
the FFT data on the lowest possible dimensions depending on the number of
processes. In addition, this row-wise decomposition provides plenty of
alternatives in data transpose, and different transpose order results in
different amount of communication. We identify the best transpose orders with
smallest communication amounts for the 3-D, 4-D, and 5-D FFTs by analyzing all
possible cases. Given both communication efficiency and scalability, our method
is promising in development of highly efficient parallel packages for the FFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6191</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6191</id><created>2013-02-25</created><updated>2014-03-21</updated><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Dual Lower Bounds for Approximate Degree and Markov-Bernstein
  Inequalities</title><categories>cs.CC</categories><comments>34 pages. Appears in ICALP 2013. To appear in the special issue of
  Information and Computation for ICALP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\epsilon$-approximate degree of a Boolean function $f: \{-1, 1\}^n \to
\{-1, 1\}$ is the minimum degree of a real polynomial that approximates $f$ to
within $\epsilon$ in the $\ell_\infty$ norm. We prove several lower bounds on
this important complexity measure by explicitly constructing solutions to the
dual of an appropriate linear program. Our first result resolves the
$\epsilon$-approximate degree of the two-level AND-OR tree for any constant
$\epsilon &gt; 0$. We show that this quantity is $\Theta(\sqrt{n})$, closing a
line of incrementally larger lower bounds. The same lower bound was recently
obtained independently by Sherstov using related techniques. Our second result
gives an explicit dual polynomial that witnesses a tight lower bound for the
approximate degree of any symmetric Boolean function, addressing a question of
\v{S}palek. Our final contribution is to reprove several Markov-type
inequalities from approximation theory by constructing explicit dual solutions
to natural linear programs. These inequalities underly the proofs of many of
the best-known approximate degree lower bounds, and have important uses
throughout theoretical computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6194</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6194</id><created>2013-02-25</created><authors><author><keyname>Such</keyname><forenames>Ondrej</forenames></author><author><keyname>Mackovicova</keyname><forenames>Lenka</forenames></author></authors><title>Phoneme discrimination using $KS$-algebra II</title><categories>cs.SD cs.LG stat.ML</categories><acm-class>I.2.7; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $KS$-algebra consists of expressions constructed with four kinds operations,
the minimum, maximum, difference and additively homogeneous generalized means.
Five families of $Z$-classifiers are investigated on binary classification
tasks between English phonemes. It is shown that the classifiers are able to
reflect well known formant characteristics of vowels, while having very small
Kolmogoroff's complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6210</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6210</id><created>2013-02-25</created><authors><author><keyname>Adhikari</keyname><forenames>Ratnadip</forenames></author><author><keyname>Agrawal</keyname><forenames>R. K.</forenames></author></authors><title>A Homogeneous Ensemble of Artificial Neural Networks for Time Series
  Forecasting</title><categories>cs.NE cs.LG</categories><comments>8 pages, 4 figures, 2 tables, 26 references, international journal</comments><msc-class>68T05</msc-class><journal-ref>International Journal of Computer Applications, Vol. 32, No. 7,
  October 2011, pp. 1-8</journal-ref><doi>10.5120/3913-5505</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enhancing the robustness and accuracy of time series forecasting models is an
active area of research. Recently, Artificial Neural Networks (ANNs) have found
extensive applications in many practical forecasting problems. However, the
standard backpropagation ANN training algorithm has some critical issues, e.g.
it has a slow convergence rate and often converges to a local minimum, the
complex pattern of error surfaces, lack of proper training parameters selection
methods, etc. To overcome these drawbacks, various improved training methods
have been developed in literature; but, still none of them can be guaranteed as
the best for all problems. In this paper, we propose a novel weighted ensemble
scheme which intelligently combines multiple training algorithms to increase
the ANN forecast accuracies. The weight for each training algorithm is
determined from the performance of the corresponding ANN model on the
validation dataset. Experimental results on four important time series depicts
that our proposed technique reduces the mentioned shortcomings of individual
ANN training algorithms to a great extent. Also it achieves significantly
better forecast accuracies than two other popular statistical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6214</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6214</id><created>2013-02-25</created><authors><author><keyname>Korobeynikov</keyname><forenames>A. V.</forenames></author><author><keyname>Islamgaliev</keyname><forenames>I. I.</forenames></author></authors><title>Modification of conceptual clustering algorithm Cobweb for numerical
  data using fuzzy membership function</title><categories>cs.AI</categories><comments>in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modification of a conceptual clustering algorithm Cobweb for the purpose of
its application for numerical data is offered. Keywords: clustering, algorithm
Cobweb, numerical data, fuzzy membership function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6220</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6220</id><created>2013-02-25</created><updated>2014-04-23</updated><authors><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Durak</keyname><forenames>Nurcan</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Directed closure measures for networks with reciprocity</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>Updated version; new results on expected directed closures for
  reciprocal configuration model</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of triangles in graphs is a standard tool in network analysis,
leading to measures such as the \emph{transitivity}, i.e., the fraction of
paths of length $2$ that participate in triangles. Real-world networks are
often directed, and it can be difficult to &quot;measure&quot; this network structure
meaningfully. We propose a collection of \emph{directed closure values} for
measuring triangles in directed graphs in a way that is analogous to
transitivity in an undirected graph. Our study of these values reveals much
information about directed triadic closure. For instance, we immediately see
that reciprocal edges have a high propensity to participate in triangles. We
also observe striking similarities between the triadic closure patterns of
different web and social networks. We perform mathematical and empirical
analysis showing that directed configuration models that preserve reciprocity
cannot capture the triadic closure patterns of real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6224</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6224</id><created>2013-02-25</created><updated>2014-06-10</updated><authors><author><keyname>Mendes</keyname><forenames>Hammurabi</forenames></author><author><keyname>Tasson</keyname><forenames>Christine</forenames></author><author><keyname>Herlihy</keyname><forenames>Maurice</forenames></author></authors><title>Distributed Computability in Byzantine Asynchronous Systems</title><categories>cs.DC</categories><comments>Will appear at the Proceedings of the 46th Annual Symposium on the
  Theory of Computing, STOC 2014</comments><acm-class>D.1.3; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we extend the topology-based approach for characterizing
computability in asynchronous crash-failure distributed systems to asynchronous
Byzantine systems. We give the first theorem with necessary and sufficient
conditions to solve arbitrary tasks in asynchronous Byzantine systems where an
adversary chooses faulty processes. In our adversarial formulation, outputs of
non-faulty processes are constrained in terms of inputs of non-faulty processes
only. For colorless tasks, an important subclass of distributed problems, the
general result reduces to an elegant model that effectively captures the
relation between the number of processes, the number of failures, as well as
the topological structure of the task's simplicial complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6243</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6243</id><created>2013-02-25</created><authors><author><keyname>Giakkoupis</keyname><forenames>George</forenames></author></authors><title>Tight Bounds for Rumor Spreading with Vertex Expansion</title><categories>cs.DM cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a bound for the classic PUSH-PULL rumor spreading protocol on
arbitrary graphs, in terms of the vertex expansion of the graph. We show that
O(log^2(n)/\alpha) rounds suffice with high probability to spread a rumor from
a single node to all n nodes, in any graph with vertex expansion at least
\alpha. This bound matches the known lower bound, and settles the question on
the relationship between rumor spreading and vertex expansion asked by
Chierichetti, Lattanzi, and Panconesi (SODA 2010). Further, some of the
arguments used in the proof may be of independent interest, as they give new
insights, for example, on how to choose a small set of nodes in which to plant
the rumor initially, to guarantee fast rumor spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6256</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6256</id><created>2013-02-25</created><updated>2013-12-25</updated><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Gebremedhin</keyname><forenames>Assefaw H.</forenames></author><author><keyname>Patwary</keyname><forenames>Md. Mostofa Ali</forenames></author></authors><title>Parallel Maximum Clique Algorithms with Applications to Network Analysis
  and Storage</title><categories>cs.SI cs.DC cs.DM cs.DS physics.soc-ph</categories><comments>11 pages</comments><msc-class>05C69</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a fast, parallel maximum clique algorithm for large sparse graphs
that is designed to exploit characteristics of social and information networks.
The method exhibits a roughly linear runtime scaling over real-world networks
ranging from 1000 to 100 million nodes. In a test on a social network with 1.8
billion edges, the algorithm finds the largest clique in about 20 minutes. Our
method employs a branch and bound strategy with novel and aggressive pruning
techniques. For instance, we use the core number of a vertex in combination
with a good heuristic clique finder to efficiently remove the vast majority of
the search space. In addition, we parallelize the exploration of the search
tree. During the search, processes immediately communicate changes to upper and
lower bounds on the size of maximum clique, which occasionally results in a
super-linear speedup because vertices with large search spaces can be pruned by
other processes. We apply the algorithm to two problems: to compute temporal
strong components and to compress graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6259</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6259</id><created>2013-02-25</created><authors><author><keyname>Adhikari</keyname><forenames>Ratnadip</forenames></author></authors><title>A Treatise on Stability of Autonomous and Non-autonomous Systems: Theory
  and Illustrative Practical Applications</title><categories>cs.IT math.IT</categories><comments>79 pages, 23 figures, book</comments><msc-class>37-01</msc-class><journal-ref>LAP Lambert Academic Publishing, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stability is a very important property of any physical system. By a stable
system, we broadly mean that small disturbances either in the system inputs or
in the initial conditions do not lead to large changes in the overall behavior
of the system. To be of practical use, a system must have to be stable. The
theory of stability is a vast, rapidly growing subject with prolific and
innovative contributions from numerous researchers. As such, an introductory
book that covers the basic concepts and minute details about this theory is
essential. The primary aim of this book is to make the readers familiar with
the various terminologies and methods related to the stability analysis of
time-invariant (autonomous) and time-varying (non-autonomous) systems. A
special treatment is given to the celebrated Liapunov's direct method which is
so far the most widely used and perhaps the best method for determining the
stability nature of both autonomous as well as non-autonomous systems. After
discussing autonomous systems to a considerable extent, the book concentrates
on the non-autonomous systems. From stability point of view, these systems are
often quite difficult to manage. Also, unlike their autonomous counterparts,
the non-autonomous systems often behave in peculiar manners which can make the
analysts arrive at misleading conclusions. Due to these issues, this book
attempts to present a careful and systematic study about the stability
properties of non-autonomous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6262</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6262</id><created>2013-02-25</created><authors><author><keyname>Noetzel</keyname><forenames>Janis</forenames></author></authors><title>The depolarising channel and Horns problem</title><categories>quant-ph cs.IT math.IT math.RT</categories><comments>7 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the action of the depolarising (qubit) channel on permutation
invariant input states. More specifically, we raise the question on which
invariant subspaces the output of the depolarising channel, given such special
input, is supported. An answer is given for equidistributed states on
isotypical subspaces, also called symmetric Werner states. Horns problem and
two of the corresponding inequalities are invoked as a method of proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6267</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6267</id><created>2013-02-25</created><authors><author><keyname>Zawoad</keyname><forenames>Shams</forenames></author><author><keyname>Dutta</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author></authors><title>SecLaaS: Secure Logging-as-a-Service for Cloud Forensics</title><categories>cs.CR cs.DC</categories><comments>To appear at the 8th ACM Symposium on Information, Computer and
  Communications Security (ASIACCS), 2013. (Acceptance rate: 16.2%)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has emerged as a popular computing paradigm in recent years.
However, today's cloud computing architectures often lack support for computer
forensic investigations. Analyzing various logs (e.g., process logs, network
logs) plays a vital role in computer forensics. Unfortunately, collecting logs
from a cloud is very hard given the black-box nature of clouds and the
multi-tenant cloud models, where many users share the same processing and
network resources. Researchers have proposed using log API or cloud management
console to mitigate the challenges of collecting logs from cloud
infrastructure. However, there has been no concrete work, which shows how to
provide cloud logs to investigator while preserving users' privacy and
integrity of the logs. In this paper, we introduce Secure-Logging-as-a-Service
(SecLaaS), which stores virtual machines' logs and provides access to forensic
investigators ensuring the confidentiality of the cloud users. Additionally,
SeclaaS preserves proofs of past log and thus protects the integrity of the
logs from dishonest investigators or cloud providers. Finally, we evaluate the
feasibility of the scheme by implementing SecLaaS for network access logs in
OpenStack - a popular open source cloud platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6274</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6274</id><created>2013-02-25</created><updated>2013-02-27</updated><authors><author><keyname>Shourbaji</keyname><forenames>Ibrahim Al</forenames></author><author><keyname>AlAmeer</keyname><forenames>Rafat</forenames></author></authors><title>Wireless Intrusion Detection Systems(WIDS)</title><categories>cs.CR cs.NI</categories><comments>6 pages,2 figures,World Science Publisher,Advances in Computer
  Science and its Applications (ACSA),Vol. 2, No. 3,February 2013</comments><msc-class>arXiv.org</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The rapid proliferation of wireless networks and mobile computing
applications has changed the landscape of network security,the wireless
networks have changed the way business, organizations work and offered a new
range of possibilities and flexibilities; It is clear that wireless solutions
are transforming the way we work and live. Employees are able to keep in touch
with their e-mail, calendar and employer from mobile devices, but on the other
hands they introduced a new security threats appeared. While an attacker needs
physical access to a wired network in order to gain access to the network and
to accomplish his goals, a wireless network allows anyone within its range to
passively monitor the traffic or even start an attack,one of the
countermeasures that can be used is the intrusion detection systems in order to
allow us to know both the threats affecting our wireless network and our system
vulnerabilities in order to prevent those attackers, the IDS its main purpose
is to manage the system and its operations and Its duty depends on .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6276</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6276</id><created>2013-02-25</created><updated>2013-06-20</updated><authors><author><keyname>Weng</keyname><forenames>Lilian</forenames></author><author><keyname>Ratkiewicz</keyname><forenames>Jacob</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos</forenames></author><author><keyname>Bonchi</keyname><forenames>Francesco</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>The Role of Information Diffusion in the Evolution of Social Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>9 pages, 10 figures, 2 tables</comments><acm-class>H.1; J.4; H.1.2</acm-class><journal-ref>Proc. 19th ACM SIGKDD Conference on Knowledge Discovery and Data
  Mining (KDD 2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every day millions of users are connected through online social networks,
generating a rich trove of data that allows us to study the mechanisms behind
human interactions. Triadic closure has been treated as the major mechanism for
creating social links: if Alice follows Bob and Bob follows Charlie, Alice will
follow Charlie. Here we present an analysis of longitudinal micro-blogging
data, revealing a more nuanced view of the strategies employed by users when
expanding their social circles. While the network structure affects the spread
of information among users, the network is in turn shaped by this communication
activity. This suggests a link creation mechanism whereby Alice is more likely
to follow Charlie after seeing many messages by Charlie. We characterize users
with a set of parameters associated with different link creation strategies,
estimated by a Maximum-Likelihood approach. Triadic closure does have a strong
effect on link formation, but shortcuts based on traffic are another key factor
in interpreting network evolution. However, individual strategies for following
other users are highly heterogeneous. Link creation behaviors can be summarized
by classifying users in different categories with distinct structural and
behavioral characteristics. Users who are popular, active, and influential tend
to create traffic-based shortcuts, making the information diffusion process
more efficient in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6288</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6288</id><created>2013-02-25</created><updated>2013-06-10</updated><authors><author><keyname>Demanet</keyname><forenames>Laurent</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Nguyen</keyname><forenames>Nam</forenames></author></authors><title>Super-resolution via superset selection and pruning</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a pursuit-like algorithm that we call the &quot;superset method&quot; for
recovery of sparse vectors from consecutive Fourier measurements in the
super-resolution regime. The algorithm has a subspace identification step that
hinges on the translation invariance of the Fourier transform, followed by a
removal step to estimate the solution's support. The superset method is always
successful in the noiseless regime (unlike L1-minimization) and generalizes to
higher dimensions (unlike the matrix pencil method). Relative robustness to
noise is demonstrated numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6292</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6292</id><created>2013-02-25</created><authors><author><keyname>Ferrett</keyname><forenames>Terry</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Torrieri</keyname><forenames>Don</forenames></author></authors><title>An Iterative Noncoherent Relay Receiver for the Two-way Relay Channel</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figure, International Conference on Communications (ICC)
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical-layer network coding improves the throughput of the two-way relay
channel by allowing multiple source terminals to transmit simultaneously to the
relay. However, it is generally not feasible to align the phases of the
multiple received signals at the relay, which motivates the exploration of
noncoherent solutions. In this paper, turbo-coded orthogonal multi-tone
frequency-shift keying (FSK) is considered for the two-way relay channel. In
contrast with analog network coding, the system considered is an instance of
digital network coding; i.e., the relay decodes the network codeword and
forwards a re-encoded version. Crucial to noncoherent digital network coding is
the implementation of the relay receiver, which is the primary focus of the
paper. The relay receiver derived in this paper supports any modulation order
that is a power of two, and features the iterative feedback of a priori
information from the turbo channel decoder to the demodulator; i.e., it uses
bit interleaved coded modulation with iterative decoding (BICM-ID). The
performance of the receiver is investigated in Rayeligh fading channels through
error-rate simulations and a capacity analysis. Results show that the BICM-ID
receiver improves energy efficiency by 0.5-0.9 dB compared to a non-iterative
receiver implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6309</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6309</id><created>2013-02-25</created><updated>2014-04-14</updated><authors><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Xu</keyname><forenames>Wenchang</forenames></author></authors><title>Reciprocal versus Parasocial Relationships in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Social Network Analysis and Mining, Springer, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many online social networks are fundamentally directed, i.e., they consist of
both reciprocal edges (i.e., edges that have already been linked back) and
parasocial edges (i.e., edges that haven't been linked back). Thus,
understanding the structures and evolutions of reciprocal edges and parasocial
ones, exploring the factors that influence parasocial edges to become
reciprocal ones, and predicting whether a parasocial edge will turn into a
reciprocal one are basic research problems.
  However, there have been few systematic studies about such problems. In this
paper, we bridge this gap using a novel large-scale Google+ dataset crawled by
ourselves as well as one publicly available social network dataset. First, we
compare the structures and evolutions of reciprocal edges and those of
parasocial edges. For instance, we find that reciprocal edges are more likely
to connect users with similar degrees while parasocial edges are more likely to
link ordinary users (e.g., users with low degrees) and popular users (e.g.,
celebrities). However, the impacts of reciprocal edges linking ordinary and
popular users on the network structures increase slowly as the social networks
evolve. Second, we observe that factors including user behaviors, node
attributes, and edge attributes all have significant impacts on the formation
of reciprocal edges. Third, in contrast to previous studies that treat
reciprocal edge prediction as either a supervised or a semi-supervised learning
problem, we identify that reciprocal edge prediction is better modeled as an
outlier detection problem. Finally, we perform extensive evaluations with the
two datasets, and we show that our proposal outperforms previous reciprocal
edge prediction approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6310</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6310</id><created>2013-02-25</created><authors><author><keyname>Adeyemo</keyname><forenames>Adesesan . B</forenames></author><author><keyname>Oketola</keyname><forenames>Adebola A.</forenames></author><author><keyname>Adetula</keyname><forenames>Emmanuel O.</forenames></author><author><keyname>Osibanjo</keyname><forenames>O.</forenames></author></authors><title>Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining
  Techniques</title><categories>cs.NE</categories><comments>11 pages, 11 figures</comments><report-no>11 pages</report-no><journal-ref>A.B ADEYEMO, A.A OKETOLA, E.O ADETULA, O.OSIBANJO (2012):
  Estimating Sectoral Pollution Load in Lagos, Nigeria Using Data Mining
  Techniques,International-Journal-of-Computer-Science-Issues, Volume 9, Issue
  6, November 2012 pages 465-475</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial pollution is often considered to be one of the prime factors
contributing to air, water and soil pollution. Sectoral pollution loads
(ton/yr) into different media (i.e. air, water and land) in Lagos were
estimated using Industrial Pollution Projected System (IPPS). These were
further studied using Artificial neural Networks (ANNs), a data mining
technique that has the ability of detecting and describing patterns in large
data sets with variables that are non- linearly related. Time Lagged Recurrent
Network (TLRN) appeared as the best Neural Network model among all the neural
networks considered which includes Multilayer Perceptron (MLP) Network,
Generalized Feed Forward Neural Network (GFNN), Radial Basis Function (RBF)
Network and Recurrent Network (RN). TLRN modelled the data-sets better than the
others in terms of the mean average error (MAE) (0.14), time (39 s) and linear
correlation coefficient (0.84). The results showed that Artificial Neural
Networks (ANNs) technique (i.e., Time Lagged Recurrent Network) is also
applicable and effective in environmental assessment study. Keywords:
Artificial Neural Networks (ANNs), Data Mining Techniques, Industrial Pollution
Projection System (IPPS), Pollution load, Pollution Intensity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6312</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6312</id><created>2013-02-25</created><authors><author><keyname>Zawoad</keyname><forenames>Shams</forenames></author><author><keyname>Hasan</keyname><forenames>Ragib</forenames></author></authors><title>Cloud Forensics: A Meta-Study of Challenges, Approaches, and Open
  Problems</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, cloud computing has become popular as a cost-effective and
efficient computing paradigm. Unfortunately, today's cloud computing
architectures are not designed for security and forensics. To date, very little
research has been done to develop the theory and practice of cloud forensics.
Many factors complicate forensic investigations in a cloud environment. First,
the storage system is no longer local. Therefore, even with a subpoena, law
enforcement agents cannot confiscate the suspect's computer and get access to
the suspect's files. Second, each cloud server contains files from many users.
Hence, it is not feasible to seize servers from a data center without violating
the privacy of many other users. Third, even if the data belonging to a
particular suspect is identified, separating it from other users' data is
difficult. Moreover, other than the cloud provider's word, there is usually no
evidence that links a given data file to a particular suspect. For such
challenges, clouds cannot be used to store healthcare, business, or national
security related data, which require audit and regulatory compliance. In this
paper, we systematically examine the cloud forensics problem and explore the
challenges and issues in cloud forensics. We then discuss existing research
projects and finally, we highlight the open problems and future directions in
cloud forensics research area. We posit that our systematic approach towards
understanding the nature and challenges of cloud forensics will allow us to
examine possible secure solution approaches, leading to increased trust on and
adoption of cloud computing, especially in business, healthcare, and national
security. This in turn will lead to lower cost and long-term benefit to our
society as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6315</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6315</id><created>2013-02-26</created><authors><author><keyname>Watanabe</keyname><forenames>Kazuho</forenames></author></authors><title>Rate-Distortion Bounds for an Epsilon-Insensitive Distortion Measure</title><categories>cs.IT cs.LG math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct evaluation of the rate-distortion function has rarely been achieved
when it is strictly greater than its Shannon lower bound. In this paper, we
consider the rate-distortion function for the distortion measure defined by an
epsilon-insensitive loss function. We first present the Shannon lower bound
applicable to any source distribution with finite differential entropy. Then,
focusing on the Laplacian and Gaussian sources, we prove that the
rate-distortion functions of these sources are strictly greater than their
Shannon lower bounds and obtain analytically evaluable upper bounds for the
rate-distortion functions. Small distortion limit and numerical evaluation of
the bounds suggest that the Shannon lower bound provides a good approximation
to the rate-distortion function for the epsilon-insensitive distortion measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6324</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6324</id><created>2013-02-26</created><authors><author><keyname>Zhou</keyname><forenames>Dingding</forenames></author><author><keyname>Chen</keyname><forenames>Songling</forenames></author><author><keyname>Dong</keyname><forenames>Shi</forenames></author></authors><title>Network traffic prediction based on ARFIMA model</title><categories>cs.NI</categories><comments>4 figures,5 tables,6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ARFIMA is a time series forecasting model, which is an improved ARMA model,
the ARFIMA model proposed in this article is demonstrated and deduced in
detail. combined with network traffic of CERNET backbone and the ARFIMA
model,the result shows that,compare to the ARMA model, the prediction
efficiency and accuracy has increased significantly, and not susceptible to
sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6325</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6325</id><created>2013-02-26</created><updated>2013-05-13</updated><authors><author><keyname>Nabeezath</keyname><forenames>Saleena</forenames></author><author><keyname>Paleri</keyname><forenames>Vineeth</forenames></author></authors><title>A Note on &quot;A polynomial-time algorithm for global value numbering&quot;</title><categories>cs.PL cs.LO</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Global Value Numbering(GVN) algorithm is considered to be complete (or
precise), if it can detect all Herbrand equivalences among expressions in a
program. A polynomial time algorithm for GVN is presented by Gulwani and Necula
(2006). Here we present two problems with this algorithm that prevents
detection of some of the Herbrand equivalences among program expressions. We
suggest improvements that will make the algorithm more precise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6328</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6328</id><created>2013-02-26</created><authors><author><keyname>Liu</keyname><forenames>Yu David</forenames><affiliation>SUNY Binghamton</affiliation></author></authors><title>Variant-Frequency Semantics for Green Futures</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2012, arXiv:1302.5798</comments><proxy>EPTCS</proxy><acm-class>D.3.3</acm-class><journal-ref>EPTCS 109, 2013, pp. 1-6</journal-ref><doi>10.4204/EPTCS.109.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an operational semantics for futures, with the primary
target on energy efficiency. The work in progress is built around an insight
that different threads can coordinate by running at different &quot;paces,&quot; so that
the time for synchronization and the resulting wasteful energy consumption can
be reduced. We exploit several inherent characteristics of futures to determine
how the paces of involving threads can be coordinated. The semantics is
inspired by recent advances in computer architectures, where the frequencies of
CPU cores can be adjusted dynamically. The work is a first-step toward a
direction where variant frequencies are directly modeled as an essential
semantic feature in concurrent programming languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6329</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6329</id><created>2013-02-26</created><authors><author><keyname>Calvert</keyname><forenames>Peter</forenames><affiliation>University of Cambridge Computer Laboratory</affiliation></author><author><keyname>Mycroft</keyname><forenames>Alan</forenames><affiliation>University of Cambridge Computer Laboratory</affiliation></author></authors><title>Mapping the Join Calculus to Heterogeneous Hardware</title><categories>cs.DC cs.PL</categories><comments>In Proceedings PLACES 2012, arXiv:1302.5798</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 109, 2013, pp. 7-12</journal-ref><doi>10.4204/EPTCS.109.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As modern architectures introduce additional heterogeneity and parallelism,
we look for ways to deal with this that do not involve specialising software to
every platform. In this paper, we take the Join Calculus, an elegant model for
concurrent computation, and show how it can be mapped to an architecture by a
Cartesian-product-style construction, thereby making use of the calculus'
inherent non-determinism to encode placement choices. This unifies the concepts
of placement and scheduling into a single task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6330</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6330</id><created>2013-02-26</created><authors><author><keyname>Bartoletti</keyname><forenames>Massimo</forenames><affiliation>Universita' degli Studi di Cagliari</affiliation></author><author><keyname>Cimoli</keyname><forenames>Tiziana</forenames><affiliation>Universita' degli Studi di Cagliari</affiliation></author><author><keyname>Pinna</keyname><forenames>G. Michele</forenames><affiliation>Universita' degli Studi di Cagliari</affiliation></author><author><keyname>Zunino</keyname><forenames>Roberto</forenames><affiliation>DISI-Universita' degli Studi di Trento and COSBI, Italy</affiliation></author></authors><title>An event-based model for contracts</title><categories>cs.LO cs.MA</categories><comments>In Proceedings PLACES 2012, arXiv:1302.5798</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 109, 2013, pp. 13-20</journal-ref><doi>10.4204/EPTCS.109.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a basic model for contracts. Our model extends event structures
with a new relation, which faithfully captures the circular dependencies among
contract clauses. We establish whether an agreement exists which respects all
the contracts at hand (i.e. all the dependencies can be resolved), and we
detect the obligations of each participant. The main technical contribution is
a correspondence between our model and a fragment of the contract logic PCL.
More precisely, we show that the reachable events are exactly those which
correspond to provable atoms in the logic. Despite of this strong
correspondence, our model improves previous work on PCL by exhibiting a
finer-grained notion of culpability, which takes into account the legitimate
orderings of events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6331</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6331</id><created>2013-02-26</created><authors><author><keyname>Carbone</keyname><forenames>Marco</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Merging Multiparty Protocols in Multiparty Choreographies</title><categories>cs.PL cs.DC</categories><comments>In Proceedings PLACES 2012, arXiv:1302.5798</comments><proxy>EPTCS</proxy><acm-class>D.3.1, F.3.2</acm-class><journal-ref>EPTCS 109, 2013, pp. 21-27</journal-ref><doi>10.4204/EPTCS.109.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreography-based programming is a powerful paradigm for defining
communication-based systems from a global viewpoint. A choreography can be
checked against multiparty protocol specifications, given as behavioural types,
that may be instantiated indefinitely at runtime. Each protocol instance is
started with a synchronisation among the involved peers.
  We analyse a simple transformation from a choreography with a possibly
unbounded number of protocol instantiations to a choreography instantiating a
single protocol, which is the merge of the original ones. This gives an
effective methodology for obtaining new protocols by composing existing ones.
Moreover, by removing all synchronisations required for starting protocol
instances, our transformation reduces the number of communications and
resources needed to execute a choreography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6332</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6332</id><created>2013-02-26</created><authors><author><keyname>Degano</keyname><forenames>Pierpaolo</forenames><affiliation>Dipartimento di Informatica - Universit&#xe0; di Pisa</affiliation></author><author><keyname>Ferrari</keyname><forenames>Gian-Luigi</forenames><affiliation>Dipartimento di Informatica - Universit&#xe0; di Pisa</affiliation></author><author><keyname>Galletta</keyname><forenames>Letterio</forenames><affiliation>Dipartimento di Informatica - Universit&#xe0; di Pisa</affiliation></author><author><keyname>Mezzetti</keyname><forenames>Gianluca</forenames><affiliation>Dipartimento di Informatica - Universit&#xe0; di Pisa</affiliation></author></authors><title>Typing Context-Dependent Behavioural Variation</title><categories>cs.PL cs.LO</categories><comments>In Proceedings PLACES 2012, arXiv:1302.5798</comments><proxy>EPTCS</proxy><acm-class>D.3.1; F.3.1; F.3.2</acm-class><journal-ref>EPTCS 109, 2013, pp. 28-33</journal-ref><doi>10.4204/EPTCS.109.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context Oriented Programming (COP) concerns the ability of programs to adapt
to changes in their running environment. A number of programming languages
endowed with COP constructs and features have been developed. However, some
foundational issues remain unclear. This paper proposes adopting static
analysis techniques to reason on and predict how programs adapt their
behaviour. We introduce a core functional language, ContextML, equipped with
COP primitives for manipulating contexts and for programming behavioural
variations. In particular, we specify the dispatching mechanism, used to select
the program fragments to be executed in the current active context. Besides the
dynamic semantics we present an annotated type system. It guarantees that the
well-typed programs adapt to any context, i.e. the dispatching mechanism always
succeeds at run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6333</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6333</id><created>2013-02-26</created><authors><author><keyname>Jongmans</keyname><forenames>Sung-Shik T. Q.</forenames></author><author><keyname>Arbab</keyname><forenames>Farhad</forenames></author></authors><title>Modularizing and Specifying Protocols among Threads</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2012, arXiv:1302.5798</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 109, 2013, pp. 34-45</journal-ref><doi>10.4204/EPTCS.109.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify three problems with current techniques for implementing protocols
among threads, which complicate and impair the scalability of multicore
software development: implementing synchronization, implementing coordination,
and modularizing protocols. To mend these deficiencies, we argue for the use of
domain-specific languages (DSL) based on existing models of concurrency. To
demonstrate the feasibility of this proposal, we explain how to use the model
of concurrency Reo as a high-level protocol DSL, which offers appropriate
abstractions and a natural separation of protocols and computations. We
describe a Reo-to-Java compiler and illustrate its use through examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6334</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6334</id><created>2013-02-26</created><authors><author><keyname>Bonfante</keyname><forenames>Guillaume</forenames><affiliation>LORIA Universit&#xe9; de Lorraine</affiliation></author><author><keyname>Guillaume</keyname><forenames>Bruno</forenames><affiliation>LORIA Inria Nancy Grand-Est</affiliation></author></authors><title>Non-simplifying Graph Rewriting Termination</title><categories>cs.CL cs.CC cs.LO</categories><comments>In Proceedings TERMGRAPH 2013, arXiv:1302.5997</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 110, 2013, pp. 4-16</journal-ref><doi>10.4204/EPTCS.110.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far, a very large amount of work in Natural Language Processing (NLP) rely
on trees as the core mathematical structure to represent linguistic
informations (e.g. in Chomsky's work). However, some linguistic phenomena do
not cope properly with trees. In a former paper, we showed the benefit of
encoding linguistic structures by graphs and of using graph rewriting rules to
compute on those structures. Justified by some linguistic considerations, graph
rewriting is characterized by two features: first, there is no node creation
along computations and second, there are non-local edge modifications. Under
these hypotheses, we show that uniform termination is undecidable and that
non-uniform termination is decidable. We describe two termination techniques
based on weights and we give complexity bound on the derivation length for
these rewriting system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6335</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6335</id><created>2013-02-26</created><authors><author><keyname>Bahr</keyname><forenames>Patrick</forenames><affiliation>Department of Computer Science, University of Copenhagen</affiliation></author></authors><title>Convergence in Infinitary Term Graph Rewriting Systems is Simple
  (Extended Abstract)</title><categories>cs.LO cs.PL</categories><comments>In Proceedings TERMGRAPH 2013, arXiv:1302.5997</comments><proxy>EPTCS</proxy><acm-class>F.4.2; F.1.1</acm-class><journal-ref>EPTCS 110, 2013, pp. 17-28</journal-ref><doi>10.4204/EPTCS.110.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this extended abstract, we present a simple approach to convergence on
term graphs that allows us to unify term graph rewriting and infinitary term
rewriting. This approach is based on a partial order and a metric on term
graphs. These structures arise as straightforward generalisations of the
corresponding structures used in infinitary term rewriting. We compare our
simple approach to a more complicated approach that we developed earlier and
show that this new approach is superior in many ways. The only unfavourable
property that we were able to identify, viz. failure of full correspondence
between weak metric and partial order convergence, is rectified by adopting a
strong convergence discipline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6336</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6336</id><created>2013-02-26</created><authors><author><keyname>Schmidt-Schauss</keyname><forenames>Manfred</forenames></author></authors><title>Linear Compressed Pattern Matching for Polynomial Rewriting (Extended
  Abstract)</title><categories>cs.LO cs.DS</categories><comments>In Proceedings TERMGRAPH 2013, arXiv:1302.5997</comments><proxy>EPTCS</proxy><acm-class>F.4.2</acm-class><journal-ref>EPTCS 110, 2013, pp. 29-40</journal-ref><doi>10.4204/EPTCS.110.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an extended abstract of an analysis of term rewriting where the
terms in the rewrite rules as well as the term to be rewritten are compressed
by a singleton tree grammar (STG). This form of compression is more general
than node sharing or representing terms as dags since also partial trees
(contexts) can be shared in the compression. In the first part efficient but
complex algorithms for detecting applicability of a rewrite rule under
STG-compression are constructed and analyzed. The second part applies these
results to term rewriting sequences.
  The main result for submatching is that finding a redex of a left-linear rule
can be performed in polynomial time under STG-compression.
  The main implications for rewriting and (single-position or parallel)
rewriting steps are: (i) under STG-compression, n rewriting steps can be
performed in nondeterministic polynomial time. (ii) under STG-compression and
for left-linear rewrite rules a sequence of n rewriting steps can be performed
in polynomial time, and (iii) for compressed rewrite rules where the left hand
sides are either DAG-compressed or ground and STG-compressed, and an
STG-compressed target term, n rewriting steps can be performed in polynomial
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6337</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6337</id><created>2013-02-26</created><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames><affiliation>Carnegie Mellon University</affiliation></author></authors><title>Evaluating functions as processes</title><categories>cs.PL cs.LO</categories><comments>In Proceedings TERMGRAPH 2013, arXiv:1302.5997</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 110, 2013, pp. 41-55</journal-ref><doi>10.4204/EPTCS.110.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A famous result by Milner is that the lambda-calculus can be simulated inside
the pi-calculus. This simulation, however, holds only modulo strong
bisimilarity on processes, i.e. there is a slight mismatch between
beta-reduction and how it is simulated in the pi-calculus. The idea is that
evaluating a lambda-term in the pi-calculus is like running an
environment-based abstract machine, rather than applying ordinary
beta-reduction. In this paper we show that such an abstract-machine evaluation
corresponds to linear weak head reduction, a strategy arising from the
representation of lambda-terms as linear logic proof nets, and that the
relation between the two is as tight as it can be. The study is also smoothly
rephrased in the call-by-value case, introducing a call-by-value analogous of
linear weak head reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6338</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6338</id><created>2013-02-26</created><authors><author><keyname>Grabmayer</keyname><forenames>Clemens</forenames><affiliation>Department of Philosophy, Utrecht University, The Netherlands</affiliation></author><author><keyname>Rochel</keyname><forenames>Jan</forenames><affiliation>Department of Computing Sciences, Utrecht University, The Netherlands</affiliation></author></authors><title>Term Graph Representations for Cyclic Lambda-Terms</title><categories>cs.LO cs.PL</categories><comments>In Proceedings TERMGRAPH 2013, arXiv:1302.5997</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 110, 2013, pp. 56-73</journal-ref><doi>10.4204/EPTCS.110.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study various representations for cyclic lambda-terms as higher-order or
as first-order term graphs. We focus on the relation between
'lambda-higher-order term graphs' (lambda-ho-term-graphs), which are
first-order term graphs endowed with a well-behaved scope function, and their
representations as 'lambda-term-graphs', which are plain first-order term
graphs with scope-delimiter vertices that meet certain scoping requirements.
Specifically we tackle the question: Which class of first-order term graphs
admits a faithful embedding of lambda-ho-term-graphs in the sense that (i) the
homomorphism-based sharing-order on lambda-ho-term-graphs is preserved and
reflected, and (ii) the image of the embedding corresponds closely to a natural
class (of lambda-term-graphs) that is closed under homomorphism?
  We systematically examine whether a number of classes of lambda-term-graphs
have this property, and we find a particular class of lambda-term-graphs that
satisfies this criterion. Term graphs of this class are built from application,
abstraction, variable, and scope-delimiter vertices, and have the
characteristic feature that the latter two kinds of vertices have back-links to
the corresponding abstraction.
  This result puts a handle on the concept of subterm sharing for higher-order
term graphs, both theoretically and algorithmically: We obtain an easily
implementable method for obtaining the maximally shared form of
lambda-ho-term-graphs. Furthermore, we open up the possibility to pull back
properties from first-order term graphs to lambda-ho-term-graphs, properties
such as the complete lattice structure of bisimulation equivalence classes with
respect to the sharing order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6339</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6339</id><created>2013-02-26</created><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames></author><author><keyname>Mackie</keyname><forenames>Ian</forenames></author><author><keyname>Walker</keyname><forenames>Matthew</forenames></author></authors><title>Bigraphical Nets</title><categories>cs.LO cs.PL</categories><comments>In Proceedings TERMGRAPH 2013, arXiv:1302.5997</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 110, 2013, pp. 74-81</journal-ref><doi>10.4204/EPTCS.110.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interaction nets are a graphical model of computation, which has been used to
define efficient evaluators for functional calculi, and specifically lambda
calculi with patterns. However, the flat structure of interaction nets forces
pattern matching and functional behaviour to be encoded at the same level,
losing some potential parallelism. In this paper, we introduce bigraphical
nets, or binets for short, as a generalisation of interaction nets using ideas
from bigraphs and port graphs, and we present a formal notation and operational
semantics for binets. We illustrate their expressive power by examples of
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6340</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6340</id><created>2013-02-26</created><authors><author><keyname>R</keyname><forenames>Kanagavalli. V.</forenames></author><author><keyname>K</keyname><forenames>Raja.</forenames></author></authors><title>A Fuzzy Logic based Method for Efficient Retrieval of Vague and
  Uncertain Spatial Expressions in Text Exploiting the Granulation of the
  Spatial Event Queries</title><categories>cs.IR</categories><comments>National Conference on Future Computing,0975 8887,IJCA,February2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The arrangement of things in n-dimensional space is specified as Spatial.
Spatial data consists of values that denote the location and shape of objects
and areas on the earths surface. Spatial information includes facts such as
location of features, the relationship of geographic features and measurements
of geographic features. The spatial cognition is a primal area of study in
various other fields such as Robotics, Psychology, Geosciences, Geography,
Political Sciences, Geographic Economy, Environmental, Mining and Petroleum
Engineering, Natural Resources, Epidemiology, Demography etc., Any text
document which contains physical location specifications such as place names,
geographic coordinates, landmarks, country names etc., are supposed to contain
the spatial information. The spatial information may also be represented using
vague or fuzzy descriptions involving linguistic terms such as near to, far
from, to the east of, very close. Given a query involving events, the aim of
this ongoing research work is to extract the relevant information from multiple
text documents, resolve the uncertainty and vagueness and translate them in to
locations in a map. The input to the system would be a text Corpus and a
Spatial Query event. The output of the system is a map showing the most
possible, disambiguated location of the event queried. The author proposes
Fuzzy Logic Techniques for resolving the uncertainty in the spatial
expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6346</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6346</id><created>2013-02-26</created><updated>2014-12-04</updated><authors><author><keyname>Richard</keyname><forenames>Adrien</forenames></author></authors><title>Fixed point theorems for Boolean networks expressed in terms of
  forbidden subnetworks</title><categories>cs.DM math.DS</categories><comments>40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in fixed points in Boolean networks, {\em i.e.} functions
$f$ from $\{0,1\}^n$ to itself. We define the subnetworks of $f$ as the
restrictions of $f$ to the subcubes of $\{0,1\}^n$, and we characterizes a
class $\mathcal{F}$ of Boolean networks satisfying the following property:
Every subnetwork of $f$ has a unique fixed point if and only if $f$ has no
subnetwork in $\mathcal{F}$. This characterization generalizes the fixed point
theorem of Shih and Dong, which asserts that if for every $x$ in $\{0,1\}^n$
there is no directed cycle in the directed graph whose the adjacency matrix is
the discrete Jacobian matrix of $f$ evaluated at point $x$, then $f$ has a
unique fixed point. Then, denoting by $\mathcal{C}^+$ (resp. $\mathcal{C}^-$)
the networks whose the interaction graph is a positive (resp. negative) cycle,
we show that the non-expansive networks of $\mathcal{F}$ are exactly the
networks of $\mathcal{C}^+\cup \mathcal{C}^-$; and for the class of
non-expansive networks we get a &quot;dichotomization&quot; of the previous forbidden
subnetwork theorem: Every subnetwork of $f$ has at most (resp. at least) one
fixed point if and only if $f$ has no subnetworks in $\mathcal{C}^+$ (resp.
$\mathcal{C}^-$) subnetwork. Finally, we prove that if $f$ is a conjunctive
network then every subnetwork of $f$ has at most one fixed point if and only if
$f$ has no subnetwork in $\mathcal{C}^+$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6352</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6352</id><created>2013-02-26</created><updated>2013-03-01</updated><authors><author><keyname>Rastaghi</keyname><forenames>Roohallah</forenames></author></authors><title>URDP: General Framework for Direct CCA2 Security from any Lattice-Based
  PKE Scheme</title><categories>cs.CR cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1302.0347, arXiv:1211.6984;
  and with arXiv:1205.5224 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design efficient lattice-based cryptosystem secure against adaptive chosen
ciphertext attack (IND-CCA2) is a challenge problem. To the date, full
CCA2-security of all proposed lattice-based PKE schemes achieved by using a
generic transformations such as either strongly unforgeable one-time signature
schemes (SU-OT-SS), or a message authentication code (MAC) and weak form of
commitment. The drawback of these schemes is that encryption requires &quot;separate
encryption&quot;. Therefore, the resulting encryption scheme is not sufficiently
efficient to be used in practice and it is inappropriate for many applications
such as small ubiquitous computing devices with limited resources such as smart
cards, active RFID tags, wireless sensor networks and other embedded devices.
  In this work, for the first time, we introduce an efficient universal random
data padding (URDP) scheme, and show how it can be used to construct a &quot;direct&quot;
CCA2-secure encryption scheme from &quot;any&quot; worst-case hardness problems in
(ideal) lattice in the standard model, resolving a problem that has remained
open till date. This novel approach is a &quot;black-box&quot; construction and leads to
the elimination of separate encryption, as it avoids using general
transformation from CPA-secure scheme to a CCA2-secure one. IND-CCA2 security
of this scheme can be tightly reduced in the standard model to the assumption
that the underlying primitive is an one-way trapdoor function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6363</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6363</id><created>2013-02-26</created><updated>2013-03-01</updated><authors><author><keyname>Azencott</keyname><forenames>Robert</forenames></author><author><keyname>Beri</keyname><forenames>Arjun</forenames></author><author><keyname>Gadhyan</keyname><forenames>Yutheeka</forenames></author><author><keyname>Joseph</keyname><forenames>Nicolas</forenames></author><author><keyname>Lehalle</keyname><forenames>Charles-Albert</forenames></author><author><keyname>Rowley</keyname><forenames>Matthew</forenames></author></authors><title>Realtime market microstructure analysis: online Transaction Cost
  Analysis</title><categories>q-fin.TR cs.IT math.IT math.ST stat.TH</categories><comments>33 pages, 12 figures</comments><msc-class>68T05, 91Gxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the practical challenge in monitoring the performance of a large
number of algorithmic trading orders, this paper provides a methodology that
leads to automatic discovery of the causes that lie behind a poor trading
performance. It also gives theoretical foundations to a generic framework for
real-time trading analysis. Academic literature provides different ways to
formalize these algorithms and show how optimal they can be from a
mean-variance, a stochastic control, an impulse control or a statistical
learning viewpoint. This paper is agnostic about the way the algorithm has been
built and provides a theoretical formalism to identify in real-time the market
conditions that influenced its efficiency or inefficiency. For a given set of
characteristics describing the market context, selected by a practitioner, we
first show how a set of additional derived explanatory factors, called anomaly
detectors, can be created for each market order. We then will present an online
methodology to quantify how this extended set of factors, at any given time,
predicts which of the orders are underperforming while calculating the
predictive power of this explanatory factor set. Armed with this information,
which we call influence analysis, we intend to empower the order monitoring
user to take appropriate action on any affected orders by re-calibrating the
trading algorithms working the order through new parameters, pausing their
execution or taking over more direct trading control. Also we intend that use
of this method in the post trade analysis of algorithms can be taken advantage
of to automatically adjust their trading action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6364</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6364</id><created>2013-02-26</created><authors><author><keyname>Alawneh</keyname><forenames>Ali Ahmad</forenames></author></authors><title>Assessing the Dimensions of Relationship Quality in B2C E Banking
  Services: An Empirical Comparative Study</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In B2C Online services contexts where relationships between customers and
service providers matter due to the lack of faceto- face interactions,
uncertainty, intangibility, hyper competition, increasing risk of fraud and
lack of trust. Relationship quality (RQ) is replacing service quality as a key
source of superior performance and competitive advantage. Accordingly, this
study investigates the dimensions of relationship quality from e-Banking
services and explores the differences among customers of Jordanian and foreign
banks. Many of the foreign banks in Jordan are equipped with large financial
capital, having high banking and financial experiences and are offering many
modern hi-tech banking services. This situation makes Jordanian banks in front
of unprecedented competition and so they have invested heavily to develop their
e-banking services, in order to strengthening their relationships with existing
customers, enhancing their trust, letting them satisfied, ensuring their
commitment and attracting of new ones. This paper aims to identify the key
dimensions that shape the relationship quality among Jordanian and foreign
banks and their customers who are utilizing the e-Banking services. Based on an
extensive review of relevant literature, we have formulated nine hypotheses and
identified three factors (satisfaction, trust, and commitment) that may affect
the competitiveness and success of Jordanian and foreign banks. Survey data
from 350 customers from four Jordanian banks and four foreign banks in Amman
city the capital of Jordan were collected and used to test the proposed
hypotheses. Based on the structural equation modeling and Ttest analyses, our
empirical analysis demonstrates several key findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6379</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6379</id><created>2013-02-26</created><authors><author><keyname>Ahmad</keyname><forenames>Faizan</forenames></author><author><keyname>Najam</keyname><forenames>Aaima</forenames></author><author><keyname>Ahmed</keyname><forenames>Zeeshan</forenames></author></authors><title>Image-based Face Detection and Recognition: &quot;State of the Art&quot;</title><categories>cs.CV</categories><comments>4 pages, 3 table, 4 figure</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 6, No 1, November 2012 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition from image or video is a popular topic in biometrics
research. Many public places usually have surveillance cameras for video
capture and these cameras have their significant value for security purpose. It
is widely acknowledged that the face recognition have played an important role
in surveillance system as it doesn't need the object's cooperation. The actual
advantages of face based identification over other biometrics are uniqueness
and acceptance. As human face is a dynamic object having high degree of
variability in its appearance, that makes face detection a difficult problem in
computer vision. In this field, accuracy and speed of identification is a main
issue.
  The goal of this paper is to evaluate various face detection and recognition
methods, provide complete solution for image based face detection and
recognition with higher accuracy, better response rate as an initial step for
video surveillance. Solution is proposed based on performed tests on various
face rich databases in terms of subjects, pose, emotions, race and light.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6383</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6383</id><created>2013-02-26</created><authors><author><keyname>Kriegl</keyname><forenames>Markus</forenames></author></authors><title>Module Border Bases</title><categories>math.AC cs.SC</categories><comments>65 pages</comments><msc-class>13P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we generalize the notion of border bases of zero-dimensional
polynomial ideals to the module setting. To this end, we introduce order
modules as a generalization of order ideals and module border bases of
submodules with finite codimension in a free module as a generalization of
border bases of zero-dimensional ideals in the first part of this paper. In
particular, we extend the division algorithm for border bases to the module
setting, show the existence and uniqueness of module border bases, and
characterize module border bases analogously like border bases via the special
generation property, border form modules, rewrite rules, commuting matrices,
and liftings of border syzygies. Furthermore, we deduce Buchberger's Criterion
for Module Border Bases and give an algorithm for the computation of module
border bases that uses linear algebra techniques. In the second part, we
further generalize the notion of module border bases to quotient modules. We
then show the connection between quotient module border bases and special
module border bases and deduce characterizations similar to the ones for module
border bases. Moreover, we give an algorithm for the computation of quotient
module border bases using linear algebra techniques, again. At last, we prove
that subideal border bases are isomorphic to special quotient module border
bases. This isomorphy immediately yields characterizations and an algorithm for
the computation of subideal border bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6390</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6390</id><created>2013-02-26</created><authors><author><keyname>Anbari</keyname><forenames>Mohammed El</forenames></author><author><keyname>Mkhadri</keyname><forenames>Abdallah</forenames></author></authors><title>The adaptive Gril estimator with a diverging number of parameters</title><categories>stat.ME cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of variables selection and estimation in linear
regression model in situations where the number of parameters diverges with the
sample size. We propose the adaptive Generalized Ridge-Lasso (\mbox{AdaGril})
which is an extension of the the adaptive Elastic Net. AdaGril incorporates
information redundancy among correlated variables for model selection and
estimation. It combines the strengths of the quadratic regularization and the
adaptively weighted Lasso shrinkage. In this paper, we highlight the grouped
selection property for AdaCnet method (one type of AdaGril) in the equal
correlation case. Under weak conditions, we establish the oracle property of
AdaGril which ensures the optimal large performance when the dimension is high.
Consequently, it achieves both goals of handling the problem of collinearity in
high dimension and enjoys the oracle property. Moreover, we show that AdaGril
estimator achieves a Sparsity Inequality, i. e., a bound in terms of the number
of non-zero components of the 'true' regression coefficient. This bound is
obtained under a similar weak Restricted Eigenvalue (RE) condition used for
Lasso. Simulations studies show that some particular cases of AdaGril
outperform its competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6391</identifier>
 <datestamp>2013-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6391</id><created>2013-02-26</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Inconsistencies of the Highly-Cited-Publications Indicator</title><categories>cs.DL physics.soc-ph</categories><comments>8 pages, 5 tables, accepted for publication in Journal of the
  American Society for Information Science and Technology</comments><journal-ref>Journal of the American Society for Information Science and
  Technology, 64(4), 1298-1302 (2013)</journal-ref><doi>10.1002/asi.22824</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  One way of evaluating individual scientists is the determination of the
number of highly cited publications, where the threshold is given by a large
reference set. It is shown that this indicator behaves in a counterintuitive
way, leading to inconsistencies in the ranking of different scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6392</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6392</id><created>2013-02-26</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>How much do different ways of calculating percentiles influence the
  derived performance indicators? - A case study</title><categories>cs.DL physics.soc-ph</categories><comments>12 pages, 3 figures, 3 tables, accepted for publication in
  Scientometrics</comments><journal-ref>Scientometrics, 97, 821-829 (2013)</journal-ref><doi>10.1007/s11192-013-0984-x</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Bibliometric indicators can be determined by comparing specific citation
records with the percentiles of a reference set. However, there exists an
ambiguity in the computation of percentiles because usually a significant
number of papers with the same citation count are found at the border between
percentile rank classes. The present case study of the citations to the journal
Europhysics Letters (EPL) in comparison with all physics papers from the Web of
Science shows the deviations which occur due to the different ways of treating
the tied papers in the evaluation of the percentage of highly cited
publications. A strong bias can occur, if the papers tied at the threshold
number of citations are all considered as highly cited or all considered as not
highly cited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6396</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6396</id><created>2013-02-26</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>How to derive an advantage from the arbitrariness of the g-index</title><categories>physics.soc-ph cs.DL</categories><comments>13 pages, 3 tables, 3 figures, accepted for publication in Journal of
  Informetrics</comments><journal-ref>Journal of Informetrics, 7, 555-561 (2013)</journal-ref><doi>10.1016/j.joi.2013.02.003</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The definition of the g-index is as arbitrary as that of the h-index, because
the threshold number g^2 of citations to the g most cited papers can be
modified by a prefactor at one's discretion, thus taking into account more or
less of the highly cited publications within a dataset. In a case study I
investigate the citation records of 26 physicists and show that the prefactor
influences the ranking in terms of the generalized g-index less than for the
generalized h-index. I propose specifically a prefactor of 2 for the g-index,
because then the resulting values are of the same order of magnitude as for the
common h-index. In this way one can avoid the disadvantage of the original
g-index, namely that the values are usually substantially larger than for the
h-index and thus the precision problem is substantially larger; while the
advantages of the g-index over the h-index are kept. Like for the generalized
h-index, also for the generalized g-index different prefactors might be more
useful for investigations which concentrate only on top scientists with high
citation frequencies or on junior researchers with small numbers of citations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6401</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6401</id><created>2013-02-26</created><authors><author><keyname>England</keyname><forenames>Matthew</forenames></author></authors><title>An implementation of CAD in Maple utilising McCallum projection</title><categories>cs.SC</categories><comments>9 pages; University of Bath, Dept. Computer Science Technical Report
  Series, 2013-02, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cylindrical algebraic decomposition (CAD) is an important tool for the
investigation of semi-algebraic sets. Originally introduced by Collins in the
1970s for use in quantifier elimination it has since found numerous
applications within algebraic geometry and beyond. Following from his original
work in 1988, McCallum presented an improved algorithm, CADW, which offered a
huge increase in the practical utility of CAD. In 2009 a team based at the
University of Western Ontario presented a new and quite separate algorithm for
CAD, which was implemented and included in the computer algebra system Maple.
As part of a wider project at Bath investigating CAD and its applications,
Collins and McCallum's CAD algorithms have been implemented in Maple. This
report details these implementations and compares them to Qepcad and the
Ontario algorithm.
  The implementations were originally undertaken to facilitate research into
the connections between the algorithms. However, the ability of the code to
guarantee order-invariant output has led to its use in new research on CADs
which are minimal for certain problems. In addition, the implementation
described here is of interest as the only full implementation of CADW, (since
Qepcad does not currently make use of McCallum's delineating polynomials), and
hence can solve problems not admissible to other CAD implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6411</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6411</id><created>2013-02-26</created><authors><author><keyname>Etessami</keyname><forenames>Kousha</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>Stochastic Context-Free Grammars, Regular Languages, and Newton's Method</title><categories>cs.FL cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing the probability that a given stochastic
context-free grammar (SCFG), G, generates a string in a given regular language
L(D) (given by a DFA, D). This basic problem has a number of applications in
statistical natural language processing, and it is also a key necessary step
towards quantitative \omega-regular model checking of stochastic context-free
processes (equivalently, 1-exit recursive Markov chains, or stateless
probabilistic pushdown processes).
  We show that the probability that G generates a string in L(D) can be
computed to within arbitrary desired precision in polynomial time (in the
standard Turing model of computation), under a rather mild assumption about the
SCFG, G, and with no extra assumption about D. We show that this assumption is
satisfied for SCFG's whose rule probabilities are learned via the well-known
inside-outside (EM) algorithm for maximum-likelihood estimation (a standard
method for constructing SCFGs in statistical NLP and biological sequence
analysis). Thus, for these SCFGs the algorithm always runs in P-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6421</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6421</id><created>2013-02-26</created><updated>2013-05-24</updated><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>ML4PG in Computer Algebra verification</title><categories>cs.LO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ML4PG is a machine-learning extension that provides statistical proof hints
during the process of Coq/SSReflect proof development. In this paper, we use
ML4PG to find proof patterns in the CoqEAL library -- a library that was
devised to verify the correctness of Computer Algebra algorithms. In
particular, we use ML4PG to help us in the formalisation of an efficient
algorithm to compute the inverse of triangular matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6424</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6424</id><created>2013-02-26</created><authors><author><keyname>sharma</keyname><forenames>Rupam kumar</forenames></author></authors><title>Generation of Biometric key for use in DES</title><categories>cs.CR</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 6, No 1, November 2012 ISSN (Online): 1694-0814</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cryptography is an important field in the area of data encryption. There are
different cryptographic techniques available varying from the simplest to
complex. One of the complex symmetric key cryptography techniques is using Data
Encryption Standard Algorithm. This paper explores a unique approach to
generation of key using fingerprint. The generated key is used as an input key
to the DES Algorithm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6426</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6426</id><created>2013-02-26</created><authors><author><keyname>Meena</keyname><forenames>A.</forenames></author><author><keyname>Raja</keyname><forenames>K.</forenames></author></authors><title>Segmentation of Alzheimers Disease in PET scan datasets using MATLAB</title><categories>cs.NE</categories><journal-ref>International Journal on Information Sciences and
  Computing,Vol.6,No.2,July 2012,PP.No.44-48</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positron Emission Tomography (PET) scan images are one of the bio medical
imaging techniques similar to that of MRI scan images but PET scan images are
helpful in finding the development of tumors.The PET scan images requires
expertise in the segmentation where clustering plays an important role in the
automation process.The segmentation of such images is manual to automate the
process clustering is used.Clustering is commonly known as unsupervised
learning process of n dimensional data sets are clustered into k groups so as
to maximize the inter cluster similarity and to minimize the intra cluster
similarity.This paper is proposed to implement the commonly used K Means and
Fuzzy CMeans (FCM) clustering algorithm.This work is implemented using MATrix
LABoratory (MATLAB) and tested with sample PET scan image. The sample data is
collected from Alzheimers Disease Neuro imaging Initiative ADNI. Medical Image
Processing and Visualization Tool (MIPAV) are used to compare the resultant
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6428</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6428</id><created>2013-02-26</created><authors><author><keyname>Li</keyname><forenames>Keying</forenames></author></authors><title>Matrix Access structure Policy used in Attribute-Based Proxy
  Re-encryption</title><categories>cs.CR</categories><comments>9 pages,1 figure, IJCSI</comments><journal-ref>International Journal of Computer Science issue Vol. 9, Issue 6,
  No 2, November 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proxy re-encryption (PRE) allows a semi-trusted proxy to convert a ciphertext
originally intended for Alice into an encryption of the same message intended
for Bob. Song Luo, Jianbin Hu, and Zhong Chen presented a novel ciphertext
policy attribute-based proxy re-encryption (CP-AB-PRE) scheme. The ciphertext
policy realized in their scheme is AND-gates policy supporting multi-value
attributes, negative attributes and wildcards. We propose a new access policies
based on LSSS matrix access structures. Our scheme still have the properties of
both PRE and CP-AB-PRE, such as unidirectionality, non-interactivity,
multi-use, allows the encryptor to decide whether the ciphertext can be
re-encrypted and allows the proxy to add access policy. Furthermore, our scheme
can be modified to outsource the policy of W2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6436</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6436</id><created>2013-02-26</created><authors><author><keyname>Nordmann</keyname><forenames>Arne</forenames></author><author><keyname>Wrede</keyname><forenames>Sebastian</forenames></author></authors><title>A Domain-Specific Language for Rich Motor Skill Architectures</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2012 (arXiv:cs/1302.5082)</comments><report-no>DSLRob/2012/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-driven software development is a promising way to cope with the
complexity of system integration in advanced robotics, as it already
demonstrated its benefits in domains with comparably challenging system
integration requirements. This paper reports on work in progress in this area
which aims to improve the research and experimentation process in a
collaborative research project developing motor skill architectures for
compliant robots. Our goal is to establish a model-driven development process
throughout the project around a domain-specific language (DSL) facilitating the
compact description of adaptive modular architectures for rich motor skills.
Incorporating further languages for other aspects (e.g. mapping to a technical
component architecture) the approach allows not only the formal description of
motor skill architectures but also automated code-generation for
experimentation on technical robot platforms. This paper reports on a first
case study exemplifying how the developed AMARSi DSL helps to conceptualize
different architectural approaches and to identify their similarities and
differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6442</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6442</id><created>2013-02-26</created><authors><author><keyname>Foug&#xe8;res</keyname><forenames>Alain-J&#xe9;r&#xf4;me</forenames></author></authors><title>A Modelling Approach Based on Fuzzy Agents</title><categories>cs.AI</categories><comments>10 pages, 8 figures, 35 references</comments><journal-ref>IJCSI-2012-9-6-4655</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling of complex systems is mainly based on the decomposition of these
systems in autonomous elements, and the identification and definitio9n of
possible interactions between these elements. For this, the agent-based
approach is a modelling solution often proposed. Complexity can also be due to
external events or internal to systems, whose main characteristics are
uncertainty, imprecision, or whose perception is subjective (i.e. interpreted).
Insofar as fuzzy logic provides a solution for modelling uncertainty, the
concept of fuzzy agent can model both the complexity and uncertainty. This
paper focuses on introducing the concept of fuzzy agent: a classical
architecture of agent is redefined according to a fuzzy perspective. A
pedagogical illustration of fuzzy agentification of a smart watering system is
then proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6452</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6452</id><created>2013-02-26</created><authors><author><keyname>Lei</keyname><forenames>Jing</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>A Conformal Prediction Approach to Explore Functional Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies conformal prediction techniques to compute simultaneous
prediction bands and clustering trees for functional data. These tools can be
used to detect outliers and clusters. Both our prediction bands and clustering
trees provide prediction sets for the underlying stochastic process with a
guaranteed finite sample behavior, under no distributional assumptions. The
prediction sets are also informative in that they correspond to the high
density region of the underlying process. While ordinary conformal prediction
has high computational cost for functional data, we use the inductive conformal
predictor, together with several novel choices of conformity scores, to
simplify the computation. Our methods are illustrated on some real data
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6454</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6454</id><created>2013-01-23</created><authors><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Dubrova</keyname><forenames>Elena</forenames></author></authors><title>Embedding of Deterministic Test Data for In-Field Testing</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new feedback shift register-based method for embedding
deterministic test patterns on-chip suitable for complementing conventional
BIST techniques for in-field testing. Our experimental results on 8 real
designs show that the presented approach outperforms the bit-flipping approach
by 24.7% on average. We also show that it is possible to exploit the uneven
distribution of don't care bits in test patterns in order to reduce the area
required for storing deterministic test patterns more than 3 times with less
than 2% fault coverage drop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6482</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6482</id><created>2013-02-26</created><updated>2013-05-06</updated><authors><author><keyname>Matousek</keyname><forenames>Jiri</forenames></author></authors><title>Near-optimal separators in string graphs</title><categories>math.CO cs.DM cs.DS</categories><comments>4 pages; minor corrections and updates compared to version 1</comments><msc-class>05C62, 05C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a string graph (an intersection graph of continuous arcs in the
plane) with m edges. Fox and Pach proved that G has a separator consisting of
O(m^{3/4}\sqrt{log m})$ vertices, and they conjectured that the bound of
O(\sqrt m) actually holds. We obtain separators with O(\sqrt m \log m)
vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6500</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6500</id><created>2013-02-26</created><updated>2016-02-02</updated><authors><author><keyname>Munaro</keyname><forenames>Andrea</forenames></author></authors><title>The VC-Dimension of Graphs with Respect to k-Connected Subgraphs</title><categories>cs.DM cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the VC-dimension of the set system on the vertex set of some graph
which is induced by the family of its $k$-connected subgraphs. In particular,
we give tight upper and lower bounds for the VC-dimension. Moreover, we show
that computing the VC-dimension is $\mathsf{NP}$-complete and that it remains
$\mathsf{NP}$-complete for split graphs and for some subclasses of planar
bipartite graphs in the cases $k = 1$ and $k = 2$. On the positive side, we
observe it can be decided in linear time for graphs of bounded clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6514</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6514</id><created>2013-02-26</created><updated>2013-05-02</updated><authors><author><keyname>Gatto</keyname><forenames>Alberto</forenames></author></authors><title>Bisimulation and p-morphism for branching-time logics with
  indistinguishability relations</title><categories>cs.LO</categories><comments>10 pages, corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Zanardo, 1998, the Peircean semantics for branching-time logics is
enriched with a notion of indistinguishability at a moment t between histories
passing through t. Trees with indistinguishability relations provide a
semantics for a temporal language with tense and modal operators. In this paper
a notion of p-morphism and a notion of bisimulation, wrt this language and
semantics, are given and a number of preservation results are proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6515</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6515</id><created>2013-02-26</created><updated>2013-04-09</updated><authors><author><keyname>Yakopcic</keyname><forenames>Chris</forenames></author><author><keyname>Taha</keyname><forenames>Tarek M.</forenames></author></authors><title>Hybrid Crossbar Architecture for a Memristor Based Memory</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new memristor crossbar architecture that is proposed
for use in a high density cache design. This design has less than 10% of the
write energy consumption than a simple memristor crossbar. Also, it has up to 4
times the bit density of an STT-MRAM system and up to 11 times the bit density
of an SRAM architecture. The proposed architecture is analyzed using a detailed
SPICE analysis that accounts for the resistance of the wires in the memristor
structure. Additionally, the memristor model used in this work has been matched
to specific device characterization data to provide accurate results in terms
of energy, area, and timing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6521</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6521</id><created>2013-02-26</created><updated>2013-04-03</updated><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Imperfect and Unmatched CSIT is Still Useful for the Frequency
  Correlated MISO Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>Accepted to ICC'13 CT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Maddah-Ali and Tse showed that the completely stale transmitter-side
channel state information (CSIT) still benefits the Degrees of Freedom (DoF) of
the Multiple-Input-Multiple-Output (MISO) Broadcast Channel (BC), there has
been much interest in the academic literature to investigate the impact of
imperfect CSIT on \emph{DoF} region of time correlated broadcast channel. Even
though the research focus has been on time correlated channels so far, a
similar but different problem concerns the frequency correlated channels.
Indeed, the imperfect CSIT also impacts the DoF region of frequency correlated
channels, as exemplified by current multi-carrier wireless systems.
  This contribution, for the first time in the literature, investigates a
general frequency correlated setting where a two-antenna transmitter has
imperfect knowledge of CSI of two single-antenna users on two adjacent
subbands. A new scheme is derived as an integration of Zero-Forcing Beamforming
(ZFBF) and the scheme proposed by Maddah-Ali and Tse. The achievable DoF region
resulted by this scheme is expressed as a function of the qualities of CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6523</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6523</id><created>2013-02-26</created><authors><author><keyname>Ding</keyname><forenames>Yin</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Sparse Frequency Analysis with Sparse-Derivative Instantaneous Amplitude
  and Phase Functions</title><categories>cs.LG</categories><comments>13 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of expressing a signal as a sum of frequency
components (sinusoids) wherein each sinusoid may exhibit abrupt changes in its
amplitude and/or phase. The Fourier transform of a narrow-band signal, with a
discontinuous amplitude and/or phase function, exhibits spectral and temporal
spreading. The proposed method aims to avoid such spreading by explicitly
modeling the signal of interest as a sum of sinusoids with time-varying
amplitudes. So as to accommodate abrupt changes, it is further assumed that the
amplitude/phase functions are approximately piecewise constant (i.e., their
time-derivatives are sparse). The proposed method is based on a convex
variational (optimization) approach wherein the total variation (TV) of the
amplitude functions are regularized subject to a perfect (or approximate)
reconstruction constraint. A computationally efficient algorithm is derived
based on convex optimization techniques. The proposed technique can be used to
perform band-pass filtering that is relatively insensitive to narrow-band
amplitude/phase jumps present in data, which normally pose a challenge (due to
transients, leakage, etc.). The method is illustrated using both synthetic
signals and human EEG data for the purpose of band-pass filtering and the
estimation of phase synchrony indexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6528</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6528</id><created>2013-02-26</created><updated>2013-03-06</updated><authors><author><keyname>Manana-Rodriguez</keyname><forenames>J.</forenames></author></authors><title>Entropy-based disciplinarity indicator: role taxonomy of journals in
  scientific communication systems and isolation degree. Knowledge
  importation/exportation profiles from journals and disciplines</title><categories>cs.DL physics.soc-ph</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research, a new indicator of disciplinarity-multidisciplinarity is
developed, discussed and applied. EBDI is based on the combination of the
frequency distribution of subject categories of journals citing or cited by the
analysis unit and the spread and diversity of the citations among subject
categories measured with Shannon-Wiener entropy. Its reproducibility,
robustness and consistence are discussed. Four of the combinations of its
values when applied to the cited and citing dimensions lead to a suggested
taxonomy of the role that the studied unit might have in terms of the
transformation of knowledge from different disciplines in the scientific
communication system and its position respect a hypothetical thematic core of
the discipline in which it has been classified. The indicator is applied to the
journals belonging to the first quartile of JCR-SSCI 2011 Library and
Information Science and an indicator-based taxonomy is applied and discussed,
pointing to differential thematic roles of the journals analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6533</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6533</id><created>2013-02-26</created><updated>2013-03-01</updated><authors><author><keyname>Cort&#xe9;s-Berrueco</keyname><forenames>Luis Enrique</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Stephens</keyname><forenames>Christopher R.</forenames></author></authors><title>Self-Organization Promotes the Evolution of Cooperation with Cultural
  Propagation</title><categories>cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper three computational models for the study of the evolution of
cooperation under cultural propagation are studied: Kin Selection, Direct
Reciprocity and Indirect Reciprocity. Two analyzes are reported, one comparing
their behavior between them and a second one identifying the impact that
different parameters have in the model dynamics. The results of these analyzes
illustrate how game transitions may occur depending of some parameters within
the models and also explain how agents adapt to these transitions by
individually choosing their attachment to a cooperative attitude. These
parameters regulate how cooperation can self-organize under different
circumstances. The emergence of the evolution of cooperation as a result of the
agent's adapting processes is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6542</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6542</id><created>2013-02-26</created><updated>2013-02-27</updated><authors><author><keyname>Lee</keyname><forenames>James R.</forenames></author><author><keyname>Moharrami</keyname><forenames>Mohammad</forenames></author></authors><title>A lower bound on dimension reduction for trees in \ell_1</title><categories>math.MG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a constant c &gt; 0 such that for every $\epsilon \in (0,1)$ and $n
\geq 1/\epsilon^2$, the following holds. Any mapping from the $n$-point star
metric into $\ell_1^d$ with bi-Lipschitz distortion $1+\epsilon$ requires
dimension $$d \geq {c\log n\over \epsilon^2\log (1/\epsilon)}.$$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6556</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6556</id><created>2013-02-26</created><updated>2013-03-11</updated><authors><author><keyname>Rekatsinas</keyname><forenames>Theodoros</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author></authors><title>On Sharing Private Data with Multiple Non-Colluding Adversaries</title><categories>cs.DB</categories><comments>14 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present SPARSI, a theoretical framework for partitioning sensitive data
across multiple non-colluding adversaries. Most work in privacy-aware data
sharing has considered disclosing summaries where the aggregate information
about the data is preserved, but sensitive user information is protected.
Nonetheless, there are applications, including online advertising, cloud
computing and crowdsourcing markets, where detailed and fine-grained user-data
must be disclosed. We consider a new data sharing paradigm and introduce the
problem of privacy-aware data partitioning, where a sensitive dataset must be
partitioned among k untrusted parties (adversaries). The goal is to maximize
the utility derived by partitioning and distributing the dataset, while
minimizing the amount of sensitive information disclosed. The data should be
distributed so that an adversary, without colluding with other adversaries,
cannot draw additional inferences about the private information, by linking
together multiple pieces of information released to her. The assumption of no
collusion is both reasonable and necessary in the above application domains
that require release of private user information. SPARSI enables us to formally
define privacy-aware data partitioning using the notion of sensitive properties
for modeling private information and a hypergraph representation for describing
the interdependencies between data entries and private information. We show
that solving privacy-aware partitioning is, in general, NP-hard, but for
specific information disclosure functions, good approximate solutions can be
found using relaxation techniques. Finally, we present a local search algorithm
applicable to generic information disclosure functions. We apply SPARSI
together with the proposed algorithms on data from a real advertising scenario
and show that we can partition data with no disclosure to any single
advertiser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6557</identifier>
 <datestamp>2013-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6557</id><created>2013-02-26</created><updated>2013-08-23</updated><authors><author><keyname>Jiang</keyname><forenames>Richard M</forenames></author></authors><title>Geodesic-based Salient Object Detection</title><categories>cs.CV cs.AI</categories><comments>The manuscript was submitted to a conference. Due to anonymous review
  policy by the conference, I'd like to withdraw it temporarily</comments><journal-ref>This is a revised version of our submissions to CVPR 2012, SIGRAPH
  Asia 2012, and CVPR 2013;</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saliency detection has been an intuitive way to provide useful cues for
object detection and segmentation, as desired for many vision and graphics
applications. In this paper, we provided a robust method for salient object
detection and segmentation. Other than using various pixel-level contrast
definitions, we exploited global image structures and proposed a new geodesic
method dedicated for salient object detection. In the proposed approach, a new
geodesic scheme, namely geodesic tunneling is proposed to tackle with textures
and local chaotic structures. With our new geodesic approach, a geodesic
saliency map is estimated in correspondence to spatial structures in an image.
Experimental evaluation on a salient object benchmark dataset validated that
our algorithm consistently outperformed a number of the state-of-art saliency
methods, yielding higher precision and better recall rates. With the robust
saliency estimation, we also present an unsupervised hierarchical salient
object cut scheme simply using adaptive saliency thresholding, which attained
the highest score in our F-measure test. We also applied our geodesic cut
scheme to a number of image editing tasks as demonstrated in additional
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6562</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6562</id><created>2013-02-26</created><updated>2013-07-29</updated><authors><author><keyname>Cullina</keyname><forenames>Daniel</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>An Improvement to Levenshtein's Upper Bound on the Cardinality of
  Deletion Correcting Codes</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider deletion correcting codes over a q-ary alphabet. It is well known
that any code capable of correcting s deletions can also correct any
combination of s total insertions and deletions. To obtain asymptotic upper
bounds on code size, we apply a packing argument to channels that perform
different mixtures of insertions and deletions. Even though the set of codes is
identical for all of these channels, the bounds that we obtain vary. Prior to
this work, only the bounds corresponding to the all insertion case and the all
deletion case were known. We recover these as special cases. The bound from the
all deletion case, due to Levenshtein, has been the best known for more than
forty five years. Our generalized bound is better than Levenshtein's bound
whenever the number of deletions to be corrected is larger than the alphabet
size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6567</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6567</id><created>2013-02-26</created><authors><author><keyname>Harrington</keyname><forenames>Heather A.</forenames></author><author><keyname>D&#xed;az</keyname><forenames>Mariano Beguerisse</forenames></author><author><keyname>Rombach</keyname><forenames>M. Puck</forenames></author><author><keyname>Keating</keyname><forenames>Laura M.</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>Teach Network Science to Teenagers</title><categories>physics.ed-ph cs.SI math.CO physics.pop-ph physics.soc-ph</categories><comments>13 pages of main text (with 5 figures) + supplementary online
  material containing additional teaching materials; the SOM is available at
  http://people.maths.ox.ac.uk/~porterm/research/harringtonetal2013-SOM.zip ;
  submitted to 'Network Science' as an editorial</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss our outreach efforts to introduce school students to network
science and explain why networks researchers should be involved in such
outreach activities. We provide overviews of modules that we have designed for
these efforts, comment on our successes and failures, and illustrate the
potentially enormous impact of such outreach efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6569</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6569</id><created>2013-02-26</created><authors><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Goncalves</keyname><forenames>Bruno</forenames></author><author><keyname>Ciulla</keyname><forenames>Fabio</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Characterizing scientific production and consumption in Physics</title><categories>physics.soc-ph cs.DL cs.SI</categories><journal-ref>Nature Scientific Reports 3, 1640 (2013)</journal-ref><doi>10.1038/srep01640</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the entire publication database of the American Physical Society
generating longitudinal (50 years) citation networks geolocalized at the level
of single urban areas. We define the knowledge diffusion proxy, and scientific
production ranking algorithms to capture the spatio-temporal dynamics of
Physics knowledge worldwide. By using the knowledge diffusion proxy we identify
the key cities in the production and consumption of knowledge in Physics as a
function of time. The results from the scientific production ranking algorithm
allow us to characterize the top cities for scholarly research in Physics.
Although we focus on a single dataset concerning a specific field, the
methodology presented here opens the path to comparative studies of the
dynamics of knowledge across disciplines and research areas
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6570</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6570</id><created>2013-02-26</created><authors><author><keyname>Xie</keyname><forenames>Jianwei</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Secure Degrees of Freedom of the Gaussian Wiretap Channel with Helpers
  and No Eavesdropper CSI: Blind Cooperative Jamming</title><categories>cs.IT cs.CR math.IT</categories><comments>To appear in the CISS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Gaussian wiretap channel with M helpers, where no
eavesdropper channel state information (CSI) is available at the legitimate
entities. The exact secure d.o.f. of the Gaussian wiretap channel with M
helpers with perfect CSI at the transmitters was found in [1], [2] to be
M/(M+1). One of the key ingredients of the optimal achievable scheme in [1],
[2] is to align cooperative jamming signals with the information symbols at the
eavesdropper to limit the information leakage rate. This required perfect
eavesdropper CSI at the transmitters. Motivated by the recent result in [3], we
propose a new achievable scheme in which cooperative jamming signals span the
entire space of the eavesdropper, but are not exactly aligned with the
information symbols. We show that this scheme achieves the same secure d.o.f.
of M/(M+1) in [1], [2] but does not require any eavesdropper CSI; the
transmitters blindly cooperative jam the eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6574</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6574</id><created>2013-02-26</created><updated>2013-03-22</updated><authors><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author><author><keyname>Chandar</keyname><forenames>Venkat</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Energy and Sampling Constrained Asynchronous Communication</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum energy, and, more generally, the minimum cost, to transmit one
bit of information has been recently derived for bursty communication when
information is available infrequently at random times at the transmitter. This
result assumes that the receiver is always in the listening mode and samples
all channel outputs until it makes a decision. If the receiver is constrained
to sample only a fraction f&gt;0 of the channel outputs, what is the cost penalty
due to sparse output sampling?
  Remarkably, there is no penalty: regardless of f&gt;0 the asynchronous capacity
per unit cost is the same as under full sampling, ie, when f=1. There is not
even a penalty in terms of decoding delay---the elapsed time between when
information is available until when it is decoded. This latter result relies on
the possibility to sample adaptively; the next sample can be chosen as a
function of past samples. Under non-adaptive sampling, it is possible to
achieve the full sampling asynchronous capacity per unit cost, but the decoding
delay gets multiplied by 1/f. Therefore adaptive sampling strategies are of
particular interest in the very sparse sampling regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6580</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6580</id><created>2013-02-26</created><authors><author><keyname>Stefanidis</keyname><forenames>Kostas</forenames></author><author><keyname>Pitoura</keyname><forenames>Evaggelia</forenames></author></authors><title>Finding the Right Set of Users: Generalized Constraints for Group
  Recommendations</title><categories>cs.IR</categories><comments>PersDB 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, group recommendations have attracted considerable attention. Rather
than recommending items to individual users, group recommenders recommend items
to groups of users. In this position paper, we introduce the problem of forming
an appropriate group of users to recommend an item when constraints apply to
the members of the group. We present a formal model of the problem and an
algorithm for its solution. Finally, we identify several directions for future
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6582</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6582</id><created>2013-02-26</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>A Case Study of the Arbitrariness of the h-Index and the
  Highly-Cited-Publications Indicator</title><categories>physics.soc-ph cs.DL</categories><comments>16 pages, 3 tables, 5 figures. arXiv admin note: text overlap with
  arXiv:1302.6396</comments><journal-ref>Journal of Informetrics, 7(2), 379-387 (2013)</journal-ref><doi>10.1016/j.joi.2012.12.006</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The arbitrariness of the h-index becomes evident, when one requires q*h
instead of h citations as the threshold for the definition of the index, thus
changing the size of the core of the most influential publications of a
dataset. I analyze the citation records of 26 physicists in order to determine
how much the prefactor q influences the ranking. Likewise, the arbitrariness of
the highly-cited-publications indicator is due to the threshold value, given
either as an absolute number of citations or as a percentage of highly cited
papers. The analysis of the 26 citation records shows that the changes in the
rankings in dependence on these thresholds are rather large and comparable with
the respective changes for the h-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6584</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6584</id><created>2013-02-26</created><updated>2013-07-17</updated><authors><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander</forenames></author></authors><title>Variational Algorithms for Marginal MAP</title><categories>stat.ML cs.AI cs.IT cs.LG math.IT</categories><comments>This is a journal version of our conference paper &quot;variational
  algorithms for marginal MAP&quot; in UAI 201 [arXiv:1202.3742]; this version is
  considerably expanded, with more detail in its development, examples,
  algorithms, and proofs; additional experiments; and a junction graph version
  of the central message-passing algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The marginal maximum a posteriori probability (MAP) estimation problem, which
calculates the mode of the marginal posterior distribution of a subset of
variables with the remaining variables marginalized, is an important inference
problem in many models, such as those with hidden variables or uncertain
parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has
attracted less attention in the literature compared to the joint MAP
(maximization) and marginalization problems. We derive a general dual
representation for marginal MAP that naturally integrates the marginalization
and maximization operations into a joint variational optimization problem,
making it possible to easily extend most or all variational-based algorithms to
marginal MAP. In particular, we derive a set of &quot;mixed-product&quot; message passing
algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product
and a novel &quot;argmax-product&quot; message updates. We also derive a class of
convergent algorithms based on proximal point methods, including one that
transforms the marginal MAP problem into a sequence of standard marginalization
problems. Theoretically, we provide guarantees under which our algorithms give
globally or locally optimal solutions, and provide novel upper bounds on the
optimal objectives. Empirically, we demonstrate that our algorithms
significantly outperform the existing approaches, including a state-of-the-art
algorithm based on local search methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6595</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6595</id><created>2013-02-26</created><authors><author><keyname>Adhikari</keyname><forenames>Ratnadip</forenames></author><author><keyname>Agrawal</keyname><forenames>R. K.</forenames></author></authors><title>Combining Multiple Time Series Models Through A Robust Weighted
  Mechanism</title><categories>cs.AI stat.AP</categories><comments>6 pages, 3 figures, 2 tables, conference</comments><msc-class>68T05</msc-class><journal-ref>International Conference on Recent Advances in Information
  Technology, 2012</journal-ref><doi>10.1109/RAIT.2012.6194621</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improvement of time series forecasting accuracy through combining multiple
models is an important as well as a dynamic area of research. As a result,
various forecasts combination methods have been developed in literature.
However, most of them are based on simple linear ensemble strategies and hence
ignore the possible relationships between two or more participating models. In
this paper, we propose a robust weighted nonlinear ensemble technique which
considers the individual forecasts from different models as well as the
correlations among them while combining. The proposed ensemble is constructed
using three well-known forecasting models and is tested for three real-world
time series. A comparison is made among the proposed scheme and three other
widely used linear combination methods, in terms of the obtained forecast
errors. This comparison shows that our ensemble scheme provides significantly
lower forecast errors than each individual model as well as each of the four
linear combination methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6602</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6602</id><created>2013-02-26</created><authors><author><keyname>Ibrahim</keyname><forenames>Lamiaa Fattouh</forenames></author><author><keyname>Harbi</keyname><forenames>Manal Hamed Al</forenames></author></authors><title>Using Modified Partitioning Around Medoids Clustering Technique in
  Mobile Network Planning</title><categories>cs.AI cs.NI</categories><comments>10 pages, 15 figures</comments><journal-ref>International Journal of Computer Science Issues Volume 9, Issue
  6, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every cellular network deployment requires planning and optimization in order
to provide adequate coverage, capacity, and quality of service (QoS).
Optimization mobile radio network planning is a very complex task, as many
aspects must be taken into account. With the rapid development in mobile
network we need effective network planning tool to satisfy the need of
customers. However, deciding upon the optimum placement for the base stations
(BS s) to achieve best services while reducing the cost is a complex task
requiring vast computational resource. This paper introduces the spatial
clustering to solve the Mobile Networking Planning problem. It addresses
antenna placement problem or the cell planning problem, involves locating and
configuring infrastructure for mobile networks by modified the original
Partitioning Around Medoids PAM algorithm. M-PAM (Modified Partitioning Around
Medoids) has been proposed to satisfy the requirements and constraints. PAM
needs to specify number of clusters (k) before starting to search for the best
locations of base stations. The M-PAM algorithm uses the radio network planning
to determine k. We calculate for each cluster its coverage and capacity and
determine if they satisfy the mobile requirements, if not we will increase (k)
and reapply algorithms depending on two methods for clustering. Implementation
of this algorithm to a real case study is presented. Experimental results and
analysis indicate that the M-PAM algorithm when applying method two is
effective in case of heavy load distribution, and leads to minimum number of
base stations, which directly affected onto the cost of planning the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6613</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6613</id><created>2013-02-26</created><authors><author><keyname>Adhikari</keyname><forenames>Ratnadip</forenames></author><author><keyname>Agrawal</keyname><forenames>R. K.</forenames></author></authors><title>An Introductory Study on Time Series Modeling and Forecasting</title><categories>cs.LG stat.ML</categories><comments>67 pages, 29 figures, 33 references, book</comments><msc-class>68T01</msc-class><journal-ref>LAP Lambert Academic Publishing, Germany, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series modeling and forecasting has fundamental importance to various
practical domains. Thus a lot of active research works is going on in this
subject during several years. Many important models have been proposed in
literature for improving the accuracy and effectiveness of time series
forecasting. The aim of this dissertation work is to present a concise
description of some popular time series forecasting models used in practice,
with their salient features. In this thesis, we have described three important
classes of time series models, viz. the stochastic, neural networks and SVM
based models, together with their inherent forecasting strengths and
weaknesses. We have also discussed about the basic issues related to time
series modeling, such as stationarity, parsimony, overfitting, etc. Our
discussion about different time series models is supported by giving the
experimental forecast results, performed on six real time series datasets.
While fitting a model to a dataset, special care is taken to select the most
parsimonious one. To evaluate forecast accuracy as well as to compare among
different models fitted to a time series, we have used the five performance
measures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of the
six datasets, we have shown the obtained forecast diagram which graphically
depicts the closeness between the original and forecasted observations. To have
authenticity as well as clarity in our discussion about time series modeling
and forecasting, we have taken the help of various published research works
from reputed journals and some standard books.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6615</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6615</id><created>2013-02-26</created><authors><author><keyname>Adhikari</keyname><forenames>Ratnadip</forenames></author><author><keyname>Agrawal</keyname><forenames>R. K.</forenames></author><author><keyname>Kant</keyname><forenames>Laxmi</forenames></author></authors><title>PSO based Neural Networks vs. Traditional Statistical Models for
  Seasonal Time Series Forecasting</title><categories>cs.NE</categories><comments>4 figures, 4 tables, 31 references, conference proceedings</comments><msc-class>68T05</msc-class><journal-ref>IEEE International Advanced Computing Conference (IACC), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seasonality is a distinctive characteristic which is often observed in many
practical time series. Artificial Neural Networks (ANNs) are a class of
promising models for efficiently recognizing and forecasting seasonal patterns.
In this paper, the Particle Swarm Optimization (PSO) approach is used to
enhance the forecasting strengths of feedforward ANN (FANN) as well as Elman
ANN (EANN) models for seasonal data. Three widely popular versions of the basic
PSO algorithm, viz. Trelea-I, Trelea-II and Clerc-Type1 are considered here.
The empirical analysis is conducted on three real-world seasonal time series.
Results clearly show that each version of the PSO algorithm achieves notably
better forecasting accuracies than the standard Backpropagation (BP) training
method for both FANN and EANN models. The neural network forecasting results
are also compared with those from the three traditional statistical models,
viz. Seasonal Autoregressive Integrated Moving Average (SARIMA), Holt-Winters
(HW) and Support Vector Machine (SVM). The comparison demonstrates that both
PSO and BP based neural networks outperform SARIMA, HW and SVM models for all
three time series datasets. The forecasting performances of ANNs are further
improved through combining the outputs from the three PSO based models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6617</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6617</id><created>2013-02-26</created><authors><author><keyname>Hunter</keyname><forenames>Timothy</forenames></author><author><keyname>Hofleitner</keyname><forenames>Aude</forenames></author><author><keyname>Reilly</keyname><forenames>Jack</forenames></author><author><keyname>Krichene</keyname><forenames>Walid</forenames></author><author><keyname>Thai</keyname><forenames>Jerome</forenames></author><author><keyname>Kouvelas</keyname><forenames>Anastasios</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre</forenames></author></authors><title>Arriving on time: estimating travel time distributions on large-scale
  road networks</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most optimal routing problems focus on minimizing travel time or distance
traveled. Oftentimes, a more useful objective is to maximize the probability of
on-time arrival, which requires statistical distributions of travel times,
rather than just mean values. We propose a method to estimate travel time
distributions on large-scale road networks, using probe vehicle data collected
from GPS. We present a framework that works with large input of data, and
scales linearly with the size of the network. Leveraging the planar topology of
the graph, the method computes efficiently the time correlations between
neighboring streets. First, raw probe vehicle traces are compressed into pairs
of travel times and number of stops for each traversed road segment using a
`stop-and-go' algorithm developed for this work. The compressed data is then
used as input for training a path travel time model, which couples a Markov
model along with a Gaussian Markov random field. Finally, scalable inference
algorithms are developed for obtaining path travel time distributions from the
composite MM-GMRF model. We illustrate the accuracy and scalability of our
model on a 505,000 road link network spanning the San Francisco Bay Area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6634</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6634</id><created>2013-02-26</created><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Li</keyname><forenames>Wenzhi</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author></authors><title>A Matrix-Field Weighted Mean-Square-Error Model for MIMO Transceiver
  Designs</title><categories>cs.IT math.IT</categories><comments>11 pages, 1 figure. Signal Processing and Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we investigate an important and famous issue, namely weighted
mean-square-error (MSE) minimization transceiver designs. In our work, for
transceiver designs a novel weighted MSE model is proposed, which is defined as
a linear matrix function with respect to the traditional data detection MSE
matrix. The new model can be interpreted an extension of weighting operation
from vector field to matrix field. Based on the proposed weighting operation a
general transceiver design is proposed, which aims at minimizing an increasing
matrix-monotone function of the output of the previous linear matrix function.
The structure of the optimal solutions is also derived. Furthermore, two
important special cases of the matrix-monotone functions are discussed in
detail. It is also revealed that these two problems are exactly equivalent to
the transceiver designs of sum MSE minimization and capacity maximization for
dual-hop amplify-and-forward (AF) MIMO relaying systems, respectively. Finally,
it is concluded that the AF relaying is undoubtedly this kind of weighting
operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6636</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6636</id><created>2013-02-26</created><updated>2013-12-09</updated><authors><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Plantenga</keyname><forenames>Todd</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>A Scalable Generative Graph Model with Community Structure</title><categories>cs.SI physics.soc-ph</categories><journal-ref>SIAM Journal on Scientific Computing, Vol. 36, No. 5, pp.
  C424-C452, September 2014</journal-ref><doi>10.1137/130914218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network data is ubiquitous and growing, yet we lack realistic generative
network models that can be calibrated to match real-world data. The recently
proposed Block Two-Level Erdss-Renyi (BTER) model can be tuned to capture two
fundamental properties: degree distribution and clustering coefficients. The
latter is particularly important for reproducing graphs with community
structure, such as social networks. In this paper, we compare BTER to other
scalable models and show that it gives a better fit to real data. We provide a
scalable implementation that requires only O(d_max) storage where d_max is the
maximum number of neighbors for a single node. The generator is trivially
parallelizable, and we show results for a Hadoop MapReduce implementation for a
modeling a real-world web graph with over 4.6 billion edges. We propose that
the BTER model can be used as a graph generator for benchmarking purposes and
provide idealized degree distributions and clustering coefficient profiles that
can be tuned for user specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6641</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6641</id><created>2013-02-26</created><updated>2013-07-16</updated><authors><author><keyname>Iacono</keyname><forenames>John</forenames></author></authors><title>Why some heaps support constant-amortized-time decrease-key operations,
  and others do not</title><categories>cs.DS</categories><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lower bound is presented which shows that a class of heap algorithms in the
pointer model with only heap pointers must spend Omega(log log n / log log log
n) amortized time on the decrease-key operation (given O(log n) amortized-time
extract-min). Intuitively, this bound shows the key to having O(1)-time
decrease-key is the ability to sort O(log n) items in O(log n) time; Fibonacci
heaps [M.L. Fredman and R. E. Tarjan. J. ACM 34(3):596-615 (1987)] do this
through the use of bucket sort. Our lower bound also holds no matter how much
data is augmented; this is in contrast to the lower bound of Fredman [J. ACM
46(4):473-501 (1999)] who showed a tradeoff between the number of augmented
bits and the amortized cost of decrease-key. A new heap data structure, the
sort heap, is presented. This heap is a simplification of the heap of Elmasry
[SODA 2009: 471-476] and shares with it a O(log log n) amortized-time
decrease-key, but with a straightforward implementation such that our lower
bound holds. Thus a natural model is presented for a pointer-based heap such
that the amortized runtime of a self-adjusting structure and amortized lower
asymptotic bounds for decrease-key differ by but a O(log log log n) factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6646</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6646</id><created>2013-02-26</created><authors><author><keyname>Wei</keyname><forenames>Zhiqing</forenames></author><author><keyname>Zhang</keyname><forenames>Qixun</forenames></author><author><keyname>Feng</keyname><forenames>Zhiyong</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>On the Construction of Radio Environment Maps for Cognitive Radio
  Networks</title><categories>cs.NI</categories><comments>6 pages, 7 figures, IEEE WCNC conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Radio Environment Map (REM) provides an effective approach to Dynamic
Spectrum Access (DSA) in Cognitive Radio Networks (CRNs). Previous results on
REM construction show that there exists a tradeoff between the number of
measurements (sensors) and REM accuracy. In this paper, we analyze this
tradeoff and determine that the REM error is a decreasing and convex function
of the number of measurements (sensors). The concept of geographic entropy is
introduced to quantify this relationship. And the influence of sensor
deployment on REM accuracy is examined using information theory techniques. The
results obtained in this paper are applicable not only for the REM, but also
for wireless sensor network deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6653</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6653</id><created>2013-02-26</created><authors><author><keyname>Wagner</keyname><forenames>David P.</forenames></author></authors><title>The Unified Segment Tree and its Application to the Rectangle
  Intersection Problem</title><categories>cs.CG cs.DS</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a variation on the multidimensional segment tree,
formed by unifying different interpretations of the dimensionalities of the
data structure. We give some new definitions to previously well-defined
concepts that arise naturally in this variation, and we show some properties
concerning the relationships between the nodes, and the regions those nodes
represent. We think these properties will enable the data to be utilized in new
situations, beyond those previously studied. As an example, we show that the
data structure can be used to solve the Rectangle Intersection Problem in a
more straightforward and natural way than had be done in the past.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6660</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6660</id><created>2013-02-26</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>Optimal rate algebraic list decoding using narrow ray class fields</title><categories>math.NT cs.CC cs.IT math.IT</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use class field theory, specifically Drinfeld modules of rank 1, to
construct a family of asymptotically good algebraic-geometric (AG) codes over
fixed alphabets. Over a field of size $\ell^2$, these codes are within
$2/(\sqrt{\ell}-1)$ of the Singleton bound. The functions fields underlying
these codes are subfields with a cyclic Galois group of the narrow ray class
field of certain function fields. The resulting codes are &quot;folded&quot; using a
generator of the Galois group. This generalizes earlier work by the first
author on folded AG codes based on cyclotomic function fields. Using the
Chebotarev density theorem, we argue the abundance of inert places of large
degree in our cyclic extension, and use this to devise a linear-algebraic
algorithm to list decode these folded codes up to an error fraction approaching
$1-R$ where $R$ is the rate. The list decoding can be performed in polynomial
time given polynomial amount of pre-processed information about the function
field.
  Our construction yields algebraic codes over constant-sized alphabets that
can be list decoded up to the Singleton bound --- specifically, for any desired
rate $R \in (0,1)$ and constant $\eps &gt; 0$, we get codes over an alphabet size
$(1/\eps)^{O(1/\eps^2)}$ that can be list decoded up to error fraction
$1-R-\eps$ confining close-by messages to a subspace with $N^{O(1/\eps^2)}$
elements. Previous results for list decoding up to error-fraction $1-R-\eps$
over constant-sized alphabets were either based on concatenation or involved
taking a carefully sampled subcode of algebraic-geometric codes. In contrast,
our result shows that these folded algebraic-geometric codes {\em themselves}
have the claimed list decoding property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6666</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6666</id><created>2013-02-27</created><authors><author><keyname>Huang</keyname><forenames>Yan</forenames></author><author><keyname>Jin</keyname><forenames>Ruoming</forenames></author><author><keyname>Bastani</keyname><forenames>Favyen</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoyang Sean</forenames></author></authors><title>Large Scale Real-time Ridesharing with Service Guarantee on Road
  Networks</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The mean occupancy rates of personal vehicle trips in the United States is
only 1.6 persons per vehicle mile. Urban traffic gridlock is a familiar scene.
Ridesharing has the potential to solve many environmental, congestion, and
energy problems. In this paper, we introduce the problem of large scale
real-time ridesharing with service guarantee on road networks. Servers and trip
requests are dynamically matched while waiting time and service time
constraints of trips are satisfied. We first propose two basic algorithms: a
branch-and-bound algorithm and an integer programing algorithm. However, these
algorithm structures do not adapt well to the dynamic nature of the ridesharing
problem. Thus, we then propose a kinetic tree algorithm capable of better
scheduling dynamic requests and adjusting routes on-the-fly. We perform
experiments on a large real taxi dataset from Shanghai. The results show that
the kinetic tree algorithm is faster than other algorithms in response time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6667</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6667</id><created>2013-02-27</created><authors><author><keyname>Good</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Su</keyname><forenames>Andrew I.</forenames></author></authors><title>Crowdsourcing for Bioinformatics</title><categories>q-bio.QM cs.CY cs.SI physics.soc-ph</categories><comments>Review</comments><doi>10.1093/bioinformatics/btt333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Bioinformatics is faced with a variety of problems that require
human involvement. Tasks like genome annotation, image analysis, knowledge-base
construction and protein structure determination all benefit from human input.
In some cases people are needed in vast quantities while in others we need just
a few with very rare abilities. Crowdsourcing encompasses an emerging
collection of approaches for harnessing such distributed human intelligence.
Recently, the bioinformatics community has begun to apply crowdsourcing in a
variety of contexts, yet few resources are available that describe how these
human-powered systems work and how to use them effectively in scientific
domains. Results: Here, we provide a framework for understanding and applying
several different types of crowdsourcing. The framework considers two broad
classes: systems for solving large-volume 'microtasks' and systems for solving
high-difficulty 'megatasks'. Within these classes, we discuss system types
including: volunteer labor, games with a purpose, microtask markets and open
innovation contests. We illustrate each system type with successful examples in
bioinformatics and conclude with a guide for matching problems to crowdsourcing
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6668</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6668</id><created>2013-02-27</created><updated>2014-08-25</updated><authors><author><keyname>Hendrickx</keyname><forenames>Julien M.</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Finite-time consensus using stochastic matrices with positive diagonals</title><categories>cs.MA cs.SY</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the possibility of reaching consensus in finite time using only
linear iterations, with the additional restrictions that the update matrices
must be stochastic with positive diagonals and consistent with a given graph
structure. We show that finite-time average consensus can always be achieved
for connected undirected graphs. For directed graphs, we show some necessary
conditions for finite-time consensus, including strong connectivity and the
presence of a simple cycle of even length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6677</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6677</id><created>2013-02-27</created><authors><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>Taming the Curse of Dimensionality: Discrete Integration by Hashing and
  Optimization</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integration is affected by the curse of dimensionality and quickly becomes
intractable as the dimensionality of the problem grows. We propose a randomized
algorithm that, with high probability, gives a constant-factor approximation of
a general discrete integral defined over an exponentially large set. This
algorithm relies on solving only a small number of instances of a discrete
combinatorial optimization problem subject to randomly generated parity
constraints used as a hash function. As an application, we demonstrate that
with a small number of MAP queries we can efficiently approximate the partition
function of discrete graphical models, which can in turn be used, for instance,
for marginal computation or model selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6683</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6683</id><created>2013-02-27</created><authors><author><keyname>Bajcinca</keyname><forenames>Naim</forenames></author><author><keyname>Kouhi</keyname><forenames>Yashar</forenames></author><author><keyname>Nenchev</keyname><forenames>Vladislav</forenames></author><author><keyname>Raisch</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Decentralized set-valued state estimation based on non-deterministic
  chains</title><categories>cs.SY</categories><doi>10.1109/ICAT.2011.6102092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general decentralized computational framework for set-valued state
estimation and prediction for the class of systems that accept a hybrid state
machine representation is considered in this article. The decentralized scheme
consists of a conjunction of distributed state machines that are specified by a
decomposition of the external signal space. While this is shown to produce, in
general, outer approximations of the outcomes of the original monolithic state
machine, here, specific rules for the signal space decomposition are devised by
utilizing structural properties of the underyling transition relation, leading
to a recovery of the exact state set results. By applying a suitable
approximation algorithm, we show that computational complexity in the
decentralized setting may thereby essentially reduce as compared to the
centralized estimation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6695</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6695</id><created>2013-02-27</created><authors><author><keyname>Manin</keyname><forenames>Yuri I.</forenames></author></authors><title>Complexity vs Energy: Theory of Computation and Theoretical Physics</title><categories>cs.CC cond-mat.stat-mech math-ph math.MP</categories><comments>23 pages. Talk at the satellite conference to ECM 2012, &quot;QQQ Algebra,
  Geometry, Information&quot;, Tallinn, July 9-12, 2012</comments><msc-class>68Q05, 68Q12, 68Q25</msc-class><doi>10.1088/1742-6596/532/1/012018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey dedicated to the analogy between the notions of {\it
complexity} in theoretical computer science and {\it energy} in physics. This
analogy is not metaphorical: I describe three precise mathematical contexts,
suggested recently, in which mathematics related to (un)computability is
inspired by and to a degree reproduces formalisms of statistical physics and
quantum field theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6700</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6700</id><created>2013-02-27</created><authors><author><keyname>Sundararajan</keyname><forenames>Mukund</forenames></author><author><keyname>Talgam-Cohen</keyname><forenames>Inbal</forenames></author></authors><title>Refine Predictions Ad Infinitum?</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how standard auction objectives in sponsored search markets change
with refinements in the prediction of the relevance (click-through rates) of
ads. We study mechanisms that optimize for a convex combination of efficiency
and revenue. We show that the objective function of such a mechanism can only
improve with refined (improved) relevance predictions, i.e., the search engine
has no disincentive to perform these refinements. More interestingly, we show
that under assumptions, refinements to relevance predictions can only improve
the efficiency of any such mechanism. Our main technical contribution is to
study how relevance refinements affect the similarity between ranking by
virtual-value (revenue ranking) and ranking by value (efficiency ranking).
Finally, we discuss implications of our results to the literature on signaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6703</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6703</id><created>2013-02-27</created><authors><author><keyname>Fyhn</keyname><forenames>Karsten</forenames></author><author><keyname>Jensen</keyname><forenames>Tobias Lindstr&#xf8;m</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author><author><keyname>Jensen</keyname><forenames>S&#xf8;ren Holdt</forenames></author></authors><title>Compressive Sensing for Spread Spectrum Receivers</title><categories>cs.IT math.IT</categories><comments>11 pages, 11 figures, 1 table, accepted for publication in IEEE
  Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of ubiquitous computing there are two design parameters of
wireless communication devices that become very important power: efficiency and
production cost. Compressive sensing enables the receiver in such devices to
sample below the Shannon-Nyquist sampling rate, which may lead to a decrease in
the two design parameters. This paper investigates the use of Compressive
Sensing (CS) in a general Code Division Multiple Access (CDMA) receiver. We
show that when using spread spectrum codes in the signal domain, the CS
measurement matrix may be simplified. This measurement scheme, named
Compressive Spread Spectrum (CSS), allows for a simple, effective receiver
design. Furthermore, we numerically evaluate the proposed receiver in terms of
bit error rate under different signal to noise ratio conditions and compare it
with other receiver structures. These numerical experiments show that though
the bit error rate performance is degraded by the subsampling in the CS-enabled
receivers, this may be remedied by including quantization in the receiver
model. We also study the computational complexity of the proposed receiver
design under different sparsity and measurement ratios. Our work shows that it
is possible to subsample a CDMA signal using CSS and that in one example the
CSS receiver outperforms the classical receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6704</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6704</id><created>2013-02-27</created><authors><author><keyname>Bajcinca</keyname><forenames>Naim</forenames></author></authors><title>Decentralized set-valued state estimation and prediction for hybrid
  systems: A symbolic approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A symbolic approach to decentralized set-valued state estimation and
prediction for systems that admit a hybrid state machine representations is
proposed. The decentralized computational scheme represents a conj unction of a
finite number of distributed state machines, which are specified by an
appropriate decomposition of the external signal space. It aims at a
distribution of computational tasks into smaller ones, allocated to individual
distributed state machines, leading to a potentially significant reduction in
the overall space/time computational complexity. We show that, in general, such
a scheme outerapproximates the state set estimates and predictions of the
original monolithic state machine. By utilizing structural properties of the
transition relation of the latter, in a next step, we propose constructive
decomposition algorithms for a recovery of the exact state set outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6738</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6738</id><created>2013-02-27</created><authors><author><keyname>Weihua</keyname><forenames>Zhan</forenames></author><author><keyname>Huahui</keyname><forenames>Chen</forenames></author><author><keyname>Jihong</keyname><forenames>Guan</forenames></author><author><keyname>Guang</keyname><forenames>Jin</forenames></author></authors><title>Finding overlapping communities in networks using evolutionary method</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure is a typical property of many real-world networks, and
has become a key to understand the dynamics of the networked systems. In these
networks most nodes apparently lie in a community while there often exists a
few nodes straddling several communities. An ideal algorithm for community
detection is preferable which can identify the overlapping communities in such
networks. To represent an overlapping division we develop a encoding schema
composed of two segments, the first one represents a disjoint partition and the
second one represents a extension of the partition that allows of multiple
memberships. We give a measure for the informativeness of a node, and present
an evolutionary method for detecting the overlapping communities in a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6764</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6764</id><created>2013-02-27</created><updated>2013-02-28</updated><authors><author><keyname>Zanetti</keyname><forenames>Marcelo Serrano</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Categorizing Bugs with Social Networks: A Case Study on Four Open Source
  Software Communities</title><categories>cs.SE cs.LG cs.SI nlin.AO physics.soc-ph</categories><comments>preprint of conference proceedings of the 35th International
  Conference on Software Engineering (ICSE 2013) - Software Engineering in
  Practice (SEIP) Track</comments><acm-class>D.2.8; K.4.3; H.1.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient bug triaging procedures are an important precondition for
successful collaborative software engineering projects. Triaging bugs can
become a laborious task particularly in open source software (OSS) projects
with a large base of comparably inexperienced part-time contributors. In this
paper, we propose an efficient and practical method to identify valid bug
reports which a) refer to an actual software bug, b) are not duplicates and c)
contain enough information to be processed right away. Our classification is
based on nine measures to quantify the social embeddedness of bug reporters in
the collaboration network. We demonstrate its applicability in a case study,
using a comprehensive data set of more than 700,000 bug reports obtained from
the Bugzilla installation of four major OSS communities, for a period of more
than ten years. For those projects that exhibit the lowest fraction of valid
bug reports, we find that the bug reporters' position in the collaboration
network is a strong indicator for the quality of bug reports. Based on this
finding, we develop an automated classification scheme that can easily be
integrated into bug tracking platforms and analyze its performance in the
considered OSS communities. A support vector machine (SVM) to identify valid
bug reports based on the nine measures yields a precision of up to 90.3% with
an associated recall of 38.9%. With this, we significantly improve the results
obtained in previous case studies for an automated early identification of bugs
that are eventually fixed. Furthermore, our study highlights the potential of
using quantitative measures of social organization in collaborative software
engineering. It also opens a broad perspective for the integration of social
awareness in the design of support infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6768</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6768</id><created>2013-02-27</created><updated>2014-06-29</updated><authors><author><keyname>Shabat</keyname><forenames>Gil</forenames></author><author><keyname>Shmueli</keyname><forenames>Yaniv</forenames></author><author><keyname>Averbuch</keyname><forenames>Amir</forenames></author></authors><title>Missing Entries Matrix Approximation and Completion</title><categories>math.NA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe several algorithms for matrix completion and matrix approximation
when only some of its entries are known. The approximation constraint can be
any whose approximated solution is known for the full matrix. For low rank
approximations, similar algorithms appears recently in the literature under
different names. In this work, we introduce new theorems for matrix
approximation and show that these algorithms can be extended to handle
different constraints such as nuclear norm, spectral norm, orthogonality
constraints and more that are different than low rank approximations. As the
algorithms can be viewed from an optimization point of view, we discuss their
convergence to global solution for the convex case. We also discuss the optimal
step size and show that it is fixed in each iteration. In addition, the derived
matrix completion flow is robust and does not require any parameters. This
matrix completion flow is applicable to different spectral minimizations and
can be applied to physics, mathematics and electrical engineering problems such
as data reconstruction of images and data coming from PDEs such as Helmholtz
equation used for electromagnetic waves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6770</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6770</id><created>2013-02-27</created><updated>2013-04-04</updated><authors><author><keyname>Benzi</keyname><forenames>Michele</forenames></author><author><keyname>Klymko</keyname><forenames>Christine</forenames></author></authors><title>Total communicability as a centrality measure</title><categories>cs.SI math.NA physics.soc-ph</categories><comments>27 pages, 6 figures</comments><msc-class>05C50, 15A16, 65F60, 90B10, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a node centrality measure based on the notion of total
communicability, defined in terms of the row sums of the exponential of the
adjacency matrix of the network. We argue that this is a natural metric for
ranking nodes in a network, and we point out that it can be computed very
rapidly even in the case of large networks. Furthermore, we propose the total
sum of node communicabilities as a useful measure of network connectivity.
Extensive numerical studies are conducted in order to compare this centrality
measure with the closely related ones of subgraph centrality [E. Estrada and J.
A. Rodriguez-Velazquez, Phys. Rev. E, 71 (2005), 056103] and Katz centrality
[L. Katz, Psychometrica, 18 (1953), pp. 39-43]. Both synthetic and real-world
networks are used in the computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6777</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6777</id><created>2013-02-27</created><authors><author><keyname>Adams</keyname><forenames>Greg</forenames></author><author><keyname>Millar</keyname><forenames>Beth</forenames></author><author><keyname>Neufeld</keyname><forenames>Eric</forenames></author><author><keyname>Philip</keyname><forenames>Tim</forenames></author></authors><title>Ending-based Strategies for Part-of-speech Tagging</title><categories>cs.CL</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-1-7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic approaches to part-of-speech tagging rely primarily on
whole-word statistics about word/tag combinations as well as contextual
information. But experience shows about 4 per cent of tokens encountered in
test sets are unknown even when the training set is as large as a million
words. Unseen words are tagged using secondary strategies that exploit word
features such as endings, capitalizations and punctuation marks. In this work,
word-ending statistics are primary and whole-word statistics are secondary.
First, a tagger was trained and tested on word endings only. Subsequent
experiments added back whole-word statistics for the words occurring most
frequently in the training set. As grew larger, performance was expected to
improve, in the limit performing the same as word-based taggers. Surprisingly,
the ending-based tagger initially performed nearly as well as the word-based
tagger; in the best case, its performance significantly exceeded that of the
word-based tagger. Lastly, and unexpectedly, an effect of negative returns was
observed - as grew larger, performance generally improved and then declined. By
varying factors such as ending length and tag-list strategy, we achieved a
success rate of 97.5 percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6779</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6779</id><created>2013-02-27</created><authors><author><keyname>Aliferis</keyname><forenames>Constantin F.</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>An Evaluation of an Algorithm for Inductive Learning of Bayesian Belief
  Networks Usin</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-8-14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian learning of belief networks (BLN) is a method for automatically
constructing belief networks (BNs) from data using search and Bayesian scoring
techniques. K2 is a particular instantiation of the method that implements a
greedy search strategy. To evaluate the accuracy of K2, we randomly generated a
number of BNs and for each of those we simulated data sets. K2 was then used to
induce the generating BNs from the simulated data. We examine the performance
of the program, and the factors that influence it. We also present a simple BN
model, developed from our results, which predicts the accuracy of K2, when
given various characteristics of the data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6780</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6780</id><created>2013-02-27</created><authors><author><keyname>Altman</keyname><forenames>Russ B.</forenames></author><author><keyname>Chen</keyname><forenames>Cheng C.</forenames></author><author><keyname>Poland</keyname><forenames>William B.</forenames></author><author><keyname>Singh</keyname><forenames>Jaswinder Pal</forenames></author></authors><title>Probabilistic Constraint Satisfaction with Non-Gaussian Noise</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-15-22</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have previously reported a Bayesian algorithm for determining the
coordinates of points in three-dimensional space from uncertain constraints.
This method is useful in the determination of biological molecular structure.
It is limited, however, by the requirement that the uncertainty in the
constraints be normally distributed. In this paper, we present an extension of
the original algorithm that allows constraint uncertainty to be represented as
a mixture of Gaussians, and thereby allows arbitrary constraint distributions.
We illustrate the performance of this algorithm on a problem drawn from the
domain of molecular structure determination, in which a multicomponent
constraint representation produces a much more accurate solution than the old
single component mechanism. The new mechanism uses mixture distributions to
decompose the problem into a set of independent problems with unimodal
constraint uncertainty. The results of the unimodal subproblems are
periodically recombined using Bayes' law, to avoid combinatorial explosion. The
new algorithm is particularly suited for parallel implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6781</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6781</id><created>2013-02-27</created><authors><author><keyname>Ayers</keyname><forenames>Derek D.</forenames></author></authors><title>A Bayesian Method Reexamined</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-23-27</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the &quot;K2&quot; network scoring metric of Cooper and Herskovits.
It shows counterintuitive results from applying this metric to simple networks.
One family of noninformative priors is suggested for assigning equal scores to
equivalent networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6782</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6782</id><created>2013-02-27</created><authors><author><keyname>Azevedo-Filho</keyname><forenames>Adriano</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Laplace's Method Approximations for Probabilistic Inference in Belief
  Networks with Continuous Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-28-36</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laplace's method, a family of asymptotic methods used to approximate
integrals, is presented as a potential candidate for the tool box of techniques
used for knowledge acquisition and probabilistic inference in belief networks
with continuous variables. This technique approximates posterior moments and
marginal posterior distributions with reasonable accuracy [errors are O(n^-2)
for posterior means] in many interesting cases. The method also seems promising
for computing approximations for Bayes factors for use in the context of model
selection, model uncertainty and mixtures of pdfs. The limitations, regularity
conditions and computational difficulties for the implementation of Laplace's
method are comparable to those associated with the methods of maximum
likelihood and posterior mode analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6783</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6783</id><created>2013-02-27</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Grove</keyname><forenames>Adam J.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Generating New Beliefs From Old</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-37-45</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work [BGHK92, BGHK93], we have studied the random-worlds approach
-- a particular (and quite powerful) method for generating degrees of belief
(i.e., subjective probabilities) from a knowledge base consisting of objective
(first-order, statistical, and default) information. But allowing a knowledge
base to contain only objective information is sometimes limiting. We
occasionally wish to include information about degrees of belief in the
knowledge base as well, because there are contexts in which old beliefs
represent important information that should influence new beliefs. In this
paper, we describe three quite general techniques for extending a method that
generates degrees of belief from objective information to one that can make use
of degrees of belief as well. All of our techniques are bloused on well-known
approaches, such as cross-entropy. We discuss general connections between the
techniques and in particular show that, although conceptually and technically
quite different, all of the techniques give the same answer when applied to the
random-worlds method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6784</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6784</id><created>2013-02-27</created><authors><author><keyname>Balke</keyname><forenames>Alexander</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Counterfactual Probabilities: Computational Methods, Bounds and
  Applications</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-46-54</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of counterfactual queries (e.g., &quot;If A were true, would C have
been true?&quot;) is important to fault diagnosis, planning, and determination of
liability. In this paper we present methods for computing the probabilities of
such queries using the formulation proposed in [Balke and Pearl, 1994], where
the antecedent of the query is interpreted as an external action that forces
the proposition A to be true. When a prior probability is available on the
causal mechanisms governing the domain, counterfactual probabilities can be
evaluated precisely. However, when causal knowledge is specified as conditional
probabilities on the observables, only bounds can computed. This paper develops
techniques for evaluating these bounds, and demonstrates their use in two
applications: (1) the determination of treatment efficacy from studies in which
subjects may choose their own treatment, and (2) the determination of liability
in product-safety litigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6786</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6786</id><created>2013-02-27</created><authors><author><keyname>Batyrshin</keyname><forenames>Ildar Z.</forenames></author></authors><title>Modus Ponens Generating Function in the Class of ^-valuations of
  Plausibility</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-55-59</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the problem of construction of inference procedures which can
manipulate with uncertainties measured in ordinal scales and fulfill to the
property of strict monotonicity of conclusion. The class of A-valuations of
plausibility is considered where operations based only on information about
linear ordering of plausibility values are used. In this class the modus ponens
generating function fulfiling to the property of strict monotonicity of
conclusions is introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6787</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6787</id><created>2013-02-27</created><authors><author><keyname>Becker</keyname><forenames>Ann</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author></authors><title>Approximation Algorithms for the Loop Cutset Problem</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-60-68</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to find a small loop curser in a Bayesian network. Finding such a
loop cutset is the first step in the method of conditioning for inference. Our
algorithm for finding a loop cutset, called MGA, finds a loop cutset which is
guaranteed in the worst case to contain less than twice the number of variables
contained in a minimum loop cutset. We test MGA on randomly generated graphs
and find that the average ratio between the number of instances associated with
the algorithms' output and the number of instances associated with a minimum
solution is 1.22.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6788</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6788</id><created>2013-02-27</created><authors><author><keyname>Besnard</keyname><forenames>Philippe</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author></authors><title>Possibility and Necessity Functions over Non-classical Logics</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-69-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an integration of possibility theory into non-classical logics. We
obtain many formal results that generalize the case where possibility and
necessity functions are based on classical logic. We show how useful such an
approach is by applying it to reasoning under uncertain and inconsistent
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6789</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6789</id><created>2013-02-27</created><authors><author><keyname>Bhatnagar</keyname><forenames>Raj</forenames></author></authors><title>Exploratory Model Building</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-77-85</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some instances of creative thinking require an agent to build and test
hypothetical theories. Such a reasoner needs to explore the space of not only
those situations that have occurred in the past, but also those that are
rationally conceivable. In this paper we present a formalism for exploring the
space of conceivable situation-models for those domains in which the knowledge
is primarily probabilistic in nature. The formalism seeks to construct
consistent, minimal, and desirable situation-descriptions by selecting suitable
domain-attributes and dependency relationships from the available domain
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6790</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6790</id><created>2013-02-27</created><authors><author><keyname>Billard</keyname><forenames>Edward A.</forenames></author></authors><title>Learning in Multi-level Stochastic games with Delayed Information</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-86-93</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed decision-makers are modeled as players in a game with two levels.
High level decisions concern the game environment and determine the willingness
of the players to form a coalition (or group). Low level decisions involve the
actions to be implemented within the chosen environment. Coalition and action
strategies are determined by probability distributions, which are updated using
learning automata schemes. The payoffs are also probabilistic and there is
uncertainty in the state vector since information is delayed. The goal is to
reach equilibrium in both levels of decision making; the results show the
conditions for instability, based on the age of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6791</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6791</id><created>2013-02-27</created><authors><author><keyname>Blythe</keyname><forenames>Jim S.</forenames></author></authors><title>Planning with External Events</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-94-101</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I describe a planning methodology for domains with uncertainty in the form of
external events that are not completely predictable. The events are represented
by enabling conditions and probabilities of occurrence. The planner is
goal-directed and backward chaining, but the subgoals are suggested by
analyzing the probability of success of the partial plan rather than being
simply the open conditions of the operators in the plan. The partial plan is
represented as a Bayesian belief net to compute its probability of success.
Since calculating the probability of success of a plan can be very expensive I
introduce two other techniques for computing it, one that uses Monte Carlo
simulation to estimate it and one based on a Markov chain representation that
uses knowledge about the dependencies between the predicates describing the
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6792</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6792</id><created>2013-02-27</created><authors><author><keyname>Bouckaert</keyname><forenames>Remco R.</forenames></author></authors><title>Properties of Bayesian Belief Network Learning Algorithms</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-102-109</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian belief network learning algorithms have three basic components: a
measure of a network structure and a database, a search heuristic that chooses
network structures to be considered, and a method of estimating the probability
tables from the database. This paper contributes to all these three topics. The
behavior of the Bayesian measure of Cooper and Herskovits and a minimum
description length (MDL) measure are compared with respect to their properties
for both limiting size and finite size databases. It is shown that the MDL
measure has more desirable properties than the Bayesian measure when a
distribution is to be learned. It is shown that selecting belief networks with
certain minimallity properties is NP-hard. This result justifies the use of
search heuristics instead of exact algorithms for choosing network structures
to be considered. In some cases, a collection of belief networks can be
represented by a single belief network which leads to a new kind of probability
table estimation called smoothing. We argue that smoothing can be efficiently
implemented by incorporating it in the search heuristic. Experimental results
suggest that for learning probabilities of belief networks smoothing is
helpful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6793</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6793</id><created>2013-02-27</created><authors><author><keyname>Bouckaert</keyname><forenames>Remco R.</forenames></author></authors><title>A Stratified Simulation Scheme for Inference in Bayesian Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-110-118</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation schemes for probabilistic inference in Bayesian belief networks
offer many advantages over exact algorithms; for example, these schemes have a
linear and thus predictable runtime while exact algorithms have exponential
runtime. Experiments have shown that likelihood weighting is one of the most
promising simulation schemes. In this paper, we present a new simulation scheme
that generates samples more evenly spread in the sample space than the
likelihood weighting scheme. We show both theoretically and experimentally that
the stratified scheme outperforms likelihood weighting in average runtime and
error in estimates of beliefs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6794</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6794</id><created>2013-02-27</created><authors><author><keyname>Chavez</keyname><forenames>Tom</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>Efficient Estimation of the Value of Information in Monte Carlo Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-119-127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expected value of information (EVI) is the most powerful measure of
sensitivity to uncertainty in a decision model: it measures the potential of
information to improve the decision, and hence measures the expected value of
outcome. Standard methods for computing EVI use discrete variables and are
computationally intractable for models that contain more than a few variables.
Monte Carlo simulation provides the basis for more tractable evaluation of
large predictive models with continuous and discrete variables, but so far
computation of EVI in a Monte Carlo setting also has appeared impractical. We
introduce an approximate approach based on pre-posterior analysis for
estimating EVI in Monte Carlo models. Our method uses a linear approximation to
the value function and multiple linear regression to estimate the linear model
from the samples. The approach is efficient and practical for extremely large
models. It allows easy estimation of EVI for perfect or partial information on
individual variables or on combinations of variables. We illustrate its
implementation within Demos (a decision modeling system), and its application
to a large model for crisis transportation planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6795</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6795</id><created>2013-02-27</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>Symbolic Probabilitistic Inference in Large BN2O Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-128-135</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A BN2O network is a two level belief net in which the parent interactions are
modeled using the noisy-or interaction model. In this paper we discuss
application of the SPI local expression language to efficient inference in
large BN2O networks. In particular, we show that there is significant
structure, which can be exploited to improve over the Quickscore result. We
further describe how symbolic techniques can provide information which can
significantly reduce the computation required for computing all cause posterior
marginals. Finally, we present a novel approximation technique with preliminary
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6796</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6796</id><created>2013-02-27</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author></authors><title>Action Networks: A Framework for Reasoning about Actions and Change
  under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-136-144</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes action networks as a semantically well-founded framework
for reasoning about actions and change under uncertainty. Action networks add
two primitives to probabilistic causal networks: controllable variables and
persistent variables. Controllable variables allow the representation of
actions as directly setting the value of specific events in the domain, subject
to preconditions. Persistent variables provide a canonical model of persistence
according to which both the state of a variable and the causal mechanism
dictating its value persist over time unless intervened upon by an action (or
its consequences). Action networks also allow different methods for quantifying
the uncertainty in causal relationships, which go beyond traditional
probabilistic quantification. This paper describes both recent results and work
in progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6797</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6797</id><created>2013-02-27</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author></authors><title>On the Relation between Kappa Calculus and Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-145-153</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the connection between kappa calculus and probabilistic reasoning in
diagnosis applications. Specifically, we abstract a probabilistic belief
network for diagnosing faults into a kappa network and compare the ordering of
faults computed using both methods. We show that, at least for the example
examined, the ordering of faults coincide as long as all the causal relations
in the original probabilistic network are taken into account. We also provide a
formal analysis of some network structures where the two methods will differ.
Both kappa rankings and infinitesimal probabilities have been used extensively
to study default reasoning and belief revision. But little has been done on
utilizing their connection as outlined above. This is partly because the
relation between kappa and probability calculi assumes that probabilities are
arbitrarily close to one (or zero). The experiments in this paper investigate
this relation when this assumption is not satisfied. The reported results have
important implications on the use of kappa rankings to enhance the knowledge
engineering of uncertainty models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6798</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6798</id><created>2013-02-27</created><authors><author><keyname>Davidson</keyname><forenames>Ron</forenames></author><author><keyname>Fehling</keyname><forenames>Michael R.</forenames></author></authors><title>A Structured, Probabilistic Representation of Action</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-154-161</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When agents devise plans for execution in the real world, they face two
important forms of uncertainty: they can never have complete knowledge about
the state of the world, and they do not have complete control, as the effects
of their actions are uncertain. While most classical planning methods avoid
explicit uncertainty reasoning, we believe that uncertainty should be
explicitly represented and reasoned about. We develop a probabilistic
representation for states and actions, based on belief networks. We define
conditional belief nets (CBNs) to capture the probabilistic dependency of the
effects of an action upon the state of the world. We also use a CBN to
represent the intrinsic relationships among entities in the environment, which
persist from state to state. We present a simple projection algorithm to
construct the belief network of the state succeeding an action, using the
environment CBN model to infer indirect effects. We discuss how the qualitative
aspects of belief networks and CBNs make them appropriate for the various
stages of the problem solving process, from model construction to the design of
planning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6799</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6799</id><created>2013-02-27</created><authors><author><keyname>Dearden</keyname><forenames>Richard</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Integrating Planning and Execution in Stochastic Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-162-169</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate planning in time-critical domains represented as Markov
Decision Processes, showing that search based techniques can be a very powerful
method for finding close to optimal plans. To reduce the computational cost of
planning in these domains, we execute actions as we construct the plan, and
sacrifice optimality by searching to a fixed depth and using a heuristic
function to estimate the value of states. Although this paper concentrates on
the search algorithm, we also discuss ways of constructing heuristic functions
suitable for this approach. Our results show that by interleaving search and
execution, close to optimal policies can be found without the computational
requirements of other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6800</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6800</id><created>2013-02-27</created><authors><author><keyname>Draper</keyname><forenames>Denise L.</forenames></author><author><keyname>Hanks</keyname><forenames>Steve</forenames></author></authors><title>Localized Partial Evaluation of Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-170-177</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most algorithms for propagating evidence through belief networks have been
exact and exhaustive: they produce an exact (point-valued) marginal probability
for every node in the network. Often, however, an application will not need
information about every n ode in the network nor will it need exact
probabilities. We present the localized partial evaluation (LPE) propagation
algorithm, which computes interval bounds on the marginal probability of a
specified query node by examining a subset of the nodes in the entire network.
Conceptually, LPE ignores parts of the network that are &quot;too far away&quot; from the
queried node to have much impact on its value. LPE has the &quot;anytime&quot; property
of being able to produce better solutions (tighter intervals) given more time
to consider more of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6801</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6801</id><created>2013-02-27</created><authors><author><keyname>Draper</keyname><forenames>Denise L.</forenames></author><author><keyname>Hanks</keyname><forenames>Steve</forenames></author><author><keyname>Weld</keyname><forenames>Daniel</forenames></author></authors><title>A Probabilistic Model of Action for Least-Commitment Planning with
  Information Gather</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-178-186</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI planning algorithms have addressed the problem of generating sequences of
operators that achieve some input goal, usually assuming that the planning
agent has perfect control over and information about the world. Relaxing these
assumptions requires an extension to the action representation that allows
reasoning both about the changes an action makes and the information it
provides. This paper presents an action representation that extends the
deterministic STRIPS model, allowing actions to have both causal and
informational effects, both of which can be context dependent and noisy. We
also demonstrate how a standard least-commitment planning algorithm can be
extended to include informational actions and contingent execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6802</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6802</id><created>2013-02-27</created><authors><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>Some Properties of Joint Probability Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-187-194</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several Artificial Intelligence schemes for reasoning under uncertainty
explore either explicitly or implicitly asymmetries among probabilities of
various states of their uncertain domain models. Even though the correct
working of these schemes is practically contingent upon the existence of a
small number of probable states, no formal justification has been proposed of
why this should be the case. This paper attempts to fill this apparent gap by
studying asymmetries among probabilities of various states of uncertain models.
By rewriting the joint probability distribution over a model's variables into a
product of individual variables' prior and conditional probability
distributions, and applying central limit theorem to this product, we can
demonstrate that the probabilities of individual states of the model can be
expected to be drawn from highly skewed, log-normal distributions. With
sufficient asymmetry in individual prior and conditional probability
distributions, a small fraction of states can be expected to cover a large
portion of the total probability space with the remaining states having
practically negligible probability. Theoretical discussion is supplemented by
simulation results and an illustrative real-world example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6803</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6803</id><created>2013-02-27</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>del Cerro</keyname><forenames>Luis Farinas</forenames></author><author><keyname>Herzig</keyname><forenames>Andreas</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>An Ordinal View of Independence with Application to Plausible Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-195-203</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ordinal view of independence is studied in the framework of possibility
theory. We investigate three possible definitions of dependence, of increasing
strength. One of them is the counterpart to the multiplication law in
probability theory, and the two others are based on the notion of conditional
possibility. These two have enough expressive power to support the whole
possibility theory, and a complete axiomatization is provided for the strongest
one. Moreover we show that weak independence is well-suited to the problems of
belief change and plausible reasoning, especially to address the problem of
blocking of property inheritance in exception-tolerant taxonomic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6804</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6804</id><created>2013-02-27</created><authors><author><keyname>de Saint-Cyr</keyname><forenames>Florence Dupin</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author></authors><title>Penalty logic and its Link with Dempster-Shafer Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-204-211</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Penalty logic, introduced by Pinkas, associates to each formula of a
knowledge base the price to pay if this formula is violated. Penalties may be
used as a criterion for selecting preferred consistent subsets in an
inconsistent knowledge base, thus inducing a non-monotonic inference relation.
A precise formalization and the main properties of penalty logic and of its
associated non-monotonic inference relation are given in the first part. We
also show that penalty logic and Dempster-Shafer theory are related, especially
in the infinitesimal case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6805</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6805</id><created>2013-02-27</created><authors><author><keyname>Ezawa</keyname><forenames>Kazuo J.</forenames></author></authors><title>Value of Evidence on Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-212-220</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce evidence propagation operations on influence
diagrams and a concept of value of evidence, which measures the value of
experimentation. Evidence propagation operations are critical for the
computation of the value of evidence, general update and inference operations
in normative expert systems which are based on the influence diagram
(generalized Bayesian network) paradigm. The value of evidence allows us to
compute directly an outcome sensitivity, a value of perfect information and a
value of control which are used in decision analysis (the science of decision
making under uncertainty). More specifically, the outcome sensitivity is the
maximum difference among the values of evidence, the value of perfect
information is the expected value of the values of evidence, and the value of
control is the optimal value of the values of evidence. We also discuss an
implementation and a relative computational efficiency issues related to the
value of evidence and the value of perfect information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6806</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6806</id><created>2013-02-27</created><authors><author><keyname>Fonck</keyname><forenames>Pascale</forenames></author></authors><title>Conditional Independence in Possibility Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-221-226</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic conditional independence is investigated: we propose a
definition of this notion similar to the one used in probability theory. The
links between independence and non-interactivity are investigated, and
properties of these relations are given. The influence of the conjunction used
to define a conditional measure of possibility is also highlighted: we examine
three types of conjunctions: Lukasiewicz - like T-norms, product-like T-norms
and the minimum operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6807</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6807</id><created>2013-02-27</created><authors><author><keyname>Fung</keyname><forenames>Robert</forenames></author><author><keyname>del Favero</keyname><forenames>Brendan</forenames></author></authors><title>Backward Simulation in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-227-234</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backward simulation is an approximate inference technique for Bayesian belief
networks. It differs from existing simulation methods in that it starts
simulation from the known evidence and works backward (i.e., contrary to the
direction of the arcs). The technique's focus on the evidence leads to improved
convergence in situations where the posterior beliefs are dominated by the
evidence rather than by the prior probabilities. Since this class of situations
is large, the technique may make practical the application of approximate
inference in Bayesian belief networks to many real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6808</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6808</id><created>2013-02-27</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Learning Gaussian Networks</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-235-243</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe algorithms for learning Bayesian networks from a combination of
user knowledge and statistical data. The algorithms have two components: a
scoring metric and a search procedure. The scoring metric takes a network
structure, statistical data, and a user's prior knowledge, and returns a score
proportional to the posterior probability of the network structure given the
data. The search procedure generates networks for evaluation by the scoring
metric. Previous work has concentrated on metrics for domains containing only
discrete variables, under the assumption that data represents a multinomial
sample. In this paper, we extend this work, developing scoring metrics for
domains containing all continuous variables or a mixture of discrete and
continuous variables, under the assumption that continuous data is sampled from
a multivariate normal distribution. Our work extends traditional statistical
approaches for identifying vanishing regression coefficients in that we
identify two important assumptions, called event equivalence and parameter
modularity, that when combined allow the construction of prior distributions
for multivariate normal parameters from a single prior Bayesian network
specified by a user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6809</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6809</id><created>2013-02-27</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Paz</keyname><forenames>Azaria</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On Testing Whether an Embedded Bayesian Network Represents a Probability
  Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-244-252</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing the validity of probabilistic models containing unmeasured (hidden)
variables is shown to be a hard task. We show that the task of testing whether
models are structurally incompatible with the data at hand, requires an
exponential number of independence evaluations, each of the form: &quot;X is
conditionally independent of Y, given Z.&quot; In contrast, a linear number of such
evaluations is required to test a standard Bayesian network (one per vertex).
On the positive side, we show that if a network with hidden variables G has a
tree skeleton, checking whether G represents a given probability model P
requires the polynomial number of such independence evaluations. Moreover, we
provide an algorithm that efficiently constructs a tree-structured Bayesian
network (with hidden variables) that represents P if such a network exists, and
further recognizes when such a network does not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6810</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6810</id><created>2013-02-27</created><authors><author><keyname>Goldman</keyname><forenames>Robert P.</forenames></author><author><keyname>Boddy</keyname><forenames>Mark S.</forenames></author></authors><title>Epsilon-Safe Planning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-253-261</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an approach to high-level conditional planning we call
epsilon-safe planning. This probabilistic approach commits us to planning to
meet some specified goal with a probability of success of at least 1-epsilon
for some user-supplied epsilon. We describe several algorithms for epsilon-safe
planning based on conditional planners. The two conditional planners we discuss
are Peot and Smith's nonlinear conditional planner, CNLP, and our own linear
conditional planner, PLINTH. We present a straightforward extension to
conditional planners for which computing the necessary probabilities is simple,
employing a commonly-made but perhaps overly-strong independence assumption. We
also discuss a second approach to epsilon-safe planning which relaxes this
independence assumption, involving the incremental construction of a
probability dependence model in conjunction with the construction of the plan
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6811</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6811</id><created>2013-02-27</created><authors><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Generating Bayesian Networks from Probability Logic Knowledge Bases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-262-269</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for dynamically generating Bayesian networks from
knowledge bases consisting of first-order probability logic sentences. We
present a subset of probability logic sufficient for representing the class of
Bayesian networks with discrete-valued nodes. We impose constraints on the form
of the sentences that guarantee that the knowledge base contains all the
probabilistic information necessary to generate a network. We define the
concept of d-separation for knowledge bases and prove that a knowledge base
with independence conditions defined by d-separation is a complete
specification of a probability distribution. We present a network generation
algorithm that, given an inference problem in the form of a query Q and a set
of evidence E, generates a network to compute P(Q|E). We prove the algorithm to
be correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6812</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6812</id><created>2013-02-27</created><authors><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author><author><keyname>Doan</keyname><forenames>AnHai</forenames></author></authors><title>Abstracting Probabilistic Actions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-270-277</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the problem of abstracting conditional probabilistic
actions. We identify two distinct types of abstraction: intra-action
abstraction and inter-action abstraction. We define what it means for the
abstraction of an action to be correct and then derive two methods of
intra-action abstraction and two methods of inter-action abstraction which are
correct according to this criterion. We illustrate the developed techniques by
applying them to actions described with the temporal action representation used
in the DRIPS decision-theoretic planner and we describe how the planner uses
abstraction to reduce the complexity of planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6813</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6813</id><created>2013-02-27</created><authors><author><keyname>Hajek</keyname><forenames>Petr</forenames></author><author><keyname>Harmancov&#xe1;</keyname><forenames>Dagmar</forenames></author><author><keyname>Esteva</keyname><forenames>Francesc</forenames></author><author><keyname>Garcia</keyname><forenames>Pere</forenames></author><author><keyname>Godo</keyname><forenames>Lluis</forenames></author></authors><title>On Modal Logics for Qualitative Possibility in a Fuzzy Setting</title><categories>cs.LO cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-278-285</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the possibilistic approach to uncertainty modeling, the paper presents
a modal logical system to reason about qualitative (comparative) statements of
the possibility (and necessity) of fuzzy propositions. We relate this
qualitative modal logic to the many--valued analogues MVS5 and MVKD45 of the
well known modal logics of knowledge and belief S5 and KD45 respectively.
Completeness results are obtained for such logics and therefore, they extend
previous existing results for qualitative possibilistic logics in the classical
non-fuzzy setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6814</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6814</id><created>2013-02-27</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author></authors><title>A New Look at Causal Independence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1994-PG-286-292</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heckerman (1993) defined causal independence in terms of a set of temporal
conditional independence statements. These statements formalized certain types
of causal interaction where (1) the effect is independent of the order that
causes are introduced and (2) the impact of a single cause on the effect does
not depend on what other causes have previously been applied. In this paper, we
introduce an equivalent a temporal characterization of causal independence
based on a functional representation of the relationship between causes and the
effect. In this representation, the interaction between causes and effect can
be written as a nested decomposition of functions. Causal independence can be
exploited by representing this decomposition in the belief network, resulting
in representations that are more efficient for inference than general causal
models. We present empirical results showing the benefits of a
causal-independence representation for belief-network inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6815</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6815</id><created>2013-02-27</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author></authors><title>Learning Bayesian Networks: The Combination of Knowledge and Statistical
  Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1994-PG-293-301</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe algorithms for learning Bayesian networks from a combination of
user knowledge and statistical data. The algorithms have two components: a
scoring metric and a search procedure. The scoring metric takes a network
structure, statistical data, and a user's prior knowledge, and returns a score
proportional to the posterior probability of the network structure given the
data. The search procedure generates networks for evaluation by the scoring
metric. Our contributions are threefold. First, we identify two important
properties of metrics, which we call event equivalence and parameter
modularity. These properties have been mostly ignored, but when combined,
greatly simplify the encoding of a user's prior knowledge. In particular, a
user can express her knowledge-for the most part-as a single prior Bayesian
network for the domain. Second, we describe local search and annealing
algorithms to be used in conjunction with scoring metrics. In the special case
where each node has at most one parent, we show that heuristic search can be
replaced with a polynomial algorithm to identify the networks with the highest
score. Third, we describe a methodology for evaluating Bayesian-network
learning algorithms. We apply this approach to a comparison of metrics and
search procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6816</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6816</id><created>2013-02-27</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>A Decision-Based View of Causality</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1994-PG-302-310</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most traditional models of uncertainty have focused on the associational
relationship among variables as captured by conditional dependence. In order to
successfully manage intelligent systems for decision making, however, we must
be able to predict the effects of actions. In this paper, we attempt to unite
two branches of research that address such predictions: causal modeling and
decision analysis. First, we provide a definition of causal dependence in
decision-analytic terms, which we derive from consequences of causal dependence
cited in the literature. Using this definition, we show how causal dependence
can be represented within an influence diagram. In particular, we identify two
inadequacies of an ordinary influence diagram as a representation for cause. We
introduce a special class of influence diagrams, called causal influence
diagrams, which corrects one of these problems, and identify situations where
the other inadequacy can be eliminated. In addition, we describe the
relationships between Howard Canonical Form and existing graphical
representations of cause.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6817</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6817</id><created>2013-02-27</created><authors><author><keyname>Heinsohn</keyname><forenames>Jochen</forenames></author></authors><title>Probabilistic Description Logics</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-311-318</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the one hand, classical terminological knowledge representation excludes
the possibility of handling uncertain concept descriptions involving, e.g.,
&quot;usually true&quot; concept properties, generalized quantifiers, or exceptions. On
the other hand, purely numerical approaches for handling uncertainty in general
are unable to consider terminological knowledge. This paper presents the
language ACP which is a probabilistic extension of terminological logics and
aims at closing the gap between the two areas of research. We present the
formal semantics underlying the language ALUP and introduce the probabilistic
formalism that is based on classes of probabilities and is realized by means of
probabilistic constraints. Besides inferring implicitly existent probabilistic
relationships, the constraints guarantee terminological and probabilistic
consistency. Altogether, the new language ALUP applies to domains where both
term descriptions and uncertainty have to be handled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6818</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6818</id><created>2013-02-27</created><authors><author><keyname>Henrion</keyname><forenames>Max</forenames></author><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author><author><keyname>del Favero</keyname><forenames>Brendan</forenames></author><author><keyname>Sanders</keyname><forenames>Gillian</forenames></author></authors><title>An Experimental Comparison of Numerical and Qualitative Probabilistic
  Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-319-326</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative and infinitesimal probability schemes are consistent with the
axioms of probability theory, but avoid the need for precise numerical
probabilities. Using qualitative probabilities could substantially reduce the
effort for knowledge engineering and improve the robustness of results. We
examine experimentally how well infinitesimal probabilities (the kappa-calculus
of Goldszmidt and Pearl) perform a diagnostic task - troubleshooting a car that
will not start by comparison with a conventional numerical belief network. We
found the infinitesimal scheme to be as good as the numerical scheme in
identifying the true fault. The performance of the infinitesimal scheme worsens
significantly for prior fault probabilities greater than 0.03. These results
suggest that infinitesimal probability methods may be of substantial practical
value for machine diagnosis with small prior fault probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6819</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6819</id><created>2013-02-27</created><authors><author><keyname>Hollunder</keyname><forenames>Bernhard</forenames></author></authors><title>An Alternative Proof Method for Possibilistic Logic and its Application
  to Terminological Logics</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-327-335</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic logic, an extension of first-order logic, deals with
uncertainty that can be estimated in terms of possibility and necessity
measures. Syntactically, this means that a first-order formula is equipped with
a possibility degree or a necessity degree that expresses to what extent the
formula is possibly or necessarily true. Possibilistic resolution yields a
calculus for possibilistic logic which respects the semantics developed for
possibilistic logic. A drawback, which possibilistic resolution inherits from
classical resolution, is that it may not terminate if applied to formulas
belonging to decidable fragments of first-order logic. Therefore we propose an
alternative proof method for possibilistic logic. The main feature of this
method is that it completely abstracts from a concrete calculus but uses as
basic operation a test for classical entailment. We then instantiate
possibilistic logic with a terminological logic, which is a decidable subclass
o f first-order logic but nevertheless much more expressive than propositional
logic. This yields an extension of terminological logics towards the
representation of uncertain knowledge which is satisfactory from a semantic as
well as algorithmic point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6820</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6820</id><created>2013-02-27</created><authors><author><keyname>Hsia</keyname><forenames>Yen-Teh</forenames></author></authors><title>Possibilistic Conditioning and Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-336-343</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an axiomatization of confidence transfer - a known conditioning
scheme - from the perspective of expectation-based inference in the sense of
Gardenfors and Makinson. Then, we use the notion of belief independence to
&quot;filter out&quot; different proposal s of possibilistic conditioning rules, all are
variations of confidence transfer. Among the three rules that we consider, only
Dempster's rule of conditioning passes the test of supporting the notion of
belief independence. With the use of this conditioning rule, we then show that
we can use local computation for computing desired conditional marginal
possibilities of the joint possibility satisfying the given constraints. It
turns out that our local computation scheme is already proposed by Shenoy.
However, our intuitions are completely different from that of Shenoy. While
Shenoy just defines a local computation scheme that fits his framework of
valuation-based systems, we derive that local computation scheme from II(,8) =
tI(,8 I a) * II(a) and appropriate independence assumptions, just like how the
Bayesians derive their local computation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6821</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6821</id><created>2013-02-27</created><authors><author><keyname>Huber</keyname><forenames>Marcus J.</forenames></author><author><keyname>Durfee</keyname><forenames>Edmund H.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>The Automated Mapping of Plans for Plan Recognition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-344-351</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To coordinate with other agents in its environment, an agent needs models of
what the other agents are trying to do. When communication is impossible or
expensive, this information must be acquired indirectly via plan recognition.
Typical approaches to plan recognition start with a specification of the
possible plans the other agents may be following, and develop special
techniques for discriminating among the possibilities. Perhaps more desirable
would be a uniform procedure for mapping plans to general structures supporting
inference based on uncertain and incomplete observations. In this paper, we
describe a set of methods for converting plans represented in a flexible
procedural language to observation models represented as probabilistic belief
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6822</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6822</id><created>2013-02-27</created><authors><author><keyname>Jaeger</keyname><forenames>Manfred</forenames></author></authors><title>A Logic for Default Reasoning About Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-352-359</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A logic is defined that allows to express information about statistical
probabilities and about degrees of belief in specific propositions. By
interpreting the two types of probabilities in one common probability space,
the semantics given are well suited to model the influence of statistical
information on the formation of subjective beliefs. Cross entropy minimization
is a key element in these semantics, the use of which is justified by showing
that the resulting logic exhibits some very reasonable properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6823</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6823</id><created>2013-02-27</created><authors><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author><author><keyname>Jensen</keyname><forenames>Frank</forenames></author></authors><title>Optimal Junction Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-360-366</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with optimality issues in connection with updating beliefs in
networks. We address two processes: triangulation and construction of junction
trees. In the first part, we give a simple algorithm for constructing an
optimal junction tree from a triangulated network. In the second part, we argue
that any exact method based on local calculations must either be less efficient
than the junction tree method, or it has an optimality problem equivalent to
that of triangulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6824</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6824</id><created>2013-02-27</created><authors><author><keyname>Jensen</keyname><forenames>Frank</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author><author><keyname>Dittmer</keyname><forenames>Soren L.</forenames></author></authors><title>From Influence Diagrams to Junction Trees</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-367-373</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to the solution of decision problems formulated as
influence diagrams. This approach involves a special triangulation of the
underlying graph, the construction of a junction tree with special properties,
and a message passing algorithm operating on the junction tree for computation
of expected utilities and optimal decision policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6825</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6825</id><created>2013-02-27</created><authors><author><keyname>Kj&#xe6;rulff</keyname><forenames>Uffe</forenames></author></authors><title>Reduction of Computational Complexity in Bayesian Networks through
  Removal of Weak Dependencies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-374-382</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a method for reducing the computational complexity of
Bayesian networks through identification and removal of weak dependencies
(removal of links from the (moralized) independence graph). The removal of a
small number of links may reduce the computational complexity dramatically,
since several fill-ins and moral links may be rendered superfluous by the
removal. The method is described in terms of impact on the independence graph,
the junction tree, and the potential functions associated with these. An
empirical evaluation of the method using large real-world networks demonstrates
the applicability of the method. Further, the method, which has been
implemented in Hugin, complements the approximation method suggested by Jensen
&amp; Andersen (1990).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6826</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6826</id><created>2013-02-27</created><authors><author><keyname>Lam</keyname><forenames>Wai</forenames></author><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author></authors><title>Using New Data to Refine a Bayesian Network</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-383-390</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the issue of refining an existent Bayesian network structure using
new data which might mention only a subset of the variables. Most previous
works have only considered the refinement of the network's conditional
probability parameters, and have not addressed the issue of refining the
network's structure. We develop a new approach for refining the network's
structure. Our approach is based on the Minimal Description Length (MDL)
principle, and it employs an adapted version of a Bayesian network learning
algorithm developed in our previous work. One of the adaptations required is to
modify the previous algorithm to account for the structure of the existent
network. The learning algorithm generates a partial network structure which can
then be used to improve the existent network. We also present experimental
evidence demonstrating the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6827</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6827</id><created>2013-02-27</created><authors><author><keyname>Lang</keyname><forenames>Jerome</forenames></author></authors><title>Syntax-based Default Reasoning as Probabilistic Model-based Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-391-398</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We view the syntax-based approaches to default reasoning as a model-based
diagnosis problem, where each source giving a piece of information is
considered as a component. It is formalized in the ATMS framework (each source
corresponds to an assumption). We assume then that all sources are independent
and &quot;fail&quot; with a very small probability. This leads to a probability
assignment on the set of candidates, or equivalently on the set of consistent
environments. This probability assignment induces a Dempster-Shafer belief
function which measures the probability that a proposition can be deduced from
the evidence. This belief function can be used in several different ways to
define a non-monotonic consequence relation. We study and compare these
consequence relations. The -case of prioritized knowledge bases is briefly
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6828</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6828</id><created>2013-02-27</created><authors><author><keyname>Langley</keyname><forenames>Pat</forenames></author><author><keyname>Sage</keyname><forenames>Stephanie</forenames></author></authors><title>Induction of Selective Bayesian Classifiers</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-399-406</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine previous work on the naive Bayesian classifier and
review its limitations, which include a sensitivity to correlated features. We
respond to this problem by embedding the naive Bayesian induction scheme within
an algorithm that c arries out a greedy search through the space of features.
We hypothesize that this approach will improve asymptotic accuracy in domains
that involve correlated features without reducing the rate of learning in ones
that do not. We report experimental results on six natural domains, including
comparisons with decision-tree induction, that support these hypotheses. In
closing, we discuss other approaches to extending naive Bayesian classifiers
and outline some directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6829</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6829</id><created>2013-02-27</created><authors><author><keyname>Lapointe</keyname><forenames>Stephane</forenames></author><author><keyname>Proulx</keyname><forenames>Rene</forenames></author></authors><title>Fuzzy Geometric Relations to Represent Hierarchical Spatial Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-407-415</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model to represent spatial information is presented in this paper. It is
based on fuzzy constraints represented as fuzzy geometric relations that can be
hierarchically structured. The concept of spatial template is introduced to
capture the idea of interrelated objects in two-dimensional space. The
representation model is used to specify imprecise or vague information
consisting in relative locations and orientations of template objects. It is
shown in this paper how a template represented by this model can be matched
against a crisp situation to recognize a particular instance of this template.
Furthermore, the proximity measure (fuzzy measure) between the instance and the
template is worked out - this measure can be interpreted as a degree of
similarity. In this context, template recognition can be viewed as a case of
fuzzy pattern recognition. The results of this work have been implemented and
applied to a complex military problem from which this work originated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6830</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6830</id><created>2013-02-27</created><authors><author><keyname>Lehner</keyname><forenames>Paul E.</forenames></author><author><keyname>Elsaesser</keyname><forenames>Christopher</forenames></author><author><keyname>Musman</keyname><forenames>Scott A.</forenames></author></authors><title>Constructing Belief Networks to Evaluate Plans</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-416-422</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the problem of constructing belief networks to evaluate
plans produced by an knowledge-based planner. Techniques are presented for
handling various types of complicating plan features. These include plans with
context-dependent consequences, indirect consequences, actions with
preconditions that must be true during the execution of an action,
contingencies, multiple levels of abstraction multiple execution agents with
partially-ordered and temporally overlapping actions, and plans which reference
specific times and time durations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6831</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6831</id><created>2013-02-27</created><authors><author><keyname>Mansell</keyname><forenames>Todd Michael</forenames></author><author><keyname>Smith</keyname><forenames>Grahame</forenames></author></authors><title>Operator Selection While Planning Under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-423-431</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the best first search strategy used by U-Plan (Mansell
1993a), a planning system that constructs quantitatively ranked plans given an
incomplete description of an uncertain environment. U-Plan uses uncertain and
incomplete evidence de scribing the environment, characterizes it using a
Dempster-Shafer interval, and generates a set of possible world states. Plan
construction takes place in an abstraction hierarchy where strategic decisions
are made before tactical decisions. Search through this abstraction hierarchy
is guided by a quantitative measure (expected fulfillment) based on decision
theory. The search strategy is best first with the provision to update expected
fulfillment and review previous decisions in the light of planning
developments. U-Plan generates multiple plans for multiple possible worlds, and
attempts to use existing plans for new world situations. A super-plan is then
constructed, based on merging the set of plans and appropriately timed
knowledge acquisition operators, which are used to decide between plan
alternatives during plan execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6832</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6832</id><created>2013-02-27</created><authors><author><keyname>Nejdl</keyname><forenames>Wolfgang</forenames></author><author><keyname>Gamper</keyname><forenames>Johann</forenames></author></authors><title>Model-Based Diagnosis with Qualitative Temporal Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-432-439</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a framework for model-based diagnosis of dynamic
systems, which extends previous work in this field by using and expressing
temporal uncertainty in the form of qualitative interval relations a la Allen.
Based on a logical framework extended by qualitative and quantitative temporal
constraints we show how to describe behavioral models (both consistency- and
abductive-based), discuss how to use abstract observations and show how
abstract temporal diagnoses are computed. This yields an expressive framework,
which allows the representation of complex temporal behavior allowing us to
represent temporal uncertainty. Due to its abstraction capabilities computation
is made independent of the number of observations and time points in a temporal
setting. An example of hepatitis diagnosis is used throughout the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6833</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6833</id><created>2013-02-27</created><authors><author><keyname>Ng</keyname><forenames>Keung-Chi</forenames></author><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author></authors><title>Incremental Dynamic Construction of Layered Polytree Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-440-446</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certain classes of problems, including perceptual data understanding,
robotics, discovery, and learning, can be represented as incremental,
dynamically constructed belief networks. These automatically constructed
networks can be dynamically extended and modified as evidence of new
individuals becomes available. The main result of this paper is the incremental
extension of the singly connected polytree network in such a way that the
network retains its singly connected polytree structure after the changes. The
algorithm is deterministic and is guaranteed to have a complexity of single
node addition that is at most of order proportional to the number of nodes (or
size) of the network. Additional speed-up can be achieved by maintaining the
path information. Despite its incremental and dynamic nature, the algorithm can
also be used for probabilistic inference in belief networks in a fashion
similar to other exact inference algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6834</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6834</id><created>2013-02-27</created><authors><author><keyname>O'Leary</keyname><forenames>Daniel E.</forenames></author></authors><title>Models of Consensus for Multiple Agent Systems</title><categories>cs.MA cs.SY</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-447-453</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models of consensus are used to manage multiple agent systems in order to
choose between different recommendations provided by the system. It is assumed
that there is a central agent that solicits recommendations or plans from other
agents. That agent the n determines the consensus of the other agents, and
chooses the resultant consensus recommendation or plan. Voting schemes such as
this have been used in a variety of domains, including air traffic control.
This paper uses an analytic model to study the use of consensus in multiple
agent systems. The binomial model is used to study the probability that the
consensus judgment is correct or incorrect. That basic model is extended to
account for both different levels of agent competence and unequal prior odds.
The analysis of that model is critical in the investigation of multiple agent
systems, since the model leads us to conclude that in some cases consensus
judgment is not appropriate. In addition, the results allow us to determine how
many agents should be used to develop consensus decisions, which agents should
be used to develop consensus decisions and under which conditions the consensus
model should be used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6835</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6835</id><created>2013-02-27</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>A Probabilistic Calculus of Actions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-454-462</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a symbolic machinery that admits both probabilistic and causal
information about a given domain and produces probabilistic statements about
the effect of actions and the impact of observations. The calculus admits two
types of conditioning operators: ordinary Bayes conditioning, P(y|X = x), which
represents the observation X = x, and causal conditioning, P(y|do(X = x)), read
the probability of Y = y conditioned on holding X constant (at x) by deliberate
action. Given a mixture of such observational and causal sentences, together
with the topology of the causal graph, the calculus derives new conditional
probabilities of both types, thus enabling one to quantify the effects of
actions (and policies) from partially specified knowledge bases, such as
Bayesian networks in which some conditional probabilities may not be available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6836</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6836</id><created>2013-02-27</created><authors><author><keyname>Pimentel</keyname><forenames>Stephen G.</forenames></author><author><keyname>Brem</keyname><forenames>Lawrence M.</forenames></author></authors><title>Robust Planning in Uncertain Environments</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-463-469</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel approach to planning which takes advantage of
decision theory to greatly improve robustness in an uncertain environment. We
present an algorithm which computes conditional plans of maximum expected
utility. This algorithm relies on a representation of the search space as an
AND/OR tree and employs a depth-limit to control computation costs. A numeric
robustness factor, which parameterizes the utility function, allows the user to
modulate the degree of risk-aversion employed by the planner. Via a look-ahead
search, the planning algorithm seeks to find an optimal plan using expected
utility as its optimization criterion. We present experimental results obtained
by applying our algorithm to a non-deterministic extension of the blocks world
domain. Our results demonstrate that the robustness factor governs the degree
of risk embodied in the conditional plans computed by our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6837</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6837</id><created>2013-02-27</created><authors><author><keyname>Pittarelli</keyname><forenames>Michael</forenames></author></authors><title>Anytime Decision Making with Imprecise Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-470-477</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines methods of decision making that are able to accommodate
limitations on both the form in which uncertainty pertaining to a decision
problem can be realistically represented and the amount of computing time
available before a decision must be made. The methods are anytime algorithms in
the sense of Boddy and Dean 1991. Techniques are presented for use with Frisch
and Haddawy's [1992] anytime deduction system, with an anytime adaptation of
Nilsson's [1986] probabilistic logic, and with a probabilistic database model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6838</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6838</id><created>2013-02-27</created><authors><author><keyname>Poland</keyname><forenames>William B.</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Three Approaches to Probability Model Selection</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-478-483</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares three approaches to the problem of selecting among
probability models to fit data (1) use of statistical criteria such as Akaike's
information criterion and Schwarz's &quot;Bayesian information criterion,&quot; (2)
maximization of the posterior probability of the model, and (3) maximization of
an ?effectiveness ratio? trading off accuracy and computational cost. The
unifying characteristic of the approaches is that all can be viewed as
maximizing a penalized likelihood function. The second approach with suitable
prior distributions has been shown to reduce to the first. This paper shows
that the third approach reduces to the second for a particular form of the
effectiveness ratio, and illustrates all three approaches with the problem of
selecting the number of components in a mixture of Gaussian distributions.
Unlike the first two approaches, the third can be used even when the candidate
models are chosen for computational efficiency, without regard to physical
interpretation, so that the likelihood and the prior distribution over models
cannot be interpreted literally. As the most general and computationally
oriented of the approaches, it is especially useful for artificial intelligence
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6839</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6839</id><created>2013-02-27</created><authors><author><keyname>Pradhan</keyname><forenames>Malcolm</forenames></author><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author><author><keyname>Middleton</keyname><forenames>Blackford</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>Knowledge Engineering for Large Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-484-490</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several techniques for knowledge engineering of large belief
networks (BNs) based on the our experiences with a network derived from a large
medical knowledge base. The noisyMAX, a generalization of the noisy-OR gate, is
used to model causal in dependence in a BN with multi-valued variables. We
describe the use of leak probabilities to enforce the closed-world assumption
in our model. We present Netview, a visualization tool based on causal
independence and the use of leak probabilities. The Netview software allows
knowledge engineers to dynamically view sub-networks for knowledge engineering,
and it provides version control for editing a BN. Netview generates
sub-networks in which leak probabilities are dynamically updated to reflect the
missing portions of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6840</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6840</id><created>2013-02-27</created><authors><author><keyname>Qi</keyname><forenames>Runping</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Solving Asymmetric Decision Problems with Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-491-497</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While influence diagrams have many advantages as a representation framework
for Bayesian decision problems, they have a serious drawback in handling
asymmetric decision problems. To be represented in an influence diagram, an
asymmetric decision problem must be symmetrized. A considerable amount of
unnecessary computation may be involved when a symmetrized influence diagram is
evaluated by conventional algorithms. In this paper we present an approach for
avoiding such unnecessary computation in influence diagram evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6841</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6841</id><created>2013-02-27</created><authors><author><keyname>Ramoni</keyname><forenames>Marco</forenames></author><author><keyname>Riva</keyname><forenames>Alberto</forenames></author></authors><title>Belief Maintenance in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-498-505</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Belief Networks (BBNs) are a powerful formalism for reasoning under
uncertainty but bear some severe limitations: they require a large amount of
information before any reasoning process can start, they have limited
contradiction handling capabilities, and their ability to provide explanations
for their conclusion is still controversial. There exists a class of reasoning
systems, called Truth Maintenance Systems (TMSs), which are able to deal with
partially specified knowledge, to provide well-founded explanation for their
conclusions, and to detect and handle contradictions. TMSs incorporating
measure of uncertainty are called Belief Maintenance Systems (BMSs). This paper
describes how a BMS based on probabilistic logic can be applied to BBNs, thus
introducing a new class of BBNs, called Ignorant Belief Networks, able to
incrementally deal with partially specified conditional dependencies, to
provide explanations, and to detect and handle contradictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6842</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6842</id><created>2013-02-27</created><authors><author><keyname>Santos</keyname><forenames>Eugene</forenames><suffix>Jr.</suffix></author><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Belief Updating by Enumerating High-Probability Independence-Based
  Assignments</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-506-513</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independence-based (IB) assignments to Bayesian belief networks were
originally proposed as abductive explanations. IB assignments assign fewer
variables in abductive explanations than do schemes assigning values to all
evidentially supported variables. We use IB assignments to approximate marginal
probabilities in Bayesian belief networks. Recent work in belief updating for
Bayes networks attempts to approximate posterior probabilities by finding a
small number of the highest probability complete (or perhaps evidentially
supported) assignments. Under certain assumptions, the probability mass in the
union of these assignments is sufficient to obtain a good approximation. Such
methods are especially useful for highly-connected networks, where the maximum
clique size or the cutset size make the standard algorithms intractable. Since
IB assignments contain fewer assigned variables, the probability mass in each
assignment is greater than in the respective complete assignment. Thus, fewer
IB assignments are sufficient, and a good approximation can be obtained more
efficiently. IB assignments can be used for efficiently approximating posterior
node probabilities even in cases which do not obey the rather strict skewness
assumptions used in previous research. Two algorithms for finding the high
probability IB assignments are suggested: one by doing a best-first heuristic
search, and another by special-purpose integer linear programming. Experimental
results show that this approach is feasible for highly connected belief
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6843</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6843</id><created>2013-02-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Andersen</keyname><forenames>Stig K.</forenames></author><author><keyname>Szolovits</keyname><forenames>Peter</forenames></author></authors><title>Global Conditioning for Probabilistic Inference in Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-514-522</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new approach to probabilistic inference on belief
networks, global conditioning, which is a simple generalization of Pearl's
(1986b) method of loopcutset conditioning. We show that global conditioning, as
well as loop-cutset conditioning, can be thought of as a special case of the
method of Lauritzen and Spiegelhalter (1988) as refined by Jensen et al (199Oa;
1990b). Nonetheless, this approach provides new opportunities for parallel
processing and, in the case of sequential processing, a tradeoff of time for
memory. We also show how a hybrid method (Suermondt and others 1990) combining
loop-cutset conditioning with Jensen's method can be viewed within our
framework. By exploring the relationships between these methods, we develop a
unifying framework in which the advantages of each approach can be combined
successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6844</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6844</id><created>2013-02-27</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Belief Induced by the Partial Knowledge of the Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-523-532</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct the belief function that quantifies the agent, beliefs about
which event of Q will occurred when he knows that the event is selected by a
chance set-up and that the probability function associated to the chance set up
is only partially known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6845</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6845</id><created>2013-02-27</created><authors><author><keyname>Snow</keyname><forenames>Paul</forenames></author></authors><title>Ignorance and the Expressiveness of Single- and Set-Valued Probability
  Models of Belief</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-531-537</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over time, there have hen refinements in the way that probability
distributions are used for representing beliefs. Models which rely on single
probability distributions depict a complete ordering among the propositions of
interest, yet human beliefs are sometimes not completely ordered. Non-singleton
sets of probability distributions can represent partially ordered beliefs.
Convex sets are particularly convenient and expressive, but it is known that
there are reasonable patterns of belief whose faithful representation require
less restrictive sets. The present paper shows that prior ignorance about three
or more exclusive alternatives and the emergence of partially ordered beliefs
when evidence is obtained defy representation by any single set of
distributions, but yield to a representation baud on several uts. The partial
order is shown to be a partial qualitative probability which shares some
intuitively appealing attributes with probability distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6846</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6846</id><created>2013-02-27</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author></authors><title>A Probabilistic Approach to Hierarchical Model-based Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-538-545</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based diagnosis reasons backwards from a functional schematic of a
system to isolate faults given observations of anomalous behavior. We develop a
fully probabilistic approach to model based diagnosis and extend it to support
hierarchical models. Our scheme translates the functional schematic into a
Bayesian network and diagnostic inference takes place in the Bayesian network.
A Bayesian network diagnostic inference algorithm is modified to take advantage
of the hierarchy to give computational gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6847</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6847</id><created>2013-02-27</created><authors><author><keyname>Studeny</keyname><forenames>Milan</forenames></author></authors><title>Semigraphoids Are Two-Antecedental Approximations of Stochastic
  Conditional Independence Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-546-552</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semigraphoid closure of every couple of CI-statements (GI=conditional
independence) is a stochastic CI-model. As a consequence of this result it is
shown that every probabilistically sound inference rule for CI-model, having at
most two antecedents, is derivable from the semigraphoid inference rules. This
justifies the use of semigraphoids as approximations of stochastic CI-models in
probabilistic reasoning. The list of all 19 potential dominant elements of the
mentioned semigraphoid closure is given as a byproduct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6848</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6848</id><created>2013-02-27</created><authors><author><keyname>Tan</keyname><forenames>Sek-Wah</forenames></author></authors><title>Exceptional Subclasses in Qualitative Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-553-559</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System Z+ [Goldszmidt and Pearl, 1991, Goldszmidt, 1992] is a formalism for
reasoning with normality defaults of the form &quot;typically if phi then + (with
strength cf)&quot; where 6 is a positive integer. The system has a critical
shortcoming in that it does not sanction inheritance across exceptional
subclasses. In this paper we propose an extension to System Z+ that rectifies
this shortcoming by extracting additional conditions between worlds from the
defaults database. We show that the additional constraints do not change the
notion of the consistency of a database. We also make comparisons with
competing default reasoning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6849</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6849</id><created>2013-02-27</created><authors><author><keyname>Wang</keyname><forenames>Pei</forenames></author></authors><title>A Defect in Dempster-Shafer Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-560-566</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By analyzing the relationships among chance, weight of evidence and degree of
beliefwe show that the assertion &quot;probability functions are special cases of
belief functions&quot; and the assertion &quot;Dempster's rule can be used to combine
belief functions based on distinct bodies of evidence&quot; together lead to an
inconsistency in Dempster-Shafer theory. To solve this problem, we must reject
some fundamental postulates of the theory. We introduce a new approach for
uncertainty management that shares many intuitive ideas with D-S theory, while
avoiding this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6850</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6850</id><created>2013-02-27</created><authors><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author></authors><title>State-space Abstraction for Anytime Evaluation of Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-567-574</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important factor determining the computational complexity of evaluating a
probabilistic network is the cardinality of the state spaces of the nodes. By
varying the granularity of the state spaces, one can trade off accuracy in the
result for computational efficiency. We present an anytime procedure for
approximate evaluation of probabilistic networks based on this idea. On
application to some simple networks, the procedure exhibits a smooth
improvement in approximation quality as computation time increases. This
suggests that state-space abstraction is one more useful control parameter for
designing real-time probabilistic reasoners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6851</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6851</id><created>2013-02-27</created><authors><author><keyname>Weydert</keyname><forenames>Emil</forenames></author></authors><title>General Belief Measures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-575-582</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probability measures by themselves, are known to be inappropriate for
modeling the dynamics of plain belief and their excessively strong
measurability constraints make them unsuitable for some representational tasks,
e.g. in the context of firstorder knowledge. In this paper, we are therefore
going to look for possible alternatives and extensions. We begin by delimiting
the general area of interest, proposing a minimal list of assumptions to be
satisfied by any reasonable quasi-probabilistic valuation concept. Within this
framework, we investigate two particularly interesting kinds of quasi-measures
which are not or much less affected by the traditional problems. * Ranking
measures, which generalize Spohn-type and possibility measures. * Cumulative
measures, which combine the probabilistic and the ranking philosophy, allowing
thereby a fine-grained account of static and dynamic belief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6852</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6852</id><created>2013-02-27</created><authors><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>Generating Graphoids from Generalised Conditional Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-583-590</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a general approach to uncertainty on product spaces, and give
sufficient conditions for the independence structures of uncertainty measures
to satisfy graphoid properties. Since these conditions are arguably more
intuitive than some of the graphoid properties, they can be viewed as
explanations why probability and certain other formalisms generate graphoids.
The conditions include a sufficient condition for the Intersection property
which can still apply even if there is a strong logical relations hip between
the variables. We indicate how these results can be used to produce theories of
qualitative conditional probability which are semi-graphoids and graphoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6853</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6853</id><created>2013-02-27</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Wang</keyname><forenames>Z. W.</forenames></author></authors><title>On Axiomatization of Probabilistic Conditional Independencies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-591-597</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the connection between probabilistic conditional
independence in uncertain reasoning and data dependency in relational
databases. As a demonstration of the usefulness of this preliminary
investigation, an alternate proof is presented for refuting the conjecture
suggested by Pearl and Paz that probabilistic conditional independencies have a
complete axiomatization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6854</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6854</id><created>2013-02-27</created><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Evidential Reasoning with Conditional Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-598-605</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the existing evidential networks with belief functions, the relations
among the variables are always represented by joint belief functions on the
product space of the involved variables. In this paper, we use conditional
belief functions to represent such relations in the network and show some
relations of these two kinds of representations. We also present a propagation
algorithm for such networks. By analyzing the properties of some special
evidential networks with conditional belief functions, we show that the
reasoning process can be simplified in such kinds of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6855</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6855</id><created>2013-02-27</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Inter-causal Independence and Heterogeneous Factorization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Tenth Conference on Uncertainty in
  Artificial Intelligence (UAI1994)</comments><proxy>auai</proxy><report-no>UAI-P-1994-PG-606-614</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that conditional independence can be used to factorize a
joint probability into a multiplication of conditional probabilities. This
paper proposes a constructive definition of inter-causal independence, which
can be used to further factorize a conditional probability. An inference
algorithm is developed, which makes use of both conditional independence and
inter-causal independence to reduce inference complexity in Bayesian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6861</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6861</id><created>2013-02-27</created><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author><author><keyname>Zamponi</keyname><forenames>Francesco</forenames></author></authors><title>The effect of quantum fluctuations on the coloring of random graphs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.CC quant-ph</categories><comments>28 pages, 17 figures</comments><journal-ref>Phys. Rev. A 87, 042322 (2013)</journal-ref><doi>10.1103/PhysRevA.87.042322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a study of the coloring problem (antiferromagnetic Potts model) of
random regular graphs, submitted to quantum fluctuations induced by a
transverse field, using the quantum cavity method and quantum Monte-Carlo
simulations. We determine the order of the quantum phase transition encountered
at low temperature as a function of the transverse field and discuss the
structure of the quantum spin glass phase. In particular, we conclude that the
quantum adiabatic algorithm would fail to solve efficiently typical instances
of these problems because of avoided level crossings within the quantum spin
glass phase, caused by a competition between energetic and entropic effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6863</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6863</id><created>2013-02-27</created><updated>2015-04-20</updated><authors><author><keyname>Gajarsk&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Obdr&#x17e;&#xe1;lek</keyname><forenames>Jan</forenames></author><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Rossmanith</keyname><forenames>Peter</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando S&#xe1;nchez</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author></authors><title>Kernelization Using Structural Parameters on Sparse Graph Classes</title><categories>cs.DS</categories><comments>A preliminary version appeared as an extended abstract in the
  proceedings of ESA 2013, and one section in the proceedings of IPEC 2014.
  Changes from the previous version: inclusion of the IPEC 2014 results; much
  stronger conclusion for the case of nowhere dense graph classes; inclusion of
  some additional problems in the framework, e.g., of the branchwidth problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meta-theorems for polynomial (linear) kernels have been the subject of
intensive research in parameterized complexity. Heretofore, meta-theorems for
linear kernels exist on graphs of bounded genus, $H$-minor-free graphs, and
$H$-topological-minor-free graphs. To the best of our knowledge, no
meta-theorems for polynomial kernels are known for any larger sparse graph
classes; e.g., for classes of bounded expansion or for nowhere dense ones. In
this paper we prove such meta-theorems for the two latter cases. More
specifically, we show that graph problems that have finite integer index (FII)
have linear kernels on graphs of bounded expansion when parameterized by the
size of a modulator to constant-treedepth graphs. For nowhere dense graph
classes, our result yields almost-linear kernels. While our parameter may seem
rather strong, we argue that a linear kernelization result on graphs of bounded
expansion with a weaker parameter (than treedepth modulator) would fail to
include some of the problems covered by our framework. Moreover, we only
require the problems to have FII on graphs of constant treedepth. This allows
us to prove linear kernels for problems such as Longest Path/Cycle, Exact
$s,t$-Path, Treewidth, and Pathwidth, which do not have FII on general graphs
(and the first two not even on bounded treewidth graphs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6866</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6866</id><created>2013-02-27</created><authors><author><keyname>Cardoso</keyname><forenames>Leonardo S.</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author><author><keyname>Cavalcanti</keyname><forenames>Francisco Rodrigo P.</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Vandermonde-subspace Frequency Division Multiplexing for Two-Tiered
  Cognitive Radio Networks</title><categories>cs.IT math.IT</categories><comments>9 pages, accepted for publication in IEEE Transactions on
  Communications</comments><doi>10.1109/TCOMM.2013.13.120109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vandermonde-subspace frequency division multiplexing (VFDM) is an overlay
spectrum sharing technique for cognitive radio. VFDM makes use of a precoder
based on a Vandermonde structure to transmit information over a secondary
system, while keeping an orthogonal frequency division multiplexing
(OFDM)-based primary system interference-free. To do so, VFDM exploits
frequency selectivity and the use of cyclic prefixes by the primary system.
Herein, a global view of VFDM is presented, including also practical aspects
such as linear receivers and the impact of channel estimation. We show that
VFDM provides a spectral efficiency increase of up to 1 bps/Hz over cognitive
radio systems based on unused band detection. We also present some key design
parameters for its future implementation and a feasible channel estimation
protocol. Finally we show that, even when some of the theoretical assumptions
are relaxed, VFDM provides non-negligible rates while protecting the primary
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6885</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6885</id><created>2013-02-27</created><authors><author><keyname>Bazaikin</keyname><forenames>Ya. V.</forenames></author><author><keyname>Baikov</keyname><forenames>V. A.</forenames></author><author><keyname>Taimanov</keyname><forenames>I. A.</forenames></author><author><keyname>Yakovlev</keyname><forenames>A. A.</forenames></author></authors><title>Numerical analysis of topological characteristics of three-dimensional
  geological models of oil and gas fields</title><categories>cs.CG math.AT</categories><comments>19 pages, 11 figures, 1 table</comments><journal-ref>Mathematical Modeling 25:10 (2013), 19-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the study of topological characteristics of random fields that are
used for numerical simulation of oil and gas reservoirs and numerical
algorithms (see arXiv:1302.3669), for computing such characteristics, for which
we demonstrate results of their applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6890</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6890</id><created>2013-02-27</created><updated>2013-10-07</updated><authors><author><keyname>Grov</keyname><forenames>Gudmund</forenames></author><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Lin</keyname><forenames>Yuhui</forenames></author></authors><title>A Graphical Language for Proof Strategies</title><categories>cs.LO</categories><doi>10.1007/978-3-642-45221-5_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex automated proof strategies are often difficult to extract, visualise,
modify, and debug. Traditional tactic languages, often based on stack-based
goal propagation, make it easy to write proofs that obscure the flow of goals
between tactics and are fragile to minor changes in input, proof structure or
changes to tactics themselves. Here, we address this by introducing a graphical
language called PSGraph for writing proof strategies. Strategies are
constructed visually by &quot;wiring together&quot; collections of tactics and evaluated
by propagating goal nodes through the diagram via graph rewriting. Tactic nodes
can have many output wires, and use a filtering procedure based on goal-types
(predicates describing the features of a goal) to decide where best to send
newly-generated sub-goals.
  In addition to making the flow of goal information explicit, the graphical
language can fulfil the role of many tacticals using visual idioms like
branching, merging, and feedback loops. We argue that this language enables
development of more robust proof strategies and provide several examples, along
with a prototype implementation in Isabelle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6900</identifier>
 <datestamp>2014-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6900</id><created>2013-02-27</created><updated>2014-06-05</updated><authors><author><keyname>Schmitt</keyname><forenames>Manuel</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>Exploiting Independent Subformulas: A Faster Approximation Scheme for
  #k-SAT</title><categories>cs.DS</categories><comments>Improves: arXiv:1107.2001</comments><acm-class>F.2.2; G.2.1</acm-class><journal-ref>Information Processing Letters 113 (2013) 337-344</journal-ref><doi>10.1016/j.ipl.2013.02.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an improvement on Thurley's recent randomized approximation scheme
for #k-SAT where the task is to count the number of satisfying truth
assignments of a Boolean function {\Phi} given as an n-variable k-CNF. We
introduce a novel way to identify independent substructures of {\Phi} and can
therefore reduce the size of the search space considerably. Our randomized
algorithm works for any k. For #3-SAT, it runs in time
O(\epsilon^{-2}*1.51426^n), for #4-SAT, it runs in time
O(\epsilon^{-2}*1.60816^n), with error bound \epsilon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6906</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6906</id><created>2013-02-27</created><authors><author><keyname>Foster</keyname><forenames>Jacob G.</forenames></author><author><keyname>Rzhetsky</keyname><forenames>Andrey</forenames></author><author><keyname>Evans</keyname><forenames>James A.</forenames></author></authors><title>Tradition and Innovation in Scientists' Research Strategies</title><categories>physics.soc-ph cs.DL cs.SI stat.AP</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What factors affect a scientist's choice of research problem? Qualitative
research in the history, philosophy, and sociology of science suggests that
this choice is shaped by an &quot;essential tension&quot; between the professional demand
for productivity and a conflicting drive toward risky innovation. We examine
this tension empirically in the context of biomedical chemistry. We use complex
networks to represent the evolving state of scientific knowledge, as expressed
in publications. We then define research strategies relative to these networks.
Scientists can introduce novel chemicals or chemical relationships--or delve
deeper into known ones. They can consolidate existing knowledge clusters, or
bridge distant ones. Analyzing such choices in aggregate, we find that the
distribution of strategies remains remarkably stable, even as chemical
knowledge grows dramatically. High-risk strategies, which explore new chemical
relationships, are less prevalent in the literature, reflecting a growing focus
on established knowledge at the expense of new opportunities. Research
following a risky strategy is more likely to be ignored but also more likely to
achieve high impact and recognition. While the outcome of a risky strategy has
a higher expected reward than the outcome of a conservative strategy, the
additional reward is insufficient to compensate for the additional risk. By
studying the winners of 137 different prizes in biomedicine and chemistry, we
show that the occasional &quot;gamble&quot; for extraordinary impact is the most
plausible explanation for observed levels of risk-taking. Our empirical
demonstration and unpacking of the &quot;essential tension&quot; suggests policy
interventions that may foster more innovative research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6911</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6911</id><created>2013-02-11</created><authors><author><keyname>Schirmer</keyname><forenames>Oskar</forenames></author></authors><title>Using Virtual Addresses with Communication Channels</title><categories>cs.DC cs.AR</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While for single processor and SMP machines, memory is the allocatable
quantity, for machines made up of large amounts of parallel computing units,
each with its own local memory, the allocatable quantity is a single computing
unit. Where virtual address management is used to keep memory coherent and
allow allocation of more than physical memory is actually available, virtual
communication channel references can be used to make computing units stay
connected across allocation and swapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6914</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6914</id><created>2013-02-27</created><authors><author><keyname>Howat</keyname><forenames>John</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author></authors><title>The Fresh-Finger Property</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unified property roughly states that searching for an element is fast
when the current access is close to a recent access. Here, &quot;close&quot; refers to
rank distance measured among all elements stored by the dictionary. We show
that distance need not be measured this way: in fact, it is only necessary to
consider a small working-set of elements to measure this rank distance. This
results in a data structure with access time that is an improvement upon those
offered by the unified property for many query sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6927</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6927</id><created>2013-02-27</created><authors><author><keyname>Anava</keyname><forenames>Oren</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Online Learning for Time Series Prediction</title><categories>cs.LG</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of predicting a time series using the
ARMA (autoregressive moving average) model, under minimal assumptions on the
noise terms. Using regret minimization techniques, we develop effective online
learning algorithms for the prediction problem, without assuming that the noise
terms are Gaussian, identically distributed or even independent. Furthermore,
we show that our algorithm's performances asymptotically approaches the
performance of the best ARMA model in hindsight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6932</identifier>
 <datestamp>2013-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6932</id><created>2013-02-27</created><updated>2013-08-19</updated><authors><author><keyname>Galas</keyname><forenames>David J.</forenames></author><author><keyname>Sakhanenko</keyname><forenames>Nikita A.</forenames></author><author><keyname>Skupin</keyname><forenames>Alexander</forenames></author><author><keyname>Ignac</keyname><forenames>Tomasz</forenames></author></authors><title>Describing the complexity of systems: multi-variable &quot;set complexity&quot;
  and the information basis of systems biology</title><categories>cs.IT math.IT q-bio.QM</categories><comments>44 pages, 12 figures; made revisions after peer review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context dependence is central to the description of complexity. Keying on the
pairwise definition of &quot;set complexity&quot; we use an information theory approach
to formulate general measures of systems complexity. We examine the properties
of multi-variable dependency starting with the concept of interaction
information. We then present a new measure for unbiased detection of
multi-variable dependency, &quot;differential interaction information.&quot; This
quantity for two variables reduces to the pairwise &quot;set complexity&quot; previously
proposed as a context-dependent measure of information in biological systems.
We generalize it here to an arbitrary number of variables. Critical limiting
properties of the &quot;differential interaction information&quot; are key to the
generalization. This measure extends previous ideas about biological
information and provides a more sophisticated basis for study of complexity.
The properties of &quot;differential interaction information&quot; also suggest new
approaches to data analysis. Given a data set of system measurements
differential interaction information can provide a measure of collective
dependence, which can be represented in hypergraphs describing complex system
interaction patterns. We investigate this kind of analysis using simulated data
sets. The conjoining of a generalized set complexity measure, multi-variable
dependency analysis, and hypergraphs is our central result. While our focus is
on complex biological systems, our results are applicable to any complex
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6934</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6934</id><created>2013-02-27</created><authors><author><keyname>Clazzer</keyname><forenames>Federico</forenames></author><author><keyname>Kissling</keyname><forenames>Christian</forenames></author></authors><title>Optimum Header Positioning in Successive Interference Cancellation (SIC)
  based Aloha</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in the IEEE International Conference on
  Communications (ICC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Access MAC protocols are simple and effective when the nature of the
traffic is unpredictable and sporadic. In the following paper, investigations
on the new Enhanced Contention Resolution ALOHA (ECRA) are presented, where
some new aspects of the protocol are investigated. Mathematical derivation and
numerical evaluation of the symbol interference probability after SIC are here
provided. Results of the optimum header positioning which is found to be in the
beginning and in the end of the packets, are exploited for the evaluation of
ECRA throughput and Packet Error Rate (PER) under imperfect knowledge of
packets positions. Remarkable gains in the maximum throughput are observed for
ECRA w.r.t. Contention Resolution ALOHA (CRA) under this assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6937</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6937</id><created>2013-02-27</created><updated>2014-06-10</updated><authors><author><keyname>Anava</keyname><forenames>Oren</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Online Convex Optimization Against Adversaries with Memory and
  Application to Statistical Arbitrage</title><categories>cs.LG</categories><comments>22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The framework of online learning with memory naturally captures learning
problems with temporal constraints, and was previously studied for the experts
setting. In this work we extend the notion of learning with memory to the
general Online Convex Optimization (OCO) framework, and present two algorithms
that attain low regret. The first algorithm applies to Lipschitz continuous
loss functions, obtaining optimal regret bounds for both convex and strongly
convex losses. The second algorithm attains the optimal regret bounds and
applies more broadly to convex losses without requiring Lipschitz continuity,
yet is more complicated to implement. We complement our theoretic results with
an application to statistical arbitrage in finance: we devise algorithms for
constructing mean-reverting portfolios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6957</identifier>
 <datestamp>2013-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6957</id><created>2013-02-27</created><authors><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Sattigeri</keyname><forenames>Prasanna</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Ensemble Sparse Models for Image Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representations with learned dictionaries have been successful in
several image analysis applications. In this paper, we propose and analyze the
framework of ensemble sparse models, and demonstrate their utility in image
restoration and unsupervised clustering. The proposed ensemble model
approximates the data as a linear combination of approximations from multiple
\textit{weak} sparse models. Theoretical analysis of the ensemble model reveals
that even in the worst-case, the ensemble can perform better than any of its
constituent individual models. The dictionaries corresponding to the individual
sparse models are obtained using either random example selection or boosted
approaches. Boosted approaches learn one dictionary per round such that the
dictionary learned in a particular round is optimized for the training examples
having high reconstruction error in the previous round. Results with compressed
recovery show that the ensemble representations lead to a better performance
compared to using a single dictionary obtained with the conventional
alternating minimization approach. The proposed ensemble models are also used
for single image superresolution, and we show that they perform comparably to
the recent approaches. In unsupervised clustering, experiments show that the
proposed model performs better than baseline approaches in several standard
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6960</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6960</id><created>2013-02-26</created><updated>2013-04-01</updated><authors><author><keyname>Bargu&#xf1;&#xf3;</keyname><forenames>Luis</forenames><affiliation>Universitat Polit&#xe9;cnica de Catalunya</affiliation></author><author><keyname>Creus</keyname><forenames>Carles</forenames><affiliation>Universitat Polit&#xe9;cnica de Catalunya</affiliation></author><author><keyname>Godoy</keyname><forenames>Guillem</forenames><affiliation>Universitat Polit&#xe9;cnica de Catalunya</affiliation></author><author><keyname>Jacquemard</keyname><forenames>Florent</forenames><affiliation>INRIA Saclay, LSV-CNRS</affiliation></author><author><keyname>Vacher</keyname><forenames>Camille</forenames><affiliation>LIFL, Univ. Lille I, INRIA Lille</affiliation></author></authors><title>Decidable Classes of Tree Automata Mixing Local and Global Constraints
  Modulo Flat Theories</title><categories>cs.LO cs.FL</categories><comments>39 pages, to appear in LMCS journal</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (April 2,
  2013) lmcs:1161</journal-ref><doi>10.2168/LMCS-9(2:1)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a class of ranked tree automata TABG generalizing both the tree
automata with local tests between brothers of Bogaert and Tison (1992) and with
global equality and disequality constraints (TAGED) of Filiot et al. (2007).
TABG can test for equality and disequality modulo a given flat equational
theory between brother subterms and between subterms whose positions are
defined by the states reached during a computation. In particular, TABG can
check that all the subterms reaching a given state are distinct. This
constraint is related to monadic key constraints for XML documents, meaning
that every two distinct positions of a given type have different values. We
prove decidability of the emptiness problem for TABG. This solves, in
particular, the open question of the decidability of emptiness for TAGED. We
further extend our result by allowing global arithmetic constraints for
counting the number of occurrences of some state or the number of different
equivalence classes of subterms (modulo a given flat equational theory)
reaching some state during a computation. We also adapt the model to unranked
ordered terms. As a consequence of our results for TABG, we prove the
decidability of a fragment of the monadic second order logic on trees extended
with predicates for equality and disequality between subtrees, and cardinality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6974</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6974</id><created>2013-02-27</created><updated>2015-02-17</updated><authors><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Talebi</keyname><forenames>M. Sadegh</forenames></author></authors><title>Spectrum Bandit Optimization</title><categories>cs.LG cs.NI math.OC</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of allocating radio channels to links in a wireless
network. Links interact through interference, modelled as a conflict graph
(i.e., two interfering links cannot be simultaneously active on the same
channel). We aim at identifying the channel allocation maximizing the total
network throughput over a finite time horizon. Should we know the average radio
conditions on each channel and on each link, an optimal allocation would be
obtained by solving an Integer Linear Program (ILP). When radio conditions are
unknown a priori, we look for a sequential channel allocation policy that
converges to the optimal allocation while minimizing on the way the throughput
loss or {\it regret} due to the need for exploring sub-optimal allocations. We
formulate this problem as a generic linear bandit problem, and analyze it first
in a stochastic setting where radio conditions are driven by a stationary
stochastic process, and then in an adversarial setting where radio conditions
can evolve arbitrarily. We provide new algorithms in both settings and derive
upper bounds on their regrets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.6990</identifier>
 <datestamp>2013-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.6990</id><created>2013-02-27</created><updated>2013-06-17</updated><authors><author><keyname>Gross</keyname><forenames>David</forenames></author><author><keyname>Walter</keyname><forenames>Michael</forenames></author></authors><title>Stabilizer information inequalities from phase space distributions</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>8 pages. See closely related work arXiv:1302.5453. v3: added appendix
  on discrete phase spaces</comments><journal-ref>J. Math. Phys. 54 , 082201 (2013)</journal-ref><doi>10.1063/1.4818950</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shannon entropy of a collection of random variables is subject to a
number of constraints, the best-known examples being monotonicity and strong
subadditivity. It remains an open question to decide which of these &quot;laws of
information theory&quot; are also respected by the von Neumann entropy of many-body
quantum states. In this article, we consider a toy version of this difficult
problem by analyzing the von Neumann entropy of stabilizer states. We find that
the von Neumann entropy of stabilizer states satisfies all balanced information
inequalities that hold in the classical case. Our argument is built on the fact
that stabilizer states have a classical model, provided by the discrete Wigner
function: The phase-space entropy of the Wigner function corresponds directly
to the von Neumann entropy of the state, which allows us to reduce to the
classical case. Our result has a natural counterpart for multi-mode Gaussian
states, which sheds some light on the general properties of the construction.
We also discuss the relation of our results to recent work by Linden, Ruskai,
and Winter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7007</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7007</id><created>2013-02-27</created><authors><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author><author><keyname>Linares-Barranco</keyname><forenames>Bernabe</forenames></author><author><keyname>Legenstein</keyname><forenames>Robert</forenames></author><author><keyname>Deligeorgis</keyname><forenames>George</forenames></author><author><keyname>Prodromakis</keyname><forenames>Themistoklis</forenames></author></authors><title>Integration of nanoscale memristor synapses in neuromorphic computing
  architectures</title><categories>cs.ET</categories><comments>For a special issue on synaptic electronics</comments><journal-ref>Nanotechnology 24 (2013) 384010</journal-ref><doi>10.1088/0957-4484/24/38/384010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventional neuro-computing architectures and artificial neural networks
have often been developed with no or loose connections to neuroscience. As a
consequence, they have largely ignored key features of biological neural
processing systems, such as their extremely low-power consumption features or
their ability to carry out robust and efficient computation using massively
parallel arrays of limited precision, highly variable, and unreliable
components. Recent developments in nano-technologies are making available
extremely compact and low-power, but also variable and unreliable solid-state
devices that can potentially extend the offerings of availing CMOS
technologies. In particular, memristors are regarded as a promising solution
for modeling key features of biological synapses due to their nanoscale
dimensions, their capacity to store multiple bits of information per element
and the low energy required to write distinct states. In this paper, we first
review the neuro- and neuromorphic-computing approaches that can best exploit
the properties of memristor and-scale devices, and then propose a novel hybrid
memristor-CMOS neuromorphic circuit which represents a radical departure from
conventional neuro-computing approaches, as it uses memristors to directly
emulate the biophysics and temporal dynamics of real synapses. We point out the
differences between the use of memristors in conventional neuro-computing
architectures and the hybrid memristor-CMOS circuit proposed, and argue how
this circuit represents an ideal building block for implementing brain-inspired
probabilistic computing paradigms that are robust to variability and
fault-tolerant by design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7008</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7008</id><created>2013-02-27</created><updated>2013-03-07</updated><authors><author><keyname>Johanson</keyname><forenames>Michael</forenames></author></authors><title>Measuring the Size of Large No-Limit Poker Games</title><categories>cs.GT</categories><report-no>TR13-01</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the field of computational game theory, games are often compared in terms
of their size. This can be measured in several ways, including the number of
unique game states, the number of decision points, and the total number of
legal actions over all decision points. These numbers are either known or
estimated for a wide range of classic games such as chess and checkers. In the
stochastic and imperfect information game of poker, these sizes are easily
computed in &quot;limit&quot; games which restrict the players' available actions, but
until now had only been estimated for the more complicated &quot;no-limit&quot; variants.
In this paper, we describe a simple algorithm for quickly computing the size of
two-player no-limit poker games, provide an implementation of this algorithm,
and present for the first time precise counts of the number of game states,
information sets, actions and terminal nodes in the no-limit poker games played
in the Annual Computer Poker Competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7014</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7014</id><created>2013-02-27</created><updated>2014-08-01</updated><authors><author><keyname>Jiang</keyname><forenames>Jiayang</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Thaler</keyname><forenames>Justin</forenames></author></authors><title>Parallel Peeling Algorithms</title><categories>cs.DS</categories><comments>Appears in SPAA 2014. Minor typo corrections relative to previous
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of several algorithms and data structures can be framed as a
peeling process on a random hypergraph: vertices with degree less than k are
removed until there are no vertices of degree less than k left. The remaining
hypergraph is known as the k-core. In this paper, we analyze parallel peeling
processes, where in each round, all vertices of degree less than k are removed.
It is known that, below a specific edge density threshold, the k-core is empty
with high probability. We show that, with high probability, below this
threshold, only (log log n)/log(k-1)(r-1) + O(1) rounds of peeling are needed
to obtain the empty k-core for r-uniform hypergraphs. Interestingly, we show
that above this threshold, Omega(log n) rounds of peeling are required to find
the non-empty k-core. Since most algorithms and data structures aim to peel to
an empty k-core, this asymmetry appears fortunate. We verify the theoretical
results both with simulation and with a parallel implementation using graphics
processing units (GPUs). Our implementation provides insights into how to
structure parallel peeling algorithms for efficiency in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7025</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7025</id><created>2013-02-27</created><authors><author><keyname>Yang</keyname><forenames>De-Nian</forenames></author><author><keyname>Hung</keyname><forenames>Hui-Ju</forenames></author><author><keyname>Lee</keyname><forenames>Wang-Chien</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Maximizing Acceptance Probability for Active Friending in On-Line Social
  Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Friending recommendation has successfully contributed to the explosive growth
of on-line social networks. Most friending recommendation services today aim to
support passive friending, where a user passively selects friending targets
from the recommended candidates. In this paper, we advocate recommendation
support for active friending, where a user actively specifies a friending
target. To the best of our knowledge, a recommendation designed to provide
guidance for a user to systematically approach his friending target, has not
been explored in existing on-line social networking services. To maximize the
probability that the friending target would accept an invitation from the user,
we formulate a new optimization problem, namely, \emph{Acceptance Probability
Maximization (APM)}, and develop a polynomial time algorithm, called
\emph{Selective Invitation with Tree and In-Node Aggregation (SITINA)}, to find
the optimal solution. We implement an active friending service with SITINA in
Facebook to validate our idea. Our user study and experimental results manifest
that SITINA outperforms manual selection and the baseline approach in solution
quality efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7028</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7028</id><created>2013-02-27</created><authors><author><keyname>Fr&#xe9;chette</keyname><forenames>Alexandre</forenames></author><author><keyname>Shepherd</keyname><forenames>F. Bruce</forenames></author><author><keyname>Thottan</keyname><forenames>Marina K.</forenames></author><author><keyname>Winzer</keyname><forenames>Peter J.</forenames></author></authors><title>Shortest Path versus Multi-Hub Routing in Networks with Uncertain Demand</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of robust network design problems motivated by the need to
scale core networks to meet increasingly dynamic capacity demands. Past work
has focused on designing the network to support all hose matrices (all matrices
not exceeding marginal bounds at the nodes). This model may be too conservative
if additional information on traffic patterns is available. Another extreme is
the fixed demand model, where one designs the network to support peak
point-to-point demands. We introduce a capped hose model to explore a broader
range of traffic matrices which includes the above two as special cases. It is
known that optimal designs for the hose model are always determined by
single-hub routing, and for the fixed- demand model are based on shortest-path
routing. We shed light on the wider space of capped hose matrices in order to
see which traffic models are more shortest path-like as opposed to hub-like. To
address the space in between, we use hierarchical multi-hub routing templates,
a generalization of hub and tree routing. In particular, we show that by adding
peak capacities into the hose model, the single-hub tree-routing template is no
longer cost-effective. This initiates the study of a class of robust network
design (RND) problems restricted to these templates. Our empirical analysis is
based on a heuristic for this new hierarchical RND problem. We also propose
that it is possible to define a routing indicator that accounts for the
strengths of the marginals and peak demands and use this information to choose
the appropriate routing template. We benchmark our approach against other
well-known routing templates, using representative carrier networks and a
variety of different capped hose traffic demands, parameterized by the relative
importance of their marginals as opposed to their point-to-point peak demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7039</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7039</id><created>2013-02-27</created><authors><author><keyname>Taileb</keyname><forenames>Mounira</forenames></author></authors><title>Content Based Image Retrieval System Using NOHIS-tree</title><categories>cs.IR cs.CV cs.DB</categories><comments>6 pages, 10th International Conference on Advances in Mobile
  Computing &amp; Multimedia (MoMM2012)</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-based image retrieval (CBIR) has been one of the most important
research areas in computer vision. It is a widely used method for searching
images in huge databases. In this paper we present a CBIR system called
NOHIS-Search. The system is based on the indexing technique NOHIS-tree. The two
phases of the system are described and the performance of the system is
illustrated with the image database ImagEval. NOHIS-Search system was compared
to other two CBIR systems; the first that using PDDP indexing algorithm and the
second system is that using the sequential search. Results show that
NOHIS-Search system outperforms the two other systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7043</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7043</id><created>2013-02-27</created><authors><author><keyname>Papalexakis</keyname><forenames>Evangelos E.</forenames></author><author><keyname>Mitchell</keyname><forenames>Tom M.</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author><author><keyname>Talukdar</keyname><forenames>Partha Pratim</forenames></author><author><keyname>Murphy</keyname><forenames>Brian</forenames></author></authors><title>Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization</title><categories>stat.ML cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we correlate neural activity in the human brain as it responds to
words, with behavioral data expressed as answers to questions about these same
words? In short, we want to find latent variables, that explain both the brain
activity, as well as the behavioral responses. We show that this is an instance
of the Coupled Matrix-Tensor Factorization (CMTF) problem. We propose
Scoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problem
and produces a sparse latent low-rank subspace of the data. In our experiments,
we find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm
for CMTF, along with a 5 fold increase in sparsity. Moreover, we extend
Scoup-SMT to handle missing data without degradation of performance. We apply
Scoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, human
subjects) tensor and a (nouns, properties) matrix, with coupling along the
nouns dimension. Scoup-SMT is able to find meaningful latent variables, as well
as to predict brain activity with competitive accuracy. Finally, we demonstrate
the generality of Scoup-SMT, by applying it on a Facebook dataset (users,
friends, wall-postings); there, Scoup-SMT spots spammer-like anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7046</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7046</id><created>2013-02-27</created><authors><author><keyname>Davaslioglu</keyname><forenames>Kemal</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Common Rate Maximization in Two-Layer Cellular Radio Systems</title><categories>cs.NI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the common rate maximization problem in two-layer cellular
networks where high-power and low-power base stations are colocated in the same
geographical area. Interference becomes a serious problem when two or more
layers are considered in the same network. For this purpose, power control in
the downlink needs to be used to limit the interference and to fully exploit
the benefits of additional layer deployments.We present an analytical framework
to the common rate maximization problem both with and without maximum power
constraints and propose a heuristic algorithm. We present simulation results
for the proposed approaches in a two-layer network setup and observe a
significant common rate increase compared to single-layer wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7048</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7048</id><created>2013-02-27</created><authors><author><keyname>Davaslioglu</keyname><forenames>Kemal</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Interference-Based Cell Selection in Heterogenous Networks</title><categories>cs.NI</categories><doi>10.1109/ITA.2013.6502931</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous cellular networks provide significant improvements in terms of
increased data rates and cell coverage, and offer reduced user rate starvation.
However, there are important problems to be solved. In this paper, we identify
that the cell selection criterion is an important factor determining the user
rates especially in the uplink transmissions and apply cell breathing to
determine the user and base station assignments. We observe that the proposed
interference-based cell selection algorithm provides better load balancing
among the base stations in the system to improve the uplink user rates. We
present the implementation steps in a typical LTE network and demonstrate the
performance improvements through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7051</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7051</id><created>2013-02-27</created><updated>2013-03-01</updated><authors><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author><author><keyname>Emara</keyname><forenames>Hassan M</forenames></author><author><keyname>Bahgat</keyname><forenames>Ahmed</forenames></author></authors><title>Polyploidy and Discontinuous Heredity Effect on Evolutionary
  Multi-Objective Optimization</title><categories>cs.NE</categories><comments>Corrected the last name of the second author to match his name in the
  paper</comments><report-no>EPM-217-2006</report-no><msc-class>60G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the effect of mimicking discontinuous heredity caused by
carrying more than one chromosome in some living organisms cells in
Evolutionary Multi-Objective Optimization algorithms. In this representation,
the phenotype may not fully reflect the genotype. By doing so we are mimicking
living organisms inheritance mechanism, where traits may be silently carried
for many generations to reappear later. Representations with different number
of chromosomes in each solution vector are tested on different benchmark
problems with high number of decision variables and objectives. A comparison
with Non-Dominated Sorting Genetic Algorithm-II is done on all problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7056</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7056</id><created>2013-02-27</created><authors><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author><author><keyname>Caragea</keyname><forenames>Doina</forenames></author><author><keyname>Hsu</keyname><forenames>William</forenames></author></authors><title>KSU KDD: Word Sense Induction by Clustering in Topic Space</title><categories>cs.CL cs.AI stat.AP stat.ML</categories><msc-class>68T05</msc-class><journal-ref>Proceedings of the 5th International Workshop on Semantic
  Evaluation, pages 367-370, Uppsala, Sweden, July 2010. Association for
  Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our language-independent unsupervised word sense induction
system. This system only uses topic features to cluster different word senses
in their global context topic space. Using unlabeled data, this system trains a
latent Dirichlet allocation (LDA) topic model then uses it to infer the topics
distribution of the test instances. By clustering these topics distributions in
their topic space we cluster them into different senses. Our hypothesis is that
closeness in topic space reflects similarity between different word senses.
This system participated in SemEval-2 word sense induction and disambiguation
task and achieved the second highest V-measure score among all other systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7069</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7069</id><created>2013-02-27</created><authors><author><keyname>Beros</keyname><forenames>Achilles</forenames></author></authors><title>Learning Theory in the Arithmetic Hierarchy</title><categories>math.LO cs.LG cs.LO</categories><comments>19 pages</comments><msc-class>03D80, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the arithmetic complexity of index sets of uniformly computably
enumerable families learnable under different learning criteria. We determine
the exact complexity of these sets for the standard notions of finite learning,
learning in the limit, behaviorally correct learning and anomalous learning in
the limit. In proving the $\Sigma_5^0$-completeness result for behaviorally
correct learning we prove a result of independent interest; if a uniformly
computably enumerable family is not learnable, then for any computable learner
there is a $\Delta_2^0$ enumeration witnessing failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7070</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7070</id><created>2013-02-27</created><authors><author><keyname>Jiang</keyname><forenames>Hong</forenames></author><author><keyname>Mathews</keyname><forenames>Boyd</forenames></author><author><keyname>Wilford</keyname><forenames>Paul</forenames></author></authors><title>Sound localization using compressive sensing</title><categories>cs.SD cs.IT math.IT</categories><comments>9 pages, 7 figures</comments><journal-ref>Proc. SENSORNETS, 2012, pp.159-166</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a sensor network with remote sensor devices, it is important to have a
method that can accurately localize a sound event with a small amount of data
transmitted from the sensors. In this paper, we propose a novel method for
localization of a sound source using compressive sensing. Instead of sampling a
large amount of data at the Nyquist sampling rate in time domain, the acoustic
sensors take compressive measurements integrated in time. The compressive
measurements can be used to accurately compute the location of a sound source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7074</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7074</id><created>2013-02-27</created><updated>2013-09-08</updated><authors><author><keyname>Chen</keyname><forenames>Xuemei</forenames></author><author><keyname>Wang</keyname><forenames>Haichao</forenames></author><author><keyname>Wang</keyname><forenames>Rongrong</forenames></author></authors><title>A null space property approach to compressed sensing with frames</title><categories>cs.IT math.FA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interesting topic in compressive sensing concerns problems of sensing and
recovering signals with sparse representations in a dictionary. In this note,
we study conditions of sensing matrices A for the L1-synthesis method to
accurately recover sparse, or nearly sparse signals in a given dictionary D. In
particular, we propose a dictionary based null space property (D-NSP) which, to
the best of our knowledge, is the first sufficient and necessary condition for
the success of the L1 recovery. This new property is then utilized to detect
some of those dictionaries whose sparse families cannot be compressed
universally. Moreover, when the dictionary is full spark, we show that AD being
NSP, which is well-known to be only sufficient for stable recovery via
L1-synthesis method, is indeed necessary as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7080</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7080</id><created>2013-02-27</created><authors><author><keyname>Emara</keyname><forenames>Hassan M</forenames></author><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author><author><keyname>Bahgat</keyname><forenames>Ahmed</forenames></author></authors><title>Parameter Identification of Induction Motor Using Modified Particle
  Swarm Optimization Algorithm</title><categories>cs.NE</categories><comments>IEEE International Symposium on Industrial Electronics Jul 2008,
  Cambridge, UK</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new technique for induction motor parameter
identification. The proposed technique is based on a simple startup test using
a standard V/F inverter. The recorded startup currents are compared to that
obtained by simulation of an induction motor model. A Modified PSO optimization
is used to find out the best model parameter that minimizes the sum square
error between the measured and the simulated currents. The performance of the
modified PSO is compared with other optimization methods including line search,
conventional PSO and Genetic Algorithms. Simulation results demonstrate the
ability of the proposed technique to capture the true values of the machine
parameters and the superiority of the results obtained using the modified PSO
over other optimization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7081</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7081</id><created>2013-02-27</created><authors><author><keyname>Santhi</keyname><forenames>R.</forenames></author><author><keyname>Priya</keyname><forenames>B.</forenames></author><author><keyname>Nandhini</keyname><forenames>J. M.</forenames></author></authors><title>Review of intelligent tutoring systems using bayesian approach</title><categories>cs.CY</categories><comments>National Conference on &quot;Computational Linguistics and Integrated
  Classical Knowledge - Rediscovered&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With advancement in computer science research on artificial intelligence and
in cognitive psychology research on human learning and performance, the next
generation of computer-based tutoring systems moved beyond the simple
presentation of pages of text or graphics. These new intelligent tutoring
systems (ITSs) called cognitive tutors; incorporated model-tracing technology
which is a cognitive model of student problem solving that captures students
multiple strategies and common misconceptions. Such Intelligent tutoring
systems or Knowledge Based Tutoring Systems can guide learners to progress in
the learning process at their best. This paper deals with the review of various
Intelligent tutoring systems using Bayesian Networks and how Bayesian Networks
can be used for efficient decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7082</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7082</id><created>2013-02-27</created><authors><author><keyname>Meena</keyname><forenames>A.</forenames></author><author><keyname>Raja</keyname><forenames>K.</forenames></author></authors><title>K Means Segmentation of Alzheimers Disease in PET scan datasets: An
  implementation</title><categories>cs.CV cs.NE</categories><comments>International Joint Conference on Advances in Signal Processing and
  Information Technology, SPIT2012</comments><journal-ref>LNICST, ISSN:1867 To 8211 pp. 158 To 162, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Positron Emission Tomography (PET) scan image requires expertise in the
segmentation where clustering algorithm plays an important role in the
automation process. The algorithm optimization is concluded based on the
performance, quality and number of clusters extracted. This paper is proposed
to study the commonly used K Means clustering algorithm and to discuss a brief
list of toolboxes for reproducing and extending works presented in medical
image analysis. This work is compiled using AForge .NET framework in windows
environment and MATrix LABoratory (MATLAB 7.0.1)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7085</identifier>
 <datestamp>2014-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7085</id><created>2013-02-28</created><updated>2014-06-12</updated><authors><author><keyname>Bekos</keyname><forenames>M.</forenames></author><author><keyname>Das</keyname><forenames>A.</forenames></author><author><keyname>Geyer</keyname><forenames>M.</forenames></author><author><keyname>Kaufmann</keyname><forenames>M.</forenames></author><author><keyname>Kobourov</keyname><forenames>S.</forenames></author><author><keyname>Veeramoni</keyname><forenames>S.</forenames></author></authors><title>On Maximum Differential Coloring of Planar Graphs</title><categories>cs.DM math.CO</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the \emph{maximum differential coloring problem}, where the vertices
of an $n$-vertex graph must be labeled with distinct numbers ranging from $1$
to $n$, so that the minimum absolute difference between two labels of any two
adjacent vertices is maximized. As the problem is \NPH for general
graphs~\cite{leung1984}, we consider planar graphs and subclasses thereof. We
initially prove that the maximum differential coloring problem remains \NPH,
even for planar graphs. Then, we present tight bounds for regular caterpillars
and spider graphs. Using these new bounds, we prove that the Miller-Pritikin
labeling scheme~\cite{miller89} for forests is optimal for regular caterpillars
and for spider graphs. Finally, we describe close-to-optimal differential
coloring algorithms for general caterpillars and biconnected triangle-free
outer-planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7088</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7088</id><created>2013-02-28</created><authors><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author></authors><title>Continuous-time Infinite Dynamic Topic Models</title><categories>cs.IR stat.AP stat.ML</categories><comments>Ph.D. dissertation, Kansas State University, 2013</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic models are probabilistic models for discovering topical themes in
collections of documents. In real world applications, these models provide us
with the means of organizing what would otherwise be unstructured collections.
They can help us cluster a huge collection into different topics or find a
subset of the collection that resembles the topical theme found in an article
at hand.
  The first wave of topic models developed were able to discover the prevailing
topics in a big collection of documents spanning a period of time. It was later
realized that these time-invariant models were not capable of modeling 1) the
time varying number of topics they discover and 2) the time changing structure
of these topics. Few models were developed to address this two deficiencies.
The online-hierarchical Dirichlet process models the documents with a time
varying number of topics. It varies the structure of the topics over time as
well. However, it relies on document order, not timestamps to evolve the model
over time. The continuous-time dynamic topic model evolves topic structure in
continuous-time. However, it uses a fixed number of topics over time.
  In this dissertation, I present a model, the continuous-time infinite dynamic
topic model, that combines the advantages of these two models 1) the
online-hierarchical Dirichlet process, and 2) the continuous-time dynamic topic
model. More specifically, the model I present is a probabilistic topic model
that does the following: 1) it changes the number of topics over continuous
time, and 2) it changes the topic structure over continuous-time.
  I compared the model I developed with the two other models with different
setting values. The results obtained were favorable to my model and showed the
need for having a model that has a continuous-time varying number of topics and
topic structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7090</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7090</id><created>2013-02-28</created><authors><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author></authors><title>Adaptive Control in Swarm Robotics</title><categories>cs.SY</categories><report-no>CIS-517-2012</report-no><msc-class>68U20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm robotic systems are mainly inspired by swarms of socials insects and
the collective emergent behavior that arises from their cooperation at the
lower lever. Despite the limited sensory ability, computational power, and
communication means of each swarm member, the swarm as a group manages to
achieve difficult tasks such as searching for food in terrains with obstacles
that individual robots cannot achieve in isolation of the other group members.
Moreover, such tasks are usually achieved without having information sharing
capabilities at the swarm level or having a centralized decision making system.
In this report, I survey the state of the field of applying adaptive control
method to increase swarm robotic systems robustness to the failure of
individual robots, and increase its efficiency in performing its task. A few
techniques for the division of labor problem are briefly presented while one of
them is given in more detail. A discussion of the advantages and disadvantages
of this system is given and suggestions of potential improvements that can be
made to the system are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7096</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7096</id><created>2013-02-28</created><authors><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author></authors><title>Using Artificial Intelligence Models in System Identification</title><categories>cs.NE cs.SY</categories><comments>MSc. Thesis, Electrical Power and Machines Dept. Cairo University,
  Egypt, May 2007</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Intelligence (AI) techniques are known for its ability in tackling
problems found to be unyielding to traditional mathematical methods. A recent
addition to these techniques are the Computational Intelligence (CI) techniques
which, in most cases, are nature or biologically inspired techniques. Different
CI techniques found their way to many control engineering applications,
including system identification, and the results obtained by many researchers
were encouraging. However, most control engineers and researchers used the
basic CI models as is or slightly modified them to match their needs.
Henceforth, the merits of one model over the other was not clear, and full
potential of these models was not exploited.
  In this research, Genetic Algorithm (GA) and Particle Swarm Optimization
(PSO) methods, which are different CI techniques, are modified to best suit the
multimodal problem of system identification. In the first case of GA, an
extension to the basic algorithm, which is inspired from nature as well, was
deployed by introducing redundant genetic material. This extension, which come
in handy in living organisms, did not result in significant performance
improvement to the basic algorithm. In the second case, the Clubs-based PSO
(C-PSO) dynamic neighborhood structure was introduced to replace the basic
static structure used in canonical PSO algorithms. This modification of the
neighborhood structure resulted in significant performance of the algorithm
regarding convergence speed, and equipped it with a tool to handle multimodal
problems.
  To understand the suitability of different GA and PSO techniques in the
problem of system identification, they were used in an induction motor's
parameter identification problem. The results enforced previous conclusions and
showed the superiority of PSO in general over the GA in such a multimodal
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7111</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7111</id><created>2013-02-28</created><authors><author><keyname>Pagnan</keyname><forenames>Ruggero</forenames></author></authors><title>Syllogisms in Rudimentary Linear Logic, Diagrammatically</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a reading of the traditional syllogistics in a fragment of the
propositional intuitionistic multiplicative linear logic and prove that with
respect to a diagrammatic logical calculus that we introduced in a previous
paper, a syllogism is provable in such a fragment if and only if it is
diagrammatically provable. We extend this result to syllogistics with
complemented terms \`a la De Morgan, with respect to a suitable extension of
the diagrammatic reasoning system for the traditional case and a corresponding
reading of such De Morgan style syllogistics in the previously referred to
fragment of linear logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7126</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7126</id><created>2013-02-28</created><updated>2013-07-31</updated><authors><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Bianconi</keyname><forenames>Ginestra</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author><author><keyname>Barthelemy</keyname><forenames>Marc</forenames></author></authors><title>Growing multiplex networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>5 pages, 3 figures (main text) + 12 pages, 6 figures (appendix)</comments><journal-ref>Phys. Rev. Lett. 111, 058701 (2013)</journal-ref><doi>10.1103/PhysRevLett.111.058701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a modeling framework for growing multiplexes where a node can
belong to different networks. We define new measures for multiplexes and we
identify a number of relevant ingredients for modeling their evolution such as
the coupling between the different layers and the arrival time distribution of
nodes. The topology of the multiplex changes significantly in the different
cases under consideration, with effects of the arrival time of nodes on the
degree distribution, average shortest paths and interdependence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7127</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7127</id><created>2013-02-28</created><updated>2015-05-07</updated><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Kusters</keyname><forenames>Vincent</forenames></author></authors><title>The Complexity of Simultaneous Geometric Graph Embedding</title><categories>cs.CG cs.CC</categories><journal-ref>Journal of Graph Algorithms and Applications (JGAA), volume 19,
  number 1, 2015, pages 259--272</journal-ref><doi>10.7155/jgaa.00356</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a collection of planar graphs $G_1,\dots,G_k$ on the same set $V$ of
$n$ vertices, the simultaneous geometric embedding (with mapping) problem, or
simply $k$-SGE, is to find a set $P$ of $n$ points in the plane and a bijection
$\phi: V \to P$ such that the induced straight-line drawings of $G_1,\dots,G_k$
under $\phi$ are all plane.
  This problem is polynomial-time equivalent to weak rectilinear realizability
of abstract topological graphs, which Kyn\v{c}l (doi:10.1007/s00454-010-9320-x)
proved to be complete for $\exists\mathbb{R}$, the existential theory of the
reals. Hence the problem $k$-SGE is polynomial-time equivalent to several other
problems in computational geometry, such as recognizing intersection graphs of
line segments or finding the rectilinear crossing number of a graph.
  We give an elementary reduction from the pseudoline stretchability problem to
$k$-SGE, with the property that both numbers $k$ and $n$ are linear in the
number of pseudolines. This implies not only the $\exists\mathbb{R}$-hardness
result, but also a $2^{2^{\Omega (n)}}$ lower bound on the minimum size of a
grid on which any such simultaneous embedding can be drawn. This bound is
tight. Hence there exists such collections of graphs that can be simultaneously
embedded, but every simultaneous drawing requires an exponential number of bits
per coordinates. The best value that can be extracted from Kyn\v{c}l's proof is
only $2^{2^{\Omega (\sqrt{n})}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7131</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7131</id><created>2013-02-28</created><authors><author><keyname>Madaan</keyname><forenames>Rosy</forenames></author><author><keyname>Sharma</keyname><forenames>A. K.</forenames></author><author><keyname>Dixit</keyname><forenames>Ashutosh</forenames></author></authors><title>Presence Factor-Oriented Blog Summarization</title><categories>cs.IR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research that has been carried out on blogs focused on blog posts only,
ignoring the title of the blog page. Also, in summarization only a set of
representative sentences are extracted. Some analysis has been done and it has
been found that the blog post contains the content that is likely to be related
to the topic of the blog post. Thus, proposed system of summarization makes use
of title contained in a blog page. The approach makes use of the Presence
factor that indicates the presence of each term of the title in each sentence
of the blog post. This is a key feature because it considers those sentences as
more relevant for summarization that contain each of the term present in the
title. The system has been implemented and evaluated experimentally. The system
has shown promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7145</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7145</id><created>2013-02-28</created><authors><author><keyname>Masood</keyname><forenames>Rao Farhat</forenames><affiliation>Member IEEE, MIE</affiliation></author></authors><title>Adaptive Modulation (QPSK, QAM)</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, introduced below are the concepts of digital modulation used
in many communication systems today. Techniques described include quadrature
phase shift keying (QPSK) and quadrature amplitude modulation (QAM) and how
these techniques can be used to increase the capacity and speed of a wireless
network. These modulation techniques are the basis of communications for
systems like cable modems, DSL modems, CDMA, 3G, Wi-Fi* (IEEE 802.11) and
WiMAX* (IEEE 802.16).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7168</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7168</id><created>2013-02-28</created><authors><author><keyname>Graben</keyname><forenames>Peter beim</forenames></author></authors><title>Order effects in dynamic semantics</title><categories>cs.LO</categories><comments>Comment on a target article &quot;A quantum question order model supported
  by empirical tests of an a priori and precise prediction&quot;, Topics in
  Cognitive Science, by Wang and Busemeyer (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In their target article, \citet{WangBusemeyer13} [A quantum question order
model supported by empirical tests of an a priori and precise prediction.
\emph{Topics in Cognitive Science}] discuss question order effects in terms of
incompatible projectors on a Hilbert space. In a similar vein, Blutner recently
presented an orthoalgebraic query language essentially relying on dynamic
update semantics. Here, I shall comment on some interesting analogies between
the different variants of dynamic semantics and generalized quantum theory to
illustrate other kinds of order effects in human cognition, such as belief
revision, the resolution of anaphors, and default reasoning that result from
the crucial non-commutativity of mental operations upon the belief state of a
cognitive agent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7172</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7172</id><created>2013-02-28</created><updated>2013-10-24</updated><authors><author><keyname>Callegari</keyname><forenames>Sergio</forenames></author><author><keyname>Bizzarri</keyname><forenames>Federico</forenames></author></authors><title>Should {\Delta}{\Sigma} Modulators Used in AC Motor Drives be Adapted to
  the Mechanical Load of the Motor?</title><categories>cs.SY</categories><comments>Sample code available at http://pydsm.googlecode.com</comments><journal-ref>Proceeding of the 19th IEEE International Conference on
  Electronics, Circuits and Systems (ICECS), 2012, pp. 849-852</journal-ref><doi>10.1109/ICECS.2012.6463619</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the use of {\Delta}{\Sigma} modulators in ac motor drives,
focusing on the many additional degrees of freedom that this option offers over
Pulse Width Modulation (PWM). Following some recent results, we show that it is
possible to fully adapt the {\Delta}{\Sigma} modulator Noise Transfer Function
(NTF) to the rest of the drive chain and that the approach can be pushed even
to a fine adaptation of the NTF to the specific motor loading condition. We
investigate whether and to what extent the adaptation should be pursued. Using
a representative test case and extensive simulation, we conclude that a mild
adaptation can be beneficial, leading to Signal to Noise Ratio (SNR)
improvements in the order a few dB, while the advantage pushing the adaptation
to the load tracking is likely to be minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7175</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7175</id><created>2013-02-28</created><updated>2013-03-01</updated><authors><author><keyname>van Hasselt</keyname><forenames>Hado</forenames></author></authors><title>Estimating the Maximum Expected Value: An Analysis of (Nested) Cross
  Validation and the Maximum Sample Average</title><categories>stat.ML cs.AI cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the accuracy of the two most common estimators for the maximum
expected value of a general set of random variables: a generalization of the
maximum sample average, and cross validation. No unbiased estimator exists and
we show that it is non-trivial to select a good estimator without knowledge
about the distributions of the random variables. We investigate and bound the
bias and variance of the aforementioned estimators and prove consistency. The
variance of cross validation can be significantly reduced, but not without
risking a large bias. The bias and variance of different variants of cross
validation are shown to be very problem-dependent, and a wrong choice can lead
to very inaccurate estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7180</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7180</id><created>2013-02-28</created><authors><author><keyname>Yi</keyname><forenames>Dong</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Hu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Fast Matching by 2 Lines of Code for Large Scale Face Recognition
  Systems</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method to apply the popular cascade classifier
into face recognition to improve the computational efficiency while keeping
high recognition rate. In large scale face recognition systems, because the
probability of feature templates coming from different subjects is very high,
most of the matching pairs will be rejected by the early stages of the cascade.
Therefore, the cascade can improve the matching speed significantly. On the
other hand, using the nested structure of the cascade, we could drop some
stages at the end of feature to reduce the memory and bandwidth usage in some
resources intensive system while not sacrificing the performance too much. The
cascade is learned by two steps. Firstly, some kind of prepared features are
grouped into several nested stages. And then, the threshold of each stage is
learned to achieve user defined verification rate (VR). In the paper, we take a
landmark based Gabor+LDA face recognition system as baseline to illustrate the
process and advantages of the proposed method. However, the use of this method
is very generic and not limited in face recognition, which can be easily
generalized to other biometrics as a post-processing module. Experiments on the
FERET database show the good performance of our baseline and an experiment on a
self-collected large scale database illustrates that the cascade can improve
the matching speed significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7191</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7191</id><created>2013-02-28</created><authors><author><keyname>Zanetti</keyname><forenames>Marcelo Serrano</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio Juan</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>The Rise and Fall of a Central Contributor: Dynamics of Social
  Organization and Performance in the Gentoo Community</title><categories>cs.SE cs.SI nlin.AO physics.soc-ph</categories><comments>preprint of conference proceedings of the 6th International Workshop
  on Cooperative and Human Aspects of Software Engineering (CHASE 2013) - ICSE
  2013 Workshop</comments><acm-class>D.2.8; D.2.9; K.4.3; K.6.1; K.8.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social organization and division of labor crucially influence the performance
of collaborative software engineering efforts. In this paper, we provide a
quantitative analysis of the relation between social organization and
performance in Gentoo, an Open Source community developing a Linux
distribution. We study the structure and dynamics of collaborations as recorded
in the project's bug tracking system over a period of ten years. We identify a
period of increasing centralization after which most interactions in the
community were mediated by a single central contributor. In this period of
maximum centralization, the central contributor unexpectedly left the project,
thus posing a significant challenge for the community. We quantify how the
rise, the activity as well as the subsequent sudden dropout of this central
contributor affected both the social organization and the bug handling
performance of the Gentoo community. We analyze social organization from the
perspective of network theory and augment our quantitative findings by
interviews with prominent members of the Gentoo community which shared their
personal insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7193</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7193</id><created>2013-02-28</created><authors><author><keyname>Mueller</keyname><forenames>Eike</forenames></author><author><keyname>Guo</keyname><forenames>Xu</forenames></author><author><keyname>Scheichl</keyname><forenames>Robert</forenames></author><author><keyname>Shi</keyname><forenames>Sinan</forenames></author></authors><title>Matrix-free GPU implementation of a preconditioned conjugate gradient
  solver for anisotropic elliptic PDEs</title><categories>cs.NA cs.DC math.NA</categories><comments>18 pages, 7 figures</comments><msc-class>65F10, 65N22, 65Y05, 65Y10</msc-class><acm-class>G.1.3; I.3.1; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many problems in geophysical and atmospheric modelling require the fast
solution of elliptic partial differential equations (PDEs) in &quot;flat&quot; three
dimensional geometries. In particular, an anisotropic elliptic PDE for the
pressure correction has to be solved at every time step in the dynamical core
of many numerical weather prediction models, and equations of a very similar
structure arise in global ocean models, subsurface flow simulations and gas and
oil reservoir modelling. The elliptic solve is often the bottleneck of the
forecast, and an algorithmically optimal method has to be used and implemented
efficiently. Graphics Processing Units have been shown to be highly efficient
for a wide range of applications in scientific computing, and recently
iterative solvers have been parallelised on these architectures. We describe
the GPU implementation and optimisation of a Preconditioned Conjugate Gradient
(PCG) algorithm for the solution of a three dimensional anisotropic elliptic
PDE for the pressure correction in NWP. Our implementation exploits the strong
vertical anisotropy of the elliptic operator in the construction of a suitable
preconditioner. As the algorithm is memory bound, performance can be improved
significantly by reducing the amount of global memory access. We achieve this
by using a matrix-free implementation which does not require explicit storage
of the matrix and instead recalculates the local stencil. Global memory access
can also be reduced by rewriting the algorithm using loop fusion and we show
that this further reduces the runtime on the GPU. We demonstrate the
performance of our matrix-free GPU code by comparing it to a sequential CPU
implementation and to a matrix-explicit GPU code which uses existing libraries.
The absolute performance of the algorithm for different problem sizes is
quantified in terms of floating point throughput and global memory bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7194</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7194</id><created>2013-02-27</created><authors><author><keyname>Li</keyname><forenames>Hongbo</forenames></author></authors><title>Normalization of Polynomials in Algebraic Invariants of
  Three-Dimensional Orthogonal Geometry</title><categories>cs.SC math.RA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In classical invariant theory, the Gr\&quot;obner base of the ideal of syzygies
and the normal forms of polynomials of invariants are two core contents. To
improve the performance of invariant theory in symbolic computing of classical
geometry, advanced invariants are introduced via Clifford product. This paper
addresses and solves the two key problems in advanced invariant theory: the
Gr\&quot;obner base of the ideal of syzygies among advanced invariants, and the
normal forms of polynomials of advanced invariants. These results beautifully
extend the straightening of Young tableaux to advanced invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7195</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7195</id><created>2013-02-28</created><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author></authors><title>Coalitional Game Theoretic Approach for Cooperative Transmission in
  Vehicular Networks</title><categories>cs.GT</categories><comments>accepted by IEEE ICC'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative transmission in vehicular networks is studied by using
coalitional game and pricing in this paper. There are several vehicles and
roadside units (RSUs) in the networks. Each vehicle has a desire to transmit
with a certain probability, which represents its data burtiness. The RSUs can
enhance the vehicles' transmissions by cooperatively relaying the vehicles'
data. We consider two kinds of cooperations: cooperation among the vehicles and
cooperation between the vehicle and RSU. First, vehicles cooperate to avoid
interfering transmissions by scheduling the transmissions of the vehicles in
each coalition. Second, a RSU can join some coalition to cooperate the
transmissions of the vehicles in that coalition. Moreover, due to the mobility
of the vehicles, we introduce the notion of encounter between the vehicle and
RSU to indicate the availability of the relay in space. To stimulate the RSU's
cooperative relaying for the vehicles, the pricing mechanism is applied. A
non-transferable utility (NTU) game is developed to analyze the behaviors of
the vehicles and RSUs. The stability of the formulated game is studied.
Finally, we present and discuss the numerical results for the 2-vehicle and
2-RSU scenario, and the numerical results verify the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7212</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7212</id><created>2013-02-28</created><updated>2013-09-23</updated><authors><author><keyname>Zheng</keyname><forenames>Min</forenames></author><author><keyname>Sun</keyname><forenames>Mingshen</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>DroidAnalytics: A Signature Based Analytic System to Collect, Extract,
  Analyze and Associate Android Malware</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Smartphones and mobile devices are rapidly becoming indispensable devices for
many users. Unfortunately, they also become fertile grounds for hackers to
deploy malware and to spread virus. There is an urgent need to have a &quot;security
analytic &amp; forensic system&quot; which can facilitate analysts to examine, dissect,
associate and correlate large number of mobile applications. An effective
analytic system needs to address the following questions: How to automatically
collect and manage a high volume of mobile malware? How to analyze a zero-day
suspicious application, and compare or associate it with existing malware
families in the database? How to perform information retrieval so to reveal
similar malicious logic with existing malware, and to quickly identify the new
malicious code segment? In this paper, we present the design and implementation
of DroidAnalytics, a signature based analytic system to automatically collect,
manage, analyze and extract android malware. The system facilitates analysts to
retrieve, associate and reveal malicious logics at the &quot;opcode level&quot;. We
demonstrate the efficacy of DroidAnalytics using 150,368 Android applications,
and successfully determine 2,494 Android malware from 102 different families,
with 342 of them being zero-day malware samples from six different families. To
the best of our knowledge, this is the first reported case in showing such a
large Android malware analysis/detection. The evaluation shows the
DroidAnalytics is a valuable tool and is effective in analyzing malware
repackaging and mutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7228</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7228</id><created>2013-02-28</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Pach</keyname><forenames>Janos</forenames></author></authors><title>Applications of a new separator theorem for string graphs</title><categories>math.CO cs.CG cs.DM</categories><comments>7 pages</comments><msc-class>05C62, 05C10, 05C35, 05C55, 05D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intersection graph of curves in the plane is called a string graph.
Matousek almost completely settled a conjecture of the authors by showing that
every string graph of m edges admits a vertex separator of size O(\sqrt{m}\log
m). In the present note, this bound is combined with a result of the authors,
according to which every dense string graph contains a large complete balanced
bipartite graph. Three applications are given concerning string graphs G with n
vertices: (i) if K_t is not a subgraph of G for some t, then the chromatic
number of G is at most (\log n)^{O(\log t)}; (ii) if K_{t,t} is not a subgraph
of G, then G has at most t(\log t)^{O(1)}n edges,; and (iii) a lopsided
Ramsey-type result, which shows that the Erdos-Hajnal conjecture almost holds
for string graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7251</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7251</id><created>2013-02-28</created><updated>2013-05-02</updated><authors><author><keyname>De Clercq</keyname><forenames>Sofie</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author><author><keyname>De Cock</keyname><forenames>Martine</forenames></author><author><keyname>Now&#xe9;</keyname><forenames>Ann</forenames></author></authors><title>Modeling Stable Matching Problems with Answer Set Programming</title><categories>cs.AI cs.LO</categories><comments>26 pages</comments><msc-class>68N17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Stable Marriage Problem (SMP) is a well-known matching problem first
introduced and solved by Gale and Shapley (1962). Several variants and
extensions to this problem have since been investigated to cover a wider set of
applications. Each time a new variant is considered, however, a new algorithm
needs to be developed and implemented. As an alternative, in this paper we
propose an encoding of the SMP using Answer Set Programming (ASP). Our encoding
can easily be extended and adapted to the needs of specific applications. As an
illustration we show how stable matchings can be found when individuals may
designate unacceptable partners and ties between preferences are allowed.
Subsequently, we show how our ASP based encoding naturally allows us to select
specific stable matchings which are optimal according to a given criterion.
Each time, we can rely on generic and efficient off-the-shelf answer set
solvers to find (optimal) stable matchings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7262</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7262</id><created>2013-02-28</created><updated>2014-01-04</updated><authors><author><keyname>Bento</keyname><forenames>Lucila M. S.</forenames></author><author><keyname>Boccardo</keyname><forenames>Davidson</forenames></author><author><keyname>Machado</keyname><forenames>Raphael C. S.</forenames></author><author><keyname>de S&#xe1;</keyname><forenames>Vin&#xed;cius G. Pereira</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>Towards a provably resilient scheme for graph-based watermarking</title><categories>cs.MM cs.CR cs.DS</categories><comments>44 pages, 6 figures. An extended abstract of this paper was published
  in Proceedings of the 39th International Workshop on Graph Theoretic Concepts
  in Computer Science (WG 2013), Lecture Notes in Computer Science 8165 (2013),
  50-63</comments><acm-class>F.2.2; G.2.2; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital watermarks have been considered a promising way to fight software
piracy. Graph-based watermarking schemes encode authorship/ownership data as
control-flow graph of dummy code. In 2012, Chroni and Nikolopoulos developed an
ingenious such scheme which was claimed to withstand attacks in the form of a
single edge removal. We extend the work of those authors in various aspects.
First, we give a formal characterization of the class of graphs generated by
their encoding function. Then, we formulate a linear-time algorithm which
recovers from ill-intentioned removals of $k \leq 2$ edges, therefore proving
their claim. Furthermore, we provide a simpler decoding function and an
algorithm to restore watermarks with an arbitrary number of missing edges
whenever at all possible. By disclosing and improving upon the resilience of
Chroni and Nikolopoulos's watermark, our results reinforce the interest in
regarding it as a possible solution to numerous applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7263</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7263</id><created>2013-02-28</created><updated>2013-03-15</updated><authors><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Herbster</keyname><forenames>Mark</forenames></author><author><keyname>Pasteris</keyname><forenames>Stephen</forenames></author></authors><title>Online Similarity Prediction of Networked Data from Known and Unknown
  Graphs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online similarity prediction problems over networked data. We
begin by relating this task to the more standard class prediction problem,
showing that, given an arbitrary algorithm for class prediction, we can
construct an algorithm for similarity prediction with &quot;nearly&quot; the same mistake
bound, and vice versa. After noticing that this general construction is
computationally infeasible, we target our study to {\em feasible} similarity
prediction algorithms on networked data. We initially assume that the network
structure is {\em known} to the learner. Here we observe that Matrix Winnow
\cite{w07} has a near-optimal mistake guarantee, at the price of cubic
prediction time per round. This motivates our effort for an efficient
implementation of a Perceptron algorithm with a weaker mistake guarantee but
with only poly-logarithmic prediction time. Our focus then turns to the
challenging case of networks whose structure is initially {\em unknown} to the
learner. In this novel setting, where the network structure is only
incrementally revealed, we obtain a mistake-bounded algorithm with a quadratic
prediction time per round.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7264</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7264</id><created>2013-02-28</created><authors><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Lee</keyname><forenames>Heunchul</forenames></author><author><keyname>Hong</keyname><forenames>Young-Jun</forenames></author><author><keyname>Kim</keyname><forenames>Gil</forenames></author></authors><title>A Practical Cooperative Multicell MIMO-OFDMA Network Based on Rank
  Coordination</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions or Wireless Communications, Accepted for
  Publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important challenge of wireless networks is to boost the cell edge
performance and enable multi-stream transmissions to cell edge users.
Interference mitigation techniques relying on multiple antennas and
coordination among cells are nowadays heavily studied in the literature.
Typical strategies in OFDMA networks include coordinated scheduling,
beamforming and power control. In this paper, we propose a novel and practical
type of coordination for OFDMA downlink networks relying on multiple antennas
at the transmitter and the receiver. The transmission ranks, i.e.\ the number
of transmitted streams, and the user scheduling in all cells are jointly
optimized in order to maximize a network utility function accounting for
fairness among users. A distributed coordinated scheduler motivated by an
interference pricing mechanism and relying on a master-slave architecture is
introduced. The proposed scheme is operated based on the user report of a
recommended rank for the interfering cells accounting for the receiver
interference suppression capability. It incurs a very low feedback and backhaul
overhead and enables efficient link adaptation. It is moreover robust to
channel measurement errors and applicable to both open-loop and closed-loop
MIMO operations. A 20% cell edge performance gain over uncoordinated LTE-A
system is shown through system level simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7270</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7270</id><created>2013-02-28</created><updated>2014-02-24</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Klein</keyname><forenames>Philip</forenames></author><author><keyname>Mathieu</keyname><forenames>Claire</forenames></author></authors><title>A polynomial-time approximation scheme for Euclidean Steiner forest</title><categories>cs.CG cs.DS</categories><comments>This version is more recent than that appearing in the FOCS
  proceedings. The partition step has been corrected and the overall
  presentation has been clarified and formalized. This paper has been accepted
  to TALG</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a randomized O(n polylog n)-time approximation scheme for the Steiner
forest problem in the Euclidean plane. For every fixed eps &gt; 0 and given n
terminals in the plane with connection requests between some pairs of
terminals, our scheme finds a (1 + eps)-approximation to the minimum-length
forest that connects every requested pair of terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7274</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7274</id><created>2013-02-28</created><updated>2013-04-11</updated><authors><author><keyname>Arag&#xf3;n</keyname><forenames>Alejandro M.</forenames></author></authors><title>A measure for the impact of research</title><categories>cs.DL</categories><comments>This paper has been published in Scientific Reports, and the final
  version is quite different (not at all compared with this version). I would
  like to withdraw it from the arXiv</comments><msc-class>65C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last few years have seen the proliferation of measures that quantify the
scientific output of researches. Yet, these measures focus on productivity,
thus fostering the &quot;publish or perish&quot; paradigm. This article proposes a
measure that aims at quantifying the impact of research de-emphasizing
productivity, thus providing scientists an alternative, conceivably fairer,
evaluation of their work. Emphasis is placed on the scientist rather than on
quantities that can grow unbounded. The measure, defined initially for
manuscripts, is then extended to researchers and institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7278</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7278</id><created>2013-02-28</created><updated>2013-05-21</updated><authors><author><keyname>Salikhov</keyname><forenames>Kamil</forenames></author><author><keyname>Sacomoto</keyname><forenames>Gustavo</forenames></author><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author></authors><title>Using cascading Bloom filters to improve the memory usage for de Brujin
  graphs</title><categories>cs.DS</categories><comments>12 pages, submitted</comments><acm-class>E.2; J.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  De Brujin graphs are widely used in bioinformatics for processing
next-generation sequencing data. Due to a very large size of NGS datasets, it
is essential to represent de Bruijn graphs compactly, and several approaches to
this problem have been proposed recently. In this work, we show how to reduce
the memory required by the algorithm of [3] that represents de Brujin graphs
using Bloom filters. Our method requires 30% to 40% less memory with respect to
the method of [3], with insignificant impact to construction time. At the same
time, our experiments showed a better query time compared to [3]. This is, to
our knowledge, the best practical representation for de Bruijn graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7280</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7280</id><created>2013-02-28</created><authors><author><keyname>Lock</keyname><forenames>Eric F.</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>Bayesian Consensus Clustering</title><categories>stat.ML cs.LG</categories><comments>32 pages, 13 figures</comments><journal-ref>Bioinformatics 29 (2013) 2610-2616</journal-ref><doi>10.1093/bioinformatics/btt425</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of clustering a set of objects based on multiple sources of data
arises in several modern applications. We propose an integrative statistical
model that permits a separate clustering of the objects for each data source.
These separate clusterings adhere loosely to an overall consensus clustering,
and hence they are not independent. We describe a computationally scalable
Bayesian framework for simultaneous estimation of both the consensus clustering
and the source-specific clusterings. We demonstrate that this flexible approach
is more robust than joint clustering of all data sources, and is more powerful
than clustering each data source separately. This work is motivated by the
integrated analysis of heterogeneous biomedical data, and we present an
application to subtype identification of breast cancer tumor samples using
publicly available data from The Cancer Genome Atlas. Software is available at
http://people.duke.edu/~el113/software.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7283</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7283</id><created>2013-02-28</created><authors><author><keyname>Grais</keyname><forenames>Emad M.</forenames></author><author><keyname>Erdogan</keyname><forenames>Hakan</forenames></author></authors><title>Source Separation using Regularized NMF with MMSE Estimates under GMM
  Priors with Online Learning for The Uncertainties</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method to enforce priors on the solution of the nonnegative
matrix factorization (NMF). The proposed algorithm can be used for denoising or
single-channel source separation (SCSS) applications. The NMF solution is
guided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussian
mixture prior models (GMM) for the source signal. In SCSS applications, the
spectra of the observed mixed signal are decomposed as a weighted linear
combination of trained basis vectors for each source using NMF. In this work,
the NMF decomposition weight matrices are treated as a distorted image by a
distortion operator, which is learned directly from the observed signals. The
MMSE estimate of the weights matrix under GMM prior and log-normal distribution
for the distortion is then found to improve the NMF decomposition results. The
MMSE estimate is embedded within the optimization objective to form a novel
regularized NMF cost function. The corresponding update rules for the new
objectives are derived in this paper. Experimental results show that, the
proposed regularized NMF algorithm improves the source separation performance
compared with using NMF without prior or with other prior models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7289</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7289</id><created>2013-02-28</created><updated>2014-07-10</updated><authors><author><keyname>Yan</keyname><forenames>Feng</forenames></author><author><keyname>Vergne</keyname><forenames>Anais</forenames></author><author><keyname>Martins</keyname><forenames>Philippe</forenames></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames></author></authors><title>Homology-based Distributed Coverage Hole Detection in Wireless Sensor
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homology theory provides new and powerful solutions to address the coverage
problems in wireless sensor networks (WSNs). They are based on algebraic
objects, such as Cech complex and Rips complex. Cech complex gives accurate
information about coverage quality but requires a precise knowledge of the
relative locations of nodes. This assumption is rather strong and hard to
implement in practical deployments. Rips complex provides an approximation of
Cech complex. It is easier to build and does not require any knowledge of nodes
location. This simplicity is at the expense of accuracy. Rips complex can not
always detect all coverage holes. It is then necessary to evaluate its
accuracy. This work proposes to use the proportion of the area of undiscovered
coverage holes as performance criteria. Investigations show that it depends on
the ratio between communication and sensing radii of a sensor. Closed-form
expressions for lower and upper bounds of the accuracy are also derived. For
those coverage holes which can be discovered by Rips complex, a homology-based
distributed algorithm is proposed to detect them. Simulation results are
consistent with the proposed analytical lower bound, with a maximum difference
of 0.5%. Upper bound performance depends on the ratio of communication and
sensing radii. Simulations also show that the algorithm can localize about 99%
coverage holes in about 99% cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7314</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7314</id><created>2013-02-28</created><authors><author><keyname>Galloway</keyname><forenames>Kevin</forenames></author><author><keyname>Sreenath</keyname><forenames>Koushil</forenames></author><author><keyname>Ames</keyname><forenames>Aaron D.</forenames></author><author><keyname>Grizzle</keyname><forenames>J. W.</forenames></author></authors><title>Torque Saturation in Bipedal Robotic Walking through Control Lyapunov
  Function Based Quadratic Programs</title><categories>cs.SY cs.RO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method for directly incorporating user-defined
control input saturations into the calculation of a control Lyapunov function
(CLF)-based walking controller for a biped robot. Previous work by the authors
has demonstrated the effectiveness of CLF controllers for stabilizing periodic
gaits for biped walkers, and the current work expands on those results by
providing a more effective means for handling control saturations. The new
approach, based on a convex optimization routine running at a 1 kHz control
update rate, is useful not only for handling torque saturations but also for
incorporating a whole family of user-defined constraints into the online
computation of a CLF controller. The paper concludes with an experimental
implementation of the main results on the bipedal robot MABEL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1302.7316</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1302.7316</id><created>2013-02-28</created><authors><author><keyname>Childs</keyname><forenames>Andrew M.</forenames></author><author><keyname>Jeffery</keyname><forenames>Stacey</forenames></author><author><keyname>Kothari</keyname><forenames>Robin</forenames></author><author><keyname>Magniez</keyname><forenames>Frederic</forenames></author></authors><title>A Time-Efficient Quantum Walk for 3-Distinctness Using Nested Updates</title><categories>quant-ph cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extension to the quantum walk search framework that facilitates
quantum walks with nested updates. We apply it to give a quantum walk algorithm
for 3-Distinctness with query complexity ~O(n^{5/7}), matching the best known
upper bound (obtained via learning graphs) up to log factors. Furthermore, our
algorithm has time complexity ~O(n^{5/7}), improving the previous ~O(n^{3/4}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0001</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0001</id><created>2013-02-28</created><updated>2014-04-10</updated><authors><author><keyname>Yan</keyname><forenames>Feng</forenames></author><author><keyname>Martins</keyname><forenames>Philippe</forenames></author><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames></author></authors><title>Accuracy of Homology based Coverage Hole Detection for Wireless Sensor
  Networks on Sphere</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1302.7289</comments><doi>10.1109/TWC.2014.2314106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homology theory has attracted great attention because it can provide novel
and powerful solutions to address coverage problems in wireless sensor
networks. They usually use an easily computable algebraic object, Rips complex,
to detect coverage holes. But Rips complex may miss some coverage holes in some
cases. In this paper, we investigate homology-based coverage hole detection for
wireless sensor networks on sphere. The situations when Rips complex may miss
coverage holes are first presented. Then we choose the proportion of the area
of coverage holes missed by Rips complex as a metric to evaluate the accuracy
of homology-based coverage hole detection approaches. Three different cases are
considered for the computation of accuracy. For each case, closed-form
expressions for lower and upper bounds of the accuracy are derived. Simulation
results are well consistent with the analytical lower and upper bounds, with
maximum differences of 0.5% and 3% respectively. Furthermore, it is shown that
the radius of sphere has little impact on the accuracy if it is much larger
than communication and sensing radii of each sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0002</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0002</id><created>2013-02-28</created><updated>2013-12-25</updated><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>On calculation of the interweight distribution of an equitable partition</title><categories>math.CO cs.DM</categories><comments>12 pages. V2: revised; 3'd formula for W added; new examples. V3:
  minor revision; ISI^{-1} replaced by S^T</comments><msc-class>05B99</msc-class><journal-ref>J. Algebr. Comb. 40(2) 2014, 373-386</journal-ref><doi>10.1007/s10801-013-0492-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive recursive and direct formulas for the interweight distribution of
an equitable partition of a hypercube. The formulas involve a three-variable
generalization of the Krawtchouk polynomials. Keywords: equitable partition;
regular partition; partition design; strong distance invariance; interweight
distribution; distance distribution; Krawtchouk polynomial
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0004</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0004</id><created>2013-02-28</created><updated>2015-05-04</updated><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author><author><keyname>Potapov</keyname><forenames>Vladimir</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>Constructions of transitive latin hypercubes</title><categories>cs.IT math.CO math.IT</categories><comments>16 pages. V2: the paper has been completely rewritten; the previous
  version can contain incorrect statements</comments><msc-class>05B15, 94B25</msc-class><journal-ref>Eur. J. Comb. 54, 2016, 51-64</journal-ref><doi>10.1016/j.ejc.2015.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function $f:\{0,...,q-1\}^n\to\{0,...,q-1\}$ invertible in each argument is
called a latin hypercube. A collection $(\pi_0,\pi_1,...,\pi_n)$ of
permutations of $\{0,...,q-1\}$ is called an autotopism of a latin hypercube
$f$ if $\pi_0f(x_1,...,x_n)=f(\pi_1x_1,...,\pi_n x_n)$ for all $x_1$, ...,
$x_n$. We call a latin hypercube isotopically transitive (topolinear) if its
group of autotopisms acts transitively (regularly) on all $q^n$ collections of
argument values. We prove that the number of nonequivalent topolinear latin
hypercubes growths exponentially with respect to $\sqrt{n}$ if $q$ is even and
exponentially with respect to $n^2$ if $q$ is divisible by a square. We show a
connection of the class of isotopically transitive latin squares with the class
of G-loops, known in noncommutative algebra, and establish the existence of a
topolinear latin square that is not a group isotope. We characterize the class
of isotopically transitive latin hypercubes of orders $q=4$ and $q=5$.
Keywords: transitive code, propelinear code, latin square, latin hypercube,
autotopism, G-loop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0018</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0018</id><created>2013-02-28</created><authors><author><keyname>Aghasi</keyname><forenames>Alireza</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Sparse Shape Reconstruction</title><categories>math.FA cs.CV math-ph math.DG math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new shape-based image reconstruction technique
applicable to a large class of imaging problems formulated in a variational
sense. Given a collection of shape priors (a shape dictionary), we define our
problem as choosing the right elements and geometrically composing them through
basic set operations to characterize desired regions in the image. This
combinatorial problem can be relaxed and then solved using classical descent
methods. The main component of this relaxation is forming certain compactly
supported functions which we call &quot;knolls&quot;, and reformulating the shape
representation as a basis expansion in terms of such functions. To select
suitable elements of the dictionary, our problem ultimately reduces to solving
a nonlinear program with sparsity constraints. We provide a new sparse
nonlinear reconstruction technique to approach this problem. The performance of
proposed technique is demonstrated with some standard imaging problems
including image segmentation, X-ray tomography and diffusive tomography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0023</identifier>
 <datestamp>2015-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0023</id><created>2013-02-28</created><authors><author><keyname>Ibrahim</keyname><forenames>Lamiaa Fattouh</forenames></author><author><keyname>Harby</keyname><forenames>Manal El</forenames></author></authors><title>Enhancing Clustering Algorithm to Plan Efficient Mobile Network</title><categories>cs.NI</categories><comments>7 Pages, 14 Figures. arXiv admin note: substantial text overlap with
  arXiv:1302.6602</comments><journal-ref>Lamiaa Fattouh Ibrahim and Manal El Harby. Article: Enhancing
  Clustering Algorithm to Plan Efficient Mobile Network. International Journal
  of Computer Applications 59(18):18-24, December 2012</journal-ref><doi>10.5120/9648-4401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development in mobile network effective network planning tool
is needed to satisfy the need of customers. However, deciding upon the optimum
placement for the base stations (BS) to achieve best services while reducing
the cost is a complex task requiring vast computational resource. This paper
addresses antenna placement problem or the cell planning problem, involves
locating and configuring infrastructure for mobile networks. The Cluster
Partitioning Around Medoids (PAM) original algorithm has been modified and a
new algorithm M-PAM (Modified-Partitioning Around Medoids) has been proposed by
the authors in a recent work. In the present paper, the M-PAM algorithm is
modified and a new algorithm CWN-PAM (Clustering with Weighted
Node-Partitioning Around Medoids) has been proposed to satisfy the requirements
and constraints. Implementation of this algorithm to a real case study is
presented. Results demonstrate the effectiveness and flexibility of the
modifying algorithm in tackling the important problem of mobile network
planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0031</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0031</id><created>2013-02-28</created><authors><author><keyname>Manita</keyname><forenames>Anatoly</forenames></author></authors><title>Time Scales in Probabilistic Models of Wireless Sensor Networks</title><categories>math.PR cs.DC cs.MA math-ph math.MP</categories><comments>31 pages</comments><msc-class>60K35, 60J27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a stochastic model of clock synchronization in a wireless network
consisting of N sensors interacting with one dedicated accurate time server.
For large N we find an estimate of the final time sychronization error for
global and relative synchronization. Main results concern a behavior of the
network on different time scales $t=t_N \to \infty$, $N \to \infty$. We discuss
existence of phase transitions and find exact time scales on which an effective
clock synchronization of the system takes place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0035</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0035</id><created>2013-02-28</created><updated>2013-03-05</updated><authors><author><keyname>Lin</keyname><forenames>Min Chih</forenames></author><author><keyname>Mizrahi</keyname><forenames>Michel J.</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>An $O^*(1.1939^n)$ time algorithm for minimum weighted dominating
  induced matching</title><categories>cs.DS cs.DM</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Say that an edge of a graph $G$ dominates itself and every other edge
adjacent to it. An edge dominating set of a graph $G=(V,E)$ is a subset of
edges $E' \subseteq E$ which dominates all edges of $G$. In particular, if
every edge of $G$ is dominated by exactly one edge of $E'$ then $E'$ is a
dominating induced matching. It is known that not every graph admits a
dominating induced matching, while the problem to decide if it does admit it is
NP-complete. In this paper we consider the problems of finding a minimum
weighted dominating induced matching, if any, and counting the number of
dominating induced matchings of a graph with weighted edges. We describe an
exact algorithm for general graphs that runs in $O^*(1.1939^n)$ time and
polynomial (linear) space. This improves over any existing exact algorithm for
the problems in consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0038</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0038</id><created>2013-02-28</created><updated>2015-07-24</updated><authors><author><keyname>Kamble</keyname><forenames>Vijay</forenames></author><author><keyname>Walrand</keyname><forenames>Jean</forenames></author></authors><title>Approximately Optimal Scheduling of an M/G/1 Queue with Heavy Tails</title><categories>math.PR cs.PF</categories><journal-ref>Queueing Systems, Volume 80, Issue 3, pp 261-271, 2015</journal-ref><doi>10.1007/s11134-015-9435-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributions with a heavy tail are difficult to estimate. If the design of a
scheduling policy is sensitive to the details of heavy tail distributions of
the service times, an approximately optimal solution is difficult to obtain.
This paper shows that the optimal scheduling of an M/G/1 queue with heavy
tailed service times does not present this difficulty and that an approximately
optimal strategy can be derived by truncating the distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0041</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0041</id><created>2013-02-28</created><authors><author><keyname>Madelaine</keyname><forenames>Florent</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>QCSP on partially reflexive cycles - the wavy line of tractability</title><categories>cs.CC</categories><comments>Extended abstract at CSR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the (non-uniform) quantified constraint satisfaction problem QCSP(H)
as H ranges over partially reflexive cycles. We obtain a complexity-theoretic
dichotomy: QCSP(H) is either in NL or is NP-hard. The separating conditions are
somewhat esoteric hence the epithet &quot;wavy line of tractability&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0044</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0044</id><created>2013-02-28</created><updated>2013-04-14</updated><authors><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author></authors><title>A connection between concurrency and language theory</title><categories>cs.LO cs.FL</categories><msc-class>68Q70, 06F99, 08A70</msc-class><acm-class>F.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that three fixed point structures equipped with (sequential)
composition, a sum operation, and a fixed point operation share the same valid
equations. These are the theories of (context-free) languages, (regular) tree
languages, and simulation equivalence classes of (regular) synchronization
trees (or processes). The results reveal a close relationship between classical
language theory and process algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0045</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0045</id><created>2013-02-28</created><updated>2013-03-10</updated><authors><author><keyname>State</keyname><forenames>Bogdan</forenames></author><author><keyname>Park</keyname><forenames>Patrick</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Mejova</keyname><forenames>Yelena</forenames></author><author><keyname>Macy</keyname><forenames>Michael</forenames></author></authors><title>The Mesh of Civilizations and International Email Flows</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 3 figures</comments><acm-class>H.3.5; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In The Clash of Civilizations, Samuel Huntington argued that the primary axis
of global conflict was no longer ideological or economic but cultural and
religious, and that this division would characterize the &quot;battle lines of the
future.&quot; In contrast to the &quot;top down&quot; approach in previous research focused on
the relations among nation states, we focused on the flows of interpersonal
communication as a bottom-up view of international alignments. To that end, we
mapped the locations of the world's countries in global email networks to see
if we could detect cultural fault lines. Using IP-geolocation on a worldwide
anonymized dataset obtained from a large Internet company, we constructed a
global email network. In computing email flows we employ a novel rescaling
procedure to account for differences due to uneven adoption of a particular
Internet service across the world. Our analysis shows that email flows are
consistent with Huntington's thesis. In addition to location in Huntington's
&quot;civilizations,&quot; our results also attest to the importance of both cultural and
economic factors in the patterning of inter-country communication ties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0050</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0050</id><created>2013-02-28</created><authors><author><keyname>Hamdi</keyname><forenames>Maziyar</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author><author><keyname>Yin</keyname><forenames>George</forenames></author></authors><title>Tracking the Empirical Distribution of a Markov-modulated
  Duplication-Deletion Random Graph</title><categories>cs.IT math.IT</categories><comments>34 pages, 8 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a Markov-modulated duplication-deletion random graph
where at each time instant, one node can either join or leave the network; the
probabilities of joining or leaving evolve according to the realization of a
finite state Markov chain. The paper comprises of 2 results. First, motivated
by social network applications, we analyze the asymptotic behavior of the
degree distribution of the Markov-modulated random graph. Using the asymptotic
degree distribution, an expression is obtained for the delay in searching such
graphs. Second, a stochastic approximation algorithm is presented to track
empirical degree distribution as it evolves over time. The tracking performance
of the algorithm is analyzed in terms of mean square error and a functional
central limit theorem is presented for the asymptotic tracking error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0058</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0058</id><created>2013-02-28</created><updated>2013-03-04</updated><authors><author><keyname>Esfahani</keyname><forenames>Mohammad Shahrokh</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author></authors><title>A Cooperative MARC Scheme Using Analogue Network Coding to Achieve
  Second-Order Diversity</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multiple access relay channel (MARC) is considered in which an
analogue-like network coding is implemented in the relay node. This analogue
coding is a simple addition of the received signals at the relay node. Using
&quot;nulling detection&quot; structure employed in V-BLAST receiver, we propose a
detection scheme in the destination which is able to provide a diversity order
of two for all users. We analytically evaluate the performance of our proposed
scheme for the MARC with two users where tight upper bounds for both uncoded
and Convolutionally coded transmission blocks are provided. We verify our
analytical evaluations by simulations and compare the results with those of
noncooperative transmission and Alamouti's scheme for the same power and rate
transmission. Our results indicate that while our proposed scheme shows a
comparable performance compared to the Alamouti's scheme, it substantially
outperforms the non-cooperate transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0066</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0066</id><created>2013-02-28</created><authors><author><keyname>Klotzb&#xfc;cher</keyname><forenames>Markus</forenames></author><author><keyname>Biggs</keyname><forenames>Geoffrey</forenames></author><author><keyname>Bruyninckx</keyname><forenames>Herman</forenames></author></authors><title>Pure Coordination using the Coordinator--Configurator Pattern</title><categories>cs.RO</categories><comments>Presented at DSLRob 2012 (arXiv:cs/1302.5082)</comments><report-no>DSLRob/2012/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work-in-progress paper reports on our efforts to improve different
aspects of coordination in complex, component-based robotic systems.
Coordination is a system level aspect concerned with commanding, configuring
and monitoring functional computations such that the system as a whole behaves
as desired. To that end a variety of models such as Petri-nets or Finite State
Machines may be utilized. These models specify actions to be executed, such as
invoking operations or configuring components to achieve a certain goal.
  This traditional approach has several disadvantages related to loss of
reusability of coordination models due to coupling with platform-specific
functionality, non-deterministic temporal behavior and limited robustness as a
result of executing platform operations within the context of the coordinator.
  To avoid these shortcomings, we propose to split this &quot;rich&quot; coordinator into
a Pure Coordinator and a Configurator. Although the coordinator remains in
charge of commanding and reacting, the execution of actions is deferred to the
Configurator. This pattern, called &quot;Coordinator-Configurator&quot;, is implemented
as a novel Configurator domain specific language that can be used together with
any model of coordination. We illustrate the approach by refactoring an
existing application that realizes a safe haptic coupling of two youBot mobile
manipulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0070</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0070</id><created>2013-02-28</created><updated>2013-04-28</updated><authors><author><keyname>Yang</keyname><forenames>Shengtian</forenames></author></authors><title>Entropy Distance</title><categories>cs.IT math.CO math.IT</categories><comments>Version 1.0.0, no. 201304290852, 18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the approach of random linear codes, a new distance in the
vector space over a finite field is defined as the logarithm of the &quot;surface
area&quot; of a Hamming ball with radius being the corresponding Hamming distance.
It is named entropy distance because of its close relation with entropy
function. It is shown that entropy distance is a metric for a non-binary field
and a pseudometric for the binary field. The entropy distance of a linear code
is defined to be the smallest entropy distance between distinct codewords of
the code. Analogues of the Gilbert bound, the Hamming bound, and the Singleton
bound are derived for the largest size of a linear code given the length and
entropy distance of the code. Furthermore, as an important property related to
lossless joint source-channel coding, the entropy distance of a linear encoder
is defined. Very tight upper and lower bounds are obtained for the largest
entropy distance of a linear encoder with given dimensions of input and output
vector spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0071</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0071</id><created>2013-02-28</created><authors><author><keyname>Mogavero</keyname><forenames>Fabio</forenames><affiliation>Universit&#xe0; degli Studi di Napoli Federico II</affiliation></author><author><keyname>Murano</keyname><forenames>Aniello</forenames><affiliation>Universit&#xe0; degli Studi di Napoli Federico II</affiliation></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames><affiliation>Rice University</affiliation></author></authors><title>Proceedings 1st International Workshop on Strategic Reasoning</title><categories>cs.GT cs.LO cs.MA</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013</journal-ref><doi>10.4204/EPTCS.112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the 1st International Workshop on
Strategic Reasoning 2013 (SR 2013), held in Rome (Italy), March 1617, 2013. The
SR workshop aims to bring together researchers, possibly with different
backgrounds, working on various aspects of strategic reasoning in computer
science, both from a theoretical and a practical point of view. This year SR
has hosted four outstanding invited talks by Krishnendu Chatterjee, Alessio R.
Lomuscio, Jean-Francois Raskin, and Michael Wooldridge. Moreover, the program
committee selected 13 papers among the 23 contributions submitted. Almost all
of them have been revised by three reviews and the contributions have been
selected according to quality and relevance to the topics of the workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0073</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0073</id><created>2013-02-28</created><updated>2013-03-19</updated><authors><author><keyname>Kartoun</keyname><forenames>Uri</forenames></author></authors><title>A Method for Comparing Hedge Funds</title><categories>q-fin.ST cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents new machine learning methods: signal composition, which
classifies time-series regardless of length, type, and quantity; and
self-labeling, a supervised-learning enhancement. The paper describes further
the implementation of the methods on a financial search engine system to
identify behavioral similarities among time-series representing monthly returns
of 11,312 hedge funds operated during approximately one decade (2000 - 2010).
The presented approach of cross-category and cross-location classification
assists the investor to identify alternative investments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0076</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0076</id><created>2013-02-28</created><updated>2013-03-19</updated><authors><author><keyname>Kartoun</keyname><forenames>Uri</forenames></author></authors><title>Bio-Signals-based Situation Comparison Approach to Predict Pain</title><categories>stat.AP cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a time-series-based classification approach to identify
similarities between bio-medical-based situations. The proposed approach allows
classifying collections of time-series representing bio-medical measurements,
i.e., situations, regardless of the type, the length and the quantity of the
time-series a situation comprised of.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0084</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0084</id><created>2013-02-28</created><updated>2013-03-08</updated><authors><author><keyname>Forbes</keyname><forenames>Michael A.</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>Explicit Noether Normalization for Simultaneous Conjugation via
  Polynomial Identity Testing</title><categories>cs.CC cs.SC math.AG math.RA</categories><comments>30 pages; updated to reflect that Theorem 4.1 (of the first version)
  is already known, as pointed out to us by Josh Grochow</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mulmuley recently gave an explicit version of Noether's Normalization lemma
for ring of invariants of matrices under simultaneous conjugation, under the
conjecture that there are deterministic black-box algorithms for polynomial
identity testing (PIT). He argued that this gives evidence that constructing
such algorithms for PIT is beyond current techniques. In this work, we show
this is not the case. That is, we improve Mulmuley's reduction and
correspondingly weaken the conjecture regarding PIT needed to give explicit
Noether Normalization. We then observe that the weaker conjecture has recently
been nearly settled by the authors, who gave quasipolynomial size hitting sets
for the class of read-once oblivious algebraic branching programs (ROABPs).
This gives the desired explicit Noether Normalization unconditionally, up to
quasipolynomial factors.
  As a consequence of our proof we give a deterministic parallel
polynomial-time algorithm for deciding if two matrix tuples have intersecting
orbit closures, under simultaneous conjugation.
  We also study the strength of conjectures that Mulmuley requires to obtain
similar results as ours. We prove that his conjectures are stronger, in the
sense that the computational model he needs PIT algorithms for is equivalent to
the well-known algebraic branching program (ABP) model, which is provably
stronger than the ROABP model.
  Finally, we consider the depth-3 diagonal circuit model as defined by Saxena,
as PIT algorithms for this model also have implications in Mulmuley's work.
Previous work have given quasipolynomial size hitting sets for this model. In
this work, we give a much simpler construction of such hitting sets, using
techniques of Shpilka and Volkovich.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0088</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0088</id><created>2013-03-01</created><updated>2013-03-11</updated><authors><author><keyname>Shende</keyname><forenames>Nirmal</forenames></author><author><keyname>Gurbuz</keyname><forenames>Ozgur</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Half-Duplex or Full-Duplex Relaying: A Capacity Analysis under
  Self-Interference</title><categories>cs.IT math.IT</categories><comments>New references added and some typos have been corrected. 6 Pages, 5
  Figures. Accepted for publication in the CISS-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper multi-antenna half-duplex and full-duplex relaying are compared
from the perspective of achievable rates. Full-duplexing operation requires
additional resources at the relay such as antennas and RF chains for
self-interference cancellation. Using a practical model for the residual
self-interference, full-duplex achievable rates and degrees of freedom are
computed for the cases for which the relay has the same number of antennas or
the same number of RF chains as in the half-duplex case, and compared with
their half-duplex counterparts. It is shown that power scaling at the relay is
necessary to maximize the the degrees of freedom in the full-duplex mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0089</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0089</id><created>2013-03-01</created><authors><author><keyname>Havemann</keyname><forenames>Frank</forenames></author><author><keyname>Heinz</keyname><forenames>Michael</forenames></author><author><keyname>Gl&#xe4;ser</keyname><forenames>Jochen</forenames></author><author><keyname>Struck</keyname><forenames>Alexander</forenames></author></authors><title>Estimating Thematic Similarity of Scholarly Papers with Their Resistance
  Distance in an Electric Network Model</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>7 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We calculate resistance distances between papers in a nearly bipartite
citation network of 492 papers and the sources cited by them. We validate that
this is a realistic measure of thematic distance if each citation link has an
electric resistance equal to the geometric mean of the number of the paper's
references and the citation number of the cited source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0093</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0093</id><created>2013-03-01</created><authors><author><keyname>Kazienko</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Musial</keyname><forenames>Katarzyna</forenames></author><author><keyname>Kajdanowicz</keyname><forenames>Tomasz</forenames></author></authors><title>Multidimensional Social Network in the Social Recommender System</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>social recommender system;Multidimensional social network (MSN);Web
  2.0;multi-layered social network;multimedia sharing system (MSS);recommender
  system;social network analysis</comments><msc-class>91D30</msc-class><acm-class>H.3.4; H.3.5</acm-class><journal-ref>Kazienko, P.; Musial, K.; Kajdanowicz, T.; , &quot;Multidimensional
  Social Network in the Social Recommender System,&quot; Systems, Man and
  Cybernetics, Part A: Systems and Humans, IEEE Transactions on , vol.41, no.4,
  pp.746-759, July 2011</journal-ref><doi>10.1109/TSMCA.2011.2132707</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All online sharing systems gather data that reflects users' collective
behaviour and their shared activities. This data can be used to extract
different kinds of relationships, which can be grouped into layers, and which
are basic components of the multidimensional social network proposed in the
paper. The layers are created on the basis of two types of relations between
humans, i.e. direct and object-based ones which respectively correspond to
either social or semantic links between individuals. For better understanding
of the complexity of the social network structure, layers and their profiles
were identified and studied on two, spanned in time, snapshots of the Flickr
population. Additionally, for each layer, a separate strength measure was
proposed. The experiments on the Flickr photo sharing system revealed that the
relationships between users result either from semantic links between objects
they operate on or from social connections of these users. Moreover, the
density of the social network increases in time. The second part of the study
is devoted to building a social recommender system that supports the creation
of new relations between users in a multimedia sharing system. Its main goal is
to generate personalized suggestions that are continuously adapted to users'
needs depending on the personal weights assigned to each layer in the
multidimensional social network. The conducted experiments confirmed the
usefulness of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0095</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0095</id><created>2013-03-01</created><authors><author><keyname>Kajdanowicz</keyname><forenames>Tomasz</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Doskocz</keyname><forenames>Piotr</forenames></author></authors><title>Label-dependent Feature Extraction in Social Networks for Node
  Classification</title><categories>cs.SI cs.LG</categories><comments>feature extraction, label-dependent features, classification, social
  network analysis, AMD social network</comments><msc-class>91D30, 68T05, 68T10</msc-class><acm-class>I.2.8; I.2.11</acm-class><journal-ref>Kajdanowicz T., Kazienko P., Doskocz P.: Label-dependent Feature
  Extraction in Social Networks for Node Classification. Lecture Notes in
  Artificial Intelligence LNAI 6430, Springer, 2010, pp. 89-102</journal-ref><doi>10.1007/978-3-642-16567-2_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method of feature extraction in the social network for within-network
classification is proposed in the paper. The method provides new features
calculated by combination of both: network structure information and class
labels assigned to nodes. The influence of various features on classification
performance has also been studied. The experiments on real-world data have
shown that features created owing to the proposed method can lead to
significant improvement of classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0136</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0136</id><created>2013-03-01</created><authors><author><keyname>Kamthe</keyname><forenames>Sanket</forenames></author><author><keyname>Gopinath</keyname><forenames>Smriti</forenames></author></authors><title>Optimal Threshold Scheduler for Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional wireless schedulers of Unicast and Multicast either fulfill
multiuser diversity or broadcast gain, but not both together. To achieve
optimal system throughput we need a scheduler that exploits both the multiuser
diversity gain and the Multicasting gain simultaneously. We first propose a new
medianthreshold scheduler that selects all users having instantaneous SNR above
the median value for transmission. The system rate equation for the proposed
scheduler is also derived. We then optimize the median-threshold so that it
performs well over an entire SNR range.With the help of simulation results we
compare performance of the proposed scheduler with schedulers like Unicast,
Multicast and other Opportunistic Multicast Schedulers (OMS) which are the best
schedulers in terms of throughput and show that the proposed optimized
threshold scheme outperforms all of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0140</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0140</id><created>2013-03-01</created><authors><author><keyname>Vaits</keyname><forenames>Nina</forenames></author><author><keyname>Moroshko</keyname><forenames>Edward</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Second-Order Non-Stationary Online Learning for Regression</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of a learner, in standard online learning, is to have the cumulative
loss not much larger compared with the best-performing function from some fixed
class. Numerous algorithms were shown to have this gap arbitrarily close to
zero, compared with the best function that is chosen off-line. Nevertheless,
many real-world applications, such as adaptive filtering, are non-stationary in
nature, and the best prediction function may drift over time. We introduce two
novel algorithms for online regression, designed to work well in non-stationary
environment. Our first algorithm performs adaptive resets to forget the
history, while the second is last-step min-max optimal in context of a drift.
We analyze both algorithms in the worst-case regret framework and show that
they maintain an average loss close to that of the best slowly changing
sequence of linear functions, as long as the cumulative drift is sublinear. In
addition, in the stationary case, when no drift occurs, our algorithms suffer
logarithmic regret, as for previous algorithms. Our bounds improve over the
existing ones, and simulations demonstrate the usefulness of these algorithms
compared with other state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0141</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0141</id><created>2013-03-01</created><authors><author><keyname>Che</keyname><forenames>Pak Hou</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>Routing for Security in Networks with Adversarial Nodes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of secure unicast transmission between two nodes in a
directed graph, where an adversary eavesdrops/jams a subset of nodes. This
adversarial setting is in contrast to traditional ones where the adversary
controls a subset of links. In particular, we study, in the main, the class of
routing-only schemes (as opposed to those allowing coding inside the network).
Routing-only schemes usually have low implementation complexity, yet a
characterization of the rates achievable by such schemes was open prior to this
work. We first propose an LP based solution for secure communication against
eavesdropping, and show that it is information-theoretically rate-optimal among
all routing-only schemes. The idea behind our design is to balance information
flow in the network so that no subset of nodes observe &quot;too much&quot; information.
Interestingly, we show that the rates achieved by our routing-only scheme are
always at least as good as, and sometimes better, than those achieved by
&quot;na\&quot;ive&quot; network coding schemes (i.e. the rate-optimal scheme designed for the
traditional scenario where the adversary controls links in a network rather
than nodes.) We also demonstrate non-trivial network coding schemes that
achieve rates at least as high as (and again sometimes better than) those
achieved by our routing schemes, but leave open the question of characterizing
the optimal rate-region of the problem under all possible coding schemes. We
then extend these routing-only schemes to the adversarial node-jamming
scenarios and show similar results. During the journey of our investigation, we
also develop a new technique that has the potential to derive non-trivial
bounds for general secure-communication schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0152</identifier>
 <datestamp>2014-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0152</id><created>2013-03-01</created><authors><author><keyname>Soltanalian</keyname><forenames>Mojtaba</forenames></author><author><keyname>Stoica</keyname><forenames>Petre</forenames></author></authors><title>Designing Unimodular Codes via Quadratic Optimization is not Always Hard</title><categories>cs.SY cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 62, no. 5, pp.
  1221-1234, 2014</journal-ref><doi>10.1109/TSP.2013.2296883</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-hard problem of optimizing a quadratic form over the unimodular vector
set arises in radar code design scenarios as well as other active sensing and
communication applications. To tackle this problem (which we call unimodular
quadratic programming (UQP)), several computational approaches are devised and
studied. A specialized local optimization scheme for UQP is introduced and
shown to yield superior results compared to general local optimization methods.
Furthermore, a \textbf{m}onotonically \textbf{er}ror-bound \textbf{i}mproving
\textbf{t}echnique (MERIT) is proposed to obtain the global optimum or a local
optimum of UQP with good sub-optimality guarantees. The provided sub-optimality
guarantees are case-dependent and generally outperform the $\pi/4$
approximation guarantee of semi-definite relaxation. Several numerical examples
are presented to illustrate the performance of the proposed method. The
examples show that for cases including several matrix structures used in radar
code design, MERIT can solve UQP efficiently in the sense of sub-optimality
guarantee and computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0154</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0154</id><created>2013-03-01</created><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Robust Estimation of Optical Phase Varying as a Continuous Resonant
  Process</title><categories>math.OC cs.SY quant-ph</categories><comments>5 pages, 10 figures, Proceedings of the 2013 Multi-Conference on
  Systems and Control</comments><doi>10.1109/CCA.2013.6662807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that adaptive homodyne estimation of continuously varying
optical phase provides superior accuracy in the phase estimate as compared to
adaptive or non-adaptive static estimation. However, most phase estimation
schemes rely on precise knowledge of the underlying parameters of the system
under measurement, and performance deteriorates significantly with changes in
these parameters; hence it is desired to develop robust estimation techniques
immune to such uncertainties. In related works, we have already shown how
adaptive homodyne estimation can be made robust to uncertainty in an underlying
parameter of the phase varying as a simplistic Ornstein-Uhlenbeck stochastic
noise process. Here, we demonstrate robust phase estimation for a more
complicated resonant noise process using a guaranteed cost robust filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0156</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0156</id><created>2013-03-01</created><authors><author><keyname>Prat</keyname><forenames>G.</forenames></author><author><keyname>Belanche</keyname><forenames>Ll.</forenames></author></authors><title>Exploiting the Accumulated Evidence for Gene Selection in Microarray
  Gene Expression Data</title><categories>cs.CE cs.LG q-bio.QM</categories><comments>10 pages, 2 algorithms A shorter version of this paper appeared in
  the Procs. of the 19th European Conference on Artificial Intelligence (ECAI
  2010)</comments><msc-class>I.5.2</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning methods have of late made significant efforts to solving
multidisciplinary problems in the field of cancer classification using
microarray gene expression data. Feature subset selection methods can play an
important role in the modeling process, since these tasks are characterized by
a large number of features and a few observations, making the modeling a
non-trivial undertaking. In this particular scenario, it is extremely important
to select genes by taking into account the possible interactions with other
gene subsets. This paper shows that, by accumulating the evidence in favour (or
against) each gene along the search process, the obtained gene subsets may
constitute better solutions, either in terms of predictive accuracy or gene
size, or in both. The proposed technique is extremely simple and applicable at
a negligible overhead in cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0157</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0157</id><created>2013-03-01</created><updated>2015-06-26</updated><authors><author><keyname>Shuai</keyname><forenames>Hong-Han</forenames></author></authors><title>Scalable Cost-Aware Multi-Way Influence Maximization</title><categories>cs.DS cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Viral marketing is different from other marketing strategies since it
leverages the influence power in intimate relationship, e.g., close friends,
family members, couples. Due to the development and popularity of social
networking services, such as Facebook, Twitter, and Pinterest, the new notion
of &quot;social media marketing&quot; has appeared in recent years and presents new
opportunities for enabling large-scale and prevalent viral marketing online. To
boost the growth of their sales, business is embracing social media in a big
way. According to USA Today, the sales of software to run corporate social
networks will grow 61\% a year and be a $6.4$ billion business by 2016.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0160</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0160</id><created>2013-03-01</created><authors><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author><author><keyname>Sripratak</keyname><forenames>Piyashat</forenames></author><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author></authors><title>Average value of solutions for the bipartite boolean quadratic programs
  and rounding algorithms</title><categories>math.OC cs.DM math.CO</categories><comments>20 pages</comments><journal-ref>Theoretical Computer Science 565 (2015), 77-89</journal-ref><doi>10.1016/j.tcs.2014.11.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider domination analysis of approximation algorithms for the bipartite
boolean quadratic programming problem (BBQP) with m+n variables. A closed form
formula is developed to compute the average objective function value A of all
solutions in O(mn) time. However, computing the median objective function value
of the solutions is shown to be NP-hard. Also, we show that any solution with
objective function value no worse than A dominates at least 2^{m+n-2} solutions
and this bound is the best possible. Further, we show that such a solution can
be identified in O(mn) time and hence the dominance ratio of this algorithm is
at least 1/4. We then show that for any fixed rational number a &gt; 1, no
polynomial time approximation algorithm exists for BBQP with dominance ratio
larger than 1-2^{(m+n)(1-a)/a}, unless P=NP. We then analyze some powerful
local search algorithms and show that they can get trapped at a local maximum
with objective function value less than A. One of our approximation algorithms
has an interesting rounding property which provides a data dependent lower
bound on the optimal objective function value. A new integer programming
formulation of BBQP is also given and computational results with our rounding
algorithms are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0167</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0167</id><created>2013-03-01</created><updated>2013-06-01</updated><authors><author><keyname>Becker</keyname><forenames>Stephen</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author></authors><title>Randomized Low-Memory Singular Value Projection</title><categories>math.OC cs.NA quant-ph</categories><comments>13 pages. This version has a revised theorem and new numerical
  experiments</comments><msc-class>90C06, 81P50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affine rank minimization algorithms typically rely on calculating the
gradient of a data error followed by a singular value decomposition at every
iteration. Because these two steps are expensive, heuristic approximations are
often used to reduce computational burden. To this end, we propose a recovery
scheme that merges the two steps with randomized approximations, and as a
result, operates on space proportional to the degrees of freedom in the
problem. We theoretically establish the estimation guarantees of the algorithm
as a function of approximation tolerance. While the theoretical approximation
requirements are overly pessimistic, we demonstrate that in practice the
algorithm performs well on the quantum tomography recovery problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0183</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0183</id><created>2013-03-01</created><updated>2013-07-02</updated><authors><author><keyname>Aguirre</keyname><forenames>Jacobo</forenames></author><author><keyname>Papo</keyname><forenames>David</forenames></author><author><keyname>Buld&#xfa;</keyname><forenames>Javier M.</forenames></author></authors><title>Successful strategies for competing networks</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.MN q-bio.PE</categories><comments>Free Supporting Information at
  http://www.nature.com/nphys/journal/v9/n4/extref/nphys2556-s1.pdf</comments><journal-ref>Nature Physics 9, 230-234 (2013)</journal-ref><doi>10.1038/nphys2556</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Competitive interactions represent one of the driving forces behind evolution
and natural selection in biological and sociological systems. For example,
animals in an ecosystem may vie for food or mates; in a market economy, firms
may compete over the same group of customers; sensory stimuli may compete for
limited neural resources in order to enter the focus of attention. Here, we
derive rules based on the spectral properties of the network governing the
competitive interactions between groups of agents organized in networks. In the
scenario studied here the winner of the competition, and the time needed to
prevail, essentially depend on the way a given network connects to its
competitors and on its internal structure. Our results allow assessing the
extent to which real networks optimize the outcome of their interaction, but
also provide strategies through which competing networks can improve on their
situation. The proposed approach is applicable to a wide range of systems that
can be modeled as networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0198</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0198</id><created>2013-03-01</created><authors><author><keyname>Peleg</keyname><forenames>Michael</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>On sparse sensing and sparse sampling of coded signals at sub-Landau
  rates</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to the ETT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances of information-theoretic understanding of sparse sampling of
continuous uncoded signals at sampling rates exceeding the Landau rate were
reported in recent works. This work examines sparse sampling of coded signals
at sub-Landau sampling rates. It is shown that with coded signals the Landau
condition may be relaxed and the sampling rate required for signal
reconstruction and for support detection can be lower than the effective
bandwidth. Equivalently, the number of measurements in the corresponding sparse
sensing problem can be smaller than the support size. Tight bounds on
information rates and on signal and support detection performance are derived
for the Gaussian sparsely sampled channel and for the frequency-sparse channel
using the context of state dependent channels. Support detection results are
verified by a simulation. When the system is high-dimensional the required SNR
is shown to be finite but high and rising with decreasing sampling rate, in
some practical applications it can be lowered by reducing the a-priory
uncertainty about the support e.g. by concentrating the frequency support into
a finite number of subbands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0210</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0210</id><created>2013-03-01</created><authors><author><keyname>Abidin</keyname><forenames>Aysajan</forenames></author><author><keyname>Larsson</keyname><forenames>Jan-&#xc5;ke</forenames></author></authors><title>Direct Proof of Security of Wegman-Carter Authentication with Partially
  Known Key</title><categories>cs.CR quant-ph</categories><comments>15 pages</comments><journal-ref>Quantum Information Processing, 13, 2155-2170, 2014</journal-ref><doi>10.1007/s11128-013-0641-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-theoretically secure (ITS) authentication is needed in Quantum
Key Distribution (QKD). In this paper, we study security of an ITS
authentication scheme proposed by Wegman &amp; Carter, in the case of partially
known authentication key. This scheme uses a new authentication key in each
authentication attempt, to select a hash function from an Almost Strongly
Universal$_2$ hash function family. The partial knowledge of the attacker is
measured as the trace distance between the authentication key distribution and
the uniform distribution; this is the usual measure in QKD. We provide direct
proofs of security of the scheme, when using partially known key, first in the
information-theoretic setting and then in terms of witness indistinguishability
as used in the Universal Composability (UC) framework. We find that if the
authentication procedure has a failure probability $\epsilon$ and the
authentication key has an $\epsilon'$ trace distance to the uniform, then under
ITS, the adversary's success probability conditioned on an authentic
message-tag pair is only bounded by $\epsilon+|\mT|\epsilon'$, where $|\mT|$ is
the size of the set of tags. Furthermore, the trace distance between the
authentication key distribution and the uniform increases to $|\mT|\epsilon'$
after having seen an authentic message-tag pair. Despite this, we are able to
prove directly that the authenticated channel is indistinguishable from an
(ideal) authentic channel (the desired functionality), except with probability
less than $\epsilon+\epsilon'$. This proves that the scheme is
($\epsilon+\epsilon'$)-UC-secure, without using the composability theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0213</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0213</id><created>2013-03-01</created><authors><author><keyname>Lord</keyname><forenames>Phillip</forenames></author></authors><title>The Semantic Web takes Wing: Programming Ontologies with Tawny-OWL</title><categories>cs.AI cs.DL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Tawny-OWL library provides a fully-programmatic environment for ontology
building; it enables the use of a rich set of tools for ontology development,
by recasting development as a form of programming. It is built in Clojure - a
modern Lisp dialect, and is backed by the OWL API. Used simply, it has a
similar syntax to OWL Manchester syntax, but it provides arbitrary
extensibility and abstraction. It builds on existing facilities for Clojure,
which provides a rich and modern programming tool chain, for versioning,
distributed development, build, testing and continuous integration. In this
paper, we describe the library, this environment and the its potential
implications for the ontology development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0222</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0222</id><created>2013-03-01</created><authors><author><keyname>kumarasamy</keyname><forenames>Saravanan</forenames></author><author><keyname>Thangaraj</keyname><forenames>T. Stephen</forenames></author></authors><title>A Pattern Recognition Approach To Secure Cipher Documents</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural phenomena show that many creatures form large social groups and move
in regular patterns. Previous In this paper, we first propose an efficient
distributed mining algorithm to jointly identify a group of moving objects and
discover their movement patterns in wireless sensor networks. Afterward, we
propose a compression algorithm, called 2P2D, which exploits the obtained group
movement patterns to reduce the amount of delivered data. The compression
algorithm includes a sequence merge and an entropy reduction phases. we
formulate a Hit Item Replacement (HIR) problem and propose a Replace algorithm
that obtains the optimal solution. Moreover, we devise three replacement rules
and derive the maximum compression ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0229</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0229</id><created>2013-03-01</created><authors><author><keyname>Shukla</keyname><forenames>Srishti</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Wireless Network-Coded Multi-Way Relaying Using Latin Hyper-Cubes</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1210.3953, arXiv:1112.1584</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical layer network-coding for the $n$-way wireless relaying scenario is
dealt with, where each of the $n$ user nodes $X_1,$ $X_2,...,X_n$ wishes to
communicate its messages to all the other $(n-1)$ nodes with the help of the
relay node R. The given scheme, based on the denoise-and-forward scheme
proposed for two-way relaying by Popovski et al. in \cite{PoY1}, employs two
phases: Multiple Access (MA) phase and Broadcast (BC) phase with each phase
utilizing one channel use and hence totally two channel uses. Physical layer
network-coding using the denoise-and-forward scheme was done for the two-way
relaying scenario in\cite{KPT}, for three-way relaying scenario in \cite{SVR},
and for four-way relaying scenario in \cite{ShR}. This paper employs
denoise-and-forward scheme for physical layer network coding of the $n$-way
relaying scenario illustrating with the help of the case $n = 5$ not dealt with
so far. It is observed that adaptively changing the network coding map used at
the relay according to the channel conditions reduces the impact of multiple
access interference which occurs at the relay during the MA phase. These
network coding maps are chosen so that they satisfy a requirement called
\textit{exclusive law}. We show that when the $n$ users transmit points from
the same $M$-PSK $(M=2^{\lambda})$ constellation, every such network coding map
that satisfies the exclusive law can be represented by a $n$-fold Latin
Hyper-Cube of side $M$. The singular fade subspaces resulting from the scheme
are described and enumerated for general values of $n$ and $M$ and are
classified based on their removability in the given scenario. A network code
map to be used by the relay for the BC phase aiming at reducing the effect of
interference at the MA stage is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0247</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0247</id><created>2013-03-01</created><authors><author><keyname>Zhang</keyname><forenames>Liang Feng</forenames></author></authors><title>A Coding-Theoretic Application of Baranyai's Theorem</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Baranyai's theorem is a well-known theorem in the theory of hypergraphs. A
corollary of this theorem says that one can partition the family of all
$u$-subsets of an $n$-element set into ${n-1\choose u-1}$ sub-families such
that each sub-family form a partition of the $n$-element set, where $n$ is
divisible by $u$. In this paper, we present a coding-theoretic application of
Baranyai's theorem (or equivalently, the corollary). More precisely, we propose
the first purely combinatorial construction of locally decodable codes. Locally
decodable codes are error-correcting codes that allow the recovery of any
message bit by looking at only a few bits of the codeword. Such codes have
attracted a lot of attention in recent years. We stress that our construction
does not improve the parameters of known constructions. What makes it
interesting is the underlying combinatorial techniques and their potential in
future applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0262</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0262</id><created>2013-03-01</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Gerbner</keyname><forenames>Daniel</forenames></author><author><keyname>Keszegh</keyname><forenames>Balazs</forenames></author><author><keyname>Toth</keyname><forenames>Csaba D.</forenames></author></authors><title>Covering Paths for Planar Point Sets</title><categories>math.CO cs.DM</categories><comments>19 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ points in the plane, a \emph{covering path} is a polygonal path
that visits all the points. If no three points are collinear, every covering
path requires at least $n/2$ segments, and $n-1$ straight line segments
obviously suffice even if the covering path is required to be noncrossing. We
show that every set of $n$ points in the plane admits a (possibly self-crossi
ng) covering path consisting of $n/2 +O(n/\log{n})$ straight line segments. If
the path is required to be noncrossing, we prove that $(1-\eps)n$ straight line
segments suffice for a small constant $\eps&gt;0$, and we exhibit $n$-element
point sets that require at least $5n/9 -O(1)$ segments in every such path.
Further, the analogous question for noncrossing \emph{covering trees} is
considered and similar bounds are obtained. Finally, it is shown that computing
a noncrossing covering path for $n$ points in the plane requires $\Omega(n
\log{n})$ time in the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0266</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0266</id><created>2013-03-01</created><updated>2014-01-23</updated><authors><author><keyname>Herrero</keyname><forenames>Mar&#xed;a Isabel</forenames></author><author><keyname>Jeronimo</keyname><forenames>Gabriela</forenames></author><author><keyname>Sabia</keyname><forenames>Juan</forenames></author></authors><title>Elimination for generic sparse polynomial systems</title><categories>math.AG cs.CC cs.SC math.AC</categories><comments>22 pages</comments><msc-class>14Q20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new probabilistic symbolic algorithm that, given a variety
defined in an n-dimensional affine space by a generic sparse system with fixed
supports, computes the Zariski closure of its projection to an l-dimensional
coordinate affine space with l &lt; n. The complexity of the algorithm depends
polynomially on combinatorial invariants associated to the supports.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0270</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0270</id><created>2013-03-01</created><authors><author><keyname>Paix&#xe3;o</keyname><forenames>Crysttian Arantes</forenames></author><author><keyname>Coelho</keyname><forenames>Fl&#xe1;vio Code&#xe7;o</forenames></author></authors><title>Computable Compressed Matrices</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The biggest cost of computing with large matrices in any modern computer is
related to memory latency and bandwidth. The average latency of modern RAM
reads is 150 times greater than a clock step of the processor. Throughput is a
little better but still 25 times slower than the CPU can consume. The
application of bitstring compression allows for larger matrices to be moved
entirely to the cache memory of the computer, which has much better latency and
bandwidth (average latency of L1 cache is 3 to 4 clock steps). This allows for
massive performance gains as well as the ability to simulate much larger models
efficiently. In this work, we propose a methodology to compress matrices in
such a way that they retain their mathematical properties. Considerable
compression of the data is also achieved in the process Thus allowing for the
computation of much larger linear problems within the same memory constraints
when compared with the traditional representation of matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0276</identifier>
 <datestamp>2013-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0276</id><created>2013-03-01</created><authors><author><keyname>Hung</keyname><forenames>Wei-Lun</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author></authors><title>AutoSynch: An Automatic-Signal Monitor Based on Predicate Tagging</title><categories>cs.DC cs.PL</categories><comments>10 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most programming languages use monitors with explicit signals for
synchronization in shared-memory programs. Requiring program- mers to signal
threads explicitly results in many concurrency bugs due to missed
notifications, or notifications on wrong condition variables. In this paper, we
describe an implementation of an au- tomatic signaling monitor in Java called
AutoSynch that eliminates such concurrency bugs by removing the burden of
signaling from the programmer. We show that the belief that automatic signaling
monitors are prohibitively expensive is wrong. For most problems, programs
based on AutoSynch are almost as fast as those based on explicit signaling. For
some, AutoSynch is even faster than explicit signaling because it never uses
signalAll, whereas the programmers end up using signalAll with the explicit
signal mechanism. AutoSynch achieves efficiency in synchronization based on
three novel ideas. We introduce an operation called globalization that enables
the predicate evaluation in every thread, thereby reducing context switches
during the execution of the program. Secondly, AutoSynch avoids signalAll by
using a property called relay invari- ance that guarantees that whenever
possible there is always at least one thread whose condition is true which has
been signaled. Finally, AutoSynch uses a technique called predicate tagging to
efficiently determine a thread that should be signaled. To evaluate the effi-
ciency of AutoSynch, we have implemented many different well- known
synchronization problems such as the producers/consumers problem, the
readers/writers problems, and the dining philosophers problem. The results show
that AutoSynch is almost as efficient as the explicit-signal monitor and even
more efficient for some cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0283</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0283</id><created>2013-02-28</created><updated>2013-03-19</updated><authors><author><keyname>Kartoun</keyname><forenames>Uri</forenames></author></authors><title>Inverse Signal Classification for Financial Instruments</title><categories>cs.LG cs.IR q-fin.ST stat.ML</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.0073</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents new machine learning methods: signal composition, which
classifies time-series regardless of length, type, and quantity; and
self-labeling, a supervised-learning enhancement. The paper describes further
the implementation of the methods on a financial search engine system using a
collection of 7,881 financial instruments traded during 2011 to identify
inverse behavior among the time-series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0284</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0284</id><created>2013-03-01</created><authors><author><keyname>Musial</keyname><forenames>Katarzyna</forenames></author><author><keyname>Kazienkol</keyname><forenames>Przemyslaw</forenames></author><author><keyname>Kajdanowicz</keyname><forenames>Tomasz</forenames></author></authors><title>Social Recommendations within the Multimedia Sharing Systems</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>recommender system, multirelational social network, multimedia
  sharing system, social network analysis, Best Paper Award. arXiv admin note:
  text overlap with arXiv:1303.0093</comments><msc-class>91D30</msc-class><acm-class>H.3.4</acm-class><journal-ref>Musial K., Kazienko P., Kajdanowicz T.: Social Recommendations
  within the Multimedia Sharing Systems. The First World Summit on the
  Knowledge Society, WSKS'08, Lecture Notes in Computer Science LNCS 5288,
  2008, pp. 364-372</journal-ref><doi>10.1007/978-3-540-87781-3_40</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social recommender system that supports the creation of new relations
between users in the multimedia sharing system is presented in the paper. To
generate suggestions the new concept of the multirelational social network was
introduced. It covers both direct as well as object-based relationships that
reflect social and semantic links between users. The main goal of the new
method is to create the personalized suggestions that are continuously adapted
to users' needs depending on the personal weights assigned to each layer from
the social network. The conducted experiments confirmed the usefulness of the
proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0296</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0296</id><created>2013-03-01</created><authors><author><keyname>Yedla</keyname><forenames>Arvind</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Performance of Spatially-Coupled LDPC Codes and Threshold Saturation
  over BICM Channels</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of binary spatially-coupled low-density parity-check
codes (SC-LDPC) when used with bit-interleaved coded-modulation (BICM) schemes.
This paper considers the cases when transmission takes place over additive
white Gaussian noise (AWGN)channels and Rayleigh fast-fading channels. The
technique of upper bounding the maximum-a-posteriori (MAP) decoding performance
of LDPC codes using an area theorem is extended for BICM schemes. The upper
bound is computed for both the optimal MAP demapper and the suboptimal
max-log-MAP (MLM) demapper. It is observed that this bound approaches the noise
threshold of BICM channels for regular LDPC codes with large degrees. The rest
of the paper extends these techniques to SC-LDPC codes and the phenomenon of
threshold saturation is demonstrated numerically. Based on numerical evidence,
we conjecture that the belief-propagation (BP) decoding threshold of SC-LDPC
codes approaches the MAP decoding threshold of the underlying LDPC ensemble on
BICM channels. Numerical results also show that SC-LDPC codes approach the BICM
capacity over different channels and modulation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0309</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0309</id><created>2013-03-01</created><updated>2013-06-01</updated><authors><author><keyname>Muandet</keyname><forenames>Krikamol</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>One-Class Support Measure Machines for Group Anomaly Detection</title><categories>stat.ML cs.LG</categories><comments>Conference on Uncertainty in Artificial Intelligence (UAI2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose one-class support measure machines (OCSMMs) for group anomaly
detection which aims at recognizing anomalous aggregate behaviors of data
points. The OCSMMs generalize well-known one-class support vector machines
(OCSVMs) to a space of probability measures. By formulating the problem as
quantile estimation on distributions, we can establish an interesting
connection to the OCSVMs and variable kernel density estimators (VKDEs) over
the input space on which the distributions are defined, bridging the gap
between large-margin methods and kernel density estimators. In particular, we
show that various types of VKDEs can be considered as solutions to a class of
regularization problems studied in this paper. Experiments on Sloan Digital Sky
Survey dataset and High Energy Particle Physics dataset demonstrate the
benefits of the proposed framework in real-world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0323</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0323</id><created>2013-03-01</created><authors><author><keyname>Elshamy</keyname><forenames>Wesam</forenames></author><author><keyname>Emara</keyname><forenames>Hassan M</forenames></author><author><keyname>Bahgat</keyname><forenames>Ahmed</forenames></author></authors><title>Clubs-based Particle Swarm Optimization</title><categories>cs.NE</categories><comments>IEEE Swarm Intelligence Symposium 2007, Honolulu, HI</comments><msc-class>68T20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new dynamic neighborhood network for particle swarm
optimization. In the proposed Clubs-based Particle Swarm Optimization (C-PSO)
algorithm, each particle initially joins a default number of what we call
'clubs'. Each particle is affected by its own experience and the experience of
the best performing member of the clubs it is a member of. Clubs membership is
dynamic, where the worst performing particles socialize more by joining more
clubs to learn from other particles and the best performing particles are made
to socialize less by leaving clubs to reduce their strong influence on other
members. Particles return gradually to default membership level when they stop
showing extreme performance. Inertia weights of swarm members are made random
within a predefined range. This proposed dynamic neighborhood algorithm is
compared with other two algorithms having static neighborhood topologies on a
set of classic benchmark problems. The results showed superior performance for
C-PSO regarding escaping local optima and convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0325</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0325</id><created>2013-03-01</created><authors><author><keyname>Gnang</keyname><forenames>Edinah K.</forenames></author><author><keyname>Devlin</keyname><forenames>Patrick</forenames></author></authors><title>Some integer formula-encodings and related algorithms</title><categories>math.CO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the special class of formulas made up of arbitrary but finite
com- binations of addition, multiplication, and exponentiation gates. The
inputs to these formulas are restricted to the integral unit 1. In connection
with such formulas, we describe two essen- tially distinct families of
canonical formula-encodings for integers, respectively deduced from the decimal
encoding and the fundamental theorem of arithmetic. Our main contribution is
the de- tailed description of two algorithms which efficiently determine the
canonical formula-encodings associated with relatively large sets of
consecutive integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0328</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0328</id><created>2013-03-01</created><updated>2013-12-01</updated><authors><author><keyname>Mayer</keyname><forenames>Ernst W.</forenames></author></authors><title>Efficient long division via Montgomery multiply</title><categories>cs.DS</categories><comments>23 pages; 8 tables v2: Tweaked formatting to cut algo-surrounding
  whitespace [pagecount -= 2] v3: Fixed incorrect powers of R in
  scaled-partial-sum formulae [7] and [11] v4: Added reference-to Eldridge &amp;
  Walter paper v5: Clarified relation between Algos A/A',D and Hensel-div;
  Clarified true-quotient mechanics; Added Haswell timings and refs to Agner
  Fog's timings pdf and gmp asm-timings ref-page</comments><msc-class>11Y16 (Primary), 68Q25, 68W40 (Secondary)</msc-class><acm-class>G.1.0; B.2.4; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel right-to-left long division algorithm based on the
Montgomery modular multiply, consisting of separate highly efficient loops with
simply carry structure for computing first the remainder (x mod q) and then the
quotient floor(x/q). These loops are ideally suited for the case where x
occupies many more machine words than the divide modulus q, and are strictly
linear time in the &quot;bitsize ratio&quot; lg(x)/lg(q). For the paradigmatic
performance test of multiword dividend and single 64-bit-word divisor,
exploitation of the inherent data-parallelism of the algorithm effectively
mitigates the long latency of hardware integer MUL operations, as a result of
which we are able to achieve respective costs for remainder-only and full-DIV
(remainder and quotient) of 6 and 12.5 cycles per dividend word on the Intel
Core 2 implementation of the x86_64 architecture, in single-threaded execution
mode. We further describe a simple &quot;bit-doubling modular inversion&quot; scheme,
which allows the entire iterative computation of the mod-inverse required by
the Montgomery multiply at arbitrarily large precision to be performed with
cost less than that of a single Newtonian iteration performed at the full
precision of the final result. We also show how the Montgomery-multiply-based
powering can be efficiently used in Mersenne and Fermat-number trial
factorization via direct computation of a modular inverse power of 2, without
any need for explicit radix-mod scalings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0339</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0339</id><created>2013-03-01</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author></authors><title>Learning Hash Functions Using Column Generation</title><categories>cs.LG</categories><comments>9 pages, published in International Conf. Machine Learning, 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Fast nearest neighbor searching is becoming an increasingly important tool in
solving many large-scale problems. Recently a number of approaches to learning
data-dependent hash functions have been developed. In this work, we propose a
column generation based method for learning data-dependent hash functions on
the basis of proximity comparison information. Given a set of triplets that
encode the pairwise proximity comparison information, our method learns hash
functions that preserve the relative comparison relationships in the data as
well as possible within the large-margin learning framework. The learning
procedure is implemented using column generation and hence is named CGHash. At
each iteration of the column generation procedure, the best hash function is
selected. Unlike most other hashing methods, our method generalizes to new data
points naturally; and has a training objective which is convex, thus ensuring
that the global optimum can be identified. Experiments demonstrate that the
proposed method learns compact binary codes and that its retrieval performance
compares favorably with state-of-the-art methods when tested on a few benchmark
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0341</identifier>
 <datestamp>2014-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0341</id><created>2013-03-01</created><updated>2014-05-16</updated><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Zhou</keyname><forenames>Wen-Xin</forenames></author></authors><title>Matrix Completion via Max-Norm Constrained Optimization</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>27 pages</comments><msc-class>62H12, 15A83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix completion has been well studied under the uniform sampling model and
the trace-norm regularized methods perform well both theoretically and
numerically in such a setting. However, the uniform sampling model is
unrealistic for a range of applications and the standard trace-norm relaxation
can behave very poorly when the sampling distribution is non-uniform.
  In this paper we propose and analyze a max-norm constrained empirical risk
minimization method for noisy matrix completion under a general sampling model.
The optimal rate of convergence is established under the Frobenius norm loss in
the context of approximately low-rank matrix reconstruction. It is shown that
the max-norm constrained method is minimax rate-optimal and it yields a uni?ed
and robust approximate recovery guarantee, with respect to the sampling
distributions. The computational effectiveness of this method is also studied,
based on a first-order algorithm for solving convex programs involving a
max-norm constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0344</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0344</id><created>2013-03-01</created><authors><author><keyname>Silva</keyname><forenames>Thiago C.</forenames></author><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Network-based stochastic competitive learning approach to disambiguation
  in collaborative networks</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Chaos 23, 013139 (2013)</journal-ref><doi>10.1063/1.4794795</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many patterns have been uncovered in complex systems through the application
of concepts and methodologies of complex networks. Unfortunately, the validity
and accuracy of the unveiled patterns are strongly dependent on the amount of
unavoidable noise pervading the data, such as the presence of homonymous
individuals in social networks. In the current paper, we investigate the
problem of name disambiguation in collaborative networks, a task that plays a
fundamental role on a myriad of scientific contexts. In special, we use an
unsupervised technique which relies on a particle competition mechanism in a
networked environment to detect the clusters. It has been shown that, in this
kind of environment, the learning process can be improved because the network
representation of data can capture topological features of the input data set.
Specifically, in the proposed disambiguating model, a set of particles is
randomly spawned into the nodes constituting the network. As time progresses,
the particles employ a movement strategy composed of a probabilistic convex
mixture of random and preferential walking policies. In the former, the walking
rule exclusively depends on the topology of the network and is responsible for
the exploratory behavior of the particles. In the latter, the walking rule
depends both on the topology and the domination levels that the particles
impose on the neighboring nodes. This type of behavior compels the particles to
perform a defensive strategy, because it will force them to revisit nodes that
are already dominated by them, rather than exploring rival territories.
Computer simulations conducted on the networks extracted from the arXiv
repository of preprint papers and also from other databases reveal the
effectiveness of the model, which turned out to be more accurate than
traditional clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0346</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0346</id><created>2013-03-01</created><authors><author><keyname>Ahmadi</keyname><forenames>Hadi</forenames></author><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author></authors><title>Secure Distance Bounding Verification using Physical-Channel Properties</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distance bounding verification (DBV), where a
proving party claims a distance and a verifying party ensures that the prover
is within the claimed distance. Current approaches to &quot;secure&quot; distance
estimation use signal's time of flight, which requires the verifier to have an
accurate clock. We study secure DBV using physical channel properties as an
alternative to time measurement. We consider a signal propagation environment
that attenuates signal as a function of distance, and then corrupts it by an
additive noise.
  We consider three attacking scenarios against DBV, namely distance fraud
(DFA), mafia fraud (MFA) and terrorist fraud (TFA) attacks. We show it is
possible to construct efficient DBV protocols with DFA and MFA security, even
against an unbounded adversary; on the other hand, it is impossible to design
TFA-secure protocols without time measurement, even with a
computationally-bounded adversary. We however provide a TFA-secure construction
under the condition that the adversary's communication capability is limited to
the bounded retrieval model (BRM). We use numerical analysis to examine the
communication complexity of the introduced DBV protocols. We discuss our
results and give directions for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0347</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0347</id><created>2013-03-01</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Altmann</keyname><forenames>Eduardo G.</forenames></author><author><keyname>Rybski</keyname><forenames>Diego</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Probing the statistical properties of unknown texts: application to the
  Voynich Manuscript</title><categories>physics.soc-ph cs.CL physics.data-an</categories><journal-ref>PLoS ONE 8(7): e67310 (2013)</journal-ref><doi>10.1371/journal.pone.0067310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the use of statistical physics methods to analyze large corpora has
been useful to unveil many patterns in texts, no comprehensive investigation
has been performed investigating the properties of statistical measurements
across different languages and texts. In this study we propose a framework that
aims at determining if a text is compatible with a natural language and which
languages are closest to it, without any knowledge of the meaning of the words.
The approach is based on three types of statistical measurements, i.e. obtained
from first-order statistics of word properties in a text, from the topology of
complex networks representing text, and from intermittency concepts where text
is treated as a time series. Comparative experiments were performed with the
New Testament in 15 different languages and with distinct books in English and
Portuguese in order to quantify the dependency of the different measurements on
the language and on the story being told in the book. The metrics found to be
informative in distinguishing real texts from their shuffled versions include
assortativity, degree and selectivity of words. As an illustration, we analyze
an undeciphered medieval manuscript known as the Voynich Manuscript. We show
that it is mostly compatible with natural languages and incompatible with
random texts. We also obtain candidates for key-words of the Voynich Manuscript
which could be helpful in the effort of deciphering it. Because we were able to
identify statistical measurements that are more dependent on the syntax than on
the semantics, the framework may also serve for text analysis in
language-dependent applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0350</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0350</id><created>2013-03-01</created><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Structure-semantics interplay in complex networks and its effects on the
  predictability of similarity in texts</title><categories>cs.CL physics.soc-ph</categories><journal-ref>Physica A 391 18 4406-4419, (2012)</journal-ref><doi>10.1016/j.physa.2012.04.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are different ways to define similarity for grouping similar texts into
clusters, as the concept of similarity may depend on the purpose of the task.
For instance, in topic extraction similar texts mean those within the same
semantic field, whereas in author recognition stylistic features should be
considered. In this study, we introduce ways to classify texts employing
concepts of complex networks, which may be able to capture syntactic, semantic
and even pragmatic features. The interplay between the various metrics of the
complex networks is analyzed with three applications, namely identification of
machine translation (MT) systems, evaluation of quality of machine translated
texts and authorship recognition. We shall show that topological features of
the networks representing texts can enhance the ability to identify MT systems
in particular cases. For evaluating the quality of MT texts, on the other hand,
high correlation was obtained with methods capable of capturing the semantics.
This was expected because the golden standards used are themselves based on
word co-occurrence. Notwithstanding, the Katz similarity, which involves
semantic and structure in the comparison of texts, achieved the highest
correlation with the NIST measurement, indicating that in some cases the
combination of both approaches can improve the ability to quantify quality in
MT. In authorship recognition, again the topological features were relevant in
some contexts, though for the books and authors analyzed good results were
obtained with semantic features as well. Because hybrid approaches encompassing
semantic and topological features have not been extensively used, we believe
that the methodology proposed here may be useful to enhance text classification
considerably, as it combines well-established strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0351</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0351</id><created>2013-03-01</created><authors><author><keyname>Babu</keyname><forenames>Ramesh</forenames></author><author><keyname>Abraham</keyname><forenames>George</forenames></author><author><keyname>Borasia</keyname><forenames>Kiransinh</forenames></author></authors><title>A Review On Securing Distributed Systems Using Symmetric Key
  Cryptography</title><categories>cs.DC cs.CR</categories><comments>7 pages, 7 figures, 1 table, Journal</comments><journal-ref>International Journal of Advances in Science and Technology, Vol.
  4, No.4, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This review is aimed to evaluate the importance of Symmetric Key Cryptography
for Security in Distributed Systems. Businesses around the world as well as
research and other such areas rely heavily on distributed systems these days.
Hence, security is also a major concern due to the openness of the system. Out
of the various available security measures, we, in this paper, concentrate in
general on the symmetric key cryptographic technique. We review two widely used
and popular symmetric key cryptographic algorithms, viz. DES and AES. These two
algorithms are evaluated on the parameters such as key size, block size, number
of iterations, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0356</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0356</id><created>2013-03-02</created><updated>2013-03-05</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Christin</keyname><forenames>Nicolas</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Procaccia</keyname><forenames>Ariel D.</forenames></author><author><keyname>Sinha</keyname><forenames>Arunesh</forenames></author></authors><title>Audit Games</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective enforcement of laws and policies requires expending resources to
prevent and detect offenders, as well as appropriate punishment schemes to
deter violators. In particular, enforcement of privacy laws and policies in
modern organizations that hold large volumes of personal information (e.g.,
hospitals, banks, and Web services providers) relies heavily on internal audit
mechanisms. We study economic considerations in the design of these mechanisms,
focusing in particular on effective resource allocation and appropriate
punishment schemes. We present an audit game model that is a natural
generalization of a standard security game model for resource allocation with
an additional punishment parameter. Computing the Stackelberg equilibrium for
this game is challenging because it involves solving an optimization problem
with non-convex quadratic constraints. We present an additive FPTAS that
efficiently computes a solution that is arbitrarily close to the optimal
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0362</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0362</id><created>2013-03-02</created><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Inductive Sparse Subspace Clustering</title><categories>cs.LG</categories><comments>2 pages</comments><journal-ref>Electronics Letters, 2013, 49, (19), p. 1222-1224</journal-ref><doi>10.1049/el.2013.1789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Subspace Clustering (SSC) has achieved state-of-the-art clustering
quality by performing spectral clustering over a $\ell^{1}$-norm based
similarity graph. However, SSC is a transductive method which does not handle
with the data not used to construct the graph (out-of-sample data). For each
new datum, SSC requires solving $n$ optimization problems in O(n) variables for
performing the algorithm over the whole data set, where $n$ is the number of
data points. Therefore, it is inefficient to apply SSC in fast online
clustering and scalable graphing. In this letter, we propose an inductive
spectral clustering algorithm, called inductive Sparse Subspace Clustering
(iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts the
assumption that high-dimensional data actually lie on the low-dimensional
manifold such that out-of-sample data could be grouped in the embedding space
learned from in-sample data. Experimental results show that iSSC is promising
in clustering out-of-sample data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0377</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0377</id><created>2013-03-02</created><authors><author><keyname>Kumar</keyname><forenames>Gunjan</forenames></author><author><keyname>Shannigrahi</keyname><forenames>Saswata</forenames></author></authors><title>New Online Algorithm for Dynamic Speed Scaling with Sleep State</title><categories>cs.DS</categories><comments>13 pages</comments><doi>10.1016/j.tcs.2015.05.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an energy-efficient scheduling problem where $n$
jobs $J_1, J_2, ..., J_n$ need to be executed such that the total energy usage
of these jobs is minimized while ensuring that each job is finished within it's
deadline. We work in an online setting where a job is known only at it's
arrival time, along with it's processing volume and deadline. In such a
setting, the currently best-known algorithm by Han et al. \cite{han} provides a
competitive ratio max $\{4, 2 + {\alpha}^{\alpha}\}$ of energy usage. In this
paper, we present a new online algorithm SqOA which provides a competitive
ratio max $\{4, 2 + (2-1/{\alpha})^\alpha 2^{\alpha-1}\}$ of energy usage. For
$\alpha \geq 3$, the competitive ratio of our algorithm is better than that of
any other existing algorithms for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0379</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0379</id><created>2013-03-02</created><authors><author><keyname>Petrenko</keyname><forenames>Alexander K.</forenames><affiliation>Institute for System Programming of Russian Academy of Sciences, Russia</affiliation></author><author><keyname>Schlingloff</keyname><forenames>Holger</forenames><affiliation>Fraunhofer FOKUS, Humboldt University of Berlin, Germany</affiliation></author></authors><title>Proceedings Eighth Workshop on Model-Based Testing</title><categories>cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013</journal-ref><doi>10.4204/EPTCS.111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Eighth Workshop on Model-Based
Testing (MBT 2013), which was held on March 17, 2013 in Rome, Italy, as a
satellite event of the European Joint Conferences on Theory and Practice of
Software, ETAPS 2013.
  The workshop is devoted to model-based testing of both software and hardware.
Model-based testing uses models describing the required behavior of the system
under consideration to guide such efforts as test selection and test results
evaluation. Testing validates the real system behavior against models and
checks that the implementation conforms to them, but is capable also to find
errors in the models themselves.
  The first MBT workshop was held in 2004, in Barcelona. At that time MBT
already had become a hot topic, but the MBT workshop was the first event
devoted mostly to this domain. Since that time the area has generated enormous
scientific interest, and today there are several specialized workshops and more
broad conferences on software and hardware design and quality assurance
covering model based testing. MBT has become one of the most powerful system
analysis tools, one of the latest cutting-edge topics related is applying MBT
in security analysis and testing. MBT workshop tries to keep up with current
trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0381</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0381</id><created>2013-03-02</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Spectral Efficient Optimization in OFDM Systems with Wireless
  Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>Submitted to EUSIPCO 2013 (Invited paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers an orthogonal frequency division multiplexing (OFDM)
point-to-point wireless communication system with simultaneous wireless
information and power transfer. We study a receiver which is able to harvest
energy from the desired signal, noise, and interference. In particular, we
consider a power splitting receiver which dynamically splits the received power
into two power streams for information decoding and energy harvesting. We
design power allocation algorithms maximizing the spectral efficiency
(bit/s/Hz) of data transmission. In particular, the algorithm design is
formulated as a nonconvex optimization problem which takes into account the
constraint on the minimum power delivered to the receiver. The problem is
solved by using convex optimization techniques and a one-dimensional search.
The optimal power allocation algorithm serves as a system benchmark scheme due
to its high complexity. To strike a balance between system performance and
computational complexity, we also propose two suboptimal algorithms which
require a low computational complexity. Simulation results demonstrate the
excellent performance of the proposed suboptimal algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0382</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0382</id><created>2013-03-02</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author><author><keyname>Stefanescu</keyname><forenames>Gh.</forenames></author></authors><title>Network algebra for synchronous dataflow</title><categories>cs.LO</categories><comments>24 pages</comments><acm-class>F.1.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an algebraic theory of synchronous dataflow networks. First, a
basic algebraic theory of networks, called BNA (Basic Network Algebra), is
introduced. This theory captures the basic algebraic properties of networks.
For synchronous dataflow networks, it is subsequently extended with additional
constants for the branching connections that occur between the cells of
synchronous dataflow networks and axioms for these additional constants. We
also give two models of the resulting theory, the one based on stream
transformers and the other based on processes as considered in process algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0388</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0388</id><created>2013-03-02</created><authors><author><keyname>Bajcinca</keyname><forenames>Naim</forenames></author></authors><title>On computation of the total set of robust discrete-time PID controllers</title><categories>cs.SY</categories><comments>ECC 2007, Kos Greece</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding the set of all multi-model robust PID and three-term
stabilizers for discrete-time systems is solved in this paper. The method uses
the fact that decoupling of parameter space at singular frequencies is
invariant under a linear transformation. The resulting stable regions are
composed by convex polygonal slices. The design problem includes the assertion
of intervals with stable polygons and the detection of stable polygons. This
paper completes the solutions to both problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0395</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0395</id><created>2013-03-02</created><authors><author><keyname>Lampoltshammer</keyname><forenames>Thomas J.</forenames></author><author><keyname>Nowotny</keyname><forenames>Thomas</forenames></author><author><keyname>Plank</keyname><forenames>Stefan</forenames></author></authors><title>ICT System Design &amp; Implementation Using Wireless Sensors to Support
  Elderly In-home Assistance</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Around the globe the number of older people in relation to the rest is
constantly growing. As a result, medical and care facilities cannot handle the
growing number of patients. Therefore, elderly in-home assistance gets more
attention an importance. Due to issues regarding memory, physical strength and
reduced self-assessment, old people face a lot of challenges in accomplishing
their activities of daily living. This thesis is meant to address these
problems by analysing the required infrastructure of a home-care facility as
well as the arising issues regarding used components, especially wireless
sensors. After the analysis, a prototype of a home-care system is designed and
implemented. Furthermore, the issue of energy consumption of the used wireless
sensor node is addressed by modifying the intelligence of the used sensor.
After that, the design and components of the prototype used for the energy
consumption analysis is explained, together with the programming structure of
the sensor nodes used in this thesis. Thereupon, the results are of the
simulations are discussed and compared with the authors' expectations. Finally
the overall outcomes of the thesis are analysed and summed up, followed by a
short outlook of further possible improvements and developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0405</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0405</id><created>2013-03-02</created><authors><author><keyname>Imtiaz</keyname><forenames>Waqas Ahmed</forenames></author><author><keyname>Afaq</keyname><forenames>Muhammad</forenames></author><author><keyname>Babar</keyname><forenames>Muhammad Asmatullah Khan</forenames></author></authors><title>mSCTP Based Decentralized Mobility Framework</title><categories>cs.NI</categories><comments>7 Pages, Journal</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol. 2, No.9, 2011, Page 106-112</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To conceive the full potential of wireless IP services, Mobile Nodes (MNs)
must be able to roam seamlessly across different networks. Mobile Stream
Control Transmission Protocol (mSCTP) is a transport layer solution, which
unlike Mobile IP (MIP), provides seamless mobility with minimum delay and
negligible packet loss. However, mSCTP fails to locate the current IP address
of the mobile node when Correspondent Node (CN) wants to initiate a session. In
this paper, we propose DHT Chord to provide the required location management.
Chord is a P2P algorithm, which can efficiently provide the IP address of the
called MN by using its key-value mapping. The proposed decentralized mobility
framework collectively exploits the multihoming feature of mSCTP, and efficient
key-value mapping of chord to provide seamless mobility. Suitability of the
framework is analyzed by preliminary analysis of chord lookup efficiency, and
mSCTP handover procedure using overlay weaver and NS-2. Performance analysis
shows that mSCTP multihoming feature and Chord efficient key-value mapping can
provide a non-delayed, reliable, and an efficient IP handover solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0407</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0407</id><created>2013-03-02</created><authors><author><keyname>Badawi</keyname><forenames>Ahmad Al</forenames></author><author><keyname>Al-Haija</keyname><forenames>Qasem Abu</forenames></author></authors><title>IRS for Computer Character Sequences Filtration: a new software tool and
  algorithm to support the IRS at tokenization process</title><categories>cs.IR</categories><comments>5 pages, 6 figures, corresponding author is Qasem Abu Al-Haija</comments><acm-class>H.3.3</acm-class><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), The Science and Information Organization (SAI), Vol. 4,
  No. 2, 2013,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tokenization is the task of chopping it up into pieces, called tokens,
perhaps at the same time throwing away certain characters, such as punctuation.
A token is an instance of token a sequence of characters in some particular
document that are grouped together as a useful semantic unit for processing.
New software tool and algorithm to support the IRS at tokenization process are
presented. Our proposed tool will filter out the three computer character
Sequences: IP-Addresses, Web URLs, Date, and Email Addresses. Our tool will use
the pattern matching algorithms and filtration methods. After this process, the
IRS can start a new tokenization process on the new retrieved text which will
be free of these sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0415</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0415</id><created>2013-03-02</created><updated>2013-04-25</updated><authors><author><keyname>Zhang</keyname><forenames>Xiujun</forenames></author><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Distributed Power Allocation for Coordinated Multipoint Transmissions in
  Distributed Antenna Systems</title><categories>cs.IT math.IT</categories><comments>11 pages, accepted by IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2013.13.120863</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the distributed power allocation problem for
coordinated multipoint (CoMP) transmissions in distributed antenna systems
(DAS). Traditional duality based optimization techniques cannot be directly
applied to this problem, because the non-strict concavity of the CoMP
transmission's achievable rate with respect to the transmission power induces
that the local power allocation subproblems have non-unique optimum solutions.
We propose a distributed power allocation algorithm to resolve this non-strict
concavity difficulty. This algorithm only requires local information exchange
among neighboring base stations serving the same user, and is thus scalable as
the network size grows. The step-size parameters of this algorithm are
determined by only local user access relationship (i.e., the number of users
served by each antenna), but do not rely on channel coefficients. Therefore,
the convergence speed of this algorithm is quite robust to different channel
fading coefficients. We rigorously prove that this algorithm converges to an
optimum solution of the power allocation problem. Simulation results are
presented to demonstrate the effectiveness of the proposed power allocation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0417</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0417</id><created>2013-03-02</created><updated>2013-04-29</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author></authors><title>On the convergence of the IRLS algorithm in Non-Local Patch Regression</title><categories>cs.CV stat.ML</categories><journal-ref>IEEE Signal Processing Letters, vol. 20(8), 815 - 818, 2013</journal-ref><doi>10.1109/LSP.2013.2268248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it was demonstrated in [CS2012,CS2013] that the robustness of the
classical Non-Local Means (NLM) algorithm [BCM2005] can be improved by
incorporating $\ell^p (0 &lt; p \leq 2)$ regression into the NLM framework. This
general optimization framework, called Non-Local Patch Regression (NLPR),
contains NLM as a special case. Denoising results on synthetic and natural
images show that NLPR consistently performs better than NLM beyond a moderate
noise level, and significantly so when $p$ is close to zero. An iteratively
reweighted least-squares (IRLS) algorithm was proposed for solving the
regression problem in NLPR, where the NLM output was used to initialize the
iterations. Based on exhaustive numerical experiments, we observe that the IRLS
algorithm is globally convergent (for arbitrary initialization) in the convex
regime $1 \leq p \leq 2$, and locally convergent (fails very rarely using NLM
initialization) in the non-convex regime $0 &lt; p &lt; 1$. In this letter, we adapt
the &quot;majorize-minimize&quot; framework introduced in [Voss1980] to explain these
observations.
  [CS2012] Chaudhury et al. (2012), &quot;Non-local Euclidean medians,&quot; IEEE Signal
Processing Letters.
  [CS2013] Chaudhury et al. (2013), &quot;Non-local patch regression: Robust image
denoising in patch space,&quot; IEEE ICASSP.
  [BCM2005] Buades et al. (2005), &quot;A review of image denoising algorithms, with
a new one,&quot; Multiscale Modeling and Simulation.
  [Voss1980] Voss et al. (1980), &quot;Linear convergence of generalized Weiszfeld's
method,&quot; Computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0418</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0418</id><created>2013-03-02</created><authors><author><keyname>Deshmukh</keyname><forenames>Dr. Anwar Pasha</forenames></author><author><keyname>Qureshi</keyname><forenames>Dr. Riyazuddin</forenames></author></authors><title>Transparent Data Encryption -- Solution for Security of Database
  Contents</title><categories>cs.DB cs.CR</categories><comments>4 Pages 2 figures</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Volume 2 No. 3, March 2011, pp 25-28. ISSN: 2156-5570(Online) &amp;
  ISSN: 2158-107X(Print)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present study deals with Transparent Data Encryption which is a
technology used to solve the problems of security of data. Transparent Data
Encryption means encrypting databases on hard disk and on any backup media.
Present day global business environment presents numerous security threats and
compliance challenges. To protect against data thefts and frauds we require
security solutions that are transparent by design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0422</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0422</id><created>2013-03-02</created><authors><author><keyname>Sariyuce</keyname><forenames>Ahmet Erdem</forenames></author><author><keyname>Kaya</keyname><forenames>Kamer</forenames></author><author><keyname>Saule</keyname><forenames>Erik</forenames></author><author><keyname>Catalyurek</keyname><forenames>Umit V.</forenames></author></authors><title>Incremental Algorithms for Network Management and Analysis based on
  Closeness Centrality</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing networks requires complex algorithms to extract meaningful
information. Centrality metrics have shown to be correlated with the importance
and loads of the nodes in network traffic. Here, we are interested in the
problem of centrality-based network management. The problem has many
applications such as verifying the robustness of the networks and controlling
or improving the entity dissemination. It can be defined as finding a small set
of topological network modifications which yield a desired closeness centrality
configuration. As a fundamental building block to tackle that problem, we
propose incremental algorithms which efficiently update the closeness
centrality values upon changes in network topology, i.e., edge insertions and
deletions. Our algorithms are proven to be efficient on many real-life
networks, especially on small-world networks, which have a small diameter and a
spike-shaped shortest distance distribution. In addition to closeness
centrality, they can also be a great arsenal for the shortest-path-based
management and analysis of the networks. We experimentally validate the
efficiency of our algorithms on large networks and show that they update the
closeness centrality values of the temporal DBLP-coauthorship network of 1.2
million users 460 times faster than it would take to compute them from scratch.
To the best of our knowledge, this is the first work which can yield practical
large-scale network management based on closeness centrality values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0425</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0425</id><created>2013-03-02</created><authors><author><keyname>Bajcinca</keyname><forenames>Naim</forenames></author></authors><title>Methods for robust PID control</title><categories>cs.SY</categories><comments>survey on robust PId control</comments><journal-ref>published in ANASH 2009 at
  http://www.alb-shkenca.org/index.php/ANASH/</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comprehensive theory for robust PID control in continuous-time and
discrete-time domain is reviewed in this paper. For a given finite set of
linear time invariant plants, algorithms for fast computation of robustly
stabilizing regions in the ($k_P, k_I, k_D$)-parameter space are introduced.
The main impetus is given by the fact that non-convex stable regions in the PID
parameter space can be built up by convex polygonal slices. A simple and an
elegant theory evolved in the last few years up to a quite mature level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0427</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0427</id><created>2013-03-02</created><authors><author><keyname>Black</keyname><forenames>Andrew P.</forenames></author></authors><title>Object-oriented programming: some history, and challenges for the next
  fifty years</title><categories>cs.PL</categories><comments>42 pages; author's version of a paper submitted to Information and
  Control</comments><msc-class>01A60 6803 68N19 68N19</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object-oriented programming is inextricably linked to the pioneering work of
Ole-Johan Dahl and Kristen Nygaard on the design of the Simula language, which
started at the Norwegian Computing Centre in the Spring of 1961. However,
object-orientation, as we think of it today---fifty years later---is the result
of a complex interplay of ideas, constraints and people. Dahl and Nygaard would
certainly recognise it as their progeny, but might also be amazed at how much
it has grown up.
  This article is based on a lecture given on 22nd August 2011, on the occasion
of the scientific opening of the Ole-Johan Dahl hus at the University of Oslo.
It looks at the foundational ideas from Simula that stand behind
object-orientation, how those ideas have evolved to become the dominant
programming paradigm, and what they have to offer as we approach the challenges
of the next fifty years of informatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0444</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0444</id><created>2013-03-02</created><updated>2013-03-19</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Reconciliation between the Tsallis maximum entropy principle and large
  deviation theory</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>Withdrawn by authors owing to serious deficiencies in the model.
  Corrections of these deficiencies cannot be ameliorated by submission of a
  new version of the same article. An entirely reworked submission is presented
  in arXiv:1303.4211</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The necessary conditions (NC) that reconcile canonical probability
distributions obtained from the q-maximum entropy principle, subjected to both
i) the additive duality of generalized statistics and ii) normal averages
expectations with the large deviation theory, are derived. The validity of
these necessary conditions is established on the basis of a result concerning
large deviation properties of conditional measures. The NC for normal averages
expectations are advantageous because they avoid the excessively prohibitive
conditions obtained by previous studies when employing other forms for defining
q-expectations. Numerical examples for an exemplary case are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0445</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0445</id><created>2013-03-02</created><authors><author><keyname>R</keyname><forenames>Kanagavalli V</forenames></author><author><keyname>K</keyname><forenames>Raja.</forenames></author></authors><title>Detecting and resolving spatial ambiguity in text using named entity
  extraction and self learning fuzzy logic techniques</title><categories>cs.IR cs.CL</categories><comments>National Conference on Recent Trends in Data Mining and Distributed
  Systems September 2011</comments><report-no>ISBN 978-81-909042-5-4 P.no.71-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information extraction identifies useful and relevant text in a document and
converts unstructured text into a form that can be loaded into a database
table. Named entity extraction is a main task in the process of information
extraction and is a classification problem in which words are assigned to one
or more semantic classes or to a default non-entity class. A word which can
belong to one or more classes and which has a level of uncertainty in it can be
best handled by a self learning Fuzzy Logic Technique. This paper proposes a
method for detecting the presence of spatial uncertainty in the text and
dealing with spatial ambiguity using named entity extraction techniques coupled
with self learning fuzzy logic techniques
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0446</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0446</id><created>2013-03-02</created><authors><author><keyname>Bonev</keyname><forenames>Boyan</forenames></author><author><keyname>Ram&#xed;rez-S&#xe1;nchez</keyname><forenames>Gema</forenames></author><author><keyname>Rojas</keyname><forenames>Sergio Ortiz</forenames></author></authors><title>Statistical sentiment analysis performance in Opinum</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The classification of opinion texts in positive and negative is becoming a
subject of great interest in sentiment analysis. The existence of many labeled
opinions motivates the use of statistical and machine-learning methods.
First-order statistics have proven to be very limited in this field. The Opinum
approach is based on the order of the words without using any syntactic and
semantic information. It consists of building one probabilistic model for the
positive and another one for the negative opinions. Then the test opinions are
compared to both models and a decision and confidence measure are calculated.
In order to reduce the complexity of the training corpus we first lemmatize the
texts and we replace most named-entities with wildcards. Opinum presents an
accuracy above 81% for Spanish opinions in the financial products domain. In
this work we discuss which are the most important factors that have impact on
the classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0447</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0447</id><created>2013-03-02</created><authors><author><keyname>Kanagavalli</keyname><forenames>V. R.</forenames></author><author><keyname>Raja</keyname><forenames>K.</forenames></author></authors><title>A Study on Application of Spatial Data Mining Techniques for Rural
  Progress</title><categories>cs.DB cs.CY</categories><comments>International Conference on Innovative Computing, information and
  communication technology ICICT09; souvenir pp no 64</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the application of Spatial Data mining Techniques to
efficiently manage the challenges faced by peripheral rural areas in analyzing
and predicting market scenario and better manage their economy. Spatial data
mining is the task of unfolding the implicit knowledge hidden in the spatial
databases. The spatial Databases contain both spatial and non-spatial
attributes of the areas under study. Finding implicit regularities, rules or
patterns hidden in spatial databases is an important task, e.g. for
geo-marketing, traffic control or environmental studies. In this paper the
focus is on the effective use of Spatial Data Mining Techniques in the field of
Economic Geography constrained to the rural areas
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0448</identifier>
 <datestamp>2013-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0448</id><created>2013-03-02</created><updated>2013-09-25</updated><authors><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Learning Stable Multilevel Dictionaries for Sparse Representations</title><categories>cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representations using learned dictionaries are being increasingly used
with success in several data processing and machine learning applications. The
availability of abundant training data necessitates the development of
efficient, robust and provably good dictionary learning algorithms. Algorithmic
stability and generalization are desirable characteristics for dictionary
learning algorithms that aim to build global dictionaries which can efficiently
model any test data similar to the training samples. In this paper, we propose
an algorithm to learn dictionaries for sparse representations from large scale
data, and prove that the proposed learning algorithm is stable and
generalizable asymptotically. The algorithm employs a 1-D subspace clustering
procedure, the K-hyperline clustering, in order to learn a hierarchical
dictionary with multiple levels. We also propose an information-theoretic
scheme to estimate the number of atoms needed in each level of learning and
develop an ensemble approach to learn robust dictionaries. Using the proposed
dictionaries, the sparse code for novel test data can be computed using a
low-complexity pursuit procedure. We demonstrate the stability and
generalization characteristics of the proposed algorithm using simulations. We
also evaluate the utility of the multilevel dictionaries in compressed recovery
and subspace learning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0452</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0452</id><created>2013-03-02</created><authors><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Yang</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Lin</keyname><forenames>Wang</forenames></author></authors><title>Domain-of-Attraction Estimation for Uncertain Non-polynomial Systems</title><categories>cs.SC math.OC</categories><doi>10.1016/j.cnsns.2013.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of computing estimates of the
domain-of-attraction for non-polynomial systems. A polynomial approximation
technique, based on multivariate polynomial interpolation and error analysis
for remaining functions, is applied to compute an uncertain polynomial system,
whose set of trajectories contains that of the original non-polynomial system.
Experiments on the benchmark non-polynomial systems show that our approach
gives better estimates of the domain-of-attraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0459</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0459</id><created>2013-03-03</created><authors><author><keyname>Nafi</keyname><forenames>Kawser Wazed</forenames></author><author><keyname>kar</keyname><forenames>Tonny Shekha</forenames></author><author><keyname>Hossain</keyname><forenames>Amjad</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A</forenames></author></authors><title>An Advanced Certain Trust Model Using Fuzzy Logic and Probabilistic
  Logic theory</title><categories>cs.CR</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 3, No. 12, pp. 164-173, [ISSN: 2156-5570] (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trustworthiness especially for service oriented system is very important
topic now a day in IT field of the whole world. Certain Trust Model depends on
some certain values given by experts and developers. Here, main parameters for
calculating trust are certainty and average rating. In this paper we have
proposed an Extension of Certain Trust Model, mainly the representation portion
based on probabilistic logic and fuzzy logic. This extended model can be
applied in a system like cloud computing, internet, website, e-commerce, etc.
to ensure trustworthiness of these platforms. The model uses the concept of
fuzzy logic to add fuzziness with certainty and average rating to calculate the
trustworthiness of a system more accurately. We have proposed two new
parameters - trust T and behavioral probability P, which will help both the
users and the developers of the system to understand its present condition
easily. The linguistic variables are defined for both T and P and then these
variables are implemented in our laboratory to verify the proposed trust model.
We represent the trustworthiness of test system for two cases of evidence value
using Fuzzy Associative Memory (FAM). We use inference rules and
defuzzification method for verifying the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0460</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0460</id><created>2013-03-03</created><authors><author><keyname>Priyadharshini</keyname><forenames>N.</forenames></author><author><keyname>Vijaya</keyname><forenames>M. S.</forenames></author></authors><title>Genetic Programming for Document Segmentation and Region Classification
  Using Discipulus</title><categories>cs.CV cs.NE</categories><comments>8 pages,13 figures</comments><journal-ref>(IJARAI) International Journal of Advanced Research in Artificial
  Intelligence, Vol. 2, No. 2, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Document segmentation is a method of rending the document into distinct
regions. A document is an assortment of information and a standard mode of
conveying information to others. Pursuance of data from documents involves ton
of human effort, time intense and might severely prohibit the usage of data
systems. So, automatic information pursuance from the document has become a big
issue. It is been shown that document segmentation will facilitate to beat such
problems. This paper proposes a new approach to segment and classify the
document regions as text, image, drawings and table. Document image is divided
into blocks using Run length smearing rule and features are extracted from
every blocks. Discipulus tool has been used to construct the Genetic
programming based classifier model and located 97.5% classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0462</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0462</id><created>2013-03-03</created><authors><author><keyname>Jahan</keyname><forenames>Moslema</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Shahriar</keyname><forenames>Gazi Abdullah</forenames></author></authors><title>Distributed Evolutionary Computation: A New Technique for Solving Large
  Number of Equations</title><categories>cs.NE</categories><journal-ref>International Journal of Parallel and Distributed Systems (IJPDS),
  Vol. 2, No.6, pp.31-49,(2011)</journal-ref><doi>10.5121/ijdps.2011.2604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary computation techniques have mostly been used to solve various
optimization and learning problems successfully. Evolutionary algorithm is more
effective to gain optimal solution(s) to solve complex problems than
traditional methods. In case of problems with large set of parameters,
evolutionary computation technique incurs a huge computational burden for a
single processing unit. Taking this limitation into account, this paper
presents a new distributed evolutionary computation technique, which decomposes
decision vectors into smaller components and achieves optimal solution in a
short time. In this technique, a Jacobi-based Time Variant Adaptive (JBTVA)
Hybrid Evolutionary Algorithm is distributed incorporating cluster computation.
Moreover, two new selection methods named Best All Selection (BAS) and Twin
Selection (TS) are introduced for selecting best fit solution vector.
Experimental results show that optimal solution is achieved for different kinds
of problems having huge parameters and a considerable speedup is obtained in
proposed distributed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0463</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0463</id><created>2013-03-03</created><authors><author><keyname>Kalogerias</keyname><forenames>Dionysios S.</forenames></author><author><keyname>Chatzipanagiotis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Zavlanos</keyname><forenames>Michael M.</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author></authors><title>Mobile Jammers for Secrecy Rate Maximization in Cooperative Networks</title><categories>cs.IT math.IT</categories><comments>ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a source (Alice) trying to communicate with a destination (Bob),
in a way that an unauthorized node (Eve) cannot infer, based on her
observations, the information that is being transmitted. The communication is
assisted by multiple multi-antenna cooperating nodes (helpers) who have the
ability to move. While Alice transmits, the helpers transmit noise that is
designed to affect the entire space except Bob. We consider the problem of
selecting the helper weights and positions that maximize the system secrecy
rate. It turns out that this optimization problem can be efficiently solved,
leading to a novel decentralized helper motion control scheme. Simulations
indicate that introducing helper mobility leads to considerable savings in
terms of helper transmit power, as well as total number of helpers required for
secrecy communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0464</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0464</id><created>2013-03-03</created><authors><author><keyname>Dey</keyname><forenames>Tanay</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author><author><keyname>Mondal</keyname><forenames>Subroto Kumar</forenames></author></authors><title>On Performance Analysis of AMBR Protocol in Mobile Ad Hoc Networks</title><categories>cs.NI</categories><journal-ref>IIUM Engineering Journal, Vol 11, No.2, pp.240-254, [ISSN:
  1511-788X] (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to mobility of nodes in ad hoc networks, the most challenging issue is to
design and to make sound analysis of a routing protocol that determines its
robustness to deliver packets in low routing packet overhead. In this paper, we
thoroughly analyzed the Adaptive Monitor Based Routing (AMBR) protocol by
varying different parameters that affect a routing protocol to measure its
performance. Analysis shows that it requires less routing control overhead
comparing with other prevalent routing protocols. An improved analytical model
is also presented in this paper. All these analyses firmly prove that AMBR is a
sound and robust protocol in terms of flooding, routing overhead and hence,
enhances reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0468</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0468</id><created>2013-03-03</created><authors><author><keyname>Mohseni</keyname><forenames>Mahsa</forenames></author></authors><title>Has your organization compliance with ISMS? A case study in an Iranian
  Bank</title><categories>cs.CY</categories><comments>23 pages,1 figures,4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this study is proposing a model to determine the gaps between
security standards requirements and the reality of implementation ISMS. The
research approach analyzes the various industry standards relevant to
information security and responses gained from interviewing with 45 individuals
of IT professionals and information security experts (who are chosen with
targeted sampling) in order to develop a model comprising factors and
subfactors which assesses compliance with ISMS (Information Security Management
System) in organizations. For hypothesis test, binomial test and for ranking of
factors and sub factors, Friedman test was done. This model tested in a bank
and the degree of compliance with ISMS calculated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0478</identifier>
 <datestamp>2013-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0478</id><created>2013-03-03</created><updated>2013-04-11</updated><authors><author><keyname>Chen</keyname><forenames>Shenshi</forenames></author></authors><title>Monomial Testing and Applications</title><categories>cs.CC</categories><comments>17 pages, 4 figures, submitted FAW-AAIM 2013. arXiv admin note:
  substantial text overlap with arXiv:1302.5898; and text overlap with
  arXiv:1007.2675, arXiv:1007.2678, arXiv:1007.2673 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we devise two algorithms for the problem of testing
$q$-monomials of degree $k$ in any multivariate polynomial represented by a
circuit, regardless of the primality of $q$. One is an $O^*(2^k)$ time
randomized algorithm. The other is an $O^*(12.8^k)$ time deterministic
algorithm for the same $q$-monomial testing problem but requiring the
polynomials to be represented by tree-like circuits. Several applications of
$q$-monomial testing are also given, including a deterministic $O^*(12.8^{mk})$
upper bound for the $m$-set $k$-packing problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0479</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0479</id><created>2013-03-03</created><updated>2013-04-03</updated><authors><author><keyname>Shen</keyname><forenames>Zhuangming</forenames></author><author><keyname>Sun</keyname><forenames>Jiuai</forenames></author><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Qin</keyname><forenames>Binjie</forenames></author></authors><title>Scale Selection of Adaptive Kernel Regression by Joint Saliency Map for
  Nonrigid Image Registration</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Joint saliency map (JSM) [1] was developed to assign high joint saliency
values to the corresponding saliency structures (called Joint Saliency
Structures, JSSs) but zero or low joint saliency values to the outliers (or
mismatches) that are introduced by missing correspondence or local large
deformations between the reference and moving images to be registered. JSM
guides the local structure matching in nonrigid registration by emphasizing
these JSSs' sparse deformation vectors in adaptive kernel regression of
hierarchical sparse deformation vectors for iterative dense deformation
reconstruction. By designing an effective superpixel-based local structure
scale estimator to compute the reference structure's structure scale, we
further propose to determine the scale (the width) of kernels in the adaptive
kernel regression through combining the structure scales to JSM-based scales of
mismatch between the local saliency structures. Therefore, we can adaptively
select the sample size of sparse deformation vectors to reconstruct the dense
deformation vectors for accurately matching the every local structures in the
two images. The experimental results demonstrate better accuracy of our method
in aligning two images with missing correspondence and local large deformation
than the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0481</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0481</id><created>2013-03-03</created><updated>2014-03-30</updated><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Situation-Aware Approach to Improve Context-based Recommender System</title><categories>cs.IR</categories><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel situation aware approach to improve a
context based recommender system. To build situation aware user profiles, we
rely on evidence issued from retrieval situations. A retrieval situation refers
to the social spatio temporal context of the user when he interacts with the
recommender system. A situation is represented as a combination of social
spatio temporal concepts inferred from ontological knowledge given social
group, location and time information. User's interests are inferred from past
user's interaction with the recommender system related to the identified
situations. They are represented using concepts issued from a domain ontology.
We also propose a method to dynamically adapt the system to the user's
interest's evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0484</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0484</id><created>2013-03-03</created><authors><author><keyname>Mitzlaff</keyname><forenames>Folke</forenames></author><author><keyname>Stumme</keyname><forenames>Gerd</forenames></author></authors><title>Onomastics 2.0 - The Power of Social Co-Occurrences</title><categories>cs.IR cs.SI physics.soc-ph</categories><comments>Historically, this is the first paper on the analysis of names in the
  context of the name search engine 'nameling'. arXiv admin note: text overlap
  with arXiv:1302.4412</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Onomastics is &quot;the science or study of the origin and forms of proper names
of persons or places.&quot; [&quot;Onomastics&quot;. Merriam-Webster.com, 2013.
http://www.merriam-webster.com (11 February 2013)]. Especially personal names
play an important role in daily life, as all over the world future parents are
facing the task of finding a suitable given name for their child. This choice
is influenced by different factors, such as the social context, language,
cultural background and, in particular, personal taste.
  With the rise of the Social Web and its applications, users more and more
interact digitally and participate in the creation of heterogeneous,
distributed, collaborative data collections. These sources of data also reflect
current and new naming trends as well as new emerging interrelations among
names.
  The present work shows, how basic approaches from the field of social network
analysis and information retrieval can be applied for discovering relations
among names, thus extending Onomastics by data mining techniques. The
considered approach starts with building co-occurrence graphs relative to data
from the Social Web, respectively for given names and city names. As a main
result, correlations between semantically grounded similarities among names
(e.g., geographical distance for city names) and structural graph based
similarities are observed.
  The discovered relations among given names are the foundation of &quot;nameling&quot;
[http://nameling.net], a search engine and academic research platform for given
names which attracted more than 30,000 users within four months,
underpinningthe relevance of the proposed methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0485</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0485</id><created>2013-03-03</created><updated>2014-04-15</updated><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Optimizing an Utility Function for Exploration / Exploitation Trade-off
  in Context-Aware Recommender System</title><categories>cs.IR</categories><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a dynamic exploration/ exploitation (exr/exp)
strategy for contextual recommender systems (CRS). Specifically, our methods
can adaptively balance the two aspects of exr/exp by automatically learning the
optimal tradeoff. This consists of optimizing a utility function represented by
a linearized form of the probability distributions of the rewards of the
clicked and the non-clicked documents already recommended. Within an offline
simulation framework we apply our algorithms to a CRS and conduct an evaluation
with real event log data. The experimental results and detailed analysis
demonstrate that our algorithms outperform existing algorithms in terms of
click-through-rate (CTR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0489</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0489</id><created>2013-03-03</created><authors><author><keyname>Patil</keyname><forenames>Leena H.</forenames></author><author><keyname>Atique</keyname><forenames>Mohammed</forenames></author></authors><title>A Semantic approach for effective document clustering using WordNet</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now a days, the text document is spontaneously increasing over the internet,
e-mail and web pages and they are stored in the electronic database format. To
arrange and browse the document it becomes difficult. To overcome such problem
the document preprocessing, term selection, attribute reduction and maintaining
the relationship between the important terms using background knowledge,
WordNet, becomes an important parameters in data mining. In these paper the
different stages are formed, firstly the document preprocessing is done by
removing stop words, stemming is performed using porter stemmer algorithm, word
net thesaurus is applied for maintaining relationship between the important
terms, global unique words, and frequent word sets get generated, Secondly,
data matrix is formed, and thirdly terms are extracted from the documents by
using term selection approaches tf-idf, tf-df, and tf2 based on their minimum
threshold value. Further each and every document terms gets preprocessed, where
the frequency of each term within the document is counted for representation.
The purpose of this approach is to reduce the attributes and find the effective
term selection method using WordNet for better clustering accuracy. Experiments
are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and
ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group
(Hardware), 20 News group (Computer Graphics) etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0490</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0490</id><created>2013-03-03</created><authors><author><keyname>Ghanshala</keyname><forenames>Kamal Kumar</forenames></author><author><keyname>Pant</keyname><forenames>Durgesh</forenames></author><author><keyname>Pandey</keyname><forenames>Jatin</forenames></author></authors><title>A Gaps Approach to Access the Efficiency and Effectiveness of
  IT-Initiatives In Rural Areas: case study of Samalta, a village in the
  central Himalayan Region of India</title><categories>cs.CY</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol. 4, No. 2, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the effectiveness and efficiency of IT initiatives in
rural areas where topology creates isolation to developmental activities. A
village is selected for the study and information is gathered through
interviews of village dwellers. These collected responses are then analyzed and
a gaps model is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0503</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0503</id><created>2013-03-03</created><updated>2013-08-31</updated><authors><author><keyname>Liu</keyname><forenames>Xiaogang</forenames></author><author><keyname>Luo</keyname><forenames>Yuan</forenames></author></authors><title>The Weight Distributions of a Class of Cyclic Codes with Three Nonzeros
  over F3</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cyclic codes have efficient encoding and decoding algorithms. The decoding
error probability and the undetected error probability are usually bounded by
or given from the weight distributions of the codes. Most researches are about
the determination of the weight distributions of cyclic codes with few
nonzeros, by using quadratic form and exponential sum but limited to low
moments. In this paper, we focus on the application of higher moments of the
exponential sum to determine the weight distributions of a class of ternary
cyclic codes with three nonzeros, combining with not only quadratic form but
also MacWilliams' identities. Another application of this paper is to emphasize
the computer algebra system Magma for the investigation of the higher moments.
In the end, the result is verified by one example using Matlab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0520</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0520</id><created>2013-03-03</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author><author><keyname>Majoros</keyname><forenames>Zsuzsa</forenames></author></authors><title>Comparison of the Information Technology Development in Slovakia and
  Hungary</title><categories>cs.CY</categories><comments>6 pages, 3 figures</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 4, No. 2, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays the role of information is increasingly important, so every company
has to provide the efficient procurement, processing, storage and visualization
of this special resource in hope to stay competitive. More and more enterprises
introduce Enterprise Resource Planning System to be able to perform the listed
functions. The article illustrates the usage of these systems in Hungary and
Slovakia, as well as tests the following presumption: the level of Information
Technology (IT) development is lower in Hungary than our northern neighbor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0523</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0523</id><created>2013-03-03</created><authors><author><keyname>Gerbner</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>M&#xe9;sz&#xe1;ros</keyname><forenames>Viola</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author><author><keyname>Pokrovskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author></authors><title>Advantage in the discrete Voronoi game</title><categories>math.CO cs.DM</categories><journal-ref>Journal of Graph Algorithms and Applications 18, no. 3 (2014),
  439-455</journal-ref><doi>10.7155/jgaa.00331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the discrete Voronoi game, where two players alternately claim
vertices of a graph for t rounds. In the end, the remaining vertices are
divided such that each player receives the vertices that are closer to his or
her claimed vertices. We prove that there are graphs for which the second
player gets almost all vertices in this game, but this is not possible for
bounded-degree graphs. For trees, the first player can get at least one quarter
of the vertices, and we give examples where she can get only little more than
one third of them. We make some general observations, relating the result with
many rounds to the result for the one-round game on the same graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0525</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0525</id><created>2013-03-03</created><authors><author><keyname>Aslanyan</keyname><forenames>Hakob</forenames></author><author><keyname>Rolim</keyname><forenames>Jose</forenames></author></authors><title>Quantitative Characterization of Randomly Roving Agents</title><categories>cs.NI</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative characterization of randomly roving agents in Agent Based
Intrusion Detection Environment (ABIDE) is studied. Formula simplifications
regarding known results and publications are given. Extended Agent Based
Intrusion Detection Environment (EABIDE) is introduced and quantitative
characterization of roving agents in EABIDE is studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0529</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0529</id><created>2013-03-03</created><updated>2013-04-22</updated><authors><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Guidotti</keyname><forenames>Alessandro</forenames></author><author><keyname>Corazza</keyname><forenames>Giovanni E.</forenames></author></authors><title>Average Rate of Downlink Heterogeneous Cellular Networks over
  Generalized Fading Channels - A Stochastic Geometry Approach</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Communications, to
  appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an analytical framework to compute the average
rate of downlink heterogeneous cellular networks. The framework leverages
recent application of stochastic geometry to other-cell interference modeling
and analysis. The heterogeneous cellular network is modeled as the
superposition of many tiers of Base Stations (BSs) having different transmit
power, density, path-loss exponent, fading parameters and distribution, and
unequal biasing for flexible tier association. A long-term averaged maximum
biased-received-power tier association is considered. The positions of the BSs
in each tier are modeled as points of an independent Poisson Point Process
(PPP). Under these assumptions, we introduce a new analytical methodology to
evaluate the average rate, which avoids the computation of the Coverage
Probability (Pcov) and needs only the Moment Generating Function (MGF) of the
aggregate interference at the probe mobile terminal. The distinguishable
characteristic of our analytical methodology consists in providing a tractable
and numerically efficient framework that is applicable to general fading
distributions, including composite fading channels with small- and mid-scale
fluctuations. In addition, our method can efficiently handle correlated
Log-Normal shadowing with little increase of the computational complexity. The
proposed MGF-based approach needs the computation of either a single or a
two-fold numerical integral, thus reducing the complexity of Pcov-based
frameworks, which require, for general fading distributions, the computation of
a four-fold integral.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0539</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0539</id><created>2013-03-03</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Ablahad</keyname><forenames>Anar Auda</forenames></author></authors><title>Novel Method for Mutational Disease Prediction using Bioinformatics
  Techniques and Backpropagation Algorithm</title><categories>cs.CE</categories><comments>7 Pages, 9 Figuers, Table 1;
  http://www.estij.org/papers/vol3no12013/23vol3no1.pdf, February 2013. arXiv
  admin note: text overlap with arXiv:1205.1923 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cancer is one of the most feared diseases in the world it has increased
disturbingly and breast cancer occurs in one out of eight women, the prediction
of malignancies plays essential roles not only in revealing human genome, but
also in discovering effective prevention and treatment of cancers. Generally
cancer disease driven by somatic mutations in an individual DNA sequence, or
genome that accumulates during the lifetime of person. This paper is proposed a
novel method can predict the disease by mutations despite The presence in gene
sequence is not necessary it are malignant, so will be compare the protein of
patient with the gene's protein of disease if there is difference between these
two proteins then can say there is malignant mutations. This method will use
bioinformatics techniques like FASTA, CLUSTALW, etc which shows whether
malignant mutations or not, then training the backpropagation algorithm using
all expected malignant mutations for a certain genes (e.g. BRCA1 and BRCA2) of
disease, and using it to test whether patient is holder the disease or not.
Implementing this novel method as the first way to predict the disease based on
mutations in the sequence of the gene that causes the disease shows two
decisions are achieved successfully, the first diagnose whether the patient has
mutations of cancer or not using bioinformatics techniques the second
classifying these mutations are related to breast cancer (e.g. BRCA1 and BRCA2)
using backpropagation with mean square rate 0.0000001. Keywords-Gene sequence;
Protein; Deoxyribonucleic Acid DNA; Malignant mutation; Bioinformatics;
Back-propagation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0540</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0540</id><created>2013-03-03</created><authors><author><keyname>Hassani</keyname><forenames>S. Hamed</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>The Space of Solutions of Coupled XORSAT Formulae</title><categories>cond-mat.dis-nn cs.DM cs.IT math.IT</categories><comments>Submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The XOR-satisfiability (XORSAT) problem deals with a system of $n$ Boolean
variables and $m$ clauses. Each clause is a linear Boolean equation (XOR) of a
subset of the variables. A $K$-clause is a clause involving $K$ distinct
variables. In the random $K$-XORSAT problem a formula is created by choosing
$m$ $K$-clauses uniformly at random from the set of all possible clauses on $n$
variables. The set of solutions of a random formula exhibits various
geometrical transitions as the ratio $\frac{m}{n}$ varies.
  We consider a {\em coupled} $K$-XORSAT ensemble, consisting of a chain of
random XORSAT models that are spatially coupled across a finite window along
the chain direction. We observe that the threshold saturation phenomenon takes
place for this ensemble and we characterize various properties of the space of
solutions of such coupled formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0542</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0542</id><created>2013-03-03</created><updated>2013-11-03</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>A multidimensional tropical optimization problem with nonlinear
  objective function and linear constraints</title><categories>math.OC cs.SY</categories><comments>29 pages, 7 figures, accepted for publication in Optimization</comments><msc-class>65K10 (Primary) 15A80, 90C48, 90B35 (Secondary)</msc-class><journal-ref>Optimization, 2015. Vol. 64, N 5. P. 1107-1129</journal-ref><doi>10.1080/02331934.2013.840624</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a multidimensional optimisation problem in the tropical
mathematics setting. The problem involves the minimisation of a nonlinear
function defined on a finite-dimensional semimodule over an idempotent
semifield subject to linear inequality constraints. We start with an overview
of known tropical optimisation problems with linear and nonlinear objective
functions. A short introduction to tropical algebra is provided to offer a
formal framework for solving the problem under study. As a preliminary result,
a solution to a linear inequality with an arbitrary matrix is presented. We
describe an example optimisation problem drawn from project scheduling and then
offer a general representation of the problem. To solve the problem, we
introduce an additional variable and reduce the problem to the solving of a
linear inequality, in which the variable plays the role of a parameter. A
necessary and sufficient condition for the inequality to hold is used to
evaluate the parameter, whereas the solution to the inequality is considered a
solution to the problem. Based on this approach, a complete direct solution in
a compact vector form is derived for the optimisation problem under fairly
general conditions. Numerical and graphical examples for two-dimensional
problems are given to illustrate the obtained results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0551</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0551</id><created>2013-03-03</created><updated>2014-05-07</updated><authors><author><keyname>Papailiopoulos</keyname><forenames>Dimitris S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Korokythakis</keyname><forenames>Stavros</forenames></author></authors><title>Sparse PCA through Low-rank Approximations</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>Long version of the ICML 2013 paper:
  http://jmlr.org/proceedings/papers/v28/papailiopoulos13.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel algorithm that computes the $k$-sparse principal
component of a positive semidefinite matrix $A$. Our algorithm is combinatorial
and operates by examining a discrete set of special vectors lying in a
low-dimensional eigen-subspace of $A$. We obtain provable approximation
guarantees that depend on the spectral decay profile of the matrix: the faster
the eigenvalue decay, the better the quality of our approximation. For example,
if the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time
approximation algorithm for any desired accuracy.
  A key algorithmic component of our scheme is a combinatorial feature
elimination step that is provably safe and in practice significantly reduces
the running complexity of our algorithm. We implement our algorithm and test it
on multiple artificial and real data sets. Due to the feature elimination step,
it is possible to perform sparse PCA on data sets consisting of millions of
entries in a few minutes. Our experimental evaluation shows that our scheme is
nearly optimal while finding very sparse vectors. We compare to the prior state
of the art and show that our scheme matches or outperforms previous algorithms
in all tested data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0556</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0556</id><created>2013-03-03</created><updated>2014-05-12</updated><authors><author><keyname>Yousefi</keyname><forenames>Siamak</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author></authors><title>A Joint Localization and Clock Bias Estimation Technique Using
  Time-of-Arrival at Multiple Antenna Receivers</title><categories>cs.SY cs.IT math.IT</categories><comments>There are some mistakes so we prefer to remove it</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a system scheme is proposed for tracking a radio emitting
target moving in two-dimensional space. The localization is based on the use of
biased time-of-arrival (TOA) measurements obtained at two asynchronous
receivers, each equipped with two closely spaced antennas. By exploiting the
multi-antenna configuration and using all the TOA measurements up to current
time step, the relative clock bias at each receiver and the target position are
jointly estimated by solving a nonlinear least-squares (NLS) problem. To this
end, a novel time recursive algorithm is proposed which fully takes advantage
of the problem structure to achieve computational efficiency while using
orthogonal transformations to ensure numerical reliability. Simulations show
that the mean-squared error (MSE) of the proposed method is much smaller than
that of existing methods with the same antenna scheme, and approaches the
Cramer-Rao lower bound (CRLB) closely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0557</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0557</id><created>2013-03-03</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Xinran</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>Security Analysis on &quot;An Authentication Code Against Pollution Attacks
  in Network Coding&quot;</title><categories>cs.CR cs.IT math.IT</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:0909.3146 by other
  authors</comments><msc-class>94A60</msc-class><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the security of the authentication code against pollution attacks
in network coding given by Oggier and Fathi and show one way to remove one very
strong condition they required. Actually, we find a way to attack their
authentication scheme. In their scheme, they considered that if some malicious
nodes in the network collude to make pollution in the network flow or make
substitution attacks to other nodes, they thought these malicious nodes must
solve a system of linear equations to recover the secret parameters. Then they
concluded that their scheme is an unconditional secure scheme. Actually, note
that the authentication tag in the scheme of Oggier and Fathi is nearly linear
on the messages, so it is very easy for any malicious node to make pollution
attack in the network flow, replacing the vector of any incoming edge by linear
combination of his incoming vectors whose coefficients have sum 1. And if the
coalition of malicious nodes can carry out decoding of the network coding, they
can easily make substitution attack to any other node even if they do not know
any information of the private key of the node. Moreover, even if their scheme
can work fruitfully, the condition in their scheme $H\leqslant M$ in a network
can be removed, where $H$ is the sum of numbers of the incoming edges at
adversaries. Under the condition $H\leqslant M$, $H$ may be large, so we need
large parameter $M$ which increases the cost of computation a lot. On the other
hand, the parameter $M$ can not be very large as it can not exceed the length
of original messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0561</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0561</id><created>2013-03-03</created><updated>2013-08-22</updated><authors><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Top-down particle filtering for Bayesian decision trees</title><categories>stat.ML cs.LG</categories><comments>ICML 2013</comments><journal-ref>JMLR W&amp;CP 28(3):280-288, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision tree learning is a popular approach for classification and
regression in machine learning and statistics, and Bayesian
formulations---which introduce a prior distribution over decision trees, and
formulate learning as posterior inference given data---have been shown to
produce competitive performance. Unlike classic decision tree learning
algorithms like ID3, C4.5 and CART, which work in a top-down manner, existing
Bayesian algorithms produce an approximation to the posterior distribution by
evolving a complete tree (or collection thereof) iteratively via local Monte
Carlo modifications to the structure of the tree, e.g., using Markov chain
Monte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that
instead works in a top-down manner, mimicking the behavior and speed of classic
algorithms. We demonstrate empirically that our approach delivers accuracy
comparable to the most popular MCMC method, but operates more than an order of
magnitude faster, and thus represents a better computation-accuracy tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0566</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0566</id><created>2013-03-03</created><authors><author><keyname>Zaki</keyname><forenames>T.</forenames><affiliation>IRFSIC Laboratory, Ibn Zohr University Agadir Morocco</affiliation><affiliation>LITIS Laboratory, University of Rouen France</affiliation></author><author><keyname>Amrouch</keyname><forenames>M.</forenames><affiliation>IRFSIC Laboratory, Ibn Zohr University Agadir Morocco</affiliation></author><author><keyname>Mammass</keyname><forenames>D.</forenames><affiliation>IRFSIC Laboratory, Ibn Zohr University Agadir Morocco</affiliation></author><author><keyname>Ennaji</keyname><forenames>A.</forenames><affiliation>LITIS Laboratory, University of Rouen France</affiliation></author></authors><title>Arabic documents classification using fuzzy R.B.F. classifier with
  sliding window</title><categories>cs.IR</categories><comments>5 pages, 2 figures</comments><journal-ref>Journal of Computing , eISSN 2151-9617 , Volume 5, Issue 1,
  January 2013</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, we propose a system for contextual and semantic Arabic
documents classification by improving the standard fuzzy model. Indeed,
promoting neighborhood semantic terms that seems absent in this model by using
a radial basis modeling. In order to identify the relevant documents to the
query. This approach calculates the similarity between related terms by
determining the relevance of each relative to documents (NEAR operator), based
on a kernel function. The use of sliding window improves the process of
classification. The results obtained on a arabic dataset of press show very
good performance compared with the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0567</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0567</id><created>2013-03-03</created><updated>2015-04-08</updated><authors><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author></authors><title>Adjacent-Channel Interference in Frequency-Hopping Ad Hoc Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, 1 table, to appear at International Conference on
  Communications (ICC) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers ad hoc networks that use the combination of coded
continuous-phase frequency-shift keying (CPFSK) and frequency-hopping multiple
access. Although CPFSK has a compact spectrum, some of the signal power
inevitably splatters into adjacent frequency channels, thereby causing
adjacent-channel interference (ACI). The amount of ACI is controlled by setting
the fractional in-band power; i.e., the fraction of the signal power that lies
within the band of each frequency channel. While this quantity is often
selected arbitrarily, a tradeoff is involved in the choice. This paper presents
a new analysis of frequency-hopping ad hoc networks that carefully incorporates
the effect of ACI. The analysis accounts for the shadowing, Nakagami fading,
CPFSK modulation index, code rate, number of frequency channels, fractional
in-band power, and spatial distribution of the interfering mobiles. Expressions
are presented for both outage probability and transmission capacity. With the
objective of maximizing the transmission capacity, the optimal fractional
in-band power that should be contained in each frequency channel is identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0572</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0572</id><created>2013-03-03</created><authors><author><keyname>Yang</keyname><forenames>En-hui</forenames></author><author><keyname>Meng</keyname><forenames>Jin</forenames></author></authors><title>New Non-asymptotic Random Channel Coding Theorems</title><categories>cs.IT math.IT</categories><comments>48 pages and 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New non-asymptotic random coding theorems (with error probability $\epsilon$
and finite block length $n$) based on Gallager parity check ensemble and
Shannon random code ensemble with a fixed codeword type are established for
discrete input arbitrary output channels. The resulting non-asymptotic
achievability bounds, when combined with non-asymptotic equipartition
properties developed in the paper, can be easily computed. Analytically, these
non-asymptotic achievability bounds are shown to be asymptotically tight up to
the second order of the coding rate as $n$ goes to infinity with either
constant or sub-exponentially decreasing $\epsilon$. Numerically, they are also
compared favourably, for finite $n$ and $\epsilon$ of practical interest, with
existing non-asymptotic achievability bounds in the literature in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0582</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0582</id><created>2013-03-03</created><updated>2013-10-04</updated><authors><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Multiple Kernel Sparse Representations for Supervised and Unsupervised
  Learning</title><categories>cs.CV</categories><doi>10.1109/TIP.2014.2322938</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex visual recognition tasks it is typical to adopt multiple
descriptors, that describe different aspects of the images, for obtaining an
improved recognition performance. Descriptors that have diverse forms can be
fused into a unified feature space in a principled manner using kernel methods.
Sparse models that generalize well to the test data can be learned in the
unified kernel space, and appropriate constraints can be incorporated for
application in supervised and unsupervised learning. In this paper, we propose
to perform sparse coding and dictionary learning in the multiple kernel space,
where the weights of the ensemble kernel are tuned based on graph-embedding
principles such that class discrimination is maximized. In our proposed
algorithm, dictionaries are inferred using multiple levels of 1-D subspace
clustering in the kernel space, and the sparse codes are obtained using a
simple levelwise pursuit scheme. Empirical results for object recognition and
image clustering show that our algorithm outperforms existing sparse coding
based approaches, and compares favorably to other state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0592</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0592</id><created>2013-03-03</created><authors><author><keyname>Huang</keyname><forenames>Yichao</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Random Beamforming with Heterogeneous Users and Selective Feedback:
  Individual Sum Rate and Individual Scaling Laws</title><categories>cs.IT math.IT</categories><comments>Submitted in March 2012. To appear in IEEE Transactions on Wireless
  Communications. Part of this paper builds upon the following letter: Y. Huang
  and B. D. Rao, &quot;Closed form sum rate of random beamforming&quot;, IEEE Commun.
  Lett., vol. 16, no. 5, pp. 630-633, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates three open problems in random beamforming based
communication systems: the scheduling policy with heterogeneous users, the
closed form sum rate, and the randomness of multiuser diversity with selective
feedback. By employing the cumulative distribution function based scheduling
policy, we guarantee fairness among users as well as obtain multiuser diversity
gain in the heterogeneous scenario. Under this scheduling framework, the
individual sum rate, namely the average rate for a given user multiplied by the
number of users, is of interest and analyzed under different feedback schemes.
Firstly, under the full feedback scheme, we derive the closed form individual
sum rate by employing a decomposition of the probability density function of
the selected user's signal-to-interference-plus-noise ratio. This technique is
employed to further obtain a closed form rate approximation with selective
feedback in the spatial dimension. The analysis is also extended to random
beamforming in a wideband OFDMA system with additional selective feedback in
the spectral dimension wherein only the best beams for the best-L resource
blocks are fed back. We utilize extreme value theory to examine the randomness
of multiuser diversity incurred by selective feedback. Finally, by leveraging
the tail equivalence method, the multiplicative effect of selective feedback
and random observations is observed to establish the individual rate scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0594</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0594</id><created>2013-03-03</created><updated>2013-05-11</updated><authors><author><keyname>Kalogerias</keyname><forenames>Dionysios S.</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author></authors><title>On the Coherence Properties of Random Euclidean Distance Matrices</title><categories>cs.IT math.IT</categories><comments>5 pages, SPAWC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we focus on the coherence properties of general random
Euclidean distance matrices, which are very closely related to the respective
matrix completion problem. This problem is of great interest in several
applications such as node localization in sensor networks with limited
connectivity. Our results can directly provide the sufficient conditions under
which an EDM can be successfully recovered with high probability from a limited
number of measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0597</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0597</id><created>2013-03-03</created><updated>2013-07-09</updated><authors><author><keyname>Zhu</keyname><forenames>Tao</forenames></author><author><keyname>Phipps</keyname><forenames>David</forenames></author><author><keyname>Pridgen</keyname><forenames>Adam</forenames></author><author><keyname>Crandall</keyname><forenames>Jedidiah R.</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>The Velocity of Censorship: High-Fidelity Detection of Microblog Post
  Deletions</title><categories>cs.CY cs.IR cs.SI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.6166</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weibo and other popular Chinese microblogging sites are well known for
exercising internal censorship, to comply with Chinese government requirements.
This research seeks to quantify the mechanisms of this censorship: how fast and
how comprehensively posts are deleted.Our analysis considered 2.38 million
posts gathered over roughly two months in 2012, with our attention focused on
repeatedly visiting &quot;sensitive&quot; users. This gives us a view of censorship
events within minutes of their occurrence, albeit at a cost of our data no
longer representing a random sample of the general Weibo population. We also
have a larger 470 million post sampling from Weibo's public timeline, taken
over a longer time period, that is more representative of a random sample.
  We found that deletions happen most heavily in the first hour after a post
has been submitted. Focusing on original posts, not reposts/retweets, we
observed that nearly 30% of the total deletion events occur within 5- 30
minutes. Nearly 90% of the deletions happen within the first 24 hours.
Leveraging our data, we also considered a variety of hypotheses about the
mechanisms used by Weibo for censorship, such as the extent to which Weibo's
censors use retrospective keyword-based censorship, and how repost/retweet
popularity interacts with censorship. We also used natural language processing
techniques to analyze which topics were more likely to be censored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0598</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0598</id><created>2013-03-03</created><authors><author><keyname>Nafi</keyname><forenames>Kawser Wazed</forenames></author><author><keyname>Kar</keyname><forenames>Tonny Shekha</forenames></author><author><keyname>Hoque</keyname><forenames>Sayed Anisul</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>A Newer User Authentication, File encryption and Distributed Server
  Based Cloud Computing Security Architecture</title><categories>cs.DC cs.CR</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 3, No. 10, pp. 181-186, [ISSN: 2156-5570] (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud computing platform gives people the opportunity for sharing
resources, services and information among the people of the whole world. In
private cloud system, information is shared among the persons who are in that
cloud. For this, security or personal information hiding process hampers. In
this paper we have proposed new security architecture for cloud computing
platform. This ensures secure communication system and hiding information from
others. AES based file encryption system and asynchronous key system for
exchanging information or data is included in this model. This structure can be
easily applied with main cloud computing features, e.g. PaaS, SaaS and IaaS.
This model also includes onetime password system for user authentication
process. Our work mainly deals with the security system of the whole cloud
computing platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0606</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0606</id><created>2013-03-04</created><updated>2014-02-20</updated><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Quantum Information Transmission over a Partially Degradable Channel</title><categories>quant-ph cs.IT math.IT</categories><comments>7 pages, 2 figures, Journal-ref: IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a quantum coding for quantum communication over a PD
(partially degradable) degradable quantum channel. For a PD channel, the
degraded environment state can be expressed from the channel output state up to
a degrading map. PD channels can be restricted to the set of optical channels
which allows for the parties to exploit the benefits in experimental quantum
communications. We show that for a PD channel, the partial degradability
property leads to higher quantum data rates in comparison to those of a
degradable channel. The PD property is particular convenient for quantum
communications and allows one to implement the experimental quantum protocols
with higher performance. We define a coding scheme for PD-channels and give the
achievable rates of quantum communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0609</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0609</id><created>2013-03-04</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Koivisto</keyname><forenames>Mikko</forenames></author><author><keyname>M&#xe4;&#xe4;tt&#xe4;</keyname><forenames>Jussi</forenames></author></authors><title>Space--Time Tradeoffs for Subset Sum: An Improved Worst Case Algorithm</title><categories>cs.DS cs.DM</categories><acm-class>F.2.1; F.2.3; G.2.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technique of Schroeppel and Shamir (SICOMP, 1981) has long been the most
efficient way to trade space against time for the SUBSET SUM problem. In the
random-instance setting, however, improved tradeoffs exist. In particular, the
recently discovered dissection method of Dinur et al. (CRYPTO 2012) yields a
significantly improved space--time tradeoff curve for instances with strong
randomness properties. Our main result is that these strong randomness
assumptions can be removed, obtaining the same space--time tradeoffs in the
worst case. We also show that for small space usage the dissection algorithm
can be almost fully parallelized. Our strategy for dealing with arbitrary
instances is to instead inject the randomness into the dissection process
itself by working over a carefully selected but random composite modulus, and
to introduce explicit space--time controls into the algorithm by means of a
&quot;bailout mechanism&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0618</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0618</id><created>2013-03-04</created><updated>2013-04-02</updated><authors><author><keyname>Arapostathis</keyname><forenames>Ari</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek S.</forenames></author><author><keyname>Kumar</keyname><forenames>K. Suresh</forenames></author></authors><title>Convergence of The Relative Value Iteration for the Ergodic Control
  Problem of Nondegenerate Diffusions under Near-Monotone Costs</title><categories>math.OC cs.SY math.AP</categories><msc-class>93E15, 93E20</msc-class><journal-ref>SIAM Journal of Control and Optimization 52 (2014), no. 1, pp.
  1-31</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relative value iteration for the ergodic control problem under a
near-monotone running cost structure for a nondegenerate diffusion controlled
through its drift. This algorithm takes the form of a quasilinear parabolic
Cauchy initial value problem in $\RR^{d}$. We show that this Cauchy problem
stabilizes, or in other words, that the solution of the quasilinear parabolic
equation converges for every bounded initial condition in $\Cc^{2}(\RR^{d})$ to
the solution of the Hamilton--Jacobi--Bellman (HJB) equation associated with
the ergodic control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0631</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0631</id><created>2013-03-04</created><authors><author><keyname>Lin</keyname><forenames>Ying-Ting</forenames></author><author><keyname>Han</keyname><forenames>Xiao-Pu</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Modeling for the Dynamics of Human Innovative Behaviors</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to promote the innovative activities is an important problem for modern
society. In this paper, combining with the evolutionary games and information
spreading, we propose a lattice model to investigate dynamics of human
innovative behaviors based on benefit-driven assumption. Simulations show
several properties in agreement with peoples' daily cognition on innovative
behaviors, such as slow diffusion of innovative behaviors, gathering of
innovative strategy on &quot;innovative centers&quot;, and quasi-localized dynamics.
Furthermore, our model also emerges rich non-Poisson properties in the
temporal-spacial patterns of the innovative status, including the scaling law
in the interval time of innovation releases and the bimodal distributions on
the spreading range of innovations, which would be universal in human
innovative behaviors. Our model provide a basic framework on the study of the
issue relevant to the evolution of human innovative behaviors and the promotion
measurement of innovative activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0633</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0633</id><created>2013-03-04</created><authors><author><keyname>Mukherjee</keyname><forenames>Subra</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author></authors><title>Omega Model for Human Detection and Counting for application in Smart
  Surveillance System</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by the significant advancements in technology and social issues such
as security management, there is a strong need for Smart Surveillance System in
our society today. One of the key features of a Smart Surveillance System is
efficient human detection and counting such that the system can decide and
label events on its own. In this paper we propose a new, novel and robust
model, The Omega Model, for detecting and counting human beings present in the
scene. The proposed model employs a set of four distinct descriptors for
identifying the unique features of the head, neck and shoulder regions of a
person. This unique head neck shoulder signature given by the Omega Model
exploits the challenges such as inter person variations in size and shape of
peoples head, neck and shoulder regions to achieve robust detection of human
beings even under partial occlusion, dynamically changing background and
varying illumination conditions. After experimentation we observe and analyze
the influences of each of the four descriptors on the system performance and
computation speed and conclude that a weight based decision making system
produces the best results. Evaluation results on a number of images indicate
the validation of our method in actual situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0634</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0634</id><created>2013-03-04</created><authors><author><keyname>Singha</keyname><forenames>Joyeeta</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author></authors><title>Indian Sign Language Recognition Using Eigen Value Weighted Euclidean
  Distance Based Classification Technique</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sign Language Recognition is one of the most growing fields of research
today. Many new techniques have been developed recently in these fields. Here
in this paper, we have proposed a system using Eigen value weighted Euclidean
distance as a classification technique for recognition of various Sign
Languages of India. The system comprises of four parts: Skin Filtering, Hand
Cropping, Feature Extraction and Classification. Twenty four signs were
considered in this paper, each having ten samples, thus a total of two hundred
forty images was considered for which recognition rate obtained was 97 percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0635</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0635</id><created>2013-03-04</created><authors><author><keyname>Kalita</keyname><forenames>Jeemoni</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author></authors><title>Recognition of Facial Expression Using Eigenvector Based Distributed
  Features and Euclidean Distance Based Decision Making Technique</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an Eigenvector based system has been presented to recognize
facial expressions from digital facial images. In the approach, firstly the
images were acquired and cropping of five significant portions from the image
was performed to extract and store the Eigenvectors specific to the
expressions. The Eigenvectors for the test images were also computed, and
finally the input facial image was recognized when similarity was obtained by
calculating the minimum Euclidean distance between the test image and the
different expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0642</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0642</id><created>2013-03-04</created><updated>2013-03-22</updated><authors><author><keyname>Guhaniyogi</keyname><forenames>Rajarshi</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author></authors><title>Bayesian Compressed Regression</title><categories>stat.ML cs.LG</categories><comments>29 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an alternative to variable selection or shrinkage in high dimensional
regression, we propose to randomly compress the predictors prior to analysis.
This dramatically reduces storage and computational bottlenecks, performing
well when the predictors can be projected to a low dimensional linear subspace
with minimal loss of information about the response. As opposed to existing
Bayesian dimensionality reduction approaches, the exact posterior distribution
conditional on the compressed data is available analytically, speeding up
computation by many orders of magnitude while also bypassing robustness issues
due to convergence and mixing problems with MCMC. Model averaging is used to
reduce sensitivity to the random projection matrix, while accommodating
uncertainty in the subspace dimension. Strong theoretical support is provided
for the approach by showing near parametric convergence rates for the
predictive density in the large p small n asymptotic paradigm. Practical
performance relative to competitors is illustrated in simulations and real data
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0644</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0644</id><created>2013-03-04</created><authors><author><keyname>Meena</keyname><forenames>A.</forenames></author><author><keyname>Raja</keyname><forenames>K.</forenames></author></authors><title>Automatic symmetry based cluster approach for anomalous brain
  identification in PET scan image : An Analysis</title><categories>cs.CV</categories><comments>International Conference on Mathematics and Computer Science: ICMCS
  2011,ISBN: 978-81-920490-0-7, January 2011,pp.no.387 - 391</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical image segmentation is referred to the segmentation of known anatomic
structures from different medical images. Normally, the medical data researches
are more complicated and an exclusive structures. This computer aided diagnosis
is used for assisting doctors in evaluating medical imagery or in recognizing
abnormal findings in a medical image. To integrate the specialized knowledge
for medical data processing is helpful to form a real useful healthcare
decision making system. This paper studies the different symmetry based
distances applied in clustering algorithms and analyzes symmetry approach for
Positron Emission Tomography (PET) scan image segmentation. Unlike CT and MRI,
the PET scan identifies the structure of blood flow to and from organs. PET
scan also helps in early diagnosis of cancer and heart, brain and gastro
intestinal ailments and to detect the progress of treatment. In this paper, the
scope diagnostic task expands for PET image in various brain functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0645</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0645</id><created>2013-03-04</created><authors><author><keyname>Meena</keyname><forenames>A.</forenames></author><author><keyname>Raja</keyname><forenames>R.</forenames></author></authors><title>Symmetry Based Cluster Approach for Automatic Recognition of the
  Epileptic Focus in Brain Using PET Scan Image : An Analysis</title><categories>cs.CV</categories><comments>National Conference:NC4T 2011, Sathyabama University,PP. No. 61 - 63</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition of epileptic focal point is the important diagnosis when
screening the epilepsy patients for latent surgical cures. The accurate
localization is challenging one because of the low spatial resolution images
with more noisy data. Positron Emission Tomography (PET) has now replaced the
issues and caring a high resolution. This paper focuses the research of
automated localization of epileptic seizures in brain functional images using
symmetry based cluster approach. This approach presents a fully automated
symmetry based brain abnormality detection method for PET sequences. PET images
are spatially normalized to Digital Imaging and Communications in Medicine
(DICOM) standard and then it has been trained using symmetry based cluster
approach using Medical Image Processing, Analysis &amp; Visualization (MIPAV) tool.
The performance evolution is considered by the metric like accuracy of
diagnosis. The obtained result is surely assists the surgeon for the automated
identification of seizures focus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0646</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0646</id><created>2013-03-04</created><authors><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author><author><keyname>Braghin</keyname><forenames>Stefano</forenames></author><author><keyname>Yong</keyname><forenames>Jackson Tan Teck</forenames></author></authors><title>The Zen of Multidisciplinary Team Recommendation</title><categories>cs.SI cs.IR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to accomplish complex tasks, it is often necessary to compose a team
consisting of experts with diverse competencies. However, for proper
functioning, it is also preferable that a team be socially cohesive. A team
recommendation system, which facilitates the search for potential team members
can be of great help both for (i) individuals who need to seek out
collaborators and (ii) managers who need to build a team for some specific
tasks.
  A decision support system which readily helps summarize such metrics, and
possibly rank the teams in a personalized manner according to the end users'
preferences, can be a great tool to navigate what would otherwise be an
information avalanche.
  In this work we present a general framework of how to compose such subsystems
together to build a composite team recommendation system, and instantiate it
for a case study of academic teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0647</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0647</id><created>2013-03-04</created><authors><author><keyname>Meena</keyname><forenames>A.</forenames></author><author><keyname>Raja</keyname><forenames>R.</forenames></author></authors><title>Spatial Fuzzy C Means PET Image Segmentation of Neurodegenerative
  Disorder</title><categories>cs.CV</categories><journal-ref>Indian Journal of Computer Science and Engineering (IJCSE), ISSN :
  0976-5166 Vol. 4 No.1 Feb-Mar 2013, pp.no: 50-55</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nuclear image has emerged as a promising research work in medical field.
Images from different modality meet its own challenge. Positron Emission
Tomography (PET) image may help to precisely localize disease to assist in
planning the right treatment for each case and saving valuable time. In this
paper, a novel approach of Spatial Fuzzy C Means (PET SFCM) clustering
algorithm is introduced on PET scan image datasets. The proposed algorithm is
incorporated the spatial neighborhood information with traditional FCM and
updating the objective function of each cluster. This algorithm is implemented
and tested on huge data collection of patients with brain neuro degenerative
disorder such as Alzheimers disease. It has demonstrated its effectiveness by
testing it for real world patient data sets. Experimental results are compared
with conventional FCM and K Means clustering algorithm. The performance of the
PET SFCM provides satisfactory results compared with other two algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0663</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0663</id><created>2013-03-04</created><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author><author><keyname>Wu</keyname><forenames>Ji</forenames></author></authors><title>Denoising Deep Neural Networks Based Voice Activity Detection</title><categories>cs.LG cs.SD stat.ML</categories><comments>This paper has been accepted by IEEE ICASSP-2013, and will be
  published online after May, 2013</comments><doi>10.1109/ICASSP.2013.6637769</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the deep-belief-networks (DBN) based voice activity detection (VAD)
has been proposed. It is powerful in fusing the advantages of multiple
features, and achieves the state-of-the-art performance. However, the deep
layers of the DBN-based VAD do not show an apparent superiority to the
shallower layers. In this paper, we propose a denoising-deep-neural-network
(DDNN) based VAD to address the aforementioned problem. Specifically, we
pre-train a deep neural network in a special unsupervised denoising greedy
layer-wise mode, and then fine-tune the whole network in a supervised way by
the common back-propagation algorithm. In the pre-training phase, we take the
noisy speech signals as the visible layer and try to extract a new feature that
minimizes the reconstruction cross-entropy loss between the noisy speech
signals and its corresponding clean speech signals. Experimental results show
that the proposed DDNN-based VAD not only outperforms the DBN-based VAD but
also shows an apparent performance improvement of the deep layers over
shallower layers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0665</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0665</id><created>2013-03-04</created><updated>2014-11-03</updated><authors><author><keyname>Garcin</keyname><forenames>Florent</forenames></author><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author><author><keyname>Faltings</keyname><forenames>Boi</forenames></author></authors><title>Personalized News Recommendation with Context Trees</title><categories>cs.IR cs.LG stat.ML</categories><journal-ref>Proceedings of the 7th ACM conference on Recommender systems
  (2013), pp. 105--112</journal-ref><doi>10.1145/2507157.2507166</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The profusion of online news articles makes it difficult to find interesting
articles, a problem that can be assuaged by using a recommender system to bring
the most relevant news stories to readers. However, news recommendation is
challenging because the most relevant articles are often new content seen by
few users. In addition, they are subject to trends and preference changes over
time, and in many cases we do not have sufficient information to profile the
reader.
  In this paper, we introduce a class of news recommendation systems based on
context trees. They can provide high-quality news recommendation to anonymous
visitors based on present browsing behaviour. We show that context-tree
recommender systems provide good prediction accuracy and recommendation
novelty, and they are sufficiently flexible to capture the unique properties of
news articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0667</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0667</id><created>2013-03-04</created><authors><author><keyname>Pal</keyname><forenames>Dipasree</forenames></author><author><keyname>Mitra</keyname><forenames>Mandar</forenames></author><author><keyname>Datta</keyname><forenames>Kalyankumar</forenames></author></authors><title>Query Expansion Using Term Distribution and Term Association</title><categories>cs.IR</categories><comments>19 pages, 1 figure, 2 result tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good term selection is an important issue for an automatic query expansion
(AQE) technique. AQE techniques that select expansion terms from the target
corpus usually do so in one of two ways. Distribution based term selection
compares the distribution of a term in the (pseudo) relevant documents with
that in the whole corpus / random distribution. Two well-known
distribution-based methods are based on Kullback-Leibler Divergence (KLD) and
Bose-Einstein statistics (Bo1). Association based term selection, on the other
hand, uses information about how a candidate term co-occurs with the original
query terms. Local Context Analysis (LCA) and Relevance-based Language Model
(RM3) are examples of association-based methods. Our goal in this study is to
investigate how these two classes of methods may be combined to improve
retrieval effectiveness. We propose the following combination-based approach.
Candidate expansion terms are first obtained using a distribution based method.
This set is then refined based on the strength of the association of terms with
the original query terms. We test our methods on 11 TREC collections. The
proposed combinations generally yield better results than each individual
method, as well as other state-of-the-art AQE approaches. En route to our
primary goal, we also propose some modifications to LCA and Bo1 which lead to
improved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0669</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0669</id><created>2013-03-04</created><authors><author><keyname>Kumagai</keyname><forenames>Wataru</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Second Order Asymptotics for Random Number Generation</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures</comments><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We treat a random number generation from an i.i.d. probability distribution
of $P$ to that of $Q$. When $Q$ or $P$ is a uniform distribution, the problems
have been well-known as the uniform random number generation and the
resolvability problem respectively, and analyzed not only in the context of the
first order asymptotic theory but also that in the second asymptotic theory. On
the other hand, when both $P$ and $Q$ are not a uniform distribution, the
second order asymptotics has not been treated. In this paper, we focus on the
second order asymptotics of a random number generation for arbitrary
probability distributions $P$ and $Q$ on a finite set. In particular, we derive
the optimal second order generation rate under an arbitrary permissible
confidence coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0691</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0691</id><created>2013-03-04</created><updated>2014-01-17</updated><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Jose M.</forenames></author></authors><title>Learning AMP Chain Graphs and some Marginal Models Thereof under
  Faithfulness: Extended Version</title><categories>stat.ML cs.AI cs.LG</categories><comments>Changes from v1 to v2: The interpretation of the antecedent of the
  rule R3 changed, which in turn implied modifying Lemma 6 and Theorem 1.
  Changes from v2 to v3: Minor improvements in the first 12 pages. A shorter
  version is to appear in International Journal of Approximate Reasoning, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)
interpretation. In particular, we present a constraint based algorithm for
learning an AMP chain graph a given probability distribution is faithful to.
Moreover, we show that the extension of Meek's conjecture to AMP chain graphs
does not hold, which compromises the development of efficient and correct
score+search learning algorithms under assumptions weaker than faithfulness.
  We also introduce a new family of graphical models that consists of
undirected and bidirected edges. We name this new family maximal
covariance-concentration graphs (MCCGs) because it includes both covariance and
concentration graphs as subfamilies. However, every MCCG can be seen as the
result of marginalizing out some nodes in an AMP CG. We describe global, local
and pairwise Markov properties for MCCGs and prove their equivalence. We
characterize when two MCCGs are Markov equivalent, and show that every Markov
equivalence class of MCCGs has a distinguished member. We present a constraint
based algorithm for learning a MCCG a given probability distribution is
faithful to.
  Finally, we present a graphical criterion for reading dependencies from a
MCCG of a probability distribution that satisfies the graphoid properties, weak
transitivity and composition. We prove that the criterion is sound and complete
in certain sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0695</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0695</id><created>2013-03-04</created><authors><author><keyname>Yassaee</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Non-Asymptotic Output Statistics of Random Binning and Its Applications</title><categories>cs.IT math.IT</categories><comments>A shot version has been submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a finite blocklength version of the Output
Statistics of Random Binning (OSRB) framework. The framework is shown to be
optimal in the point-to-point case. New second order regions for broadcast
channel and wiretap channel with strong secrecy criterion are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0696</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0696</id><created>2013-03-04</created><authors><author><keyname>Yassaee</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>A Technique for Deriving One-Shot Achievability Results in Network
  Information Theory</title><categories>cs.IT math.IT</categories><comments>A short version has been submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel technique to prove a one-shot version of
achievability results in network information theory. The technique is not based
on covering and packing lemmas. In this technique, we use an stochastic encoder
and decoder with a particular structure for coding that resembles both the ML
and the joint-typicality coders. Although stochastic encoders and decoders do
not usually enhance the capacity region, their use simplifies the analysis. The
Jensen inequality lies at the heart of error analysis, which enables us to deal
with the expectation of many terms coming from stochastic encoders and decoders
at once. The technique is illustrated via several examples: point-to-point
channel coding, Gelfand-Pinsker, Broadcast channel (Marton), Berger-Tung,
Heegard-Berger/Kaspi, Multiple description coding and Joint source-channel
coding over a MAC. Most of our one-shot results are new. The asymptotic forms
of these expressions is the same as that of classical results. Our one-shot
bounds in conjunction with multi-dimensional Berry-Essen CLT imply new results
in the finite blocklength regime. In particular applying the one-shot result
for the memoryless broadcast channel in the asymptotic case, we get the entire
region of Marton's inner bound without any need for time-sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0699</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0699</id><created>2013-03-04</created><authors><author><keyname>Romano</keyname><forenames>Gianmarco</forenames></author><author><keyname>Ciuonzo</keyname><forenames>Domenico</forenames></author><author><keyname>Rossi</keyname><forenames>Pierluigi Salvo</forenames></author><author><keyname>Palmieri</keyname><forenames>Francesco</forenames></author></authors><title>Low-complexity dominance-based Sphere Decoder for MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sphere decoder (SD) is an attractive low-complexity alternative to
maximum likelihood (ML) detection in a variety of communication systems. It is
also employed in multiple-input multiple-output (MIMO) systems where the
computational complexity of the optimum detector grows exponentially with the
number of transmit antennas. We propose an enhanced version of the SD based on
an additional cost function derived from conditions on worst case interference,
that we call dominance conditions. The proposed detector, the king sphere
decoder (KSD), has a computational complexity that results to be not larger
than the complexity of the sphere decoder and numerical simulations show that
the complexity reduction is usually quite significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0707</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0707</id><created>2013-03-04</created><updated>2014-04-18</updated><authors><author><keyname>Ferrante</keyname><forenames>Augusto</forenames></author><author><keyname>Laurenti</keyname><forenames>Nicola</forenames></author><author><keyname>Masiero</keyname><forenames>Chiara</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author></authors><title>On the Achievable Error Region of Physical Layer Authentication
  Techniques over Rayleigh Fading Channels</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a physical layer message authentication procedure based on the comparison
of channel estimates obtained from the received messages, we focus on an outer
bound on the type I/II error probability region. Channel estimates are modelled
as multivariate Gaussian vectors, and we assume that the attacker has only some
side information on the channel estimate, which he does not know directly. We
derive the attacking strategy that provides the tightest bound on the error
region, given the statistics of the side information. This turns out to be a
zero mean, circularly symmetric Gaussian density whose correlation matrices may
be obtained by solving a constrained optimization problem. We propose an
iterative algorithm for its solution: Starting from the closed form solution of
a relaxed problem, we obtain, by projection, an initial feasible solution;
then, by an iterative procedure, we look for the fixed point solution of the
problem. Numerical results show that for cases of interest the iterative
approach converges, and perturbation analysis shows that the found solution is
a local minimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0716</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0716</id><created>2013-03-04</created><updated>2013-04-08</updated><authors><author><keyname>Kyureghyan</keyname><forenames>Gohar M.</forenames></author><author><keyname>Suder</keyname><forenames>Valentin</forenames></author></authors><title>On Inversion in Z_{2^n-1}</title><categories>math.NT cs.DM</categories><comments>The first part of this work is an extended version of the results
  presented in ISIT12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we determined explicitly the multiplicative inverses of the
Dobbertin and Welch APN exponents in Z_{2^n-1}, and we described the binary
weights of the inverses of the Gold and Kasami exponents. We studied the
function \de(n), which for a fixed positive integer d maps integers n\geq 1 to
the least positive residue of the inverse of d modulo 2^n-1, if it exists. In
particular, we showed that the function \de is completely determined by its
values for 1 \leq n \leq \ordb, where \ordb is the order of 2 modulo the
largest odd divisor of d.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0718</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0718</id><created>2013-03-04</created><authors><author><keyname>Bronski</keyname><forenames>Jared C.</forenames></author><author><keyname>DeVille</keyname><forenames>Lee</forenames></author></authors><title>Spectral Theory for Networks with Attractive and Repulsive Interactions</title><categories>math.SP cond-mat.dis-nn cs.SI</categories><comments>27 pages; 9 Figures</comments><msc-class>91D30, 34D20, 05C22,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a wealth of applied problems that can be posed as a dynamical system
defined on a network with both attractive and repulsive interactions. Some
examples include: understanding synchronization properties of nonlinear
oscillator;, the behavior of groups, or cliques, in social networks; the study
of optimal convergence for consensus algorithm; and many other examples.
Frequently the problems involve computing the index of a matrix, i.e. the
number of positive and negative eigenvalues, and the dimension of the kernel.
In this paper we consider one of the most common examples, where the matrix
takes the form of a signed graph Laplacian. We show that the there are
topological constraints on the index of the Laplacian matrix related to the
dimension of a certain homology group. In certain situations, when the homology
group is trivial, the index of the operator is rigid and is determined only by
the topology of the network and is independent of the strengths of the
interactions. In general these constraints give upper and lower bounds on the
number of positive and negative eigenvalues, with the dimension of the homology
group counting the number of eigenvalue crossings. The homology group also
gives a natural decomposition of the dynamics into &quot;fixed&quot; degrees of freedom,
whose index does not depend on the edge-weights, and an orthogonal set of
&quot;free&quot; degrees of freedom, whose index changes as the edge weights change. We
also present some numerical studies of this problem for large random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0722</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0722</id><created>2013-03-04</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Kosar</keyname><forenames>Toma&#x17e;</forenames></author><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Mernik</keyname><forenames>Marjan</forenames></author></authors><title>EasyTime++: A case study of incremental domain-specific language
  development</title><categories>cs.PL</categories><journal-ref>Information technology and control, 42(1), 77--85, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EasyTime is a domain-specific language (DSL) for measuring time during sports
competitions. A distinguishing feature of DSLs is that they are much more
amenable to change, and EasyTime is no exception in this regard. This paper
introduces two new EasyTime features: classifications of competitors into
categories, and the inclusion of competitions where the number of laps must be
dynamically determined. It shows how such extensions can be incrementally added
into the base-language reusing most of the language specifications. Two case
studies are presented showing the suitability of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0725</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0725</id><created>2013-03-04</created><authors><author><keyname>Sharma</keyname><forenames>Namita</forenames></author><author><keyname>Sahula</keyname><forenames>Vineet</forenames></author><author><keyname>Ravikumar</keyname><forenames>C. P.</forenames></author></authors><title>Energy Aware Task Scheduling for Soft Real Time Systems using an
  Analytical Approach for Energy Estimation</title><categories>cs.OH</categories><comments>12 pages, 4 Figures, 3 Tables</comments><journal-ref>IJASCSE, VOL 1, ISSUE 4, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded systems have pervaded all walks of our life. With the increasing
importance of mobile embedded systems and flexible applications, considerable
progress in research has been made for power management. Power constraints are
increasingly becoming the critical component of the design specifications of
these systems. It helps in pre-determining the suitable hardware architecture
for the target application. The aim of this paper is to present a technique to
estimate 'pre-run time' and 'power' of a software mapped onto a hardware
system; guaranteeing the compliance of temporal constraints while generating a
schedule of tasks of software. Real time systems must handle several
independent macro-tasks, each represented by a task graph, which includes
communications and precedence constraints. We propose a novel approach for
power estimation of embedded software using the Control Data Flow Graph (CDFG)
or task graph model. This methodology uses an existing Hierarchical Concurrent
Flow Graph (HCFG) technique for the power analysis of the CDFGs. We have
evaluated our technique for energy efficient scheduling over various task graph
benchmarks. The results obtained prove the utility and efficacy of our proposed
approach for power analysis of embedded software. We also present a methodology
to obtain an energy optimal voltage assignment and perform scheduling by taking
advantage of the relaxation in execution time of tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0726</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0726</id><created>2013-03-04</created><updated>2013-08-09</updated><authors><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Hellerstein</keyname><forenames>Lisa</forenames></author><author><keyname>Kletenik</keyname><forenames>Devorah</forenames></author></authors><title>Approximation Algorithms for Stochastic Boolean Function Evaluation and
  Stochastic Submodular Set Cover</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Boolean Function Evaluation is the problem of determining the
value of a given Boolean function f on an unknown input x, when each bit of x_i
of x can only be determined by paying an associated cost c_i. The assumption is
that x is drawn from a given product distribution, and the goal is to minimize
the expected cost. This problem has been studied in Operations Research, where
it is known as &quot;sequential testing&quot; of Boolean functions. It has also been
studied in learning theory in the context of learning with attribute costs. We
consider the general problem of developing approximation algorithms for
Stochastic Boolean Function Evaluation. We give a 3-approximation algorithm for
evaluating Boolean linear threshold formulas. We also present an approximation
algorithm for evaluating CDNF formulas (and decision trees) achieving a factor
of O(log kd), where k is the number of terms in the DNF formula, and d is the
number of clauses in the CNF formula. In addition, we present approximation
algorithms for simultaneous evaluation of linear threshold functions, and for
ranking of linear functions.
  Our function evaluation algorithms are based on reductions to the Stochastic
Submodular Set Cover (SSSC) problem. This problem was introduced by Golovin and
Krause. They presented an approximation algorithm for the problem, called
Adaptive Greedy. Our main technical contribution is a new approximation
algorithm for the SSSC problem, which we call Adaptive Dual Greedy. It is an
extension of the Dual Greedy algorithm for Submodular Set Cover due to Fujito,
which is a generalization of Hochbaum's algorithm for the classical Set Cover
Problem. We also give a new bound on the approximation achieved by the Adaptive
Greedy algorithm of Golovin and Krause.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0727</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0727</id><created>2013-03-04</created><authors><author><keyname>Lopes</keyname><forenames>Miles E.</forenames></author></authors><title>The Convergence Rate of Majority Vote under Exchangeability</title><categories>math.PR cs.SI math.ST stat.ML stat.TH</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Majority vote plays a fundamental role in many applications of statistics,
such as ensemble classifiers, crowdsourcing, and elections. When using majority
vote as a prediction rule, it is of basic interest to ask &quot;How many votes are
needed to obtain a reliable prediction?&quot; In the context of binary
classification with Random Forests or Bagging, we give a precise answer: If
err_t denotes the test error achieved by the majority vote of t \geq 1
classifiers, and err* denotes its nominal limiting value, then under basic
regularity conditions, err_t = err* + c/t + o(1/t), where c is a constant given
by a simple formula. More generally, we show that if V_1,V_2,... is an
exchangeable Bernoulli sequence with mixture distribution F, and the majority
vote is written as M_t=median(V_1,...,V_t), then 1-\E[M_t] = F(1/2)+
(F&quot;(1/2)/8)(1/t)+o(1/t) when F is sufficiently smooth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0728</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0728</id><created>2013-03-04</created><updated>2014-01-10</updated><authors><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>Ramakrishna</keyname><forenames>G.</forenames></author><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author></authors><title>Computing Minimum Cycle Bases in Weighted Partial 2-Trees in Linear Time</title><categories>cs.DS cs.DM</categories><comments>major revision of the paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a linear time algorithm for computing an implicit linear space
representation of a minimum cycle basis (MCB) in weighted partial 2-trees,
i.e., graphs of treewidth two. The implicit representation can be made explicit
in a running time that is proportional to the size of the MCB.
  Our algorithm improves the result of Borradaile, Sankowski, and Wulff-Nilsen
[Min $st$-cut Oracle for Planar Graphs with Near-Linear Preprocessing Time,
FOCS 2010]---which computes for all planar graphs an implicit $O(n \log n)$
space representation of an MCB in $O(n \log^5 n)$ time---by a polylog factor
for the special case of partial 2-trees. Such an improvement was achieved
previously only for outerplanar graphs [Liu and Lu: Minimum Cycle Bases of
Weighted Outerplanar Graphs, IPL 110:970--974, 2010].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0730</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0730</id><created>2013-03-04</created><updated>2014-08-30</updated><authors><author><keyname>Karimi</keyname><forenames>Ahmad</forenames></author><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author></authors><title>Diagonalizing by Fixed-Points</title><categories>math.LO cs.LO</categories><comments>12 pages</comments><msc-class>18A10, 18A15, 03B44, 03A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A universal schema for diagonalization was popularized by N. S. Yanofsky
(2003) in which the existence of a (diagonolized-out and contradictory) object
implies the existence of a fixed-point for a certain function. It was shown
that many self-referential paradoxes and diagonally proved theorems can fit in
that schema. Here, we fit more theorems in the universal schema of
diagonalization, such as Euclid's theorem on the infinitude of the primes and
new proofs of Boolos (1997) for Cantor's theorem on the non-equinumerosity of a
set with its powerset. Then, in Linear Temporal Logic, we show the
non-existence of a fixed-point in this logic whose proof resembles the argument
of Yablo's paradox. Thus, Yablo's paradox turns for the first time into a
genuine mathematico-logical theorem in the framework of Linear Temporal Logic.
Again the diagonal schema of the paper is used in this proof; and also it is
shown that G. Priest's inclosure schema (1997) can fit in our universal
diagonal/fixed-point schema. We also show the existence of dominating
(Ackermann-like) functions (which dominate a given countable set of
functions---like primitive recursives) using the schema.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0738</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0738</id><created>2013-03-04</created><authors><author><keyname>Karawia</keyname><forenames>A. A.</forenames></author></authors><title>New Symbolic Algorithms For Solving A General Bordered Tridiagonal
  Linear System</title><categories>cs.SC cs.NA math.NA</categories><msc-class>15A15, 15A23, 68W30, 11Y05, 33F10, F.2.1, G.1.0</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the author present reliable symbolic algorithms for solving a
general bordered tridiagonal linear system. The first algorithm is based on the
LU decomposition of the coefficient matrix and the computational cost of it is
O(n). The second is based on The Sherman-Morrison-Woodbury formula. The
algorithms are implementable to the Computer Algebra System (CAS) such as
MAPLE, MATLAB and MATHEMATICA. Three examples are presented for the sake of
illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0742</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0742</id><created>2013-03-04</created><authors><author><keyname>Barth&#xe9;lemy</keyname><forenames>Quentin</forenames></author><author><keyname>Gouy-Pailler</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Isaac</keyname><forenames>Yoann</forenames></author><author><keyname>Souloumiac</keyname><forenames>Antoine</forenames></author><author><keyname>Larue</keyname><forenames>Anthony</forenames></author><author><keyname>Mars</keyname><forenames>J&#xe9;r&#xf4;me I.</forenames></author></authors><title>Multivariate Temporal Dictionary Learning for EEG</title><categories>cs.LG q-bio.NC stat.ML</categories><journal-ref>Published in Journal of Neuroscience Methods, vol. 215, pp. 19-28,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the issue of representing electroencephalographic
(EEG) signals in an efficient way. While classical approaches use a fixed Gabor
dictionary to analyze EEG signals, this article proposes a data-driven method
to obtain an adapted dictionary. To reach an efficient dictionary learning,
appropriate spatial and temporal modeling is required. Inter-channels links are
taken into account in the spatial multivariate model, and shift-invariance is
used for the temporal model. Multivariate learned kernels are informative (a
few atoms code plentiful energy) and interpretable (the atoms can have a
physiological meaning). Using real EEG data, the proposed method is shown to
outperform the classical multichannel matching pursuit used with a Gabor
dictionary, as measured by the representative power of the learned dictionary
and its spatial flexibility. Moreover, dictionary learning can capture
interpretable patterns: this ability is illustrated on real data, learning a
P300 evoked potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0766</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0766</id><created>2013-03-04</created><authors><author><keyname>Hovden</keyname><forenames>Robert</forenames></author></authors><title>Bibliometrics for Internet Media: Applying the h-Index to YouTube</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>Article Accepted For Publication In: Journal Of The American Society
  For Information Science And Technology, 2013</comments><doi>10.1002/asi.22936</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The h-index can be a useful metric for evaluating a person's output of
Internet media. Here we advocate and demonstrate adaption of the h-index and
the g-index to the top video content creators on YouTube. The h-index for
Internet video media is based on videos and their view counts. The index h is
defined as the number of videos with &gt;= h*10^5 views. The index g is defined as
the number of videos with &gt;= g*10^5 views on average. When compared to a video
creator's total view count, the h-index and g-index better capture both
productivity and impact in a single metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0775</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0775</id><created>2013-03-04</created><updated>2013-06-10</updated><authors><author><keyname>Ozdemir</keyname><forenames>Onur</forenames></author><author><keyname>Li</keyname><forenames>Ruoyu</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Hybrid Maximum Likelihood Modulation Classification Using Multiple
  Radios</title><categories>cs.IT math.IT stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The performance of a modulation classifier is highly sensitive to channel
signal-to-noise ratio (SNR). In this paper, we focus on amplitude-phase
modulations and propose a modulation classification framework based on
centralized data fusion using multiple radios and the hybrid maximum likelihood
(ML) approach. In order to alleviate the computational complexity associated
with ML estimation, we adopt the Expectation Maximization (EM) algorithm. Due
to SNR diversity, the proposed multi-radio framework provides robustness to
channel SNR. Numerical results show the superiority of the proposed approach
with respect to single radio approaches as well as to modulation classifiers
using moments based estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0777</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0777</id><created>2013-03-04</created><updated>2014-10-02</updated><authors><author><keyname>Lerner</keyname><forenames>Vladimir S.</forenames></author></authors><title>Integrating hidden information which is observed and the observer
  information regularities</title><categories>nlin.AO cs.IT math.IT</categories><comments>22 pages,7 figures. arXiv admin note: text overlap with
  arXiv:1212.1710</comments><msc-class>58J65, 60J65, 93B52, 93E02, 93E15, 93E30</msc-class><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Bayesian integral functional measure of entropy-uncertainty (EF) on
trajectories of Markov multi-dimensional diffusion process is cutting off by
interactive impulses (controls). Each cutoff minimax of EF superimposes and
entangles conjugated fractions in microprocess, enclosing the captured entropy
fractions as source of an information unit. The impulse step-up action launches
the unit formation and step-down action finishes it and brings energy from the
interactive jump. This finite jump transfers the entangled entropy from
uncertain Yes-logic to the certain-information No-logic information unit whose
measuring at end of the cut kills final entropy-uncertainty and limits unit.
The Yes-No logic holds Bit Participator creating elementary information
observer without physical pre-law. Cooperating two units in doublet and an
opposite directional information unit in triplet forms minimal stable
structure. Information path functional (IPF) integrates multiple hidden
information contributions along the cutting process correlations in information
units of cooperating doublets-triplets, bound by free information, and enfolds
the sequence of enclosing triplet structures in the information network (IN)
that sequentially decreases the entropy and maximizes information. The IN bound
triplets release free information rising information forces enable attracting
new information unit and ordering it. While IPF collects the information units,
the IN performs logical computing using doublet-triplet code. The IN different
levels unite logic of quantum micro- and macro- information processes,
composing quantum and/or classical computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0780</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0780</id><created>2013-03-04</created><authors><author><keyname>Jan&#x10d;ar</keyname><forenames>Petr</forenames></author><author><keyname>Srba</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Note on Undecidability of Bisimilarity for Second-Order Pushdown
  Processes</title><categories>cs.LO cs.FL</categories><acm-class>F.4.3; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadbent and G\&quot;oller (FSTTCS 2012) proved the undecidability of
bisimulation equivalence for processes generated by epsilon-free second-order
pushdown automata. We add a few remarks concerning the used proof technique,
called Defender's forcing, and the related undecidability proof for first-order
pushdown automata with epsilon-transitions (Jan\v{c}ar and Srba, JACM 2008).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0783</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0783</id><created>2013-03-01</created><updated>2013-12-02</updated><authors><author><keyname>Li</keyname><forenames>Cong</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>Epidemic threshold in directed networks</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 13 figures</comments><journal-ref>Physical Review E, vol. 88, No. 6, p. 062802,2013</journal-ref><doi>10.1103/PhysRevE.88.062802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemics have so far been mostly studied in undirected networks. However,
many real-world networks, such as the social network Twitter and the WWW
networks, upon which information, emotion or malware spreads, are shown to be
directed networks, composed of both unidirectional links and bidirectional
links. We define the directionality as the percentage of unidirectional links.
The epidemic threshold for the susceptible-infected-susceptible (SIS) epidemic
has been proved to be 1/lambda_{1} in directed networks by N-intertwined
Mean-field Approximation, where lambda_{1}, also called as spectral radius, is
the largest eigenvalue of the adjacency matrix. Here, we propose two algorithms
to generate directed networks with a given degree distribution, where the
directionality can be controlled. The effect of directionality on the spectral
radius lambda_{1}, principal eigenvector x_{1}, spectral gap
lambda_{1}-|lambda_{2}|) and algebraic connectivity |mu_{N-1}| is studied.
Important findings are that the spectral radius lambda_{1} decreases with the
directionality, and the spectral gap and the algebraic connectivity increase
with the directionality. The extent of the decrease of the spectral radius
depends on both the degree distribution and the degree-degree correlation
rho_{D}. Hence, the epidemic threshold of directed networks is larger than that
of undirected networks, and a random walk converges to its steady-state faster
in directed networks than in undirected networks with degree distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0786</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0786</id><created>2013-03-04</created><authors><author><keyname>Harjes</keyname><forenames>Kristine</forenames><affiliation>McDaniel College</affiliation></author><author><keyname>Naumov</keyname><forenames>Pavel</forenames><affiliation>McDaniel College</affiliation></author></authors><title>Functional Dependence in Strategic Games (extended abstract)</title><categories>cs.GT cs.LO</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 9-15</journal-ref><doi>10.4204/EPTCS.112.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies properties of functional dependencies between strategies of
players in Nash equilibria of multi-player strategic games. The main focus is
on the properties of functional dependencies in the context of a fixed
dependency graph for pay-off functions. A logical system describing properties
of functional dependence for any given graph is proposed and is proven to be
complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0787</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0787</id><created>2013-03-04</created><authors><author><keyname>Grandi</keyname><forenames>Umberto</forenames><affiliation>University of Padova</affiliation></author><author><keyname>Loreggia</keyname><forenames>Andrea</forenames><affiliation>University of Padova</affiliation></author><author><keyname>Rossi</keyname><forenames>Francesca</forenames><affiliation>University of Padova</affiliation></author><author><keyname>Venable</keyname><forenames>Kristen Brent</forenames><affiliation>Tulane University and IHMC</affiliation></author><author><keyname>Walsh</keyname><forenames>Toby</forenames><affiliation>NICTA and UNSW</affiliation></author></authors><title>Restricted Manipulation in Iterative Voting: Convergence and Condorcet
  Efficiency</title><categories>cs.AI cs.GT</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 17-24</journal-ref><doi>10.4204/EPTCS.112.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In collective decision making, where a voting rule is used to take a
collective decision among a group of agents, manipulation by one or more agents
is usually considered negative behavior to be avoided, or at least to be made
computationally difficult for the agents to perform. However, there are
scenarios in which a restricted form of manipulation can instead be beneficial.
In this paper we consider the iterative version of several voting rules, where
at each step one agent is allowed to manipulate by modifying his ballot
according to a set of restricted manipulation moves which are computationally
easy and require little information to be performed. We prove convergence of
iterative voting rules when restricted manipulation is allowed, and we present
experiments showing that most iterative voting rules have a higher Condorcet
efficiency than their non-iterative version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0788</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0788</id><created>2013-03-04</created><authors><author><keyname>Asher</keyname><forenames>Nicholas</forenames><affiliation>CNRS Research Director, IRIT, Universite Paul Sabatier, Toulouse</affiliation></author><author><keyname>Paul</keyname><forenames>Soumya</forenames><affiliation>Post-doctoral fellow, IRIT, Universite Paul Sabatier, Toulouse</affiliation></author></authors><title>Infinite games with uncertain moves</title><categories>cs.GT cs.LO</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 25-32</journal-ref><doi>10.4204/EPTCS.112.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study infinite two-player games where one of the players is unsure about
the set of moves available to the other player. In particular, the set of moves
of the other player is a strict superset of what she assumes it to be. We
explore what happens to sets in various levels of the Borel hierarchy under
such a situation. We show that the sets at every alternate level of the
hierarchy jump to the next higher level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0789</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0789</id><created>2013-03-04</created><authors><author><keyname>Bulling</keyname><forenames>Nils</forenames><affiliation>Clausthal University of Technology</affiliation></author><author><keyname>Goranko</keyname><forenames>Valentin</forenames><affiliation>Technical University of Denmark</affiliation></author></authors><title>How to Be Both Rich and Happy: Combining Quantitative and Qualitative
  Strategic Reasoning about Multi-Player Games (Extended Abstract)</title><categories>cs.LO cs.MA</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 33-41</journal-ref><doi>10.4204/EPTCS.112.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a logical framework combining a game-theoretic study of abilities
of agents to achieve quantitative objectives in multi-player games by
optimizing payoffs or preferences on outcomes with a logical analysis of the
abilities of players for achieving qualitative objectives of players, i.e.,
reaching or maintaining game states with desired properties. We enrich
concurrent game models with payoffs for the normal form games associated with
the states of the model and propose a quantitative extension of the logic ATL*
enabling the combination of quantitative and qualitative reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0790</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0790</id><created>2013-03-04</created><authors><author><keyname>Dimitrova</keyname><forenames>Rayna</forenames><affiliation>Saarland University, Germany</affiliation></author><author><keyname>Finkbeiner</keyname><forenames>Bernd</forenames><affiliation>Saarland University, Germany</affiliation></author></authors><title>Lossy Channel Games under Incomplete Information</title><categories>cs.LO cs.GT</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 43-51</journal-ref><doi>10.4204/EPTCS.112.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate lossy channel games under incomplete
information, where two players operate on a finite set of unbounded FIFO
channels and one player, representing a system component under consideration
operates under incomplete information, while the other player, representing the
component's environment is allowed to lose messages from the channels. We argue
that these games are a suitable model for synthesis of communication protocols
where processes communicate over unreliable channels. We show that in the case
of finite message alphabets, games with safety and reachability winning
conditions are decidable and finite-state observation-based strategies for the
component can be effectively computed. Undecidability for (weak) parity
objectives follows from the undecidability of (weak) parity perfect information
games where only one player can lose messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0791</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0791</id><created>2013-03-04</created><authors><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Parker</keyname><forenames>David</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>Simaitis</keyname><forenames>Aistis</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Strategic Analysis of Trust Models for User-Centric Networks</title><categories>cs.GT cs.MA</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 53-59</journal-ref><doi>10.4204/EPTCS.112.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strategic analysis of a trust model that has recently been
proposed for promoting cooperative behaviour in user-centric networks. The
mechanism for cooperation is based on a combination of reputation and virtual
currency schemes in which service providers reward paying customers and punish
non-paying ones by adjusting their reputation, and hence the price they pay for
services. We model and analyse this system using PRISM-games, a tool that
performs automated verification and strategy synthesis for stochastic
multi-player games using the probabilistic alternating-time temporal logic with
rewards (rPATL). We construct optimal strategies for both service users and
providers, which expose potential risks of the cooperation mechanism and which
we use to devise improvements that counteract these risks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0792</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0792</id><created>2013-03-04</created><authors><author><keyname>Pedersen</keyname><forenames>Truls</forenames><affiliation>Dept. of Information science and media studies, University of Bergen, Norway</affiliation></author><author><keyname>Dyrkolbotn</keyname><forenames>Sjur</forenames><affiliation>Durham Law School, Durham University, United Kingdom</affiliation></author><author><keyname>Ka&#x17a;mierczak</keyname><forenames>Piotr</forenames><affiliation>Dept. of Computing, Mathematics and Physics, Bergen University College, Norway</affiliation></author><author><keyname>Parmann</keyname><forenames>Erik</forenames><affiliation>Dept. of Informatics, University of Bergen, Norway</affiliation></author></authors><title>Concurrent Game Structures with Roles</title><categories>cs.LO cs.MA</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 112, 2013, pp. 61-69</journal-ref><doi>10.4204/EPTCS.112.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the following paper we present a new semantics for the well-known
strategic logic ATL. It is based on adding roles to concurrent game structures,
that is at every state, each agent belongs to exactly one role, and the role
specifies what actions are available to him at that state. We show advantages
of the new semantics, provide motivating examples based on sensor networks, and
analyze model checking complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0793</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0793</id><created>2013-03-04</created><authors><author><keyname>Busard</keyname><forenames>Simon</forenames><affiliation>UCLouvain, Belgium</affiliation></author><author><keyname>Pecheur</keyname><forenames>Charles</forenames><affiliation>UCLouvain, Belgium</affiliation></author><author><keyname>Qu</keyname><forenames>Hongyang</forenames><affiliation>University of Oxford, UK</affiliation></author><author><keyname>Raimondi</keyname><forenames>Franco</forenames><affiliation>Middlesex University, UK</affiliation></author></authors><title>Reasoning about Strategies under Partial Observability and Fairness
  Constraints</title><categories>cs.LO cs.MA</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 71-79</journal-ref><doi>10.4204/EPTCS.112.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of extensions exist for Alternating-time Temporal Logic; some of
these mix strategies and partial observability but, to the best of our
knowledge, no work provides a unified framework for strategies, partial
observability and fairness constraints. In this paper we propose ATLK^F_po, a
logic mixing strategies under partial observability and epistemic properties of
agents in a system with fairness constraints on states, and we provide a model
checking algorithm for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0794</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0794</id><created>2013-03-04</created><authors><author><keyname>Guelev</keyname><forenames>Dimitar P.</forenames><affiliation>Institute of Mathematics and Informatics, Bulgarian Academy of Sciences</affiliation></author></authors><title>Reducing Validity in Epistemic ATL to Validity in Epistemic CTL</title><categories>cs.LO cs.AI cs.MA</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 81-89</journal-ref><doi>10.4204/EPTCS.112.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a validity preserving translation from a subset of epistemic
Alternating-time Temporal Logic (ATL) to epistemic Computation Tree Logic
(CTL). The considered subset of epistemic ATL is known to have the finite model
property and decidable model-checking. This entails the decidability of
validity but the implied algorithm is unfeasible. Reducing the validity problem
to that in a corresponding system of CTL makes the techniques for automated
deduction for that logic available for the handling of the apparently more
complex system of ATL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0795</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0795</id><created>2013-03-04</created><authors><author><keyname>Chareton</keyname><forenames>Christophe</forenames><affiliation>Onera</affiliation></author><author><keyname>Brunel</keyname><forenames>Julien</forenames><affiliation>Onera</affiliation></author><author><keyname>Chemouil</keyname><forenames>David</forenames><affiliation>Onera</affiliation></author></authors><title>Towards an Updatable Strategy Logic</title><categories>cs.LO</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 91-98</journal-ref><doi>10.4204/EPTCS.112.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is about temporal multi-agent logics. Several of these
formalisms have been already presented (ATL-ATL*, ATLsc, SL). They enable to
express the capacities of agents in a system to ensure the satisfaction of
temporal properties. Particularly, SL and ATLsc enable several agents to
interact in a context mixing the different strategies they play in a semantical
game. We generalize this possibility by proposing a new formalism, Updating
Strategy Logic (USL). In USL, an agent can also refine its own strategy. The
gain in expressive power rises the notion of &quot;sustainable capacities&quot; for
agents.
  USL is built from SL. It mainly brings to SL the two following modifications:
semantically, the successor of a given state is not uniquely determined by the
data of one choice from each agent. Syntactically, we introduce in the language
an operator, called an &quot;unbinder&quot;, which explicitely deletes the binding of a
strategy to an agent. We show that USL is strictly more expressive than SL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0796</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0796</id><created>2013-03-04</created><authors><author><keyname>Kirchner</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>Inria</affiliation></author></authors><title>A rewriting point of view on strategies</title><categories>cs.LO</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 99-105</journal-ref><doi>10.4204/EPTCS.112.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an expository contribution reporting on published work. It
focusses on an approach followed in the rewriting community to formalize the
concept of strategy. Based on rewriting concepts, several definitions of
strategy are reviewed and connected: in order to catch the higher-order nature
of strategies, a strategy is defined as a proof term expressed in the rewriting
logic or in the rewriting calculus; to address in a coherent way deduction and
computation, a strategy is seen as a subset of derivations; and to recover the
definition of strategy in sequential path-building games or in functional
programs, a strategy is considered as a partial function that associates to a
reduction-in-progress, the possible next steps in the reduction sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0797</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0797</id><created>2013-03-04</created><authors><author><keyname>Br&#xfc;tsch</keyname><forenames>Benedikt</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Synthesizing Structured Reactive Programs via Deterministic Tree
  Automata</title><categories>cs.LO</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 107-113</journal-ref><doi>10.4204/EPTCS.112.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing approaches to the synthesis of reactive systems typically involve
the construction of transition systems such as Mealy automata. However, in
order to obtain a succinct representation of the desired system, structured
programs can be a more suitable model. In 2011, Madhusudan proposed an
algorithm to construct a structured reactive program for a given omega-regular
specification without synthesizing a transition system first. His procedure is
based on two-way alternating omega-automata on finite trees that recognize the
set of &quot;correct&quot; programs.
  We present a more elementary and direct approach using only deterministic
bottom-up tree automata that compute so-called signatures for a given program.
In doing so, we extend Madhusudan's results to the wider class of programs with
bounded delay, which may read several input symbols before producing an output
symbol (or vice versa). As a formal foundation, we inductively define a
semantics for such programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0798</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0798</id><created>2013-03-04</created><authors><author><keyname>Maubert</keyname><forenames>Bastien</forenames></author><author><keyname>Pinchinat</keyname><forenames>Sophie</forenames></author><author><keyname>Bozzelli</keyname><forenames>Laura</forenames></author></authors><title>The Complexity of Synthesizing Uniform Strategies</title><categories>cs.GT cs.CC cs.LO</categories><comments>In Proceedings SR 2013, arXiv:1303.0071</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 112, 2013, pp. 115-122</journal-ref><doi>10.4204/EPTCS.112.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate uniformity properties of strategies. These properties involve
sets of plays in order to express useful constraints on strategies that are not
\mu-calculus definable. Typically, we can state that a strategy is
observation-based. We propose a formal language to specify uniformity
properties, interpreted over two-player turn-based arenas equipped with a
binary relation between plays. This way, we capture e.g. games with winning
conditions expressible in epistemic temporal logic, whose underlying
equivalence relation between plays reflects the observational capabilities of
agents (for example, synchronous perfect recall). Our framework naturally
generalizes many other situations from the literature. We establish that the
problem of synthesizing strategies under uniformity constraints based on
regular binary relations between plays is non-elementary complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0799</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0799</id><created>2013-03-04</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author></authors><title>Crowdsourced Judgement Elicitation with Endogenous Proficiency</title><categories>cs.GT</categories><comments>To appear in WWW 2013</comments><msc-class>91-XX</msc-class><acm-class>H.1.2; J.4; K.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing is now widely used to replace judgement by an expert authority
with an aggregate evaluation from a number of non-experts, in applications
ranging from rating and categorizing online content to evaluation of student
assignments in massively open online courses via peer grading. A key issue in
these settings, where direct monitoring is infeasible, is incentivizing agents
in the `crowd' to put in effort to make good evaluations, as well as to
truthfully report their evaluations. This leads to a new family of information
elicitation problems with unobservable ground truth, where an agent's
proficiency- the probability with which she correctly evaluates the underlying
ground truth- is endogenously determined by her strategic choice of how much
effort to put into the task.
  Our main contribution is a simple, new, mechanism for binary information
elicitation for multiple tasks when agents have endogenous proficiencies, with
the following properties: (i) Exerting maximum effort followed by truthful
reporting of observations is a Nash equilibrium. (ii) This is the equilibrium
with maximum payoff to all agents, even when agents have different maximum
proficiencies, can use mixed strategies, and can choose a different strategy
for each of their tasks. Our information elicitation mechanism requires only
minimal bounds on the priors, asks agents to only report their own evaluations,
and does not require any conditions on a diverging number of agent reports per
task to achieve its incentive properties. The main idea behind our mechanism is
to use the presence of multiple tasks and ratings to identify and penalize
low-effort agreement: the mechanism rewards agents for agreeing with a
`reference' rater on a task but also penalizes for blind agreement by
subtracting out a statistic term designed so that agents obtain reward only
when they put effort into their observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0808</identifier>
 <datestamp>2013-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0808</id><created>2013-03-04</created><updated>2013-06-12</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Sequential decoding of a general classical-quantum channel</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages; accepted for publication in the Proceedings of the Royal
  Society A</comments><journal-ref>Proceedings of the Royal Society A vol. 469, no. 2157, page
  20130259 (September 2013)</journal-ref><doi>10.1098/rspa.2013.0259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since a quantum measurement generally disturbs the state of a quantum system,
one might think that it should not be possible for a sender and receiver to
communicate reliably when the receiver performs a large number of sequential
measurements to determine the message of the sender. We show here that this
intuition is not true, by demonstrating that a sequential decoding strategy
works well even in the most general &quot;one-shot&quot; regime, where we are given a
single instance of a channel and wish to determine the maximal number of bits
that can be communicated up to a small failure probability. This result follows
by generalizing a non-commutative union bound to apply for a sequence of
general measurements. We also demonstrate two ways in which a receiver can
recover a state close to the original state after it has been decoded by a
sequence of measurements that each succeed with high probability. The second of
these methods will be useful in realizing an efficient decoder for fully
quantum polar codes, should a method ever be found to realize an efficient
decoder for classical-quantum polar codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0817</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0817</id><created>2013-03-04</created><updated>2015-04-07</updated><authors><author><keyname>Sefidgaran</keyname><forenames>Milad</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>On Cooperation in Multi-Terminal Computation and Rate Distortion</title><categories>cs.IT math.IT</categories><comments>31 pages, Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A receiver wants to compute a function of two correlated sources separately
observed by two transmitters. One of the transmitters may send a possibly
private message to the other transmitter in a cooperation phase before both
transmitters communicate to the receiver. For this network configuration this
paper investigates both a function computation setup, wherein the receiver
wants to compute a given function of the sources exactly, and a rate distortion
setup, wherein the receiver wants to compute a given function within some
distortion.
  For the function computation setup, a general inner bound to the rate region
is established and shown to be tight in a number of cases: partially invertible
functions, full cooperation between transmitters, one-round point-to-point
communication, two-round point-to-point communication, and the cascade setup
where the transmitters and the receiver are aligned. In particular it is shown
that the ratio of the total number of transmitted bits without cooperation and
the total number of transmitted bits with cooperation can be arbitrarily large.
Furthermore, one bit of cooperation suffices to arbitrarily reduce the amount
of information both transmitters need to convey to the receiver.
  For the rate distortion version, an inner bound to the rate region is
exhibited which always includes, and sometimes strictly, the convex hull of
Kaspi-Berger's related inner bounds. The strict inclusion is shown via two
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0818</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0818</id><created>2013-03-04</created><updated>2015-02-03</updated><authors><author><keyname>Ollivier</keyname><forenames>Yann</forenames></author></authors><title>Riemannian metrics for neural networks I: feedforward networks</title><categories>cs.NE cs.IT cs.LG math.DG math.IT</categories><comments>(5th version, minor changes)</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe four algorithms for neural network training, each adapted to
different scalability constraints. These algorithms are mathematically
principled and invariant under a number of transformations in data and network
representation, from which performance is thus independent. These algorithms
are obtained from the setting of differential geometry, and are based on either
the natural gradient using the Fisher information matrix, or on Hessian
methods, scaled down in a specific way to allow for scalability while keeping
some of their key mathematical properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0821</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0821</id><created>2013-03-04</created><authors><author><keyname>Sherette</keyname><forenames>Jessica</forenames></author><author><keyname>Wenk</keyname><forenames>Carola</forenames></author></authors><title>Simple Curve Embedding</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a curve f and a surface S, how hard is it to find a simple curve f' in
S that is the most similar to f?
  We introduce and study this simple curve embedding problem for piecewise
linear curves and surfaces in R^2 and R^3, under Hausdorff distance, weak
Frechet distance, and Frechet distance as similarity measures for curves.
Surprisingly, while several variants of the problem turn out to have
polynomial-time solutions, we show that in R^3 the simple curve embedding
problem is NP-hard under Frechet distance even if S is a plane, as well as
under weak Frechet distance if S is a terrain. Additionally, these results give
insight into the difficulty of computing the Frechet distance between surfaces,
and they imply that the partial Frechet distance between non-planar surfaces is
NP-hard as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0857</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0857</id><created>2013-03-04</created><updated>2013-04-18</updated><authors><author><keyname>Book</keyname><forenames>Theodore</forenames></author><author><keyname>Pridgen</keyname><forenames>Adam</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>Longitudinal Analysis of Android Ad Library Permissions</title><categories>cs.CR</categories><comments>Most 2013</comments><journal-ref>Mobile Security Technologies (MoST), May 2013, San Francisco CA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates changes over time in the behavior of Android ad
libraries. Taking a sample of 100,000 apps, we extract and classify the ad
libraries. By considering the release dates of the applications that use a
specific ad library version, we estimate the release date for the library, and
thus build a chronological map of the permissions used by various ad libraries
over time. We find that the use of most permissions has increased over the last
several years, and that more libraries are able to use permissions that pose
particular risks to user privacy and security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0861</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0861</id><created>2013-03-04</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Structural and Cognitive Bottlenecks to Information Access in Social
  Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>in Proceedings of ACM Hypertext and Social Media Conference (HT'13),
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information in networks is non-uniformly distributed, enabling individuals in
certain network positions to get preferential access to information. Social
scientists have developed influential theories about the role of network
structure in information access. These theories were validated through numerous
studies, which examined how individuals leverage their social networks for
competitive advantage, such as a new job or higher compensation. It is not
clear how these theories generalize to online networks, which differ from
real-world social networks in important respects, including asymmetry of social
links. We address this problem by analyzing how users of the social news
aggregator Digg adopt stories recommended by friends, i.e., users they follow.
We measure the impact different factors, such as network position and activity
rate; have on access to novel information, which in Digg's case means set of
distinct news stories. We show that a user can improve his information access
by linking to active users, though this becomes less effective as the number of
friends, or their activity, grows due to structural network constraints. These
constraints arise because users in structurally diverse position within the
follower graph have topically diverse interests from their friends. Moreover,
though in most cases user's friends are exposed to almost all the information
available in the network, after they make their recommendations, the user sees
only a small fraction of the available information. Our study suggests that
cognitive and structural bottlenecks limit access to novel information in
online social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0866</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0866</id><created>2013-03-04</created><authors><author><keyname>LeJeune</keyname><forenames>David W.</forenames><suffix>Jr</suffix></author></authors><title>Adaptive Partitioning and its Applicability to a Highly Scalable and
  Available Geo-Spatial Indexing Solution</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satellite Tracking of People (STOP) tracks thousands of GPS-enabled devices
24 hours a day and 365 days a year. With locations captured for each device
every minute, STOP servers receive tens of millions of points each day. In
addition to cataloging these points in real-time, STOP must also respond to
questions from customers such as, &quot;What devices of mine were at this location
two months ago?&quot; They often then broaden their question to one such as, &quot;Which
of my devices have ever been at this location?&quot; The processing requirements
necessary to answer these questions while continuing to process inbound data in
real-time is non-trivial.
  To meet this demand, STOP developed Adaptive Partitioning to provide a
cost-effective and highly available hardware platform for the geographical and
time-spatial indexing capabilities necessary for responding to customer data
requests while continuing to catalog inbound data in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0868</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0868</id><created>2013-03-04</created><updated>2013-03-13</updated><authors><author><keyname>Xie</keyname><forenames>Jierui</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>LabelRank: A Stabilized Label Propagation Algorithm for Community
  Detection in Networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>Proc. IEEE Network Science Workshop, 2013</comments><journal-ref>Proc. IEEE Network Science Workshop, West Point, NY, 2013, pp. 138
  - 143</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important challenge in big data analysis nowadays is detection of cohesive
groups in large-scale networks, including social networks, genetic networks,
communication networks and so. In this paper, we propose LabelRank, an
efficient algorithm detecting communities through label propagation. A set of
operators is introduced to control and stabilize the propagation dynamics.
These operations resolve the randomness issue in traditional label propagation
algorithms (LPA), stabilizing the discovered communities in all runs of the
same network. Tests on real-world networks demonstrate that LabelRank
significantly improves the quality of detected communities compared to LPA, as
well as other popular algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0875</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0875</id><created>2013-03-04</created><authors><author><keyname>Romano</keyname><forenames>Sergio</forenames></author><author><keyname>Sigman</keyname><forenames>Mariano</forenames></author><author><keyname>Figueira</keyname><forenames>Santiago</forenames></author></authors><title>LT^2C^2: A language of thought with Turing-computable Kolmogorov
  complexity</title><categories>q-bio.NC cs.AI</categories><comments>14 pages, 4 figures</comments><proxy>Luis Pugnaloni</proxy><journal-ref>Papers in Physics 5, 050001 (2013)</journal-ref><doi>10.4279/PIP.050001</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a theoretical effort to connect the theory of
program size to psychology by implementing a concrete language of thought with
Turing-computable Kolmogorov complexity (LT^2C^2) satisfying the following
requirements: 1) to be simple enough so that the complexity of any given finite
binary sequence can be computed, 2) to be based on tangible operations of human
reasoning (printing, repeating,...), 3) to be sufficiently powerful to generate
all possible sequences but not too powerful as to identify regularities which
would be invisible to humans. We first formalize LT^2C^2, giving its syntax and
semantics and defining an adequate notion of program size. Our setting leads to
a Kolmogorov complexity function relative to LT^2C^2 which is computable in
polynomial time, and it also induces a prediction algorithm in the spirit of
Solomonoff's inductive inference theory. We then prove the efficacy of this
language by investigating regularities in strings produced by participants
attempting to generate random strings. Participants had a profound
understanding of randomness and hence avoided typical misconceptions such as
exaggerating the number of alternations. We reasoned that remaining
regularities would express the algorithmic nature of human thoughts, revealed
in the form of specific patterns. Kolmogorov complexity relative to LT^2C^2
passed three expected tests examined here: 1) human sequences were less complex
than control PRNG sequences, 2) human sequences were not stationary, showing
decreasing values of complexity resulting from fatigue, 3) each individual
showed traces of algorithmic stability since fitting of partial sequences was
more effective to predict subsequent sequences than average fits. This work
extends on previous efforts to combine notions of Kolmogorov complexity theory
and algorithmic information theory to psychology, by explicitly ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0890</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0890</id><created>2013-03-04</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Set-Membership Conjugate Gradient Constrained Adaptive Filtering
  Algorithm for Beamforming</title><categories>cs.IT math.IT</categories><comments>3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new linearly constrained minimum variance (LCMV) beamformer
that combines the set-membership (SM) technique with the conjugate gradient
(CG) method, and develop a low-complexity adaptive filtering algorithm for
beamforming. The proposed algorithm utilizes a CG-based vector and a variable
forgetting factor to perform the data-selective updates that are controlled by
a time-varying bound related to the parameters. For the update, the CG-based
vector is calculated iteratively (one iteration per update) to obtain the
filter parameters and to avoid the matrix inversion. The resulting iterations
construct a space of feasible solutions that satisfy the constraints of the
LCMV optimization problem. The proposed algorithm reduces the computational
complexity significantly and shows an enhanced convergence and tracking
performance over existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0908</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0908</id><created>2013-03-04</created><authors><author><keyname>Babu</keyname><forenames>Rajasekhara</forenames></author><author><keyname>V.</keyname><forenames>Krishnakumar</forenames></author><author><keyname>Abraham</keyname><forenames>George</forenames></author><author><keyname>Borasia</keyname><forenames>Kiransinh</forenames></author></authors><title>KRAB Algorithm - A Revised Algorithm for Incremental Call Graph
  Generation</title><categories>cs.PL</categories><comments>6 pages, 7 figures, 1 Table, Conference</comments><journal-ref>World Applied Programming, Vol (2), Issue (5), May 2012. 294-299,
  Special section for proceeding of International E-Conference on Information
  Technology and Applications (IECITA), ISSN: 2222-2510</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is aimed to present the importance and implementation of an
incremental call graph plugin. An algorithm is proposed for the call graph
implementation which has better overall performance than the algorithm that has
been proposed previously. In addition to this, the algorithm has been
empirically proved to have excellent performance on recursive codes. The
algorithm also readily checks for function skip and returns exceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0910</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0910</id><created>2013-03-04</created><authors><author><keyname>Saravanaguru</keyname><forenames>RA. K.</forenames></author><author><keyname>Abraham</keyname><forenames>George</forenames></author><author><keyname>Ventakasubramanian</keyname><forenames>Krishnakumar</forenames></author><author><keyname>Borasia</keyname><forenames>Kiransinh</forenames></author></authors><title>Securing Web Services Using XML Signature and XML Encryption</title><categories>cs.CR</categories><comments>6 pages, 8 figures, 1 table</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is aimed to evaluate the importance of XML Signature and XML
Encryption in Web Service Security. In today's business scenario, organizations
are investing huge amount of resources in Web Services. Web Service
Transactions are done mainly through plain-text XML formats like SOAP and WSDL,
hence hacking into them is not a tedious task. XML Signature and XML Encryption
ensure security to XML documents as well as retain the structure of documents,
thereby making it easy to implement them. These two methods are evaluated on
the parameters of authentication, authorization, integration, confidentiality
and non-repudiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0916</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0916</id><created>2013-03-04</created><updated>2013-09-16</updated><authors><author><keyname>Salcedo</keyname><forenames>Bruno</forenames></author></authors><title>Implementation without commitment in moral hazard environments</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interdependent-choice equilibrium is defined as an extension of correlated
equilibrium in which the mediator is able to choose the timing of her signals,
and observe the actions taken by the players. The set of interdependent-choice
equilibria is a nonempty, closed and convex polytope. It characterizes all the
outcomes that can be implemented in single shot interactions without
repetition, side payments, binding contracts or any other form of delegation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0926</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0926</id><created>2013-03-05</created><updated>2015-01-13</updated><authors><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Hu</keyname><forenames>Zhi</forenames></author></authors><title>Injectivity w.r.t. Distribution of Elements in the Compressed Sequences
  Derived from Primitive Sequences over $Z/p^eZ$</title><categories>cs.IT math.IT</categories><comments>42 pages, updated version</comments><msc-class>11T71, 11B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p\geq3$ be a prime and $e\geq2$ an integer. Let $\sigma(x)$ be a
primitive polynomial of degree $n$ over $Z/p^eZ$ and $G'(\sigma(x),p^e)$ the
set of primitive linear recurring sequences generated by $\sigma(x)$. A
compressing map $\varphi$ on $Z/p^eZ$ naturally induces a map $\hat{\varphi}$
on $G'(\sigma(x),p^e)$. For a subset $D$ of the image of
$\varphi$,$\hat{\varphi}$ is called to be injective w.r.t. $D$-uniformity if
the distribution of elements of $D$ in the compressed sequence implies all
information of the original primitive sequence. In this correspondence, for at
least $1-2(p-1)/(p^n-1)$ of primitive polynomials of degree $n$, a clear
criterion on $\varphi$ is obtained to decide whether $\hat{\varphi}$ is
injective w.r.t. $D$-uniformity, and the majority of maps on $Z/p^eZ$ induce
injective maps on $G'(\sigma(x),p^e)$. Furthermore, a sufficient condition on
$\varphi$ is given to ensure injectivity of $\hat{\varphi}$ w.r.t.
$D$-uniformity. It follows from the sufficient condition that if $\sigma(x)$ is
strongly primitive and the compressing map $\varphi(x)=f(x_{e-1})$, where
$f(x_{e-1})$ is a permutation polynomial over $\mathbb{F}_{p}$, then
$\hat{\varphi}$ is injective w.r.t. $D$-uniformity for $\emptyset\neq
D\subset\mathbb{F}_{p}$. Moreover, we give three specific families of
compressing maps which induce injective maps on $G'(\sigma(x),p^e)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0930</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0930</id><created>2013-03-05</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Xinran</forenames></author><author><keyname>Fu</keyname><forenames>Fang-Wei</forenames></author></authors><title>An Authentication Scheme for Subspace Codes over Network Based on Linear
  Codes</title><categories>cs.CR cs.IT math.IT</categories><comments>18 pages</comments><msc-class>94A60</msc-class><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding provides the advantage of maximizing the usage of network
resources, and has great application prospects in future network
communications. However, the properties of network coding also make the
pollution attack more serious. In this paper, we give an unconditional secure
authentication scheme for network coding based on a linear code $C$.
Safavi-Naini and Wang gave an authentication code for multi-receivers and
multiple messages. We notice that the scheme of Safavi-Naini and Wang is
essentially constructed with Reed-Solomon codes. And we modify their
construction slightly to make it serve for authenticating subspace codes over
linear network. Also, we generalize the construction with linear codes. The
generalization to linear codes has the similar advantages as generalizing
Shamir's secret sharing scheme to linear secret sharing sceme based on linear
codes. One advantage of this generalization is that for a fixed message space,
our scheme allows arbitrarily many receivers to check the integrity of their
own messages, while the scheme with Reed-Solomon codes has a constraint on the
number of verifying receivers. Another advantage is that we introduce access
structure in the generalized scheme. Massey characterized the access structure
of linear secret sharing scheme by minimal codewords in the dual code whose
first component is 1. We slightly modify the definition of minimal codewords.
Let $C$ be a $[V,k]$ linear code. For any coordinate $i\in \{1,2,\cdots,V\}$, a
codeword $\vec{c}$ in $C$ is called minimal respect to $i$ if the codeword
$\vec{c}$ has component 1 at the $i$-th coordinate and there is no other
codeword whose $i$-th component is 1 with support strictly contained in that of
$\vec{c}$. Then the security of receiver $R_i$ in our authentication scheme is
characterized by the minimal codewords respect to $i$ in the dual code
$C^\bot$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0934</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0934</id><created>2013-03-05</created><authors><author><keyname>Tacchetti</keyname><forenames>Andrea</forenames></author><author><keyname>Mallapragada</keyname><forenames>Pavan K</forenames></author><author><keyname>Santoro</keyname><forenames>Matteo</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author></authors><title>GURLS: a Least Squares Library for Supervised Learning</title><categories>cs.LG cs.AI cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present GURLS, a least squares, modular, easy-to-extend software library
for efficient supervised learning. GURLS is targeted to machine learning
practitioners, as well as non-specialists. It offers a number state-of-the-art
training strategies for medium and large-scale learning, and routines for
efficient model selection. The library is particularly well suited for
multi-output problems (multi-category/multi-label). GURLS is currently
available in two independent implementations: Matlab and C++. It takes
advantage of the favorable properties of regularized least squares algorithm to
exploit advanced tools in linear algebra. Routines to handle computations with
very large matrices by means of memory-mapped storage and distributed task
execution are available. The package is distributed under the BSD licence and
is available for download at https://github.com/CBCL/GURLS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0943</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0943</id><created>2013-03-05</created><authors><author><keyname>Hu</keyname><forenames>Bao-Gang</forenames></author><author><keyname>Xing</keyname><forenames>Hong-Jie</forenames></author></authors><title>A New Approach of Deriving Bounds between Entropy and Error from Joint
  Distribution: Case Study for Binary Classifications</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures, 2 Appendixes including Maple code. A refined
  version based on the previous version entitled &quot;Analytical bounds between
  entropy and error probability in binary classifications&quot;, appeared as
  arXiv:1205.6602v1[cs.IT] in May 30, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing upper and lower bounds between entropy and error are mostly
derived through an inequality means without linking to joint distributions. In
fact, from either theoretical or application viewpoint, there exists a need to
achieve a complete set of interpretations to the bounds in relation to joint
distributions. For this reason, in this work we propose a new approach of
deriving the bounds between entropy and error from a joint distribution. The
specific case study is given on binary classifications, which can justify the
need of the proposed approach. Two basic types of classification errors are
investigated, namely, the Bayesian and non-Bayesian errors. For both errors, we
derive the closed-form expressions of upper bound and lower bound in relation
to joint distributions. The solutions show that Fano's lower bound is an exact
bound for any type of errors in a relation diagram of &quot;Error Probability vs.
Conditional Entropy&quot;. A new upper bound for the Bayesian error is derived with
respect to the minimum prior probability, which is generally tighter than
Kovalevskij's upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0944</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0944</id><created>2013-03-05</created><updated>2013-08-02</updated><authors><author><keyname>Chiarelli</keyname><forenames>Nina</forenames></author><author><keyname>Milanic</keyname><forenames>Martin</forenames></author></authors><title>Total Domishold Graphs: a Generalization of Threshold Graphs, with
  Connections to Threshold Hypergraphs</title><categories>math.CO cs.DM</categories><comments>19 pages, 1 figure</comments><msc-class>05C69</msc-class><journal-ref>Discrete Applied Mathematics 179 (2014) 1-12</journal-ref><doi>10.1016/j.dam.2014.09.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A total dominating set in a graph is a set of vertices such that every vertex
of the graph has a neighbor in the set. We introduce and study graphs that
admit non-negative real weights associated to their vertices such that a set of
vertices is a total dominating set if and only if the sum of the corresponding
weights exceeds a certain threshold. We show that these graphs, which we call
total domishold graphs, form a non-hereditary class of graphs properly
containing the classes of threshold graphs and the complements of domishold
graphs, and are closely related to threshold Boolean functions and threshold
hypergraphs. We present a polynomial time recognition algorithm of total
domishold graphs, and characterize graphs in which the above property holds in
a hereditary sense. Our characterization is obtained by studying a new family
of hypergraphs, defined similarly as the Sperner hypergraphs, which may be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0964</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0964</id><created>2013-03-05</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Kapur</keyname><forenames>Tina</forenames></author><author><keyname>Fedorov</keyname><forenames>Andriy</forenames></author><author><keyname>Pieper</keyname><forenames>Steve</forenames></author><author><keyname>Miller</keyname><forenames>James V.</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Harini</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Golby</keyname><forenames>Alexandra</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author><author><keyname>Kikinis</keyname><forenames>Ron</forenames></author></authors><title>GBM Volumetry using the 3D Slicer Medical Image Computing Platform</title><categories>cs.CV</categories><comments>7 pages, 6 figures, 2 tables, 1 equation, 43 references</comments><journal-ref>Sci. Rep. 3, 1364, 2013</journal-ref><doi>10.1038/srep01364</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volumetric change in glioblastoma multiforme (GBM) over time is a critical
factor in treatment decisions. Typically, the tumor volume is computed on a
slice-by-slice basis using MRI scans obtained at regular intervals. (3D)Slicer
- a free platform for biomedical research - provides an alternative to this
manual slice-by-slice segmentation process, which is significantly faster and
requires less user interaction. In this study, 4 physicians segmented GBMs in
10 patients, once using the competitive region-growing based GrowCut
segmentation module of Slicer, and once purely by drawing boundaries completely
manually on a slice-by-slice basis. Furthermore, we provide a variability
analysis for three physicians for 12 GBMs. The time required for GrowCut
segmentation was on an average 61% of the time required for a pure manual
segmentation. A comparison of Slicer-based segmentation with manual
slice-by-slice segmentation resulted in a Dice Similarity Coefficient of 88.43
+/- 5.23% and a Hausdorff Distance of 2.32 +/- 5.23 mm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0966</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0966</id><created>2013-03-05</created><authors><author><keyname>Czerwi&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Martens</keyname><forenames>Wim</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Efficient Separability of Regular Languages by Subsequences and Suffixes</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When can two regular word languages K and L be separated by a simple
language? We investigate this question and consider separation by piecewise-
and suffix-testable languages and variants thereof. We give characterizations
of when two languages can be separated and present an overview of when these
problems can be decided in polynomial time if K and L are given by
nondeterministic automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0969</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0969</id><created>2013-03-05</created><authors><author><keyname>Mas&#xe1;kov&#xe1;</keyname><forenames>Zuzana</forenames></author><author><keyname>Pelantov&#xe1;</keyname><forenames>Edita</forenames></author></authors><title>Enumerating Abelian Returns to Prefixes of Sturmian Words</title><categories>cs.FL math.CO</categories><comments>19pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We follow the works of Puzynina and Zamboni, and Rigo et al. on abelian
returns in Sturmian words. We determine the cardinality of the set
$\mathcal{APR}_u$ of abelian returns of all prefixes of a Sturmian word $u$ in
terms of the coefficients of the continued fraction of the slope, dependingly
on the intercept. We provide a simple algorithm for finding the set
$\mathcal{APR}_u$ and we determine it for the characteristic Sturmian words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.0970</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.0970</id><created>2013-03-05</created><authors><author><keyname>Kashirin</keyname><forenames>V. V.</forenames></author><author><keyname>Dijkstra</keyname><forenames>L. J.</forenames></author></authors><title>A heuristic optimization method for mitigating the impact of a virus
  attack</title><categories>cs.SI physics.soc-ph q-bio.PE</categories><comments>To appear in the proceedings of the International Conference on
  Computational Science (ICCS) in Barcelona. 11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking precautions before or during the start of a virus outbreak can heavily
reduce the number of infected. The question which individuals should be
immunized in order to mitigate the impact of the virus on the rest of
population has received quite some attention in the literature. The dynamics of
the of a virus spread through a population is often represented as information
spread over a complex network. The strategies commonly proposed to determine
which nodes are to be selected for immunization often involve only one
centrality measure at a time, while often the topology of the network seems to
suggest that a single metric is insufficient to capture the influence of a node
entirely.
  In this work we present a generic method based on a genetic algorithm (GA)
which does not rely explicitly on any centrality measures during its search but
only exploits this type of information to narrow the search space. The fitness
of an individual is defined as the estimated expected number of infections of a
virus following SIR dynamics. The proposed method is evaluated on two contact
networks: the Goodreau's Faux Mesa high school and the US air transportation
network. The GA method manages to outperform the most common strategies based
on a single metric for the air transportation network and its performance is
comparable with the best performing strategy for the high school network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1006</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1006</id><created>2013-03-05</created><authors><author><keyname>Peleska</keyname><forenames>Jan</forenames><affiliation>University of Bremen, Verified Systems International GmbH, Bremen, Germany</affiliation></author></authors><title>Industrial-Strength Model-Based Testing - State of the Art and Current
  Challenges</title><categories>cs.SE</categories><comments>In Proceedings MBT 2013, arXiv:1303.0379</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013, pp. 3-28</journal-ref><doi>10.4204/EPTCS.111.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As of today, model-based testing (MBT) is considered as leading-edge
technology in industry. We sketch the different MBT variants that - according
to our experience - are currently applied in practice, with special emphasis on
the avionic, railway and automotive domains. The key factors for successful
industrial-scale application of MBT are described, both from a scientific and a
managerial point of view. With respect to the former view, we describe the
techniques for automated test case, test data and test procedure generation for
concurrent reactive real-time systems which are considered as the most
important enablers for MBT in practice. With respect to the latter view, our
experience with introducing MBT approaches in testing teams are sketched.
Finally, the most challenging open scientific problems whose solutions are
bound to improve the acceptance and effectiveness of MBT in industry are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1007</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1007</id><created>2013-03-05</created><authors><author><keyname>Grabowski</keyname><forenames>Jens</forenames><affiliation>University of Goettingen, Goettingen, Germany</affiliation></author><author><keyname>Kuliamin</keyname><forenames>Victor</forenames><affiliation>Institute for System Programming of Russian Academy of Sciences, Moscow, Russia</affiliation></author><author><keyname>Feudjio</keyname><forenames>Alain-Georges Vouffo</forenames><affiliation>Thales, Germany</affiliation></author><author><keyname>Wu-Hen-Chang</keyname><forenames>Antal</forenames><affiliation>Ericsson, Hungary</affiliation></author><author><keyname>Zoric</keyname><forenames>Milan</forenames><affiliation>ETSI, France</affiliation></author></authors><title>Towards the Usage of MBT at ETSI</title><categories>cs.SE</categories><comments>In Proceedings MBT 2013, arXiv:1303.0379</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013, pp. 30-34</journal-ref><doi>10.4204/EPTCS.111.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2012 the Specialists Task Force (STF) 442 appointed by the European
Telcommunication Standards Institute (ETSI) explored the possibilities of using
Model Based Testing (MBT) for test development in standardization. STF 442
performed two case studies and developed an MBT-methodology for ETSI. The case
studies were based on the ETSI-standards GeoNetworking protocol (ETSI TS 102
636) and the Diameter-based Rx protocol (ETSI TS 129 214). Models have been
developed for parts of both standards and four different MBT-tools have been
employed for generating test cases from the models. The case studies were
successful in the sense that all the tools were able to produce the test suites
having the same test adequacy as the corresponding manually developed
conformance test suites. The MBT-methodology developed by STF 442 is based on
the experiences with the case studies. It focusses on integrating MBT into the
sophisticated standardization process at ETSI. This paper summarizes the
results of the STF 442 work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1008</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1008</id><created>2013-03-05</created><authors><author><keyname>Nunes</keyname><forenames>Isabel</forenames><affiliation>University of Lisbon, Lisboa, Portugal</affiliation></author><author><keyname>Lu&#xed;s</keyname><forenames>Filipe</forenames><affiliation>University of Lisbon, Lisboa, Portugal</affiliation></author></authors><title>Testing Java implementations of algebraic specifications</title><categories>cs.SE</categories><comments>In Proceedings MBT 2013, arXiv:1303.0379</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013, pp. 35-50</journal-ref><doi>10.4204/EPTCS.111.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on exploiting a specification and the structures that
satisfy it, to obtain a means of comparing implemented and expected behaviours
and find the origin of faults in implementations. We present an approach to the
creation of tests that are based on those specification-compliant structures,
and to the interpretation of those tests' results leading to the discovery of
the method responsible for an eventual test failure. Results of comparative
experiments with a tool implementing this approach are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1009</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1009</id><created>2013-03-05</created><authors><author><keyname>Noroozi</keyname><forenames>Neda</forenames><affiliation>Eindhoven University of Technology Eindhoven, The Netherlands</affiliation></author><author><keyname>Mousavi</keyname><forenames>Mohammad Reza</forenames><affiliation>Eindhoven University of Technology Eindhoven, The Netherlands and Center for Research on Embedded Systems</affiliation></author><author><keyname>Willemse</keyname><forenames>Tim A. C.</forenames><affiliation>Eindhoven University of Technology Eindhoven, The Netherlands</affiliation></author></authors><title>Decomposability in Input Output Conformance Testing</title><categories>cs.SE cs.LO</categories><comments>In Proceedings MBT 2013, arXiv:1303.0379</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013, pp. 51-66</journal-ref><doi>10.4204/EPTCS.111.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of deriving a specification for a third-party component,
based on the specification of the system and the environment in which the
component is supposed to reside. Particularly, we are interested in using
component specifications for conformance testing of black-box components, using
the theory of input-output conformance (ioco) testing. We propose and prove
sufficient criteria for decompositionality, i.e., that components conforming to
the derived specification will always compose to produce a correct system with
respect to the system specification. We also study the criteria for strong
decomposability, by which we can ensure that only those components conforming
to the derived specification can lead to a correct system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1010</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1010</id><created>2013-03-05</created><authors><author><keyname>Chupilko</keyname><forenames>Mikhail</forenames><affiliation>Institute for System Programming of Russian Academy of Sciences, Moscow, Russia</affiliation></author><author><keyname>Kamkin</keyname><forenames>Alexander</forenames><affiliation>Institute for System Programming of Russian Academy of Sciences, Moscow, Russia</affiliation></author></authors><title>Runtime Verification Based on Executable Models: On-the-Fly Matching of
  Timed Traces</title><categories>cs.SE</categories><comments>In Proceedings MBT 2013, arXiv:1303.0379</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013, pp. 67-81</journal-ref><doi>10.4204/EPTCS.111.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runtime verification is checking whether a system execution satisfies or
violates a given correctness property. A procedure that automatically, and
typically on the fly, verifies conformance of the system's behavior to the
specified property is called a monitor. Nowadays, a variety of formalisms are
used to express properties on observed behavior of computer systems, and a lot
of methods have been proposed to construct monitors. However, it is a frequent
situation when advanced formalisms and methods are not needed, because an
executable model of the system is available. The original purpose and structure
of the model are out of importance; rather what is required is that the system
and its model have similar sets of interfaces. In this case, monitoring is
carried out as follows. Two &quot;black boxes&quot;, the system and its reference model,
are executed in parallel and stimulated with the same input sequences; the
monitor dynamically captures their output traces and tries to match them. The
main problem is that a model is usually more abstract than the real system,
both in terms of functionality and timing. Therefore, trace-to-trace matching
is not straightforward and allows the system to produce events in different
order or even miss some of them. The paper studies on-the-fly conformance
relations for timed systems (i.e., systems whose inputs and outputs are
distributed along the time axis). It also suggests a practice-oriented
methodology for creating and configuring monitors for timed systems based on
executable models. The methodology has been successfully applied to a number of
industrial projects of simulation-based hardware verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1011</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1011</id><created>2013-03-05</created><authors><author><keyname>Wei&#xdf;leder</keyname><forenames>Stephan</forenames><affiliation>Fraunhofer-Institute FOKUS, Berlin, Germany</affiliation></author><author><keyname>Lackner</keyname><forenames>Hartmut</forenames><affiliation>Fraunhofer-Institute FOKUS, Berlin, Germany</affiliation></author></authors><title>Top-Down and Bottom-Up Approach for Model-Based Testing of Product Lines</title><categories>cs.SE</categories><comments>In Proceedings MBT 2013, arXiv:1303.0379</comments><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 111, 2013, pp. 82-94</journal-ref><doi>10.4204/EPTCS.111.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems tend to become more and more complex. This has a direct impact on
system engineering processes. Two of the most important phases in these
processes are requirements engineering and quality assurance. Two significant
complexity drivers located in these phases are the growing number of product
variants that have to be integrated into the requirements engineering and the
ever growing effort for manual test design. There are modeling techniques to
deal with both complexity drivers like, e.g., feature modeling and model-based
test design. Their combination, however, has been seldom the focus of
investigation. In this paper, we present two approaches to combine feature
modeling and model-based testing as an efficient quality assurance technique
for product lines. We present the corresponding difficulties and approaches to
overcome them. All explanations are supported by an example of an online shop
product line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1016</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1016</id><created>2013-03-05</created><updated>2013-03-08</updated><authors><author><keyname>Wang</keyname><forenames>Joshua</forenames></author></authors><title>Space-Efficient Las Vegas Algorithms for K-SUM</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using hashing techniques, this paper develops a family of space-efficient Las
Vegas randomized algorithms for $k$-SUM problems. This family includes an
algorithm that can solve 3-SUM in $O(n^2)$ time and $O(\sqrt{n})$ space. It
also establishes a new time-space upper bound for SUBSET-SUM, which can be
solved by a Las Vegas algorithm in $O^*(2^{(1-\sqrt{\8/9\beta})n})$ time and
$O^*(2^{\beta n})$ space, for any $\beta \in [0, \9/32]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1023</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1023</id><created>2013-03-05</created><updated>2013-03-13</updated><authors><author><keyname>Kiti&#x107;</keyname><forenames>Srdjan</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Madhu</keyname><forenames>Nilesh</forenames></author><author><keyname>Hopwood</keyname><forenames>Michael Peter</forenames></author><author><keyname>Spriet</keyname><forenames>Ann</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>Christophe</forenames></author></authors><title>Consistent Iterative Hard Thresholding For Signal Declipping</title><categories>cs.SD</categories><comments>ICASSP2013 conference paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clipping or saturation in audio signals is a very common problem in signal
processing, for which, in the severe case, there is still no satisfactory
solution. In such case, there is a tremendous loss of information, and
traditional methods fail to appropriately recover the signal. We propose a
novel approach for this signal restoration problem based on the framework of
Iterative Hard Thresholding. This approach, which enforces the consistency of
the reconstructed signal with the clipped observations, shows superior
performance in comparison to the state-of-the-art declipping algorithms. This
is confirmed on synthetic and on actual high-dimensional audio data processing,
both on SNR and on subjective user listening evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1026</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1026</id><created>2013-03-05</created><updated>2015-07-08</updated><authors><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author></authors><title>Non-overlapping codes</title><categories>cs.DM cs.IT math.CO math.IT</categories><comments>14 pages. Extra explanations added at some points, and an extra
  citation. To appear in IEEE Trans Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that a $q$-ary length $n$ code is \emph{non-overlapping} if the set of
non-trivial prefixes of codewords and the set of non-trivial suffices of
codewords are disjoint. These codes were first studied by Levenshtein in 1964,
motivated by applications in synchronisation. More recently these codes were
independently invented (under the name \emph{cross-bifix-free} codes) by
Baji\'c and Stojanovi\'c.
  We provide a simple construction for a class of non-overlapping codes which
has optimal cardinality whenever $n$ divides $q$. Moreover, for all parameters
$n$ and $q$ we show that a code from this class is close to optimal, in the
sense that it has cardinality within a constant factor of an upper bound due to
Levenshtein from 1970. Previous constructions have cardinality within a
constant factor of the upper bound only when $q$ is fixed.
  Chee, Kiah, Purkayastha and Wang showed that a $q$-ary length $n$
non-overlapping code contains at most $q^n/(2n-1)$ codewords; this bound is
weaker than the Levenshtein bound. Their proof appealed to the application in
synchronisation: we provide a direct combinatorial argument to establish the
bound of Chee \emph{et al}.
  We also consider codes of short length, finding the leading term of the
maximal cardinality of a non-overlapping code when $n$ is fixed and
$q\rightarrow \infty$. The largest cardinality of non-overlapping codes of
lengths $3$ or less is determined exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1038</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1038</id><created>2013-03-05</created><authors><author><keyname>Tarable</keyname><forenames>Alberto</forenames></author><author><keyname>Nordio</keyname><forenames>Alessandro</forenames></author><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author></authors><title>Anytime Reliable LDPC Convolutional Codes for Networked Control over
  Wireless Channel</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, 3 figures, submitted to IEEE International Symposium on
  Information Theory 2013 (ISIT 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of stabilizing an unstable system through
networked control over the wireless medium. In such a situation a remote sensor
communicates the measurements to the system controller through a noisy channel.
In particular, in the AWGN scenario, we show that protograph-based LDPC
convolutional codes achieve anytime reliability and we also derive a lower
bound to the signal-to-noise ratio required to stabilize the system. Moreover,
on the Rayleigh-fading channel, we show by simulations that resorting to
multiple sensors allows to achieve a diversity gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1039</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1039</id><created>2013-03-05</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author></authors><title>On interval edge-colorings of outerplanar graphs</title><categories>math.CO cs.DM</categories><comments>9 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An edge-coloring of a graph $G$ with colors $1,\ldots,t$ is called an
interval $t$-coloring if all colors are used, and the colors of edges incident
to any vertex of $G$ are distinct and form an interval of integers. A graph $G$
is interval colorable if it has an interval $t$-coloring for some positive
integer $t$. For an interval colorable graph $G$, the least value of $t$ for
which $G$ has an interval $t$-coloring is denoted by $w(G)$. A graph $G$ is
outerplanar if it can be embedded in the plane so that all its vertices lie on
the same (unbounded) face. In this paper we show that if $G$ is a 2-connected
outerplanar graph with $\Delta(G)=3$, then $G$ is interval colorable and
\begin{center} $w(G)=\left\{\begin{tabular}{ll} 3, &amp; if $| V(G)|$ is even, \ 4,
&amp; if $| V(G)|$ is odd. \end{tabular}% \right.$ \end{center} We also give a
negative answer to the question of Axenovich on the outerplanar triangulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1048</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1048</id><created>2013-03-05</created><authors><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Iyer</keyname><forenames>Parthasarathy P.</forenames></author></authors><title>Cloud Computing -- An Approach with Modern Cryptography</title><categories>cs.CR</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are proposing an algorithm which uses AES technique of
128/192/256 bit cipher key in encryption and decryption of data. AES provides
high security as compared to other encryption techniques along with RSA. Cloud
computing provides the customer with the requested services. It refers to
applications and services that run on distributed network using virtualized
resources and accessed by common IP and network standard. While providing data
services it is becoming important to provide security for data. In cloud
computing keeping data secure is an important issue to be focused. Even though
AES was designed for military purposes, now a days it is been commercially
adopted worldwide as it can encrypt most confidential document, as well as it
can work in most restricted areas, and offers good defense against various
attack techniques, and security level to protect data for next 2-3 decades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1051</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1051</id><created>2013-03-05</created><authors><author><keyname>Ayachi</keyname><forenames>I.</forenames></author><author><keyname>Kammarti</keyname><forenames>R.</forenames></author><author><keyname>Ksouri</keyname><forenames>M.</forenames></author><author><keyname>Borne</keyname><forenames>P.</forenames></author><author><keyname>:</keyname></author><author><keyname>LAGIS</keyname></author><author><keyname>de Lille</keyname><forenames>Ecole Centrale</forenames></author><author><keyname>:</keyname></author><author><keyname>LACS</keyname></author><author><keyname>de Tunis</keyname><forenames>Ecole Nationale des Ingenieurs</forenames></author></authors><title>A Genetic algorithm to solve the container storage space allocation
  problem</title><categories>cs.NE</categories><comments>2010 International conference on Computational Intelligence and
  Vehicular System (CIVS)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presented a genetic algorithm (GA) to solve the container storage
problem in the port. This problem is studied with different container types
such as regular, open side, open top, tank, empty and refrigerated containers.
The objective of this problem is to determine an optimal containers
arrangement, which respects customers delivery deadlines, reduces the rehandle
operations of containers and minimizes the stop time of the container ship. In
this paper, an adaptation of the genetic algorithm to the container storage
problem is detailed and some experimental results are presented and discussed.
The proposed approach was compared to a Last In First Out (LIFO) algorithm
applied to the same problem and has recorded good results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1090</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1090</id><created>2013-03-05</created><authors><author><keyname>Jerez</keyname><forenames>Juan L.</forenames></author><author><keyname>Goulart</keyname><forenames>Paul J.</forenames></author><author><keyname>Richter</keyname><forenames>Stefan</forenames></author><author><keyname>Constantinides</keyname><forenames>George A.</forenames></author><author><keyname>Kerrigan</keyname><forenames>Eric C.</forenames></author><author><keyname>Morari</keyname><forenames>Manfred</forenames></author></authors><title>Embedded Online Optimization for Model Predictive Control at Megahertz
  Rates</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faster, cheaper, and more power efficient optimization solvers than those
currently offered by general-purpose solutions are required for extending the
use of model predictive control (MPC) to resource-constrained embedded
platforms. We propose several custom computational architectures for different
first-order optimization methods that can handle linear-quadratic MPC problems
with input, input-rate, and soft state constraints. We provide analysis
ensuring the reliable operation of the resulting controller under reduced
precision fixed-point arithmetic. Implementation of the proposed architectures
in FPGAs shows that satisfactory control performance at a sample rate beyond 1
MHz is achievable even on low-end devices, opening up new possibilities for the
application of MPC on embedded systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1093</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1093</id><created>2013-03-05</created><updated>2013-05-17</updated><authors><author><keyname>Jain</keyname><forenames>Siddharth</forenames></author><author><keyname>Bansal</keyname><forenames>Rakesh Kumar</forenames></author></authors><title>On Large Deviation Property of Recurrence Times</title><categories>cs.IT math.IT</categories><comments>5 pages, International Symposium on Information Theory 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the study by Ornstein and Weiss on the asymptotic behavior of the
normalized version of recurrence times and establish the large deviation
property for a certain class of mixing processes. Further, an estimator for
entropy based on recurrence times is proposed for which large deviation
behavior is proved for stationary and ergodic sources satisfying similar mixing
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1095</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1095</id><created>2013-03-05</created><authors><author><keyname>Kang</keyname><forenames>Byungjun</forenames></author><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author></authors><title>A New Achievable Scheme for Interference Relay Channels</title><categories>cs.IT math.IT</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish an achievable rate region for discrete memoryless interference
relay channels that consist of two source-destination pairs and one or more
relays. We develop an achievable scheme combining Han-Kobayashi and noisy
network coding schemes. We apply our achievability to two cases. First, we
characterize the capacity region of a class of discrete memoryless interference
relay channels. This class naturally generalizes the injective deterministic
discrete memoryless interference channel by El Gamal and Costa and the
deterministic discrete memoryless relay channel with orthogonal receiver
components by Kim. Moreover, for the Gaussian interference relay channel with
orthogonal receiver components, we show that our scheme achieves a better sum
rate than that of noisy network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1098</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1098</id><created>2013-03-05</created><updated>2013-05-17</updated><authors><author><keyname>Jain</keyname><forenames>Siddharth</forenames></author><author><keyname>Bansal</keyname><forenames>Rakesh Kumar</forenames></author></authors><title>On Match Lengths and the Asymptotic Behavior of Sliding Window
  Lempel-Ziv Algorithm for Zero Entropy Sequences</title><categories>cs.IT math.IT</categories><comments>5 pages, International Symposium on Information Theory, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sliding Window Lempel-Ziv (SWLZ) algorithm has been studied from various
perspectives in information theory literature. In this paper, we provide a
general law which defines the asymptotics of match length for stationary and
ergodic zero entropy processes. Moreover, we use this law to choose the match
length $L_o$ in the almost sure optimality proof of Fixed Shift Variant of
Lempel-Ziv (FSLZ) and SWLZ algorithms given in literature. First, through an
example of stationary and ergodic processes generated by irrational rotation we
establish that for a window size of $n_w$ a compression ratio given by
$O(\frac{\log n_w}{{n_w}^a})$ where $a$ is arbitrarily close to 1 and $0 &lt; a &lt;
1$, is obtained under the application of FSLZ and SWLZ algorithms. Further, we
give a general expression for the compression ratio for a class of stationary
and totally ergodic processes with zero entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1115</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1115</id><created>2013-03-05</created><updated>2015-06-09</updated><authors><author><keyname>Furber</keyname><forenames>Robert W. J.</forenames><affiliation>Radboud University Nijmegen</affiliation></author><author><keyname>Jacobs</keyname><forenames>Bart P. F.</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>From Kleisli Categories to Commutative C*-algebras: Probabilistic
  Gelfand Duality</title><categories>math.CT cs.LO</categories><comments>28 pages, journal version of CALCO 2013 paper of the same title</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (2:5) 2015</journal-ref><doi>10.2168/LMCS-11(2:5)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  C*-algebras form rather general and rich mathematical structures that can be
studied with different morphisms (preserving multiplication, or not), and with
different properties (commutative, or not). These various options can be used
to incorporate various styles of computation (set-theoretic, probabilistic,
quantum) inside categories of C*-algebras. At first, this paper concentrates on
the commutative case and shows that there are functors from several Kleisli
categories, of monads that are relevant to model probabilistic computations, to
categories of C*-algebras. This yields a new probabilistic version of Gelfand
duality, involving the &quot;Radon&quot; monad on the category of compact Hausdorff
spaces. We then show that the state space functor from C*-algebras to
Eilenberg-Moore algebras of the Radon monad is full and faithful. This allows
us to obtain an appropriately commuting state-and-effect triangle for
C*-algebras.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1119</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1119</id><created>2013-03-05</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author><author><keyname>Ang</keyname><forenames>L. -M.</forenames></author><author><keyname>Seng</keyname><forenames>K. P.</forenames></author></authors><title>Termite-hill: From natural to artificial termites in sensor networks</title><categories>cs.NI</categories><comments>26 pages, 4 figures</comments><journal-ref>A.M. Zungeru, L.-M. Ang, K.P. Seng. Termite-hill: From Natural to
  Artificial Termites in Sensor Networks, International Journal of Swarm
  Intelligence Research (IGI-Publishers), vol. 3(4), pp. 1-23, 2012</journal-ref><doi>10.4018/jsir.2012100101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Termites present a very good natural metaphor to evolutionary computation.
While each individuals computational power is small compared to more evolved
species, it is the power of their colonies that inspires communication
engineers. This paper presents a study of artificial termites in sensor
networks for the purpose of solving its routing problem. The behaviors of each
of the termites in their colony allow their simulation in a restricted
environment. The simulating behavior demonstrates how the termites make use of
an auto-catalytic behavior in order to collectively find a solution for a posed
problem in reasonable time. The derived algorithm termed Termite-hill
demonstrates the principle of termites behavior to routing problem solving in
the real applications of sensor networks. The performance of the algorithm was
tested on static and dynamic sink scenarios. The results as compared with other
routing algorithms and with varying network density show that Termite-hill is
scalable and improved on network energy consumption with a control over
best-effort-service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1132</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1132</id><created>2013-03-05</created><updated>2013-11-16</updated><authors><author><keyname>Ren</keyname><forenames>Qingchun</forenames></author><author><keyname>Sam</keyname><forenames>Steven V</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author></authors><title>Tropicalization of classical moduli spaces</title><categories>math.AG cs.SC math.CO</categories><comments>33 pages</comments><journal-ref>Math. Comput. Sci. 8 (2014), no. 2, 119-145</journal-ref><doi>10.1007/s11786-014-0185-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The image of the complement of a hyperplane arrangement under a monomial map
can be tropicalized combinatorially using matroid theory. We apply this to
classical moduli spaces that are associated with complex reflection
arrangements. Starting from modular curves, we visit the Segre cubic, the Igusa
quartic, and moduli of marked del Pezzo surfaces of degrees 2 and 3. Our
primary example is the Burkhardt quartic, whose tropicalization is a
3-dimensional fan in 39-dimensional space. This effectuates a synthesis of
concrete and abstract approaches to tropical moduli of genus 2 curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1136</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1136</id><created>2013-03-05</created><updated>2013-11-05</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Grid Minors in Damaged Grids</title><categories>cs.DM math.CO</categories><comments>12 pages, 5 figures. This version adds an application to treewidth
  criticality, strengthens the upper and lower bounds for hypercube subgraphs,
  and includes a new discussion of connections to coding theory</comments><acm-class>G.2.2</acm-class><journal-ref>Electronic J. Combinatorics 21(3), Paper P3.20 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove upper and lower bounds on the size of the largest square grid graph
that is a subgraph, minor, or shallow minor of a graph in the form of a larger
square grid from which a specified number of vertices have been deleted. Our
bounds are tight to within constant factors. We also provide less-tight bounds
on analogous problems for higher-dimensional grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1144</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1144</id><created>2013-03-05</created><authors><author><keyname>Qiu</keyname><forenames>Chenlu</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>Recursive Sparse Recovery in Large but Structured Noise - Part 2</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recursively recovering a time sequence of sparse
vectors, St, from measurements Mt := St + Lt that are corrupted by structured
noise Lt which is dense and can have large magnitude. The structure that we
require is that Lt should lie in a low dimensional subspace that is either
fixed or changes &quot;slowly enough&quot;; and the eigenvalues of its covariance matrix
are &quot;clustered&quot;. We do not assume any model on the sequence of sparse vectors.
Their support sets and their nonzero element values may be either independent
or correlated over time (usually in many applications they are correlated). The
only thing required is that there be some support change every so often. We
introduce a novel solution approach called Recursive Projected Compressive
Sensing with cluster-PCA (ReProCS-cPCA) that addresses some of the limitations
of earlier work. Under mild assumptions, we show that, with high probability,
ReProCS-cPCA can exactly recover the support set of St at all times; and the
reconstruction errors of both St and Lt are upper bounded by a time-invariant
and small value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1152</identifier>
 <datestamp>2014-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1152</id><created>2013-03-05</created><updated>2014-04-25</updated><authors><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author></authors><title>An Equivalence between the Lasso and Support Vector Machines</title><categories>cs.LG stat.ML</categories><comments>Book chapter in Regularization, Optimization, Kernels, and Support
  Vector Machines, Johan A.K. Suykens, Marco Signoretto, Andreas Argyriou
  (Editors), 2014</comments><msc-class>65C60, 90C25, 68T05</msc-class><acm-class>F.2.2; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the relation of two fundamental tools in machine learning and
signal processing, that is the support vector machine (SVM) for classification,
and the Lasso technique used in regression. We show that the resulting
optimization problems are equivalent, in the following sense. Given any
instance of an $\ell_2$-loss soft-margin (or hard-margin) SVM, we construct a
Lasso instance having the same optimal solutions, and vice versa.
  As a consequence, many existing optimization algorithms for both SVMs and
Lasso can also be applied to the respective other problem instances. Also, the
equivalence allows for many known theoretical insights for SVM and Lasso to be
translated between the two settings. One such implication gives a simple
kernelized version of the Lasso, analogous to the kernels used in the SVM
setting. Another consequence is that the sparsity of a Lasso solution is equal
to the number of support vectors for the corresponding SVM instance, and that
one can use screening rules to prune the set of support vectors. Furthermore,
we can relate sublinear time algorithms for the two problems, and give a new
such algorithm variant for the Lasso. We also study the regularization paths
for both methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1201</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1201</id><created>2013-03-05</created><authors><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Multi-Pair Amplify-and-Forward Relaying with Very Large Antenna Arrays</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Communicatons (ICC), Budapest,
  Hungary, June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multi-pair relay channel where multiple sources simultaneously
communicate with destinations using a relay. Each source or destination has
only a single antenna, while the relay is equipped with a very large antenna
array. We investigate the power efficiency of this system when maximum ratio
combining/maximal ratio transmission (MRC/MRT) or zero-forcing (ZF) processing
is used at the relay. Using a very large array, the transmit power of each
source or relay (or both) can be made inversely proportional to the number of
relay antennas while maintaining a given quality-of-service. At the same time,
the achievable sum rate can be increased by a factor of the number of
source-destination pairs. We show that when the number of antennas grows to
infinity, the asymptotic achievable rates of MRC/MRT and ZF are the same if we
scale the power at the sources. Depending on the large scale fading effect,
MRC/MRT can outperform ZF or vice versa if we scale the power at the relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1208</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1208</id><created>2013-03-05</created><updated>2015-07-31</updated><authors><author><keyname>Blanchard</keyname><forenames>Gilles</forenames></author><author><keyname>Flaska</keyname><forenames>Marek</forenames></author><author><keyname>Handy</keyname><forenames>Gregory</forenames></author><author><keyname>Pozzi</keyname><forenames>Sara</forenames></author><author><keyname>Scott</keyname><forenames>Clayton</forenames></author></authors><title>Classification with Asymmetric Label Noise: Consistency and Maximal
  Denoising</title><categories>stat.ML cs.LG</categories><comments>This major revision includes the following additions: Necessity of
  proposed conditions, consistency for convex risk minimization, rate of
  convergence for mixture proportion estimation, and experimental results with
  Matlab code online</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world classification problems, the labels of training examples
are randomly corrupted. Most previous theoretical work on classification with
label noise assumes that the two classes are separable, that the label noise is
independent of the true class label, or that the noise proportions for each
class are known. In this work, we give conditions that are necessary and
sufficient for the true class-conditional distributions to be identifiable.
These conditions are weaker than those analyzed previously, and allow for the
classes to be nonseparable and the noise levels to be asymmetric and unknown.
The conditions essentially state that a majority of the observed labels are
correct and that the true class-conditional distributions are &quot;mutually
irreducible,&quot; a concept we introduce that limits the similarity of the two
distributions. For any label noise problem, there is a unique pair of true
class-conditional distributions satisfying the proposed conditions, and we
argue that this pair corresponds in a certain sense to maximal denoising of the
observed distributions.
  Our results are facilitated by a connection to &quot;mixture proportion
estimation,&quot; which is the problem of estimating the maximal proportion of one
distribution that is present in another. We establish a novel rate of
convergence result for mixture proportion estimation, and apply this to obtain
consistency of a discrimination rule based on surrogate loss minimization.
Experimental results on benchmark data and a nuclear particle classification
problem demonstrate the efficacy of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1209</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1209</id><created>2013-03-05</created><authors><author><keyname>Ghazi</keyname><forenames>Badih</forenames></author><author><keyname>Hassanieh</keyname><forenames>Haitham</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Katabi</keyname><forenames>Dina</forenames></author><author><keyname>Price</keyname><forenames>Eric</forenames></author><author><keyname>Shi</keyname><forenames>Lixin</forenames></author></authors><title>Sample-Optimal Average-Case Sparse Fourier Transform in Two Dimensions</title><categories>cs.DS cs.IT math.IT</categories><comments>30 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present the first sample-optimal sublinear time algorithms for the sparse
Discrete Fourier Transform over a two-dimensional sqrt{n} x sqrt{n} grid. Our
algorithms are analyzed for /average case/ signals. For signals whose spectrum
is exactly sparse, our algorithms use O(k) samples and run in O(k log k) time,
where k is the expected sparsity of the signal. For signals whose spectrum is
approximately sparse, our algorithm uses O(k log n) samples and runs in O(k
log^2 n) time; the latter algorithm works for k=Theta(sqrt{n}). The number of
samples used by our algorithms matches the known lower bounds for the
respective signal models.
  By a known reduction, our algorithms give similar results for the
one-dimensional sparse Discrete Fourier Transform when n is a power of a small
composite number (e.g., n = 6^t).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1217</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1217</id><created>2013-03-05</created><authors><author><keyname>Lin</keyname><forenames>Jing</forenames></author><author><keyname>Nassar</keyname><forenames>Marcel</forenames></author><author><keyname>Evans</keyname><forenames>Brian L.</forenames></author></authors><title>Impulsive Noise Mitigation in Powerline Communications Using Sparse
  Bayesian Learning</title><categories>stat.ML cs.IT math.IT</categories><comments>To appear in IEEE Journal on Selected Areas of Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Additive asynchronous and cyclostationary impulsive noise limits
communication performance in OFDM powerline communication (PLC) systems.
Conventional OFDM receivers assume additive white Gaussian noise and hence
experience degradation in communication performance in impulsive noise.
Alternate designs assume a parametric statistical model of impulsive noise and
use the model parameters in mitigating impulsive noise. These receivers require
overhead in training and parameter estimation, and degrade due to model and
parameter mismatch, especially in highly dynamic environments. In this paper,
we model impulsive noise as a sparse vector in the time domain without any
other assumptions, and apply sparse Bayesian learning methods for estimation
and mitigation without training. We propose three iterative algorithms with
different complexity vs. performance trade-offs: (1) we utilize the noise
projection onto null and pilot tones to estimate and subtract the noise
impulses; (2) we add the information in the data tones to perform joint noise
estimation and OFDM detection; (3) we embed our algorithm into a decision
feedback structure to further enhance the performance of coded systems. When
compared to conventional OFDM PLC receivers, the proposed receivers achieve SNR
gains of up to 9 dB in coded and 10 dB in uncoded systems in the presence of
impulsive noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1220</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1220</id><created>2013-03-05</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Reduced-Rank DOA Estimation based on Joint Iterative Subspace
  Optimization and Grid Search</title><categories>cs.IT math.IT</categories><comments>3 figures</comments><journal-ref>DSP 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel reduced-rank algorithm for direction of
arrival (DOA) estimation based on the minimum variance (MV) power spectral
evaluation. It is suitable to DOA estimation with large arrays and can be
applied to arbitrary array geometries. The proposed DOA estimation algorithm is
formulated as a joint optimization of a subspace projection matrix and an
auxiliary reduced-rank parameter vector with respect to the MV and grid search.
A constrained least squares method is employed to solve this joint optimization
problem for the output power over the grid. The proposed algorithm is described
for problems of large number of users' direction finding with or without exact
information of the number of sources, and does not require the singular value
decomposition (SVD). The spatial smoothing (SS) technique is also employed in
the proposed algorithm for dealing with correlated sources problem. Simulations
are conducted with comparisons against existent algorithms to show the improved
performance of the proposed algorithm in different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1232</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1232</id><created>2013-03-05</created><authors><author><keyname>Ram&#xed;rez</keyname><forenames>Jessica</forenames></author><author><keyname>Asahara</keyname><forenames>Masayuki</forenames></author><author><keyname>Matsumoto</keyname><forenames>Yuji</forenames></author></authors><title>Japanese-Spanish Thesaurus Construction Using English as a Pivot</title><categories>cs.CL cs.AI</categories><journal-ref>In Proceeding of The Third International Joint Conference on
  Natural Language Processing (IJCNLP-08), Hyderabad, India. pages 473-480,
  2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the results of research with the goal of automatically creating a
multilingual thesaurus based on the freely available resources of Wikipedia and
WordNet. Our goal is to increase resources for natural language processing
tasks such as machine translation targeting the Japanese-Spanish language pair.
Given the scarcity of resources, we use existing English resources as a pivot
for creating a trilingual Japanese-Spanish-English thesaurus. Our approach
consists of extracting the translation tuples from Wikipedia, disambiguating
them by mapping them to WordNet word senses. We present results comparing two
methods of disambiguation, the first using VSM on Wikipedia article texts and
WordNet definitions, and the second using categorical information extracted
from Wikipedia, We find that mixing the two methods produces favorable results.
Using the proposed method, we have constructed a multilingual
Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same
method can be applied to any pair of languages that are linked to English in
Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1238</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1238</id><created>2013-03-05</created><authors><author><keyname>Chang</keyname><forenames>Hung-Fu</forenames></author><author><keyname>Lu</keyname><forenames>Stephen C-Y.</forenames></author></authors><title>Toward the Integration of Traditional and Agile Approaches</title><categories>cs.SE</categories><comments>6 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol. 4 Issue 2, page 9-13, Feb. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The agile approach uses continuous delivery, instead of distinct procedure,
to work closer with customers and to respond faster requirement changes. All of
these are against the traditional plan driven approach. Due to agile method's
characteristics and its success in the real world practices, a number of
discussions regarding the differences between agile and traditional approaches
emerged recently and many studies intended to integrate both methods to
synthesize the benefits from these two sides. However, this type of research
often concludes from observations of a development activity or surveys after a
project. To provide a more objective supportive evidence of comparing these two
approaches, our research analyzes the source codes, logs, and notes. We argue
that the agile and traditional approaches share common characteristics, which
can be considered as the glue for integrating both methods. In our study, we
collect all the submissions from the version control repository, and meeting
notes and discussions. By applying our suggested analysis method, we illustrate
the shared properties between agile and traditional approaches; thus, different
development phases, like implementation and test, can still be identified in
agile development history. This result not only provides a positive result for
our hypothesis but also offers a suggestion for a better integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1243</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1243</id><created>2013-03-05</created><authors><author><keyname>Hossain</keyname><forenames>Md. Amjad</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Kawser</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>A Generalized Hybrid Real-Coded Quantum Evolutionary Algorithm Based on
  Particle Swarm Theory with Arithmetic Crossover</title><categories>cs.NE</categories><comments>http://airccse.org/journal/jcsit/0810ijcsit15.pdf</comments><journal-ref>International Journal of Computers Science and Information
  Technology (IJCSIT), Vol. 2, No. 4, pp.172-187, (2010)</journal-ref><doi>10.5121/ijcsit.2010.2415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a generalized Hybrid Real-coded Quantum Evolutionary
Algorithm (HRCQEA) for optimizing complex functions as well as combinatorial
optimization. The main idea of HRCQEA is to devise a new technique for mutation
and crossover operators. Using the evolutionary equation of PSO a
Single-Multiple gene Mutation (SMM) is designed and the concept of Arithmetic
Crossover (AC) is used in the new Crossover operator. In HRCQEA, each triploid
chromosome represents a particle and the position of the particle is updated
using SMM and Quantum Rotation Gate (QRG), which can make the balance between
exploration and exploitation. Crossover is employed to expand the search space,
Hill Climbing Selection (HCS) and elitism help to accelerate the convergence
speed. Simulation results on Knapsack Problem and five benchmark complex
functions with high dimension show that HRCQEA performs better in terms of
ability to discover the global optimum and convergence speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1247</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1247</id><created>2013-03-05</created><authors><author><keyname>Kouzehgar</keyname><forenames>M.</forenames></author><author><keyname>Badamchizadeh</keyname><forenames>M. A.</forenames></author><author><keyname>Khanmohammadi</keyname><forenames>S.</forenames></author></authors><title>Fuzzy Petri Nets for Human Behavior Verification and Validation</title><categories>cs.CY</categories><comments>9 pages, 4 figures, 1 table</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 2, No. 12, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regarding the rapid growth of the size and complexity of simulation
applications, designing applicable and affordable verification and validation
(V&amp;V) structures is an important problem. On the other hand, nowadays human
behavior models are principles to make decision in many simulations and in
order to have valid decisions based on a reliable human decision model, first
the model must pass the validation and verification criteria. Usually human
behavior models are represented as fuzzy rule bases. In all the recent works,
V&amp;V process is applied on a ready given rule-base. In this work, we are first
supposed to construct a fuzzy rule-base and then apply the V&amp;V process on it.
Considering the professor-student interaction as the case-study, in order to
construct the rule base, a questionnaire is designed in a special way to be
transformed to a hierarchical fuzzy rule-base. The constructed fuzzy rule base
is then mapped to a fuzzy Petri net and then within the verification
(generating and searching the reachability graph) and validation (reasoning the
Petri net) process is searched for probable structural and semantic errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1264</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1264</id><created>2013-03-06</created><authors><author><keyname>Belohlavek</keyname><forenames>Radim</forenames></author><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Discovery of factors in matrices with grades</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to decomposition and factor analysis of matrices with
ordinal data. The matrix entries are grades to which objects represented by
rows satisfy attributes represented by columns, e.g. grades to which an image
is red, a product has a given feature, or a person performs well in a test. We
assume that the grades form a bounded scale equipped with certain aggregation
operators and conforms to the structure of a complete residuated lattice. We
present a greedy approximation algorithm for the problem of decomposition of
such matrix in a product of two matrices with grades under the restriction that
the number of factors be small. Our algorithm is based on a geometric insight
provided by a theorem identifying particular rectangular-shaped submatrices as
optimal factors for the decompositions. These factors correspond to formal
concepts of the input data and allow an easy interpretation of the
decomposition. We present illustrative examples and experimental evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1271</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1271</id><created>2013-03-06</created><updated>2013-08-22</updated><authors><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author><author><keyname>Tsang</keyname><forenames>Ivor W.</forenames></author><author><keyname>Kwok</keyname><forenames>James T.</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Convex and Scalable Weakly Labeled SVMs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of learning from weakly labeled data,
where labels of the training examples are incomplete. This includes, for
example, (i) semi-supervised learning where labels are partially known; (ii)
multi-instance learning where labels are implicitly known; and (iii) clustering
where labels are completely unknown. Unlike supervised learning, learning with
weak labels involves a difficult Mixed-Integer Programming (MIP) problem.
Therefore, it can suffer from poor scalability and may also get stuck in local
minimum. In this paper, we focus on SVMs and propose the WellSVM via a novel
label generation strategy. This leads to a convex relaxation of the original
MIP, which is at least as tight as existing convex Semi-Definite Programming
(SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVM
subproblems that are much more scalable than previous convex SDP relaxations.
Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised
learning; (ii) multi-instance learning for locating regions of interest in
content-based information retrieval; and (iii) clustering, clearly demonstrate
improved performance, and WellSVM is also readily applicable on large data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1279</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1279</id><created>2013-03-06</created><authors><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Equilateral L-Contact Graphs</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider {\em L-graphs}, that is contact graphs of axis-aligned L-shapes
in the plane, all with the same rotation. We provide several characterizations
of L-graphs, drawing connections to Schnyder realizers and canonical orders of
maximally planar graphs. We show that every contact system of L's can always be
converted to an equivalent one with equilateral L's. This can be used to show a
stronger version of a result of Thomassen, namely, that every planar graph can
be represented as a contact system of square-based cuboids.
  We also study a slightly more restricted version of equilateral L-contact
systems and show that these are equivalent to homothetic triangle contact
representations of maximally planar graphs. We believe that this new
interpretation of the problem might allow for efficient algorithms to find
homothetic triangle contact representations, that do not use Schramm's monster
packing theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1280</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1280</id><created>2013-03-06</created><authors><author><keyname>Lajugie</keyname><forenames>R&#xe9;mi</forenames><affiliation>LIENS</affiliation></author><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS</affiliation></author></authors><title>Large-Margin Metric Learning for Partitioning Problems</title><categories>cs.LG stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider unsupervised partitioning problems, such as
clustering, image segmentation, video segmentation and other change-point
detection problems. We focus on partitioning problems based explicitly or
implicitly on the minimization of Euclidean distortions, which include
mean-based change-point detection, K-means, spectral clustering and normalized
cuts. Our main goal is to learn a Mahalanobis metric for these unsupervised
problems, leading to feature weighting and/or selection. This is done in a
supervised way by assuming the availability of several potentially partially
labelled datasets that share the same metric. We cast the metric learning
problem as a large-margin structured prediction problem, with proper definition
of regularizers and losses, leading to a convex optimization problem which can
be solved efficiently with iterative techniques. We provide experiments where
we show how learning the metric may significantly improve the partitioning
performance in synthetic examples, bioinformatics, video segmentation and image
segmentation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1285</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1285</id><created>2013-03-06</created><authors><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author></authors><title>Bandlimited Signal Reconstruction From the Distribution of Unknown
  Sampling Locations</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Submitted to SampTA 2013 workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the reconstruction of bandlimited fields from samples taken at
unknown but statistically distributed sampling locations. The setup is
motivated by distributed sampling where precise knowledge of sensor locations
can be difficult.
  Periodic one-dimensional bandlimited fields are considered for sampling.
Perfect samples of the field at independent and identically distributed
locations are obtained. The statistical realization of sampling locations is
not known. First, it is shown that a bandlimited field cannot be uniquely
determined with samples taken at statistically distributed but unknown
locations, even if the number of samples is infinite. Next, it is assumed that
the order of sample locations is known. In this case, using insights from
order-statistics, an estimate for the field with useful asymptotic properties
is designed. Distortion (mean-squared error) and central-limit are established
for this estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1292</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1292</id><created>2013-03-06</created><updated>2013-03-22</updated><authors><author><keyname>Kundu</keyname><forenames>Atreyee</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author></authors><title>Stabilizing switching signals for switched linear systems</title><categories>cs.SY math.OC</categories><comments>15 pages; corrected version with new references</comments><msc-class>93D20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with stability of continuous-time switched linear systems
under constrained switching. Given a family of linear systems, possibly
containing unstable dynamics, we characterize a new class of switching signals
under which the switched linear system generated by it and the family of
systems is globally asymptotically stable. Our characterization of such
stabilizing switching signals involves the asymptotic frequency of switching,
the asymptotic fraction of activation of the constituent systems, and the
asymptotic densities of admissible transitions among them. Our techniques
employ multiple Lyapunov-like functions, and extend preceding results both in
scope and applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1312</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1312</id><created>2013-03-06</created><authors><author><keyname>Pedersen</keyname><forenames>Niels Lovmand</forenames></author><author><keyname>Fleury</keyname><forenames>Carles Navarro Manch&#xf3;n Bernard Henri</forenames></author></authors><title>A Fast Iterative Bayesian Inference Algorithm for Sparse Channel
  Estimation</title><categories>stat.ML cs.IT math.IT</categories><comments>in Proc. IEEE Int. Communications Conf. (ICC), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a Bayesian channel estimation algorithm for
multicarrier receivers based on pilot symbol observations. The inherent sparse
nature of wireless multipath channels is exploited by modeling the prior
distribution of multipath components' gains with a hierarchical representation
of the Bessel K probability density function; a highly efficient, fast
iterative Bayesian inference method is then applied to the proposed model. The
resulting estimator outperforms other state-of-the-art Bayesian and
non-Bayesian estimators, either by yielding lower mean squared estimation error
or by attaining the same accuracy with improved convergence rate, as shown in
our numerical evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1347</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1347</id><created>2013-03-06</created><updated>2013-10-27</updated><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Constant Unary Constraints and Symmetric Real-Weighted Counting
  Constraint Satisfaction Problems</title><categories>cs.CC</categories><comments>10pt, A4, 21 pages. This is a complete version of the paper (under a
  slightly concise title) that appeared in the Proceedings of the 23rd
  International Symposium on Algorithms and Computation (ISAAC 2012), Taipei,
  Taiwan, December 19-21, 2012, Lecture Notes in Computer Science,
  Springer-Verlag, vol.7676, pp.237-246, 2012</comments><journal-ref>Theory of Computing Systems, vol. 55, pp. 170-201, 2014</journal-ref><doi>10.1007/s00224-013-9518-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A unary constraint (on the Boolean domain) is a function from {0,1} to the
set of real numbers. A free use of auxiliary unary constraints given besides
input instances has proven to be useful in establishing a complete
classification of the computational complexity of approximately solving
weighted counting Boolean constraint satisfaction problems (or #CSPs). In
particular, two special constant unary constraints are a key to an arity
reduction of arbitrary constraints, sufficient for the desired classification.
In an exact counting model, both constant unary constraints are always assumed
to be available since they can be eliminated efficiently using an arbitrary
nonempty set of constraints. In contrast, we demonstrate in an approximate
counting model, that at least one of them is efficiently approximated and thus
eliminated approximately by a nonempty constraint set. This fact directly leads
to an efficient construction of polynomial-time randomized
approximation-preserving Turing reductions (or AP-reductions) from #CSPs with
designated constraints to any given #CSPs composed of symmetric real-valued
constraints of arbitrary arities even in the presence of arbitrary extra unary
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1354</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1354</id><created>2013-03-06</created><authors><author><keyname>Baccelli</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Singh</keyname><forenames>Chandramani</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Adaptive Spatial Aloha, Fairness and Stochastic Geometry</title><categories>cs.NI cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims at combining adaptive protocol design, utility maximization
and stochastic geometry. We focus on a spatial adaptation of Aloha within the
framework of ad hoc networks. We consider quasi-static networks in which
mobiles learn the local topology and incorporate this information to adapt
their medium access probability (MAP) selection to their local environment. We
consider the cases where nodes cooperate in a distributed way to maximize the
global throughput or to achieve either proportional fair or max-min fair medium
access. In the proportional fair case, we show that nodes can compute their
optimal MAPs as solutions to certain fixed point equations. In the maximum
throughput case, the optimal MAPs are obtained through a Gibbs Sampling based
algorithm. In the max min case, these are obtained as the solution of a convex
optimization problem. The main performance analysis result of the paper is that
this type of distributed adaptation can be analyzed using stochastic geometry
in the proportional fair case. In this case, we show that, when the nodes form
a homogeneous Poisson point process in the Euclidean plane, the distribution of
the optimal MAP can be obtained from that of a certain shot noise process
w.r.t. the node Poisson point process and that the mean utility can also be
derived from this distribution. We discuss the difficulties to be faced for
analyzing the performance of the other cases (maximum throughput and max-min
fairness). Numerical results illustrate our findings and quantify the gains
brought by spatial adaptation in such networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1369</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1369</id><created>2013-03-06</created><updated>2013-08-01</updated><authors><author><keyname>Kim</keyname><forenames>Jung Yeol</forenames><affiliation>Korea University</affiliation></author><author><keyname>Goh</keyname><forenames>K. -I.</forenames><affiliation>Korea University</affiliation></author></authors><title>Coevolution and correlated multiplexity in multiplex networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>5 pages, 4 figures, published in PRL</comments><journal-ref>Phys. Rev. Lett. 111, 058702 (2013)</journal-ref><doi>10.1103/PhysRevLett.111.058702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distinct channels of interaction in a complex networked system define network
layers, which co-exist and co-operate for the system's function. Towards
realistic modeling and understanding such multiplex systems, we introduce and
study a class of growing multiplex network models in which different network
layers coevolve, and examine how the entangled growth of coevolving layers can
shape the overall network structure. We show analytically and numerically that
the coevolution can induce strong degree correlations across layers, as well as
modulate degree distributions. We further show that such a coevolution-induced
correlated multiplexity can alter the system's response to dynamical process,
exemplified by the suppressed susceptibility to a threshold cascade process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1379</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1379</id><created>2013-03-06</created><authors><author><keyname>Deveci</keyname><forenames>Mehmet</forenames></author><author><keyname>Kaya</keyname><forenames>Kamer</forenames></author><author><keyname>Ucar</keyname><forenames>Bora</forenames></author><author><keyname>Catalyurek</keyname><forenames>Umit V.</forenames></author></authors><title>GPU accelerated maximum cardinality matching algorithms for bipartite
  graphs</title><categories>cs.DC</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design, implement, and evaluate GPU-based algorithms for the maximum
cardinality matching problem in bipartite graphs. Such algorithms have a
variety of applications in computer science, scientific computing,
bioinformatics, and other areas. To the best of our knowledge, ours is the
first study which focuses on GPU implementation of the maximum cardinality
matching algorithms. We compare the proposed algorithms with serial and
multicore implementations from the literature on a large set of real-life
problems where in majority of the cases one of our GPU-accelerated algorithms
is demonstrated to be faster than both the sequential and multicore
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1384</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1384</id><created>2013-03-06</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author><author><keyname>Russo</keyname><forenames>Federica</forenames></author></authors><title>Causality in concurrent systems</title><categories>cs.DC cs.AI</categories><comments>This is an interdisciplinary paper. It addresses a class of causal
  models developed in computer science from an epistemic perspective, namely in
  terms of philosophy of causality</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent systems identify systems, either software, hardware or even
biological systems, that are characterized by sets of independent actions that
can be executed in any order or simultaneously. Computer scientists resort to a
causal terminology to describe and analyse the relations between the actions in
these systems. However, a thorough discussion about the meaning of causality in
such a context has not been developed yet. This paper aims to fill the gap.
First, the paper analyses the notion of causation in concurrent systems and
attempts to build bridges with the existing philosophical literature,
highlighting similarities and divergences between them. Second, the paper
analyses the use of counterfactual reasoning in ex-post analysis in concurrent
systems (i.e. execution trace analysis).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1399</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1399</id><created>2013-03-06</created><updated>2014-04-21</updated><authors><author><keyname>Sobocinski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Stephens</keyname><forenames>Owen</forenames></author></authors><title>Reachability via Compositionality in Petri nets</title><categories>cs.LO</categories><acm-class>F.3.2; D.2.2; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel technique for checking reachability in Petri nets that
relies on a recently introduced compositional algebra of nets. We prove that
the technique is correct, and discuss our implementation. We report promising
experimental results on some well-known examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1414</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1414</id><created>2013-03-06</created><updated>2014-07-07</updated><authors><author><keyname>Harris</keyname><forenames>Kameron Decker</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Dynamical influence processes on networks: General theory and
  applications to social contagion</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures</comments><journal-ref>Physical Review E 88, 022816 (2013)</journal-ref><doi>10.1103/PhysRevE.88.022816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study binary state dynamics on a network where each node acts in response
to the average state of its neighborhood. Allowing varying amounts of
stochasticity in both the network and node responses, we find different
outcomes in random and deterministic versions of the model. In the limit of a
large, dense network, however, we show that these dynamics coincide. We
construct a general mean field theory for random networks and show this
predicts that the dynamics on the network are a smoothed version of the average
response function dynamics. Thus, the behavior of the system can range from
steady state to chaotic depending on the response functions, network
connectivity, and update synchronicity. As a specific example, we model the
competing tendencies of imitation and non-conformity by incorporating an
off-threshold into standard threshold models of social contagion. In this way
we attempt to capture important aspects of fashions and societal trends. We
compare our theory to extensive simulations of this &quot;limited imitation
contagion&quot; model on Poisson random graphs, finding agreement between the
mean-field theory and stochastic simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1417</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1417</id><created>2013-03-06</created><authors><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Iyer</keyname><forenames>Parthasarathy P.</forenames></author></authors><title>Inter-Cloud Data Security Strategies</title><categories>cs.CR</categories><comments>5 pages, 1 Table. arXiv admin note: text overlap with
  arXiv:0907.2485, arXiv:0903.0694 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a complex infrastructure of software, hardware,
processing, and storage that is available as a service. Cloud computing offers
immediate access to large numbers of the world's most sophisticated
supercomputers and their corresponding processing power, interconnected at
various locations around the world, proffering speed in the tens of trillions
of computations per second. Information in databases and software scattered
around the Internet. There are many service providers in the internet, we can
call each service as a cloud, each cloud service will exchange data with other
cloud, so when the data is exchanged between the clouds, there exist the
problem of security. Security is an important issue for cloud computing, both
in terms of legal compliance and user trust, and needs to be considered at
every phase of design. In contrast to traditional solutions, where the IT
services are under proper physical, logical and personnel controls, Cloud
Computing moves the application software and databases to the large data
centers, where the management of the data and services may not be trustworthy.
This unique attribute, however, poses many new security challenges. Cloud
computing seems to offer some incredible benefits for communicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1418</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1418</id><created>2013-03-06</created><authors><author><keyname>McCracken</keyname><forenames>Merrick</forenames></author><author><keyname>Bocca</keyname><forenames>Maurizio</forenames></author><author><keyname>Patwari</keyname><forenames>Neal</forenames></author></authors><title>Joint Ultra-wideband and Signal Strength-based Through-building Tracking
  for Tactical Operations</title><categories>cs.OH</categories><comments>9 pages, conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate device free localization (DFL) based on received signal strength
(RSS) measurements requires placement of radio transceivers on all sides of the
target area. Accuracy degrades dramatically if sensors do not surround the
area. However, law enforcement officers sometimes face situations where it is
not possible or practical to place sensors on all sides of the target room or
building. For example, for an armed subject barricaded in a motel room, police
may be able to place sensors in adjacent rooms, but not in front of the room,
where the subject would see them. In this paper, we show that using two
ultra-wideband (UWB) impulse radios, in addition to multiple RSS sensors,
improves the localization accuracy, particularly on the axis where no sensors
are placed (which we call the x-axis). We introduce three methods for combining
the RSS and UWB data. By using UWB radios together with RSS sensors, it is
still possible to localize a person through walls even when the devices are
placed only on two sides of the target area. Including the data from the UWB
radios can reduce the localization area of uncertainty by more than 60%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1419</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1419</id><created>2013-03-05</created><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>Statistical Proof Pattern Recognition: Automated or Interactive?</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compare different existing approaches employed in data
mining of big proof libraries in automated and interactive theorem proving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1420</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1420</id><created>2013-03-05</created><updated>2013-05-24</updated><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Mata</keyname><forenames>Gadea</forenames></author><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author><author><keyname>S&#xe1;enz</keyname><forenames>Rub&#xe9;n</forenames></author></authors><title>Verifying a platform for digital imaging: a multi-tool strategy</title><categories>cs.SE cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fiji is a Java platform widely used by biologists and other experimental
scientists to process digital images. In particular, in our research - made
together with a biologists team; we use Fiji in some pre-processing steps
before undertaking a homological digital processing of images. In a previous
work, we have formalised the correctness of the programs which use homological
techniques to analyse digital images. However, the verification of Fiji's
pre-processing step was missed. In this paper, we present a multi-tool approach
filling this gap, based on the combination of Why/Krakatoa, Coq and ACL2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1434</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1434</id><created>2013-03-06</created><updated>2014-10-18</updated><authors><author><keyname>Johnson</keyname><forenames>Samuel D.</forenames></author><author><keyname>D'Souza</keyname><forenames>Raissa M.</forenames></author></authors><title>Inequality and Network Formation Games</title><categories>cs.GT</categories><comments>27 pages. 4 figures. Accepted to Internet Mathematics (2014)</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the matter of inequality in network formation games. We
employ a quantity that we are calling the Nash Inequality Ratio (NIR), defined
as the maximal ratio between the highest and lowest costs incurred to
individual agents in a Nash equilibrium strategy, to characterize the extent to
which inequality is possible in equilibrium. We give tight upper bounds on the
NIR for the network formation games of Fabrikant et al. (PODC '03) and Ehsani
et al. (SPAA '11). With respect to the relationship between equality and social
efficiency, we show that, contrary to common expectations, efficiency does not
necessarily come at the expense of increased inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1441</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1441</id><created>2013-03-06</created><updated>2014-01-25</updated><authors><author><keyname>Sarkar</keyname><forenames>Kamal</forenames></author></authors><title>A Hybrid Approach to Extract Keyphrases from Medical Documents</title><categories>cs.IR cs.CL</categories><journal-ref>International Journal of Computer Applications 63(18):14-19,
  February 2013. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/10565-5528</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyphrases are the phrases, consisting of one or more words, representing the
important concepts in the articles. Keyphrases are useful for a variety of
tasks such as text summarization, automatic indexing,
clustering/classification, text mining etc. This paper presents a hybrid
approach to keyphrase extraction from medical documents. The keyphrase
extraction approach presented in this paper is an amalgamation of two methods:
the first one assigns weights to candidate keyphrases based on an effective
combination of features such as position, term frequency, inverse document
frequency and the second one assign weights to candidate keyphrases using some
knowledge about their similarities to the structure and characteristics of
keyphrases available in the memory (stored list of keyphrases). An efficient
candidate keyphrase identification method as the first component of the
proposed keyphrase extraction system has also been introduced in this paper.
The experimental results show that the proposed hybrid approach performs better
than some state-of-the art keyphrase extraction approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1454</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1454</id><created>2013-03-06</created><authors><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author><author><keyname>Simon</keyname><forenames>Herbert A.</forenames></author></authors><title>Causality in Bayesian Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-3-11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of causal interpretation of the graphical structure of
Bayesian belief networks (BBNs). We review the concept of causality explicated
in the domain of structural equations models and show that it is applicable to
BBNs. In this view, which we call mechanism-based, causality is defined within
models and causal asymmetries arise when mechanisms are placed in the context
of a system. We lay the link between structural equations models and BBNs
models and formulate the conditions under which the latter can be given causal
interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1455</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1455</id><created>2013-03-06</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>From Conditional Oughts to Qualitative Decision Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-12-20</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary theme of this investigation is a decision theoretic account of
conditional ought statements (e.g., &quot;You ought to do A, if C&quot;) that rectifies
glaring deficiencies in classical deontic logic. The resulting account forms a
sound basis for qualitative decision theory, thus providing a framework for
qualitative planning under uncertainty. In particular, we show that adding
causal relationships (in the form of a single graph) as part of an epistemic
state is sufficient to facilitate the analysis of action sequences, their
consequences, their interaction with observations, their expected utilities
and, hence, the synthesis of plans and strategies under uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1456</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1456</id><created>2013-03-06</created><authors><author><keyname>Altman</keyname><forenames>Russ B.</forenames></author></authors><title>A Probabilistic Algorithm for Calculating Structure: Borrowing from
  Simulated Annealing</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-23-31</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed a general Bayesian algorithm for determining the
coordinates of points in a three-dimensional space. The algorithm takes as
input a set of probabilistic constraints on the coordinates of the points, and
an a priori distribution for each point location. The output is a
maximum-likelihood estimate of the location of each point. We use the extended,
iterated Kalman filter, and add a search heuristic for optimizing its solution
under nonlinear conditions. This heuristic is based on the same principle as
the simulated annealing heuristic for other optimization problems. Our method
uses any probabilistic constraints that can be expressed as a function of the
point coordinates (for example, distance, angles, dihedral angles, and
planarity). It assumes that all constraints have Gaussian noise. In this paper,
we describe the algorithm and show its performance on a set of synthetic data
to illustrate its convergence properties, and its applicability to domains such
ng molecular structure determination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1457</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1457</id><created>2013-03-06</created><authors><author><keyname>Musman</keyname><forenames>Scott A.</forenames></author><author><keyname>Chang</keyname><forenames>L. W.</forenames></author></authors><title>A Study of Scaling Issues in Bayesian Belief Networks for Ship
  Classification</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-32-39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problems associated with scaling involve active and challenging research
topics in the area of artificial intelligence. The purpose is to solve real
world problems by means of AI technologies, in cases where the complexity of
representation of the real world problem is potentially combinatorial. In this
paper, we present a novel approach to cope with the scaling issues in Bayesian
belief networks for ship classification. The proposed approach divides the
conceptual model of a complex ship classification problem into a set of small
modules that work together to solve the classification problem while preserving
the functionality of the original model. The possible ways of explaining sensor
returns (e.g., the evidence) for some features, such as portholes along the
length of a ship, are sometimes combinatorial. Thus, using an exhaustive
approach, which entails the enumeration of all possible explanations, is
impractical for larger problems. We present a network structure (referred to as
Sequential Decomposition, SD) in which each observation is associated with a
set of legitimate outcomes which are consistent with the explanation of each
observed piece of evidence. The results show that the SD approach allows one to
represent feature-observation relations in a manageable way and achieve the
same explanatory power as an exhaustive approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1458</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1458</id><created>2013-03-06</created><authors><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author></authors><title>Tradeoffs in Constructing and Evaluating Temporal Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-40-47</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the tradeoffs which need to be considered in reasoning
using probabilistic network representations, such as Influence Diagrams (IDs).
In particular, we examine the tradeoffs entailed in using Temporal Influence
Diagrams (TIDs) which adequately capture the temporal evolution of a dynamic
system without prohibitive data and computational requirements. Three
approaches for TID construction which make different tradeoffs are examined:
(1) tailoring the network at each time interval to the data available (rather
then just copying the original Bayes Network for all time intervals); (2)
modeling the evolution of a parsimonious subset of variables (rather than all
variables); and (3) model selection approaches, which seek to minimize some
measure of the predictive accuracy of the model without introducing too many
parameters, which might cause &quot;overfitting&quot; of the model. Methods of evaluating
the accuracy/efficiency of the tradeoffs are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1459</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1459</id><created>2013-03-06</created><authors><author><keyname>Lehmann</keyname><forenames>Harold P.</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>End-User Construction of Influence Diagrams for Bayesian Statistics</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-48-54</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diagrams are ideal knowledge representations for Bayesian
statistical models. However, these diagrams are difficult for end users to
interpret and to manipulate. We present a user-based architecture that enables
end users to create and to manipulate the knowledge representation. We use the
problem of physicians' interpretation of two-arm parallel randomized clinical
trials (TAPRCT) to illustrate the architecture and its use. There are three
primary data structures. Elements of statistical models are encoded as
subgraphs of a restricted class of influence diagram. The interpretations of
those elements are mapped into users' language in a domain-specific, user-based
semantic interface, called a patient-flow diagram, in the TAPRCT problem.
Pennitted transformations of the statistical model that maintain the semantic
relationships of the model are encoded in a metadata-state diagram, called the
cohort-state diagram, in the TAPRCT problem. The algorithm that runs the system
uses modular actions called construction steps. This framework has been
implemented in a system called THOMAS, that allows physicians to interpret the
data reported from a TAPRCT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1460</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1460</id><created>2013-03-06</created><authors><author><keyname>LaValle</keyname><forenames>Steven M.</forenames></author><author><keyname>Hutchinson</keyname><forenames>Seth A.</forenames></author></authors><title>On Considering Uncertainty and Alternatives in Low-Level Vision</title><categories>cs.AI cs.CV</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-55-63</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the uncertainty issues involved in the low-level
vision task of image segmentation. Researchers in computer vision have worked
extensively on this problem, in which the goal is to partition (or segment) an
image into regions that are homogeneous or uniform in some sense. This
segmentation is often utilized by some higher level process, such as an object
recognition system. We show that by considering uncertainty in a Bayesian
formalism, we can use statistical image models to build an approximate
representation of a probability distribution over a space of alternative
segmentations. We give detailed descriptions of the various levels of
uncertainty associated with this problem, discuss the interaction of prior and
posterior distributions, and provide the operations for constructing this
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1461</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1461</id><created>2013-03-06</created><authors><author><keyname>Dagum</keyname><forenames>Paul</forenames></author><author><keyname>Galper</keyname><forenames>Adam</forenames></author></authors><title>Forecasting Sleep Apnea with Dynamic Network Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-64-71</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic network models (DNMs) are belief networks for temporal reasoning. The
DNM methodology combines techniques from time series analysis and probabilistic
reasoning to provide (1) a knowledge representation that integrates
noncontemporaneous and contemporaneous dependencies and (2) methods for
iteratively refining these dependencies in response to the effects of exogenous
influences. We use belief-network inference algorithms to perform forecasting,
control, and discrete event simulation on DNMs. The belief network formulation
allows us to move beyond the traditional assumptions of linearity in the
relationships among time-dependent variables and of normality in their
probability distributions. We demonstrate the DNM methodology on an important
forecasting problem in medicine. We conclude with a discussion of how the
methodology addresses several limitations found in traditional time series
analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1462</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1462</id><created>2013-03-06</created><authors><author><keyname>Regan</keyname><forenames>Peter J.</forenames></author></authors><title>Normative Engineering Risk Management Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-72-79</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a normative system design that incorporates diagnosis,
dynamic evolution, decision making, and information gathering. A single
influence diagram demonstrates the design's coherence, yet each activity is
more effectively modeled and evaluated separately. Application to offshore oil
platforms illustrates the design. For this application, the normative system is
embedded in a real-time expert system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1463</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1463</id><created>2013-03-06</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Shwe</keyname><forenames>Michael</forenames></author></authors><title>Diagnosis of Multiple Faults: A Sensitivity Analysis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1993-PG-80-87</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the diagnostic accuracy of three diagnostic inference models: the
simple Bayes model, the multimembership Bayes model, which is isomorphic to the
parallel combination function in the certainty-factor model, and a model that
incorporates the noisy OR-gate interaction. The comparison is done on 20
clinicopathological conference (CPC) cases from the American Journal of
Medicine-challenging cases describing actual patients often with multiple
disorders. We find that the distributions produced by the noisy OR model agree
most closely with the gold-standard diagnoses, although substantial differences
exist between the distributions and the diagnoses. In addition, we find that
the multimembership Bayes model tends to significantly overestimate the
posterior probabilities of diseases, whereas the simple Bayes model tends to
significantly underestimate the posterior probabilities. Our results suggest
that additional work to refine the noisy OR model for internal medicine will be
worthwhile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1464</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1464</id><created>2013-03-06</created><authors><author><keyname>Dagum</keyname><forenames>Paul</forenames></author><author><keyname>Galper</keyname><forenames>Adam</forenames></author></authors><title>Additive Belief-Network Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-91-98</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inherent intractability of probabilistic inference has hindered the
application of belief networks to large domains. Noisy OR-gates [30] and
probabilistic similarity networks [18, 17] escape the complexity of inference
by restricting model expressiveness. Recent work in the application of
belief-network models to time-series analysis and forecasting [9, 10] has given
rise to the additive belief network model (ABNM). We (1) discuss the nature and
implications of the approximations made by an additive decomposition of a
belief network, (2) show greater efficiency in the induction of additive models
when available data are scarce, (3) generalize probabilistic inference
algorithms to exploit the additive decomposition of ABNMs, (4) show greater
efficiency of inference, and (5) compare results on inference with a simple
additive belief network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1465</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1465</id><created>2013-03-06</created><authors><author><keyname>Diez</keyname><forenames>Francisco Javier</forenames></author></authors><title>Parameter Adjustment in Bayes Networks. The generalized noisy OR-gate</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-99-105</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiegelhalter and Lauritzen [15] studied sequential learning in Bayesian
networks and proposed three models for the representation of conditional
probabilities. A forth model, shown here, assumes that the parameter
distribution is given by a product of Gaussian functions and updates them from
the _ and _r messages of evidence propagation. We also generalize the noisy
OR-gate for multivalued variables, develop the algorithm to compute probability
in time proportional to the number of parents (even in networks with loops) and
apply the learning model to this gate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1466</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1466</id><created>2013-03-06</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>A fuzzy relation-based extension of Reggia's relational model for
  diagnosis handling uncertain and incomplete information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-106-113</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational models for diagnosis are based on a direct description of the
association between disorders and manifestations. This type of model has been
specially used and developed by Reggia and his co-workers in the late eighties
as a basic starting point for approaching diagnosis problems. The paper
proposes a new relational model which includes Reggia's model as a particular
case and which allows for a more expressive representation of the observations
and of the manifestations associated with disorders. The model distinguishes,
i) between manifestations which are certainly absent and those which are not
(yet) observed, and ii) between manifestations which cannot be caused by a
given disorder and manifestations for which we do not know if they can or
cannot be caused by this disorder. This new model, which can handle uncertainty
in a non-probabilistic way, is based on possibility theory and so-called
twofold fuzzy sets, previously introduced by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1467</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1467</id><created>2013-03-06</created><authors><author><keyname>Elvang-G&#xf8;ransson</keyname><forenames>Morten</forenames></author><author><keyname>Krause</keyname><forenames>Paul J.</forenames></author><author><keyname>Fox</keyname><forenames>John</forenames></author></authors><title>Dialectic Reasoning with Inconsistent Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-114-121</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From an inconsistent database non-trivial arguments may be constructed both
for a proposition, and for the contrary of that proposition. Therefore,
inconsistency in a logical database causes uncertainty about which conclusions
to accept. This kind of uncertainty is called logical uncertainty. We define a
concept of &quot;acceptability&quot;, which induces a means for differentiating
arguments. The more acceptable an argument, the more confident we are in it. A
specific interest is to use the acceptability classes to assign linguistic
qualifiers to propositions, such that the qualifier assigned to a propositions
reflects its logical uncertainty. A more general interest is to understand how
classes of acceptability can be defined for arguments constructed from an
inconsistent database, and how this notion of acceptability can be devised to
reflect different criteria. Whilst concentrating on the aspects of assigning
linguistic qualifiers to propositions, we also indicate the more general
significance of the notion of acceptability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1468</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1468</id><created>2013-03-06</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Causal Independence for Knowledge Acquisition and Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1993-PG-122-127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I introduce a temporal belief-network representation of causal independence
that a knowledge engineer can use to elicit probabilistic models. Like the
current, atemporal belief-network representation of causal independence, the
new representation makes knowledge acquisition tractable. Unlike the atemproal
representation, however, the temporal representation can simplify inference,
and does not require the use of unobservable variables. The representation is
less general than is the atemporal representation, but appears to be useful for
many practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1469</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1469</id><created>2013-03-06</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Klein</keyname><forenames>Adrian</forenames></author></authors><title>Utility-Based Abstraction and Categorization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-128-135</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a utility-based approach to categorization. We construct
generalizations about events and actions by considering losses associated with
failing to distinguish among detailed distinctions in a decision model. The
utility-based methods transform detailed states of the world into more abstract
categories comprised of disjunctions of the states. We show how we can cluster
distinctions into groups of distinctions at progressively higher levels of
abstraction, and describe rules for decision making with the abstractions. The
techniques introduce a utility-based perspective on the nature of concepts, and
provide a means of simplifying decision models used in automated reasoning
systems. We demonstrate the techniques by describing the capabilities and
output of TUBA, a program for utility-based abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1470</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1470</id><created>2013-03-06</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Sensitivity Analysis for Probability Assessments in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-136-142</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When eliciting probability models from experts, knowledge engineers may
compare the results of the model with expert judgment on test scenarios, then
adjust model parameters to bring the behavior of the model more in line with
the expert's intuition. This paper presents a methodology for analytic
computation of sensitivity values to measure the impact of small changes in a
network parameter on a target probability value or distribution. These values
can be used to guide knowledge elicitation. They can also be used in a gradient
descent algorithm to estimate parameter values that maximize a measure of
goodness-of-fit to both local and holistic probability assessments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1471</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1471</id><created>2013-03-06</created><authors><author><keyname>Lemmer</keyname><forenames>John F.</forenames></author></authors><title>Causal Modeling</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-143-151</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal Models are like Dependency Graphs and Belief Nets in that they provide
a structure and a set of assumptions from which a joint distribution can, in
principle, be computed. Unlike Dependency Graphs, Causal Models are models of
hierarchical and/or parallel processes, rather than models of distributions
(partially) known to a model builder through some sort of gestalt. As such,
Causal Models are more modular, easier to build, more intuitive, and easier to
understand than Dependency Graph Models. Causal Models are formally defined and
Dependency Graph Models are shown to be a special case of them. Algorithms
supporting inference are presented. Parsimonious methods for eliciting
dependent probabilities are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1472</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1472</id><created>2013-03-06</created><authors><author><keyname>Matzkevich</keyname><forenames>Izhar</forenames></author><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>Some Complexity Considerations in the Combination of Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-152-158</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One topic that is likely to attract an increasing amount of attention within
the Knowledge-base systems research community is the coordination of
information provided by multiple experts. We envision a situation in which
several experts independently encode information as belief networks. A
potential user must then coordinate the conclusions and recommendations of
these networks to derive some sort of consensus. One approach to such a
consensus is the fusion of the contributed networks into a single, consensus
model prior to the consideration of any case-specific data (specific
observations, test results). This approach requires two types of combination
procedures, one for probabilities, and one for graphs. Since the combination of
probabilities is relatively well understood, the key barriers to this approach
lie in the realm of graph theory. This paper provides formal definitions of
some of the operations necessary to effect the necessary graphical
combinations, and provides complexity analyses of these procedures. The paper's
key result is that most of these operations are NP-hard, and its primary
message is that the derivation of ?good? consensus networks must be done
heuristically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1473</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1473</id><created>2013-03-06</created><authors><author><keyname>Matzkevich</keyname><forenames>Izhar</forenames></author><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>Deriving a Minimal I-map of a Belief Network Relative to a Target
  Ordering of its Nodes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-159-165</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper identifies and solves a new optimization problem: Given a belief
network (BN) and a target ordering on its variables, how can we efficiently
derive its minimal I-map whose arcs are consistent with the target ordering? We
present three solutions to this problem, all of which lead to directed acyclic
graphs based on the original BN's recursive basis relative to the specified
ordering (such a DAG is sometimes termed the boundary DAG drawn from the given
BN relative to the said ordering [5]). Along the way, we also uncover an
important general principal about arc reversals: when reordering a BN according
to some target ordering, (while attempting to minimize the number of arcs
generated), the sequence of arc reversals should follow the topological
ordering induced by the original belief network's arcs to as great an extent as
possible. These results promise to have a significant impact on the derivation
of consensus models, as well as on other algorithms that require the
reconfiguration and/or combination of BN's.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1474</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1474</id><created>2013-03-06</created><authors><author><keyname>Poh</keyname><forenames>Kim-Leng</forenames></author><author><keyname>Fehling</keyname><forenames>Michael R.</forenames></author></authors><title>Probabilistic Conceptual Network: A Belief Representation Scheme for
  Utility-Based Categorization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-166-173</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic conceptual network is a knowledge representation scheme
designed for reasoning about concepts and categorical abstractions in
utility-based categorization. The scheme combines the formalisms of abstraction
and inheritance hierarchies from artificial intelligence, and probabilistic
networks from decision analysis. It provides a common framework for
representing conceptual knowledge, hierarchical knowledge, and uncertainty. It
facilitates dynamic construction of categorization decision models at varying
levels of abstraction. The scheme is applied to an automated machining problem
for reasoning about the state of the machine at varying levels of abstraction
in support of actions for maintaining competitiveness of the plant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1475</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1475</id><created>2013-03-06</created><authors><author><keyname>Poh</keyname><forenames>Kim-Leng</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Reasoning about the Value of Decision-Model Refinement: Methods and
  Application</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-174-182</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the value of extending the completeness of a decision model
along different dimensions of refinement. Specifically, we analyze the expected
value of quantitative, conceptual, and structural refinement of decision
models. We illustrate the key dimensions of refinement with examples. The
analyses of value of model refinement can be used to focus the attention of an
analyst or an automated reasoning system on extensions of a decision model
associated with the greatest expected value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1476</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1476</id><created>2013-03-06</created><authors><author><keyname>Poland</keyname><forenames>William B.</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Mixtures of Gaussians and Minimum Relative Entropy Techniques for
  Modeling Continuous Uncertainties</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-183-190</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems of probabilistic inference and decision making under uncertainty
commonly involve continuous random variables. Often these are discretized to a
few points, to simplify assessments and computations. An alternative
approximation is to fit analytically tractable continuous probability
distributions. This approach has potential simplicity and accuracy advantages,
especially if variables can be transformed first. This paper shows how a
minimum relative entropy criterion can drive both transformation and fitting,
illustrating with a power and logarithm family of transformations and mixtures
of Gaussian (normal) distributions, which allow use of efficient influence
diagram methods. The fitting procedure in this case is the well-known EM
algorithm. The selection of the number of components in a fitted mixture
distribution is automated with an objective that trades off accuracy and
computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1477</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1477</id><created>2013-03-06</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Valuation Networks and Conditional Independence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-191-199</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valuation networks have been proposed as graphical representations of
valuation-based systems (VBSs). The VBS framework is able to capture many
uncertainty calculi including probability theory, Dempster-Shafer's
belief-function theory, Spohn's epistemic belief theory, and Zadeh's
possibility theory. In this paper, we show how valuation networks encode
conditional independence relations. For the probabilistic case, the class of
probability models encoded by valuation networks includes undirected graph
models, directed acyclic graph models, directed balloon graph models, and
recursive causal graph models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1478</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1478</id><created>2013-03-06</created><authors><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Relevant Explanations: Allowing Disjunctive Assignments</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-200-207</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relevance-based explanation is a scheme in which partial assignments to
Bayesian belief network variables are explanations (abductive conclusions). We
allow variables to remain unassigned in explanations as long as they are
irrelevant to the explanation, where irrelevance is defined in terms of
statistical independence. When multiple-valued variables exist in the system,
especially when subsets of values correspond to natural types of events, the
over specification problem, alleviated by independence-based explanation,
resurfaces. As a solution to that, as well as for addressing the question of
explanation specificity, it is desirable to collapse such a subset of values
into a single value on the fly. The equivalent method, which is adopted here,
is to generalize the notion of assignments to allow disjunctive assignments. We
proceed to define generalized independence based explanations as maximum
posterior probability independence based generalized assignments (GIB-MAPs).
GIB assignments are shown to have certain properties that ease the design of
algorithms for computing GIB-MAPs. One such algorithm is discussed here, as
well as suggestions for how other algorithms may be adapted to compute
GIB-MAPs. GIB-MAP explanations still suffer from instability, a problem which
may be addressed using ?approximate? conditional independence as a condition
for irrelevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1479</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1479</id><created>2013-03-06</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author></authors><title>A Generalization of the Noisy-Or Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-208-215</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Noisy-Or model is convenient for describing a class of uncertain
relationships in Bayesian networks [Pearl 1988]. Pearl describes the Noisy-Or
model for Boolean variables. Here we generalize the model to nary input and
output variables and to arbitrary functions other than the Boolean OR function.
This generalization is a useful modeling aid for construction of Bayesian
networks. We illustrate with some examples including digital circuit diagnosis
and network reliability analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1480</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1480</id><created>2013-03-06</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author></authors><title>Using First-Order Probability Logic for the Construction of Bayesian
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-219-226</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a mechanism for constructing graphical models, specifically
Bayesian networks, from a knowledge base of general probabilistic information.
The unique feature of our approach is that it uses a powerful first-order
probabilistic logic for expressing the general knowledge base. This logic
allows for the representation of a wide range of logical and probabilistic
information. The model construction procedure we propose uses notions from
direct inference to identify pieces of local statistical information from the
knowledge base that are most appropriate to the particular event we want to
reason about. These pieces are composed to generate a joint probability
distribution specified as a Bayesian network. Although there are fundamental
difficulties in dealing with fully general knowledge, our procedure is
practical for quite rich knowledge bases and it supports the construction of a
far wider range of networks than allowed for by current template technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1481</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1481</id><created>2013-03-06</created><authors><author><keyname>desJardins</keyname><forenames>Marie</forenames></author></authors><title>Representing and Reasoning With Probabilistic Knowledge: A Bayesian
  Approach</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-227-234</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PAGODA (Probabilistic Autonomous Goal-Directed Agent) is a model for
autonomous learning in probabilistic domains [desJardins, 1992] that
incorporates innovative techniques for using the agent's existing knowledge to
guide and constrain the learning process and for representing, reasoning with,
and learning probabilistic knowledge. This paper describes the probabilistic
representation and inference mechanism used in PAGODA. PAGODA forms theories
about the effects of its actions and the world state on the environment over
time. These theories are represented as conditional probability distributions.
A restriction is imposed on the structure of the theories that allows the
inference mechanism to find a unique predicted distribution for any action and
world state description. These restricted theories are called uniquely
predictive theories. The inference mechanism, Probability Combination using
Independence (PCI), uses minimal independence assumptions to combine the
probabilities in a theory to make probabilistic predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1482</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1482</id><created>2013-03-06</created><authors><author><keyname>Egar</keyname><forenames>John W.</forenames></author><author><keyname>Musen</keyname><forenames>Mark A.</forenames></author></authors><title>Graph-Grammar Assistance for Automated Generation of Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-235-242</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most difficult aspects of modeling complex dilemmas in
decision-analytic terms is composing a diagram of relevance relations from a
set of domain concepts. Decision models in domains such as medicine, however,
exhibit certain prototypical patterns that can guide the modeling process.
Medical concepts can be classified according to semantic types that have
characteristic positions and typical roles in an influence-diagram model. We
have developed a graph-grammar production system that uses such inherent
interrelationships among medical terms to facilitate the modeling of medical
decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1483</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1483</id><created>2013-03-06</created><authors><author><keyname>Lam</keyname><forenames>Wai</forenames></author><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author></authors><title>Using Causal Information and Local Measures to Learn Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-243-250</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work we developed a method of learning Bayesian Network models
from raw data. This method relies on the well known minimal description length
(MDL) principle. The MDL principle is particularly well suited to this task as
it allows us to tradeoff, in a principled way, the accuracy of the learned
network against its practical usefulness. In this paper we present some new
results that have arisen from our work. In particular, we present a new local
way of computing the description length. This allows us to make significant
improvements in our search algorithm. In addition, we modify our algorithm so
that it can take into account partial domain information that might be provided
by a domain expert. The local computation of description length also opens the
door for local refinement of an existent network. The feasibility of our
approach is demonstrated by experiments involving networks of a practical size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1484</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1484</id><created>2013-03-06</created><authors><author><keyname>Musick</keyname><forenames>Ron</forenames></author></authors><title>Minimal Assumption Distribution Propagation in Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-251-258</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As belief networks are used to model increasingly complex situations, the
need to automatically construct them from large databases will become
paramount. This paper concentrates on solving a part of the belief network
induction problem: that of learning the quantitative structure (the conditional
probabilities), given the qualitative structure. In particular, a theory is
presented that shows how to propagate inference distributions in a belief
network, with the only assumption being that the given qualitative structure is
correct. Most inference algorithms must make at least this assumption. The
theory is based on four network transformations that are sufficient for any
inference in a belief network. Furthermore, the claim is made that contrary to
popular belief, error will not necessarily grow as the inference chain grows.
Instead, for QBN belief nets induced from large enough samples, the error is
more likely to decrease as the size of the inference chain increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1485</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1485</id><created>2013-03-06</created><authors><author><keyname>Singh</keyname><forenames>Moninder</forenames></author><author><keyname>Valtorta</keyname><forenames>Marco</forenames></author></authors><title>An Algorithm for the Construction of Bayesian Network Structures from
  Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-259-265</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous algorithms for the construction of Bayesian belief network
structures from data have been either highly dependent on conditional
independence (CI) tests, or have required an ordering on the nodes to be
supplied by the user. We present an algorithm that integrates these two
approaches - CI tests are used to generate an ordering on the nodes from the
database which is then used to recover the underlying Bayesian network
structure using a non CI based method. Results of preliminary evaluation of the
algorithm on two networks (ALARM and LED) are presented. We also discuss some
algorithm performance issues and open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1486</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1486</id><created>2013-03-06</created><authors><author><keyname>Suzuki</keyname><forenames>Joe</forenames></author></authors><title>A Construction of Bayesian Networks from Databases Based on an MDL
  Principle</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-266-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses learning stochastic rules especially on an
inter-attribute relation based on a Minimum Description Length (MDL) principle
with a finite number of examples, assuming an application to the design of
intelligent relational database systems. The stochastic rule in this paper
consists of a model giving the structure like the dependencies of a Bayesian
Belief Network (BBN) and some stochastic parameters each indicating a
conditional probability of an attribute value given the state determined by the
other attributes' values in the same record. Especially, we propose the
extended version of the algorithm of Chow and Liu in that our learning
algorithm selects the model in the range where the dependencies among the
attributes are represented by some general plural number of trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1487</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1487</id><created>2013-03-06</created><authors><author><keyname>Yuan</keyname><forenames>Soe-Tsyr</forenames></author></authors><title>Knowledge-Based Decision Model Construction for Hierarchical Diagnosis:
  A Preliminary Report</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-274-281</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous methods for probabilistic reasoning in large, complex belief or
decision networks are currently being developed. There has been little research
on automating the dynamic, incremental construction of decision models. A
uniform value-driven method of decision model construction is proposed for the
hierarchical complete diagnosis. Hierarchical complete diagnostic reasoning is
formulated as a stochastic process and modeled using influence diagrams. Given
observations, this method creates decision models in order to obtain the best
actions sequentially for locating and repairing a fault at minimum cost. This
method construct decision models incrementally, interleaving probe actions with
model construction and evaluation. The method treats meta-level and baselevel
tasks uniformly. That is, the method takes a decision-theoretic look at the
control of search in causal pathways and structural hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1488</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1488</id><created>2013-03-06</created><authors><author><keyname>Burnell</keyname><forenames>Lisa J.</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>A Synthesis of Logical and Probabilistic Reasoning for Program
  Understanding and Debugging</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-285-291</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the integration of logical and uncertain reasoning methods to
identify the likely source and location of software problems. To date, software
engineers have had few tools for identifying the sources of error in complex
software packages. We describe a method for diagnosing software problems
through combining logical and uncertain reasoning analyses. Our preliminary
results suggest that such methods can be of value in directing the attention of
software engineers to paths of an algorithm that have the highest likelihood of
harboring a programming error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1489</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1489</id><created>2013-03-06</created><authors><author><keyname>Che</keyname><forenames>Peter</forenames></author><author><keyname>Neapolitan</keyname><forenames>Richard E.</forenames></author><author><keyname>Kenevan</keyname><forenames>James</forenames></author><author><keyname>Evens</keyname><forenames>Martha</forenames></author></authors><title>An Implementation of a Method for Computing the Uncertainty in Inferred
  Probabilities in Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-292-300</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the belief network has been used increasingly to model
systems in Al that must perform uncertain inference. The development of
efficient algorithms for probabilistic inference in belief networks has been a
focus of much research in AI. Efficient algorithms for certain classes of
belief networks have been developed, but the problem of reporting the
uncertainty in inferred probabilities has received little attention. A system
should not only be capable of reporting the values of inferred probabilities
and/or the favorable choices of a decision; it should report the range of
possible error in the inferred probabilities and/or choices. Two methods have
been developed and implemented for determining the variance in inferred
probabilities in belief networks. These methods, the Approximate Propagation
Method and the Monte Carlo Integration Method are discussed and compared in
this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1490</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1490</id><created>2013-03-06</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>Incremental Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-301-308</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propositional representation services such as truth maintenance systems offer
powerful support for incremental, interleaved, problem-model construction and
evaluation. Probabilistic inference systems, in contrast, have lagged behind in
supporting this incrementality typically demanded by problem solvers. The
problem, we argue, is that the basic task of probabilistic inference is
typically formulated at too large a grain-size. We show how a system built
around a smaller grain-size inference task can have the desired incrementality
and serve as the basis for a low-level (propositional) probabilistic
representation service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1491</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1491</id><created>2013-03-06</created><authors><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author><author><keyname>Kirman</keyname><forenames>Jak</forenames></author><author><keyname>Nicholson</keyname><forenames>Ann</forenames></author></authors><title>Deliberation Scheduling for Time-Critical Sequential Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-309-316</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for time-critical decision making involving sequential
tasks and stochastic processes. The method employs several iterative refinement
routines for solving different aspects of the decision making problem. This
paper concentrates on the meta-level control problem of deliberation
scheduling, allocating computational resources to these routines. We provide
different models corresponding to optimization problems that capture the
different circumstances and computational strategies for decision making under
time constraints. We consider precursor models in which all decision making is
performed prior to execution and recurrent models in which decision making is
performed in parallel with execution, accounting for the states observed during
execution and anticipating future states. We describe algorithms for precursor
and recurrent models and provide the results of our empirical investigations to
date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1492</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1492</id><created>2013-03-06</created><authors><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>Intercausal Reasoning with Uninstantiated Ancestor Nodes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-317-325</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intercausal reasoning is a common inference pattern involving probabilistic
dependence of causes of an observed common effect. The sign of this dependence
is captured by a qualitative property called product synergy. The current
definition of product synergy is insufficient for intercausal reasoning where
there are additional uninstantiated causes of the common effect. We propose a
new definition of product synergy and prove its adequacy for intercausal
reasoning with direct and indirect evidence for the common effect. The new
definition is based on a new property matrix half positive semi-definiteness, a
weakened form of matrix positive semi-definiteness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1493</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1493</id><created>2013-03-06</created><updated>2015-05-16</updated><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Inference Algorithms for Similarity Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1993-PG-326-334</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine two types of similarity networks each based on a distinct notion
of relevance. For both types of similarity networks we present an efficient
inference algorithm that works under the assumption that every event has a
nonzero probability of occurrence. Another inference algorithm is developed for
type 1 similarity networks that works under no restriction, albeit less
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1494</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1494</id><created>2013-03-06</created><authors><author><keyname>Lehner</keyname><forenames>Paul E.</forenames></author><author><keyname>Sadigh</keyname><forenames>Azar</forenames></author></authors><title>Two Procedures for Compiling Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-335-341</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two algorithms are presented for &quot;compiling&quot; influence diagrams into a set of
simple decision rules. These decision rules define simple-to-execute, complete,
consistent, and near-optimal decision procedures. These compilation algorithms
can be used to derive decision procedures for human teams solving time
constrained decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1495</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1495</id><created>2013-03-06</created><authors><author><keyname>Li</keyname><forenames>Zhaoyu</forenames></author><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>An efficient approach for finding the MPE in belief networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-342-349</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a belief network with evidence, the task of finding the I most probable
explanations (MPE) in the belief network is that of identifying and ordering
the I most probable instantiations of the non-evidence nodes of the belief
network. Although many approaches have been proposed for solving this problem,
most work only for restricted topologies (i.e., singly connected belief
networks). In this paper, we will present a new approach for finding I MPEs in
an arbitrary belief network. First, we will present an algorithm for finding
the MPE in a belief network. Then, we will present a linear time algorithm for
finding the next MPE after finding the first MPE. And finally, we will discuss
the problem of finding the MPE for a subset of variables of a belief network,
and show that the problem can be efficiently solved by this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1496</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1496</id><created>2013-03-06</created><authors><author><keyname>Mansell</keyname><forenames>Todd Michael</forenames></author></authors><title>A Method for Planning Given Uncertain and Incomplete Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-350-358</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes ongoing research into planning in an uncertain
environment. In particular, it introduces U-Plan, a planning system that
constructs quantitatively ranked plans given an incomplete description of the
state of the world. U-Plan uses a DempsterShafer interval to characterise
uncertain and incomplete information about the state of the world. The planner
takes as input what is known about the world, and constructs a number of
possible initial states with representations at different abstraction levels. A
plan is constructed for the initial state with the greatest support, and this
plan is tested to see if it will work for other possible initial states. All,
part, or none of the existing plans may be used in the generation of the plans
for the remaining possible worlds. Planning takes place in an abstraction
hierarchy where strategic decisions are made before tactical decisions. A
super-plan is then constructed, based on merging the set of plans and the
appropriately timed acquisition of essential knowledge, which is used to decide
between plan alternatives. U-Plan usually produces a super-plan in less time
than a classical planner would take to produce a set of plans, one for each
possible world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1497</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1497</id><created>2013-03-06</created><authors><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>The use of conflicts in searching Bayesian networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-359-367</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses how conflicts (as used by the consistency-based
diagnosis community) can be adapted to be used in a search-based algorithm for
computing prior and posterior probabilities in discrete Bayesian Networks. This
is an &quot;anytime&quot; algorithm, that at any stage can estimate the probabilities and
give an error bound. Whereas the most popular Bayesian net algorithms exploit
the structure of the network for efficiency, we exploit probability
distributions for efficiency; this algorithm is most suited to the case with
extreme probabilities. This paper presents a solution to the inefficiencies
found in naive algorithms, and shows how the tools of the consistency-based
diagnosis community (namely conflicts) can be used effectively to improve the
efficiency. Empirical results with networks having tens of thousands of nodes
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1498</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1498</id><created>2013-03-06</created><authors><author><keyname>Rojas-Guzman</keyname><forenames>Carlos</forenames></author><author><keyname>Kramer</keyname><forenames>Mark A.</forenames></author></authors><title>GALGO: A Genetic ALGOrithm Decision Support Tool for Complex Uncertain
  Systems Modeled with Bayesian Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-368-375</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian belief networks can be used to represent and to reason about complex
systems with uncertain, incomplete and conflicting information. Belief networks
are graphs encoding and quantifying probabilistic dependence and conditional
independence among variables. One type of reasoning of interest in diagnosis is
called abductive inference (determination of the global most probable system
description given the values of any partial subset of variables). In some
cases, abductive inference can be performed with exact algorithms using
distributed network computations but it is an NP-hard problem and complexity
increases drastically with the presence of undirected cycles, number of
discrete states per variable, and number of variables in the network. This
paper describes an approximate method based on genetic algorithms to perform
abductive inference in large, multiply connected networks for which complexity
is a concern when using most exact methods and for which systematic search
methods are not feasible. The theoretical adequacy of the method is discussed
and preliminary experimental results are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1499</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1499</id><created>2013-03-06</created><authors><author><keyname>Sarkar</keyname><forenames>Sumit</forenames></author></authors><title>Using Tree-Decomposable Structures to Approximate Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-376-382</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree structures have been shown to provide an efficient framework for
propagating beliefs [Pearl,1986]. This paper studies the problem of finding an
optimal approximating tree. The star decomposition scheme for sets of three
binary variables [Lazarsfeld,1966; Pearl,1986] is shown to enhance the class of
probability distributions that can support tree structures; such structures are
called tree-decomposable structures. The logarithm scoring rule is found to be
an appropriate optimality criterion to evaluate different tree-decomposable
structures. Characteristics of such structures closest to the actual belief
network are identified using the logarithm rule, and greedy and exact
techniques are developed to find the optimal approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1500</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1500</id><created>2013-03-06</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Ndilikilikesha</keyname><forenames>Pierre</forenames></author></authors><title>Using Potential Influence Diagrams for Probabilistic Inference and
  Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-383-390</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential influence diagram is a generalization of the standard
&quot;conditional&quot; influence diagram, a directed network representation for
probabilistic inference and decision analysis [Ndilikilikesha, 1991]. It allows
efficient inference calculations corresponding exactly to those on undirected
graphs. In this paper, we explore the relationship between potential and
conditional influence diagrams and provide insight into the properties of the
potential influence diagram. In particular, we show how to convert a potential
influence diagram into a conditional influence diagram, and how to view the
potential influence diagram operations in terms of the conditional influence
diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1501</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1501</id><created>2013-03-06</created><authors><author><keyname>Verma</keyname><forenames>Tom S.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Deciding Morality of Graphs is NP-complete</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-391-399</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to find a causal explanation for data presented in the form of
covariance and concentration matrices it is necessary to decide if the graph
formed by such associations is a projection of a directed acyclic graph (dag).
We show that the general problem of deciding whether such a dag exists is
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1502</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1502</id><created>2013-03-06</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Qi</keyname><forenames>Runping</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Incremental computation of the value of perfect information in
  stepwise-decomposable influence diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-400-407</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To determine the value of perfect information in an influence diagram, one
needs first to modify the diagram to reflect the change in information
availability, and then to compute the optimal expected values of both the
original diagram and the modified diagram. The value of perfect information is
the difference between the two optimal expected values. This paper is about how
to speed up the computation of the optimal expected value of the modified
diagram by making use of the intermediate computation results obtained when
computing the optimal expected value of the original diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1503</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1503</id><created>2013-03-06</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Argumentative inference in uncertain and inconsistent knowledge bases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-411-419</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents and discusses several methods for reasoning from
inconsistent knowledge bases. A so-called argumentative-consequence relation
taking into account the existence of consistent arguments in favor of a
conclusion and the absence of consistent arguments in favor of its contrary, is
particularly investigated. Flat knowledge bases, i.e. without any priority
between their elements, as well as prioritized ones where some elements are
considered as more strongly entrenched than others are studied under different
consequence relations. Lastly a paraconsistent-like treatment of prioritized
knowledge bases is proposed, where both the level of entrenchment and the level
of paraconsistency attached to a formula are propagated. The priority levels
are handled in the framework of possibility theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1504</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1504</id><created>2013-03-06</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Argument Calculus and Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-420-427</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major reason behind the success of probability calculus is that it
possesses a number of valuable tools, which are based on the notion of
probabilistic independence. In this paper, I identify a notion of logical
independence that makes some of these tools available to a class of
propositional databases, called argument databases. Specifically, I suggest a
graphical representation of argument databases, called argument networks, which
resemble Bayesian networks. I also suggest an algorithm for reasoning with
argument networks, which resembles a basic algorithm for reasoning with
Bayesian networks. Finally, I show that argument networks have several
applications: Nonmonotonic reasoning, truth maintenance, and diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1505</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1505</id><created>2013-03-06</created><authors><author><keyname>Fox</keyname><forenames>John</forenames></author><author><keyname>Krause</keyname><forenames>Paul J.</forenames></author><author><keyname>Elvang-G&#xf8;ransson</keyname><forenames>Morten</forenames></author></authors><title>Argumentation as a General Framework for Uncertain Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-428-434</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Argumentation is the process of constructing arguments about propositions,
and the assignment of statements of confidence to those propositions based on
the nature and relative strength of their supporting arguments. The process is
modelled as a labelled deductive system, in which propositions are doubly
labelled with the grounds on which they are based and a representation of the
confidence attached to the argument. Argument construction is captured by a
generalized argument consequence relation based on the ^,--fragment of minimal
logic. Arguments can be aggregated by a variety of numeric and symbolic
flattening functions. This approach appears to shed light on the common logical
structure of a variety of quantitative, qualitative and defeasible uncertainty
calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1506</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1506</id><created>2013-03-06</created><authors><author><keyname>Parsons</keyname><forenames>Simon</forenames></author><author><keyname>Mamdani</keyname><forenames>E. H.</forenames></author></authors><title>On reasoning in networks with qualitative uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-435-442</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper some initial work towards a new approach to qualitative
reasoning under uncertainty is presented. This method is not only applicable to
qualitative probabilistic reasoning, as is the case with other methods, but
also allows the qualitative propagation within networks of values based upon
possibility theory and Dempster-Shafer evidence theory. The method is applied
to two simple networks from which a large class of directed graphs may be
constructed. The results of this analysis are used to compare the qualitative
behaviour of the three major quantitative uncertainty handling formalisms, and
to demonstrate that the qualitative integration of the formalisms is possible
under certain assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1507</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1507</id><created>2013-03-06</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Wang</keyname><forenames>Z. W.</forenames></author></authors><title>Qualitative Measures of Ambiguity</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-443-450</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a qualitative measure of ambiguity and analyses its
relationship with other measures of uncertainty. Probability measures relative
likelihoods, while ambiguity measures vagueness surrounding those judgments.
Ambiguity is an important representation of uncertain knowledge. It deals with
a different, type of uncertainty modeled by subjective probability or belief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1508</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1508</id><created>2013-03-06</created><authors><author><keyname>Bordley</keyname><forenames>Robert F.</forenames></author></authors><title>A Bayesian Variant of Shafer's Commonalities For Modelling Unforeseen
  Events</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-453-460</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shafer's theory of belief and the Bayesian theory of probability are two
alternative and mutually inconsistent approaches toward modelling uncertainty
in artificial intelligence. To help reduce the conflict between these two
approaches, this paper reexamines expected utility theory-from which Bayesian
probability theory is derived. Expected utility theory requires the decision
maker to assign a utility to each decision conditioned on every possible event
that might occur. But frequently the decision maker cannot foresee all the
events that might occur, i.e., one of the possible events is the occurrence of
an unforeseen event. So once we acknowledge the existence of unforeseen events,
we need to develop some way of assigning utilities to decisions conditioned on
unforeseen events. The commonsensical solution to this problem is to assign
similar utilities to events which are similar. Implementing this commonsensical
solution is equivalent to replacing Bayesian subjective probabilities over the
space of foreseen and unforeseen events by random set theory probabilities over
the space of foreseen events. This leads to an expected utility principle in
which normalized variants of Shafer's commonalities play the role of subjective
probabilities. Hence allowing for unforeseen events in decision analysis causes
Bayesian probability theory to become much more similar to Shaferian theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1509</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1509</id><created>2013-03-06</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>The Probability of a Possibility: Adding Uncertainty to Default Rules</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-461-468</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a semantics for adding uncertainty to conditional logics for
default reasoning and belief revision. We are able to treat conditional
sentences as statements of conditional probability, and express rules for
revision such as &quot;If A were believed, then B would be believed to degree p.&quot;
This method of revision extends conditionalization by allowing meaningful
revision by sentences whose probability is zero. This is achieved through the
use of counterfactual probabilities. Thus, our system accounts for the best
properties of qualitative methods of update (in particular, the AGM theory of
revision) and probabilistic methods. We also show how our system can be viewed
as a unification of probability theory and possibility theory, highlighting
their orthogonality and providing a means for expressing the probability of a
possibility. We also demonstrate the connection to Lewis's method of imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1510</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1510</id><created>2013-03-06</created><authors><author><keyname>Driankov</keyname><forenames>Dimiter</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author></authors><title>Possibilistic decreasing persistence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-469-476</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key issue in the handling of temporal data is the treatment of persistence;
in most approaches it consists in inferring defeasible confusions by
extrapolating from the actual knowledge of the history of the world; we propose
here a gradual modelling of persistence, following the idea that persistence is
decreasing (the further we are from the last time point where a fluent is known
to be true, the less certainly true the fluent is); it is based on possibility
theory, which has strong relations with other well-known ordering-based
approaches to nonmonotonic reasoning. We compare our approach with Dean and
Kanazawa's probabilistic projection. We give a formal modelling of the
decreasing persistence problem. Lastly, we show how to infer nonmonotonic
conclusions using the principle of decreasing persistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1511</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1511</id><created>2013-03-06</created><authors><author><keyname>Guan</keyname><forenames>Jiwen W.</forenames></author><author><keyname>Bell</keyname><forenames>David A.</forenames></author></authors><title>Discounting and Combination Operations in Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-477-484</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evidential reasoning is now a leading topic in Artificial Intelligence.
Evidence is represented by a variety of evidential functions. Evidential
reasoning is carried out by certain kinds of fundamental operation on these
functions. This paper discusses two of the basic operations on evidential
functions, the discount operation and the well-known orthogonal sum operation.
We show that the discount operation is not commutative with the orthogonal sum
operation, and derive expressions for the two operations applied to the various
evidential function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1512</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1512</id><created>2013-03-06</created><authors><author><keyname>Kohlas</keyname><forenames>Jurg</forenames></author><author><keyname>Monney</keyname><forenames>Paul-Andre</forenames></author></authors><title>Probabilistic Assumption-Based Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-485-491</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical propositional assumption-based model is extended to incorporate
probabilities for the assumptions. Then it is placed into the framework of
evidence theory. Several authors like Laskey, Lehner (1989) and Provan (1990)
already proposed a similar point of view, but the first paper is not as much
concerned with mathematical foundations, and Provan's paper develops into a
different direction. Here we thoroughly develop and present the mathematical
foundations of this theory, together with computational methods adapted from
Reiter, De Kleer (1987) and Inoue (1992). Finally, recently proposed techniques
for computing degrees of support are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1513</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1513</id><created>2013-03-06</created><authors><author><keyname>Moral</keyname><forenames>Serafin</forenames></author><author><keyname>de Campos</keyname><forenames>Luis M.</forenames></author></authors><title>Partially Specified Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-492-499</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a procedure to determine a complete belief function from
the known values of belief for some of the subsets of the frame of discerment.
The method is based on the principle of minimum commitment and a new principle
called the focusing principle. This additional principle is based on the idea
that belief is specified for the most relevant sets: the focal elements. The
resulting procedure is compared with existing methods of building complete
belief functions: the minimum specificity principle and the least commitment
principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1514</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1514</id><created>2013-03-06</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Jeffrey's rule of conditioning generalized to belief functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-500-505</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jeffrey's rule of conditioning has been proposed in order to revise a
probability measure by another probability function. We generalize it within
the framework of the models based on belief functions. We show that several
forms of Jeffrey's conditionings can be defined that correspond to the
geometrical rule of conditioning and to Dempster's rule of conditioning,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1515</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1515</id><created>2013-03-06</created><authors><author><keyname>Song</keyname><forenames>Fengming</forenames></author><author><keyname>Liang</keyname><forenames>Ping</forenames></author></authors><title>Inference with Possibilistic Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-506-514</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the concept of possibilistic evidence which is a possibility
distribution as well as a body of evidence is proposed over an infinite
universe of discourse. The inference with possibilistic evidence is
investigated based on a unified inference framework maintaining both the
compatibility of concepts and the consistency of the probability logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1516</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1516</id><created>2013-03-06</created><authors><author><keyname>Wagner</keyname><forenames>Carl G.</forenames></author><author><keyname>Tonn</keyname><forenames>Bruce</forenames></author></authors><title>Constructing Lower Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-515-518</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An elaboration of Dempster's method of constructing belief functions suggests
a broadly applicable strategy for constructing lower probabilities under a
variety of evidentiary constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1517</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1517</id><created>2013-03-06</created><authors><author><keyname>Wang</keyname><forenames>Pei</forenames></author></authors><title>Belief Revision in Probability Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-519-526</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a probability-based reasoning system, Bayes' theorem and its variations
are often used to revise the system's beliefs. However, if the explicit
conditions and the implicit conditions of probability assignments `me properly
distinguished, it follows that Bayes' theorem is not a generally applicable
revision rule. Upon properly distinguishing belief revision from belief
updating, we see that Jeffrey's rule and its variations are not revision rules,
either. Without these distinctions, the limitation of the Bayesian approach is
often ignored or underestimated. Revision, in its general form, cannot be done
in the Bayesian approach, because a probability distribution function alone
does not contain the information needed by the operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1518</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1518</id><created>2013-03-06</created><authors><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>The Assumptions Behind Dempster's Rule</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-527-534</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the concept of a combination rule for belief functions.
It is shown that two fairly simple and apparently reasonable assumptions
determine Dempster's rule, giving a new justification for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1519</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1519</id><created>2013-03-06</created><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author><author><keyname>Hsia</keyname><forenames>Yen-Teh</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>A Belief-Function Based Decision Support System</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI1993)</comments><proxy>auai</proxy><report-no>UAI-P-1993-PG-535-542</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a decision support system based on belief functions
and the pignistic transformation. The system is an integration of an evidential
system for belief function propagation and a valuation-based system for
Bayesian decision analysis. The two subsystems are connected through the
pignistic transformation. The system takes as inputs the user's &quot;gut feelings&quot;
about a situation and suggests what, if any, are to be tested and in what
order, and it does so with a user friendly interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1559</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1559</id><created>2013-03-06</created><updated>2014-05-29</updated><authors><author><keyname>Ausiello</keyname><forenames>G.</forenames></author><author><keyname>Franciosa</keyname><forenames>P. G.</forenames></author><author><keyname>Italiano</keyname><forenames>G. F.</forenames></author><author><keyname>Ribichini</keyname><forenames>A.</forenames></author></authors><title>On Resilient Graph Spanners</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and investigate a new notion of resilience in graph spanners.
Let $S$ be a spanner of a graph $G$. Roughly speaking, we say that a spanner
$S$ is resilient if all its point-to-point distances are resilient to edge
failures. Namely, whenever any edge in $G$ fails, then as a consequence of this
failure all distances do not degrade in $S$ substantially more than in $G$
(i.e., the relative distance increases in $S$ are very close to those in the
underlying graph $G$). In this paper we show that sparse resilient spanners
exist, and that they can be computed efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1561</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1561</id><created>2013-03-06</created><authors><author><keyname>Liu</keyname><forenames>Yanpei</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Kim</keyname><forenames>Nam Sung</forenames></author></authors><title>Queuing Theoretic Analysis of Power-performance Tradeoff in
  Power-efficient Computing</title><categories>cs.PF math.PR</categories><comments>Paper published in CISS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the power-performance relationship of power-efficient
computing from a queuing theoretic perspective. We investigate the interplay of
several system operations including processing speed, system on/off decisions,
and server farm size. We identify that there are oftentimes &quot;sweet spots&quot; in
power-efficient operations: there exist optimal combinations of processing
speed and system settings that maximize power efficiency. For the single server
case, a widely deployed threshold mechanism is studied. We show that there
exist optimal processing speed and threshold value pairs that minimize the
power consumption. This holds for the threshold mechanism with job batching.
For the multi-server case, it is shown that there exist best processing speed
and server farm size combinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1571</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1571</id><created>2013-03-06</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Reduced-rank Adaptive Constrained Constant Modulus Beamforming
  Algorithms based on Joint Iterative Optimization of Filters</title><categories>cs.IT math.IT</categories><comments>4 figures</comments><journal-ref>DSP, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a reduced-rank scheme for adaptive beamforming based on
the constrained joint iterative optimization of filters. We employ this scheme
to devise two novel reduced-rank adaptive algorithms according to the constant
modulus (CM) criterion with different constraints. The first devised algorithm
is formulated as a constrained joint iterative optimization of a projection
matrix and a reduced-rank filter with respect to the CM criterion subject to a
constraint on the array response. The constrained constant modulus (CCM)
expressions for the projection matrix and the reduced-rank weight vector are
derived, and a low-complexity adaptive algorithm is presented to jointly
estimate them for implementation. The second proposed algorithm is extended
from the first one and implemented according to the CM criterion subject to a
constraint on the array response and an orthogonal constraint on the projection
matrix. The Gram-Schmidt (GS) technique is employed to achieve this orthogonal
constraint and improve the performance. Simulation results are given to show
superior performance of the proposed algorithms in comparison with existing
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1585</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1585</id><created>2013-03-06</created><authors><author><keyname>Sankararaman</keyname><forenames>Swaminathan</forenames></author><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>M&#xf8;lhave</keyname><forenames>Thomas</forenames></author><author><keyname>Boedihardjo</keyname><forenames>Arnold P.</forenames></author></authors><title>Computing Similarity between a Pair of Trajectories</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent advances in sensing and tracking technology, trajectory data is
becoming increasingly pervasive and analysis of trajectory data is becoming
exceedingly important. A fundamental problem in analyzing trajectory data is
that of identifying common patterns between pairs or among groups of
trajectories. In this paper, we consider the problem of identifying similar
portions between a pair of trajectories, each observed as a sequence of points
sampled from it.
  We present new measures of trajectory similarity --- both local and global
--- between a pair of trajectories to distinguish between similar and
dissimilar portions. Our model is robust under noise and outliers, it does not
make any assumptions on the sampling rates on either trajectory, and it works
even if they are partially observed. Additionally, the model also yields a
scalar similarity score which can be used to rank multiple pairs of
trajectories according to similarity, e.g. in clustering applications. We also
present efficient algorithms for computing the similarity under our measures;
the worst-case running time is quadratic in the number of sample points.
  Finally, we present an extensive experimental study evaluating the
effectiveness of our approach on real datasets, comparing with it with earlier
approaches, and illustrating many issues that arise in trajectory data. Our
experiments show that our approach is highly accurate in distinguishing similar
and dissimilar portions as compared to earlier methods even with sparse
sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1597</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1597</id><created>2013-03-06</created><authors><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames><affiliation>IIIT-Hyderabad, India</affiliation></author></authors><title>Concurrent Cyber Physical Systems:Tensor State Space Representation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper, state space representation of concurrent, linearly
coupled dynamical systems is discussed. It is reasoned that the Tensor State
Space Representation (TSSR) proposed in [Rama1] is directly applicable in such
a problem. Also some discussion on linearly coupled, concurrent systems
evolving on multiple time scales is included. Briefly new ideas related to
distributed signal processing in cyber physical systems are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1599</identifier>
 <datestamp>2013-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1599</id><created>2013-03-06</created><authors><author><keyname>Yan</keyname><forenames>Xiao-Yong</forenames></author><author><keyname>Fan</keyname><forenames>Ying</forenames></author><author><keyname>Di</keyname><forenames>Zengru</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Wu</keyname><forenames>Jinshan</forenames></author></authors><title>Efficient learning strategy of Chinese characters based on network
  approach</title><categories>physics.soc-ph cs.CL cs.SI</categories><comments>8 pages, 6 figures</comments><journal-ref>PLoS ONE 8(8): e69745 (2013)</journal-ref><doi>10.1371/journal.pone.0069745</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on network analysis of hierarchical structural relations among Chinese
characters, we develop an efficient learning strategy of Chinese characters. We
regard a more efficient learning method if one learns the same number of useful
Chinese characters in less effort or time. We construct a node-weighted network
of Chinese characters, where character usage frequencies are used as node
weights. Using this hierarchical node-weighted network, we propose a new
learning method, the distributed node weight (DNW) strategy, which is based on
a new measure of nodes' importance that takes into account both the weight of
the nodes and the hierarchical structure of the network. Chinese character
learning strategies, particularly their learning order, are analyzed as
dynamical processes over the network. We compare the efficiency of three
theoretical learning methods and two commonly used methods from mainstream
Chinese textbooks, one for Chinese elementary school students and the other for
students learning Chinese as a second language. We find that the DNW method
significantly outperforms the others, implying that the efficiency of current
learning methods of major textbooks can be greatly improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1609</identifier>
 <datestamp>2014-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1609</id><created>2013-03-07</created><authors><author><keyname>Wang</keyname><forenames>He</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Reed</keyname><forenames>Mark C.</forenames></author></authors><title>Physical Layer Security in Cellular Networks: A Stochastic Geometry
  Approach</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Wireless Communications</comments><journal-ref>IEEE Trans. on Wireless Commun., vol. 12, no. 6, pp. 2776-2787,
  Jun. 2013</journal-ref><doi>10.1109/TWC.2013.041713.120865</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the information-theoretic secrecy performance in
large-scale cellular networks based on a stochastic geometry framework. The
locations of both base stations and mobile users are modeled as independent
two-dimensional Poisson point processes. We consider two important features of
cellular networks, namely, information exchange between base stations and cell
association, to characterize their impact on the achievable secrecy rate of an
arbitrary downlink transmission with a certain portion of the mobile users
acting as potential eavesdroppers. In particular, tractable results are
presented under diverse assumptions on the availability of eavesdroppers'
location information at the serving base station, which captures the benefit
from the exchange of the location information between base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1613</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1613</id><created>2013-03-07</created><authors><author><keyname>Rusu</keyname><forenames>Andrei</forenames></author></authors><title>On parametrical expressibility in the free void-generated diagonalizable
  algebra</title><categories>math.LO cs.DM cs.LO</categories><comments>4 pages</comments><msc-class>03F45, 03B45, 03G25, 06E25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper we show that there are infinitely many classes of term
functions in the free-void generated diagonalizable algebra, which are
precomplete with respect to parametrical expressibility of functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1624</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1624</id><created>2013-03-07</created><authors><author><keyname>Wong</keyname><forenames>Yongkang</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>On Robust Face Recognition via Sparse Encoding: the Good, the Bad, and
  the Ugly</title><categories>cs.CV</categories><journal-ref>IET Biometrics, Vol. 3, No. 4, pp. 176-189, 2014</journal-ref><doi>10.1049/iet-bmt.2013.0033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of face recognition, Sparse Representation (SR) has received
considerable attention during the past few years. Most of the relevant
literature focuses on holistic descriptors in closed-set identification
applications. The underlying assumption in SR-based methods is that each class
in the gallery has sufficient samples and the query lies on the subspace
spanned by the gallery of the same class. Unfortunately, such assumption is
easily violated in the more challenging face verification scenario, where an
algorithm is required to determine if two faces (where one or both have not
been seen before) belong to the same person. In this paper, we first discuss
why previous attempts with SR might not be applicable to verification problems.
We then propose an alternative approach to face verification via SR.
Specifically, we propose to use explicit SR encoding on local image patches
rather than the entire face. The obtained sparse signals are pooled via
averaging to form multiple region descriptors, which are then concatenated to
form an overall face descriptor. Due to the deliberate loss spatial relations
within each region (caused by averaging), the resulting descriptor is robust to
misalignment &amp; various image deformations. Within the proposed framework, we
evaluate several SR encoding techniques: l1-minimisation, Sparse Autoencoder
Neural Network (SANN), and an implicit probabilistic technique based on
Gaussian Mixture Models. Thorough experiments on AR, FERET, exYaleB, BANCA and
ChokePoint datasets show that the proposed local SR approach obtains
considerably better and more robust performance than several previous
state-of-the-art holistic SR methods, in both verification and closed-set
identification problems. The experiments also show that l1-minimisation based
encoding has a considerably higher computational than the other techniques, but
leads to higher recognition rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1626</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1626</id><created>2013-03-07</created><authors><author><keyname>Hansen</keyname><forenames>Johan P.</forenames></author></authors><title>Forms and Linear Network Codes</title><categories>cs.IT math.AG math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general theory to obtain linear network codes utilizing forms
and obtain explicit families of equidimensional vector spaces, in which any
pair of distinct vector spaces intersect in the same small dimension. The
theory is inspired by the methods of the author utilizing the osculating spaces
of Veronese varieties.
  Linear network coding transmits information in terms of a basis of a vector
space and the information is received as a basis of a possibly altered vector
space. Ralf Koetter and Frank R. Kschischang introduced a metric on the set af
vector spaces and showed that a minimal distance decoder for this metric
achieves correct decoding if the dimension of the intersection of the
transmitted and received vector space is sufficiently large.
  The vector spaces in our construction are equidistant in the above metric and
the distance between any pair of vector spaces is large making them suitable
for linear network coding.
  The parameters of the resulting linear network codes are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1633</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1633</id><created>2013-03-07</created><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Dai</keyname><forenames>Yu-Hong</forenames></author></authors><title>Joint power and admission control via p norm minimization deflation</title><categories>cs.IT math.IT</categories><comments>2013 IEEE International Conference on Acoustics, Speech, and Signal
  Processing</comments><doi>10.1109/TSP.2012.2236319</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an interference network, joint power and admission control aims to support
a maximum number of links at their specified signal to interference plus noise
ratio (SINR) targets while using a minimum total transmission power. In our
previous work, we formulated the joint control problem as a sparse
$\ell_0$-minimization problem and relaxed it to a $\ell_1$-minimization
problem. In this work, we propose to approximate the $\ell_0$-optimization
problem to a p norm minimization problem where $0&lt;p&lt;1$, since intuitively p
norm will approximate 0 norm better than 1 norm. We first show that the
$\ell_p$-minimization problem is strongly NP-hard and then derive a
reformulation of it such that the well developed interior-point algorithms can
be applied to solve it. The solution to the $\ell_p$-minimization problem can
efficiently guide the link's removals (deflation). Numerical simulations show
the proposed heuristic outperforms the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1635</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1635</id><created>2013-03-07</created><authors><author><keyname>Nasehi</keyname><forenames>Hassanali</forenames></author><author><keyname>Javan</keyname><forenames>Nastooh Taheri</forenames></author><author><keyname>Aghababa</keyname><forenames>Amir Bagheri</forenames></author><author><keyname>Birgani</keyname><forenames>Yasna Ghanbari</forenames></author></authors><title>Improving Energy Efficiency in MANETs by Multi-Path Routing</title><categories>cs.NI</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  5, No. 1, February 2013</journal-ref><doi>10.5121/ijwmn.2013.5113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some multi-path routing algorithm in MANET, simultaneously send information
to the destination through several directions to reduce end-to-end delay. In
all these algorithms, the sent traffic through a path affects the adjacent path
and unintentionally increases the delay due to the use of adjacent paths.
Because, there are repetitive competitions among neighboring nodes, in order to
obtain the joint channel in adjacent paths. The represented algorithm in this
study tries to discover the distinct paths between source and destination nodes
with using Omni directional antennas, to send information through these
simultaneously. For this purpose, the number of active neighbors is counted in
each direction with using a strategy. These criterions are effectively used to
select routes. Proposed algorithm is based on AODV routing algorithm, and in
the end it is compared with AOMDV, AODVM, and IZM-DSR algorithms which are
multi-path routing algorithms based on AODV and DSR. Simulation results show
that using the proposed algorithm creates a significant improvement in energy
efficiency and reducing end-to-end delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1640</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1640</id><created>2013-03-07</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Testing Mutual Duality of Planar Graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>14 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study the problem \mpd, which asks for two planar graphs
$G_1$ and $G_2$ whether $G_1$ can be embedded such that its dual is isomorphic
to $G_2$. Our algorithmic main result is an NP-completeness proof for the
general case and a linear-time algorithm for biconnected graphs. To shed light
onto the combinatorial structure of the duals of a planar graph, we consider
the \emph{common dual relation} $\sim$, where $G_1 \sim G_2$ if and only if
they have a common dual. While $\sim$ is generally not transitive, we show that
the restriction to biconnected graphs is an equivalence relation. In this case,
being dual to each other carries over to the equivalence classes, i.e., two
graphs are dual to each other if and only if any two elements of their
respective equivalence classes are dual to each other.
  To achieve the efficient testing algorithm for \mpd on biconnected graphs, we
devise a succinct representation of the equivalence class of a biconnected
planar graph. It is similar to SPQR-trees and represents exactly the graphs
that are contained in the equivalence class. The testing algorithm then works
by testing in linear time whether two such representations are isomorphic. We
note that a special case of \mpd is testing whether a graph $G$ is self-dual.
Our algorithm handles the case where $G$ is biconnected and our NP-hardness
proof extends to testing self-duality of general planar graphs and also to
testing map self-duality, where a graph $G$ is map self-dual if it admits a
planar embedding $\mathcal G$ such that $G^\star$ is isomorphic to $G$, and
additionally the embedding induced by $\mathcal G$ on $G^\star$ is $\mathcal
G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1643</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1643</id><created>2013-03-07</created><authors><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Subashini</keyname><forenames>R.</forenames></author></authors><title>$d$-COS-R is FPT via Interval Deletion</title><categories>cs.DS</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary matrix $M$ has the Consecutive Ones Property (COP) if there exists a
permutation of columns that arranges the ones consecutively in all the rows.
Given a matrix, the $d$-COS-R problem is to determine if there exists a set of
at most $d$ rows whose deletion results in a matrix with COP. We consider the
parameterized complexity of this problem with respect to the number $d$ of rows
to be deleted as the parameter. The closely related Interval Deletion problem
has recently shown to be FPT [Y. Cao and D. Marx, Interval Deletion is
Fixed-Parameter Tractable, arXiv:1211.5933 [cs.DS],2012]. In this work, we
describe a recursive depth-bounded search tree algorithm in which the problems
at the leaf-level are solved as instances of Interval Deletion. The running
time of the algorithm is dominated by the running time of Interval Deletion,
and therefore we show that $d$-COS-R is fixed-parameter tractable and has a
run-time of $O^*(10^d)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1645</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1645</id><created>2013-03-07</created><authors><author><keyname>Mukherjee</keyname><forenames>Rajdeep</forenames></author><author><keyname>Ghosh</keyname><forenames>Priyankar</forenames></author><author><keyname>Dasgupta</keyname><forenames>Pallab</forenames></author><author><keyname>Pal</keyname><forenames>Ajit</forenames></author></authors><title>A Multi-objective Perspective for Operator Scheduling using Fine-grained
  DVS Architecture</title><categories>cs.OH</categories><comments>18 pages, 6 figures, International journal of VLSI design &amp;
  Communication Systems (VLSICS)</comments><acm-class>B.5.2</acm-class><doi>10.5121/vlsic.2013.4109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stringent power budget of fine grained power managed digital integrated
circuits have driven chip designers to optimize power at the cost of area and
delay, which were the traditional cost criteria for circuit optimization. The
emerging scenario motivates us to revisit the classical operator scheduling
problem under the availability of DVFS enabled functional units that can
trade-off cycles with power. We study the design space defined due to this
trade-off and present a branch-and-bound(B/B) algorithm to explore this state
space and report the pareto-optimal front with respect to area and power. The
scheduling also aims at maximum resource sharing and is able to attain
sufficient area and power gains for complex benchmarks when timing constraints
are relaxed by sufficient amount. Experimental results show that the algorithm
that operates without any user constraint(area/power) is able to solve the
problem for most available benchmarks, and the use of power budget or area
budget constraints leads to significant performance gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1646</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1646</id><created>2013-03-07</created><updated>2013-06-27</updated><authors><author><keyname>de Keijzer</keyname><forenames>Bart</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Sch&#xe4;fer</keyname><forenames>Guido</forenames></author><author><keyname>Telelis</keyname><forenames>Orestis</forenames></author></authors><title>On the Inefficiency of Standard Multi-Unit Auctions</title><categories>cs.GT</categories><comments>21st European Symposium on Algorithms (ESA), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two standard multi-unit auction formats for allocating multiple
units of a single good to multi-demand bidders. The first one is the
Discriminatory Auction, which charges every winner his winning bids. The second
is the Uniform Price Auction, which determines a uniform price to be paid per
unit. Variants of both formats find applications ranging from the allocation of
state bonds to investors, to online sales over the internet, facilitated by
popular online brokers. For these formats, we consider two bidding interfaces:
(i) standard bidding, which is most prevalent in the scientific literature, and
(ii) uniform bidding, which is more popular in practice. In this work, we
evaluate the economic inefficiency of both multi-unit auction formats for both
bidding interfaces, by means of upper and lower bounds on the Price of Anarchy
for pure Nash equilibria and mixed Bayes-Nash equilibria. Our developments
improve significantly upon bounds that have been obtained recently in
[Markakis, Telelis, SAGT 2012] and [Syrgkanis, Tardos, STOC 2013] for
submodular valuation functions. Moreover, we consider for the first time
bidders with subadditive valuation functions for these auction formats. Our
results signify that these auctions are nearly efficient, which provides
further justification for their use in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1647</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1647</id><created>2013-03-07</created><updated>2014-01-09</updated><authors><author><keyname>Michalopoulos</keyname><forenames>Diomidis S.</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Relay Selection for Simultaneous Information Transmission and Wireless
  Energy Transfer: A Tradeoff Perspective</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In certain applications, relay terminals can be employed to simultaneously
deliver information and energy to a designated receiver and a radio frequency
(RF) energy harvester, respectively. In such scenarios, the relay that is
preferable for information transmission does not necessarily coincide with the
relay with the strongest channel to the energy harvester, since the
corresponding channels fade independently. Relay selection thus entails a
tradeoff between the efficiency of the information transfer to the receiver and
the amount of energy transferred to the energy harvester. The study of this
tradeoff is the subject on which this work mainly focuses. Specifically, we
investigate the behavior of the ergodic capacity and the outage probability of
the information transmission to the receiver, for a given amount of energy
transferred to the RF energy harvester. We propose two relay selection methods
that apply to any number of available relays. Furthermore, for the case of two
relays, we develop the optimal relay selection method in a maximum capacity /
minimum outage probability sense, for a given energy transfer constraint. A
close-to-optimal selection method that is easier to analyze than the optimal
one is also examined. Closed-form expressions for the capacity-energy and the
outage-energy tradeoffs of the developed schemes are provided and corroborated
by simulations. Interesting insights on the aforementioned tradeoffs are
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1651</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1651</id><created>2013-03-07</created><updated>2013-05-06</updated><authors><author><keyname>Scharpff</keyname><forenames>Tobias</forenames></author><author><keyname>Iglberger</keyname><forenames>Klaus</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Ruede</keyname><forenames>Ulrich</forenames></author></authors><title>Model-guided Performance Analysis of the Sparse Matrix-Matrix
  Multiplication</title><categories>cs.PF cs.MS</categories><comments>8 pages, 12 figures. Small corrections w.r.t. previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving high efficiency with numerical kernels for sparse matrices is of
utmost importance, since they are part of many simulation codes and tend to use
most of the available compute time and resources. In addition, especially in
large scale simulation frameworks the readability and ease of use of
mathematical expressions are essential components for the continuous
maintenance, modification, and extension of software. In this context, the
sparse matrix-matrix multiplication is of special interest. In this paper we
thoroughly analyze the single-core performance of sparse matrix-matrix
multiplication kernels in the Blaze Smart Expression Template (SET) framework.
We develop simple models for estimating the achievable maximum performance, and
use them to assess the efficiency of our implementations. Additionally, we
compare these kernels with several commonly used SET-based C++ libraries,
which, just as Blaze, aim at combining the requirements of high performance
with an elegant user interface. For the different sparse matrix structures
considered here, we show that our implementations are competitive or faster
than those of the other SET libraries for most problem sizes on a current Intel
multicore processor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1654</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1654</id><created>2013-03-07</created><authors><author><keyname>James</keyname><forenames>Matthew R.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author></authors><title>A Popov Stability Condition for Uncertain Linear Quantum Systems</title><categories>quant-ph cs.SY math.OC</categories><comments>A shortened version to appear in the proceedings of the 2013 American
  Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a Popov type approach to the problem of robust stability
for a class of uncertain linear quantum systems subject to unknown
perturbations in the system Hamiltonian. A general stability result is given
for a general class of perturbations to the system Hamiltonian. Then, the
special case of a nominal linear quantum system is considered with quadratic
perturbations to the system Hamiltonian. In this case, a robust stability
condition is given in terms of a frequency domain condition which is of the
same form as the standard Popov stability condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1663</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1663</id><created>2013-03-07</created><authors><author><keyname>Baicu</keyname><forenames>Floarea</forenames></author><author><keyname>Baches</keyname><forenames>Maria Alexandra</forenames></author></authors><title>Impact Analysis for Risks in Informatics Systems</title><categories>q-fin.GN cs.CY</categories><comments>12 pages, 3 figures, 5 refferences, ENEC 2008, ISSN 265-2550</comments><journal-ref>F.BAICU, M.A.BACHES - Impact Analysis for Risks in Informatics
  Systems, ENEC 2008, PP. 269-281</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper are presented methods of impact analysis on informatics system
security accidents, qualitative and quantitative methods, starting with risk
and informational system security definitions. It is presented the relationship
between the risks of exploiting vulnerabilities of security system, security
level of these informatics systems, probability of exploiting the weak points
subject to financial losses of a company, respectively impact of a security
accident on the company. Herewith are presented some examples concerning losses
caused by excesses within informational systems and depicted from the study
carried out by CSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1667</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1667</id><created>2013-03-07</created><authors><author><keyname>da Silva</keyname><forenames>Francisco Assis</forenames></author><author><keyname>Artero</keyname><forenames>Almir Olivette</forenames></author><author><keyname>de Paiva</keyname><forenames>Maria Stela Veludo</forenames></author><author><keyname>Barbosa</keyname><forenames>Ricardo Luis</forenames></author></authors><title>ALPRS - A New Approach for License Plate Recognition using the Sift
  Algorithm</title><categories>cs.CV</categories><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.1, February 2013</journal-ref><doi>10.5121/sipij.2013.4102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for the automatic license plate
recognition, which includes the SIFT algorithm in step to locate the plate in
the input image. In this new approach, besides the comparison of the features
obtained with the SIFT algorithm, the correspondence between the spatial
orientations and the positioning associated with the keypoints is also
observed. Afterwards, an algorithm is used for the character recognition of the
plates, very fast, which makes it possible its application in real time. The
results obtained with the proposed approach presented very good success rates,
so much for locating the characters in the input image, as for their
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1671</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1671</id><created>2013-03-07</created><authors><author><keyname>Krithika</keyname><forenames>R.</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author></authors><title>Another Disjoint Compression Algorithm for OCT</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an elegant O*(2^k) algorithm for the disjoint compression problem
for Odd Cycle Transversal based on a reduction to Above Guarantee Vertex Cover.
We believe that this algorithm refines the understanding of the Odd Cycle
Transversal algorithm by Reed, Smith and Vetta.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1691</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1691</id><created>2013-03-07</created><authors><author><keyname>Rey</keyname><forenames>Anja</forenames></author><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>False-Name Manipulation in Weighted Voting Games is Hard for
  Probabilistic Polynomial Time</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  False-name manipulation refers to the question of whether a player in a
weighted voting game can increase her power by splitting into several players
and distributing her weight among these false identities. Analogously to this
splitting problem, the beneficial merging problem asks whether a coalition of
players can increase their power in a weighted voting game by merging their
weights. Aziz et al. [ABEP11] analyze the problem of whether merging or
splitting players in weighted voting games is beneficial in terms of the
Shapley-Shubik and the normalized Banzhaf index, and so do Rey and Rothe [RR10]
for the probabilistic Banzhaf index. All these results provide merely
NP-hardness lower bounds for these problems, leaving the question about their
exact complexity open. For the Shapley--Shubik and the probabilistic Banzhaf
index, we raise these lower bounds to hardness for PP, &quot;probabilistic
polynomial time&quot;, and provide matching upper bounds for beneficial merging and,
whenever the number of false identities is fixed, also for beneficial
splitting, thus resolving previous conjectures in the affirmative. It follows
from our results that beneficial merging and splitting for these two power
indices cannot be solved in NP, unless the polynomial hierarchy collapses,
which is considered highly unlikely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1693</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1693</id><created>2013-03-07</created><updated>2013-04-30</updated><authors><author><keyname>Park</keyname><forenames>Jaehyun</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Joint Wireless Information and Energy Transfer in a Two-User MIMO
  Interference Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates joint wireless information and energy transfer in a
two-user MIMO interference channel, in which each receiver either decodes the
incoming information data (information decoding, ID) or harvests the RF energy
(energy harvesting, EH) to operate with a potentially perpetual energy supply.
In the two-user interference channel, we have four different scenarios
according to the receiver mode -- ($ID_1$, $ID_2$), ($EH_1$, $EH_2$), ($EH_1$,
$ID_2$), and ($ID_1$, $EH_2$). While the maximum information bit rate is
unknown and finding the optimal transmission strategy is still open for
($ID_1$, $ID_2$), we have derived the optimal transmission strategy achieving
the maximum harvested energy for ($EH_1$, $EH_2$). For ($EH_1$, $ID_2$), and
($ID_1$, $EH_2$), we find a necessary condition of the optimal transmission
strategy and, accordingly, identify the achievable rate-energy (R-E) tradeoff
region for two transmission strategies that satisfy the necessary condition -
maximum energy beamforming (MEB) and minimum leakage beamforming (MLB).
Furthermore, a new transmission strategy satisfying the necessary condition -
signal-to-leakage-and-energy ratio (SLER) maximization beamforming - is
proposed and shown to exhibit a better R-E region than the MEB and the MLB
strategies. Finally, we propose a mode scheduling method to switch between
($EH_1$, $ID_2$) and ($ID_1$, $EH_2$) based on the SLER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1695</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1695</id><created>2013-03-07</created><authors><author><keyname>Bisht</keyname><forenames>Raj Kishor</forenames></author><author><keyname>Bisht</keyname><forenames>Ila Pant</forenames></author></authors><title>Effect of Query Formation on Web Search Engine Results</title><categories>cs.IR</categories><comments>6 pages, journal</comments><journal-ref>International Journal of Natural Language Computing, 2013, Vol 2,
  No. 1, 31-36</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query in a search engine is generally based on natural language. A query can
be expressed in more than one way without changing its meaning as it depends on
thinking of human being at a particular moment. Aim of the searcher is to get
most relevant results immaterial of how the query has been expressed. In the
present paper, we have examined the results of search engine for change in
coverage and similarity of first few results when a query is entered in two
semantically same but in different formats. Searching has been made through
Google search engine. Fifteen pairs of queries have been chosen for the study.
The t-test has been used for the purpose and the results have been checked on
the basis of total documents found, similarity of first five and first ten
documents found in the results of a query entered in two different formats. It
has been found that the total coverage is same but first few results are
significantly different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1697</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1697</id><created>2013-03-07</created><authors><author><keyname>Bhujbal</keyname><forenames>Avinash</forenames></author><author><keyname>Jagtap</keyname><forenames>Ashish</forenames></author><author><keyname>Gurav</keyname><forenames>Devendra</forenames></author><author><keyname>Jameskutty</keyname><forenames>Tino</forenames></author></authors><title>Secure Video Streaming Plug-In</title><categories>cs.MM cs.CR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Video sharing sites like YouTube, Metacafe, Dailymotion, Vimeo, etc. provide
a platform for media content sharing among its users. Some of these videos are
copyright protected and restricted from being downloaded and saved. But users
can use various download managers or application programs to download and save
these videos. This affects the incoming traffic on these websites reducing
their hit rate and consequently reducing their revenue. Adobe Flash Player is
the most commonly used player for watching online videos. It uses RTMP (Real
Time Messaging Protocol) to stream audio, video and data over the Internet,
between a Flash Player and Adobe Flash Media Server.Here, we propose a plug-in
that enables the site owner control over downloading of videos from such
website. The plug-in will be installed at the client side with the consent of
the user. When the video is being played this plug-in will send unique keys to
the media server. The server will continue streaming the video after verifying
the keys. Download managers or application programs will not be able to
download the videos as they wont be able to create the unique keys that need to
be sent to the server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1700</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1700</id><created>2013-03-07</created><authors><author><keyname>Campillo-Gimenez</keyname><forenames>Boris</forenames></author><author><keyname>Jouini</keyname><forenames>Wassim</forenames></author><author><keyname>Bayat</keyname><forenames>Sahar</forenames></author><author><keyname>Cuggia</keyname><forenames>Marc</forenames></author></authors><title>K-Nearest Neighbour algorithm coupled with logistic regression in
  medical case-based reasoning systems. Application to prediction of access to
  the renal transplant waiting list in Brittany</title><categories>cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Introduction. Case Based Reasoning (CBR) is an emerg- ing decision making
paradigm in medical research where new cases are solved relying on previously
solved similar cases. Usually, a database of solved cases is provided, and
every case is described through a set of attributes (inputs) and a label
(output). Extracting useful information from this database can help the CBR
system providing more reliable results on the yet to be solved cases.
Objective. For that purpose we suggest a general frame- work where a CBR
system, viz. K-Nearest Neighbor (K-NN) algorithm, is combined with various
information obtained from a Logistic Regression (LR) model. Methods. LR is
applied, on the case database, to assign weights to the attributes as well as
the solved cases. Thus, five possible decision making systems based on K-NN
and/or LR were identified: a standalone K-NN, a standalone LR and three soft
K-NN algorithms that rely on the weights based on the results of the LR. The
evaluation of the described approaches is performed in the field of renal
transplant access waiting list. Results and conclusion. The results show that
our suggested approach, where the K-NN algorithm relies on both weighted
attributes and cases, can efficiently deal with non relevant attributes,
whereas the four other approaches suffer from this kind of noisy setups. The
robustness of this approach suggests interesting perspectives for medical
problem solving tools using CBR methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1703</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1703</id><created>2013-03-07</created><authors><author><keyname>Boubekeur</keyname><forenames>Fatiha</forenames></author><author><keyname>Azzoug</keyname><forenames>Wassila</forenames></author></authors><title>Concept-based indexing in text information retrieval</title><categories>cs.IR cs.CL</categories><comments>18 pages, 5 tables, 3 figures</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 1, February 2013</journal-ref><doi>10.5121/ijcsit</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional information retrieval systems rely on keywords to index documents
and queries. In such systems, documents are retrieved based on the number of
shared keywords with the query. This lexical-focused retrieval leads to
inaccurate and incomplete results when different keywords are used to describe
the documents and queries. Semantic-focused retrieval approaches attempt to
overcome this problem by relying on concepts rather than on keywords to
indexing and retrieval. The goal is to retrieve documents that are semantically
relevant to a given user query. This paper addresses this issue by proposing a
solution at the indexing level. More precisely, we propose a novel approach for
semantic indexing based on concepts identified from a linguistic resource. In
particular, our approach relies on the joint use of WordNet and WordNetDomains
lexical databases for concept identification. Furthermore, we propose a
semantic-based concept weighting scheme that relies on a novel definition of
concept centrality. The resulting system is evaluated on the TIME test
collection. Experimental results show the effectiveness of our proposition over
traditional IR approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1716</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1716</id><created>2013-03-07</created><authors><author><keyname>Benzaken</keyname><forenames>V&#xe9;ronique</forenames><affiliation>LRI</affiliation></author><author><keyname>Castagna</keyname><forenames>Giuseppe</forenames><affiliation>PPS</affiliation></author><author><keyname>Nguy\~&#xea;n</keyname><forenames>Kim</forenames><affiliation>LRI</affiliation></author><author><keyname>Sim&#xe9;on</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author></authors><title>Static and dynamic semantics of NoSQL languages</title><categories>cs.PL cs.DB</categories><proxy>ccsd</proxy><journal-ref>POPL, Rome : Italy (2013)</journal-ref><doi>10.1145/2429069.2429083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a calculus for processing semistructured data that spans
differences of application area among several novel query languages, broadly
categorized as &quot;NoSQL&quot;. This calculus lets users define their own operators,
capturing a wider range of data processing capabilities, whilst providing a
typing precision so far typical only of primitive hard-coded operators. The
type inference algorithm is based on semantic type checking, resulting in type
information that is both precise, and flexible enough to handle structured and
semistructured data. We illustrate the use of this calculus by encoding a large
fragment of Jaql, including operations and iterators over JSON, embedded SQL
expressions, and co-grouping, and show how the encoding directly yields a
typing discipline for Jaql as it is, namely without the addition of any type
definition or type annotation in the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1717</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1717</id><created>2013-03-07</created><updated>2015-05-23</updated><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>Oracle Pushdown Automata, Nondeterministic Reducibilities, and the CFL
  Hierarchy over the Family of Context-Free Languages</title><categories>cs.FL cs.CC</categories><comments>This is a complete version of an extended abstract that appeared
  under a slightly different title in the Proceedings of the 40th International
  Conference on Current Trends in Theory and Practice of Computer Science
  (SOFSEM 2014), High Tatras, Slovakia, January 25-30, 2014, Lecture Notes in
  Computer Science, Springer-Verlag, vol.8327, pp.514-525, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To expand a fundamental theory of context-free languages, we equip
nondeterministic one-way pushdown automata with additional oracle mechanisms,
which naturally induce various nondeterministic reducibilities among formal
languages. As a natural restriction of NP-reducibility, we introduce a notion
of many-one CFL reducibility and conduct a ground work to formulate a coherent
framework for further expositions. Two more powerful reducibilities--bounded
truth-table and Turing CFL-reducibilities--are also discussed in comparison.
The Turing CFL-reducibility, in particular, helps us introduce an exquisite
hierarchy, called the CFL hierarchy, built over the family CFL of context-free
languages. For each level of this hierarchy, its basic structural properties
are proven and three alternative characterizations are presented. The second
level is not included in NC(2) unless NP= NC(2). The first and second levels of
the hierarchy are different. The rest of the hierarchy (more strongly, the
Boolean hierarchy built over each level of the hierarchy) is also infinite
unless the polynomial hierarchy over NP collapses. This follows from a
characterization of the Boolean hierarchy over the k-th level of the polynomial
hierarchy in terms of the Boolean hierarchy over the k+1st level of the CFL
hierarchy using log-space many-one reductions. Similarly, the complexity class
Theta(k) is related to the closure of the k-th level of the CFL hierarchy under
log-space truth-table reductions. We also argue that the CFL hierarchy
coincides with a hierarchy over CFL built by application of many-one
CFL-reductions. We show that BPCFL--a bounded-error probabilistic version of
CFL--is not included in CFL even in the presence of advice. Employing a known
circuit lower bound and a switching lemma, we exhibit a relativized world where
BPCFL is not located within the second level of the CFL hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1719</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1719</id><created>2013-01-25</created><authors><author><keyname>Colonnese</keyname><forenames>Stefania</forenames></author><author><keyname>Cusani</keyname><forenames>Roberto</forenames></author><author><keyname>Rinauro</keyname><forenames>Stefano</forenames></author><author><keyname>Ruggiero</keyname><forenames>Giorgia</forenames></author><author><keyname>Scarano</keyname><forenames>Gaetano</forenames></author></authors><title>Efficient Compressive Sampling of Spatially Sparse Fields in Wireless
  Sensor Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to EURASIP Journal on Advances in Signal Processing</comments><journal-ref>EURASIP Journal on Advances in Signal Processing 2013, 2013:136</journal-ref><doi>10.1186/1687-6180-2013-136</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSN), i.e. networks of autonomous, wireless sensing
nodes spatially deployed over a geographical area, are often faced with
acquisition of spatially sparse fields. In this paper, we present a novel
bandwidth/energy efficient CS scheme for acquisition of spatially sparse fields
in a WSN. The paper contribution is twofold. Firstly, we introduce a sparse,
structured CS matrix and we analytically show that it allows accurate
reconstruction of bidimensional spatially sparse signals, such as those
occurring in several surveillance application. Secondly, we analytically
evaluate the energy and bandwidth consumption of our CS scheme when it is
applied to data acquisition in a WSN. Numerical results demonstrate that our CS
scheme achieves significant energy and bandwidth savings wrt state-of-the-art
approaches when employed for sensing a spatially sparse field by means of a
WSN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1724</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1724</id><created>2013-03-07</created><authors><author><keyname>Teixeira</keyname><forenames>Ana L.</forenames></author><author><keyname>Leal</keyname><forenames>Jo&#xe3;o P.</forenames></author><author><keyname>Falcao</keyname><forenames>Andre O</forenames></author></authors><title>Automated Identification and Classification of Stereochemistry:
  Chirality and Double Bond Stereoisomerism</title><categories>cs.CE physics.chem-ph q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stereoisomers have the same molecular formula and the same atom connectivity
and their existence can be related to the presence of different
three-dimensional arrangements. Stereoisomerism is of great importance in many
different fields since the molecular properties and biological effects of the
stereoisomers are often significantly different. Most drugs for example, are
often composed of a single stereoisomer of a compound, and while one of them
may have therapeutic effects on the body, another may be toxic. A challenging
task is the automatic detection of stereoisomers using line input
specifications such as SMILES or InChI since it requires information about
group theory (to distinguish stereoisomers using mathematical information about
its symmetry), topology and geometry of the molecule. There are several
software packages that include modules to handle stereochemistry, especially
the ones to name a chemical structure and/or view, edit and generate chemical
structure diagrams. However, there is a lack of software capable of
automatically analyzing a molecule represented as a graph and generate a
classification of the type of isomerism present in a given atom or bond.
Considering the importance of stereoisomerism when comparing chemical
structures, this report describes a computer program for analyzing and
processing steric information contained in a chemical structure represented as
a molecular graph and providing as output a binary classification of the isomer
type based on the recommended conventions. Due to the complexity of the
underlying issue, specification of stereochemical information is currently
limited to explicit stereochemistry and to the two most common types of
stereochemistry caused by asymmetry around carbon atoms: chiral atom and double
bond. A Webtool to automatically identify and classify stereochemistry is
available at http://nams.lasige.di.fc.ul.pt/tools.php
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1725</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1725</id><created>2013-03-07</created><updated>2013-03-08</updated><authors><author><keyname>Begriche</keyname><forenames>Youcef</forenames></author><author><keyname>Thameri</keyname><forenames>Messaoud</forenames></author><author><keyname>Abed-Meraim</keyname><forenames>Karim</forenames></author></authors><title>Exact Conditional and Unconditional Cram\`er-Rao Bounds for Near Field
  Localization</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the Cram\`er-Rao lower Bound (CRB) for the source
localization problem in the near field. More specifically, we use the exact
expression of the delay parameter for the CRB derivation and show how this
exact CRB can be significantly different from the one given in the literature
and based on an approximate time delay expression (usually considered in the
Fresnel region). This CRB derivation is then generalized by considering the
exact expression of the received power profile (i.e., variable gain case)
which, to our best knowledge, has been ignored in the literature. Finally, we
exploit the CRB expression to introduce the new concept of Near Field
Localization (NFL) region for a target localization performance associated to
the application at hand. We illustrate the usefulness of the proposed CRB
derivation and its developments as well as the NFL region concept through
numerical simulations in different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1728</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1728</id><created>2013-03-07</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author><author><keyname>Abraham-Attah</keyname><forenames>P. O.</forenames></author></authors><title>A Digital Automatic Sliding Door with a Room Light Control System</title><categories>cs.OH</categories><comments>17 pages, 17 figures, Journal paper</comments><journal-ref>A.M. Zungeru, P.O. Abraham-Attah. A Digital Automatic Sliding Door
  with a Room Light Control System, International Journal of Information
  Technology, Modeling and Computing (IJITMC), vol. 1(1), pp. 1-17, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic door is an automated movable barrier installed in the entry of a
room or building to restrict access, provide ease of opening a door or provide
visual privacy. As a result of enhanced civilization and modernization, the
human nature demands more comfort to his life. The man seeks ways to do things
easily and which saves time. So thus, the automatic gates are one of the
examples that human nature invent to bring comfort and ease in its daily life.
To this end, we model and design an automatic sliding door with a room light
control system to provide the mentioned needs. This was achieved by considering
some factors such as economy, availability of components and research
materials, efficiency, compatibility and portability and also durability in the
design process. The performance of the system after test met design
specifications. This system works on the principle of breaking an infrared beam
of light, sensed by a photodiode. It consists of two transmitting infrared
diodes and two receiving photo-diodes. The first one is for someone coming in
and the second one is for someone going out of the room. The photodiodes are
connected to comparators, which give a lower output when the beam is broken and
high output when transmitting normally. The general operation of the work and
performance is dependent on the presence of an intruder entering through the
door and how close he/she is in closer to the door. The door is meant to open
automatically but in a case where there is no power supply trying to force the
door open would damage the mechanical control system of the unit. The overall
work was implemented with a constructed work, tested working and perfectly
functional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1732</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1732</id><created>2013-03-07</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author></authors><title>Design and Development of an Ultrasonic Motion Detector</title><categories>cs.OH physics.ins-det</categories><comments>7 figures, 13 pages</comments><journal-ref>A.M. Zungeru. Design and Development of an Ultrasonic Motion
  Detector, International Journal of Security, Privacy and Trust Management
  (IJSPTM), vol. 2(1), pp. 1-13, 2013</journal-ref><doi>10.5121/ijsptm.2013.2101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ultrasonic motion detector devices emit ultrasonic sound energy into an
area of interest (monitored area), and this further reacts to a change in the
reflected energy pattern. The system uses a technique that is based on a
frequency shift in reflected energy to detect a movement or change in position
(motion). In this system, ultrasonic sound is transmitted from the transmitting
device which is normally in the form of energy. The transmitted sound utilizes
air as its medium and this travel in a wave type motion. The wave is reflected
back from the surroundings in the room/hallway and the device hears a pitch
characteristic of the protected environment. In this system, the wave pattern
is disturbed and reflected back more quickly, thus increasing the pitch and
signaling an alarm whenever motion is detected. The main contribution of this
work is the design of a circuit that can sense motion through movement of
anything, a low cost and portable motion detector, and the design of a circuit
that can be used to trigger another circuit whether to ON or OFF depending on
the circuit attached to it. Generally, the design is made to detect movement or
moving object in a an enclosed area. In this work, a transmitter transducer
generates a signal at a frequency of 40khz, and when the signal is blocked by
any moving object, and this in turn, triggers a buzzer via a timing circuit.
This system works on the principle of the signal interference by a moving body,
and the system is dependent on the presence of an intruder or moving object
within a monitored area. The system after design and construction was tested
and found to work in accordance with specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1733</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1733</id><created>2013-03-07</created><updated>2013-05-31</updated><authors><author><keyname>London</keyname><forenames>Ben</forenames></author><author><keyname>Rekatsinas</keyname><forenames>Theodoros</forenames></author><author><keyname>Huang</keyname><forenames>Bert</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Multi-relational Learning Using Weighted Tensor Decomposition with
  Modular Loss</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a modular framework for multi-relational learning via tensor
decomposition. In our learning setting, the training data contains multiple
types of relationships among a set of objects, which we represent by a sparse
three-mode tensor. The goal is to predict the values of the missing entries. To
do so, we model each relationship as a function of a linear combination of
latent factors. We learn this latent representation by computing a low-rank
tensor decomposition, using quasi-Newton optimization of a weighted objective
function. Sparsity in the observed data is captured by the weighted objective,
leading to improved accuracy when training data is limited. Exploiting sparsity
also improves efficiency, potentially up to an order of magnitude over
unweighted approaches. In addition, our framework accommodates arbitrary
combinations of smooth, task-specific loss functions, making it better suited
for learning different types of relations. For the typical cases of real-valued
functions and binary relations, we propose several loss functions and derive
the associated parameter gradients. We evaluate our method on synthetic and
real data, showing significant improvements in both accuracy and scalability
over related factorization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1734</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1734</id><created>2013-03-07</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author></authors><title>An electronic digital combination lock: A precise and reliable security
  system</title><categories>cs.OH</categories><comments>13 pages, 9 figures, 1 Table</comments><journal-ref>A.M. Zungeru. An electronic digital combination lock: A precise
  and reliable security system. International Journal of Security, Privacy and
  Trust Management (IJSPTM), vol. 2(1), pp. 29-41, 2013</journal-ref><doi>10.5121/ijsptm.2013.2103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing rate of crime, attacks by thieves, intruders and vandals,
despite all forms of security gadgets and locks still need the attention of
researchers to find a permanent solution to the well being of lives and
properties of individuals. To this end, we design a cheap and effective
security system for buildings, cars, safes, doors and gates, so as to prevent
unauthorized person from having access to ones properties through the use of
codes, we therefore experiment the application of electronic devices as locks.
However, a modular approach was employed in the design in which the combination
lock was divided into units and each unit designed separately before being
coupled to form a whole functional system. During the design, we conducted
Twenty tests with the first eight combinations being four in number, the next
seven tests being five and the last five combinations being six. This was done
because of the incorporation of 2 dummy switches in the combinations. From the
result obtained, combinations 8, 11, 13 gave the correct output combination.
However, 8 being the actual combination gave the required output. The general
operation of the system and performance is dependent on the key combinations.
The overall system was constructed and tested and it works perfectly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1738</identifier>
 <datestamp>2013-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1738</id><created>2013-03-07</created><updated>2013-10-16</updated><authors><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Mixing local and global information for community detection in large
  networks</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Journal of Computer and System Sciences 80(1):72-87, 2014</journal-ref><doi>10.1016/j.jcss.2013.03.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering large complex networks plays a key role in several
scientific fields ranging from Biology to Sociology and Computer Science. Many
approaches to clustering complex networks are based on the idea of maximizing a
network modularity function. Some of these approaches can be classified as
global because they exploit knowledge about the whole network topology to find
clusters. Other approaches, instead, can be interpreted as local because they
require only a partial knowledge of the network topology, e.g., the neighbors
of a vertex. Global approaches are able to achieve high values of modularity
but they do not scale well on large networks and, therefore, they cannot be
applied to analyze on-line social networks like Facebook or YouTube. In
contrast, local approaches are fast and scale up to large, real-life networks,
at the cost of poorer results than those achieved by local methods. In this
article we propose a glocal method to maximizing modularity, i.e., our method
uses information at the global level, yet its scalability on large networks is
comparable to that of local methods. The proposed method is called COmplex
Network CLUster DEtection (or, shortly, CONCLUDE.) It works in two stages: in
the first stage it uses an information-propagation model, based on random and
non-backtracking walks of finite length, to compute the importance of each edge
in keeping the network connected (called edge centrality.) Then, edge
centrality is used to map network vertices onto points of an Euclidean space
and to compute distances between all pairs of connected vertices. In the second
stage, CONCLUDE uses the distances computed in the first stage to partition the
network into clusters. CONCLUDE is computationally efficient since in the
average case its cost is roughly linear in the number of edges of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1740</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1740</id><created>2013-03-07</created><authors><author><keyname>Gowrishankar</keyname><forenames>K.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>Dr. C.</forenames></author><author><keyname>Kaniezhil</keyname><forenames>R.</forenames></author></authors><title>Maximum Possibility Of Spectrum Access In Cognitive Radio Using Fuzzy
  Logic System</title><categories>cs.NI</categories><comments>8 pages. arXiv admin note: substantial text overlap with
  arXiv:1210.3437; and text overlap with arXiv:1109.0257 by other authors</comments><journal-ref>IJERA-Vol. 2, Issue4, July-August 2012, pp.1408-1415</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many spectrum methods have been proposed to use spectrum effectively, the
opportunistic spectrum access has become the most viable approach to achieve
near-optimal spectrum utilization by allowing secondary users to sense and
access available spectrum opportunistically. However, a naive spectrum access
for secondary users can make spectrum utilization inefficient and increase
interference to adjacent users. In this paper, we propose a novel approach
using Fuzzy Logic System (FLS) to control the spectrum access. We have to
minimize the call blocking and interference. The linguistic knowledge of
spectrum access are based on three descriptors namely spectrum utilization
efficiency of the secondary user, its degree of mobility, and its distance to
the primary user. The spectrum is chosen for accessing based on the maximum
possibility for better utilization of the spectrum
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1741</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1741</id><created>2013-03-07</created><authors><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>Enhancing community detection using a network weighting strategy</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>28 pages, 2 figures</comments><journal-ref>Information Sciences, 222:648-668, 2013</journal-ref><doi>10.1016/j.ins.2012.08.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A community within a network is a group of vertices densely connected to each
other but less connected to the vertices outside. The problem of detecting
communities in large networks plays a key role in a wide range of research
areas, e.g. Computer Science, Biology and Sociology. Most of the existing
algorithms to find communities count on the topological features of the network
and often do not scale well on large, real-life instances.
  In this article we propose a strategy to enhance existing community detection
algorithms by adding a pre-processing step in which edges are weighted
according to their centrality w.r.t. the network topology. In our approach, the
centrality of an edge reflects its contribute to making arbitrary graph
tranversals, i.e., spreading messages over the network, as short as possible.
Our strategy is able to effectively complements information about network
topology and it can be used as an additional tool to enhance community
detection. The computation of edge centralities is carried out by performing
multiple random walks of bounded length on the network. Our method makes the
computation of edge centralities feasible also on large-scale networks. It has
been tested in conjunction with three state-of-the-art community detection
algorithms, namely the Louvain method, COPRA and OSLOM. Experimental results
show that our method raises the accuracy of existing algorithms both on
synthetic and real-life datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1747</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1747</id><created>2013-03-07</created><authors><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Ricciardello</keyname><forenames>Angela</forenames></author></authors><title>A Novel Measure of Edge Centrality in Social Networks</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>28 pages, 5 figures</comments><journal-ref>Knowledge-based Systems, 30:136-150, 2012</journal-ref><doi>10.1016/j.knosys.2012.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of assigning centrality values to nodes and edges in graphs has
been widely investigated during last years. Recently, a novel measure of node
centrality has been proposed, called k-path centrality index, which is based on
the propagation of messages inside a network along paths consisting of at most
k edges. On the other hand, the importance of computing the centrality of edges
has been put into evidence since 1970's by Anthonisse and, subsequently by
Girvan and Newman. In this work we propose the generalization of the concept of
k-path centrality by defining the k-path edge centrality, a measure of
centrality introduced to compute the importance of edges. We provide an
efficient algorithm, running in O(k m), being m the number of edges in the
graph. Thus, our technique is feasible for large scale network analysis.
Finally, the performance of our algorithm is analyzed, discussing the results
obtained against large online social network datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1748</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1748</id><created>2013-03-07</created><authors><author><keyname>Fiori</keyname><forenames>Simone</forenames></author><author><keyname>Kaneko</keyname><forenames>Tetsuya</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshihisa</forenames></author></authors><title>Mixed Maps for Kolmogoroff-Nagumo-Type Averaging on the Compact Stiefel
  Manifold</title><categories>math.NA cs.OH math.DG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present research work proposes a new fast fixed-point averaging algorithm
on the compact Stiefel manifold based on a mixed retraction/lifting pair.
Numerical comparisons between fixed-point algorithms based on the proposed
non-associated retraction/lifting map pair and two associated
retraction/lifting pairs confirm that the averaging algorithm based on a
combination of mixed maps is remarkably less computationally demanding than the
same averaging algorithm based on any of the constituent associated
retraction/lifting pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1749</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1749</id><created>2013-03-07</created><updated>2013-10-08</updated><authors><author><keyname>Olsson</keyname><forenames>Carl</forenames></author><author><keyname>Ulen</keyname><forenames>Johannes</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author></authors><title>Simplifying Energy Optimization using Partial Enumeration</title><categories>cs.CV</categories><comments>13 pages, 16 figures. &quot;Partial Enumeration and Curvature
  Regularization&quot; In International Conference on Computer Vision (ICCV), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energies with high-order non-submodular interactions have been shown to be
very useful in vision due to their high modeling power. Optimization of such
energies, however, is generally NP-hard. A naive approach that works for small
problem instances is exhaustive search, that is, enumeration of all possible
labelings of the underlying graph. We propose a general minimization approach
for large graphs based on enumeration of labelings of certain small patches.
This partial enumeration technique reduces complex high-order energy
formulations to pairwise Constraint Satisfaction Problems with unary costs
(uCSP), which can be efficiently solved using standard methods like TRW-S. Our
approach outperforms a number of existing state-of-the-art algorithms on well
known difficult problems (e.g. curvature regularization, stereo,
deconvolution); it gives near global minimum and better speed.
  Our main application of interest is curvature regularization. In the context
of segmentation, our partial enumeration technique allows to evaluate curvature
directly on small patches using a novel integral geometry approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1751</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1751</id><created>2013-03-07</created><authors><author><keyname>Imtiaz</keyname><forenames>Waqas Ahmed</forenames></author><author><keyname>Shil</keyname><forenames>Shimul</forenames></author><author><keyname>Rahamn</keyname><forenames>A. K. M Mahfuzur</forenames></author></authors><title>Three Layer Hierarchical Model for Chord</title><categories>cs.NI</categories><comments>6 Pages, Journal</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 11, 2012, Page 96-101</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing popularity of decentralized P2P architecture emphasizes on the
need to come across an overlay structure that can provide efficient content
discovery mechanism, accommodate high churn rate and adapt to failures.
Traditional p2p systems are not able to solve the problems relating scalability
and high churn rates. Hierarchical model were introduced to provide better
fault isolation, effective bandwidth utilization, a superior adaptation to the
underlying physical network and a reduction of the lookup path length as
additional advantages. It is more efficient and easier to manage than
traditional p2p networks. This paper discusses a further step in p2p hierarchy
via 3-layers hierarchical model with distributed database architecture in
different layer, each of which is connected through its root. The peers are
divided into three categories according to their physical stability and
strength. They are Ultra Super-peer, Super-peer and Ordinary Peer and we assign
these peers to first, second and third level of hierarchy respectively. Peers
in a group in lower layer have their own local database which hold as
associated super-peer in middle layer and access the database among the peers
through user queries. In our 3-layer hierarchical model for DHT algorithms, we
used an advanced Chord algorithm with optimized finger table which can remove
the redundant entry in the finger table in upper layer that influences the
system to reduce the lookup latency. Our research work finally resulted that
our model really provides faster search since the network lookup latency is
decreased by reducing the number of hops. The peers in such network then can
contribute with improve functionality and can perform well in P2P networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1754</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1754</id><created>2013-03-07</created><updated>2013-10-28</updated><authors><author><keyname>Luce</keyname><forenames>Robert</forenames></author><author><keyname>Ng</keyname><forenames>Esmond</forenames></author></authors><title>On the minimum FLOPs problem in the sparse Cholesky factorization</title><categories>math.NA cs.CC</categories><comments>Fix various spelling errors, move auxiliary proof to appendix, add
  two figures</comments><msc-class>65F50, 65F05, 68Q17</msc-class><journal-ref>SIAM. J. Matrix Anal. &amp; Appl. 35-1 (2014), pp. 1-21</journal-ref><doi>10.1137/130912438</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior to computing the Cholesky factorization of a sparse, symmetric positive
definite matrix, a reordering of the rows and columns is computed so as to
reduce both the number of fill elements in Cholesky factor and the number of
arithmetic operations (FLOPs) in the numerical factorization. These two metrics
are clearly somehow related and yet it is suspected that these two problems are
different. However, no rigorous theoretical treatment of the relation of these
two problems seems to have been given yet. In this paper we show by means of an
explicit, scalable construction that the two problems are different in a very
strict sense. In our construction no ordering, that is optimal for the fill, is
optimal with respect to the number of FLOPs, and vice versa.
  Further, it is commonly believed that minimizing the number of FLOPs is no
easier than minimizing the fill (in the complexity sense), but so far no proof
appears to be known. We give a reduction chain that shows the NP hardness of
minimizing the number of arithmetic operations in the Cholesky factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1760</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1760</id><created>2013-03-07</created><updated>2013-06-25</updated><authors><author><keyname>Hsu</keyname><forenames>Kung-Chuan</forenames></author><author><keyname>Brun</keyname><forenames>Todd A.</forenames></author></authors><title>Family of Finite Geometry Low-Density Parity-Check Codes for Quantum Key
  Expansion</title><categories>quant-ph cs.IT math.IT</categories><comments>9 pages, 7 figures, 3 tables, contents added in Sec. IV, minor
  changes</comments><journal-ref>Phys. Rev. A 87, 062332 (2013)</journal-ref><doi>10.1103/PhysRevA.87.062332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a quantum key expansion (QKE) protocol based on
entanglement-assisted quantum error-correcting codes (EAQECCs). In these
protocols, a seed of a previously shared secret key is used in the
post-processing stage of a standard quantum key distribution protocol like the
Bennett-Brassard 1984 protocol, in order to produce a larger secret key. This
protocol was proposed by Luo and Devetak, but codes leading to good performance
have not been investigated. We look into a family of EAQECCs generated by
classical finite geometry (FG) low-density parity-check (LDPC) codes, for which
very efficient iterative decoders exist. A critical observation is that almost
all errors in the resulting secret key result from uncorrectable block errors
that can be detected by an additional syndrome check and an additional sampling
step. Bad blocks can then be discarded. We make some changes to the original
protocol to avoid the consumption of the preshared key when the protocol fails.
This allows us to greatly reduce the bit error rate of the key at the cost of a
minor reduction in the key production rate, but without increasing the
consumption rate of the preshared key. We present numerical simulations for the
family of FG LDPC codes, and show that this improved QKE protocol has a good
net key production rate even at relatively high error rates, for appropriate
choices of these codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1761</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1761</id><created>2013-03-07</created><authors><author><keyname>Bhargava</keyname><forenames>Mayank</forenames></author><author><keyname>Polzehl</keyname><forenames>Tim</forenames></author></authors><title>Improving Automatic Emotion Recognition from speech using Rhythm and
  Temporal feature</title><categories>cs.CV</categories><comments>Appeared in ICECIT-2012, Srinivasa Ramanujan Institute of Technology,
  Anantapur</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to improve automatic emotion recognition from speech by
incorporating rhythm and temporal features. Research on automatic emotion
recognition so far has mostly been based on applying features like MFCCs, pitch
and energy or intensity. The idea focuses on borrowing rhythm features from
linguistic and phonetic analysis and applying them to the speech signal on the
basis of acoustic knowledge only. In addition to this we exploit a set of
temporal and loudness features. A segmentation unit is employed in starting to
separate the voiced/unvoiced and silence parts and features are explored on
different segments. Thereafter different classifiers are used for
classification. After selecting the top features using an IGR filter we are
able to achieve a recognition rate of 80.60 % on the Berlin Emotion Database
for the speaker dependent framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1763</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1763</id><created>2013-03-07</created><updated>2015-05-26</updated><authors><author><keyname>Cain</keyname><forenames>Alan J.</forenames></author><author><keyname>Pfeiffer</keyname><forenames>Markus</forenames></author></authors><title>Decision problems for word-hyperbolic semigroups</title><categories>math.GR cs.FL</categories><comments>33 pages; substantially revised to include the undecidability of the
  isomorphism problem and the problem of determining automaticity</comments><msc-class>20M05 (Primary), 20M35, 68Q45 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies decision problems for semigroups that are word-hyperbolic
in the sense of Duncan &amp; Gilman. A fundamental investigation reveals that the
natural definition of a `word-hyperbolic structure' has to be strengthened
slightly in order to define a unique semigroup up to isomorphism. The
isomorphism problem is proven to be undecidable for word-hyperbolic semigroups
(in contrast to the situation for word-hyperbolic groups). It is proved that it
is undecidable whether a word-hyperbolic semigroup is automatic, asynchronously
automatic, biautomatic, or asynchronously biautomatic. (These properties do not
hold in general for word-hyperbolic semigroups.) It is proved that the uniform
word problem for word-hyperbolic semigroup is solvable in polynomial time
(improving on the previous exponential-time algorithm). Algorithms are
presented for deciding whether a word-hyperbolic semigroup is a monoid, a
group, a completely simple semigroup, a Clifford semigroup, or a free
semigroup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1777</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1777</id><created>2013-02-08</created><authors><author><keyname>Darkhovsky</keyname><forenames>Boris</forenames></author><author><keyname>Pyriatinska</keyname><forenames>Alexandra</forenames></author></authors><title>Epsilon-complexity of continuous functions</title><categories>math-ph cs.IT math.IT math.MP physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A formal definition of epsilon-complexity of an individual continuous
function defined on a unit cube is proposed. This definition is consistent with
the Kolmogorov's idea of the complexity of an object. A definition of
epsilon-complexity for a class of continuous functions with a given modulus of
continuity is also proposed. Additionally, an explicit formula for the
epsilon-complexity of a functional class is obtained. As a consequence, the
paper finds that the epsilon-complexity for the Holder class of functions can
be characterized by a pair of real numbers. Based on these results the papers
formulates a conjecture concerning the epsilon-complexity of an individual
function from the Holder class. We also propose a conjecture about
characterization of epsilon-complexity of a function from the Holder class
given on a discrete grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1778</identifier>
 <datestamp>2013-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1778</id><created>2013-03-07</created><authors><author><keyname>Parruca</keyname><forenames>Donald</forenames></author><author><keyname>Grysla</keyname><forenames>Marius</forenames></author><author><keyname>G&#xf6;rtzen</keyname><forenames>Simon</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author></authors><title>Analytical Model of Proportional Fair Scheduling in Interference-limited
  OFDMA/LTE Networks</title><categories>cs.NI</categories><comments>7 pages, 6 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various system tasks like interference coordination, handover decisions,
admission control etc. in upcoming cellular networks require precise mid-term
(spanning over a few seconds) performance models. Due to channel-dependent
scheduling at the base station, these performance models are not simple to
obtain. Furthermore, upcoming cellular systems will be interference-limited,
hence, the way interference is modeled is crucial for the accuracy. In this
paper we present an analytical model for the SINR distribution of the
\textit{scheduled} subcarriers of an OFDMA system with proportional fair
scheduling. The model takes the precise SINR distribution into account. We
furthermore refine our model with respect to uniform modulation and coding, as
applied in LTE networks. The derived models are validated by means of
simulations. In additon, we show that our models are approximate estimators for
the performance of rate-based proportional fair scheduling, while they
outperform some simpler prediction models from related work significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1786</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1786</id><created>2013-03-07</created><updated>2013-04-19</updated><authors><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Slivovsky</keyname><forenames>Friedrich</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Meta-Kernelization with Structural Parameters</title><categories>cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meta-kernelization theorems are general results that provide polynomial
kernels for large classes of parameterized problems. The known
meta-kernelization theorems, in particular the results of Bodlaender et al.
(FOCS'09) and of Fomin et al. (FOCS'10), apply to optimization problems
parameterized by solution size. We present the first meta-kernelization
theorems that use a structural parameters of the input and not the solution
size. Let C be a graph class. We define the C-cover number of a graph to be a
the smallest number of modules the vertex set can be partitioned into, such
that each module induces a subgraph that belongs to the class C. We show that
each graph problem that can be expressed in Monadic Second Order (MSO) logic
has a polynomial kernel with a linear number of vertices when parameterized by
the C-cover number for any fixed class C of bounded rank-width (or
equivalently, of bounded clique-width, or bounded Boolean width). Many graph
problems such as Independent Dominating Set, c-Coloring, and c-Domatic Number
are covered by this meta-kernelization result. Our second result applies to MSO
expressible optimization problems, such as Minimum Vertex Cover, Minimum
Dominating Set, and Maximum Clique. We show that these problems admit a
polynomial annotated kernel with a linear number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1795</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1795</id><created>2013-03-07</created><updated>2014-01-24</updated><authors><author><keyname>Ahmed</keyname><forenames>Elsayed</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Rate Gain Region and Design Tradeoffs for Full-Duplex Wireless
  Communications</title><categories>cs.IT math.IT</categories><comments>Accepted on 09-May-2013 for publications at IEEE Transactions on
  Wireless Communications (check the IEEE website for the final published
  version)</comments><journal-ref>Wireless Communications, IEEE Transactions on , vol.12, no.7,
  pp.3556,3565, July 2013</journal-ref><doi>10.1109/TWC.2013.060413.121871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analytically study the regime in which practical
full-duplex systems can achieve larger rates than an equivalent half-duplex
systems. The key challenge in practical full-duplex systems is uncancelled
self-interference signal, which is caused by a combination of hardware and
implementation imperfections. Thus, we first present a signal model which
captures the effect of significant impairments such as oscillator phase noise,
low-noise amplifier noise figure, mixer noise, and analog-to-digital converter
quantization noise. Using the detailed signal model, we study the rate gain
region, which is defined as the region of received signal-of-interest strength
where full-duplex systems outperform half-duplex systems in terms of achievable
rate. The rate gain region is derived as a piece-wise linear approximation in
log-domain, and numerical results show that the approximation closely matches
the exact region. Our analysis shows that when phase noise dominates mixer and
quantization noise, full-duplex systems can use either active analog
cancellation or base-band digital cancellation to achieve near-identical rate
gain regions. Finally, as a design example, we numerically investigate the
full-duplex system performance and rate gain region in typical indoor
environments for practical wireless applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1827</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1827</id><created>2013-03-07</created><authors><author><keyname>Catanese</keyname><forenames>Salvatore</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author></authors><title>Forensic Analysis of Phone Call Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>18 pages, 10 figures</comments><journal-ref>Social Network Analysis and Mining, 3(1):15-33, 2013</journal-ref><doi>10.1007/s13278-012-0060-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of preventing and fighting crime, the analysis of mobile phone
traffic, among actors of a criminal network, is helpful in order to reconstruct
illegal activities on the base of the relationships connecting those specific
individuals. Thus, forensic analysts and investigators require new advanced
tools and techniques which allow them to manage these data in a meaningful and
efficient way. In this paper we present LogAnalysis, a tool we developed to
provide visual data representation and filtering, statistical analysis features
and the possibility of a temporal analysis of mobile phone activities. Its
adoption may help in unveiling the structure of a criminal network and the
roles and dynamics of communications among its components. By using
LogAnalysis, forensic investigators could deeply understand hierarchies within
criminal organizations, for example discovering central members that provide
connections among different sub-groups, etc. Moreover, by analyzing the
temporal evolution of the contacts among individuals, or by focusing on
specific time windows they could acquire additional insights on the data they
are analyzing. Finally, we put into evidence how the adoption of LogAnalysis
may be crucial to solve real cases, providing as example a number of case
studies inspired by real forensic investigations led by one of the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1829</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1829</id><created>2013-03-07</created><authors><author><keyname>Meyer</keyname><forenames>Fernand</forenames></author></authors><title>Watersheds on edge or node weighted graphs &quot;par l'exemple&quot;</title><categories>cs.CV</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Watersheds have been defined both for node and edge weighted graphs. We show
that they are identical: for each edge (resp.\ node) weighted graph exists a
node (resp. edge) weighted graph with the same minima and catchment basin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1840</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1840</id><created>2013-03-07</created><authors><author><keyname>Lindzey</keyname><forenames>Nathan</forenames></author><author><keyname>McConnell</keyname><forenames>Ross</forenames></author></authors><title>On Finding Lekkerkerker-Boland Subgraphs</title><categories>cs.DM cs.DS math.CO</categories><comments>Submitted to WG 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lekkerkerker and Boland characterized the minimal forbidden induced subgraphs
for the class of interval graphs. We give a linear-time algorithm to find one
in any graph that is not an interval graph. Tucker characterized the minimal
forbidden submatrices of matrices that do not have the consecutive-ones
property. We give a linear-time algorithm to find one in any matrix that does
not have the consecutive-ones property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1847</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1847</id><created>2013-03-07</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Wang</keyname><forenames>Rongrong</forenames></author></authors><title>Random Subdictionaries and Coherence Conditions for Sparse Signal
  Recovery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most frequently used condition for sampling matrices employed in
compressive sampling is the restricted isometry (RIP) property of the matrix
when restricted to sparse signals. At the same time, imposing this condition
makes it difficult to find explicit matrices that support recovery of signals
from sketches of the optimal (smallest possible)dimension. A number of attempts
have been made to relax or replace the RIP property in sparse recovery
algorithms. We focus on the relaxation under which the near-isometry property
holds for most rather than for all submatrices of the sampling matrix, known as
statistical RIP or StRIP condition. We show that sampling matrices of
dimensions $m\times N$ with maximum coherence $\mu=O((k\log^3 N)^{-1/4})$ and
mean square coherence $\bar \mu^2=O(1/(k\log N))$ support stable recovery of
$k$-sparse signals using Basis Pursuit. These assumptions are satisfied in many
examples. As a result, we are able to construct sampling matrices that support
recovery with low error for sparsity $k$ higher than $\sqrt m,$ which exceeds
the range of parameters of the known classes of RIP matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1848</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1848</id><created>2013-03-07</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Blind Adaptive Beamforming Based on Constrained Constant Modulus RLS
  Algorithm for Smart Antennas</title><categories>cs.IT math.IT</categories><comments>3 figures</comments><journal-ref>ISWCS 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of blind adaptive beamforming
algorithms for smart antennas in realistic environments. A constrained constant
modulus (CCM) design criterion is described and used for deriving a recursive
least squares (RLS) type optimization algorithm. Furthermore, two kinds of
scenarios are considered in the paper for analyzing its performance.
Simulations are performed to compare the performance of the proposed method to
other well-known methods for blind adaptive beamforming. Results indicate that
the proposed method has a significant faster convergence rate, better
robustness to changeable environments and better tracking capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1849</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1849</id><created>2013-03-07</created><updated>2013-06-03</updated><authors><author><keyname>Gittens</keyname><forenames>Alex</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Revisiting the Nystrom Method for Improved Large-Scale Machine Learning</title><categories>cs.LG cs.DS cs.NA</categories><comments>60 pages, 15 color figures; updated proof of Frobenius norm bounds,
  added comparison to projection-based low-rank approximations, and an analysis
  of the power method applied to SPSD sketches</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reconsider randomized algorithms for the low-rank approximation of
symmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel
matrices that arise in data analysis and machine learning applications. Our
main results consist of an empirical evaluation of the performance quality and
running time of sampling and projection methods on a diverse suite of SPSD
matrices. Our results highlight complementary aspects of sampling versus
projection methods; they characterize the effects of common data preprocessing
steps on the performance of these algorithms; and they point to important
differences between uniform sampling and nonuniform sampling methods based on
leverage scores. In addition, our empirical results illustrate that existing
theory is so weak that it does not provide even a qualitative guide to
practice. Thus, we complement our empirical results with a suite of worst-case
theoretical bounds for both random sampling and random projection methods.
These bounds are qualitatively superior to existing bounds---e.g. improved
additive-error bounds for spectral and Frobenius norm error and relative-error
bounds for trace norm error---and they point to future directions to make these
algorithms useful in even larger-scale machine learning applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1858</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1858</id><created>2013-03-07</created><authors><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>On the Minimum Distance of Generalized Spatially Coupled LDPC Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE International Symposium on Information Theory
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Families of generalized spatially-coupled low-density parity-check (GSC-LDPC)
code ensembles can be formed by terminating protograph-based generalized LDPC
convolutional (GLDPCC) codes. It has previously been shown that ensembles of
GSC-LDPC codes constructed from a protograph have better iterative decoding
thresholds than their block code counterparts, and that, for large termination
lengths, their thresholds coincide with the maximum a-posteriori (MAP) decoding
threshold of the underlying generalized LDPC block code ensemble. Here we show
that, in addition to their excellent iterative decoding thresholds, ensembles
of GSC-LDPC codes are asymptotically good and have large minimum distance
growth rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1868</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1868</id><created>2013-03-07</created><authors><author><keyname>Arif</keyname><forenames>Chusnul</forenames></author><author><keyname>Mizoguchi</keyname><forenames>Masaru</forenames></author><author><keyname>Setiawan</keyname><forenames>Budi Indra</forenames></author><author><keyname>Doi</keyname><forenames>Ryoichi</forenames></author></authors><title>Estimation of soil moisture in paddy field using Artificial Neural
  Networks</title><categories>cs.NE physics.ao-ph</categories><comments>5 Pages</comments><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence (IJARAI), Vol. 1, No. 1:17-21, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In paddy field, monitoring soil moisture is required for irrigation
scheduling and water resource allocation, management and planning. The current
study proposes an Artificial Neural Networks (ANN) model to estimate soil
moisture in paddy field with limited meteorological data. Dynamic of ANN model
was adopted to estimate soil moisture with the inputs of reference
evapotranspiration (ETo) and precipitation. ETo was firstly estimated using the
maximum, average and minimum values of air temperature as the inputs of model.
The models were performed under different weather conditions between the two
paddy cultivation periods. Training process of model was carried out using the
observation data in the first period, while validation process was conducted
based on the observation data in the second period. Dynamic of ANN model
estimated soil moisture with R2 values of 0.80 and 0.73 for training and
validation processes, respectively, indicated that tight linear correlations
between observed and estimated values of soil moisture were observed. Thus, the
ANN model reliably estimates soil moisture with limited meteorological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1869</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1869</id><created>2013-03-07</created><authors><author><keyname>Ahmad</keyname><forenames>Usman</forenames></author><author><keyname>Subrata</keyname><forenames>Dewa Made</forenames></author><author><keyname>Arif</keyname><forenames>Chusnul</forenames></author></authors><title>Speaking Plant Approach for Automatic Fertigation System in Greenhouse</title><categories>cs.OH</categories><comments>12 Pages</comments><journal-ref>International Journal of Signal Processing, Image Processing and
  Pattern Recognition Vol. 4, No. 3, September, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, many vegetables are grown insidegreenhouses in which environment is
controlled and nutrition can be supplied through water supply using electrical
pump, namely fertigation. Dosage of nutrition in water for many vegetable
plants are also known so that by controllingwater supply all the needsfor the
plants to grow are available. Furthermore, water supply can be controlled using
electrical pump which is activated according to theplants conditionin relation
with water supply. In order to supply water and nutrition in the right amount
and time, plants condition can be observed using a CCD camera attached to image
processing facilitiesto develop a speaking plant approach. In this study,
plants development during their growing periodare observedusing image
processing. Three populationsof tomato plants, with less, enough, and exceeded
nutrition in water,are captured using a CCD camera every three days, and the
images were analyzed using a developed computer program for the heightof
plants. The results showed that the development of the plants can be monitored
using this method. After that, the responseof plant growth in the same
condition was monitored, and the responsewas used as input for the fertigation
system to turn electrical pump automatically on and off, so the fertigation
system could maintain the growth of the plants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1870</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1870</id><created>2013-03-07</created><authors><author><keyname>Batoul</keyname><forenames>Aicha</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>On Isodual Cyclic Codes over Finite Fields and Finite Chain Rings:
  Monomial Equivalence</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper present the construction cyclic isodual codes over finite fields
and finite chain rings. These codes are monomially equivalent to their dual.
Conditions are given for the existence of cyclic isodual codes. In addition,
the concept of duadic codes over finite fields is extended to finite chain
rings. Several constructions of isodual cyclic codes and self-dual codes are
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1872</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1872</id><created>2013-03-07</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author></authors><title>An Efficient Dynamic Programming Algorithm for the Generalized LCS
  Problem with Multiple Substring Exclusion Constrains</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1301.7183</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a generalized longest common subsequence problem
with multiple substring exclusion constrains. For the two input sequences $X$
and $Y$ of lengths $n$ and $m$, and a set of $d$ constrains $P=\{P_1,...,P_d\}$
of total length $r$, the problem is to find a common subsequence $Z$ of $X$ and
$Y$ excluding each of constrain string in $P$ as a substring and the length of
$Z$ is maximized. The problem was declared to be NP-hard\cite{1}, but we
finally found that this is not true. A new dynamic programming solution for
this problem is presented in this paper. The correctness of the new algorithm
is proved. The time complexity of our algorithm is $O(nmr)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1877</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1877</id><created>2013-03-07</created><updated>2015-04-25</updated><authors><author><keyname>Qi</keyname><forenames>Feng</forenames></author><author><keyname>Li</keyname><forenames>Wen-Hui</forenames></author></authors><title>A logarithmically completely monotonic function involving the ratio of
  gamma functions</title><categories>math.CA cs.IT math.IT</categories><comments>11 pages</comments><msc-class>26A48, 33B15, 65R10</msc-class><journal-ref>Journal of Applied Analysis and Computation 5 (2015), no. 4,
  626--634</journal-ref><doi>10.11948/2015049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, the authors concisely survey and review some functions
involving the gamma function and its various ratios, simply state their
logarithmically complete monotonicity and related results, and find necessary
and sufficient conditions for a new function involving the ratio of two gamma
functions and originating from the coding gain to be logarithmically completely
monotonic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1880</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1880</id><created>2013-03-07</created><updated>2014-09-16</updated><authors><author><keyname>Saleena</keyname><forenames>Nabizath</forenames></author><author><keyname>Paleri</keyname><forenames>Vineeth</forenames></author></authors><title>A Simple Algorithm for Global Value Numbering</title><categories>cs.PL</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global Value Numbering(GVN) is a method for detecting redundant computations
in programs. Here, we introduce the problem of Global Value Numbering in its
original form, as conceived by Kildall(1973), and present an algorithm which is
a simpler variant of Kildall's. The algorithm uses the concept of value
expression - an abstraction of a set of expressions - enabling a representation
of the equivalence information which is compact and simple to manipulate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1882</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1882</id><created>2013-03-07</created><authors><author><keyname>AlShourbaji</keyname><forenames>Ibrahim</forenames></author></authors><title>An Overview of Wireless Local Area Network (WLAN)</title><categories>cs.NI cs.CR</categories><comments>8 pages, International Journal of Computer Science and Information
  Security(IJCSIS), Vol. 11, No. 2, February 2013. arXiv admin note: text
  overlap with arXiv:0710.4818 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless Communication is an application of science and technology that has
come to be vital for modern existence. From the early radio and telephone to
current devices such as mobile phones and laptops, accessing the global network
has become the most essential and indispensable part of our lifestyle. Wireless
communication is an ever developing field, and the future holds many
possibilities in this area. One expectation for the future in this field is
that, the devices can be developed to support communication with higher data
rates and more security. Research in this area suggests that a dominant means
of supporting such communication capabilities will be through the use of
Wireless LANs. As the deployment of Wireless LAN increases well around the
globe, it is increasingly important for us to understand different technologies
and to select the most appropriate one.
  This paper provides a detailed study of the available wireless LAN
technologies and the concerned issues, will give a brief description of what
wireless LANs are, the need of Wireless LAN, History of wireless LAN,
advantages of Wireless Networks, with summarizing the related work on WLAN in
academic area, Wireless LAN technologies, some risks attacks against wireless
technologies, suggesting some recommendations to protect wireless LAN network
from attack, Finally we propose some research issues should be focused on in
the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1890</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1890</id><created>2013-03-08</created><updated>2013-03-27</updated><authors><author><keyname>Harper</keyname><forenames>Marc</forenames></author></authors><title>The Inherent Randomness of Evolving Populations</title><categories>math.DS cs.IT math.IT q-bio.PE</categories><msc-class>91A22, 94A15</msc-class><journal-ref>Phys. Rev. E 89, 032709 (2014)</journal-ref><doi>10.1103/PhysRevE.89.032709</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy rates of the Wright-Fisher process, the Moran process, and
generalizations are computed and used to compare these processes and their
dependence on standard evolutionary parameters. Entropy rates are measures of
the variation dependent on both short-run and long-run behavior, and allow the
relationships between mutation, selection, and population size to be examined.
Bounds for the entropy rate are given for the Moran process (independent of
population size) and for the Wright-Fisher process (bounded for fixed
population size). A generational Moran process is also presented for comparison
to the Wright-Fisher Process. Results include analytic results and
computational extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1904</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1904</id><created>2013-03-08</created><authors><author><keyname>Coquel</keyname><forenames>Anne-Sophie</forenames><affiliation>Insa Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes / UCBL, LIRIS</affiliation></author><author><keyname>Jacob</keyname><forenames>Jean-Pascal</forenames><affiliation>MAP5</affiliation></author><author><keyname>Primet</keyname><forenames>Ma&#xeb;l</forenames><affiliation>MAP5</affiliation></author><author><keyname>Demarez</keyname><forenames>Alice</forenames><affiliation>MAP5</affiliation></author><author><keyname>Dimiccoli</keyname><forenames>Mariella</forenames><affiliation>MAP5</affiliation></author><author><keyname>Julou</keyname><forenames>Thomas</forenames><affiliation>LPS</affiliation></author><author><keyname>Moisan</keyname><forenames>Lionel</forenames><affiliation>MAP5</affiliation></author><author><keyname>Lindner</keyname><forenames>Ariel B.</forenames><affiliation>Insa Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes / UCBL</affiliation></author><author><keyname>Berry</keyname><forenames>Hugues</forenames><affiliation>Insa Lyon / INRIA Grenoble Rh&#xf4;ne-Alpes / UCBL</affiliation></author></authors><title>Localization of protein aggregation in Escherichia coli is governed by
  diffusion and nucleoid macromolecular crowding effect</title><categories>q-bio.CB cs.CE</categories><comments>PLoS Computational Biology (2013)</comments><proxy>ccsd</proxy><doi>10.1371/journal.pcbi.1003038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggregates of misfolded proteins are a hallmark of many age-related diseases.
Recently, they have been linked to aging of Escherichia coli (E. coli) where
protein aggregates accumulate at the old pole region of the aging bacterium.
Because of the potential of E. coli as a model organism, elucidating aging and
protein aggregation in this bacterium may pave the way to significant advances
in our global understanding of aging. A first obstacle along this path is to
decipher the mechanisms by which protein aggregates are targeted to specific
intercellular locations. Here, using an integrated approach based on
individual-based modeling, time-lapse fluorescence microscopy and automated
image analysis, we show that the movement of aging-related protein aggregates
in E. coli is purely diffusive (Brownian). Using single-particle tracking of
protein aggregates in live E. coli cells, we estimated the average size and
diffusion constant of the aggregates. Our results evidence that the aggregates
passively diffuse within the cell, with diffusion constants that depend on
their size in agreement with the Stokes-Einstein law. However, the aggregate
displacements along the cell long axis are confined to a region that roughly
corresponds to the nucleoid-free space in the cell pole, thus confirming the
importance of increased macromolecular crowding in the nucleoids. We thus used
3d individual-based modeling to show that these three ingredients (diffusion,
aggregation and diffusion hindrance in the nucleoids) are sufficient and
necessary to reproduce the available experimental data on aggregate
localization in the cells. Taken together, our results strongly support the
hypothesis that the localization of aging-related protein aggregates in the
poles of E. coli results from the coupling of passive diffusion- aggregation
with spatially non-homogeneous macromolecular crowding. They further support
the importance of &quot;soft&quot; intracellular structuring (based on macromolecular
crowding) in diffusion-based protein localization in E. coli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1911</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1911</id><created>2013-03-08</created><updated>2014-07-14</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Multiuser MISO Beamforming for Simultaneous Wireless Information and
  Power Transfer</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Signal Processing</comments><doi>10.1109/TSP.2014.2340817</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous wireless information and power transfer (SWIPT) is anticipated
to have abundant applications in future machine or sensor based wireless
networks by providing wireless data and energy access at the same time. In this
paper, we study a multiuser multiple-input single-output (MISO) broadcast SWIPT
system, where a multi-antenna access point (AP) sends information and energy
simultaneously via spatial multiplexing to multiple single-antenna receivers
each of which implements information decoding (ID) or energy harvesting (EH).
Since EH receivers in practice operate with considerably higher received power
than ID receivers, we propose a receiver location based transmission
scheduling, where receivers that are close to the AP are scheduled for EH while
those more distant from the AP for ID. We aim to maximize the weighted
sum-power transferred to all EH receivers subject to a given set of minimum
signal-to-interference-and-noise ratio (SINR) constraints at different ID
receivers. In particular, we consider two types of ID receivers (referred to as
Type I and Type II, respectively) without or with the capability of cancelling
the interference from (a priori known) energy signals. For each type of ID
receivers, we formulate the joint information and energy transmit beamforming
design as a non-convex quadratically constrained quadratic program (QCQP).
First, we obtain the globally optimal solutions for our formulated QCQPs by
applying an optimization technique so-called semidefinite relaxation (SDR). It
is shown via SDR that no dedicated energy beam is needed for Type I ID
receivers to achieve the optimal solution; while for Type II ID receivers,
employing no more than one energy beam is optimal. Next, we establish a new
form of the celebrated uplink-downlink duality to develop alternative
algorithms to obtain the same optimal solutions as SDR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1912</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1912</id><created>2013-03-08</created><authors><author><keyname>Megow</keyname><forenames>Nicole</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>Competitive-Ratio Approximation Schemes for Minimizing the Makespan in
  the Online-List Model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider online scheduling on multiple machines for jobs arriving
one-by-one with the objective of minimizing the makespan. For any number of
identical parallel or uniformly related machines, we provide a
competitive-ratio approximation scheme that computes an online algorithm whose
competitive ratio is arbitrarily close to the best possible competitive ratio.
We also determine this value up to any desired accuracy. This is the first
application of competitive-ratio approximation schemes in the online-list
model. The result proves the applicability of the concept in different online
models. We expect that it fosters further research on other online problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1913</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1913</id><created>2013-03-08</created><authors><author><keyname>Balaji</keyname><forenames>S. Arun</forenames></author><author><keyname>Baskaran</keyname><forenames>K.</forenames></author></authors><title>Design and Development of Artificial Neural Networking (ANN) system
  using sigmoid activation function to predict annual rice production in
  Tamilnadu</title><categories>cs.NE</categories><comments>19 pages, 7 figures, published in the International Journal of
  Computer Science, Engineering and Information Technology (IJCSEIT), Vol.3,
  No.1, February 2013</comments><report-no>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.3, No.1, February 2013</report-no><msc-class>14J60 (Primary) 14F05, 14J26 (Secondary)</msc-class><acm-class>F.2.2; I.2.7</acm-class><journal-ref>IJCSEIT, Vol.3, No.1, February 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction of annual rice production in all the 31 districts of Tamilnadu is
an important decision for the Government of Tamilnadu. Rice production is a
complex process and non linear problem involving soil, crop, weather, pest,
disease, capital, labour and management parameters. ANN software was designed
and developed with Feed Forward Back Propagation (FFBP) network to predict rice
production. The input layer has six independent variables like area of
cultivation and rice production in three seasons like Kuruvai, Samba and Kodai.
The popular sigmoid activation function was adopted to convert input data into
sigmoid values. The hidden layer computes the summation of six sigmoid values
with six sets of weightages. The final output was converted into sigmoid values
using a sigmoid transfer function. ANN outputs are the predicted results. The
error between original data and ANN output values were computed. A threshold
value of 10-9 was used to test whether the error is greater than the threshold
level. If the error is greater than threshold then updating of weights was done
all summations were done by back propagation. This process was repeated until
error equal to zero. The predicted results were printed and it was found to be
exactly matching with the expected values. It shows that the ANN prediction was
100% accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1915</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1915</id><created>2013-03-08</created><authors><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author></authors><title>Spatially Selective Artificial-Noise Aided Transmit Optimization for
  MISO Multi-Eves Secrecy Rate Maximization</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Signal Process., 2013</comments><doi>10.1109/TSP.2013.2253771</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an MISO channel overheard by multiple eavesdroppers. Our goal is to
design an artificial noise (AN)-aided transmit strategy, such that the
achievable secrecy rate is maximized subject to the sum power constraint.
AN-aided secure transmission has recently been found to be a promising approach
for blocking eavesdropping attempts. In many existing studies, the confidential
information transmit covariance and the AN covariance are not simultaneously
optimized. In particular, for design convenience, it is common to prefix the AN
covariance as a specific kind of spatially isotropic covariance. This paper
considers joint optimization of the transmit and AN covariances for secrecy
rate maximization (SRM), with a design flexibility that the AN can take any
spatial pattern. Hence, the proposed design has potential in jamming the
eavesdroppers more effectively, based upon the channel state information (CSI).
We derive an optimization approach to the SRM problem through both analysis and
convex conic optimization machinery. We show that the SRM problem can be recast
as a single-variable optimization problem, and that resultant problem can be
efficiently handled by solving a sequence of semidefinite programs. Our
framework deals with a general setup of multiple multi-antenna eavesdroppers,
and can cater for additional constraints arising from specific application
scenarios, such as interference temperature constraints in interference
networks. We also generalize the framework to an imperfect CSI case where a
worst-case robust SRM formulation is considered. A suboptimal but safe solution
to the outage-constrained robust SRM design is also investigated. Simulation
results show that the proposed AN-aided SRM design yields significant secrecy
rate gains over an optimal no-AN design and the isotropic AN design, especially
when there are more eavesdroppers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1929</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1929</id><created>2013-03-08</created><authors><author><keyname>Padr&#xf3;</keyname><forenames>Muntsa</forenames></author><author><keyname>Bel</keyname><forenames>N&#xfa;ria</forenames></author><author><keyname>Necsulescu</keyname><forenames>Silvia</forenames></author></authors><title>Towards the Fully Automatic Merging of Lexical Resources: A Step Forward</title><categories>cs.CL</categories><comments>7 pages, 1 figure, 5 tables. Also available in UPF institutional
  repository (http://hdl.handle.net/10230/20417)</comments><journal-ref>LREC 2012 Workshop on Language Resource Merging; 2012 May 22;
  Istanbul, Turkey. Paris: European Language Resources Association; 2012. p.
  8-14</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This article reports on the results of the research done towards the fully
automatically merging of lexical resources. Our main goal is to show the
generality of the proposed approach, which have been previously applied to
merge Spanish Subcategorization Frames lexica. In this work we extend and apply
the same technique to perform the merging of morphosyntactic lexica encoded in
LMF. The experiments showed that the technique is general enough to obtain good
results in these two different tasks which is an important step towards
performing the merging of lexical resources fully automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1930</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1930</id><created>2013-03-08</created><authors><author><keyname>Bel</keyname><forenames>N&#xfa;ria</forenames></author><author><keyname>Romeo</keyname><forenames>Lauren</forenames></author><author><keyname>Padr&#xf3;</keyname><forenames>Muntsa</forenames></author></authors><title>Automatic lexical semantic classification of nouns</title><categories>cs.CL</categories><comments>8 pages, 8 tables. Also available in UPF institutional repository
  (http://hdl.handle.net/10230/20420)</comments><journal-ref>Proceedings of the Eight International Conference on Language
  Resources and Evaluation (LREC'12); 2012 May 23-25; Istanbul, Turkey. Paris:
  European Language Resources Association; 2012. p. 1448-1455</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The work we present here addresses cue-based noun classification in English
and Spanish. Its main objective is to automatically acquire lexical semantic
information by classifying nouns into previously known noun lexical classes.
This is achieved by using particular aspects of linguistic contexts as cues
that identify a specific lexical class. Here we concentrate on the task of
identifying such cues and the theoretical background that allows for an
assessment of the complexity of the task. The results show that, despite of the
a-priori complexity of the task, cue-based classification is a useful tool in
the automatic acquisition of lexical semantic classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1931</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1931</id><created>2013-03-08</created><authors><author><keyname>V&#xe1;zquez</keyname><forenames>Silvia</forenames></author><author><keyname>Bel</keyname><forenames>N&#xfa;ria</forenames></author></authors><title>A Classification of Adjectives for Polarity Lexicons Enhancement</title><categories>cs.CL</categories><comments>5 pages, 7 tables. Also available in UPF institutional repository
  (http://hdl.handle.net/10230/20419)</comments><journal-ref>Proceedings of the Eight International Conference on Language
  Resources and Evaluation (LREC'12); 2012 May 23-25; Istanbul, Turkey. Paris:
  European Language Resources Association; 2012. p. 3557-3561</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Subjective language detection is one of the most important challenges in
Sentiment Analysis. Because of the weight and frequency in opinionated texts,
adjectives are considered a key piece in the opinion extraction process. These
subjective units are more and more frequently collected in polarity lexicons in
which they appear annotated with their prior polarity. However, at the moment,
any polarity lexicon takes into account prior polarity variations across
domains. This paper proves that a majority of adjectives change their prior
polarity value depending on the domain. We propose a distinction between domain
dependent and domain independent adjectives. Moreover, our analysis led us to
propose a further classification related to subjectivity degree: constant,
mixed and highly subjective adjectives. Following this classification, polarity
values will be a better support for Sentiment Analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1932</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1932</id><created>2013-03-08</created><authors><author><keyname>Bel</keyname><forenames>N&#xfa;ria</forenames></author><author><keyname>Papavasiliou</keyname><forenames>Vassilis</forenames></author><author><keyname>Prokopidis</keyname><forenames>Prokopis</forenames></author><author><keyname>Toral</keyname><forenames>Antonio</forenames></author><author><keyname>Arranz</keyname><forenames>Victoria</forenames></author></authors><title>Mining and Exploiting Domain-Specific Corpora in the PANACEA Platform</title><categories>cs.CL</categories><comments>3 pages. Also available in UPF institutional repository
  (http://hdl.handle.net/10230/20416)</comments><journal-ref>Proceedings of the 5th Workshop on Building and Using Comparable
  Corpora at the Eighth International Conference on Language Resources and
  Evaluation (LREC-2012); 2012 May 23-25; Istanbul, Turkey. Paris: ELRA; 2012.
  p. 24-26</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The objective of the PANACEA ICT-2007.2.2 EU project is to build a platform
that automates the stages involved in the acquisition, production, updating and
maintenance of the large language resources required by, among others, MT
systems. The development of a Corpus Acquisition Component (CAC) for extracting
monolingual and bilingual data from the web is one of the most innovative
building blocks of PANACEA. The CAC, which is the first stage in the PANACEA
pipeline for building Language Resources, adopts an efficient and distributed
methodology to crawl for web documents with rich textual content in specific
languages and predefined domains. The CAC includes modules that can acquire
parallel data from sites with in-domain content available in more than one
language. In order to extrinsically evaluate the CAC methodology, we have
conducted several experiments that used crawled parallel corpora for the
identification and extraction of parallel sentences using sentence alignment.
The corpora were then successfully used for domain adaptation of Machine
Translation Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1942</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1942</id><created>2013-03-08</created><updated>2013-09-09</updated><authors><author><keyname>Svorenova</keyname><forenames>Maria</forenames></author><author><keyname>Cerna</keyname><forenames>Ivana</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Optimal Control of MDPs with Temporal Logic Constraints</title><categories>cs.LO</categories><comments>Technical report accompanying the CDC 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on formal synthesis of control policies for finite
Markov decision processes with non-negative real-valued costs. We develop an
algorithm to automatically generate a policy that guarantees the satisfaction
of a correctness specification expressed as a formula of Linear Temporal Logic,
while at the same time minimizing the expected average cost between two
consecutive satisfactions of a desired property. The existing solutions to this
problem are sub-optimal. By leveraging ideas from automata-based model checking
and game theory, we provide an optimal solution. We demonstrate the approach on
an illustrative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1948</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1948</id><created>2013-03-08</created><authors><author><keyname>Brooke</keyname><forenames>Phillip J.</forenames></author><author><keyname>Paige</keyname><forenames>Richard F.</forenames></author></authors><title>The Value of User-Visible Internet Cryptography</title><categories>cs.CR</categories><acm-class>K.4.4; D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptographic mechanisms are used in a wide range of applications, including
email clients, web browsers, document and asset management systems, where
typical users are not cryptography experts. A number of empirical studies have
demonstrated that explicit, user-visible cryptographic mechanisms are not
widely used by non-expert users, and as a result arguments have been made that
cryptographic mechanisms need to be better hidden or embedded in end-user
processes and tools. Other mechanisms, such as HTTPS, have cryptography
built-in and only become visible to the user when a dialogue appears due to a
(potential) problem. This paper surveys deployed and potential technologies in
use, examines the social and legal context of broad classes of users, and from
there, assesses the value and issues for those users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1950</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1950</id><created>2013-03-08</created><authors><author><keyname>Vaniachine</keyname><forenames>A. V.</forenames></author><author><keyname>ATLAS</keyname><forenames>on behalf of the</forenames></author><author><keyname>Collaborations</keyname><forenames>CMS</forenames></author></authors><title>Advancements in Big Data Processing in the ATLAS and CMS Experiments</title><categories>cs.DC cs.DB hep-ex</categories><comments>7 pages, 7 figures</comments><report-no>ANL-HEP-CP-12-77; ATL-SOFT-PROC-2012-068</report-no><journal-ref>In: Proc. of the Fifth International Conference &quot;Distributed
  computing and Grid-technologies in Science and Education&quot; (Dubna, July 16-21,
  2012), Dubna, JINR, 2012, p. 224</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The ever-increasing volumes of scientific data present new challenges for
distributed computing and Grid technologies. The emerging Big Data revolution
drives exploration in scientific fields including nanotechnology, astrophysics,
high-energy physics, biology and medicine. New initiatives are transforming
data-driven scientific fields enabling massive data analysis in new ways. In
petascale data processing scientists deal with datasets, not individual files.
As a result, a task (comprised of many jobs) became a unit of petascale data
processing on the Grid. Splitting of a large data processing task into jobs
enabled fine-granularity checkpointing analogous to the splitting of a large
file into smaller TCP/IP packets during data transfers. Transferring large data
in small packets achieves reliability through automatic re-sending of the
dropped TCP/IP packets. Similarly, transient job failures on the Grid can be
recovered by automatic re-tries to achieve reliable six sigma production
quality in petascale data processing on the Grid. The computing experience of
the ATLAS and CMS experiments provides foundation for reliability engineering
scaling up Grid technologies for data processing beyond the petascale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1951</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1951</id><created>2013-03-08</created><authors><author><keyname>Otair</keyname><forenames>Dr. Mohammed</forenames></author></authors><title>Approximate k-nearest neighbour based spatial clustering using k-d tree</title><categories>cs.DB</categories><report-no>Vol.5, No.1, February 2013</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Different spatial objects that vary in their characteristics, such as
molecular biology and geography, are presented in spatial areas. Methods to
organize, manage, and maintain those objects in a structured manner are
required. Data mining raised different techniques to overcome these
requirements. There are many major tasks of data mining, but the mostly used
task is clustering. Data set within the same cluster share common features that
give each cluster its characteristics. In this paper, an implementation of
Approximate kNN-based spatial clustering algorithm using the K-d tree is
proposed. The major contribution achieved by this research is the use of the
k-d tree data structure for spatial clustering, and comparing its performance
to the brute-force approach. The results of the work performed in this paper
revealed better performance using the k-d tree, compared to the traditional
brute-force approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1952</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1952</id><created>2013-03-08</created><authors><author><keyname>Venkatadri</keyname><forenames>Giridhari</forenames></author><author><keyname>Veeramani</keyname><forenames>Mahendran</forenames></author><author><keyname>C</keyname><forenames>Siva Ram Murthy</forenames></author></authors><title>Improving DTN Routing Performance Using Many-to-Many Communication: A
  Performance Modeling Study</title><categories>cs.NI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-Tolerant Networks (DTNs) have emerged as an exciting research area with
a number of useful applications. Most of these applications would benefit
greatly by a reduction in the message delivery delay experienced in the
network. The delay performance of DTNs is adversely affected by contention,
especially severe in the presence of higher traffic rates and node densities.
Many-to-Many (M2M) communication can handle this contention much better than
traditional one-to- one communication employing CSMA. In this paper, for the
first time, we analytically model the expected delivery delay of a DTN
employing epidemic routing and M2M communication. The accuracy of our model is
demonstrated by matching the analytical results against those from simulations.
We also show using simulations that M2M communication significantly improves
the delay performance (with respect to one-to-one CSMA) for high contention
scenarios. We believe our work will enable the effective application of M2M
communication to reduce delivery delays in DTNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1969</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1969</id><created>2013-03-08</created><authors><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>Arithmetic Branching Programs with Memory</title><categories>cs.CC</categories><comments>18 pages, 3 figures</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the well known characterization of $\vpws$ as the class of
polynomials computed by polynomial size arithmetic branching programs to other
complexity classes. In order to do so we add additional memory to the
computation of branching programs to make them more expressive. We show that
allowing different types of memory in branching programs increases the
computational power even for constant width programs. In particular, this leads
to very natural and robust characterizations of $\vp$ and $\vnp$ by branching
programs with memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1971</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1971</id><created>2013-03-08</created><authors><author><keyname>Santos</keyname><forenames>Mariana de Azevedo</forenames></author><author><keyname>Bermejo</keyname><forenames>Paulo Henrique de Souza</forenames></author><author><keyname>de Oliveira</keyname><forenames>Marcelo Silva</forenames></author><author><keyname>Tonelli</keyname><forenames>Adriano Ol&#xed;mpio</forenames></author><author><keyname>Seidel</keyname><forenames>Enio J&#xfa;nior</forenames></author></authors><title>Improving the management of cost and scope in software projects using
  agile practices</title><categories>cs.SE</categories><comments>47-64pp</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 5, No 1, February 2013</journal-ref><doi>10.5121/ijcsit.2013.5104</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  While organizations want to develop software products with reduced cost and
flexible scope, stories about the applicability of agile practices to improve
project development and performance in the software industry are scarce and
focused on specific methodologies such as Scrum and XP. Given these facts, this
paper aims to investigate, through practitioners' perceptions of value, which
agile practices are being used to improve two performance criteria for software
projects-cost and scope. Using a multivariate statistical technique known as
Exploratory Factor Analysis (EFA), the results suggest that the use of agile
practices can be represented in factors which describe different applications
in software development process to improve cost and scope. Also, we conclude
that some agile practices should be used together in order to get better
efficiency on cost and scope in four development aspects: improving (a) team
abilities, (b)management of requirements, (c) quality of the code developed,
and (d) delivery of software on-budget and on-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1994</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1994</id><created>2013-03-08</created><updated>2013-03-11</updated><authors><author><keyname>Bonsangue</keyname><forenames>Marcello</forenames></author><author><keyname>Caltais</keyname><forenames>Georgiana</forenames></author><author><keyname>Goriac</keyname><forenames>Eugen-Ioan</forenames></author><author><keyname>Lucanu</keyname><forenames>Dorel</forenames></author><author><keyname>Rutten</keyname><forenames>Jan</forenames></author><author><keyname>Silva</keyname><forenames>Alexandra</forenames></author></authors><title>Automatic Equivalence Proofs for Non-deterministic Coalgebras</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A notion of generalized regular expressions for a large class of systems
modeled as coalgebras, and an analogue of Kleene's theorem and Kleene algebra,
were recently proposed by a subset of the authors of this paper. Examples of
the systems covered include infinite streams, deterministic automata, Mealy
machines and labelled transition systems. In this paper, we present a novel
algorithm to decide whether two expressions are bisimilar or not. The procedure
is implemented in the automatic theorem prover CIRC, by reducing coinduction to
an entailment relation between an algebraic specification and an appropriate
set of equations. We illustrate the generality of the tool with three examples:
infinite streams of real numbers, Mealy machines and labelled transition
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.1998</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.1998</id><created>2013-03-08</created><authors><author><keyname>Barbulescu</keyname><forenames>Razvan</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Selecting polynomials for the Function Field Sieve</title><categories>cs.CR math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Function Field Sieve algorithm is dedicated to computing discrete
logarithms in a finite field GF(q^n), where q is small an prime power. The
scope of this article is to select good polynomials for this algorithm by
defining and measuring the size property and the so-called root and
cancellation properties. In particular we present an algorithm for rapidly
testing a large set of polynomials. Our study also explains the behaviour of
inseparable polynomials, in particular we give an easy way to see that the
algorithm encompass the Coppersmith algorithm as a particular case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2003</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2003</id><created>2013-03-08</created><authors><author><keyname>Kahl</keyname><forenames>Thomas</forenames></author></authors><title>Weak morphisms of higher dimensional automata</title><categories>cs.FL math.AT</categories><msc-class>68Q45, 68Q85, 55U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce weak morphisms of higher dimensional automata and use them to
define preorder relations for HDAs, among which homeomorphic abstraction and
trace equivalent abstraction. It is shown that homeomorphic abstraction is
essentially always stronger than trace equivalent abstraction. We also define
the trace language of an HDA and show that, for a large class of HDAs, it is
invariant under trace equivalent abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2013</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2013</id><created>2013-03-08</created><authors><author><keyname>Wolff</keyname><forenames>J Gerard</forenames></author></authors><title>Computing as compression: the SP theory of intelligence</title><categories>cs.AI</categories><comments>8 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1212.0229</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an overview of the SP theory of intelligence and its
central idea that artificial intelligence, mainstream computing, and much of
human perception and cognition, may be understood as information compression.
  The background and origins of the SP theory are described, and the main
elements of the theory, including the key concept of multiple alignment,
borrowed from bioinformatics but with important differences. Associated with
the SP theory is the idea that redundancy in information may be understood as
repetition of patterns, that compression of information may be achieved via the
matching and unification (merging) of patterns, and that computing and
information compression are both fundamentally probabilistic. It appears that
the SP system is Turing-equivalent in the sense that anything that may be
computed with a Turing machine may, in principle, also be computed with an SP
machine.
  One of the main strengths of the SP theory and the multiple alignment concept
is in modelling concepts and phenomena in artificial intelligence. Within that
area, the SP theory provides a simple but versatile means of representing
different kinds of knowledge, it can model both the parsing and production of
natural language, with potential for the understanding and translation of
natural languages, it has strengths in pattern recognition, with potential in
computer vision, it can model several kinds of reasoning, and it has
capabilities in planning, problem solving, and unsupervised learning.
  The paper includes two examples showing how alternative parsings of an
ambiguous sentence may be modelled as multiple alignments, and another example
showing how the concept of multiple alignment may be applied in medical
diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2017</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2017</id><created>2013-03-08</created><authors><author><keyname>Adebiyi</keyname><forenames>A.</forenames></author><author><keyname>Arreymbi</keyname><forenames>Johnnes</forenames></author><author><keyname>Imafidon</keyname><forenames>Chris</forenames></author></authors><title>Security Assessment of Software Design using Neural Network</title><categories>cs.CR cs.NE</categories><comments>7 pages, 1 figure, 4 tables, (IJARAI) International Journal of
  Advanced Research in Artificial Intelligence, Vol. 1(4), 2012, pp.1-7,
  ISSN:2165-4069 (Online), ISSN:2165-4050 (Print)</comments><journal-ref>(IJARAI) International Journal of Advanced Research in Artificial
  Intelligence, Vol. 1(4), 2012, pp.1-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security flaws in software applications today has been attributed mostly to
design flaws. With limited budget and time to release software into the market,
many developers often consider security as an afterthought. Previous research
shows that integrating security into software applications at a later stage of
software development lifecycle (SDLC) has been found to be more costly than
when it is integrated during the early stages. To assist in the integration of
security early in the SDLC stages, a new approach for assessing security during
the design phase by neural network is investigated in this paper. Our findings
show that by training a back propagation neural network to identify attack
patterns, possible attacks can be identified from design scenarios presented to
it. The result of performance of the neural network is presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2025</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2025</id><created>2013-03-08</created><updated>2013-06-26</updated><authors><author><keyname>Berlingerio</keyname><forenames>Michele</forenames></author><author><keyname>Pinelli</keyname><forenames>Fabio</forenames></author><author><keyname>Calabrese</keyname><forenames>Francesco</forenames></author></authors><title>ABACUS: frequent pAttern mining-BAsed Community discovery in
  mUltidimensional networkS</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community Discovery in complex networks is the problem of detecting, for each
node of the network, its membership to one of more groups of nodes, the
communities, that are densely connected, or highly interactive, or, more in
general, similar, according to a similarity function. So far, the problem has
been widely studied in monodimensional networks, i.e. networks where only one
connection between two entities can exist. However, real networks are often
multidimensional, i.e., multiple connections between any two nodes can exist,
either reflecting different kinds of relationships, or representing different
values of the same type of tie. In this context, the problem of Community
Discovery has to be redefined, taking into account multidimensional structure
of the graph. We define a new concept of community that groups together nodes
sharing memberships to the same monodimensional communities in the different
single dimensions. As we show, such communities are meaningful and able to
group highly correlated nodes, even if they might not be connected in any of
the monodimensional networks. We devise ABACUS (Apriori-BAsed Community
discoverer in mUltidimensional networkS), an algorithm that is able to extract
multidimensional communities based on the apriori itemset miner applied to
monodimensional community memberships. Experiments on two different real
multidimensional networks confirm the meaningfulness of the introduced
concepts, and open the way for a new class of algorithms for community
discovery that do not rely on the dense connections among nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2033</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2033</id><created>2013-03-08</created><updated>2015-02-23</updated><authors><author><keyname>Liepins</keyname><forenames>Vilnis</forenames></author></authors><title>Extended Fourier analysis of signals</title><categories>cs.DS cs.IT cs.NA math.IT</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This summary of the doctoral thesis is created to emphasize the close
connection of the proposed spectral analysis method with the Discrete Fourier
Transform (DFT), the most extensively studied and frequently used approach in
the history of signal processing. It is shown that in a typical application
case, where uniform data readings are transformed to the same number of
uniformly spaced frequencies, the results of the classical DFT and proposed
approach coincide. The difference in performance appears when the length of the
DFT is selected to be greater than the length of the data. The DFT solves the
unknown data problem by padding readings with zeros up to the length of the
DFT, while the proposed Extended DFT (EDFT) deals with this situation in a
different way, it uses the Fourier integral transform as a target and optimizes
the transform basis in the extended frequency range without putting such
restrictions on the time domain. Consequently, the Inverse DFT (IDFT) applied
to the result of EDFT returns not only known readings, but also the
extrapolated data, where classical DFT is able to give back just zeros, and
higher resolution are achieved at frequencies where the data has been
successfully extended. It has been demonstrated that EDFT able to process data
with missing readings or gaps inside or even nonuniformly distributed data.
Thus, EDFT significantly extends the usability of the DFT-based methods, where
previously these approaches have been considered as not applicable. The EDFT
founds the solution in an iterative way and requires repeated calculations to
get the adaptive basis, and this makes it numerical complexity much higher
compared to DFT. This disadvantage was a serious problem in the 1990s, when the
method has been proposed. Fortunately, since then the power of computers has
increased so much that nowadays EDFT application could be considered as a real
alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2041</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2041</id><created>2013-03-08</created><updated>2013-05-13</updated><authors><author><keyname>Askalidis</keyname><forenames>Georgios</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Kwanashie</keyname><forenames>Augustine</forenames></author><author><keyname>Manlove</keyname><forenames>David F.</forenames></author><author><keyname>Pountourakis</keyname><forenames>Emmanouil</forenames></author></authors><title>Socially stable matchings in the Hospitals / Residents problem</title><categories>cs.GT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Hospitals/Residents (HR) problem, agents are partitioned into
hospitals and residents. Each agent wishes to be matched to an agent in the
other set and has a strict preference over these potential matches. A matching
is stable if there are no blocking pairs, i.e., no pair of agents that prefer
each other to their assigned matches. Such a situation is undesirable as it
could lead to a deviation in which the blocking pair form a private arrangement
outside the matching. This however assumes that the blocking pair have social
ties or communication channels to facilitate the deviation. Relaxing the
stability definition to take account of the potential lack of social ties
between agents can yield larger stable matchings.
  In this paper, we define the Hospitals/Residents problem under Social
Stability (HRSS) which takes into account social ties between agents by
introducing a social network graph to the HR problem. Edges in the social
network graph correspond to resident-hospital pairs in the HR instance that
know one another. Pairs that do not have corresponding edges in the social
network graph can belong to a matching M but they can never block M. Relative
to a relaxed stability definition for HRSS, called social stability, we show
that socially stable matchings can have different sizes and the problem of
finding a maximum socially stable matching is NP-hard, though approximable
within 3/2. Furthermore we give polynomial time algorithms for three special
cases of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2042</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2042</id><created>2013-03-08</created><updated>2013-12-13</updated><authors><author><keyname>Amy</keyname><forenames>Matthew</forenames></author><author><keyname>Maslov</keyname><forenames>Dmitri</forenames></author><author><keyname>Mosca</keyname><forenames>Michele</forenames></author></authors><title>Polynomial-time T-depth Optimization of Clifford+T circuits via Matroid
  Partitioning</title><categories>quant-ph cs.ET</categories><comments>Version 2 contains substantial improvements and extensions to the
  previous version. We describe a new, more robust algorithm and achieve
  significantly improved experimental results</comments><journal-ref>IEEE Transactions on Computer-Aided Design of Integrated Circuits
  and Systems 33(10): 1476-1489, 2014</journal-ref><doi>10.1109/TCAD.2014.2341953</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work in quantum circuit optimization has been performed in isolation
from the results of quantum fault-tolerance. Here we present a polynomial-time
algorithm for optimizing quantum circuits that takes the actual implementation
of fault-tolerant logical gates into consideration. Our algorithm
re-synthesizes quantum circuits composed of Clifford group and T gates, the
latter being typically the most costly gate in fault-tolerant models, e.g.,
those based on the Steane or surface codes, with the purpose of minimizing both
T-count and T-depth. A major feature of the algorithm is the ability to
re-synthesize circuits with additional ancillae to reduce T-depth at
effectively no cost. The tested benchmarks show up to 65.7% reduction in
T-count and up to 87.6% reduction in T-depth without ancillae, or 99.7%
reduction in T-depth using ancillae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2043</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2043</id><created>2013-03-08</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author></authors><title>Orientation and Connectivity Based Criteria for Asymptotic Consensus</title><categories>cs.DC</categories><comments>22 pages, 1 figure</comments><msc-class>93A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we establish orientation and connectivity based criteria for
the agreement algorithm to achieve asymptotic consensus in the context of
time-varying topology and communication delays. These criteria unify and extend
many earlier convergence results on the agreement algorithm for deterministic
and discrete-time multiagent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2048</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2048</id><created>2013-03-08</created><authors><author><keyname>Yoo</keyname><forenames>Juhwan</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Harms</keyname><forenames>Andrew</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Finding Zeros: Greedy Detection of Holes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, motivated by the setting of white-space detection [1], we
present theoretical and empirical results for detection of the zero-support E
of x \in Cp (xi = 0 for i \in E) with reduced-dimension linear measurements. We
propose two low- complexity algorithms based on one-step thresholding [2] for
this purpose. The second algorithm is a variant of the first that further
assumes the presence of group-structure in the target signal [3] x. Performance
guarantees for both algorithms based on the worst- case and average coherence
(group coherence) of the measurement matrix is presented along with the
empirical performance of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2054</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2054</id><created>2013-03-08</created><authors><author><keyname>Dhifli</keyname><forenames>Wajdi</forenames></author><author><keyname>Saidi</keyname><forenames>Rabie</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>Mining Representative Unsubstituted Graph Patterns Using Prior
  Similarity Matrix</title><categories>cs.CE cs.LG</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  One of the most powerful techniques to study protein structures is to look
for recurrent fragments (also called substructures or spatial motifs), then use
them as patterns to characterize the proteins under study. An emergent trend
consists in parsing proteins three-dimensional (3D) structures into graphs of
amino acids. Hence, the search of recurrent spatial motifs is formulated as a
process of frequent subgraph discovery where each subgraph represents a spatial
motif. In this scope, several efficient approaches for frequent subgraph
discovery have been proposed in the literature. However, the set of discovered
frequent subgraphs is too large to be efficiently analyzed and explored in any
further process. In this paper, we propose a novel pattern selection approach
that shrinks the large number of discovered frequent subgraphs by selecting the
representative ones. Existing pattern selection approaches do not exploit the
domain knowledge. Yet, in our approach we incorporate the evolutionary
information of amino acids defined in the substitution matrices in order to
select the representative subgraphs. We show the effectiveness of our approach
on a number of real datasets. The results issued from our experiments show that
our approach is able to considerably decrease the number of motifs while
enhancing their interestingness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2059</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2059</id><created>2013-03-08</created><authors><author><keyname>Durand</keyname><forenames>Arnaud</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author></authors><title>Structural Tractability of Counting of Solutions to Conjunctive Queries</title><categories>cs.LO</categories><comments>29 pages, 3 figures Preliminary version appeared in ICDT'2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the problem of counting solutions to conjunctive
queries. We consider a parameter called the \emph{quantified star size} of a
formula $\varphi$ which measures how the free variables are spread in
$\varphi$. We show that for conjunctive queries that admit nice decomposition
properties (such as being of bounded treewidth or generalized hypertree width)
bounded quantified star size exactly characterizes the classes of queries for
which counting the number of solutions is tractable. This also allows us to
fully characterize the conjunctive queries for which counting the solutions is
tractable in the case of bounded arity. To illustrate the applicability of our
results, we also show that computing the quantified star size of a formula is
possible in time $n^{O(k)}$ for queries of generalized hypertree width $k$.
Furthermore, quantified star size is even fixed parameter tractable
parameterized by some other width measures, while it is $\W{1}$-hard for
generalized hypertree width and thus unlikely to be fixed parameter tractable.
We finally show how to compute an approximation of quantified star size in
polynomial time where the approximation ratio depends on the width of the
input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2062</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2062</id><created>2013-03-08</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>N. A.</forenames></author><author><keyname>Shakir</keyname><forenames>M.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Ubiquitous HealthCare in Wireless Body Area Networks - A Survey</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.2240</comments><journal-ref>Journal of Basic Applied Scientific Research (JBASR), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in wireless communication, system on chip and low power sensor nodes
allowed realization of Wireless Body Area Network (WBAN). WBAN comprised of
tiny sensors, which collect information of patient's vital signs and provide a
real time feedback. In addition, WBAN also supports many applications including
Ubiquitous HealthCare (UHC), entertainment, gaming, military, etc. UHC is
required by elderly people to facilitate them with instant monitoring anywhere
they move around. In this paper, different standards used in WBAN were also
discussed briefly. Path loss in WBAN and its impact on communication was
presented with the help of simulations, which were performed for different
models of In-Body communication and different factors (such as, attenuation,
frequency, distance etc) influencing path loss in On-Body communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2070</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2070</id><created>2013-03-08</created><updated>2013-04-24</updated><authors><author><keyname>Benedetti</keyname><forenames>Bruno</forenames></author><author><keyname>Lutz</keyname><forenames>Frank H.</forenames></author></authors><title>Knots in collapsible and non-collapsible balls</title><categories>math.CO cs.CG math.GT</categories><comments>25 pages, 5 figures, 11 tables, references updated</comments><msc-class>57Q15, 52B22, 57M25</msc-class><journal-ref>Electronic Journal of Combinatorics 20 (2013), No.3, Paper P31, 29
  pages</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct the first explicit example of a simplicial 3-ball B_{15,66} that
is not collapsible. It has only 15 vertices. We exhibit a second 3-ball
B_{12,38} with 12 vertices that is collapsible and evasive, but not shellable.
Finally, we present the first explicit triangulation of a 3-sphere S_{18, 125}
(with only 18 vertices) that is not locally constructible. All these examples
are based on knotted subcomplexes with only three edges; the knots are the
trefoil, the double trefoil, and the triple trefoil, respectively. The more
complicated the knot is, the more distant the triangulation is from being
polytopal, collapsible, etc. Further consequences of our work are:
  (1) Unshellable 3-spheres may have vertex-decomposable barycentric
subdivisions.
  (This shows the strictness of an implication proven by Billera and Provan.)
  (2) For d-balls, vertex-decomposable implies non-evasive implies collapsible,
and for d=3 all implications are strict.
  (This answers a question by Barmak.)
  (3) Locally constructible 3-balls may contain a double trefoil knot as a
3-edge subcomplex.
  (This improves a result of Benedetti and Ziegler.)
  (4) Rudin's ball is non-evasive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2071</identifier>
 <datestamp>2015-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2071</id><created>2013-03-08</created><updated>2015-01-23</updated><authors><author><keyname>Wolff</keyname><forenames>J. Gerard</forenames></author></authors><title>Application of the SP theory of intelligence to the understanding of
  natural vision and the development of computer vision</title><categories>cs.CV cs.AI</categories><comments>40 pages, 16 figures</comments><journal-ref>SpringerPlus, 3, 552, 2014</journal-ref><doi>10.1186/2193-1801-3-552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SP theory of intelligence aims to simplify and integrate concepts in
computing and cognition, with information compression as a unifying theme. This
article discusses how it may be applied to the understanding of natural vision
and the development of computer vision. The theory, which is described quite
fully elsewhere, is described here in outline but with enough detail to ensure
that the rest of the article makes sense.
  Low level perceptual features such as edges or corners may be identified by
the extraction of redundancy in uniform areas in a manner that is comparable
with the run-length encoding technique for information compression.
  The concept of multiple alignment in the SP theory may be applied to the
recognition of objects, and to scene analysis, with a hierarchy of parts and
sub-parts, and at multiple levels of abstraction.
  The theory has potential for the unsupervised learning of visual objects and
classes of objects, and suggests how coherent concepts may be derived from
fragments.
  As in natural vision, both recognition and learning in the SP system is
robust in the face of errors of omission, commission and substitution.
  The theory suggests how, via vision, we may piece together a knowledge of the
three-dimensional structure of objects and of our environment, it provides an
account of how we may see things that are not objectively present in an image,
and how we recognise something despite variations in the size of its retinal
image. And it has things to say about the phenomena of lightness constancy and
colour constancy, the role of context in recognition, and ambiguities in visual
perception.
  A strength of the SP theory is that it provides for the integration of vision
with other sensory modalities and with other aspects of intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2072</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2072</id><created>2013-03-08</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Hayat</keyname><forenames>S.</forenames></author><author><keyname>Shakir</keyname><forenames>M.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Energy Efficient MAC Protocols in Wireless Body Area Sensor Networks -A
  Survey</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1207.2567</comments><journal-ref>Journal of Basic Applied Scientific Research (JBASR), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first presented an analytically discussion about energy
efficiency of Medium Access Control (MAC) protocols for Wireless Body Area
Sensor Networks (WBASNs). For this purpose, different energy efficient MAC
protocols with their respective energy optimization techniques; Low Power
Listening (LPL), Scheduled Contention and Time Division Multiple Access (TDMA),
are elaborated. We also analytically compared path loss models for In-body,
On-body and Off-body communications in WBASNs. These three path loss scenarios
are simulated in MATLAB and results shown that path loss is more in In-body
communication because of less energy level to take care of tissues and organs
located inside human body. Secondly, power model for WBASNs of Carrier Sense
Multiple Access with Collision Avoidance (CSMA/CA) and beacon mode is also
presented. MATLAB simulations results shown that power of CSMA/CA mode is less
as compared to beacon mode. Finally, we suggested that hybrid mode is more
useful to achieve optimization in power consumption, which consequently results
in high energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2087</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2087</id><created>2013-03-08</created><authors><author><keyname>Zhu</keyname><forenames>Fangfang</forenames></author><author><keyname>Chen</keyname><forenames>Biao</forenames></author></authors><title>Capacity Bounds and Sum Rate Capacities of a CLass of Discrete
  Memoryless Interference Channels</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Inf. Theory. arXiv admin note: substantial
  text overlap with arXiv:1207.3322</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the capacity of a class of discrete memoryless
interference channels where interference is defined analogous to that of
Gaussian interference channel with one-sided weak interference. The sum-rate
capacity of this class of channels is determined. As with the Gaussian case,
the sum-rate capacity is achieved by letting the transceiver pair subject to
interference communicate at a rate such that its message can be decoded at the
unintended receiver using single user detection. It is also established that
this class of discrete memoryless interference channels is equivalent in
capacity region to certain degraded interference channels. This allows the
construction of capacity outer-bounds using the capacity regions of associated
degraded broadcast channels. The same technique is then used to determine the
sum-rate capacity of discrete memoryless interference channels with mixed
interference as defined in the paper. The obtained capacity bounds and sum-rate
capacities are used to resolve the capacities of several new discrete
memoryless interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2096</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2096</id><created>2013-03-08</created><authors><author><keyname>Woods</keyname><forenames>Alfredo Garcia</forenames></author></authors><title>Gene-Machine, a new search heuristic algorithm</title><categories>cs.NE cs.AI</categories><comments>GeneMachine uses the chromosome notion, genes and evolution, but it
  differs from genetic algorithms, in that it does not use mutation, nor
  population of individuals, neither the notion of generation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces Gene-Machine, an efficient and new search heuristic
algorithm, based in the building-block hypothesis. It is inspired by natural
evolution, but does not use some of the concepts present in genetic algorithms
like population, mutation and generation. This heuristic exhibits good
performance in comparison with genetic algorithms, and can be used to generate
useful solutions to optimization and search problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2104</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2104</id><created>2013-03-08</created><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author><author><keyname>Wu</keyname><forenames>Ji</forenames></author></authors><title>Transfer Learning for Voice Activity Detection: A Denoising Deep Neural
  Network Perspective</title><categories>cs.LG</categories><comments>This paper has been submitted to the conference &quot;INTERSPEECH2013&quot; in
  March 4, 2013 for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mismatching problem between the source and target noisy corpora severely
hinder the practical use of the machine-learning-based voice activity detection
(VAD). In this paper, we try to address this problem in the transfer learning
prospective. Transfer learning tries to find a common learning machine or a
common feature subspace that is shared by both the source corpus and the target
corpus. The denoising deep neural network is used as the learning machine.
Three transfer techniques, which aim to learn common feature representations,
are used for analysis. Experimental results demonstrate the effectiveness of
the transfer learning schemes on the mismatch problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2108</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2108</id><created>2013-03-11</created><authors><author><keyname>da Silva</keyname><forenames>Wagner Barreto</forenames></author><author><keyname>Freitas</keyname><forenames>Corina da Costa</forenames></author><author><keyname>Sant'Anna</keyname><forenames>Sidnei Jo&#xe3;o Siqueira</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author></authors><title>Classification of Segments in PolSAR Imagery by Minimum Stochastic
  Distances Between Wishart Distributions</title><categories>cs.CV stat.AP</categories><comments>Accepted for publication on the IEEE Journal of Selected Topics in
  Applied Earth Observations and Remote Sensing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new classifier for Polarimetric SAR (PolSAR) images is proposed and
assessed in this paper. Its input consists of segments, and each one is
assigned the class which minimizes a stochastic distance. Assuming the complex
Wishart model, several stochastic distances are obtained from the h-phi family
of divergences, and they are employed to derive hypothesis test statistics that
are also used in the classification process. This article also presents, as a
novelty, analytic expressions for the test statistics based on the following
stochastic distances between complex Wishart models: Kullback-Leibler,
Bhattacharyya, Hellinger, R\'enyi, and Chi-Square; also, the test statistic
based on the Bhattacharyya distance between multivariate Gaussian distributions
is presented. The classifier performance is evaluated using simulated and real
PolSAR data. The simulated data are based on the complex Wishart model, aiming
at the analysis of the proposal well controlled data. The real data refer to
the complex L-band image, acquired during the 1994 SIR-C mission. The results
of the proposed classifier are compared with those obtained by a Wishart
per-pixel/contextual classifier, and we show the better performance of the
region-based classification. The influence of the statistical modeling is
assessed by comparing the results using the Bhattacharyya distance between
multivariate Gaussian distributions for amplitude data. The results with
simulated data indicate that the proposed classification method has a very good
performance when the data follow the Wishart model. The proposed classifier
also performs better than the per-pixel/contextual classifier and the
Bhattacharyya Gaussian distance using SIR-C PolSAR data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2130</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2130</id><created>2013-03-08</created><updated>2013-10-21</updated><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Convex Discriminative Multitask Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multitask clustering tries to improve the clustering performance of multiple
tasks simultaneously by taking their relationship into account. Most existing
multitask clustering algorithms fall into the type of generative clustering,
and none are formulated as convex optimization problems. In this paper, we
propose two convex Discriminative Multitask Clustering (DMTC) algorithms to
address the problems. Specifically, we first propose a Bayesian DMTC framework.
Then, we propose two convex DMTC objectives within the framework. The first
one, which can be seen as a technical combination of the convex multitask
feature learning and the convex Multiclass Maximum Margin Clustering (M3C),
aims to learn a shared feature representation. The second one, which can be
seen as a combination of the convex multitask relationship learning and M3C,
aims to learn the task relationship. The two objectives are solved in a uniform
procedure by the efficient cutting-plane algorithm. Experimental results on a
toy problem and two benchmark datasets demonstrate the effectiveness of the
proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2132</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2132</id><created>2013-03-08</created><updated>2014-04-22</updated><authors><author><keyname>Zhang</keyname><forenames>Xiao-Lei</forenames></author></authors><title>Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization
  and Layered Clustering-Based Approach</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important classifier ensemble for multiclass classification problems is
Error-Correcting Output Codes (ECOCs). It bridges multiclass problems and
binary-class classifiers by decomposing multiclass problems to a serial
binary-class problems. In this paper, we present a heuristic ternary code,
named Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). It
starts with an arbitrary valid ECOC and iterates the following two steps until
the training risk converges. The first step, named Layered Clustering based
ECOC (LC-ECOC), constructs multiple strong classifiers on the most confusing
binary-class problem. The second step adds the new classifiers to ECOC by a
novel Optimized Weighted (OW) decoding algorithm, where the optimization
problem of the decoding is solved by the cutting plane algorithm. Technically,
LC-ECOC makes the heuristic training process not blocked by some difficult
binary-class problem. OW decoding guarantees the non-increase of the training
risk for ensuring a small code length. Results on 14 UCI datasets and a music
genre classification problem demonstrate the effectiveness of WOLC-ECOC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2136</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2136</id><created>2013-03-08</created><authors><author><keyname>Kofidis</keyname><forenames>E.</forenames></author><author><keyname>Katselis</keyname><forenames>D.</forenames></author><author><keyname>Rontogiannis</keyname><forenames>A.</forenames></author><author><keyname>Theodoridis</keyname><forenames>S.</forenames></author></authors><title>Preamble-based Channel Estimation in OFDM/OQAM Systems: A Review</title><categories>cs.IT math.IT</categories><comments>This is an early version of a paper to appear in Signal Processing
  (Elsevier)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filter bank-based multicarrier communications (FBMC) have recently attracted
increased interest in both wired (e.g., xDSL, PLC) and wireless (e.g.,
cognitive radio) applications, due to their enhanced flexibility, higher
spectral efficiency, and better spectral containment compared to conventional
OFDM. A particular type of FBMC, the so-called FBMC/OQAM or OFDM/OQAM system,
consisting of pulse shaped OFDM carrying offset QAM (OQAM) symbols, has
received increasing attention due to, among other features, its higher spectral
efficiency and implementation simplicity. It suffers, however, from an
imaginary inter-carrier/inter-symbol interference that complicates signal
processing tasks such as channel estimation. This paper focuses on channel
estimation for OFDM/OQAM systems based on a known preamble. A review of the
existing preamble structures and associated channel estimation methods is
given, for both single- (SISO) and multiple-antenna (MIMO) systems. The various
preambles are compared via simulations in both mildly and highly frequency
selective channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2140</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2140</id><created>2013-03-08</created><updated>2013-11-01</updated><authors><author><keyname>Ooms</keyname><forenames>Jeroen</forenames></author></authors><title>Possible Directions for Improving Dependency Versioning in R</title><categories>cs.SE cs.MS stat.CO</categories><journal-ref>The R Journal Vol. 5/1, June 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most powerful features of R is its infrastructure for contributed
code. The built-in package manager and complementary repositories provide a
great system for development and exchange of code, and have played an important
role in the growth of the platform towards the de-facto standard in statistical
computing that it is today. However, the number of packages on CRAN and other
repositories has increased beyond what might have been foreseen, and is
revealing some limitations of the current design. One such problem is the
general lack of dependency versioning in the infrastructure. This paper
explores this problem in greater detail, and suggests approaches taken by other
open source communities that might work for R as well. Three use cases are
defined that exemplify the issue, and illustrate how improving this aspect of
package management could increase reliability while supporting further growth
of the R community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2143</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2143</id><created>2013-03-08</created><authors><author><keyname>van Rooijen</keyname><forenames>Lorijn</forenames></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames></author></authors><title>The separation problem for regular languages by piecewise testable
  languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation is a classical problem in mathematics and computer science. It
asks whether, given two sets belonging to some class, it is possible to
separate them by another set of a smaller class. We present and discuss the
separation problem for regular languages. We then give a direct polynomial time
algorithm to check whether two given regular languages are separable by a
piecewise testable language, that is, whether a $B{\Sigma}1(&lt;)$ sentence can
witness that the languages are indeed disjoint. The proof is a reformulation
and a refinement of an algebraic argument already given by Almeida and the
second author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2147</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2147</id><created>2013-03-08</created><updated>2013-10-25</updated><authors><author><keyname>Irfan</keyname><forenames>Mohammad T.</forenames></author><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author></authors><title>On Influence, Stable Behavior, and the Most Influential Individuals in
  Networks: A Game-Theoretic Approach</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>Accepted to AI Journal, subject to addressing the reviewers' points
  (which are addressed in this version). An earlier version of the article
  appeared in AAAI-11</comments><msc-class>68T01, 68W40, 68Q25</msc-class><acm-class>I.2.0; J.4; F.2.0</acm-class><journal-ref>Artificial Intelligence (2014), pp. 79-119</journal-ref><doi>10.1016/j.artint.2014.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to the study of influence in strategic settings
where the action of an individual depends on that of others in a
network-structured way. We propose \emph{influence games} as a
\emph{game-theoretic} model of the behavior of a large but finite networked
population. Influence games allow \emph{both} positive and negative
\emph{influence factors}, permitting reversals in behavioral choices. We
embrace \emph{pure-strategy Nash equilibrium (PSNE)}, an important solution
concept in non-cooperative game theory, to formally define the \emph{stable
outcomes} of an influence game and to predict potential outcomes without
explicitly considering intricate dynamics. We address an important problem in
network influence, the identification of the \emph{most influential
individuals}, and approach it algorithmically using PSNE computation.
\emph{Computationally}, we provide (a) complexity characterizations of various
problems on influence games; (b) efficient algorithms for several special cases
and heuristics for hard cases; and (c) approximation algorithms, with provable
guarantees, for the problem of identifying the most influential individuals.
\emph{Experimentally}, we evaluate our approach using both synthetic influence
games as well as several real-world settings of general interest, each
corresponding to a separate branch of the U.S. Government.
\emph{Mathematically,} we connect influence games to important game-theoretic
models: \emph{potential and polymatrix games}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2149</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2149</id><created>2013-03-08</created><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Optimal Equivocation in Secrecy Systems a Special Case of
  Distortion-based Characterization</title><categories>cs.IT math.IT</categories><comments>Invited to ITA 2013, 3 pages, no figures, using IEEEtran.cls</comments><msc-class>94A62</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work characterizing the optimal performance of secrecy systems has
made use of a distortion-like metric for partial secrecy as a replacement for
the more traditional metric of equivocation. In this work we use the log-loss
function to show that the optimal performance limits characterized by
equivocation are, in fact, special cases of distortion-based counterparts. This
observation illuminates why equivocation doesn't tell the whole story of
secrecy. It also justifies the causal-disclosure framework for secrecy (past
source symbols and actions revealed to the eavesdropper).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2156</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2156</id><created>2013-03-08</created><authors><author><keyname>Gao</keyname><forenames>Heng</forenames></author><author><keyname>Li</keyname><forenames>Yongbao</forenames></author><author><keyname>Li</keyname><forenames>Qiudan</forenames></author><author><keyname>Zeng</keyname><forenames>Daniel</forenames></author></authors><title>The Powerful Model Adpredictor for Search Engine Switching Detection
  Challenge</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The purpose of the Switching Detection Challenge in the 2013 WSCD workshop
was to predict users' search engine swithcing actions given records about
search sessions and logs.Our solution adopted the powerful prediction model
Adpredictor and utilized the method of feature engineering. We successfully
applied the click through rate (CTR) prediction model Adpredicitor into our
solution framework, and then the discovery of effective features and the
multiple classification of different switching type make our model outperforms
many competitors. We achieved an AUC score of 0.84255 on the private
leaderboard and ranked the 5th among all the competitors in the competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2162</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2162</id><created>2013-03-08</created><authors><author><keyname>Durocher</keyname><forenames>Stephane</forenames></author><author><keyname>Mehrabi</keyname><forenames>Saeed</forenames></author></authors><title>New Hardness Results for Guarding Orthogonal Polygons with Sliding
  Cameras</title><categories>cs.CG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be an orthogonal polygon. Consider a sliding camera that travels back
and forth along an orthogonal line segment $s\in P$ as its \emph{trajectory}.
The camera can see a point $p\in P$ if there exists a point $q\in s$ such that
$pq$ is a line segment normal to $s$ that is completely inside $P$. In the
\emph{minimum-cardinality sliding cameras problem}, the objective is to find a
set $S$ of sliding cameras of minimum cardinality to guard $P$ (i.e., every
point in $P$ can be seen by some sliding camera) while in the
\emph{minimum-length sliding cameras problem} the goal is to find such a set
$S$ so as to minimize the total length of trajectories along which the cameras
in $S$ travel.
  In this paper, we first settle the complexity of the minimum-length sliding
cameras problem by showing that it is polynomial tractable even for orthogonal
polygons with holes, answering a question asked by Katz and Morgenstern (2011).
We next show that the minimum-cardinality sliding cameras problem is
\textsc{NP}-hard when $P$ is allowed to have holes, which partially answers
another question asked by Katz and Morgenstern (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2168</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2168</id><created>2013-03-09</created><authors><author><keyname>Mori</keyname><forenames>Ryuhei</forenames></author></authors><title>New Understanding of the Bethe Approximation and the Replica Method</title><categories>cond-mat.stat-mech cs.IT math.IT</categories><comments>Doctoral thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, new generalizations of the Bethe approximation and new
understanding of the replica method are proposed. The Bethe approximation is an
efficient approximation for graphical models, which gives an asymptotically
accurate estimate of the partition function for many graphical models. The
Bethe approximation explains the well-known message passing algorithm, belief
propagation, which is exact for tree graphical models. It is also known that
the cluster variational method gives the generalized Bethe approximation,
called the Kikuchi approximation, yielding the generalized belief propagation.
In the thesis, a new series of generalization of the Bethe approximation is
proposed, which is named the asymptotic Bethe approximation. The asymptotic
Bethe approximation is derived from the characterization of the Bethe free
energy using graph covers, which was recently obtained by Vontobel. The
asymptotic Bethe approximation can be expressed in terms of the edge zeta
function by using Watanabe and Fukumizu's result about the Hessian of the Bethe
entropy. The asymptotic Bethe approximation is confirmed to be better than the
conventional Bethe approximation on some conditions. For this purpose, Chertkov
and Chernyak's loop calculus formula is employed, which shows that the error of
the Bethe approximation can be expressed as a sum of weights corresponding to
generalized loops, and generalized for non-binary finite alphabets by using
concepts of information geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2169</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2169</id><created>2013-03-09</created><authors><author><keyname>Aldar</keyname><forenames>Dilip S.</forenames></author></authors><title>Distributed Fuzzy Optimal Spectrum Sensing In Cognitive Radio</title><categories>cs.NI</categories><journal-ref>Dilip S. Aldar, &quot; Distributed Fuzzy Optimal Spectrum Sensing In
  Cognitive Radio&quot;, IRECOS,Vol. 7 n. 6, pp. 2788-2793,Nov.2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The spectrum is a scarce resource and must utilize efficiently, the cognitive
radio is a prospective solution for underutilized spectrum. The spectrum
sensing is a key functionality to alleviate interference of secondary user to
primary. The cognitive radios must detect the existence of the primary user and
vacate the band for the primary immediately if the primary user detected. In
cooperative sensing, every cognitive radio communicates their decision to
fusion center via reporting channel. The reporting channels are not error free,
which results corruption in the secondary's decision or information due to
multipath fading and shadowing. This paper investigates the distributed fuzzy
optimal cooperative spectrum sensing. The data and decision fusion with fuzzy
detection is investigated in this paper. The simulation result shows the
significant improvement in sensing performance over AND, OR and majority rules.
The optimality in spectrum sensing is achieved by the proposed method with 1/3
of total malicious secondary users. The proposed scheme outperforms in the
presence of malicious users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2171</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2171</id><created>2013-03-09</created><authors><author><keyname>Kothapalli</keyname><forenames>Kishore</forenames></author><author><keyname>Banerjee</keyname><forenames>Dip Sankar</forenames></author><author><keyname>Narayanan</keyname><forenames>P. J.</forenames></author><author><keyname>Sood</keyname><forenames>Surinder</forenames></author><author><keyname>Bahl</keyname><forenames>Aman Kumar</forenames></author><author><keyname>Sharma</keyname><forenames>Shashank</forenames></author><author><keyname>Lad</keyname><forenames>Shrenik</forenames></author><author><keyname>Singh</keyname><forenames>Krishna Kumar</forenames></author><author><keyname>Matam</keyname><forenames>Kiran</forenames></author><author><keyname>Bharadwaj</keyname><forenames>Sivaramakrishna</forenames></author><author><keyname>Nigam</keyname><forenames>Rohit</forenames></author><author><keyname>Sakurikar</keyname><forenames>Parikshit</forenames></author><author><keyname>Deshpande</keyname><forenames>Aditya</forenames></author><author><keyname>Misra</keyname><forenames>Ishan</forenames></author><author><keyname>Choudhary</keyname><forenames>Siddharth</forenames></author><author><keyname>Gupta</keyname><forenames>Shubham</forenames></author></authors><title>CPU and/or GPU: Revisiting the GPU Vs. CPU Myth</title><categories>cs.DC</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel computing using accelerators has gained widespread research
attention in the past few years. In particular, using GPUs for general purpose
computing has brought forth several success stories with respect to time taken,
cost, power, and other metrics. However, accelerator based computing has
signifi- cantly relegated the role of CPUs in computation. As CPUs evolve and
also offer matching computational resources, it is important to also include
CPUs in the computation. We call this the hybrid computing model. Indeed, most
computer systems of the present age offer a degree of heterogeneity and
therefore such a model is quite natural.
  We reevaluate the claim of a recent paper by Lee et al.(ISCA 2010). We argue
that the right question arising out of Lee et al. (ISCA 2010) should be how to
use a CPU+GPU platform efficiently, instead of whether one should use a CPU or
a GPU exclusively. To this end, we experiment with a set of 13 diverse
workloads ranging from databases, image processing, sparse matrix kernels, and
graphs. We experiment with two different hybrid platforms: one consisting of a
6-core Intel i7-980X CPU and an NVidia Tesla T10 GPU, and another consisting of
an Intel E7400 dual core CPU with an NVidia GT520 GPU. On both these platforms,
we show that hybrid solutions offer good advantage over CPU or GPU alone
solutions. On both these platforms, we also show that our solutions are 90%
resource efficient on average.
  Our work therefore suggests that hybrid computing can offer tremendous
advantages at not only research-scale platforms but also the more realistic
scale systems with significant performance gains and resource efficiency to the
large scale user community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2173</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2173</id><created>2013-03-09</created><updated>2014-04-27</updated><authors><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>de Lima</keyname><forenames>Carlos H. Morais</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>Cardieri</keyname><forenames>Paulo</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Throughput Analysis of Cognitive Wireless Networks with Poisson
  Distributed Nodes Based on Location Information</title><categories>cs.IT cs.MA math.IT</categories><comments>Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a statistical characterization of the individual
achievable rates in bits/s/Hz and the spatial throughput of bipolar Poisson
wireless networks in bits/s/Hz/m$^2$. We assume that all transmitters have a
cognitive ability to know the distance to their receiver's closest interferers
so they can individually tune their coding rates to avoid outage events for
each spatial realization. Considering that the closest interferer approximates
the aggregate interference of all transmitters treated as noise, we derive
closed-form expressions for the probability density function of the achievable
rates under two decoding rules: treating interference as noise, and jointly
detecting the strongest interfering signals treating the others as noise. Based
on these rules and the bipolar model, we approximate the expected maximum
spatial throughput, showing the best performance of the latter decoding rule.
These results are also compared to the reference scenario where the
transmitters do not have cognitive ability, coding their messages at
predetermined rates that are chosen to optimize the expected spatial throughput
-- regardless of particular realizations -- which yields outages. We prove
that, when the same decoding rule and network density are considered, the
cognitive spatial throughput always outperforms the other option.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2175</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2175</id><created>2013-03-09</created><authors><author><keyname>Farahani</keyname><forenames>Samira Shirinabadi</forenames></author><author><keyname>Zarhoun</keyname><forenames>Ronak</forenames></author><author><keyname>Moaiyeri</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Navi</keyname><forenames>Keivan</forenames></author></authors><title>An efficient cntfet-based 7-input minority gate</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complementary metal oxide semiconductor technology (CMOS) has been faced
critical challenges in nano-scale regime. CNTFET (Carbon Nanotube Field effect
transistor) technology is a promising alternative for CMOS technology. In this
paper, we proposed a novel 7-input minority gate in CNTFET technology that has
only 9 CNTFETs. Minority function is utilized in the voting systems for
decision making and also it is used in data mining. This proposed 7-input
minority gate is utilized less fewer transistors than the conventional CMOS
method which utilizes many transistors for implementing sum of products. By
means of this proposed 7-input minority gate, a 4-input NAND gate can be
implemented, which gets better the conventional design in terms of delay and
energy efficiency and has much more deriving power at its output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2176</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2176</id><created>2013-03-09</created><authors><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Fu</keyname><forenames>Yan</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Scaling behavior of online human activity</title><categories>physics.soc-ph cs.SI</categories><journal-ref>EPL, 100 (2012) 48004</journal-ref><doi>10.1209/0295-5075/100/48004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development of Internet technology enables human explore the web
and record the traces of online activities. From the analysis of these
large-scale data sets (i.e. traces), we can get insights about dynamic behavior
of human activity. In this letter, the scaling behavior and complexity of human
activity in the e-commerce, such as music, book, and movie rating, are
comprehensively investigated by using detrended fluctuation analysis technique
and multiscale entropy method. Firstly, the interevent time series of rating
behaviors of these three type medias show the similar scaling property with
exponents ranging from 0.53 to 0.58, which implies that the collective
behaviors of rating media follow a process embodying self-similarity and
long-range correlation. Meanwhile, by dividing the users into three groups
based their activities (i.e., rating per unit time), we find that the scaling
exponents of interevent time series in three groups are different. Hence, these
results suggest the stronger long-range correlations exist in these collective
behaviors. Furthermore, their information complexities vary from three groups.
To explain the differences of the collective behaviors restricted to three
groups, we study the dynamic behavior of human activity at individual level,
and find that the dynamic behaviors of a few users have extremely small scaling
exponents associating with long-range anticorrelations. By comparing with the
interevent time distributions of four representative users, we can find that
the bimodal distributions may bring the extraordinary scaling behaviors. These
results of analyzing the online human activity in the e-commerce may not only
provide insights to understand its dynamic behaviors but also be applied to
acquire the potential economic interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2184</identifier>
 <datestamp>2014-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2184</id><created>2013-03-09</created><updated>2014-07-15</updated><authors><author><keyname>Bouboulis</keyname><forenames>Pantelis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author><author><keyname>Mavroforakis</keyname><forenames>Charalampos</forenames></author><author><keyname>Dalla</keyname><forenames>Leoni</forenames></author></authors><title>Complex Support Vector Machines for Regression and Quaternary
  Classification</title><categories>cs.LG stat.ML</categories><comments>Manuscript accepted in IEEE Transactions on Neural Networks and
  Learning Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new framework for complex Support Vector Regression as
well as Support Vector Machines for quaternary classification. The method
exploits the notion of widely linear estimation to model the input-out relation
for complex-valued data and considers two cases: a) the complex data are split
into their real and imaginary parts and a typical real kernel is employed to
map the complex data to a complexified feature space and b) a pure complex
kernel is used to directly map the data to the induced complex feature space.
The recently developed Wirtinger's calculus on complex reproducing kernel
Hilbert spaces (RKHS) is employed in order to compute the Lagrangian and derive
the dual optimization problem. As one of our major results, we prove that any
complex SVM/SVR task is equivalent with solving two real SVM/SVR tasks
exploiting a specific real kernel which is generated by the chosen complex
kernel. In particular, the case of pure complex kernels leads to the generation
of new kernels, which have not been considered before. In the classification
case, the proposed framework inherently splits the complex space into four
parts. This leads naturally in solving the four class-task (quaternary
classification), instead of the typical two classes of the real SVM. In turn,
this rationale can be used in a multiclass problem as a split-class scenario
based on four classes, as opposed to the one-versus-all method; this can lead
to significant computational savings. Experiments demonstrate the effectiveness
of the proposed framework for regression and classification tasks that involve
complex data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2201</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2201</id><created>2013-03-09</created><authors><author><keyname>D'Osualdo</keyname><forenames>Emanuele</forenames></author><author><keyname>Kochems</keyname><forenames>Jonathan</forenames></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>Automatic Verification of Erlang-Style Concurrency</title><categories>cs.PL</categories><comments>12 pages plus appendix, 4 figures, 1 table. The tool is available at
  http://mjolnir.cs.ox.ac.uk/soter/</comments><acm-class>D.2.4</acm-class><doi>10.1007/978-3-642-38856-9_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to verify safety properties of Erlang-style,
higher-order concurrent programs automatically. Inspired by Core Erlang, we
introduce Lambda-Actor, a prototypical functional language with
pattern-matching algebraic data types, augmented with process creation and
asynchronous message-passing primitives. We formalise an abstract model of
Lambda-Actor programs called Actor Communicating System (ACS) which has a
natural interpretation as a vector addition system, for which some verification
problems are decidable. We give a parametric abstract interpretation framework
for Lambda-Actor and use it to build a polytime computable, flow-based,
abstract semantics of Lambda-Actor programs, which we then use to bootstrap the
ACS construction, thus deriving a more accurate abstract model of the input
program. We have constructed Soter, a tool implementation of the verification
method, thereby obtaining the first fully-automatic, infinite-state model
checker for a core fragment of Erlang. We find that in practice our abstraction
technique is accurate enough to verify an interesting range of safety
properties. Though the ACS coverability problem is Expspace-complete, Soter can
analyse these verification problems surprisingly efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2202</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2202</id><created>2013-03-09</created><authors><author><keyname>Munoz-Canavate</keyname><forenames>Antonio</forenames></author><author><keyname>Hipola</keyname><forenames>Pedro</forenames></author></authors><title>Information Transfer in the Agricultural Sector in Spain</title><categories>cs.DL cs.CY</categories><journal-ref>Antonio Munoz-Canavate, Pedro Hipola (2010). Information Transfer
  in the Agricultural Sector in Spain. Journal of Agricultural &amp; Food
  Information, 11: 2, 123-142. DOI: 10.1080/10496501003682496</journal-ref><doi>10.1080/10496501003682496</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines the structures of information transfer to the
agricultural (production) and agro-alimentary (transformation and
commercialization of the products) sector within Spain. An historical
perspective is provided to better illustrate the reality and complexity of
Spain with regard to the systems of agrarian extension, agricultural research,
the resources provided by Spain's central administration, and the use of
information by related enterprises.
  The Service of Agrarian Extension appeared in Spain in the 1950s, and new
political-administrative structures (agribusiness associations, cooperatives)
were founded when Spain became a democratic nation in the late 1970s and with
the arrival of electronic information, largely in the 1990s. We describe the
research and technological centers supporting innovation in the agro-alimentary
sector and the communication media dedicated to the agricultural sector. The
article illustrates that the systems of agricultural information in Spain have
been largely derived from initiatives of the Public Administration, with few
private initiatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2211</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2211</id><created>2013-03-09</created><authors><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author><author><keyname>Acharjee</keyname><forenames>Suvojit</forenames></author><author><keyname>Biswas</keyname><forenames>Debalina</forenames></author><author><keyname>Das</keyname><forenames>Achintya</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Sheli Sinha</forenames></author></authors><title>Medical Information Embedding in Compressed Watermarked Intravascular
  Ultrasound Video</title><categories>cs.MM cs.CV</categories><comments>Pages-7 Fig.-15 Tables-2</comments><journal-ref>Scientific Bulletin of the Politehnica University of Timisoara -
  Transactions on Electronics and Communications p-ISSN 1583-3380 , vol.
  57(71), no. 2, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In medical field, intravascular ultrasound (IVUS) is a tomographic imaging
modality, which can identify the boundaries of different layers of blood
vessels. IVUS can detect myocardial infarction (heart attack) that remains
ignored and unattended when only angioplasty is done. During the past decade,
it became easier for some individuals or groups to copy and transmits digital
information without the permission of the owner. For increasing authentication
and security of copyrights, digital watermarking, an information hiding
technique, was introduced. Achieving watermarking technique with lesser amount
of distortion in biomedical data is a challenging task. Watermark can be
embedded into an image or in a video. As video data is a huge amount of
information, therefore a large storage area is needed which is not feasible. In
this case motion vector based video compression is done to reduce size. In this
present paper, an Electronic Patient Record (EPR) is embedded as watermark
within an IVUS video and then motion vector is calculated. This proposed method
proves robustness as the extracted watermark has good PSNR value and less MSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2215</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2215</id><created>2013-03-09</created><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Expensive Optimisation: A Metaheuristics Perspective</title><categories>cs.NE</categories><comments>7 pages</comments><msc-class>97R40</msc-class><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,ISSN-2156-5570(Online, Vol. 4, No. 2, 2013, pp. 203-209</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic, iterative search methods such as Evolutionary Algorithms (EAs)
are proven to be efficient optimizers. However, they require evaluation of the
candidate solutions which may be prohibitively expensive in many real world
optimization problems. Use of approximate models or surrogates is being
explored as a way to reduce the number of such evaluations. In this paper we
investigated three such methods. The first method (DAFHEA) partially replaces
an expensive function evaluation by its approximate model. The approximation is
realized with support vector machine (SVM) regression models. The second method
(DAFHEA II) is an enhancement on DAFHEA to accommodate for uncertain
environments. The third one uses surrogate ranking with preference learning or
ordinal regression. The fitness of the candidates is estimated by modeling
their rank. The techniques' performances on some of the benchmark numerical
optimization problems have been reported. The comparative benefits and
shortcomings of both techniques have been identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2219</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2219</id><created>2013-03-09</created><authors><author><keyname>Ryabko</keyname><forenames>Boris</forenames></author></authors><title>The Vernam cipher is robust to small deviations from randomness</title><categories>cs.CR cs.IT math.IT</categories><msc-class>94-XX Information and communication, circuits</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vernam cipher (or one-time pad) has played an important rule in
cryptography because it is a perfect secrecy system. For example, if an English
text (presented in binary system) $X_1 X_2 ... $ is enciphered according to the
formula $Z_i = (X_i + Y_i) \mod 2 $, where $Y_1 Y_2 ...$ is a key sequence
generated by the Bernoulli source with equal probabilities of 0 and 1, anyone
who knows $Z_1 Z_2 ... $ has no information about $X_1 X_2 ... $ without the
knowledge of the key $Y_1 Y_2 ...$. (The best strategy is to guess $X_1 X_2 ...
$ not paying attention to $Z_1 Z_2 ... $.)
  But what should one say about secrecy of an analogous method where the key
sequence $Y_1 Y_2 ...$ is generated by the Bernoulli source with a small bias,
say, $P(0) = 0.49, $ $ P(1) = 0.51$? To the best of our knowledge, there are no
theoretical estimates for the secrecy of such a system, as well as for the
general case where $X_1 X_2 ... $ (the plaintext) and key sequence are
described by stationary ergodic processes. We consider the running-key ciphers
where the plaintext and the key are generated by stationary ergodic sources and
show how to estimate the secrecy of such systems. In particular, it is shown
that, in a certain sense, the Vernam cipher is robust to small deviations from
randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2221</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2221</id><created>2013-03-09</created><authors><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Nefedov</keyname><forenames>Nikolai</forenames></author></authors><title>Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann
  Manifolds</title><categories>cs.LG cs.CV cs.SI stat.ML</categories><journal-ref>IEEE Transactions on Signal Processing, vol. 62, no. 4, pp.
  905-918, February 2014</journal-ref><doi>10.1109/TSP.2013.2295553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relationships between entities in datasets are often of multiple nature, like
geographical distance, social relationships, or common interests among people
in a social network, for example. This information can naturally be modeled by
a set of weighted and undirected graphs that form a global multilayer graph,
where the common vertex set represents the entities and the edges on different
layers capture the similarities of the entities in term of the different
modalities. In this paper, we address the problem of analyzing multi-layer
graphs and propose methods for clustering the vertices by efficiently merging
the information provided by the multiple modalities. To this end, we propose to
combine the characteristics of individual graph layers using tools from
subspace analysis on a Grassmann manifold. The resulting combination can then
be viewed as a low dimensional representation of the original data which
preserves the most important information from diverse relationships between
entities. We use this information in new clustering methods and test our
algorithm on several synthetic and real world datasets where we demonstrate
superior or competitive performances compared to baseline and state-of-the-art
techniques. Our generic framework further extends to numerous analysis and
learning problems that involve different types of information on graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2223</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2223</id><created>2013-03-09</created><authors><author><keyname>McDonald</keyname><forenames>Eric</forenames></author><author><keyname>Brown</keyname><forenames>C. Titus</forenames></author></authors><title>khmer: Working with Big Data in Bioinformatics</title><categories>cs.CE q-bio.GN</categories><comments>Invited chapter for forthcoming book on Performance of Open Source
  Applications</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce design and optimization considerations for the 'khmer' package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2238</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2238</id><created>2013-03-09</created><authors><author><keyname>Braeunig</keyname><forenames>Jean-Philippe</forenames><affiliation>CEA-DAM-DIF</affiliation></author><author><keyname>Crouseilles</keyname><forenames>Nicolas</forenames><affiliation>IRMAR, INRIA - IRMAR</affiliation></author><author><keyname>Grandgirard</keyname><forenames>Virginie</forenames><affiliation>IRFM</affiliation></author><author><keyname>Latu</keyname><forenames>Guillaume</forenames><affiliation>IRFM</affiliation></author><author><keyname>Mehrenberger</keyname><forenames>Michel</forenames><affiliation>INRIA Nancy - Grand Est / IECN / LSIIT / IRMA, IRMA</affiliation></author><author><keyname>Sonnendr&#xfc;cker</keyname><forenames>Eric</forenames><affiliation>INRIA Nancy - Grand Est / IECN / LSIIT / IRMA, IRMA</affiliation></author></authors><title>Some numerical aspects of the conservative PSM scheme in a 4D
  drift-kinetic code</title><categories>math.NA cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this work is simulation of magnetised plasmas in the ITER
project framework. In this context, kinetic Vlasov-Poisson like models are used
to simulate core turbulence in the tokamak in a toroidal geometry. This leads
to heavy simulations because a 6D dimensional problem has to be solved, even if
reduced to a 5D in so called gyrokinetic models. Accurate schemes, parallel
algorithms need to be designed to bear these simulations. This paper describes
the numerical studies to improve robustness of the conservative PSM scheme in
the context of its development in the GYSELA code. In this paper, we only
consider the 4D drift-kinetic model which is the backbone of the 5D gyrokinetic
models and relevant to build a robust and accurate numerical method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2242</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2242</id><created>2013-03-09</created><authors><author><keyname>Pais</keyname><forenames>Darren</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>Adaptive Network Dynamics and Evolution of Leadership in Collective
  Migration</title><categories>nlin.AO cs.SI physics.soc-ph q-bio.PE</categories><comments>Submitted to Physica D: Nonlinear Phenomena</comments><doi>10.1016/j.physd.2013.04.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of leadership in migratory populations depends not only on
costs and benefits of leadership investments but also on the opportunities for
individuals to rely on cues from others through social interactions. We derive
an analytically tractable adaptive dynamic network model of collective
migration with fast timescale migration dynamics and slow timescale adaptive
dynamics of individual leadership investment and social interaction. For large
populations, our analysis of bifurcations with respect to investment cost
explains the observed hysteretic effect associated with recovery of migration
in fragmented environments. Further, we show a minimum connectivity threshold
above which there is evolutionary branching into leader and follower
populations. For small populations, we show how the topology of the underlying
social interaction network influences the emergence and location of leaders in
the adaptive system. Our model and analysis can describe other adaptive network
dynamics involving collective tracking or collective learning of a noisy,
unknown signal, and likewise can inform the design of robotic networks where
agents use decentralized strategies that balance direct environmental
measurements with agent interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2251</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2251</id><created>2013-03-09</created><authors><author><keyname>You</keyname><forenames>Yang</forenames></author><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Duan</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Ningning</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>Zero-point attracting projection algorithm for sequential compressive
  sensing</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><journal-ref>IEICE Electronics Express, 9(4):314-319, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential Compressive Sensing, which may be widely used in sensing devices,
is a popular topic of recent research. This paper proposes an online recovery
algorithm for sparse approximation of sequential compressive sensing. Several
techniques including warm start, fast iteration, and variable step size are
adopted in the proposed algorithm to improve its online performance. Finally,
numerical simulations demonstrate its better performance than the relative art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2255</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2255</id><created>2013-03-09</created><authors><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>A Robust Zero-point Attraction LMS Algorithm on Near Sparse System
  Identification</title><categories>cs.IT math.IT</categories><comments>20 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The newly proposed $l_1$ norm constraint zero-point attraction Least Mean
Square algorithm (ZA-LMS) demonstrates excellent performance on exact sparse
system identification. However, ZA-LMS has less advantage against standard LMS
when the system is near sparse. Thus, in this paper, firstly the near sparse
system modeling by Generalized Gaussian Distribution is recommended, where the
sparsity is defined accordingly. Secondly, two modifications to the ZA-LMS
algorithm have been made. The $l_1$ norm penalty is replaced by a partial $l_1$
norm in the cost function, enhancing robustness without increasing the
computational complexity. Moreover, the zero-point attraction item is weighted
by the magnitude of estimation error which adjusts the zero-point attraction
force dynamically. By combining the two improvements, Dynamic Windowing ZA-LMS
(DWZA-LMS) algorithm is further proposed, which shows better performance on
near sparse system identification. In addition, the mean square performance of
DWZA-LMS algorithm is analyzed. Finally, computer simulations demonstrate the
effectiveness of the proposed algorithm and verify the result of theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2257</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2257</id><created>2013-03-09</created><authors><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author><author><keyname>Mei</keyname><forenames>Shunliang</forenames></author></authors><title>A stochastic gradient approach on compressive sensing signal
  reconstruction based on adaptive filtering framework</title><categories>cs.IT math.IT</categories><comments>28 pages, 8 figures</comments><journal-ref>IEEE Journal of Selected topics in Signal Processing,
  4(2):409-420, 2010</journal-ref><doi>10.1109/JSTSP.2009.2039173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the methodological similarity between sparse signal reconstruction
and system identification, a new approach for sparse signal reconstruction in
compressive sensing (CS) is proposed in this paper. This approach employs a
stochastic gradient-based adaptive filtering framework, which is commonly used
in system identification, to solve the sparse signal reconstruction problem.
Two typical algorithms for this problem: $l_0$-least mean square ($l_0$-LMS)
algorithm and $l_0$-exponentially forgetting window LMS ($l_0$-EFWLMS)
algorithm are hence introduced here. Both the algorithms utilize a zero
attraction method, which has been implemented by minimizing a continuous
approximation of $l_0$ norm of the studied signal. To improve the performances
of these proposed algorithms, an $l_0$-zero attraction projection ($l_0$-ZAP)
algorithm is also adopted, which has effectively accelerated their convergence
rates, making them much faster than the other existing algorithms for this
problem. Advantages of the proposed approach, such as its robustness against
noise etc., are demonstrated by numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2261</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2261</id><created>2013-03-09</created><authors><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Mei</keyname><forenames>Shunliang</forenames></author></authors><title>l_0 Norm Constraint LMS Algorithm for Sparse System Identification</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><journal-ref>IEEE Signal Processing Letters, 16(9):774-777, 2009</journal-ref><doi>10.1109/LSP.2009.2024736</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve the performance of Least Mean Square (LMS) based system
identification of sparse systems, a new adaptive algorithm is proposed which
utilizes the sparsity property of such systems. A general approximating
approach on $l_0$ norm -- a typical metric of system sparsity, is proposed and
integrated into the cost function of the LMS algorithm. This integration is
equivalent to add a zero attractor in the iterations, by which the convergence
rate of small coefficients, that dominate the sparse system, can be effectively
improved. Moreover, using partial updating method, the computational complexity
is reduced. The simulations demonstrate that the proposed algorithm can
effectively improve the performance of LMS-based identification algorithms on
sparse system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2270</identifier>
 <datestamp>2014-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2270</id><created>2013-03-09</created><updated>2014-04-06</updated><authors><author><keyname>Coucheney</keyname><forenames>Pierre</forenames></author><author><keyname>Gaujal</keyname><forenames>Bruno</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author></authors><title>Penalty-regulated dynamics and robust learning procedures in games</title><categories>math.OC cs.GT cs.LG</categories><comments>33 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from a heuristic learning scheme for N-person games, we derive a new
class of continuous-time learning dynamics consisting of a replicator-like
drift adjusted by a penalty term that renders the boundary of the game's
strategy space repelling. These penalty-regulated dynamics are equivalent to
players keeping an exponentially discounted aggregate of their on-going payoffs
and then using a smooth best response to pick an action based on these
performance scores. Owing to this inherent duality, the proposed dynamics
satisfy a variant of the folk theorem of evolutionary game theory and they
converge to (arbitrarily precise) approximations of Nash equilibria in
potential games. Motivated by applications to traffic engineering, we exploit
this duality further to design a discrete-time, payoff-based learning algorithm
which retains these convergence properties and only requires players to observe
their in-game payoffs: moreover, the algorithm remains robust in the presence
of stochastic perturbations and observation errors, and it does not require any
synchronization between players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2277</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2277</id><created>2013-03-09</created><authors><author><keyname>Gomes</keyname><forenames>Guilherme de Castro Mendes</forenames></author><author><keyname>de Oliveira</keyname><forenames>Vitor Campos</forenames></author><author><keyname>de Almeida</keyname><forenames>Jussara Marques</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Marcos Andr&#xe9;</forenames></author></authors><title>Is Learning to Rank Worth It? A Statistical Analysis of Learning to Rank
  Methods</title><categories>cs.IR</categories><comments>7 pages, 10 tables, 14 references. Original (short) paper published
  in the Brazilian Symposium on Databases, 2012 (SBBD2012). Current revision
  submitted to the Journal of Information and Data Management (JIDM)</comments><acm-class>H.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Learning to Rank (L2R) research field has experienced a fast paced growth
over the last few years, with a wide variety of benchmark datasets and
baselines available for experimentation. We here investigate the main
assumption behind this field, which is that, the use of sophisticated L2R
algorithms and models, produce significant gains over more traditional and
simple information retrieval approaches. Our experimental results surprisingly
indicate that many L2R algorithms, when put up against the best individual
features of each dataset, may not produce statistically significant
differences, even if the absolute gains may seem large. We also find that most
of the reported baselines are statistically tied, with no clear winner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2280</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2280</id><created>2013-03-09</created><updated>2014-01-23</updated><authors><author><keyname>Razeghi-Jahromi</keyname><forenames>Mohammad</forenames></author><author><keyname>Seyedi</keyname><forenames>Alireza</forenames></author></authors><title>Stabilization of Networked Control Systems with Sparse
  Observer-Controller Networks</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide a set of stability conditions for linear
time-invariant networked control systems with arbitrary topology, using a
Lyapunov direct approach. We then use these stability conditions to provide a
novel low-complexity algorithm for the design of a sparse observer-based
control network. We employ distributed observers by employing the output of
other nodes to improve the stability of each observer dynamics. To avoid
unbounded growth of controller and observer gains, we impose bounds on their
norms. The effects of relaxation of these bounds is discussed when trying to
find the complete decentralization conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2282</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2282</id><created>2013-03-09</created><authors><author><keyname>Zhang</keyname><forenames>Xiyong</forenames></author><author><keyname>Gao</keyname><forenames>Guangpu</forenames></author></authors><title>On the conjecture about the nonexistence of rotation symmetric bent
  functions</title><categories>cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a different approach to the proof of the
nonexistence of homogeneous rotation symmetric bent functions. As a result, we
obtain some new results which support the conjecture made in this journal,
i.e., there are no homogeneous rotation symmetric bent functions of degree &gt;2.
Also we characterize homogeneous degree 2 rotation symmetric bent functions by
using GCD of polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2284</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2284</id><created>2013-03-09</created><authors><author><keyname>Ai</keyname><forenames>Yuncan</forenames></author><author><keyname>Ai</keyname><forenames>Hannan</forenames></author><author><keyname>Meng</keyname><forenames>Fanmei</forenames></author><author><keyname>Zhao</keyname><forenames>Lei</forenames></author></authors><title>GenomeFingerprinter and universal genome fingerprint analysis for
  systematic comparative genomics</title><categories>q-bio.GN cs.CE math.NA</categories><comments>63 pages, 15 figures, 5 tables</comments><doi>10.1371/journal.pone.0077912</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  How to compare whole genome sequences at large scale has not been achieved
via conventional methods based on pair-wisely base-to-base comparison;
nevertheless, no attention was paid to handle in-one-sitting a number of
genomes crossing genetic category (chromosome, plasmid, and phage) with farther
divergences (much less or no homologous) over large size ranges (from Kbp to
Mbp). We created a new method, GenomeFingerprinter, to unambiguously produce
three-dimensional coordinates from a sequence, followed by one
three-dimensional plot and six two-dimensional trajectory projections to
illustrate whole genome fingerprints. We further developed a set of concepts
and tools and thereby established a new method, universal genome fingerprint
analysis. We demonstrated their applications through case studies on over a
hundred of genome sequences. Particularly, we defined the total genetic
component configuration (TGCC) (i.e., chromosome, plasmid, and phage) for
describing a strain as a system, and the universal genome fingerprint map
(UGFM) of TGCC for differentiating a strain as a universal system, as well as
the systematic comparative genomics (SCG) for comparing in-one-sitting a number
of genomes crossing genetic category in diverse strains. By using UGFM,
UGFM-TGCC, and UGFM-TGCC-SCG, we compared a number of genome sequences with
farther divergences (chromosome, plasmid, and phage; bacterium, archaeal
bacterium, and virus) over large size ranges (6Kbp~5Mbp), giving new insights
into critical problematic issues in microbial genomics in the post-genomic era.
This paper provided a new method for rapidly computing, geometrically
visualizing, and intuitively comparing genome sequences at fingerprint level,
and hence established a new method of universal genome fingerprint analysis for
systematic comparative genomics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2285</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2285</id><created>2013-03-09</created><authors><author><keyname>Green</keyname><forenames>Oded</forenames></author><author><keyname>David</keyname><forenames>Lior</forenames></author><author><keyname>Galperin</keyname><forenames>Ami</forenames></author><author><keyname>Birk</keyname><forenames>Yitzhak</forenames></author></authors><title>Efficient Parallel Computation of the Estimated Covariance Matrix</title><categories>cs.DS cs.DM</categories><acm-class>B.3.2; D.1.3; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation of a signal's estimated covariance matrix is an important
building block in signal processing, e.g., for spectral estimation. Each matrix
element is a sum of products of elements in the input matrix taken over a
sliding window. Any given product contributes to multiple output elements,
thereby complicating parallelization. We present a novel algorithm that attains
very high parallelism without repeating multiplications or requiring inter-core
synchronization. Key to this is the assignment to each core of distinct
diagonal segments of the output matrix, selected such that no multiplications
need to be repeated yet only one core writes to any given output-matrix
element, and exploitation of a shared memory (including L1 cache) that obviates
the need for a corresponding awkward partitioning of the memory among cores.
Implementation on Plurality's HyperCore shared-memory many-core architecture
demonstrates linear speedup of up to 64 cores and speedups of ~85X for 128
cores. On an x86 system we demonstrate that the new algorithm has consider
parallel speedups but also show that a sequential implementation of the new
algorithm outperforms the parallel implementation of the baseline approach. On
a quad-core x86 system, the new algorithm is 20X faster than sequential
baseline and 5X than parallel implementation of the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2289</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2289</id><created>2013-03-10</created><updated>2014-03-15</updated><authors><author><keyname>Nedic</keyname><forenames>Angelia</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Distributed optimization over time-varying directed graphs</title><categories>math.OC cs.DC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed optimization by a collection of nodes, each having
access to its own convex function, whose collective goal is to minimize the sum
of the functions. The communications between nodes are described by a
time-varying sequence of directed graphs, which is uniformly strongly
connected. For such communications, assuming that every node knows its
out-degree, we develop a broadcast-based algorithm, termed the
subgradient-push, which steers every node to an optimal value under a standard
assumption of subgradient boundedness. The subgradient-push requires no
knowledge of either the number of agents or the graph sequence to implement.
Our analysis shows that the subgradient-push algorithm converges at a rate of
$O(\ln(t)/\sqrt{t})$, where the constant depends on the initial values at the
nodes, the subgradient norms, and, more interestingly, on both the consensus
speed and the imbalances of influence among the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2292</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2292</id><created>2013-03-10</created><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Das</keyname><forenames>Karen</forenames></author><author><keyname>Raheja</keyname><forenames>Sonia</forenames></author></authors><title>Intelligent Approaches to interact with Machines using Hand Gesture
  Recognition in Natural way: A Survey</title><categories>cs.HC cs.CV</categories><journal-ref>IJCSES, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand gestures recognition (HGR) is one of the main areas of research for the
engineers, scientists and bioinformatics. HGR is the natural way of Human
Machine interaction and today many researchers in the academia and industry are
working on different application to make interactions more easy, natural and
convenient without wearing any extra device. HGR can be applied from games
control to vision enabled robot control, from virtual reality to smart home
systems. In this paper we are discussing work done in the area of hand gesture
recognition where focus is on the intelligent approaches including soft
computing based methods like artificial neural network, fuzzy logic, genetic
algorithms etc. The methods in the preprocessing of image for segmentation and
hand image construction also taken into study. Most researchers used fingertips
for hand detection in appearance based modeling. Finally the comparison of
results given by different researchers is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2294</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2294</id><created>2013-03-10</created><authors><author><keyname>Safaei</keyname><forenames>Farshad</forenames></author><author><keyname>Sotoodeh</keyname><forenames>Hamidreza</forenames></author></authors><title>Comprehensive Analysis on the Vulnerability and Efficiency of P2P
  Networks under Static Failures and Targeted Attacks</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Peer to peer systems are the networks consisting of a group of nodes possible
to be as wide as the Internet. These networks are required of evaluation
mechanisms and distributed control and configurations, so each peer will be
able to communicate with other peers. Resilience to faults, failures and
attacks, are the main requirements of most communication systems and networks
today. Thus, since P2P networks can be individually used as an infrastructure
and an alternative for many other communication networks, they have to be more
reliable, and resilient to the faults, failures and attacks compared to the
client and server approach. In this work, we present a detailed study on the
behavior of various P2P networks toward faults and failures, and focus on
fault-tolerance subject. We consider two different static failure scenarios:
a)a random strategy in which nodes or edges of the network will be removed with
an equal probability and without any knowledge of the networks infrastructure,
b)a targeted strategy that uses some information about the nodes, and in which
the nodes with the highest degree have the most priority to be attacked. By
static faults, we mean a situation where the nodes or components encounter some
faults before the network starts to work or through its operation, and will
remain faulty to the end of the work session. Our goal is to introduce various
measures to analyzing P2P networks evaluating their vulnerability rate. The
presented criteria can be used for evaluating the reliability and vulnerability
of P2P networks toward both random and targeted failures. There is no limit to
the number and types of failures, the presented measures are able to be used
for different types of failures and even a wide range of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2308</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2308</id><created>2013-03-10</created><updated>2014-04-15</updated><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Improving adaptation of ubiquitous recommander systems by using
  reinforcement learning and collaborative filtering</title><categories>cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1301.4351</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide development of mobile applications provides a considerable amount of
data of all types (images, texts, sounds, videos, etc.). Thus, two main issues
have to be considered: assist users in finding information and reduce search
and navigation time. In this sense, context-based recommender systems (CBRS)
propose the user the adequate information depending on her/his situation. Our
work consists in applying machine learning techniques and reasoning process in
order to bring a solution to some of the problems concerning the acceptance of
recommender systems by users, namely avoiding the intervention of experts,
reducing cold start problem, speeding learning process and adapting to the
user's interest. To achieve this goal, we propose a fundamental modification in
terms of how we model the learning of the CBRS. Inspired by models of human
reasoning developed in robotic, we combine reinforcement learning and
case-based reasoning to define a contextual recommendation process based on
different context dimensions (cognitive, social, temporal, geographic). This
paper describes an ongoing work on the implementation of a CBRS based on a
hybrid Q-learning (HyQL) algorithm which combines Q-learning, collaborative
filtering and case-based reasoning techniques. It also presents preliminary
results by comparing HyQL and the standard Q-Learning w.r.t. solving the cold
start problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2309</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2309</id><created>2013-03-10</created><authors><author><keyname>Montorsi</keyname><forenames>Francesco</forenames></author><author><keyname>Mazuelas</keyname><forenames>Santiago</forenames></author><author><keyname>Vitetta</keyname><forenames>Giorgio M.</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>On the Performance Limits of Map-Aware Localization</title><categories>cs.IT math.IT stat.AP</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Establishing bounds on the accuracy achievable by localization techniques
represents a fundamental technical issue. Bounds on localization accuracy have
been derived for cases in which the position of an agent is estimated on the
basis of a set of observations and, possibly, of some a priori information
related to them (e.g., information about anchor positions and properties of the
communication channel). In this manuscript new bounds are derived under the
assumption that the localization system is map-aware, i.e., it can benefit not
only from the availability of observations, but also from the a priori
knowledge provided by the map of the environment where it operates. Our results
show that: a) map-aware estimation accuracy can be related to some features of
the map (e.g., its shape and area) even though, in general, the relation is
complicated; b) maps are really useful in the presence of some combination of
low signal-to-noise ratios and specific geometrical features of the map (e.g.,
the size of obstructions); c) in most cases, there is no need of refined maps
since additional details do not improve estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2310</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2310</id><created>2013-03-10</created><authors><author><keyname>Li</keyname><forenames>Xiaohui</forenames></author><author><keyname>Ceikute</keyname><forenames>Vaida</forenames></author><author><keyname>Jensen</keyname><forenames>Christian S.</forenames></author><author><keyname>Tan</keyname><forenames>Kian-Lee</forenames></author></authors><title>Trajectory Based Optimal Segment Computation in Road Network Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a location for a new facility such that the facility attracts the
maximal number of customers is a challenging problem. Existing studies either
model customers as static sites and thus do not consider customer movement, or
they focus on theoretical aspects and do not provide solutions that are shown
empirically to be scalable. Given a road network, a set of existing facilities,
and a collection of customer route traversals, an optimal segment query returns
the optimal road network segment(s) for a new facility. We propose a practical
framework for computing this query, where each route traversal is assigned a
score that is distributed among the road segments covered by the route
according to a score distribution model. The query returns the road segment(s)
with the highest score. To achieve low latency, it is essential to prune the
very large search space. We propose two algorithms that adopt different
approaches to computing the query. Algorithm AUG uses graph augmentation, and
ITE uses iterative road-network partitioning. Empirical studies with real data
sets demonstrate that the algorithms are capable of offering high performance
in realistic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2314</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2314</id><created>2013-03-10</created><authors><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Bijral</keyname><forenames>Avleen</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Mini-Batch Primal and Dual Methods for SVMs</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of using mini-batches in stochastic optimization of
SVMs. We show that the same quantity, the spectral norm of the data, controls
the parallelization speedup obtained for both primal stochastic subgradient
descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it
to derive novel variants of mini-batched SDCA. Our guarantees for both methods
are expressed in terms of the original nonsmooth primal problem based on the
hinge-loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2317</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2317</id><created>2013-03-10</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Proposition d'une technique de gestion de projet dans les startups</title><categories>cs.OH</categories><comments>in French</comments><acm-class>K.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project is part of the development of mobile CRM. It aims to develop a
management application client named NOMALYS. This application allows the
commercial and business leaders to see their CRM Mobile. We have focused in
this project on the techniques of projects management, this study allowed to
classify different techniques for managing software projects and proposed the
most closely technique that match the needs of the studied company.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2330</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2330</id><created>2013-03-10</created><authors><author><keyname>Sreelakshmi</keyname><forenames>M. S.</forenames></author><author><keyname>Venkataraman</keyname><forenames>D.</forenames></author></authors><title>Image compression using anti-forensics method</title><categories>cs.MM cs.CV</categories><comments>9 pages 8 figures IJCSEA journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of image forensics methods are available which are capable of
identifying image tampering. But these techniques are not capable of addressing
the anti-forensics method which is able to hide the trace of image tampering.
In this paper anti-forensics method for digital image compression has been
proposed. This anti-forensics method is capable of removing the traces of image
compression. Additionally, technique is also able to remove the traces of
blocking artifact that are left by image compression algorithms that divide an
image into segments during compression process. This method is targeted to
remove the compression fingerprints of JPEG compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2364</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2364</id><created>2013-03-10</created><authors><author><keyname>Jankowski</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>The Multidimensional Study of Viral Campaigns as Branching Processes</title><categories>cs.SI physics.soc-ph</categories><comments>In proceedings of the 4th International Conference on Social
  Informatics, SocInfo 2012</comments><journal-ref>Jankowski, J., Michalski, R., Kazienko, P.: The Multidimensional
  Study of Viral Campaigns as Branching Processes. K. Aberer et al. (Eds.):
  LNCS, vol. 7710, pp. 462-474, Springer, Berlin Heidelberg (2012)</journal-ref><doi>10.1007/978-3-642-35386-4_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Viral campaigns on the Internet may follow variety of models, depending on
the content, incentives, personal attitudes of sender and recipient to the
content and other factors. Due to the fact that the knowledge of the campaign
specifics is essential for the campaign managers, researchers are constantly
evaluating models and real-world data. The goal of this article is to present
the new knowledge obtained from studying two viral campaigns that took place in
a virtual world which followed the branching process. The results show that it
is possible to reduce the time needed to estimate the model parameters of the
campaign and, moreover, some important aspects of time-generations relationship
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2365</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2365</id><created>2013-03-10</created><authors><author><keyname>Jankowski</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Ciuberek</keyname><forenames>Sylwia</forenames></author><author><keyname>Zbieg</keyname><forenames>Anita</forenames></author><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author></authors><title>Studying Paths of Participation in Viral Diffusion Process</title><categories>cs.SI physics.soc-ph</categories><comments>In proceedings of the 4th International Conference on Social
  Informatics, SocInfo 2012</comments><journal-ref>Jankowski, J., Ciuberek, S., Zbieg, A., Michalski, R.: Studying
  Paths of Participation in Viral Diffusion Process. K. Aberer et al. (Eds.):
  LNCS, vol. 7710, pp. 503-516, Springer, Berlin Heidelberg (2012)</journal-ref><doi>10.1007/978-3-642-35386-4_37</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authors propose a conceptual model of participation in viral diffusion
process composed of four stages: awareness, infection, engagement and action.
To verify the model it has been applied and studied in the virtual social chat
environment settings. The study investigates the behavioral paths of actions
that reflect the stages of participation in the diffusion and presents
shortcuts, that lead to the final action, i.e. the attendance in a virtual
event. The results show that the participation in each stage of the process
increases the probability of reaching the final action. Nevertheless, the
majority of users involved in the virtual event did not go through each stage
of the process but followed the shortcuts. That suggests that the viral
diffusion process is not necessarily a linear sequence of human actions but
rather a dynamic system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2369</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2369</id><created>2013-03-10</created><authors><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Jankowski</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Negative Effects of Incentivised Viral Campaigns for Activity in Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>In proceedings of the 2nd International Conference on Social
  Computing and its Applications, SCA 2012</comments><journal-ref>Michalski, R., Jankowski, J., Kazienko, P.: Negative Effects of
  Incentivised Viral Campaigns for Activity in Social Networks. IEEE Computer
  Society, pp. 391-398 (2012)</journal-ref><doi>10.1109/CGC.2012.95</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Viral campaigns are crucial methods for word-of-mouth marketing in social
communities. The goal of these campaigns is to encourage people for activity.
The problem of incentivised and non-incentivised campaigns is studied in the
paper. Based on the data collected within the real social networking site both
approaches were compared. The experimental results revealed that a highly
motivated campaign not necessarily provides better results due to overlapping
effect. Additional studies have shown that the behaviour of individual
community members in the campaign based on their service profile can be
predicted but the classification accuracy may be limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2379</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2379</id><created>2013-03-10</created><updated>2013-05-13</updated><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>Conditions for Robustness of Polar Codes in the Presence of Channel
  Mismatch</title><categories>cs.IT math.IT</categories><comments>Shorter version of this paper will be presented at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A challenging problem related to the design of polar codes is &quot;robustness
against channel parameter variations&quot; as stated in Ar{\i}kan's original work.
In this paper, we describe how the problem of robust polar code design can be
viewed as a mismatch decoding problem. We propose conditions which ensure a
polar encoder/decoder designed for a mismatched B-DMC can be used to
communicate reliably. In particular, the analysis shows that the original polar
code construction method is robust over the class of binary symmetric channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2389</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2389</id><created>2013-03-10</created><authors><author><keyname>Taeb</keyname><forenames>Armeen</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>Maximin Analysis of Message Passing Algorithms for Recovering Block
  Sparse Signals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering a block (or group) sparse signal from
an underdetermined set of random linear measurements, which appear in
compressed sensing applications such as radar and imaging. Recent results of
Donoho, Johnstone, and Montanari have shown that approximate message passing
(AMP) in combination with Stein's shrinkage outperforms group LASSO for large
block sizes. In this paper, we prove that, for a fixed block size and in the
strong undersampling regime (i.e., having very few measurements compared to the
ambient dimension), AMP cannot improve upon group LASSO, thereby complementing
the results of Donoho et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2395</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2395</id><created>2013-03-10</created><authors><author><keyname>Sun</keyname><forenames>Xu</forenames></author><author><keyname>Duan</keyname><forenames>Jinqiao</forenames></author><author><keyname>Li</keyname><forenames>Xiaofan</forenames></author><author><keyname>Wang</keyname><forenames>Xiangjun</forenames></author></authors><title>State estimation under non-Gaussian Levy noise: A modified Kalman
  filtering method</title><categories>math.DS cs.IT cs.LG math.IT math.PR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kalman filter is extensively used for state estimation for linear systems
under Gaussian noise. When non-Gaussian L\'evy noise is present, the
conventional Kalman filter may fail to be effective due to the fact that the
non-Gaussian L\'evy noise may have infinite variance. A modified Kalman filter
for linear systems with non-Gaussian L\'evy noise is devised. It works
effectively with reasonable computational cost. Simulation results are
presented to illustrate this non-Gaussian filtering method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2409</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2409</id><created>2013-03-10</created><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Lin</keyname><forenames>Feng</forenames></author><author><keyname>Peng</keyname><forenames>Kemao</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author><author><keyname>Lee</keyname><forenames>Tong H.</forenames></author></authors><title>Finite-time Stabilization of Circular Formations using Bearing-only
  Measurements</title><categories>cs.SY math.OC</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies decentralized formation control of multiple vehicles when
each vehicle can only measure the local bearings of their neighbors by using
bearing-only sensors. Since the inter-vehicle distance cannot be measured, the
target formation involves no distance constraints. More specifically, the
target formation considered in this paper is an angle-constrained circular
formation, where each vehicle has exactly two neighbors and the angle at each
vehicle subtended by its two neighbors is pre-specified. To stabilize the
target formation, we propose a discontinuous control law that only requires the
sign information of the angle errors. Due to the discontinuity of the proposed
control law, the stability of the closed-loop system is analyzed by employing a
locally Lipschitz Lyapunov function and nonsmooth analysis tools. We prove that
the target formation is locally finite-time stable with collision avoidance
guaranteed. The evolution of the vehicle positions in the plane is also
characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2413</identifier>
 <datestamp>2013-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2413</id><created>2013-03-10</created><authors><author><keyname>Wu</keyname><forenames>Lu-Lu</forenames></author><author><keyname>Zhou</keyname><forenames>Hai-Jun</forenames></author><author><keyname>Alava</keyname><forenames>Mikko</forenames></author><author><keyname>Aurell</keyname><forenames>Erik</forenames></author><author><keyname>Orponen</keyname><forenames>Pekka</forenames></author></authors><title>Witness of unsatisfiability for a random 3-satisfiability formula</title><categories>cs.CC cond-mat.dis-nn</categories><comments>9 pages, 7 figures included. Submitted to Physical Review E</comments><journal-ref>Physical Review E 87, 052807 (2013)</journal-ref><doi>10.1103/PhysRevE.87.052807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random 3-satisfiability (3-SAT) problem is in the unsatisfiable (UNSAT)
phase when the clause density $\alpha$ exceeds a critical value $\alpha_s
\approx 4.267$. However, rigorously proving the unsatisfiability of a given
large 3-SAT instance is extremely difficult. In this paper we apply the
mean-field theory of statistical physics to the unsatisfiability problem, and
show that a specific type of UNSAT witnesses (Feige-Kim-Ofek witnesses) can in
principle be constructed when the clause density $\alpha &gt; 19$. We then
construct Feige-Kim-Ofek witnesses for single 3-SAT instances through a simple
random sampling algorithm and a focused local search algorithm. The random
sampling algorithm works only when $\alpha$ scales at least linearly with the
variable number $N$, but the focused local search algorithm works for clause
densty $\alpha &gt; c N^{b}$ with $b \approx 0.59$ and prefactor $c \approx 8$.
The exponent $b$ can be further decreased by enlarging the single parameter $S$
of the focused local search algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2414</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2414</id><created>2013-03-10</created><updated>2015-09-10</updated><authors><author><keyname>Weng</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Djuric</keyname><forenames>Petar</forenames></author></authors><title>A Bayesian Approach to Data Fusion in Sensor Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the fusion problem in wireless sensor networks,
where the cross-correlation between the estimates is unknown. To solve the
problem within the Bayesian framework, we assume that the covariance matrix has
a prior distribution. We also assume that we know the covariance of each
estimate, i.e., the diagonal block of the entire covariance matrix (of the
random vector consisting of the two estimates). We then derive the conditional
distribution of the off-diagonal blocks, which is the cross-correlation of our
interest. We show that when there are two nodes, the conditional distribution
happens to be the inverted matrix variate $t$-distribution, from which we can
readily sample. For more than two nodes, the conditional distribution is no
longer the inverted matrix variate $t$-distribution. But we show that we can
decompose it into several sampling problems, each of which is the inverted
matrix variate $t$-distribution and therefore we can still sample from it.
Since we can sample from this distribution, it enables us to use the Monte
Carlo method to compute the minimum mean square error estimate for the fusion
problem. We use two models to generate experiment data and demonstrate the
generality of our method. Simulation results show that the proposed method
works better than the popular covariance intersection method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2416</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2416</id><created>2013-03-10</created><updated>2013-08-05</updated><authors><author><keyname>Keenan</keyname><forenames>Alexandra</forenames></author><author><keyname>Schweller</keyname><forenames>Robert</forenames></author><author><keyname>Sherman</keyname><forenames>Michael</forenames></author><author><keyname>Zhong</keyname><forenames>Xingsi</forenames></author></authors><title>Fast Arithmetic in Algorithmic Self-Assembly</title><categories>cs.DS cs.CC cs.CG</categories><acm-class>F.1.1; F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the time complexity of computing the sum and
product of two $n$-bit numbers within the tile self-assembly model. The
(abstract) tile assembly model is a mathematical model of self-assembly in
which system components are square tiles with different glue types assigned to
tile edges. Assembly is driven by the attachment of singleton tiles to a
growing seed assembly when the net force of glue attraction for a tile exceeds
some fixed threshold. Within this frame work, we examine the time complexity of
computing the sum or product of 2 n-bit numbers, where the input numbers are
encoded in an initial seed assembly, and the output is encoded in the final,
terminal assembly of the system. We show that the problems of addition and
multiplication have worst case lower bounds of $\Omega(\sqrt{n})$ in 2D
assembly, and $\Omega(\sqrt[3]{n})$ in 3D assembly. In the case of addition, we
design algorithms for both 2D and 3D that meet this bound with worst case run
times of $O(\sqrt{n})$ and $O(\sqrt[3]{n})$ respectively, which beats the
previous best known upper bound of O(n). Further, we consider average case
complexity of addition over uniformly distributed n-bit strings and show how to
achieve $O(\log n)$ average case time with a simultaneous $O(\sqrt{n})$ worst
case run time in 2D. For multiplication, we present an $O(n^{5/6})$ time
multiplication algorithm which works in 3D, which beats the previous best known
upper bound of O(n). As additional evidence for the speed of our algorithms, we
implement our addition algorithms, along with the simpler O(n) time addition
algorithm, into a probabilistic run-time simulator and compare the timing
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2417</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2417</id><created>2013-03-10</created><authors><author><keyname>Jin</keyname><forenames>Xiao-Bo</forenames></author><author><keyname>Geng</keyname><forenames>Guang-Gang</forenames></author></authors><title>Linear NDCG and Pair-wise Loss</title><categories>cs.LG stat.ML</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear NDCG is used for measuring the performance of the Web content quality
assessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will prove
that the DCG error equals a new pair-wise loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2430</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2430</id><created>2013-03-11</created><updated>2013-05-19</updated><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author></authors><title>Quantum and Concept Combination, Entangled Measurements and Prototype
  Theory</title><categories>cs.AI cs.CL quant-ph</categories><comments>5 pages, 1 figure</comments><journal-ref>Topics in Cognitive Science, 6, pp. 129-137, 2014</journal-ref><doi>10.1111/tops.12073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the meaning of the violation of the marginal probability law for
situations of correlation measurements where entanglement is identified. We
show that for quantum theory applied to the cognitive realm such a violation
does not lead to the type of problems commonly believed to occur in situations
of quantum theory applied to the physical realm. We briefly situate our quantum
approach for modeling concepts and their combinations with respect to the
notions of 'extension' and 'intension' in theories of meaning, and in existing
concept theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2437</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2437</id><created>2013-03-11</created><authors><author><keyname>Paul</keyname><forenames>Joseph Suresh</forenames></author><author><keyname>Pillai</keyname><forenames>Uma Krishna Swamy</forenames></author><author><keyname>Thomas</keyname><forenames>Nyjin</forenames></author></authors><title>Least-Squares FIR Models of Low-Resolution MR data for Efficient
  Phase-Error Compensation with Simultaneous Artefact Removal</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal space models in both phase-encode, and frequency-encode directions are
presented for extrapolation of 2D partial kspace. Using the boxcar
representation of low-resolution spatial data, and a geometrical representation
of signal space vectors in both positive and negative phase-encode directions,
a robust predictor is constructed using a series of signal space projections.
Compared to some of the existing phase-correction methods that require
acquisition of a pre-determined set of fractional kspace lines, the proposed
predictor is found to be more efficient, due to its capability of exhibiting an
equivalent degree of performance using only half the number of fractional
lines. Robust filtering of noisy data is achieved using a second signal space
model in the frequency-encode direction, bypassing the requirement of a prior
highpass filtering operation. The signal space is constructed from Fourier
Transformed samples of each row in the low-resolution image. A set of FIR
filters are estimated by fitting a least squares model to this signal space.
Partial kspace extrapolation using the FIR filters is shown to result in
artifact-free reconstruction, particularly in respect of Gibbs ringing and
streaking type artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2438</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2438</id><created>2013-03-11</created><updated>2014-04-03</updated><authors><author><keyname>Geng</keyname><forenames>Guang-Gang</forenames></author><author><keyname>Yang</keyname><forenames>Xiu-Tao</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Meng</keyname><forenames>Chi-Jie</forenames></author></authors><title>A Taxonomy of Hyperlink Hiding Techniques</title><categories>cs.IR</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden links are designed solely for search engines rather than visitors. To
get high search engine rankings, link hiding techniques are usually used for
the profitability of black industries, such as illicit game servers, false
medical services, illegal gambling, and less attractive high-profit industry,
etc. This paper investigates hyperlink hiding techniques on the Web, and gives
a detailed taxonomy. We believe the taxonomy can help develop appropriate
countermeasures. Study on 5,583,451 Chinese sites' home pages indicate that
link hidden techniques are very prevalent on the Web. We also tried to explore
the attitude of Google towards link hiding spam by analyzing the PageRank
values of relative links. The results show that more should be done to punish
the hidden link spam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2439</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2439</id><created>2013-03-11</created><authors><author><keyname>Paul</keyname><forenames>Joseph Suresh</forenames></author><author><keyname>Mathew</keyname><forenames>Joshin John</forenames></author><author><keyname>Naroth</keyname><forenames>Souparnika Kandoth</forenames></author><author><keyname>Kesavadas</keyname><forenames>Chandrasekar</forenames></author></authors><title>Voxel-wise Weighted MR Image Enhancement using an Extended Neighborhood
  Filter</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an edge preserving and denoising filter for enhancing the features
in images, which contain an ROI having a narrow spatial extent. Typical
examples include angiograms, or ROI spatially distributed in multiple locations
and contained within an outlying region, such as in multiple-sclerosis. The
filtering involves determination of multiplicative weights in the spatial
domain using an extended set of neighborhood directions. Equivalently, the
filtering operation may be interpreted as a combination of directional filters
in the frequency domain, with selective weighting for spatial frequencies
contained within each direction. The advantages of the proposed filter in
comparison to specialized non-linear filters, which operate on diffusion
principle, are illustrated using numerical phantom data. The performance
evaluation is carried out on simulated images from BrainWeb database for
multiple-sclerosis, acute ischemic stroke using clinically acquired FLAIR
images and MR angiograms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2446</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2446</id><created>2013-03-11</created><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Barbano</keyname><forenames>Paolo Emilio</forenames></author><author><keyname>Nagy</keyname><forenames>Mate Levente</forenames></author><author><keyname>Krauthammer</keyname><forenames>Michael</forenames></author></authors><title>Broadening the Scope of Nanopublications</title><categories>cs.DL cs.IR</categories><comments>To appear in the Proceedings of the 10th Extended Semantic Web
  Conference (ESWC 2013)</comments><report-no>LNCS 7882</report-no><doi>10.1007/978-3-642-38288-8_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach for extending the existing concept of
nanopublications --- tiny entities of scientific results in RDF representation
--- to broaden their application range. The proposed extension uses English
sentences to represent informal and underspecified scientific claims. These
sentences follow a syntactic and semantic scheme that we call AIDA (Atomic,
Independent, Declarative, Absolute), which provides a uniform and succinct
representation of scientific assertions. Such AIDA nanopublications are
compatible with the existing nanopublication concept and enjoy most of its
advantages such as information sharing, interlinking of scientific findings,
and detailed attribution, while being more flexible and applicable to a much
wider range of scientific results. We show that users are able to create AIDA
sentences for given scientific results quickly and at high quality, and that it
is feasible to automatically extract and interlink AIDA nanopublications from
existing unstructured data sources. To demonstrate our approach, a web-based
interface is introduced, which also exemplifies the use of nanopublications for
non-scientific content, including meta-nanopublications that describe other
nanopublications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2447</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2447</id><created>2013-03-11</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Compton</keyname><forenames>Michael</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Context-aware Sensor Search, Selection and Ranking Model for Internet of
  Things Middleware</title><categories>cs.NI</categories><journal-ref>Proceedings of the IEEE 14th International Conference on Mobile
  Data Management (MDM), Milan, Italy, June, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As we are moving towards the Internet of Things (IoT), the number of sensors
deployed around the world is growing at a rapid pace. Market research has shown
a significant growth of sensor deployments over the past decade and has
predicted a substantial acceleration of the growth rate in the future. It is
also evident that the increasing number of IoT middleware solutions are
developed in both research and commercial environments. However, sensor search
and selection remain a critical requirement and a challenge. In this paper, we
present CASSARAM, a context-aware sensor search, selection, and ranking model
for Internet of Things to address the research challenges of selecting sensors
when large numbers of sensors with overlapping and sometimes redundant
functionality are available. CASSARAM proposes the search and selection of
sensors based on user priorities. CASSARAM considers a broad range of
characteristics of sensors for search such as reliability, accuracy, battery
life just to name a few. Our approach utilises both semantic querying and
quantitative reasoning techniques. User priority based weighted Euclidean
distance comparison in multidimensional space technique is used to index and
rank sensors. Our objectives are to highlight the importance of sensor search
in IoT paradigm, identify important characteristics of both sensors and data
acquisition processes which help to select sensors, understand how semantic and
statistical reasoning can be combined together to address this problem in an
efficient manner. We developed a tool called CASSARA to evaluate the proposed
model in terms of resource consumption and response time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2448</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2448</id><created>2013-03-11</created><authors><author><keyname>Bel</keyname><forenames>N&#xfa;ria</forenames></author><author><keyname>Coll</keyname><forenames>Maria</forenames></author><author><keyname>Resnik</keyname><forenames>Gabriela</forenames></author></authors><title>Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon
  Production</title><categories>cs.CL</categories><comments>7 pages, 2 figures. Also available in UPF institutional repository
  (http://hdl.handle.net/10230/20325)</comments><journal-ref>Proceedings of the 23rd International Conference on Computational
  Linguistics (Coling 2010); 2010 Aug 23-27; Beijing, CN. Stroudsburg: ACL;
  2010. p. 46-52</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this work we present the results of our experimental work on the
develop-ment of lexical class-based lexica by automatic means. The objective is
to as-sess the use of linguistic lexical-class based information as a feature
selection methodology for the use of classifiers in quick lexical development.
The results show that the approach can help in re-ducing the human effort
required in the development of language resources sig-nificantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2449</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2449</id><created>2013-03-11</created><authors><author><keyname>Romeo</keyname><forenames>Lauren</forenames></author><author><keyname>Mendes</keyname><forenames>Sara</forenames></author><author><keyname>Bel</keyname><forenames>N&#xfa;ria</forenames></author></authors><title>Using qualia information to identify lexical semantic classes in an
  unsupervised clustering task</title><categories>cs.CL</categories><comments>10 pages, 5 tables. Also available in UPF institutional repository
  (http://hdl.handle.net/10230/20383)</comments><journal-ref>Proceedings of COLING 2012: Posters: 24th International Conference
  on Computational Linguistics COLING 2012; 2012 December 8-15; Mumbai, India.
  Mumbai: The COLING 2012 Organizing Committee; 2012. p. 1029-1038</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Acquiring lexical information is a complex problem, typically approached by
relying on a number of contexts to contribute information for classification.
One of the first issues to address in this domain is the determination of such
contexts. The work presented here proposes the use of automatically obtained
FORMAL role descriptors as features used to draw nouns from the same lexical
semantic class together in an unsupervised clustering task. We have dealt with
three lexical semantic classes (HUMAN, LOCATION and EVENT) in English. The
results obtained show that it is possible to discriminate between elements from
different lexical semantic classes using only FORMAL role information, hence
validating our initial hypothesis. Also, iterating our method accurately
accounts for fine-grained distinctions within lexical classes, namely
distinctions involving ambiguous expressions. Moreover, a filtering and
bootstrapping strategy employed in extracting FORMAL role descriptors proved to
minimize effects of sparse data and noise in our task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2453</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2453</id><created>2013-03-11</created><updated>2013-03-18</updated><authors><author><keyname>Kartzow</keyname><forenames>Alexander</forenames></author></authors><title>Collapsible Pushdown Graphs of Level 2 are Tree-Automatic</title><categories>cs.LO cs.FL math.LO</categories><comments>Journal version of arXiv:0912.4110, accepted for publication in LMCS</comments><proxy>Logical Methods In Computer Science</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (March 20,
  2013) lmcs:1220</journal-ref><doi>10.2168/LMCS-9(1:12)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that graphs generated by collapsible pushdown systems of level 2 are
tree-automatic. Even if we allow epsilon-contractions and reachability
predicates (with regular constraints) for pairs of configurations, the
structures remain tree-automatic whence their first-order logic theories are
decidable. As a corollary we obtain the tree-automaticity of the second level
of the Caucal-hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2462</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2462</id><created>2013-03-11</created><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Vanier</keyname><forenames>Pascal</forenames><affiliation>LIF</affiliation></author></authors><title>Characterizations of periods of multidimensional shifts</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the sets of periods of multidimensional shifts of finite type
(SFTs) are exactly the sets of integers of the complexity class $\NE$. We also
show that the functions counting their number are the functions of #E. We also
give characterizations of some other notions of periodicity. We finish the
paper by giving some characterizations for sofic and effective subshifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2465</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2465</id><created>2013-03-11</created><authors><author><keyname>Reddy</keyname><forenames>Vikas</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>A Low-Complexity Algorithm for Static Background Estimation from
  Cluttered Image Sequences in Surveillance Contexts</title><categories>cs.CV</categories><acm-class>I.4.5; I.4.8; G.3</acm-class><journal-ref>EURASIP Journal on Image and Video Processing, 2011</journal-ref><doi>10.1155/2011/164956</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  For the purposes of foreground estimation, the true background model is
unavailable in many practical circumstances and needs to be estimated from
cluttered image sequences. We propose a sequential technique for static
background estimation in such conditions, with low computational and memory
requirements. Image sequences are analysed on a block-by-block basis. For each
block location a representative set is maintained which contains distinct
blocks obtained along its temporal line. The background estimation is carried
out in a Markov Random Field framework, where the optimal labelling solution is
computed using iterated conditional modes. The clique potentials are computed
based on the combined frequency response of the candidate block and its
neighbourhood. It is assumed that the most appropriate block results in the
smoothest response, indirectly enforcing the spatial continuity of structures
within a scene. Experiments on real-life surveillance videos demonstrate that
the proposed method obtains considerably better background estimates (both
qualitatively and quantitatively) than median filtering and the recently
proposed &quot;intervals of stable intensity&quot; method. Further experiments on the
Wallflower dataset suggest that the combination of the proposed method with a
foreground segmentation algorithm results in improved foreground segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2467</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2467</id><created>2013-03-11</created><updated>2013-04-11</updated><authors><author><keyname>Gor&#xed;n</keyname><forenames>Daniel</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Lutz</forenames></author></authors><title>Simulations and Bisimulations For Coalgebraic Modal Logics</title><categories>cs.LO math.LO</categories><msc-class>03B45, 18A15, 68Q85, 68Q87</msc-class><acm-class>F.4.1; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a notion of Lambda-simulation for coalgebraic modal logics,
parametric on the choice Lambda of predicate liftings for a functor T. We show
this notion is adequate in several ways: i) it preserves truth of positive
formulas, ii) for Lambda a separating set of monotone predicate liftings, the
associated notion of Lambda-bisimulation corresponds to T-behavioural
equivalence (moreover Lambda-n-bisimulations correspond to T-n-behavioural
equivalence), and iii) in fact, for Lambda-separating and T preserving weak
pullbacks, difunctional Lambda-bisimulations are T-bisimulations. In essence,
we arrive at a modular notion of equivalence that, when used with a separating
set of monotone predicate liftings, coincides with T-behavioural equivalence
regardless of whether T preserves weak pullbacks (unlike the notion of
T-bisimilarity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2478</identifier>
 <datestamp>2013-05-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2478</id><created>2013-03-11</created><updated>2013-05-15</updated><authors><author><keyname>Camby</keyname><forenames>Eglantine</forenames></author><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>The Price of Connectivity for Vertex Cover</title><categories>cs.DM math.CO</categories><comments>19 pages, 8 figures</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vertex cover number of a graph is the minimum number of vertices that are
needed to cover all edges. When those vertices are further required to induce a
connected subgraph, the corresponding number is called the connected vertex
cover number, and is always greater or equal to the vertex cover number.
  Connected vertex covers are found in many applications, and the relationship
between those two graph invariants is therefore a natural question to
investigate. For that purpose, we introduce the {\em Price of Connectivity},
defined as the ratio between the two vertex cover numbers. We prove that the
price of connectivity is at most 2 for arbitrary graphs. We further consider
graph classes in which the price of connectivity of every induced subgraph is
bounded by some real number $t$. We obtain forbidden induced subgraph
characterizations for every real value $t \leq 3/2$.
  We also investigate critical graphs for this property, namely, graphs whose
price of connectivity is strictly greater than that of any proper induced
subgraph. Those are the only graphs that can appear in a forbidden subgraph
characterization for the hereditary property of having a price of connectivity
at most $t$. In particular, we completely characterize the critical graphs that
are also chordal.
  Finally, we also consider the question of computing the price of connectivity
of a given graph. Unsurprisingly, the decision version of this question is
NP-hard. In fact, we show that it is even complete for the class $\Theta_2^P =
P^{NP[\log]}$, the class of decision problems that can be solved in polynomial
time, provided we can make $O(\log n)$ queries to an NP-oracle. This paves the
way for a thorough investigation of the complexity of problems involving ratios
of graph invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2487</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2487</id><created>2013-03-11</created><updated>2014-04-07</updated><authors><author><keyname>Esperet</keyname><forenames>Louis</forenames></author><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author></authors><title>Coloring planar graphs with three colors and no large monochromatic
  components</title><categories>math.CO cs.DM</categories><comments>v3: fixed a notation issue in Section 3</comments><journal-ref>Combinatorics, Probability, and Computing, 23/4:551--570, 2014</journal-ref><doi>10.1017/S0963548314000170</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of a function $f :\mathbb{N} \to \mathbb{N}$ such that
the vertices of every planar graph with maximum degree $\Delta$ can be
3-colored in such a way that each monochromatic component has at most
$f(\Delta)$ vertices. This is best possible (the number of colors cannot be
reduced and the dependence on the maximum degree cannot be avoided) and answers
a question raised by Kleinberg, Motwani, Raghavan, and Venkatasubramanian in
1997. Our result extends to graphs of bounded genus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2489</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2489</id><created>2013-03-11</created><authors><author><keyname>Navarro-P&#xe9;rez</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Rybalchenko</keyname><forenames>Andrey</forenames></author></authors><title>Separation Logic Modulo Theories</title><categories>cs.LO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logical reasoning about program data often requires dealing with heap
structures as well as scalar data types. Recent advances in Satisfiability
Modular Theory (SMT) already offer efficient procedures for dealing with
scalars, yet they lack any support for dealing with heap structures. In this
paper, we present an approach that integrates Separation Logic---a prominent
logic for reasoning about list segments on the heap---and SMT. We follow a
model-based approach that communicates aliasing among heap cells between the
SMT solver and the Separation Logic reasoning part. An experimental evaluation
using the Z3 solver indicates that our approach can effectively put to work the
advances in SMT for dealing with heap structures. This is the first decision
procedure for the combination of separation logic with SMT theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2506</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2506</id><created>2013-03-11</created><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>Monte-Carlo utility estimates for Bayesian reinforcement learning</title><categories>cs.LG stat.ML</categories><comments>6 pages, 4 figures, 1 table, submitted to IEEE conference on decision
  and control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a set of algorithms for Monte-Carlo Bayesian
reinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the
Bayes-optimal value function is employed to construct an optimistic policy.
Secondly, gradient-based algorithms for approximate upper and lower bounds are
introduced. Finally, we introduce a new class of gradient algorithms for
Bayesian Bellman error minimisation. We theoretically show that the gradient
methods are sound. Experimentally, we demonstrate the superiority of the upper
bound method in terms of reward obtained. However, we also show that the
Bayesian Bellman error method is a close second, despite its significant
computational simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2507</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2507</id><created>2013-03-11</created><authors><author><keyname>Cooper</keyname><forenames>S. Barry</forenames></author></authors><title>What Makes a Computation Unconventional?</title><categories>cs.LO</categories><comments>Based on an invited lecture for the 'Symposium on
  Natural/Unconventional Computing and Its Philosophical Significance' at the
  AISB/IACAP World Congress 2012, University of Birmingham, July 2-6, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coherent mathematical overview of computation and its generalisations is
described. This conceptual framework is sufficient to comfortably host a wide
range of contemporary thinking on embodied computation and its models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2514</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2514</id><created>2013-03-11</created><updated>2013-03-12</updated><authors><author><keyname>Wawrzyniak</keyname><forenames>Wojciech</forenames></author></authors><title>A local constant-factor approximation algorithm for MDS problem in
  anonymous network</title><categories>cs.DS</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In research on distributed local algorithms it is commonly assumed that each
vertex has a unique identifier in the entire graph. However, it turns out that
in case of certain classes of graphs (for example not lift-closed bounded
degree graphs) identifiers are unnecessary and only a port ordering is needed.
One of the open issues was whether identifiers are essential in planar graphs.
In this paper, we answer this question and we propose an algorithm which
returns constant approximation of the MDS problem in CONGEST model. The
algorithm doesn't use any additional information about the structure of the
graph and the nodes don't have unique identifiers. We hope that this paper will
be very helpful as a hint for further comparisons of the unique identifier
model and the model with only a port numbering in other classes of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2542</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2542</id><created>2013-03-11</created><updated>2014-07-07</updated><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Rehman</keyname><forenames>Obaid Ur</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Robust Smoothing for Estimating Optical Phase Varying as a Continuous
  Resonant Process</title><categories>math.OC cs.SY quant-ph</categories><comments>6 pages, 7 figures, Proceedings of the 2014 European Control
  Conference, pp. 896-901</comments><doi>10.1109/ECC.2014.6862385</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous phase estimation is known to be superior in accuracy as compared
to static estimation. The estimation process is, however, desired to be made
robust to uncertainties in the underlying parameters. Here, homodyne phase
estimation of coherent and squeezed states of light, evolving continuously
under the influence of a second-order resonant noise process, are made robust
to parameter uncertainties using a robust fixed-interval smoother, designed for
uncertain systems satisfying a certain integral quadratic constraint. We
observe that such a robust smoother provides improved worst-case performance
over the optimal smoother and also performs better than a robust filter for the
uncertain system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2545</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2545</id><created>2013-03-11</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Optimization of the parity-check matrix density in QC-LDPC code-based
  McEliece cryptosystems</title><categories>cs.IT cs.CR math.IT</categories><comments>10 pages, 4 figures. To be presented at IEEE ICC 2013 - Workshop on
  Information Security over Noisy and Lossy Communication Systems. Copyright
  transferred to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LDPC) codes are one of the most promising families
of codes to replace the Goppa codes originally used in the McEliece
cryptosystem. In fact, it has been shown that by using quasi-cyclic low-density
parity-check (QC-LDPC) codes in this system, drastic reductions in the public
key size can be achieved, while maintaining fixed security levels. Recently,
some proposals have appeared in the literature using codes with denser
parity-check matrices, named moderate-density parity-check (MDPC) codes.
However, the density of the parity-check matrices to be used in QC-LDPC
code-based variants of the McEliece cryptosystem has never been optimized. This
paper aims at filling such gap, by proposing a procedure for selecting the
density of the private parity-check matrix, based on the security level and the
decryption complexity. We provide some examples of the system parameters
obtained through the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2546</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2546</id><created>2013-03-11</created><authors><author><keyname>Lavnikevich</keyname><forenames>Nikolay</forenames></author></authors><title>On the Complexity of Maximum Clique Algorithms: usage of coloring
  heuristics leads to the 2^(n\5) algorithm running time lower bound</title><categories>cs.DS cs.DM math.CO</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum Clique Problem(MCP) is one of the 21 original NP--complete problems
enumerated by Karp in 1972. In recent years a large number of exact methods to
solve MCP have been appeared(Babel, Wood, Kumlander, Fahle, Li, Tomita and
etc). Most of them are branch and bound algorithms that use branching rule
introduced by Balas and Yu and based on coloring heuristics to establish an
upper bound on the clique number. They differ from each other primarily in
vertex preordering and vertex coloring methods. Current methods of worst case
running time analysis for branch and bound algorithms do not allow to provide
tight upper bounds. This motivates the study of lower bounds for such
algorithms. We prove 2^(n\5) lower bound for group of MCP algorithms based on
usage of coloring heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2547</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2547</id><created>2013-03-11</created><authors><author><keyname>Rifa</keyname><forenames>J.</forenames></author><author><keyname>Zinoviev</keyname><forenames>V.</forenames></author></authors><title>On a family of binary completely transitive codes with growing covering
  radius</title><categories>cs.IT math.IT</categories><comments>Submitted to Discrete mathematics. March 10, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new family of binary linear completely transitive (and, therefore,
completely regular) codes is constructed. The covering radius of these codes is
growing with the length of the code. In particular, for any integer r &gt; 1,
there exist two codes with d=3, covering radius r and length 2r(4r-1) and
(2r+1)(4r+1), respectively. These new completely transitive codes induce, as
coset graphs, a family of distance-transitive graphs of growing diameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2552</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2552</id><created>2013-03-07</created><updated>2013-03-27</updated><authors><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author><author><keyname>Richardson</keyname><forenames>Francois-Dominique</forenames></author><author><keyname>Adouni</keyname><forenames>Sarra</forenames></author><author><keyname>Gagnon</keyname><forenames>Francois</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author></authors><title>Multi-User Multi-Carrier Differential Chaos Shift Keying Communication
  System</title><categories>cs.OH</categories><comments>Accepted in the IEEE International Wireless Communications and Mobile
  Computing Conference (IWCMC 2013)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, a multi user Multi-Carrier Differential Chaos Shift Keying
(MC-DCSK) modulation is presented. The system endeavors to provide a good
trade-off between robustness, energy efficiency and high data rate, while still
being simple. In this architecture of MC-DCSK system, for each user, chaotic
reference sequence is transmitted over a predefined subcarrier frequency.
Multiple modulated data streams are transmitted over the remaining subcarriers
allocated for each user. This transmitter structure saves energy and increases
the spectral efficiency of the conventional DCSK system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2553</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2553</id><created>2013-03-11</created><authors><author><keyname>Chen</keyname><forenames>Jen-Yeu</forenames></author><author><keyname>Tseng</keyname><forenames>Yi-ying</forenames></author></authors><title>Distributed Intrusion Detection of Byzantine Attacks in Wireless
  Networks with Random Linear Network Coding</title><categories>cs.NI</categories><journal-ref>International Journal of Distributed Sensor Networks, Volume 2012
  (2012), Article ID 758340, 10 pages</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding is an elegant technique where, instead of simply relaying the
packets of information they receive, the nodes of a network are allowed to
combine \emph{several} packets together for transmission and this technique can
be used to achieve the maximum possible information flow in a network and save
the needed number of packet transmissions. Moreover, in an energy-constraint
wireless network such as Wireless Sensor Network (a typical type of wireless ad
hoc network), applying network coding to reduce the number of wireless
transmissions can also prolong the life time of sensor nodes. Although applying
network coding in a wireless sensor network is obviously beneficial, due to the
operation that one transmitting information is actually combination of multiple
other information, it is possible that an error propagation may occur in the
network. This special characteristic also exposes network coding system to a
wide range of error attacks, especially Byzantine attacks. When some adversary
nodes generate error data in the network with network coding, those erroneous
information will be mixed at intermeidate nodes and thus corrupt all the
information reaching a destination. Recent research efforts have shown that
network coding can be combined with classical error control codes and
cryptography for secure communication or misbehavior detection. Nevertheless,
when it comes to Byzantine attacks, these results have limited effect. In fact,
unless we find out those adversary nodes and isolate them, network coding may
perform much worse than pure routing in the presence of malicious nodes. In
this paper, a distributed hierarchical algorithm based on random linear network
coding is developed to detect, locate and isolate malicious nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2554</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2554</id><created>2013-03-11</created><authors><author><keyname>Popova</keyname><forenames>Viara</forenames></author><author><keyname>Fahland</keyname><forenames>Dirk</forenames></author><author><keyname>Dumas</keyname><forenames>Marlon</forenames></author></authors><title>Artifact Lifecycle Discovery</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artifact-centric modeling is a promising approach for modeling business
processes based on the so-called business artifacts - key entities driving the
company's operations and whose lifecycles define the overall business process.
While artifact-centric modeling shows significant advantages, the overwhelming
majority of existing process mining methods cannot be applied (directly) as
they are tailored to discover monolithic process models. This paper addresses
the problem by proposing a chain of methods that can be applied to discover
artifact lifecycle models in Guard-Stage-Milestone notation. We decompose the
problem in such a way that a wide range of existing (non-artifact-centric)
process discovery and analysis methods can be reused in a flexible manner. The
methods presented in this paper are implemented as software plug-ins for ProM,
a generic open-source framework and architecture for implementing process
mining tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2558</identifier>
 <datestamp>2014-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2558</id><created>2013-03-11</created><updated>2014-02-13</updated><authors><author><keyname>Mennle</keyname><forenames>Timo</forenames></author><author><keyname>Seuken</keyname><forenames>Sven</forenames></author></authors><title>Hybrid Mechanisms: Trading off Efficiency and Strategyproofness in
  One-Sided Matching</title><categories>cs.GT</categories><comments>Working Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study one-sided matching mechanisms where agents have vNM utility
functions and report ordinal preferences. Strong impossibility results restrict
the design space of strategyproof mechanisms in this domain. Improving on
efficiency beyond the ex-post efficiency of Random Serial Dictatorship (RSD)
generally requires trade-offs. In this paper, we introduce hybrid mechanisms,
which are convex combinations of existing mechanism, and we show that they are
a powerful yet simple method for trading off strategyproofness and efficiency.
We present conditions under which hybrid mechanisms remain partially
strategyproof with respect to a given bound for the degree of
strategyproofness. At the same time, these hybrids have better efficiency
properties than the less efficient component. Our approach can be successfully
applied to create hybrids of RSD and the Probabilistic Serial mechanism (PS),
as well as hybrids of RSD and the adaptive Boston mechanism (ABM). We provide
numerical results demonstrating that the improvements can be significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2564</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2564</id><created>2013-03-11</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Mandrescu</keyname><forenames>Eugen</forenames></author></authors><title>On f-Symmetries of the Independence Polynomial</title><categories>cs.DM math.CO</categories><comments>10 pages</comments><msc-class>05C31, 05C69 (Primary) 05C76 (Secondary)</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An independent set in a graph is a set of pairwise non-adjacent vertices, and
a(G) is the size of a maximum independent set in the graph G. If s_{k} is the
number of independent sets of cardinality k in G, then
I(G;x)=s_0+s_1*x+s_2*x^2+...+s_a*x^a,a=a(G), is called the independence
polynomial of G (I. Gutman and F. Harary, 1983). If s_{a-i}=f(i)*s_{i} holds
for every i, then I(G;x) is called f-symmetric (f-palindromic). If f(i)=1, then
I(G;x) is symmetric (palindromic). The corona of the graphs G and H is the
graph G*H obtained by joining each vertex of G to all the vertices of a copy of
H. In this paper we show that if H is a graph with p vertices, q edges, and
alpha(H)=2, then I(G*H;x) is f-symmetric for some elegant function f. In
particular, if H = K_{r}-e, we show that I(G*H;x) is symmetric and unimodal,
with a unique mode. This finding generalizes results due to (Stevanovic, 1998)
and (Mandrescu, 2012) claiming that I(G*(K_2-e);x)=I(G*2K_1;x) is symmetric and
unimodal for every graph G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2579</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2579</id><created>2013-03-11</created><authors><author><keyname>Warsi</keyname><forenames>Naqueeb Ahmad</forenames></author></authors><title>One-shot source coding with coded side information available at the
  decoder</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-shot achievable rate region for source coding when coded side information
is available at the decoder (source coding with a helper) is proposed. The
achievable region proposed is in terms of conditional smooth max Renyi entropy
and smooth max Renyi divergence. Asymptotically (in the limit of large block
lengths) this region is quantified in terms of spectral-sup conditional entropy
rate and spectral- sup mutual information rate. In particular, it coincides
with the rate region derived in the limit of unlimited independent and
identically distributed copies of the sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2580</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2580</id><created>2013-03-07</created><authors><author><keyname>Kochkarev</keyname><forenames>B. S.</forenames></author></authors><title>Proof of the hypothesis Edmonds's, not polynomial of NPC-problems and
  classification of the problems with polynomial certificates</title><categories>cs.CC</categories><comments>4 pages</comments><msc-class>D.4.6</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the affirmation $P\subseteq NP$ (in computer science)
erroneously and we prove the justice of the hypotesis J.Edmonds's $P\neq NP$.
We show further that all the $NP$-complete problems is not polynomial and we
give the classification of the problems with the polynomial certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2587</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2587</id><created>2013-03-11</created><updated>2013-06-08</updated><authors><author><keyname>Huang</keyname><forenames>Yichao</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Multicell Random Beamforming with CDF-based Scheduling: Exact Rate and
  Scaling Laws</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multicell multiuser MIMO downlink employing random beamforming as the
transmission scheme, the heterogeneous large scale channel effects of intercell
and intracell interference complicate analysis of distributed scheduling based
systems. In this paper, we extend the analysis in [1] and [2] to study the
aforementioned challenging scenario. The cumulative distribution function
(CDF)-based scheduling policy utilized in [1] and [2] is leveraged to maintain
fairness among users and simultaneously obtain multiuser diversity gain. The
closed form expression of the individual sum rate for each user is derived
under the CDF-based scheduling policy. More importantly, with this distributed
scheduling policy, we conduct asymptotic (in users) analysis to determine the
limiting distribution of the signal-to-interference-plus-noise ratio, and
establish the individual scaling laws for each user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2593</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2593</id><created>2013-03-04</created><authors><author><keyname>Ansari</keyname><forenames>Adeel</forenames></author><author><keyname>Shafie</keyname><forenames>Afza Bt</forenames></author><author><keyname>Said</keyname><forenames>Abas B Md</forenames></author><author><keyname>Ansari</keyname><forenames>Seema</forenames></author></authors><title>Independent Component Analysis for Filtering Airwaves in Seabed Logging
  Application</title><categories>cs.OH physics.geo-ph</categories><comments>7 pages, 13 figures</comments><journal-ref>International Journal of Advanced Studies in Computers, Science
  and Engineering (IJASCSE), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marine controlled source electromagnetic (CSEM) sensing method used for the
detection of hydrocarbons based reservoirs in seabed logging application does
not perform well due to the presence of the airwaves (or sea-surface). These
airwaves interfere with the signal that comes from the subsurface seafloor and
also tend to dominate in the receiver response at larger offsets. The task is
to identify these air waves and the way they interact, and to filter them out.
In this paper, a popular method for counteracting with the above stated problem
scenario is Independent Component Analysis (ICA). Independent component
analysis (ICA) is a statistical method for transforming an observed
multidimensional or multivariate dataset into its constituent components
(sources) that are statistically as independent from each other as possible.
ICA-type de-convolution algorithm that is FASTICA is considered for mixed
signals de-convolution and considered convenient depending upon the nature of
the source and noise model. The results from the FASTICA algorithm are shown
and evaluated. In this paper, we present the FASTICA algorithm for the seabed
logging application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2595</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2595</id><created>2013-03-11</created><authors><author><keyname>Paul</keyname><forenames>Norbert</forenames></author><author><keyname>Bradley</keyname><forenames>Patrick Erik</forenames></author><author><keyname>Breunig</keyname><forenames>Martin</forenames></author></authors><title>Integrating Space, Time, Version and Scale Using Alexandrov Topologies</title><categories>cs.DB</categories><comments>International Symposium on Spatial and Temporal Databases SSTD 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces a novel approach to spatial database design. Instead
of extending the canonical Solid-Face-Edge-Vertex schema by, say, &quot;hypersolids&quot;
these classes are generalised to a common type SpatialEntity, and the
individual BoundedBy relations between two consecutive classes are generalised
to one BoundedBy relation on SpatialEntity instances. Then the pair
(SpatialEntity, BoundedBy) is a so-called incidence graph.
  The novelty about this approach uses the observation that an incidence graph
represents a topological space of SpatialEntity instances because the
BoundedBy-relation defines a so-called Alexandrov topology for them turning
them into a topological space. So spatial data becomes part of mathematical
topology and topology can be immediately applied to spatial data. For example,
continuous functions between two instances of spatial data allow the consistent
modelling of generalisation. Further, it is also possible to establish a formal
topological definition of spatial data dimension, and every topological data
model of arbitrary dimension gets a simple uniform data model. This model
covers space-time, and the version history of a spatial model can be
represented by an Alexandrov topology, too. By integrating space, time,
version, and scale into one single schema, topological queries across those
aspects are enabled through topological constructions. In fact, the topological
constructions cover a relationally complete query language for spaces and can
be redefined to operate accordingly on their graph representations.
  With these observations a relational database schema for a spatial data model
of dimension 6 and more is developed. The schema seamlessly integrates 4D
space-time, levels of detail and version history, and it can be easily expanded
to also contain non-spatial information or be linked to other data sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2607</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2607</id><created>2013-03-11</created><updated>2014-04-09</updated><authors><author><keyname>Isack</keyname><forenames>Hossam</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author></authors><title>Joint optimization of fitting &amp; matching in multi-view reconstruction</title><categories>cs.CV</categories><comments>33 pages, 8 figures, 2 tables, to appear in IEEE conference on
  Computer Vision and Pattern Recognition (CVPR), June 2014</comments><report-no>Technical Report 755, Computer Science department, UWO, London,
  Canada, ISBN: 978-0-7714-2980-4</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many standard approaches for geometric model fitting are based on pre-matched
image features. Typically, such pre-matching uses only feature appearances
(e.g. SIFT) and a large number of non-unique features must be discarded in
order to control the false positive rate. In contrast, we solve feature
matching and multi-model fitting problems in a joint optimization framework.
This paper proposes several fit-&amp;-match energy formulations based on a
generalization of the assignment problem. We developed an efficient solver
based on min-cost-max-flow algorithm that finds near optimal solutions. Our
approach significantly increases the number of detected matches. In practice,
energy-based joint fitting &amp; matching allows to increase the distance between
view-points previously restricted by robustness of local SIFT-matching and to
improve the model fitting accuracy when compared to state-of-the-art
multi-model fitting techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2610</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2610</id><created>2013-03-11</created><authors><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Rajan</keyname><forenames>Deepta</forenames></author><author><keyname>Puri</keyname><forenames>Anup</forenames></author><author><keyname>Frakes</keyname><forenames>David</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Kernel Sparse Models for Automated Tumor Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose sparse coding-based approaches for segmentation of
tumor regions from MR images. Sparse coding with data-adapted dictionaries has
been successfully employed in several image recovery and vision problems. The
proposed approaches obtain sparse codes for each pixel in brain magnetic
resonance images considering their intensity values and location information.
Since it is trivial to obtain pixel-wise sparse codes, and combining multiple
features in the sparse coding setup is not straightforward, we propose to
perform sparse coding in a high-dimensional feature space where non-linear
similarities can be effectively modeled. We use the training data from
expert-segmented images to obtain kernel dictionaries with the kernel K-lines
clustering procedure. For a test image, sparse codes are computed with these
kernel dictionaries, and they are used to identify the tumor regions. This
approach is completely automated, and does not require user intervention to
initialize the tumor regions in a test image. Furthermore, a low complexity
segmentation approach based on kernel sparse codes, which allows the user to
initialize the tumor region, is also presented. Results obtained with both the
proposed approaches are validated against manual segmentation by an expert
radiologist, and the proposed methods lead to accurate tumor identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2619</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2619</id><created>2013-03-11</created><authors><author><keyname>Power</keyname><forenames>Russell</forenames></author></authors><title>Making Systems More Robust with Flexible RPC Lookup</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern distributed systems use names everywhere. Lockservices such as Chubby
and ZooKeeper provide an effective mechanism for mapping from application names
to server instances, but proper usage of them requires a large amount of
error-prone boiler-plate code.
  Application programmers often try to write wrappers to abstract away this
logic, but it turns out there is a more general and easier way of handling the
issue. We show that by extending the existing name resolution capabilities of
RPC libraries, we can remove the need for such annoying boiler-plate code while
at the same time making our services more robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2631</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2631</id><created>2013-03-11</created><updated>2013-09-07</updated><authors><author><keyname>Somaraju</keyname><forenames>Ram A.</forenames></author><author><keyname>Sarlette</keyname><forenames>Alain</forenames></author><author><keyname>Thienpont</keyname><forenames>Hugo</forenames></author></authors><title>Quantum filtering using POVM measurements</title><categories>quant-ph cs.SY math.PR</categories><comments>6 Pages, Accepted to CDC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this work is to develop a recursive, discrete time quantum
filtering equation for a system that interacts with a probe, on which
measurements are performed according to the Positive Operator Valued Measures
(POVMs) framework. POVMs are the most general measurements one can make on a
quantum system and although in principle they can be reformulated as projective
measurements on larger spaces, for which filtering results exist, a direct
treatment of POVMs is more natural and can simplify the filter computations for
some applications. Hence we formalize the notion of strongly commuting (Davies)
instruments which allows one to develop joint measurement statistics for POVM
type measurements. This allows us to prove the existence of conditional POVMs,
which is essential for the development of a filtering equation. We demonstrate
that under generally satisfied assumptions, knowing the observed probe POVM
operator is sufficient to uniquely specify the quantum filtering evolution for
the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2636</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2636</id><created>2013-03-11</created><authors><author><keyname>Gurakan</keyname><forenames>Berk</forenames></author><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Energy Cooperation in Energy Harvesting Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Communications, March 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In energy harvesting communications, users transmit messages using energy
harvested from nature during the course of communication. With an optimum
transmit policy, the performance of the system depends only on the energy
arrival profiles. In this paper, we introduce the concept of energy
cooperation, where a user wirelessly transmits a portion of its energy to
another energy harvesting user. This enables shaping and optimization of the
energy arrivals at the energy-receiving node, and improves the overall system
performance, despite the loss incurred in energy transfer. We consider several
basic multi-user network structures with energy harvesting and wireless energy
transfer capabilities: relay channel, two-way channel and multiple access
channel. We determine energy management policies that maximize the system
throughput within a given duration using a Lagrangian formulation and the
resulting KKT optimality conditions. We develop a two-dimensional directional
water-filling algorithm which optimally controls the flow of harvested energy
in two dimensions: in time (from past to future) and among users (from
energy-transferring to energy-receiving) and show that a generalized version of
this algorithm achieves the boundary of the capacity region of the two-way
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2643</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2643</id><created>2013-03-11</created><authors><author><keyname>Liu</keyname><forenames>Hairong</forenames></author><author><keyname>Latecki</keyname><forenames>Longin Jan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Revealing Cluster Structure of Graph by Path Following Replicator
  Dynamic</title><categories>cs.LG cs.GT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we propose a path following replicator dynamic, and
investigate its potentials in uncovering the underlying cluster structure of a
graph. The proposed dynamic is a generalization of the discrete replicator
dynamic. The replicator dynamic has been successfully used to extract dense
clusters of graphs; however, it is often sensitive to the degree distribution
of a graph, and usually biased by vertices with large degrees, thus may fail to
detect the densest cluster. To overcome this problem, we introduce a dynamic
parameter, called path parameter, into the evolution process. The path
parameter can be interpreted as the maximal possible probability of a current
cluster containing a vertex, and it monotonically increases as evolution
process proceeds. By limiting the maximal probability, the phenomenon of some
vertices dominating the early stage of evolution process is suppressed, thus
making evolution process more robust. To solve the optimization problem with a
fixed path parameter, we propose an efficient fixed point algorithm. The time
complexity of the path following replicator dynamic is only linear in the
number of edges of a graph, thus it can analyze graphs with millions of
vertices and tens of millions of edges on a common PC in a few minutes.
Besides, it can be naturally generalized to hypergraph and graph with edges of
different orders. We apply it to four important problems: maximum clique
problem, densest k-subgraph problem, structure fitting, and discovery of
high-density regions. The extensive experimental results clearly demonstrate
its advantages, in terms of robustness, scalability and flexility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2646</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2646</id><created>2013-03-11</created><authors><author><keyname>Leipzig</keyname><forenames>Jeremy</forenames></author></authors><title>Work Issues in Software Engineering</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Using data from a web-based survey of software developers, the author
attempts to determine root causes of &quot;death march&quot; projects and excessive work
hours in the software industry in relation to company practices and management.
Special emphasis is placed on the factor of business/technical supervisor
background. An analysis of variance revealed significant differences between
these supervisor groups with regard to a &quot;Pointy-Haired Boss&quot; (PHB) sentiment
index. This difference, combined with correlations between the PHB index and
the endpoints of project failure and use of software engineering practices,
indicate some disparity in the suitability of businessbackground supervisors to
manage software development projects compared with their technical-background
counterparts. Other survey data points to improved project management skills as
the biggest necessity for supervisors in the business-background group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2651</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2651</id><created>2013-03-10</created><updated>2014-03-30</updated><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Hybrid Q-Learning Applied to Ubiquitous recommender system</title><categories>cs.LG cs.IR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1301.4351,
  arXiv:1303.2308</comments><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous information access becomes more and more important nowadays and
research is aimed at making it adapted to users. Our work consists in applying
machine learning techniques in order to bring a solution to some of the
problems concerning the acceptance of the system by users. To achieve this, we
propose a fundamental shift in terms of how we model the learning of
recommender system: inspired by models of human reasoning developed in robotic,
we combine reinforcement learning and case-base reasoning to define a
recommendation process that uses these two approaches for generating
recommendations on different context dimensions (social, temporal, geographic).
We describe an implementation of the recommender system based on this
framework. We also present preliminary results from experiments with the system
and show how our approach increases the recommendation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2654</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2654</id><created>2013-03-11</created><authors><author><keyname>Stojanovski</keyname><forenames>Toni Draganov</forenames></author><author><keyname>Marina</keyname><forenames>Ninoslav</forenames></author></authors><title>Secure Wireless Communications via Cooperative Transmitting</title><categories>cs.CR</categories><comments>Submitted for presentation at the 2013 IEEE International Symposium
  on Information Theory, Istanbul, Turkey, July 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theoretic secrecy is combined with cryptographic secrecy to
create a secret-key exchange protocol for wireless networks. A network of
transmitters, which already have cryptographically secured channels between
them, cooperate to exchange a secret key with a new receiver at a random
location, in the presence of passive eavesdroppers at unknown locations. Two
spatial point processes: homogeneous Poisson process and independent uniformly
distributed points are used for the spatial distributions of transmitters and
eavesdroppers. We analyse the impact of the number of cooperating transmitters
and the number of eavesdroppers on the area fraction where secure communication
is possible. Upper bounds on the probability of existence of positive secrecy
between the cooperating transmitters and the receiver are derived. The
closeness of the upper bounds to the real value is then estimated by means of
numerical simulations. Simulations also indicate that a deterministic spatial
distribution for the transmitters e.g. hexagonal and square lattices, increases
the probability of existence of positive secrecy capacity compared to the
random spatial distributions. For the same number of friendly nodes,
cooperative transmitting provides a dramatically larger secrecy region than
cooperative jamming and cooperative relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2663</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2663</id><created>2013-03-11</created><updated>2013-10-04</updated><authors><author><keyname>Smith</keyname><forenames>Laura M.</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Garcia-Cardona</keyname><forenames>Cristina</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author></authors><title>Spectral Clustering with Epidemic Diffusion</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><comments>6 pages, to appear in Physical Review E</comments><acm-class>I.5.3</acm-class><doi>10.1103/PhysRevE.88.042813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering is widely used to partition graphs into distinct modules
or communities. Existing methods for spectral clustering use the eigenvalues
and eigenvectors of the graph Laplacian, an operator that is closely associated
with random walks on graphs. We propose a new spectral partitioning method that
exploits the properties of epidemic diffusion. An epidemic is a dynamic process
that, unlike the random walk, simultaneously transitions to all the neighbors
of a given node. We show that the replicator, an operator describing epidemic
diffusion, is equivalent to the symmetric normalized Laplacian of a reweighted
graph with edges reweighted by the eigenvector centralities of their incident
nodes. Thus, more weight is given to edges connecting more central nodes. We
describe a method that partitions the nodes based on the componentwise ratio of
the replicator's second eigenvector to the first, and compare its performance
to traditional spectral clustering techniques on synthetic graphs with known
community structure. We demonstrate that the replicator gives preference to
dense, clique-like structures, enabling it to more effectively discover
communities that may be obscured by dense intercommunity linking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2675</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2675</id><created>2013-03-07</created><authors><author><keyname>Iqbal</keyname><forenames>Muneeb</forenames></author><author><keyname>Khan</keyname><forenames>Atif Ali</forenames></author><author><keyname>Naseer</keyname><forenames>Oumair</forenames></author></authors><title>A legal perspective of E-business and E-marketing for small and medium
  enterprises</title><categories>cs.CY</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic businesses are witnessing enormous growth as more and more people
are switching to online platforms. The widespread use of Internet has opened
new channels to operate trade for many businesses. Also electronic marketing
has become a proven channel of passing on the word to the customers. Legal and
ethical issues quickly become an area of concern. In this research
recommendations are made to harmonize IT and Internet Laws. A novel approach is
proposed to promote legal risk management culture in organizations. It begins
with revising current state of regulations surrounding eBusinesses and
electronic marketing. The proposed approach offers risk management by
considering risk mitigation strategy, educating people and use of information
technology. Monitoring compliance requirements are met by reviewing the latest
changes in regulations and rewarding the employees who ensures the successful
implementation of the strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2682</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2682</id><created>2013-03-11</created><authors><author><keyname>Stacey</keyname><forenames>Blake C.</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Principles of Security: Human, Cyber, and Biological</title><categories>cs.CR nlin.AO physics.soc-ph q-bio.PE</categories><comments>report for the Chief of Naval Operations Strategic Studies Group,
  June 2008; made public 28 February 2013</comments><report-no>New England Complex Systems Institute Report 2008-06-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cybersecurity attacks are a major and increasing burden to economic and
social systems globally. Here we analyze the principles of security in
different domains and demonstrate an architectural flaw in current
cybersecurity. Cybersecurity is inherently weak because it is missing the
ability to defend the overall system instead of individual computers. The
current architecture enables all nodes in the computer network to communicate
transparently with one another, so security would require protecting every
computer in the network from all possible attacks. In contrast, other systems
depend on system-wide protections. In providing conventional security, police
patrol neighborhoods and the military secures borders, rather than defending
each individual household. Likewise, in biology, the immune system provides
security against viruses and bacteria using primarily action at the skin,
membranes, and blood, rather than requiring each cell to defend itself. We
propose applying these same principles to address the cybersecurity challenge.
This will require: (a) Enabling pervasive distribution of self-propagating
securityware and creating a developer community for such securityware, and (b)
Modifying the protocols of internet routers to accommodate adaptive security
software that would regulate internet traffic. The analysis of the immune
system architecture provides many other principles that should be applied to
cybersecurity. Among these principles is a careful interplay of detection and
action that includes evolutionary improvement. However, achieving significant
security gains by applying these principles depends strongly on remedying the
underlying architectural limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2685</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2685</id><created>2013-03-11</created><authors><author><keyname>Gadde</keyname><forenames>Akshay</forenames></author><author><keyname>Narang</keyname><forenames>Sunil K</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author></authors><title>Bilateral Filter: Graph Spectral Interpretation and Extensions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the bilateral filter proposed by Tomasi and Manduchi,
as a spectral domain transform defined on a weighted graph. The nodes of this
graph represent the pixels in the image and a graph signal defined on the nodes
represents the intensity values. Edge weights in the graph correspond to the
bilateral filter coefficients and hence are data adaptive. Spectrum of a graph
is defined in terms of the eigenvalues and eigenvectors of the graph Laplacian
matrix. We use this spectral interpretation to generalize the bilateral filter
and propose more flexible and application specific spectral designs of
bilateral-like filters. We show that these spectral filters can be implemented
with k-iterative bilateral filtering operations and do not require expensive
diagonalization of the Laplacian matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2709</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2709</id><created>2013-03-11</created><authors><author><keyname>LeBlanc</keyname><forenames>Heath J.</forenames></author><author><keyname>Zhang</keyname><forenames>Haotian</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author><author><keyname>Koutsoukos</keyname><forenames>Xenofon</forenames></author></authors><title>Resilient Continuous-Time Consensus in Fractional Robust Networks</title><categories>cs.SY</categories><comments>to appear at the American Control Conference in June 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the continuous-time consensus problem in the presence
of adversaries. The networked multi-agent system is modeled as a switched
system, where the normal agents have integrator dynamics and the switching
signal determines the topology of the network. We consider several models of
omniscient adversaries under the assumption that at most a fraction of any
normal agent's neighbors may be adversaries. Under this fractional assumption
on the interaction between normal and adversary agents, we show that a novel
graph theoretic metric, called fractional robustness, is useful for analyzing
the network topologies under which the normal agents achieve consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2720</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2720</id><created>2013-03-11</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Cai</keyname><forenames>Yunlong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Low-Complexity Constrained Constant Modulus SG-based Beamforming
  Algorithms with Variable Step Size</title><categories>cs.IT math.IT</categories><comments>3 figures, 1 table ICASSP 2008. arXiv admin note: substantial text
  overlap with arXiv:1303.1848</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two low-complexity adaptive step size algorithms are
investigated for blind adaptive beamforming. Both of them are used in a
stochastic gradient (SG) algorithm, which employs the constrained constant
modulus (CCM) criterion as the design approach. A brief analysis is given for
illustrating their properties. Simulations are performed to compare the
performances of the novel algorithms with other well-known methods. Results
indicate that the proposed algorithms achieve superior performance, better
convergence behavior and lower computational complexity in both stationary and
non-stationary environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2721</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2721</id><created>2013-03-11</created><authors><author><keyname>Cheng</keyname><forenames>Yi</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Guaranteed Performance Leader-follower Control for Multi-agent Systems
  with Linear IQC-Constrained Coupling</title><categories>cs.SY</categories><comments>A shortened version will appear in the Proceedings of the 2013
  American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the leader-follower control problem for a linear
multi-agent system with undirected topology and linear coupling subject to
integral quadratic constraints (IQCs). A consensus-type control protocol is
proposed based on each agent's states relative its neighbors. In addition a
selected set of agents uses for control their states relative the leader. Using
a coordinate transformation, the consensus analysis of the multi-agent system
is recast as a decentralized robust control problem for an auxiliary
interconnected large scale system. Based on this interconnected large scale
system, sufficient conditions are obtained which guarantee that the system
tracks the leader. These conditions guarantee a suboptimal bound on the system
tracking performance. The effectiveness of the proposed method is demonstrated
using a simulation example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2725</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2725</id><created>2013-03-11</created><authors><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author></authors><title>Robust blind methods using $\ell_p$ quasi norms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was shown in a previous work that some blind methods can be made robust to
channel order overmodeling by using the $\ell_1$ or $\ell_p$ quasi-norms.
However, no theoretical argument has been provided to support this statement.
In this work, we study the robustness of subspace blind based methods using
$\ell_1$ or $\ell_p$ quasi-norms. For the $\ell_1$ norm, we provide the
sufficient and necessary condition that the channel should satisfy in order to
ensure its identifiability in the noise-less case. We then study its frequency
of occurrence, and deduce the effect of channel parameters on the robustness of
blind subspace methods using $\ell_1$ norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2730</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2730</id><created>2013-03-11</created><authors><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Is Cheeger-type Approximation Possible for Nonuniform Sparsest Cut?</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the {\em nonuniform sparsest cut} problem, given two undirected graphs $G$
and $H$ over the same set of vertices $V$, we want to find a cut $(S,V-S)$ that
minimizes the ratio between the fraction of $G$-edges that are cut and the
fraction of $H$-edges that are cut. The ratio (which is at most 1 in an optimal
solution) is called the {\em sparsity} of the cut. In the {\em uniform sparsest
cut} problem, $H$ is a clique over $V$. If $G$ is regular, it is possible to
find a solution to the uniform sparsest cut of cost $O(\sqrt{opt})$ in nearly
linear time. Is such an approximation, which we call &quot;Cheege-type&quot;
approximation, achievable in the non-uniform case?
  We show that the answer is negative, assuming the Unique Games Conjecture,
for general H. Furthermore, the Leighton-Rao linear programming relaxation and
the spectral relaxation fail to find such an approximation even if $H$ is a
clique over a subset of vertices. Using semidefinite programming, however, we
can find Cheeger-type approximations in polynomial time whenever the adjacency
matrix of $H$ has rank 1. (This includes the cases in which $H$ is a clique
over a subset of vertices.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2735</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2735</id><created>2013-03-11</created><authors><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author><author><keyname>Wang</keyname><forenames>Pengwei</forenames></author></authors><title>Efficient Codes for Limited View Adversarial Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce randomized Limited View (LV) adversary codes that provide
protection against an adversary that uses their partial view of the
communication to construct an adversarial error vector to be added to the
channel. For a codeword of length N, the adversary selects a subset of \rho_rN
of the codeword components to &quot;see&quot;, and then &quot;adds&quot; an adversarial error
vector of weight \rho_wN to the codeword. Performance of the code is measured
by the probability of the decoder failure in recovering the sent message. An
(N, q^{RN},\delta)-limited view adversary code ensures that the success chance
of the adversary in making decoder fail, is bounded by \delta when the
information rate of the code is at least R. Our main motivation to study these
codes is providing protection for wireless communication at the physical layer
of networks.
  We formalize the definition of adversarial error and decoder failure,
construct a code with efficient encoding and decoding that allows the adversary
to, depending on the code rate, read up to half of the sent codeword and add
error on the same coordinates. The code is non-linear, has an efficient
decoding algorithm, and is constructed using a message authentication code
(MAC) and a Folded Reed-Solomon (FRS) code. The decoding algorithm uses an
innovative approach that combines the list decoding algorithm of the FRS codes
and the MAC verification algorithm to eliminate the exponential size of the
list output from the decoding algorithm. We discuss application of our results
to Reliable Message Transmission problem, and open problems for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2739</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2739</id><created>2013-03-11</created><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Machine Learning for Bioclimatic Modelling</title><categories>cs.LG stat.AP</categories><comments>8 pages</comments><msc-class>97R30</msc-class><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 4, No. 2, 2013, pp. 1-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning (ML) approaches are widely used to generate bioclimatic
models for prediction of geographic range of organism as a function of climate.
Applications such as prediction of range shift in organism, range of invasive
species influenced by climate change are important parameters in understanding
the impact of climate change. However, success of machine learning-based
approaches depends on a number of factors. While it can be safely said that no
particular ML technique can be effective in all applications and success of a
technique is predominantly dependent on the application or the type of the
problem, it is useful to understand their behavior to ensure informed choice of
techniques. This paper presents a comprehensive review of machine
learning-based bioclimatic model generation and analyses the factors
influencing success of such models. Considering the wide use of statistical
techniques, in our discussion we also include conventional statistical
techniques used in bioclimatic modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2745</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2745</id><created>2013-03-11</created><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Evolutionary Approaches to Expensive Optimisation</title><categories>cs.NE</categories><comments>7 pages</comments><msc-class>97R40</msc-class><journal-ref>(IJARAI) International Journal of Advanced Research in Artificial
  Intelligence,Vol. 2, No. 3, 2013, pp. 53-59</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surrogate assisted evolutionary algorithms (EA) are rapidly gaining
popularity where applications of EA in complex real world problem domains are
concerned. Although EAs are powerful global optimizers, finding optimal
solution to complex high dimensional, multimodal problems often require very
expensive fitness function evaluations. Needless to say, this could brand any
population-based iterative optimization technique to be the most crippling
choice to handle such problems. Use of approximate model or surrogates provides
a much cheaper option. However, naturally this cheaper option comes with its
own price. This paper discusses some of the key issues involved with use of
approximation in evolutionary algorithm, possible best practices and solutions.
Answers to the following questions have been sought: what type of fitness
approximation to be used; which approximation model to use; how to integrate
the approximation model in EA; how much approximation to use; and how to ensure
reliable approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2751</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2751</id><created>2013-03-11</created><authors><author><keyname>Hangarge</keyname><forenames>Mallikarjun</forenames></author></authors><title>Gaussian Mixture Model for Handwritten Script Identification</title><categories>cs.CV</categories><comments>Appeared in ICECIT-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Gaussian Mixture Model (GMM) to identify the script of
handwritten words of Roman, Devanagari, Kannada and Telugu scripts. It
emphasizes the significance of directional energies for identification of
script of the word. It is robust to varied image sizes and different styles of
writing. A GMM is modeled using a set of six novel features derived from
directional energy distributions of the underlying image. The standard
deviation of directional energy distributions are computed by decomposing an
image matrix into right and left diagonals. Furthermore, deviation of
horizontal and vertical distributions of energies is also built-in to GMM. A
dataset of 400 images out of 800 (200 of each script) are used for training GMM
and the remaining is for testing. An exhaustive experimentation is carried out
at bi-script, tri-script and multi-script level and achieved script
identification accuracies in percentage as 98.7, 98.16 and 96.91 respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2764</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2764</id><created>2013-03-11</created><authors><author><keyname>Lin</keyname><forenames>Na</forenames></author><author><keyname>Liu</keyname><forenames>Hong-Dong</forenames></author><author><keyname>Gong</keyname><forenames>Chang-Qing</forenames></author></authors><title>Research and Simulation on Drivers' Route Choice Behavior Cognition
  Model</title><categories>cs.NI</categories><comments>6 pages,8 figures,a table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper studied the behavior-cognitive model of drivers during their
travel based on the current research on driver behavior. Firstly, a route
choice behavior-cognitive model was proposed for describing the decision-making
mechanism of drivers during his travel; then, simulation experiments were
carried out on the cosimulation VBc-vissim platform. From the experimental
results, dynamic behavior features of drivers during their travel can be
properly explained by the behavior-cognitive model, thus optimal path can be
obtained from this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2766</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2766</id><created>2013-03-11</created><updated>2013-10-09</updated><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Gunawan</keyname><forenames>Erry</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Optimized Transmission with Improper Gaussian Signaling in the K-User
  MISO Interference Channel</title><categories>cs.IT math.IT</categories><comments>27 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the achievable rate region of the K-user Gaussian
multiple-input single-output interference channel (MISO-IC) with the
interference treated as noise, when improper or circularly asymmetric complex
Gaussian signaling is applied. The transmit optimization with improper Gaussian
signaling involves not only the signal covariance matrix as in the conventional
proper or circularly symmetric Gaussian signaling, but also the signal
pseudo-covariance matrix, which is conventionally set to zero in proper
Gaussian signaling. By exploiting the separable rate expression with improper
Gaussian signaling, we propose a separate transmit covariance and
pseudo-covariance optimization algorithm, which is guaranteed to improve the
users' achievable rates over the conventional proper Gaussian signaling. In
particular, for the pseudo-covariance optimization, we establish the optimality
of rank-1 pseudo-covariance matrices, given the optimal rank-1 transmit
covariance matrices for achieving the Pareto boundary of the rate region. Based
on this result, we are able to greatly reduce the number of variables in the
pseudo-covariance optimization problem and thereby develop an efficient
solution by applying the celebrated semidefinite relaxation (SDR) technique.
Finally, we extend the result to the Gaussian MISO broadcast channel (MISO-BC)
with improper Gaussian signaling or so-called widely linear transmit precoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2772</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2772</id><created>2013-03-11</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Further analysis of the binary Euclidean algorithm</title><categories>cs.DS</categories><comments>An old Technical Report which no longer seems to be available from
  the Oxford University website</comments><report-no>PRG TR-7-99</report-no><msc-class>68Q25 (Primary) 68W40, 65Y20 (Secondary)</msc-class><acm-class>F.2.1</acm-class><journal-ref>Technical Report PRG TR-7-99, Oxford University Computing
  Laboratory, November 1999, 18 pp</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The binary Euclidean algorithm is a variant of the classical Euclidean
algorithm. It avoids multiplications and divisions, except by powers of two, so
is potentially faster than the classical algorithm on a binary machine. We
describe the binary algorithm and consider its average case behaviour. In
particular, we correct some errors in the literature, discuss some results of
Vall\'ee, and describe a numerical computation which supports a conjecture of
Vall\'ee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2774</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2774</id><created>2013-03-11</created><updated>2013-06-27</updated><authors><author><keyname>Huang</keyname><forenames>Yichao</forenames></author><author><keyname>Tan</keyname><forenames>Chee Wei</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Joint Beamforming and Power Control in Coordinated Multicell: Max-Min
  Duality, Effective Network and Large System Transition</title><categories>cs.IT math.IT</categories><comments>Some typos in the version publised in the IEEE Transactions on
  Wireless Communications are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies joint beamforming and power control in a coordinated
multicell downlink system that serves multiple users per cell to maximize the
minimum weighted signal-to-interference-plus-noise ratio. The optimal solution
and distributed algorithm with geometrically fast convergence rate are derived
by employing the nonlinear Perron-Frobenius theory and the multicell network
duality. The iterative algorithm, though operating in a distributed manner,
still requires instantaneous power update within the coordinated cluster
through the backhaul. The backhaul information exchange and message passing may
become prohibitive with increasing number of transmit antennas and increasing
number of users. In order to derive asymptotically optimal solution, random
matrix theory is leveraged to design a distributed algorithm that only requires
statistical information. The advantage of our approach is that there is no
instantaneous power update through backhaul. Moreover, by using nonlinear
Perron-Frobenius theory and random matrix theory, an effective primal network
and an effective dual network are proposed to characterize and interpret the
asymptotic solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2779</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2779</id><created>2013-03-12</created><authors><author><keyname>Penninger</keyname><forenames>Rainer</forenames></author><author><keyname>Vigan</keyname><forenames>Ivo</forenames></author></authors><title>Point Set Isolation Using Unit Disks is NP-complete</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the situation where one is given a set S of points in the plane
and a collection D of unit disks embedded in the plane. We show that finding a
minimum cardinality subset of D such that any path between any two points in S
is intersected by at least one disk is NP-complete. This settles an open
problem raised by Matt Gibson et al[1]. Using a similar reduction, we show that
finding a minimum cardinality subset D' of D such that R^2 - (D - D') consists
of a single connected region is also NP-complete. Lastly, we show that the
Multiterminal Cut Problem remains NP-complete when restricted to unit disk
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2783</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2783</id><created>2013-03-12</created><authors><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Wong</keyname><forenames>Yongkang</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Combined Learning of Salient Local Descriptors and Distance Metrics for
  Image Set Face Verification</title><categories>cs.CV</categories><acm-class>I.5.1; I.5.4; I.2.10</acm-class><journal-ref>IEEE International Conference on Advanced Video and Signal-Based
  Surveillance (AVSS), pp, 294-299, 2012</journal-ref><doi>10.1109/AVSS.2012.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast to comparing faces via single exemplars, matching sets of face
images increases robustness and discrimination performance. Recent image set
matching approaches typically measure similarities between subspaces or
manifolds, while representing faces in a rigid and holistic manner. Such
representations are easily affected by variations in terms of alignment,
illumination, pose and expression. While local feature based representations
are considerably more robust to such variations, they have received little
attention within the image set matching area. We propose a novel image set
matching technique, comprised of three aspects: (i) robust descriptors of face
regions based on local features, partly inspired by the hierarchy in the human
visual system, (ii) use of several subspace and exemplar metrics to compare
corresponding face regions, (iii) jointly learning which regions are the most
discriminative while finding the optimal mixing weights for combining metrics.
Face recognition experiments on LFW, PIE and MOBIO face datasets show that the
proposed algorithm obtains considerably better performance than several recent
state-of-the-art techniques, such as Local Principal Angle and the Kernel
Affine Hull Method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2784</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2784</id><created>2013-03-12</created><authors><author><keyname>Just</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Ernst</keyname><forenames>Michael D.</forenames></author><author><keyname>Fraser</keyname><forenames>Gordon</forenames></author></authors><title>Using State Infection Conditions to Detect Equivalent Mutants and Speed
  up Mutation Analysis</title><categories>cs.SE</categories><report-no>DPA-13021</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutation analysis evaluates test suites and testing techniques by measuring
how well they detect seeded defects (mutants). Even though well established in
research, mutation analysis is rarely used in practice due to scalability
problems --- there are multiple mutations per code statement leading to a large
number of mutants, and hence executions of the test suite. In addition, the use
of mutation to improve test suites is futile for mutants that are equivalent,
which means that there exists no test case that distinguishes them from the
original program.
  This paper introduces two optimizations based on state infection conditions,
i.e., conditions that determine for a test execution whether the same execution
on a mutant would lead to a different state. First, redundant test execution
can be avoided by monitoring state infection conditions, leading to an overall
performance improvement. Second, state infection conditions can aid in
identifying equivalent mutants, thus guiding efforts to improve test suites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2789</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2789</id><created>2013-03-12</created><authors><author><keyname>Saad</keyname><forenames>Hussein</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author></authors><title>A Cooperative Q-learning Approach for Real-time Power Allocation in
  Femtocell Networks</title><categories>cs.MA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of distributed interference management
of cognitive femtocells that share the same frequency range with macrocells
(primary user) using distributed multi-agent Q-learning. We formulate and solve
three problems representing three different Q-learning algorithms: namely,
centralized, distributed and partially distributed power control using
Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest,
characterizes the global optimum. Each of DPC-Q and PDPC-Q works in two
different learning paradigms: Independent (IL) and Cooperative (CL). The former
is considered the simplest form for applying Qlearning in multi-agent
scenarios, where all the femtocells learn independently. The latter is the
proposed scheme in which femtocells share partial information during the
learning process in order to strike a balance between practical relevance and
performance. In terms of performance, the simulation results showed that the CL
paradigm outperforms the IL paradigm and achieves an aggregate femtocells
capacity that is very close to the optimal one. For the practical relevance
issue, we evaluate the robustness and scalability of DPC-Q, in real time, by
deploying new femtocells in the system during the learning process, where we
showed that DPC-Q in the CL paradigm is scalable to large number of femtocells
and more robust to the network dynamics compared to the IL paradigm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2792</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2792</id><created>2013-03-12</created><authors><author><keyname>Taha</keyname><forenames>Walid</forenames></author><author><keyname>Philippsen</keyname><forenames>Roland</forenames></author></authors><title>Modeling Basic Aspects of Cyber-Physical Systems</title><categories>cs.RO</categories><comments>Presented at DSLRob 2012 (arXiv:cs/1302.5082)</comments><report-no>DSLRob/2012/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing novel cyber-physical systems entails significant, costly physical
experimentation. Simulation tools can enable the virtualization of experiments.
Unfortunately, current tools have shortcomings that limit their utility for
virtual experimentation. Language research can be especially helpful in
addressing many of these problems. As a first step in this direction, we
consider the question of determining what language features are needed to model
cyber-physical systems. Using a series of elementary examples of cyber-physical
systems, we reflect on the extent to which a small, experimental
domain-specific formalism called Acumen suffices for this purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2799</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2799</id><created>2013-03-12</created><updated>2013-05-21</updated><authors><author><keyname>Fyhn</keyname><forenames>Karsten</forenames></author><author><keyname>Dadkhahi</keyname><forenames>Hamid</forenames></author><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author></authors><title>Spectral Compressive Sensing with Polar Interpolation</title><categories>cs.IT math.IT</categories><comments>v1: 5 pages, 2 figures, accepted for publication at ICASSP 2013.
  v2,v3: This version corrects minor typos in Algorithm 1 from the published
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing approaches to compressive sensing of frequency-sparse signals
focuses on signal recovery rather than spectral estimation. Furthermore, the
recovery performance is limited by the coherence of the required sparsity
dictionaries and by the discretization of the frequency parameter space. In
this paper, we introduce a greedy recovery algorithm that leverages a
band-exclusion function and a polar interpolation function to address these two
issues in spectral compressive sensing. Our algorithm is geared towards line
spectral estimation from compressive measurements and outperforms most existing
approaches in fidelity and tolerance to noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2812</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2812</id><created>2013-03-12</created><authors><author><keyname>Bacci</keyname><forenames>Giacomo</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Luise</keyname><forenames>Marco</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Energy-Efficient Power Control for Contention-Based Synchronization in
  OFDMA Systems with Discrete Powers and Limited Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work derives a distributed and iterative algorithm by which mobile
terminals can selfishly control their transmit powers during the
synchronization procedure specified by the IEEE 802.16m and the 3GPP-LTE
standards for orthogonal frequency-division multiple-access technologies. The
proposed solution aims at maximizing the energy efficiency of the network and
is derived on the basis of a finite noncooperative game in which the players
have discrete action sets of transmit powers. The set of Nash equilibria of the
game is investigated, and a distributed power control algorithm is proposed to
achieve synchronization in an energy-efficient manner under the assumption that
the feedback from the base station is limited. Numerical results show that the
proposed solution improves the energy efficiency as well as the timing
estimation accuracy of the network compared to existing alternatives, while
requiring a reasonable amount of information to be exchanged on the return
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2817</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2817</id><created>2013-03-12</created><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>D'Amico</keyname><forenames>Antonio A.</forenames></author><author><keyname>Rong</keyname><forenames>Yue</forenames></author></authors><title>A Tutorial on the Optimization of Amplify-and-Forward MIMO Relay Systems</title><categories>cs.IT math.IT</categories><journal-ref>IEEE J. Select. Areas Commun., vol. 30, no. 8, Sept. 2012</journal-ref><doi>10.1109/JSAC.2012.120904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable promise of multiple-input multiple-output (MIMO) wireless
channels has motivated an intense research activity to characterize the
theoretical and practical issues associated with the design of transmit
(source) and receive (destination) processing matrices under different
operating conditions. This activity was primarily focused on point-to-point
(single-hop) communications but more recently there has been an extensive work
on two-hop or multi-hop settings in which single or multiple relays are used to
deliver the information from the source to the destination. The aim of this
tutorial is to provide an up-to-date overview of the fundamental results and
practical implementation issues of designing amplify-and-forward MIMO relay
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2820</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2820</id><created>2013-03-12</created><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>D'Amico</keyname><forenames>Antonio A.</forenames></author></authors><title>Power Allocation in Two-Hop Amplify-and-Forward MIMO Relay Systems with
  QoS requirements</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Signal Process., vol. 60, no. 5, May 2012</journal-ref><doi>10.1109/TSP.2012.2187198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of minimizing the total power consumption while satisfying
different quality-of-service (QoS) requirements in a two-hop multiple-input
multiple-output network with a single non-regenerative relay is considered. As
shown by Y. Rong in [1], the optimal processing matrices for both linear and
non-linear transceiver architectures lead to the diagonalization of the
source-relay-destination channel so that the power minimization problem reduces
to properly allocating the available power over the established links.
Unfortunately, finding the solution of this problem is numerically difficult as
it is not in a convex form. To overcome this difficulty, existing solutions
rely on the computation of upper- and lower-bounds that are hard to obtain or
require the relaxation of the QoS constraints. In this work, a novel approach
is devised for both linear and non-linear transceiver architectures, which
allows to closely approximate the solutions of the non-convex power allocation
problems with those of convex ones easy to compute in closed-form by means of
multi-step procedures of reduced complexity. Computer simulations are used to
assess the performance of the proposed approach and to make comparisons with
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2823</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2823</id><created>2013-03-12</created><updated>2013-09-27</updated><authors><author><keyname>P&#xe9;rez-Cruz</keyname><forenames>Fernando</forenames></author><author><keyname>Van Vaerenbergh</keyname><forenames>Steven</forenames></author><author><keyname>Murillo-Fuentes</keyname><forenames>Juan Jos&#xe9;</forenames></author><author><keyname>L&#xe1;zaro-Gredilla</keyname><forenames>Miguel</forenames></author><author><keyname>Santamaria</keyname><forenames>Ignacio</forenames></author></authors><title>Gaussian Processes for Nonlinear Signal Processing</title><categories>cs.LG cs.IT math.IT stat.ML</categories><journal-ref>IEEE Signal Processing Magazine, vol.30, no.4, pp.40-50, July 2013</journal-ref><doi>10.1109/MSP.2013.2250352</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GPs) are versatile tools that have been successfully
employed to solve nonlinear estimation problems in machine learning, but that
are rarely used in signal processing. In this tutorial, we present GPs for
regression as a natural nonlinear extension to optimal Wiener filtering. After
establishing their basic formulation, we discuss several important aspects and
extensions, including recursive and adaptive algorithms for dealing with
non-stationarity, low-complexity solutions, non-Gaussian noise models and
classification scenarios. Furthermore, we provide a selection of relevant
applications to wireless digital communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2824</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2824</id><created>2013-03-12</created><authors><author><keyname>Kang</keyname><forenames>Ty</forenames></author></authors><title>Fourth-order flows in surface modelling</title><categories>cs.GR math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short article is a brief account of the usage of fourth-order curvature
flow in surface modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2826</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2826</id><created>2013-03-12</created><authors><author><keyname>Darling</keyname><forenames>William M.</forenames></author><author><keyname>Song</keyname><forenames>Fei</forenames></author></authors><title>Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA</title><categories>cs.CL</categories><comments>Currently under review for the journal Computational Linguistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a probabilistic generative model for text based on
semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA).
POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and
long-range semantic patterns (topics) that exist in document collections. This
results in word distributions that are specific to both topics (sports,
education, ...) and parts-of-speech (nouns, verbs, ...). For example,
multinomial distributions over words are uncovered that can be understood as
&quot;nouns about weather&quot; or &quot;verbs about law&quot;. We describe the model and an
approximate inference algorithm and then demonstrate the quality of the learned
topics both qualitatively and quantitatively. Then, we discuss an NLP
application where the output of POSLDA can lead to strong improvements in
quality: unsupervised part-of-speech tagging. We describe algorithms for this
task that make use of POSLDA-learned distributions that result in improved
performance beyond the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2830</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2830</id><created>2013-03-12</created><authors><author><keyname>Ravazzi</keyname><forenames>Chiara</forenames></author><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author></authors><title>Almost sure convergence of a randomized algorithm for relative
  localization in sensor networks</title><categories>cs.SY math.OC</categories><comments>2 figures; submitted to conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper regards the relative localization problem in sensor networks. We
study a randomized algorithm, which is based on input-driven consensus dynamics
and involves pairwise &quot;gossip&quot; communications and updates. Due to the
randomness of the updates, the state of this algorithm ergodically oscillates
around a limit value. Exploiting the ergodicity of the dynamics, we show that
the time-average of the state almost surely converges to the least-squares
solution of the localization problem. Remarkably, the computation of the
time-average does not require the sensors to share any common clock. Hence, the
proposed algorithm is fully distributed and asynchronous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2837</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2837</id><created>2013-03-12</created><authors><author><keyname>Iutzeler</keyname><forenames>Franck</forenames></author><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author><author><keyname>Ciblat</keyname><forenames>Philippe</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Asynchronous Distributed Optimization using a Randomized Alternating
  Direction Method of Multipliers</title><categories>cs.DC math.OC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of networked agents endowed with private cost functions and
seeking to find a consensus on the minimizer of the aggregate cost. A new class
of random asynchronous distributed optimization methods is introduced. The
methods generalize the standard Alternating Direction Method of Multipliers
(ADMM) to an asynchronous setting where isolated components of the network are
activated in an uncoordinated fashion. The algorithms rely on the introduction
of randomized Gauss-Seidel iterations of a Douglas-Rachford operator for
finding zeros of a sum of two monotone operators. Convergence to the sought
minimizers is provided under mild connectivity conditions. Numerical results
sustain our claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2844</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2844</id><created>2013-03-12</created><authors><author><keyname>Felzenszwalb</keyname><forenames>Pedro F.</forenames></author></authors><title>A Stochastic Grammar for Natural Shapes</title><categories>cs.CV</categories><doi>10.1007/978-1-4471-5195-1_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider object detection using a generic model for natural shapes. A
common approach for object recognition involves matching object models directly
to images. Another approach involves building intermediate representations via
a generic grouping processes. We argue that these two processes (model-based
recognition and grouping) may use similar computational mechanisms. By defining
a generic model for shapes we can use model-based techniques to implement a
mid-level vision grouping process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2857</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2857</id><created>2013-03-12</created><authors><author><keyname>Jebbar</keyname><forenames>Mostafa</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author><author><keyname>Benammar</keyname><forenames>Othman</forenames></author></authors><title>Integration of SOA and Cloud Computing in RM-ODP</title><categories>cs.SE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of ODP is according to ITU-T Recommendation X.901 stated as
follows: The objective of ODP standardization is the development of standards
that allow the benefits of distributing information processing services to be
realized in an environment of heterogeneous IT resources and multiple
organizational domains. These standards address constraints on system
specification and the provision of a system infrastructure that accommodate
difficulties inherent in the design and programming of distributed systems.
This objective seems to cover cloud computing systems. Therefore, we in this
paper discuss the concepts of cloud, and discuss the use of RM-ODP for
specifying the solution. We indicate that the current RM-ODP may be too
abstract for the purpose, and indicate how to adapt RM-ODP to fit the purpose
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2860</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2860</id><created>2013-03-12</created><authors><author><keyname>M&#xfc;hlenthaler</keyname><forenames>Moritz</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>Fairness in Academic Course Timetabling</title><categories>cs.AI cs.DS</categories><comments>appeared in PATAT 2012, pp. 114-130</comments><acm-class>I.2.8; F.2.2</acm-class><doi>10.1007/s10479-014-1553-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of creating fair course timetables in the setting of
a university. Our motivation is to improve the overall satisfaction of
individuals concerned (students, teachers, etc.) by providing a fair timetable
to them. The central idea is that undesirable arrangements in the course
timetable, i.e., violations of soft constraints, should be distributed in a
fair way among the individuals. We propose two formulations for the fair course
timetabling problem that are based on max-min fairness and Jain's fairness
index, respectively. Furthermore, we present and experimentally evaluate an
optimization algorithm based on simulated annealing for solving max-min fair
course timetabling problems. The new contribution is concerned with measuring
the energy difference between two timetables, i.e., how much worse a timetable
is compared to another timetable with respect to max-min fairness. We introduce
three different energy difference measures and evaluate their impact on the
overall algorithm performance. The second proposed problem formulation focuses
on the tradeoff between fairness and the total amount of soft constraint
violations. Our experimental evaluation shows that the known best solutions to
the ITC2007 curriculum-based course timetabling instances are quite fair with
respect to Jain's fairness index. However, the experiments also show that the
fairness can be improved further for only a rather small increase in the total
amount of soft constraint violations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2868</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2868</id><created>2013-03-12</created><authors><author><keyname>Camby</keyname><forenames>Eglantine</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>A Note on Connected Dominating Set in Graphs Without Long Paths And
  Cycles</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ratio of the connected domination number, $\gamma_c$, and the domination
number, $\gamma$, is strictly bounded from above by 3. It was shown by
Zverovich that for every connected $(P_5,C_5)$-free graph, $\gamma_c = \gamma$.
  In this paper, we investigate the interdependence of $\gamma$ and $\gamma_c$
in the class of $(P_k,C_k)$-free graphs, for $k \ge 6$. We prove that for every
connected $(P_6,C_6)$-free graph, $\gamma_c \le \gamma + 1$ holds, and there is
a family of $(P_6,C_6)$-free graphs with arbitrarily large values of $\gamma$
attaining this bound. Moreover, for every connected $(P_8,C_8)$-free graph,
$\gamma_c / \gamma \le 2$, and there is a family of $(P_7,C_7)$-free graphs
with arbitrarily large values of $\gamma$ attaining this bound. In the class of
$(P_9,C_9)$-free graphs, the general bound $\gamma_c / \gamma \le 3$ is
asymptotically sharp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2870</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2870</id><created>2013-03-12</created><updated>2014-07-31</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>CoMP Meets Smart Grid: A New Communication and Energy Cooperation
  Paradigm</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we pursue a unified study on smart grid and coordinated
multi-point (CoMP) enabled wireless communication by investigating a new joint
communication and energy cooperation approach. We consider a practical CoMP
system with clustered multiple-antenna base stations (BSs) cooperatively
communicating with multiple single-antenna mobile terminals (MTs), where each
BS is equipped with local renewable energy generators to supply power and also
a smart meter to enable two-way energy flow with the grid. We propose a new
energy cooperation paradigm, where a group of BSs dynamically share their
renewable energy for more efficient operation via locally injecting/drawing
power to/from an aggregator with a zero effective sum-energy exchanged. Under
this new energy cooperation model, we consider the downlink transmission in one
CoMP cluster with cooperative zero-forcing (ZF) based precoding at the BSs. We
maximize the weighted sum-rate for all MTs by jointly optimizing the transmit
power allocations at cooperative BSs and their exchanged energy amounts subject
to a new type of power constraints featuring energy cooperation among BSs with
practical loss ratios. Our new setup with BSs' energy cooperation generalizes
the conventional CoMP transmit optimization under BSs' sum-power or
individual-power constraints. Finally, we validate our results by simulations
under various practical setups, and show that the proposed joint communication
and energy cooperation scheme substantially improves the downlink throughput of
CoMP systems powered by smart grid and renewable energy, as compared to other
suboptimal designs without communication and/or energy cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2873</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2873</id><created>2013-03-12</created><authors><author><keyname>Bamman</keyname><forenames>David</forenames></author><author><keyname>Anderson</keyname><forenames>Adam</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Inferring Social Rank in an Old Assyrian Trade Network</title><categories>cs.CY cs.SI</categories><comments>Digital Humanities 2013 (Lincoln, Nebraska)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present work in jointly inferring the unique individuals as well as their
social rank within a collection of letters from an Old Assyrian trade colony in
K\&quot;ultepe, Turkey, settled by merchants from the ancient city of Assur for
approximately 200 years between 1950-1750 BCE, the height of the Middle Bronze
Age. Using a probabilistic latent-variable model, we leverage pairwise social
differences between names in cuneiform tablets to infer a single underlying
social order that best explains the data we observe. Evaluating our output with
published judgments by domain experts suggests that our method may be used for
building informed hypotheses that are driven by data, and that may offer
promising avenues for directed research by Assyriologists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2896</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2896</id><created>2013-03-12</created><updated>2014-07-31</updated><authors><author><keyname>Gay</keyname><forenames>Simon J.</forenames><affiliation>University of Glasgow</affiliation></author><author><keyname>Puthoor</keyname><forenames>Ittoop Vergheese</forenames><affiliation>University of Glasgow</affiliation></author></authors><title>Application of Quantum Process Calculus to Higher Dimensional Quantum
  Protocols</title><categories>cs.LO quant-ph</categories><comments>In Proceedings QPL 2012, arXiv:1407.8427</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 158, 2014, pp. 15-28</journal-ref><doi>10.4204/EPTCS.158.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the use of quantum process calculus to describe and analyze
quantum communication protocols, following the successful field of formal
methods from classical computer science. We have extended the quantum process
calculus to describe d-dimensional quantum systems, which has not been done
before. We summarise the necessary theory in the generalisation of quantum
gates and Bell states and use the theory to apply the quantum process calculus
CQP to quantum protocols, namely qudit teleportation and superdense coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2912</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2912</id><created>2013-03-12</created><updated>2013-09-17</updated><authors><author><keyname>Frigola</keyname><forenames>Roger</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl Edward</forenames></author></authors><title>Integrated Pre-Processing for Bayesian Nonlinear System Identification
  with Gaussian Processes</title><categories>cs.AI cs.RO cs.SY stat.ML</categories><comments>Proceedings of the 52th IEEE International Conference on Decision and
  Control (CDC), Firenze, Italy, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce GP-FNARX: a new model for nonlinear system identification based
on a nonlinear autoregressive exogenous model (NARX) with filtered regressors
(F) where the nonlinear regression problem is tackled using sparse Gaussian
processes (GP). We integrate data pre-processing with system identification
into a fully automated procedure that goes from raw data to an identified
model. Both pre-processing parameters and GP hyper-parameters are tuned by
maximizing the marginal likelihood of the probabilistic model. We obtain a
Bayesian model of the system's dynamics which is able to report its uncertainty
in regions where the data is scarce. The automated approach, the modeling of
uncertainty and its relatively low computational cost make of GP-FNARX a good
candidate for applications in robotics and adaptive control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2920</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2920</id><created>2013-03-12</created><updated>2013-03-26</updated><authors><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Approximating 1-dimensional TSP Requires Omega(n log n) Comparisons</title><categories>cs.DS</categories><comments>Superseded by &quot;On the complexity of approximating Euclidean traveling
  salesman tours and minimum spanning trees&quot;, by Das et al; Algorithmica
  19:447-460 (1997)</comments><msc-class>68W25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a short proof that any comparison-based n^(1-epsilon)-approximation
algorithm for the 1-dimensional Traveling Salesman Problem (TSP) requires
Omega(n log n) comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2933</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2933</id><created>2013-03-09</created><authors><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Cardieri</keyname><forenames>Paulo</forenames></author><author><keyname>Kretzschmar</keyname><forenames>William A.</forenames><suffix>Jr.</suffix></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Interference Networks: A Complex System View</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an unusual view of interference wireless networks based
on complex system thinking. To proceed with this analysis, a literature review
of the different applications of complex systems is firstly presented to
illustrate how such an approach can be used in a wide range of research topics,
from economics to linguistics. Then the problem of quantifying the fundamental
limits of wireless systems where the co-channel interference is the main
limiting factor is described and hence contextualized in the perspective of
complex systems. Specifically some possible internal and external pressures
that the network elements may suffer are identified as, for example, queue
stability, maximum packet loss rate and transmit power constraint. Besides,
other important external factors such as mobility and incoming traffic are also
pointed out. As a study case, a decentralized point-to-point interference
network is described and several claims about the optimal design setting for
different network states and under two mobility conditions, namely quasi-static
and highly mobile, are stated based on results found in the literature. Using
these claims as a background, the design of a robust adaptive algorithm that
each network element should run is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2952</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2952</id><created>2013-03-12</created><authors><author><keyname>Li</keyname><forenames>Shi</forenames></author><author><keyname>Tucci</keyname><forenames>Gabriel H</forenames></author></authors><title>Traffic Congestion in Expanders, $(p,\delta)$--Hyperbolic Spaces and
  Product of Trees</title><categories>math.CO cs.DM cs.NI</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we define the notion of $(p,\delta)$--Gromov hyperbolic space
where we relax Gromov's {\it slimness} condition to allow that not all but a
positive fraction of all triangles are $\delta$--slim. Furthermore, we study
maximum vertex congestion under geodesic routing and show that it scales as
$\Omega(p^2n^2/D_n^2)$ where $D_n$ is the diameter of the graph. We also
construct a constant degree family of expanders with congestion $\Theta(n^2)$
in contrast with random regular graphs that have congestion $O(n\log^{3}(n))$.
Finally, we study traffic congestion on graphs defined as product of trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2963</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2963</id><created>2013-03-12</created><authors><author><keyname>M&#xf6;mke</keyname><forenames>Tobias</forenames></author></authors><title>A Competitive Ratio Approximation Scheme for the k-Server Problem in
  Fixed Finite Metrics</title><categories>cs.DS</categories><comments>9 pages</comments><acm-class>F.1.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to restrict the analysis of a class of online problems that
includes the $k$-server problem in finite metrics such that we only have to
consider finite sequences of request. When applying the restrictions, both the
optimal offline solutions and the best possible deterministic or randomized
online solutions only differ by at most an arbitrarily small constant factor
from the corresponding solutions without restrictions. Furthermore, we show how
to obtain an algorithm with best possible deterministic or randomized
competitive ratio for the restricted setup. Thus, for each fixed finite metrics
our result qualifies as a competitive ratio approximation scheme as defined by
G\&quot;unther et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2966</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2966</id><created>2013-03-12</created><authors><author><keyname>Flammini</keyname><forenames>Francesco</forenames></author><author><keyname>Mazzocca</keyname><forenames>Nicola</forenames></author><author><keyname>Orazzo</keyname><forenames>Antonio</forenames></author></authors><title>Automatic instantiation of abstract tests on specific configurations for
  large critical control systems</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer-based control systems have grown in size, complexity, distribution
and criticality. In this paper a methodology is presented to perform an
abstract testing of such large control systems in an efficient way: an abstract
test is specified directly from system functional requirements and has to be
instantiated in more test runs to cover a specific configuration, comprising
any number of control entities (sensors, actuators and logic processes). Such a
process is usually performed by hand for each installation of the control
system, requiring a considerable time effort and being an error prone
verification activity. To automate a safe passage from abstract tests, related
to the so called generic software application, to any specific installation, an
algorithm is provided, starting from a reference architecture and a state-based
behavioural model of the control software. The presented approach has been
applied to a railway interlocking system, demonstrating its feasibility and
effectiveness in several years of testing experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2967</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2967</id><created>2013-03-12</created><authors><author><keyname>Choffrut</keyname><forenames>A. Bertoni. Ch.</forenames></author><author><keyname>D'Alessandro</keyname><forenames>F.</forenames></author></authors><title>Quantum finite automata and linear context-free languages: a decidable
  problem</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the so-called measure once finite quantum automata model
introduced by Moore and Crutchfield in 2000. We show that given a language
recognized by such a device and a linear context-free language, it is
recursively decidable whether or not they have a nonempty intersection. This
extends a result of Blondel et al. which can be interpreted as solving the
problem with the free monoid in place of the family of linear context-free
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2974</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2974</id><created>2013-03-12</created><authors><author><keyname>Blakey</keyname><forenames>Ed</forenames></author></authors><title>Complexity-Style Resources in Cryptography</title><categories>cs.CR cs.CC</categories><comments>21 pages. 4 figures. To appear in Information and Computation
  (special issue on Information Security as a Resource)</comments><journal-ref>Information and Computation, vol. 226, pp. 3 - 15, 2013</journal-ref><doi>10.1016/j.ic.2013.03.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previously, the author has developed a framework within which to quantify and
compare the resources consumed during computational-especially unconventional
computational-processes (adding to the familiar resources of run-time and
memory space such non-standard resources as precision and energy); it is
natural and beneficial in this framework to employ various complexity-theoretic
tools and techniques. Here, we seek an analogous treatment not of computational
processes but of cryptographic protocols and similar, so as to be able to apply
the existing arsenal of complexity-theoretic methods in new ways, in the
derivation and verification of protocols in a wider, cryptographic context.
Accordingly, we advocate a framework in which one may view as resources the
costs-which may be related to computation, communication, information
(including side-channel information) or availability of primitives, for
example-incurred when executing cryptographic protocols, coin-tossing schemes,
etc. The ultimate aim is to formulate as a resource, and be able to analyse
complexity-theoretically, the security of these protocols and schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2975</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2975</id><created>2013-03-12</created><updated>2013-06-09</updated><authors><author><keyname>Grov</keyname><forenames>Gudmund</forenames></author><author><keyname>Maclean</keyname><forenames>Ewen</forenames></author></authors><title>Towards Automated Proof Strategy Generalisation</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to automatically generalise (interactive) proofs and use such
generalisations to discharge related conjectures is a very hard problem which
remains unsolved. Here, we develop a notion of goal types to capture key
properties of goals, which enables abstractions over the specific order and
number of sub-goals arising when composing tactics. We show that the goal types
form a lattice, and utilise this property in the techniques we develop to
automatically generalise proof strategies in order to reuse it for proofs of
related conjectures. We illustrate our approach with an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2981</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2981</id><created>2013-03-12</created><updated>2014-03-30</updated><authors><author><keyname>Chonev</keyname><forenames>Ventsislav</forenames></author><author><keyname>Ouaknine</keyname><forenames>Jo&#xeb;l</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>On the Complexity of the Orbit Problem</title><categories>cs.CC cs.DS</categories><acm-class>F.2.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider higher-dimensional versions of Kannan and Lipton's Orbit
Problem---determining whether a target vector space V may be reached from a
starting point x under repeated applications of a linear transformation A.
Answering two questions posed by Kannan and Lipton in the 1980s, we show that
when V has dimension one, this problem is solvable in polynomial time, and when
V has dimension two or three, the problem is in NP^{RP}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.2987</identifier>
 <datestamp>2013-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.2987</id><created>2013-03-12</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Strelkowa</keyname><forenames>Natalja</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author></authors><title>On Periodic Reference Tracking Using Batch-Mode Reinforcement Learning
  with Application to Gene Regulatory Network Control</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the periodic reference tracking problem in the
framework of batch-mode reinforcement learning, which studies methods for
solving optimal control problems from the sole knowledge of a set of
trajectories. In particular, we extend an existing batch-mode reinforcement
learning algorithm, known as Fitted Q Iteration, to the periodic reference
tracking problem. The presented periodic reference tracking algorithm
explicitly exploits a priori knowledge of the future values of the reference
trajectory and its periodicity. We discuss the properties of our approach and
illustrate it on the problem of reference tracking for a synthetic biology gene
regulatory network known as the generalised repressilator. This system can
produce decaying but long-lived oscillations, which makes it an interesting
system for the tracking problem. In our companion paper we also take a look at
the regulation problem of the toggle switch system, where the main goal is to
drive the system's states to a specific bounded region in the state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3018</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3018</id><created>2013-03-12</created><updated>2015-05-25</updated><authors><author><keyname>Zhang</keyname><forenames>Zhenliang</forenames></author><author><keyname>Chong</keyname><forenames>Edwin K. P.</forenames></author><author><keyname>Pezeshki</keyname><forenames>Ali</forenames></author><author><keyname>Moran</keyname><forenames>William</forenames></author></authors><title>String Submodular Functions with Curvature Constraints</title><categories>cs.DS cs.DM</categories><comments>to appear in IEEE Transaction on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of objectively choosing a string of actions to optimize an
objective function that is string submodular has been considered in [1]. There
it is shown that the greedy strategy, consisting of a string of actions that
only locally maximizes the step-wise gain in the objective function achieves at
least a (1-e^{-1})-approximation to the optimal strategy. This paper improves
this approximation by introducing additional constraints on curvatures, namely,
total backward curvature, total forward curvature, and elemental forward
curvature. We show that if the objective function has total backward curvature
\sigma, then the greedy strategy achieves at least a
\frac{1}{\sigma}(1-e^{-\sigma})-approximation of the optimal strategy. If the
objective function has total forward curvature \epsilon, then the greedy
strategy achieves at least a (1-\epsilon)-approximation of the optimal
strategy. Moreover, we consider a generalization of the diminishing-return
property by defining the elemental forward curvature. We also consider the
problem of maximizing the objective function subject to general a
string-matroid constraint. We investigate an applications of string submodular
functions with curvature constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3026</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3026</id><created>2013-03-12</created><authors><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author></authors><title>Stochastic Service Curve and Delay Bound Analysis: A Single Node Case</title><categories>cs.PF cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A packet-switched network node with constant capacity (in bps) is considered,
where packets within each flow are served in the first in first out (FIFO)
manner. While this single node system is perhaps the simplest computer
communication system, its stochastic service curve characterization and
independent case analysis in the context of stochastic network calculus
(snetcal) are still basic and many crucial questions surprisingly remain open.
Specifically, when the input is a single flow, what stochastic service curve
and delay bound does the node provide? When the considered flow shares the node
with another flow, what stochastic service curve and delay bound does the node
provide to the considered flow, and if the two flows are independent, can this
independence be made use of and how? The aim of this paper is to provide
answers to these fundamental questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3036</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3036</id><created>2013-03-12</created><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, IRIT</affiliation></author></authors><title>Type-theoretical natural language semantics: on the system F for meaning
  assembly</title><categories>cs.LO cs.CL math.LO</categories><proxy>ccsd</proxy><journal-ref>TYPES 2013, Toulouse : France (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents and extends our type theoretical framework for a
compositional treatment of natural language semantics with some lexical
features like coercions (e.g. of a town into a football club) and copredication
(e.g. on a town as a set of people and as a location). The second order typed
lambda calculus was shown to be a good framework, and here we discuss how to
introduced predefined types and coercive subtyping which are much more natural
than internally coded similar constructs. Linguistic applications of these new
features are also exemplified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3047</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3047</id><created>2013-03-12</created><authors><author><keyname>Stampar</keyname><forenames>Miroslav</forenames></author></authors><title>Data Retrieval over DNS in SQL Injection Attacks</title><categories>cs.CR cs.DB cs.NI</categories><comments>7 pages, 3 figures, 1 table. Presented at PHDays 2012 security
  conference, Moscow, Russia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an advanced SQL injection technique where DNS resolution
process is exploited for retrieval of malicious SQL query results. Resulting
DNS requests are intercepted by attackers themselves at the controlled remote
name server extracting valuable data. Open source SQL injection tool sqlmap has
been adjusted to automate this task. With modifications done, attackers are
able to use this technique for fast and low profile data retrieval, especially
in cases where other standard ones fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3049</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3049</id><created>2013-03-12</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>On Optimal Jamming Over an Additive Noise Channel</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>Submitted to 2013 Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of optimal zero-delay jamming over an
additive noise channel. Early work had already solved this problem for a
Gaussian source and channel. Building on a sequence of recent results on
conditions for linearity of optimal estimation, and of optimal mappings in
source-channel coding, we derive the saddle-point solution to the jamming
problem for general sources and channels, without recourse to Gaussian
assumptions. We show that linearity conditions play a pivotal role in jamming,
in the sense that the optimal jamming strategy is to effectively force both
transmitter and receiver to default to linear mappings, i.e., the jammer
ensures, whenever possible, that the transmitter and receiver cannot benefit
from non-linear strategies. This result is shown to subsume the known result
for Gaussian source and channel. We analyze conditions and general settings
where such unbeatable strategy can indeed be achieved by the jammer. Moreover,
we provide the procedure to approximate optimal jamming in the remaining
(source-channel) cases where the jammer cannot impose linearity on the
transmitter and the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3055</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3055</id><created>2013-03-12</created><authors><author><keyname>Abbasi-Yadkori</keyname><forenames>Yasin</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>Online Learning in Markov Decision Processes with Adversarially Chosen
  Transition Probability Distributions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning Markov decision processes with finite state
and action spaces when the transition probability distributions and loss
functions are chosen adversarially and are allowed to change with time. We
introduce an algorithm whose regret with respect to any policy in a comparison
class grows as the square root of the number of rounds of the game, provided
the transition probabilities satisfy a uniform mixing condition. Our approach
is efficient as long as the comparison class is polynomial and we can compute
expectations over sample paths for each policy. Designing an efficient
algorithm with small regret for the general case remains an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3058</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3058</id><created>2013-03-12</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo</forenames></author></authors><title>Robust Auxiliary Vector Filtering with Constrained Constant Modulus
  Design for Beamforming</title><categories>cs.IT math.IT</categories><comments>2 figures</comments><journal-ref>ICASSP 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an auxiliary vector filtering (AVF) algorithm based on a
constrained constant modulus (CCM) design for robust adaptive beamforming. This
scheme provides an efficient way to deal with filters with a large number of
elements. The proposed beamformer decomposes the adaptive filter into a
constrained (reference vector filters) and an unconstrained (auxiliary vector
filters) components. The weight vector is iterated by subtracting the scaling
auxiliary vector from the reference vector. The scalar factor and the auxiliary
vector depend on each other and are jointly calculated according to the CCM
criterion. The proposed robust AVF algorithm provides an iterative exchange of
information between the scalar factor and the auxiliary vector and thus leads
to a fast convergence and an improved steady-state performance over the
existing techniques. Simulations are performed to show the performance and the
robustness of the proposed scheme and algorithm in several scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3067</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3067</id><created>2013-03-12</created><authors><author><keyname>Lim</keyname><forenames>Chuan Kai Kenneth.</forenames></author><author><keyname>Prodromakis</keyname><forenames>T.</forenames></author></authors><title>Computing Motion with 3D Memristive Grid</title><categories>cs.CV q-bio.NC</categories><comments>Supplementary Materials not uploaded online yet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the relative motion of objects is an important navigation task that
we routinely perform by relying on inherently unreliable biological cells in
the retina. The non-linear and adaptive response of memristive devices make
them excellent building blocks for realizing complex synaptic-like
architectures that are common in the human retina. Here, we introduce a novel
memristive thresholding scheme that facilitates the detection of moving edges.
In addition, a double-layered 3-D memristive network is employed for modeling
the motion computations that take place in both the Outer Plexiform Layer (OPL)
and Inner Plexiform Layer (IPL) that enables the detection of on-center and
off-center transient responses. Applying the transient detection results, it is
shown that it is possible to generate an estimation of the speed and direction
a moving object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3071</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3071</id><created>2013-03-12</created><authors><author><keyname>Kumar</keyname><forenames>S.</forenames></author><author><keyname>Sehgal</keyname><forenames>R.</forenames></author><author><keyname>Singh</keyname><forenames>P.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author></authors><title>Nepenthes Honeypots based Botnet Detection</title><categories>cs.CR</categories><comments>http://ojs.academypublisher.com/index.php/jait/article/view/jait0304215221</comments><journal-ref>JAIT 3(4), 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The numbers of the botnet attacks are increasing day by day and the detection
of botnet spreading in the network has become very challenging. Bots are having
specific characteristics in comparison of normal malware as they are controlled
by the remote master server and usually dont show their behavior like normal
malware until they dont receive any command from their master server. Most of
time bot malware are inactive, hence it is very difficult to detect. Further
the detection or tracking of the network of theses bots requires an
infrastructure that should be able to collect the data from a diverse range of
data sources and correlate the data to bring the bigger picture in view. In
this paper, we are sharing our experience of botnet detection in the private
network as well as in public zone by deploying the nepenthes honeypots. The
automated framework for malware collection using nepenthes and analysis using
anti-virus scan are discussed. The experimental results of botnet detection by
enabling nepenthes honeypots in network are shown. Also we saw that existing
known bots in our network can be detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3072</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3072</id><created>2013-03-12</created><authors><author><keyname>Kong</keyname><forenames>Zhaodan</forenames></author><author><keyname>&#xd6;zcimder</keyname><forenames>Kayhan</forenames></author><author><keyname>Fuller</keyname><forenames>Nathan</forenames></author><author><keyname>Greco</keyname><forenames>Alison</forenames></author><author><keyname>Theriault</keyname><forenames>Diane</forenames></author><author><keyname>Wu</keyname><forenames>Zheng</forenames></author><author><keyname>Kunz</keyname><forenames>Thomas</forenames></author><author><keyname>Betke</keyname><forenames>Margrit</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>Optical Flow Sensing and the Inverse Perception Problem for Flying Bats</title><categories>cs.SY</categories><comments>20 Pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The movements of birds, bats, and other flying species are governed by
complex sensorimotor systems that allow the animals to react to stationary
environmental features as well as to wind disturbances, other animals in nearby
airspace, and a wide variety of unexpected challenges. The paper and talk will
describe research that analyzes the three-dimensional trajectories of bats
flying in a habitat in Texas. The trajectories are computed with stereoscopic
methods using data from synchronous thermal videos that were recorded with high
temporal and spatial resolution from three viewpoints. Following our previously
reported work, we examine the possibility that bat trajectories in this habitat
are governed by optical flow sensing that interpolates periodic distance
measurements from echolocation. Using an idealized geometry of bat eyes, we
introduce the concept of time-to-transit, and recall some research that
suggests that this quantity is computed by the animals' visual cortex. Several
steering control laws based on time-to-transit are proposed for an idealized
flight model, and it is shown that these can be used to replicate the observed
flight of what we identify as typical bats. Although the vision-based motion
control laws we propose and the protocols for switching between them are quite
simple, some of the trajectories that have been synthesized are qualitatively
bat-like. Examination of the control protocols that generate these trajectories
suggests that bat motions are governed both by their reactions to a subset of
key feature points as well by their memories of where these feature points are
located.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3077</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3077</id><created>2013-03-12</created><updated>2013-03-13</updated><authors><author><keyname>Gobithaasan</keyname><forenames>R. U.</forenames></author><author><keyname>Jamaludin</keyname><forenames>M. A</forenames></author></authors><title>Using Mathematica &amp; Matlab for CAGD/CAD research and education</title><categories>cs.CY cs.CG cs.GR</categories><comments>Proceedings of the 2nd International Conference on Research and
  Education in Mathematics (ICREM 2), May 25th- 26th 2005, Serdang, Malaysia,
  Pg. 518-525</comments><msc-class>65D17</msc-class><acm-class>J.6; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In CAGD/CAD research and education, users are involved with development of
mathematical algorithms and followed by the analysis of the resultant
algorithm. This process involves geometric display which can only be carried
out with high end graphics display. There are many approaches practiced and one
of the so-called easiest approaches is by using C/C++ programming language and
OpenGL application program interface, API. There are practitioners uses C/C++
programming language to develop the algorithms and finally utilize AutoCAD for
graphics display. On the other hand, high end CAD users manage to use Auto Lisp
as their programming language in AutoCAD. Nevertheless, these traditional ways
are definitely time consuming. This paper introduces an alternative method
whereby the practitioners may maximize scientific computation programs, SCPs:
Mathematica and MATLAB in the context of CAGD/CAD for research and education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3087</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3087</id><created>2013-03-13</created><authors><author><keyname>Hangarge</keyname><forenames>Mallikarjun</forenames></author><author><keyname>Santosh</keyname><forenames>K. C.</forenames></author><author><keyname>Doddamani</keyname><forenames>Srikanth</forenames></author><author><keyname>Pardeshi</keyname><forenames>Rajmohan</forenames></author></authors><title>Statistical Texture Features based Handwritten and Printed Text
  Classification in South Indian Documents</title><categories>cs.CV</categories><comments>Appeared in ICECIT-2102</comments><report-no>Volume 1,Number 32</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use statistical texture features for handwritten and
printed text classification. We primarily aim for word level classification in
south Indian scripts. Words are first extracted from the scanned document. For
each extracted word, statistical texture features are computed such as mean,
standard deviation, smoothness, moment, uniformity, entropy and local range
including local entropy. These feature vectors are then used to classify words
via k-NN classifier. We have validated the approach over several different
datasets. Scripts like Kannada, Telugu, Malayalam and Hindi i.e., Devanagari
are primarily employed where an average classification rate of 99.26% is
achieved. In addition, to provide an extensibility of the approach, we address
Roman script by using publicly available dataset and interesting results are
reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3100</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3100</id><created>2013-03-13</created><authors><author><keyname>Kang</keyname><forenames>Myung Gil</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author></authors><title>Ergodic Interference Alignment with Delayed Feedback</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new ergodic interference alignment techniques for $K$-user
interference channels with delayed feedback. Two delayed feedback scenarios are
considered -- delayed channel information at transmitter (CIT) and delayed
output feedback. It is proved that the proposed techniques achieve total
$2K/(K+2)$ DoF which is higher than that by the retrospective interference
alignment for the delayed feedback scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3110</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3110</id><created>2013-03-13</created><authors><author><keyname>Arnau</keyname><forenames>Jesus</forenames></author><author><keyname>Rico-Alvari&#xf1;o</keyname><forenames>Alberto</forenames></author><author><keyname>Mosquera</keyname><forenames>Carlos</forenames></author></authors><title>Adaptive Transmission Techniques for Mobile Satellite Links</title><categories>cs.OH</categories><comments>Presented at the 30th AIAA International Communications Satellite
  Systems Conference (ICSSC), Ottawa, Canada, 2012. Best Professional Paper
  Award</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adapting the transmission rate in an LMS channel is a challenging task
because of the relatively fast time variations, of the long delays involved,
and of the difficulty in mapping the parameters of a time-varying channel into
communication performance. In this paper, we propose two strategies for dealing
with these impairments, namely, multi-layer coding (MLC) in the forward link,
and open-loop adaptation in the return link. Both strategies rely on
physical-layer abstraction tools for predicting the link performance. We will
show that, in both cases, it is possible to increase the average spectral
efficiency while at the same time keeping the outage probability under a given
threshold. To do so, the forward link strategy will rely on introducing some
latency in the data stream by using retransmissions. The return link, on the
other hand, will rely on a statistical characterization of a physical-layer
abstraction measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3134</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3134</id><created>2013-03-13</created><authors><author><keyname>Boujut</keyname><forenames>Hugo</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Buso</keyname><forenames>Vincent</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Bourmaud</keyname><forenames>Guillaume</forenames><affiliation>IMS</affiliation></author><author><keyname>Benois-Pineau</keyname><forenames>Jenny</forenames><affiliation>LaBRI</affiliation></author><author><keyname>M&#xe9;gret</keyname><forenames>R&#xe9;mi</forenames><affiliation>IMS</affiliation></author><author><keyname>Domenger</keyname><forenames>Jean-Philippe</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Ga&#xeb;stel</keyname><forenames>Yann</forenames><affiliation>ISPED</affiliation></author><author><keyname>Dartigues</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Egocentric vision IT technologies for Alzheimer disease assessment and
  studies</title><categories>cs.HC cs.CV</categories><comments>RITS - Recherche en Imagerie et Technologies pour la Sant\'e, France
  (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Egocentric vision technology consists in capturing the actions of persons
from their own visual point of view using wearable camera sensors. We apply
this new paradigm to instrumental activities monitoring with the objective of
providing new tools for the clinical evaluation of the impact of the disease on
persons with dementia. In this paper, we introduce the current state of the
development of this technology and focus on two technology modules: automatic
location estimation and visual saliency estimation for content interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3145</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3145</id><created>2013-03-13</created><updated>2013-03-14</updated><authors><author><keyname>Wang</keyname><forenames>Pu</forenames></author><author><keyname>Emmerich</keyname><forenames>Michael</forenames></author><author><keyname>Li</keyname><forenames>Rui</forenames></author><author><keyname>Tang</keyname><forenames>Ke</forenames></author><author><keyname>Baeck</keyname><forenames>Thomas</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>Convex Hull-Based Multi-objective Genetic Programming for Maximizing ROC
  Performance</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ROC is usually used to analyze the performance of classifiers in data mining.
ROC convex hull (ROCCH) is the least convex major-ant (LCM) of the empirical
ROC curve, and covers potential optima for the given set of classifiers.
Generally, ROC performance maximization could be considered to maximize the
ROCCH, which also means to maximize the true positive rate (tpr) and minimize
the false positive rate (fpr) for each classifier in the ROC space. However,
tpr and fpr are conflicting with each other in the ROCCH optimization process.
Though ROCCH maximization problem seems like a multi-objective optimization
problem (MOP), the special characters make it different from traditional MOP.
In this work, we will discuss the difference between them and propose convex
hull-based multi-objective genetic programming (CH-MOGP) to solve ROCCH
maximization problems. Convex hull-based sort is an indicator based selection
scheme that aims to maximize the area under convex hull, which serves as a
unary indicator for the performance of a set of points. A selection procedure
is described that can be efficiently implemented and follows similar design
principles than classical hyper-volume based optimization algorithms. It is
hypothesized that by using a tailored indicator-based selection scheme CH-MOGP
gets more efficient for ROC convex hull approximation than algorithms which
compute all Pareto optimal points. To test our hypothesis we compare the new
CH-MOGP to MOGP with classical selection schemes, including NSGA-II, MOEA/D)
and SMS-EMOA. Meanwhile, CH-MOGP is also compared with traditional machine
learning algorithms such as C4.5, Naive Bayes and Prie. Experimental results
based on 22 well-known UCI data sets show that CH-MOGP outperforms
significantly traditional EMOAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3152</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3152</id><created>2013-03-13</created><authors><author><keyname>Machado</keyname><forenames>Bruno Brandoli</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Wesley Nunes</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Material quality assessment of silk nanofibers based on swarm
  intelligence</title><categories>cs.CV</categories><comments>11 pages 6 figures</comments><journal-ref>J. Phys.: Conf. Ser. 410 012163 2013</journal-ref><doi>10.1088/1742-6596/410/1/012163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel approach for texture analysis based on
artificial crawler model. Our method assumes that each agent can interact with
the environment and each other. The evolution process converges to an
equilibrium state according to the set of rules. For each textured image, the
feature vector is composed by signatures of the live agents curve at each time.
Experimental results revealed that combining the minimum and maximum signatures
into one increase the classification rate. In addition, we pioneer the use of
autonomous agents for characterizing silk fibroin scaffolds. The results
strongly suggest that our approach can be successfully employed for texture
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3154</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3154</id><created>2013-03-13</created><updated>2014-04-22</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Hou</keyname><forenames>Wei</forenames></author><author><keyname>Dong</keyname><forenames>Hongbin</forenames></author><author><keyname>He</keyname><forenames>Feidun</forenames></author></authors><title>Mixed Strategy May Outperform Pure Strategy: An Initial Study</title><categories>cs.NE cs.GT</categories><doi>10.1109/CEC.2013.6557618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In pure strategy meta-heuristics, only one search strategy is applied for all
time. In mixed strategy meta-heuristics, each time one search strategy is
chosen from a strategy pool with a probability and then is applied. An example
is classical genetic algorithms, where either a mutation or crossover operator
is chosen with a probability each time. The aim of this paper is to compare the
performance between mixed strategy and pure strategy meta-heuristic algorithms.
First an experimental study is implemented and results demonstrate that mixed
strategy evolutionary algorithms may outperform pure strategy evolutionary
algorithms on the 0-1 knapsack problem in up to 77.8% instances. Then
Complementary Strategy Theorem is rigorously proven for applying mixed strategy
at the population level. The theorem asserts that given two meta-heuristic
algorithms where one uses pure strategy 1 and another uses pure strategy 2, the
condition of pure strategy 2 being complementary to pure strategy 1 is
sufficient and necessary if there exists a mixed strategy meta-heuristics
derived from these two pure strategies and its expected number of generations
to find an optimal solution is no more than that of using pure strategy 1 for
any initial population, and less than that of using pure strategy 1 for some
initial population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3163</identifier>
 <datestamp>2013-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3163</id><created>2013-03-13</created><updated>2013-06-12</updated><authors><author><keyname>Kawaguchi</keyname><forenames>Kenji</forenames></author><author><keyname>Araya</keyname><forenames>Mauricio</forenames></author></authors><title>A Greedy Approximation of Bayesian Reinforcement Learning with Probably
  Optimistic Transition Model</title><categories>cs.AI cs.LG stat.ML</categories><comments>the 13th International Workshop on Adaptive and Learning Agents at
  AAMAS'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Reinforcement Learning (RL) is capable of not only incorporating
domain knowledge, but also solving the exploration-exploitation dilemma in a
natural way. As Bayesian RL is intractable except for special cases, previous
work has proposed several approximation methods. However, these methods are
usually too sensitive to parameter values, and finding an acceptable parameter
setting is practically impossible in many applications. In this paper, we
propose a new algorithm that greedily approximates Bayesian RL to achieve
robustness in parameter space. We show that for a desired learning behavior,
our proposed algorithm has a polynomial sample complexity that is lower than
those of existing algorithms. We also demonstrate that the proposed algorithm
naturally outperforms other existing algorithms when the prior distributions
are not significantly misleading. On the other hand, the proposed algorithm
cannot handle greatly misspecified priors as well as the other algorithms can.
This is a natural consequence of the fact that the proposed algorithm is
greedier than the other algorithms. Accordingly, we discuss a way to select an
appropriate algorithm for different tasks based on the algorithms' greediness.
We also introduce a new way of simplifying Bayesian planning, based on which
future work would be able to derive new algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3164</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3164</id><created>2013-03-13</created><authors><author><keyname>Sawant</keyname><forenames>Uma</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Soumen</forenames></author></authors><title>Features and Aggregators for Web-scale Entity Search</title><categories>cs.IR</categories><comments>10 pages, 12 figures including tables</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on two research issues in entity search: scoring a document or
snippet that potentially supports a candidate entity, and aggregating scores
from different snippets into an entity score. Proximity scoring has been
studied in IR outside the scope of entity search. However, aggregation has been
hardwired except in a few cases where probabilistic language models are used.
We instead explore simple, robust, discriminative ranking algorithms, with
informative snippet features and broad families of aggregation functions. Our
first contribution is a study of proximity-cognizant snippet features. In
contrast with prior work which uses hardwired &quot;proximity kernels&quot; that
implement a fixed decay with distance, we present a &quot;universal&quot; feature
encoding which jointly expresses the perplexity (informativeness) of a query
term match and the proximity of the match to the entity mention. Our second
contribution is a study of aggregation functions. Rather than train the ranking
algorithm on snippets and then aggregate scores, we directly train on entities
such that the ranking algorithm takes into account the aggregation function
being used. Our third contribution is an extensive Web-scale evaluation of the
above algorithms on two data sets having quite different properties and
behavior. The first one is the W3C dataset used in TREC-scale enterprise
search, with pre-annotated entity mentions. The second is a Web-scale
open-domain entity search dataset consisting of 500 million Web pages, which
contain about 8 billion token spans annotated automatically with two million
entities from 200,000 entity types in Wikipedia. On the TREC dataset, the
performance of our system is comparable to the currently prevalent systems. On
the much larger and noisier Web dataset, our system delivers significantly
better performance than all other systems, with 8% MAP improvement over the
closest competitor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3165</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3165</id><created>2013-03-13</created><updated>2013-04-02</updated><authors><author><keyname>Esmaeilzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author></authors><title>Joint Optimization of Throughput and Packet Drop Rate for Delay
  Sensitive Applications in TDD Satellite Network Coded Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>14 pages, 7 figures, 2 tables. Submitted to IEEE Transactions on
  Communications</comments><journal-ref>IEEE Trans. Commun., vol. 62, no. 2, pp. 676-690, Feb. 2014</journal-ref><doi>10.1109/TCOMM.2014.011014.130248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the issue of throughput and packet drop rate (PDR)
optimization as two performance metrics for delay sensitive applications in
network coded time division duplex (TDD) satellite systems with large round
trip times (RTT). We adopt random linear network coding (RLNC) under two
different scenarios, feedback-less and with feedback, and our goal is to
jointly optimize the mean throughputs and PDRs of users in the system. For this
purpose, we propose a systematic framework and start with formulating and
optimizing these performance metrics for the single-user case. This framework
enables us to analytically compare the performance metrics under different
system parameters and settings. By comparing RLNC schemes under feedback-less
and feedback scenarios for different RTTs, we show that the feedback-less
schemes outperform the schemes with feedback in TDD systems with large RTTs.
Then, we extend the study of feedback-less RLNC schemes to the multi-user
broadcast case. Here, we consider a number of different broadcast scenarios and
optimize the system parameters such that the best overall performance is
achieved. Furthermore, the complicated interplay of the mean throughputs and
PDRs of different users with different packet erasure conditions in each of the
considered broadcast scenarios is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3166</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3166</id><created>2013-03-13</created><authors><author><keyname>Lauria</keyname><forenames>Massimo</forenames></author><author><keyname>Pudl&#xe1;k</keyname><forenames>Pavel</forenames></author><author><keyname>R&#xf6;dl</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Thapen</keyname><forenames>Neil</forenames></author></authors><title>The complexity of proving that a graph is Ramsey</title><categories>cs.CC</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We say that a graph with $n$ vertices is $c$-Ramsey if it does not contain
either a clique or an independent set of size $c \log n$. We define a CNF
formula which expresses this property for a graph $G$. We show a
superpolynomial lower bound on the length of resolution proofs that $G$ is
$c$-Ramsey, for every graph $G$. Our proof makes use of the fact that every
Ramsey graph must contain a large subgraph with some of the statistical
properties of the random graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3170</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3170</id><created>2013-03-13</created><authors><author><keyname>Hines</keyname><forenames>Peter</forenames></author></authors><title>Types and forgetfulness in categorical linguistics and quantum mechanics</title><categories>cs.CL math.CT quant-ph</categories><comments>37 pages, 4 figures</comments><journal-ref>in C. Heunen, M. Sadrzadeh, E. Grefenstette (ed.s), Quantum
  Physics and Linguistics: a compositional diagrammatic discourse, Oxford
  University Press (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of types in categorical models of meaning is investigated. A general
scheme for how typed models of meaning may be used to compare sentences,
regardless of their grammatical structure is described, and a toy example is
used as an illustration. Taking as a starting point the question of whether the
evaluation of such a type system 'loses information', we consider the
parametrized typing associated with connectives from this viewpoint.
  The answer to this question implies that, within full categorical models of
meaning, the objects associated with types must exhibit a simple but subtle
categorical property known as self-similarity. We investigate the category
theory behind this, with explicit reference to typed systems, and their
monoidal closed structure. We then demonstrate close connections between such
self-similar structures and dagger Frobenius algebras. In particular, we
demonstrate that the categorical structures implied by the polymorphically
typed connectives give rise to a (lax unitless) form of the special forms of
Frobenius algebras known as classical structures, used heavily in abstract
categorical approaches to quantum mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3176</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3176</id><created>2013-03-13</created><authors><author><keyname>Blakey</keyname><forenames>Ed</forenames></author></authors><title>Cellular Automata get their Wires Crossed</title><categories>nlin.CG cs.NI cs.SY</categories><comments>16 pages. 8 figures. Presented at NCMA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In three spatial dimensions, communication channels are free to pass over or
under each other so as to cross without intersecting; in two dimensions,
assuming channels of strictly positive thickness, this is not the case. It is
natural, then, to ask whether one can, in a suitable, two-dimensional model,
cross two channels in such a way that each successfully conveys its data, in
particular without the channels interfering at the intersection. We formalize
this question by modelling channels as cellular automata, and answer it
affirmatively by exhibiting systems whereby channels are crossed without
compromising capacity. We consider the efficiency (in various senses) of these
systems, and mention potential applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3177</identifier>
 <datestamp>2013-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3177</id><created>2013-02-15</created><updated>2013-06-26</updated><authors><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author><author><keyname>Richardson</keyname><forenames>Francois-Dominique</forenames></author><author><keyname>Gagnon</keyname><forenames>Francois</forenames></author></authors><title>Design and Analysis of a Multi-Carrier Differential Chaos Shift Keying
  Communication System</title><categories>cs.OH</categories><comments>This paper has been accepted in IEEE trans. on Communications (will
  appear soon on IEEE explore). arXiv admin note: substantial text overlap with
  arXiv:1303.2552</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A new Multi-Carrier Differential Chaos Shift Keying (MC-DCSK) modulation is
presented in this paper. The system endeavors to provide a good trade-off
between robustness, energy efficiency and high data rate, while still being
simple compared to conventional multi-carrier spread spectrum systems. This
system can be seen as a parallel extension of the DCSK modulation where one
chaotic reference sequence is transmitted over a predefined subcarrier
frequency. Multiple modulated data streams are transmitted over the remaining
subcarriers. This transmitter structure increases the spectral efficiency of
the conventional DCSK system and uses less energy. The receiver design makes
this system easy to implement where no radio frequency (RF) delay circuit is
needed to demodulate received data. Various system design parameters are
discussed throughout the paper, including the number of subcarriers, the
spreading factor, and the transmitted energy. Once the design is explained, the
bit error rate performance of the MC-DCSK system is computed and compared to
the conventional DCSK system under an additive white Gaussian noise (AWGN) and
Rayleigh channels. Simulation results confirm the advantages of this new hybrid
design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3181</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3181</id><created>2013-03-13</created><authors><author><keyname>H&#xe4;gg</keyname><forenames>Per</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>On Optimal Input Design for Feed-forward Control</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers optimal input design when the intended use of the
identified model is to construct a feed-forward controller based on measurable
disturbances. The objective is to find a minimum power excitation signal to be
used in system identification experiment, such that the corresponding
model-based feed-forward controller guarantees, with a given probability, that
the variance of the output signal is within given specifications. To start
with, some low order model problems are analytically solved and fundamental
properties of the optimal input signal solution are presented. The optimal
input signal contains feed-forward control and depends of the noise model and
transfer function of the system in a specific way. Next, we show how to apply
the partial correlation approach to closed loop optimal experiment design to
the general feed-forward problem. A framework for optimal input signal design
for feed-forward control is presented and numerically evaluated on a
temperature control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3182</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3182</id><created>2013-03-13</created><authors><author><keyname>Bouwmeester</keyname><forenames>Henricus</forenames></author></authors><title>Tiled Algorithms for Matrix Computations on Multicore Architectures</title><categories>cs.NA cs.MS math.NA</categories><comments>PhD Thesis, 2012 http://math.ucdenver.edu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current computer architecture has moved towards the multi/many-core
structure. However, the algorithms in the current sequential dense numerical
linear algebra libraries (e.g. LAPACK) do not parallelize well on
multi/many-core architectures. A new family of algorithms, the tile algorithms,
has recently been introduced to circumvent this problem. Previous research has
shown that it is possible to write efficient and scalable tile algorithms for
performing a Cholesky factorization, a (pseudo) LU factorization, and a QR
factorization. The goal of this thesis is to study tiled algorithms in a
multi/many-core setting and to provide new algorithms which exploit the current
architecture to improve performance relative to current state-of-the-art
libraries while maintaining the stability and robustness of these libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3183</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3183</id><created>2013-03-12</created><updated>2015-02-25</updated><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Strelkowa</keyname><forenames>Natalja</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author></authors><title>Toggling a Genetic Switch Using Reinforcement Learning</title><categories>cs.SY cs.CE cs.LG q-bio.MN</categories><comments>12 pages, presented at the 9th French Meeting on Planning, Decision
  Making and Learning, Li\`ege (Belgium), May 12-13, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of optimal exogenous control of gene
regulatory networks. Our approach consists in adapting an established
reinforcement learning algorithm called the fitted Q iteration. This algorithm
infers the control law directly from the measurements of the system's response
to external control inputs without the use of a mathematical model of the
system. The measurement data set can either be collected from wet-lab
experiments or artificially created by computer simulations of dynamical models
of the system. The algorithm is applicable to a wide range of biological
systems due to its ability to deal with nonlinear and stochastic system
dynamics. To illustrate the application of the algorithm to a gene regulatory
network, the regulation of the toggle switch system is considered. The control
objective of this problem is to drive the concentrations of two specific
proteins to a target region in the state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3194</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3194</id><created>2013-03-13</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>Properties of the Polarization Transformations for the Likelihood Ratios
  of Symmetric B-DMCs</title><categories>cs.IT math.IT</categories><comments>Submitted to CWIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate, starting with a symmetric B-DMC, the evolution
of various probabilities of the likelihood ratios of the synthetic channels
created by the recursive application of the basic polarization transformations.
The analysis provides a new perspective into the theory of channel polarization
initiated by Ar{\i}kan and helps us to address a problem related to
approximating the computations of the likelihood ratios of the synthetic
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3207</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3207</id><created>2013-03-13</created><updated>2015-03-04</updated><authors><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author><author><keyname>Bhan</keyname><forenames>Nirav</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Satpathi</keyname><forenames>Siddhartha</forenames></author></authors><title>Group-Sparse Model Selection: Hardness and Relaxations</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>34 pages. Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-based sparsity models are proven instrumental in linear regression
problems for recovering signals from much fewer measurements than standard
compressive sensing. The main promise of these models is the recovery of
&quot;interpretable&quot; signals through the identification of their constituent groups.
In this paper, we establish a combinatorial framework for group-model selection
problems and highlight the underlying tractability issues. In particular, we
show that the group-model selection problem is equivalent to the well-known
NP-hard weighted maximum coverage problem (WMC). Leveraging a graph-based
understanding of group models, we describe group structures which enable
correct model selection in polynomial time via dynamic programming.
Furthermore, group structures that lead to totally unimodular constraints have
tractable discrete as well as convex relaxations. We also present a
generalization of the group-model that allows for within group sparsity, which
can be used to model hierarchical sparsity. Finally, we study the Pareto
frontier of group-sparse approximations for two tractable models, among which
the tree sparsity model, and illustrate selection and computation trade-offs
between our framework and the existing convex relaxations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3229</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3229</id><created>2013-03-13</created><authors><author><keyname>Dragusin</keyname><forenames>Radu</forenames><affiliation>DTU Compute, Technical University of Denmark, Denmark</affiliation><affiliation>Department of Computer Science, University of Copenhagen, Denmark</affiliation></author><author><keyname>Petcu</keyname><forenames>Paula</forenames><affiliation>DTU Compute, Technical University of Denmark, Denmark</affiliation><affiliation>Findwise, Copenhagen, Denmark</affiliation></author><author><keyname>Lioma</keyname><forenames>Christina</forenames><affiliation>DTU Compute, Technical University of Denmark, Denmark</affiliation><affiliation>Department of Computer Science, University of Copenhagen, Denmark</affiliation></author><author><keyname>Larsen</keyname><forenames>Birger</forenames><affiliation>Information Systems and Interaction Design, Royal School of Library and Information Science, Copenhagen, Denmark</affiliation></author><author><keyname>J&#xf8;rgensen</keyname><forenames>Henrik L.</forenames><affiliation>Department of Clinical Biochemistry, Bispebjerg Hospital, Copenhagen, Denmark</affiliation></author><author><keyname>Cox</keyname><forenames>Ingemar J.</forenames><affiliation>DTU Compute, Technical University of Denmark, Denmark</affiliation><affiliation>Department of Computer Science, University College London, London, United Kingdom</affiliation></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames><affiliation>DTU Compute, Technical University of Denmark, Denmark</affiliation></author><author><keyname>Ingwersen</keyname><forenames>Peter</forenames><affiliation>Information Systems and Interaction Design, Royal School of Library and Information Science, Copenhagen, Denmark</affiliation></author><author><keyname>Winther</keyname><forenames>Ole</forenames><affiliation>DTU Compute, Technical University of Denmark, Denmark</affiliation></author></authors><title>FindZebra: A search engine for rare diseases</title><categories>cs.IR cs.DL</categories><journal-ref>International Journal of Medical Informatics, Available online 23
  February 2013, ISSN 1386-5056</journal-ref><doi>10.1016/j.ijmedinf.2013.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: The web has become a primary information resource about illnesses
and treatments for both medical and non-medical users. Standard web search is
by far the most common interface for such information. It is therefore of
interest to find out how well web search engines work for diagnostic queries
and what factors contribute to successes and failures. Among diseases, rare (or
orphan) diseases represent an especially challenging and thus interesting class
to diagnose as each is rare, diverse in symptoms and usually has scattered
resources associated with it. Methods: We use an evaluation approach for web
search engines for rare disease diagnosis which includes 56 real life
diagnostic cases, state-of-the-art evaluation measures, and curated information
resources. In addition, we introduce FindZebra, a specialized (vertical) rare
disease search engine. FindZebra is powered by open source search technology
and uses curated freely available online medical information. Results:
FindZebra outperforms Google Search in both default setup and customised to the
resources used by FindZebra. We extend FindZebra with specialized
functionalities exploiting medical ontological information and UMLS medical
concepts to demonstrate different ways of displaying the retrieved results to
medical experts. Conclusions: Our results indicate that a specialized search
engine can improve the diagnostic quality without compromising the ease of use
of the currently widely popular web search engines. The proposed evaluation
approach can be valuable for future development and benchmarking. The FindZebra
search engine is available at http://www.findzebra.com/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3233</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3233</id><created>2013-03-13</created><authors><author><keyname>Flesca</keyname><forenames>Sergio</forenames></author><author><keyname>Furfaro</keyname><forenames>Filippo</forenames></author><author><keyname>Parisi</keyname><forenames>Francesco</forenames></author></authors><title>Consistency Checking and Querying in Probabilistic Databases under
  Integrity Constraints</title><categories>cs.DB</categories><comments>Probabilistic databases, Integrity constraints, Consistency checking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the issue of incorporating a particular yet expressive form of
integrity constraints (namely, denial constraints) into probabilistic
databases. To this aim, we move away from the common way of giving semantics to
probabilistic databases, which relies on considering a unique interpretation of
the data, and address two fundamental problems: consistency checking and query
evaluation. The former consists in verifying whether there is an interpretation
which conforms to both the marginal probabilities of the tuples and the
integrity constraints. The latter is the problem of answering queries under a
&quot;cautious&quot; paradigm, taking into account all interpretations of the data in
accordance with the constraints. In this setting, we investigate the complexity
of the above-mentioned problems, and identify several tractable cases of
practical relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3235</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3235</id><created>2013-03-13</created><updated>2015-03-22</updated><authors><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Mladen</forenames></author><author><keyname>Stanojevi&#x107;</keyname><forenames>Ivan</forenames></author><author><keyname>&#x160;enk</keyname><forenames>Vojin</forenames></author></authors><title>On the Entropy of Couplings</title><categories>cs.IT math.IT</categories><comments>18 pages (single-column). Compared to v1, the material is
  reorganized, Section IV.C is removed (the results will possible appear
  elsewhere), Propositions 3.2 and 3.7 are added. Accepted for publication in
  Information and Computation</comments><msc-class>94A17, 60E99, 68Q17</msc-class><journal-ref>Inf. Comput., vol. 242, pp. 369-382, June 2015</journal-ref><doi>10.1016/j.ic.2015.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, some general properties of Shannon information measures are
investigated over sets of probability distributions with restricted marginals.
Certain optimization problems associated with these functionals are shown to be
NP-hard, and their special cases are found to be essentially
information-theoretic restatements of well-known computational problems, such
as the SUBSET SUM and the 3-PARTITION. The notion of minimum entropy coupling
is introduced and its relevance is demonstrated in information-theoretic,
computational, and statistical contexts. Finally, a family of pseudometrics (on
the space of discrete probability distributions) defined by these couplings is
studied, in particular their relation to the total variation distance, and a
new characterization of the conditional entropy is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3240</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3240</id><created>2013-03-13</created><updated>2014-11-14</updated><authors><author><keyname>Nicolaou</keyname><forenames>Mihalis A.</forenames></author><author><keyname>Zafeiriou</keyname><forenames>Stefanos</forenames></author><author><keyname>Pantic</keyname><forenames>Maja</forenames></author></authors><title>A Unified Framework for Probabilistic Component Analysis</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a unifying framework which reduces the construction of
probabilistic component analysis techniques to a mere selection of the latent
neighbourhood, thus providing an elegant and principled framework for creating
novel component analysis models as well as constructing probabilistic
equivalents of deterministic component analysis methods. Under our framework,
we unify many very popular and well-studied component analysis algorithms, such
as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA),
Locality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some of
which have no probabilistic equivalents in literature thus far. We firstly
define the Markov Random Fields (MRFs) which encapsulate the latent
connectivity of the aforementioned component analysis techniques; subsequently,
we show that the projection directions produced by all PCA, LDA, LPP and SFA
are also produced by the Maximum Likelihood (ML) solution of a single joint
probability density function, composed by selecting one of the defined MRF
priors while utilising a simple observation model. Furthermore, we propose
novel Expectation Maximization (EM) algorithms, exploiting the proposed joint
PDF, while we generalize the proposed methodologies to arbitrary connectivities
via parameterizable MRF products. Theoretical analysis and experiments on both
simulated and real world data show the usefulness of the proposed framework, by
deriving methods which well outperform state-of-the-art equivalents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3245</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3245</id><created>2013-03-13</created><authors><author><keyname>Rocha</keyname><forenames>Luis Enrique Correa</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D</forenames></author></authors><title>Flow Motifs Reveal Limitations of the Static Framework to Represent
  Human interactions</title><categories>physics.soc-ph cs.SI physics.data-an q-bio.QM</categories><doi>10.1103/PhysRevE.87.042814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks are commonly used to define underlying interaction structures where
infections, information, or other quantities may spread. Although the standard
approach has been to aggregate all links into a static structure, some studies
suggest that the time order in which the links are established may alter the
dynamics of spreading. In this paper, we study the impact of the time ordering
in the limits of flow on various empirical temporal networks. By using a random
walk dynamics, we estimate the flow on links and convert the original
undirected network (temporal and static) into a directed flow network. We then
introduce the concept of flow motifs and quantify the divergence in the
representativity of motifs when using the temporal and static frameworks. We
find that the regularity of contacts and persistence of vertices (common in
email communication and face-to-face interactions) result on little differences
in the limits of flow for both frameworks. On the other hand, in the case of
communication within a dating site (and of a sexual network), the flow between
vertices changes significantly in the temporal framework such that the static
approximation poorly represents the structure of contacts. We have also
observed that cliques with 3 and 4 vertices con- taining only low-flow links
are more represented than the same cliques with all high-flow links. The
representativity of these low-flow cliques is higher in the temporal framework.
Our results suggest that the flow between vertices connected in cliques depend
on the topological context in which they are placed and in the time sequence in
which the links are established. The structure of the clique alone does not
completely characterize the potential of flow between the vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3247</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3247</id><created>2013-03-13</created><authors><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Shankaranarayanan</keyname><forenames>N. K.</forenames></author></authors><title>Performance of a random-access wireless network with a mix of full- and
  half-duplex stations</title><categories>cs.IT cs.NI math.IT</categories><comments>9 pages, submitted to IEEE Communication Letters</comments><journal-ref>IEEE Communication Letters, vol.17, no.11, pp.2200--2203, November
  2013</journal-ref><doi>10.1109/LCOMM.2013.100713.131892</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the performance of a random-access time-slotted
wireless network with a single access point and a mix of half- and full- duplex
stations. Full-duplex transmissions involve data transmitted simultaneously in
both directions, and this influences the dynamics of the queue at the access
point. Given the probabilities of channel access by the nodes, this paper
provides generalized analytical formulations for the throughputs for each
station. Special cases related to a 802.11 DCA based system as well as a
full-fairness system are discussed, which provide insights into the changes
introduced by the new technology of full-duplex wireless.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3250</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3250</id><created>2013-03-13</created><updated>2013-03-15</updated><authors><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author></authors><title>Reconstruction of Directed Networks from Consensus Dynamics</title><categories>cs.SI math.OC physics.soc-ph</categories><comments>6 pages</comments><journal-ref>S. Shahrampour and V.M. Preciado,&quot;Reconstruction of Directed
  Networks from Consensus Dynamics,&quot; in Proc. American Control Conference, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of identifying the topology of an unknown,
weighted, directed network running a consensus dynamics. We propose a
methodology to reconstruct the network topology from the dynamic response when
the system is stimulated by a wide-sense stationary noise of unknown power
spectral density. The method is based on a node-knockout, or grounding,
procedure wherein the grounded node broadcasts zero without being eliminated
from the network. In this direction, we measure the empirical cross-power
spectral densities of the outputs between every pair of nodes for both grounded
and ungrounded consensus to reconstruct the unknown topology of the network. We
also establish that in the special cases of undirected or purely unidirectional
networks, the reconstruction does not need grounding. Finally, we extend our
results to the case of a directed network assuming a general dynamics, and
prove that the developed method can detect edges and their direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3251</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3251</id><created>2013-03-13</created><authors><author><keyname>Xiao</keyname><forenames>Li</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Wang</keyname><forenames>Wenjie</forenames></author></authors><title>Multi-Stage Robust Chinese Remainder Theorem</title><categories>cs.IT cs.CE cs.CR math.IT math.NT</categories><comments>26 pages, 2 figures</comments><doi>10.1109/TSP.2014.2339798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the traditional Chinese remainder theorem (CRT) is not
robust in the sense that a small error in a remainder may cause a large error
in the reconstruction solution. A robust CRT was recently proposed for a
special case when the greatest common divisor (gcd) of all the moduli is more
than 1 and the remaining integers factorized by the gcd of all the moduli are
co-prime. In this special case, a closed-form reconstruction from erroneous
remainders was proposed and a necessary and sufficient condition on the
remainder errors was obtained. It basically says that the reconstruction error
is upper bounded by the remainder error level $\tau$ if $\tau$ is smaller than
a quarter of the gcd of all the moduli. In this paper, we consider the robust
reconstruction problem for a general set of moduli. We first present a
necessary and sufficient condition for the remainder errors for a robust
reconstruction from erroneous remainders with a general set of muduli and also
a corresponding robust reconstruction method. This can be thought of as a
single stage robust CRT. We then propose a two-stage robust CRT by grouping the
moduli into several groups as follows. First, the single stage robust CRT is
applied to each group. Then, with these robust reconstructions from all the
groups, the single stage robust CRT is applied again across the groups. This is
then easily generalized to multi-stage robust CRT. Interestingly, with this
two-stage robust CRT, the robust reconstruction holds even when the remainder
error level $\tau$ is above the quarter of the gcd of all the moduli. In this
paper, we also propose an algorithm on how to group a set of moduli for a
better reconstruction robustness of the two-stage robust CRT in some special
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3256</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3256</id><created>2013-03-13</created><updated>2013-09-06</updated><authors><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author></authors><title>Structural Results and Explicit Solution for Two-Player LQG Systems on a
  Finite Time Horizon</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that linear dynamical systems with Gaussian noise and
quadratic cost (LQG) satisfy a separation principle. Finding the optimal
controller amounts to solving separate dual problems; one for control and one
for estimation. For the discrete-time finite-horizon case, each problem is a
simple forward or backward recursion. In this paper, we consider a
generalization of the LQG problem in which there are two controllers. Each
controller is responsible for one of two system inputs, but has access to
different subsets of the available measurements. Our paper has three main
contributions. First, we prove a fundamental structural result: sufficient
statistics for the controllers can be expressed as conditional means of the
global state. Second, we give explicit state-space formulae for the optimal
controller. These formulae are reminiscent of the classical LQG solution with
dual forward and backward recursions, but with the important difference that
they are intricately coupled. Lastly, we show how these recursions can be
solved efficiently, with computational complexity comparable to that of the
centralized problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3257</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3257</id><created>2013-03-13</created><updated>2013-11-24</updated><authors><author><keyname>Parisi</keyname><forenames>Fabio</forenames></author><author><keyname>Strino</keyname><forenames>Francesco</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Kluger</keyname><forenames>Yuval</forenames></author></authors><title>Ranking and combining multiple predictors without labeled data</title><categories>stat.ML cs.LG</categories><comments>Supplementary Information is included at the end of the manuscript.
  This is a revision of our original submission of the manuscript entitled &quot;The
  student's dilemma: ranking and improving prediction at test time without
  access to training data&quot;, which is now entitled &quot;Ranking and combining
  multiple predictors without labeled data&quot;</comments><journal-ref>Proc. Natl. Acad. Sci. U.S.A. 111 (2014) 1253-1258</journal-ref><doi>10.1073/pnas.1219097111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a broad range of classification and decision making problems, one is given
the advice or predictions of several classifiers, of unknown reliability, over
multiple questions or queries. This scenario is different from the standard
supervised setting, where each classifier accuracy can be assessed using
available labeled data, and raises two questions: given only the predictions of
several classifiers over a large set of unlabeled test data, is it possible to
a) reliably rank them; and b) construct a meta-classifier more accurate than
most classifiers in the ensemble? Here we present a novel spectral approach to
address these questions. First, assuming conditional independence between
classifiers, we show that the off-diagonal entries of their covariance matrix
correspond to a rank-one matrix. Moreover, the classifiers can be ranked using
the leading eigenvector of this covariance matrix, as its entries are
proportional to their balanced accuracies. Second, via a linear approximation
to the maximum likelihood estimator, we derive the Spectral Meta-Learner (SML),
a novel ensemble classifier whose weights are equal to this eigenvector
entries. On both simulated and real data, SML typically achieves a higher
accuracy than most classifiers in the ensemble and can provide a better
starting point than majority voting, for estimating the maximum likelihood
solution. Furthermore, SML is robust to the presence of small malicious groups
of classifiers designed to veer the ensemble prediction away from the (unknown)
ground truth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3262</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3262</id><created>2013-03-12</created><updated>2013-06-15</updated><authors><author><keyname>Gonzalez</keyname><forenames>Elias</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Balog</keyname><forenames>Robert</forenames></author><author><keyname>Enjeti</keyname><forenames>Prasad</forenames></author></authors><title>Information theoretically secure, enhanced Johnson noise based key
  distribution over the smart grid with switched filters</title><categories>cs.CR</categories><comments>polished, updated</comments><journal-ref>PLoS ONE 8 (2013) e70206</journal-ref><doi>10.1371/journal.pone.0070206</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce a protocol with a reconfigurable filter system to create
non-overlapping single loops in the smart power grid for the realization of the
Kirchhoff-Law-Johnson-(like)-Noise secure key distribution system. The protocol
is valid for one-dimensional radial networks (chain-like power line) which are
typical of the electricity distribution network between the utility and the
customer. The speed of the protocol (the number of steps needed) versus grid
size is analyzed. When properly generalized, such a system has the potential to
achieve unconditionally secure key distribution over the smart power grid of
arbitrary geometrical dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3263</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3263</id><created>2013-03-13</created><authors><author><keyname>Swaminathan</keyname><forenames>J. N.</forenames></author><author><keyname>Kumar</keyname><forenames>P.</forenames></author><author><keyname>Vinoth</keyname><forenames>M.</forenames></author></authors><title>Performance Analysis of LMS Filter for SSPA Linearization in Different
  Modulation Conditions</title><categories>cs.OH</categories><comments>Appeared in ICECIT-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SSPA has wide application in Communication system, but its high output
power varies due to its non linear gain. Pre-distortion method plays major role
in power amplifier linearization. Polynomial is one of the methods used. The
error estimation in Polynomial method is carried out by LMS Filter. Our main
work is to analysis the error estimation performance of the LMS Filter for the
Solid state power amplifiers (SSPA) in different modulation conditions. Here we
are calculating the ACP and analyzing how effectively the memoryless non
linearity has been reduced for all digital modulation techniques. All the
analysis and results are taken using Matlab software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3319</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3319</id><created>2013-03-13</created><authors><author><keyname>Tan</keyname><forenames>Anhui</forenames></author></authors><title>A new type of judgement theorems for attribute characters in information
  system</title><categories>cs.DS cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research of attribute characters in information system which contains
core, necessary, unnecessary is a basic and important issue in attribute
reduct. Many methods for the judgement of attribute characters are based on the
relationship between the objects and attributes. In this paper, a new type of
judgement theorems which are absolutely based on the relationship among
attributes is proposed for the judgement of attribute characters. The method is
through comparing the two new attribute sets $E(a)$ and $N(a)$ with respect to
the designated attribute $a$ which is proposed in this paper. We conclude that
which type of the attribute $a$ belongs to is determined by the relationship
between $E(a)$ and $N(a)$ in essence. Secondly, more concise and clear results
are given about the judgment of the attribute characters through analyzing the
properties of refinement and precise-refinement between $E(a)$ and $N(a)$ in
topology. In addition, the relationship among attributes are discussed which is
useful for constructing a reduct in the last section of this paper. In the
last, we propose a reduct algorithm based on $E(a)$, and this algorithm is an
extended application of the analysis of attribute characters above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3320</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3320</id><created>2013-03-13</created><authors><author><keyname>Espinosa</keyname><forenames>Luis A. Duffaut</forenames></author><author><keyname>Miao</keyname><forenames>Z.</forenames></author><author><keyname>Petersen</keyname><forenames>I. R.</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author><author><keyname>James</keyname><forenames>M. R.</forenames></author></authors><title>On the preservation of commutation and anticommutation relations of
  N-level quantum systems</title><categories>math.OC cs.SY quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to provide conditions under which a quantum
stochastic differential equation (QSDE) preserves the commutation and
anticommutation relations of the SU(n) algebra, and thus describes the
evolution of an open n-level quantum system. One of the challenges in the
approach lies in the handling of the so-called anomaly coefficients of SU(n).
Then, it is shown that the physical realizability conditions recently developed
by the authors for open n-level quantum systems also imply preservation of
commutation and anticommutation relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3341</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3341</id><created>2013-03-14</created><authors><author><keyname>Srinivasan</keyname><forenames>Srimathy</forenames></author></authors><title>Codes on Varieties as Codes on Curves</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery of algebraic geometric codes constructed on curves led to
generalizing this construction on higher dimensional varieties. In this paper,
we use a theorem of B. Poonen to show that the codes obtained from higher
dimensional varieties can be realized as codes on curves. One of the important
consequences of this result is that the search for good codes on varieties that
beat the existing bounds can be restricted to the case of curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3358</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3358</id><created>2013-03-14</created><updated>2015-05-08</updated><authors><author><keyname>Burdis</keyname><forenames>Joseph M.</forenames></author><author><keyname>Kogan</keyname><forenames>Irina A.</forenames></author><author><keyname>Hong</keyname><forenames>Hoon</forenames></author></authors><title>Object-Image Correspondence for Algebraic Curves under Projections</title><categories>math.AG cs.CG math.DG</categories><comments>significantly improved version (corrected and completed) of
  arXiv:1202.1303; v2: Proof of Theorem 4 corrected</comments><proxy>Sigma</proxy><journal-ref>SIGMA 9 (2013), 023, 31 pages</journal-ref><doi>10.3842/SIGMA.2013.023</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present a novel algorithm for deciding whether a given planar curve is an
image of a given spatial curve, obtained by a central or a parallel projection
with unknown parameters. The motivation comes from the problem of establishing
a correspondence between an object and an image, taken by a camera with unknown
position and parameters. A straightforward approach to this problem consists of
setting up a system of conditions on the projection parameters and then
checking whether or not this system has a solution. The computational advantage
of the algorithm presented here, in comparison to algorithms based on the
straightforward approach, lies in a significant reduction of a number of real
parameters that need to be eliminated in order to establish existence or
non-existence of a projection that maps a given spatial curve to a given planar
curve. Our algorithm is based on projection criteria that reduce the projection
problem to a certain modification of the equivalence problem of planar curves
under affine and projective transformations. To solve the latter problem we
make an algebraic adaptation of signature construction that has been used to
solve the equivalence problems for smooth curves. We introduce a notion of a
classifying set of rational differential invariants and produce explicit
formulas for such invariants for the actions of the projective and the affine
groups on the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3371</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3371</id><created>2013-03-14</created><updated>2013-06-03</updated><authors><author><keyname>Sobocinski</keyname><forenames>Pawel</forenames></author></authors><title>Nets, relations and linking diagrams</title><categories>cs.LO math.CT math.LO</categories><comments>15 pages, Proceedings of 5th Conference on Algebra and Coalgebra in
  Computer Science (CALCO), Warsaw, Poland, 3-6 September 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent work, the author and others have studied compositional algebras of
Petri nets. Here we consider mathematical aspects of the pure linking algebras
that underly them. We characterise composition of nets without places as the
composition of spans over appropriate categories of relations, and study the
underlying algebraic structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3381</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3381</id><created>2013-03-14</created><updated>2016-02-22</updated><authors><author><keyname>Hillion</keyname><forenames>Erwan</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>Discrete versions of the transport equation and the Shepp-Olkin
  conjecture</title><categories>math.PR cs.IT math.IT</categories><comments>Published at http://dx.doi.org/10.1214/14-AOP973 in the Annals of
  Probability (http://www.imstat.org/aop/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOP-AOP973</report-no><journal-ref>Annals of Probability 2016, Vol. 44, No. 1, 276-306</journal-ref><doi>10.1214/14-AOP973</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework to consider transport problems for integer-valued
random variables. We introduce weighting coefficients which allow us to
characterize transport problems in a gradient flow setting, and form the basis
of our introduction of a discrete version of the Benamou-Brenier formula.
Further, we use these coefficients to state a new form of weighted
log-concavity. These results are applied to prove the monotone case of the
Shepp-Olkin entropy concavity conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3386</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3386</id><created>2013-03-14</created><authors><author><keyname>Avigdor-elgrabli</keyname><forenames>Noa</forenames></author><author><keyname>Rabani</keyname><forenames>Yuval</forenames></author></authors><title>An Optimal Randomized Online Algorithm for Reordering Buffer Management</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an $O(\log\log k)$-competitive randomized online algorithm for
reordering buffer management, where $k$ is the buffer size. Our bound matches
the lower bound of Adamaszek et al. (STOC 2011). Our algorithm has two stages
which are executed online in parallel. The first stage computes
deterministically a feasible fractional solution to an LP relaxation for
reordering buffer management. The second stage &quot;rounds&quot; using randomness the
fractional solution. The first stage is based on the online primal-dual schema,
combined with a dual fitting argument. As multiplicative weights steps and dual
fitting steps are interleaved and in some sense conflicting, combining them is
challenging. We also note that we apply the primal-dual schema to a relaxation
with mixed packing and covering constraints. We pay the $O(\log\log k)$
competitive factor for the gap between the computed LP solution and the optimal
LP solution. The second stage gives an online algorithm that converts the LP
solution to an integral solution, while increasing the cost by an O(1) factor.
This stage generalizes recent results that gave a similar approximation factor
for rounding the LP solution, albeit using an offline rounding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3400</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3400</id><created>2013-03-14</created><updated>2015-10-09</updated><authors><author><keyname>Hoydis</keyname><forenames>Jakob</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>The Second-Order Coding Rate of the MIMO Rayleigh Block-Fading Channel</title><categories>cs.IT math.IT</categories><comments>31 pages, 3 figures, accepted for publications in IEEE Trans. Inf.
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The second-order coding rate of the multiple-input multiple-output (MIMO)
quasi-static Rayleigh fading channel is studied. We tackle this problem via an
information-spectrum approach and statistical bounds based on recent random
matrix theory techniques. We derive a central limit theorem (CLT) to analyze
the information density in the regime where the block-length n and the number
of transmit and receive antennas K and N, respectively, grow simultaneously
large. This result leads to the characterization of closed-form upper and lower
bounds on the optimal average error probability when the coding rate is within
O((nK)^-1/2) of the asymptotic capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3422</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3422</id><created>2013-03-14</created><authors><author><keyname>Osswald</keyname><forenames>Christophe</forenames><affiliation>STIC, Lab-STICC</affiliation></author></authors><title>Controling the number of focal elements</title><categories>cs.IT cs.DS math.IT</categories><comments>Belief 2012, Compi\`egne : France (2012)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-29461-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic belief assignment can have up to 2^n focal elements, and combining
them with a simple conjunctive operator will need O(2^2n) operations. This
article proposes some techniques to limit the size of the focal sets of the
bbas to be combined while preserving a large part of the information they
carry. The first section revisits some well-known definitions with an
algorithmic point of vue. The second section proposes a matrix way of building
the least committed isopignistic, and extends it to some other bodies of
evidence. The third section adapts the k-means algorithm for an unsupervized
clustering of the focal elements of a given bba.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3427</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3427</id><created>2013-03-14</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author></authors><title>Distributed Space-Time Coding of Over-the-Air Superimposed Packets in
  Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new cooperative packet transmission scheme that
allows independent sources to superimpose over-the-air their packet
transmissions. Relay nodes are used and cooperative diversity is combined with
distributed space-time block coding (STBC). With the proposed scheme the
participating relays create a ST code for the over-the-air superimposed symbols
that are received locally and without proceeding to any decoding step
beforehand. The advantage of the proposed scheme is that communication is
completed in fewer transmission slots because of the concurrent packet
transmissions, while the diversity benefit from the use of the STBC results in
higher decoding performance. The proposed scheme does not depend on the STBC
that is applied at the relays. Simulation results reveal significant throughput
benefits even in the low SNR regime.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="42000" completeListSize="102538">1122234|43001</resumptionToken>
</ListRecords>
</OAI-PMH>
