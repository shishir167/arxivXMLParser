<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:02:37Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|91001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07596</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07596</id><created>2016-01-27</created><authors><author><keyname>Chicano</keyname><forenames>Francisco</forenames></author><author><keyname>Whitley</keyname><forenames>Darrell</forenames></author><author><keyname>Tinos</keyname><forenames>Renato</forenames></author></authors><title>Efficient Hill-Climber for Multi-Objective Pseudo-Boolean Optimization</title><categories>cs.AI cs.NE</categories><comments>Paper accepted for publication in the 16th European Conference on
  Evolutionary Computation for Combinatorial Optimisation (EvoCOP 2016)</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local search algorithms and iterated local search algorithms are a basic
technique. Local search can be a stand along search methods, but it can also be
hybridized with evolutionary algorithms. Recently, it has been shown that it is
possible to identify improving moves in Hamming neighborhoods for k-bounded
pseudo-Boolean optimization problems in constant time. This means that local
search does not need to enumerate neighborhoods to find improving moves. It
also means that evolutionary algorithms do not need to use random mutation as a
operator, except perhaps as a way to escape local optima. In this paper, we
show how improving moves can be identified in constant time for multiobjective
problems that are expressed as k-bounded pseudo-Boolean functions. In
particular, multiobjective forms of NK Landscapes and Mk Landscapes are
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07597</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07597</id><created>2016-01-27</created><authors><author><keyname>Zhou</keyname><forenames>Shanyu</forenames></author><author><keyname>Seferoglu</keyname><forenames>Hulya</forenames></author><author><keyname>Koyuncu</keyname><forenames>Erdem</forenames></author></authors><title>Flow Control and Scheduling for Shared FIFO Queues over Wireless
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the performance of First-In, First-Out (FIFO) queues over
wireless networks. We characterize the stability region of a general scenario
where an arbitrary number of FIFO queues, which are served by a wireless
medium, are shared by an arbitrary number of flows. In general, the stability
region of this system is non-convex. Thus, we develop a convex inner-bound on
the stability region, which is provably tight in certain cases. The convexity
of the inner bound allows us to develop a resource allocation scheme; dFC.
Based on the structure of dFC, we develop a stochastic flow control and
scheduling algorithm; qFC. We show that qFC achieves optimal operating point in
the convex inner bound. Simulation results show that our algorithms
significantly improve the throughput of wireless networks with FIFO queues, as
compared to the well-known queue-based flow control and max-weight scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07604</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07604</id><created>2016-01-27</created><updated>2016-02-02</updated><authors><author><keyname>Pitones</keyname><forenames>Yuriko</forenames></author><author><keyname>Martinez-Bernal</keyname><forenames>Jose</forenames></author><author><keyname>Villarreal</keyname><forenames>Rafael H.</forenames></author></authors><title>Footprint functions of complete intersections</title><categories>math.AC cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1512.06868</comments><msc-class>13P25, 94B60, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the footprint function, with respect to a monomial order, of
complete intersection graded ideals in a polynomial ring with coefficients in a
field. For graded ideals of dimension one, whose initial ideal is a complete
intersection, we give a formula for the footprint function and a sharp lower
bound for the corresponding minimum distance function. This allows us to
recover a formula for the minimum distance of an affine cartesian code and the
fact that in this case the minimum distance and the footprint functions
coincide. Then we present an extension of a result of Alon and F\&quot;uredi, about
coverings of the cube $\{0,1\}^n$ by affine hyperplanes, in terms of the
regularity of a vanishing ideal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07613</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07613</id><created>2016-01-27</created><authors><author><keyname>Giuliani</keyname><forenames>Andrew</forenames></author><author><keyname>Krivodonova</keyname><forenames>Lilia</forenames></author></authors><title>Edge coloring in unstructured CFD codes</title><categories>cs.DC</categories><comments>19 pages, 11 figures, 8 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a way of preventing race conditions in the evaluation of the
surface integral contribution in discontinuous Galerkin and finite volume flow
solvers by coloring the edges (or faces) of the computational mesh. In this
work we use a partitioning algorithm that separates the edges of triangular
elements into three groups and the faces of quadrangular and tetrahedral
elements into four groups; we then extend this partitioning to adaptively
refined, nonconforming meshes. We use the ascribed coloring to reduce code
memory requirements and optimize accessing the elemental data in memory. This
process reduces memory access latencies and speeds up computations on graphics
processing units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07621</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07621</id><created>2016-01-27</created><authors><author><keyname>Racah</keyname><forenames>Evan</forenames></author><author><keyname>Ko</keyname><forenames>Seyoon</forenames></author><author><keyname>Sadowski</keyname><forenames>Peter</forenames></author><author><keyname>Bhimji</keyname><forenames>Wahid</forenames></author><author><keyname>Tull</keyname><forenames>Craig</forenames></author><author><keyname>Oh</keyname><forenames>Sang-Yun</forenames></author><author><keyname>Baldi</keyname><forenames>Pierre</forenames></author><author><keyname>Prabhat</keyname></author></authors><title>Revealing Fundamental Physics from the Daya Bay Neutrino Experiment
  using Deep Neural Networks</title><categories>stat.ML cs.LG physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experiments in particle physics produce enormous quantities of data that must
be analyzed and interpreted by teams of physicists. This analysis is often
exploratory, where scientists are unable to enumerate the possible types of
signal prior to performing the experiment. Thus, tools for summarizing,
clustering, visualizing and classifying high-dimensional data are essential. In
this work, we show that meaningful physical content can be revealed by
transforming the raw data into a learned high-level representation using deep
neural networks, with measurements taken at the Daya Bay Neutrino Experiment as
a case study. We further show how convolutional deep neural networks can
provide an effective classification filter with greater than 97% accuracy
across different classes of physics events, significantly better than other
machine learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07625</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07625</id><created>2016-01-27</created><authors><author><keyname>Lee</keyname><forenames>Yong Chan</forenames></author><author><keyname>Jang</keyname><forenames>Won Chol</forenames></author><author><keyname>Sin</keyname><forenames>Yong Hak</forenames></author></authors><title>A Method for RFO Estimation Using Phase Analysis of Pilot Symbols in
  OFDM Systems</title><categories>cs.OH</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method for CFO/RFO estimation based on proportional
coefficients extraction in OFDM system is proposed, which may be applied to any
pilot symbol pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07629</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07629</id><created>2016-01-27</created><authors><author><keyname>Friedland</keyname><forenames>Shmuel</forenames></author><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>The Computational Complexity of Duality</title><categories>math.OC cs.CC</categories><comments>13 pages</comments><msc-class>15B48, 52A41, 65F35, 90C46, 90C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for any given norm ball or proper cone, weak membership in its
dual ball or dual cone is polynomial-time reducible to weak membership in the
given ball or cone. A consequence is that the weak membership or membership
problem for a ball or cone is NP-hard if and only if the corresponding problem
for the dual ball or cone is NP-hard. In a similar vein, we show that
computation of the dual norm of a given norm is polynomial-time reducible to
computation of the given norm. This extends to convex functions satisfying a
polynomial growth condition: for such a given function, computation of its
Fenchel dual/conjugate is polynomial-time reducible to computation of the given
function. Hence the computation of a norm or a convex function of
polynomial-growth is NP-hard if and only if the computation of its dual norm or
Fenchel dual is NP-hard. We discuss implications of these results on the weak
membership problem for a symmetric convex body and its polar dual, the
polynomial approximability of Mahler volume, and the weak membership problem
for the epigraph of a convex function with polynomial growth and that of its
Fenchel dual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07630</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07630</id><created>2016-01-27</created><authors><author><keyname>Yuan</keyname><forenames>Jiangye</forenames></author><author><keyname>Cheriyadat</keyname><forenames>Anil M.</forenames></author></authors><title>Automatic Generation of Building Models Using 2D Maps and Street View
  Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach for generating simple 3D building models by
combining building footprints from 2D maps with street level images. The
approach works with crowd sourced maps such as the OpenStreetMap and street
level images acquired by a calibrated camera mounted on a moving vehicle.
Buildings are modeled as boxes extruded from building footprints with the
height information estimated from images. Building footprints are elevated in
world coordinates and projected onto images. Building heights are estimated by
scoring projected footprints based on their alignment with visible building
features in images. However, it is challenging to achieve accurate projections
due to camera pose errors inherited from external sensors resulting in
incorrect height estimation. We derive a solution to precisely locate cameras
on maps using correspondence between image features and building footprints. We
tightly couple the camera localization and height estimation steps to produce
an effective method for 3D building model generation. Experiments using Google
Street View images and publicly available map data show the promise of our
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07633</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07633</id><created>2016-01-27</created><authors><author><keyname>Andriatahiny</keyname><forenames>Harinaivo</forenames></author></authors><title>The Generalized Reed-Muller codes and the radical powers of a modular
  algebra</title><categories>cs.IT math.IT</categories><msc-class>16N40, 94B05, 12E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First, a new proof of Berman and Charpin's characterization of the
Reed-Muller codes over the binary field or over an arbitrary prime field is
presented. These codes are considered as the powers of the radical of a modular
algebra. Secondly, the same method is used for the study of the Generalized
Reed-Muller codes over a non prime field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07648</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07648</id><created>2016-01-28</created><updated>2016-02-04</updated><authors><author><keyname>Moyou</keyname><forenames>Mark</forenames></author><author><keyname>Corring</keyname><forenames>John</forenames></author><author><keyname>Peter</keyname><forenames>Adrian</forenames></author><author><keyname>Rangarajan</keyname><forenames>Anand</forenames></author></authors><title>A Grassmannian Graph Approach to Affine Invariant Feature Matching</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a novel and practical approach to address one of the
longstanding problems in computer vision: 2D and 3D affine invariant feature
matching. Our Grassmannian Graph (GrassGraph) framework employs a two stage
procedure that is capable of robustly recovering correspondences between two
unorganized, affinely related feature (point) sets. The first stage maps the
feature sets to an affine invariant Grassmannian representation, where the
features are mapped into the same subspace. It turns out that coordinate
representations extracted from the Grassmannian differ by an arbitrary
orthonormal matrix. In the second stage, by approximating the Laplace-Beltrami
operator (LBO) on these coordinates, this extra orthonormal factor is
nullified, providing true affine-invariant coordinates which we then utilize to
recover correspondences via simple nearest neighbor relations. The resulting
GrassGraph algorithm is empirically shown to work well in non-ideal scenarios
with noise, outliers, and occlusions. Our validation benchmarks use an
unprecedented 440,000+ experimental trials performed on 2D and 3D datasets,
with a variety of parameter settings and competing methods. State-of-the-art
performance in the majority of these extensive evaluations confirm the utility
of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07649</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07649</id><created>2016-01-28</created><authors><author><keyname>Liu</keyname><forenames>Fayao</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author></authors><title>Discriminative Training of Deep Fully-connected Continuous CRF with
  Task-specific Loss</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works on deep conditional random fields (CRF) have set new records on
many vision tasks involving structured predictions. Here we propose a
fully-connected deep continuous CRF model for both discrete and continuous
labelling problems. We exemplify the usefulness of the proposed model on
multi-class semantic labelling (discrete) and the robust depth estimation
(continuous) problems.
  In our framework, we model both the unary and the pairwise potential
functions as deep convolutional neural networks (CNN), which are jointly
learned in an end-to-end fashion. The proposed method possesses the main
advantage of continuously-valued CRF, which is a closed-form solution for the
Maximum a posteriori (MAP) inference.
  To better adapt to different tasks, instead of using the commonly employed
maximum likelihood CRF parameter learning protocol, we propose task-specific
loss functions for learning the CRF parameters.
  It enables direct optimization of the quality of the MAP estimates during the
course of learning.
  Specifically, we optimize the multi-class classification loss for the
semantic labelling task and the Turkey's biweight loss for the robust depth
estimation problem.
  Experimental results on the semantic labelling and robust depth estimation
tasks demonstrate that the proposed method compare favorably against both
baseline and state-of-the-art methods.
  In particular, we show that although the proposed deep CRF model is
continuously valued, with the equipment of task-specific loss, it achieves
impressive results even on discrete labelling tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07661</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07661</id><created>2016-01-28</created><authors><author><keyname>Cai</keyname><forenames>Bolun</forenames></author><author><keyname>Xu</keyname><forenames>Xiangmin</forenames></author><author><keyname>Jia</keyname><forenames>Kui</forenames></author><author><keyname>Qing</keyname><forenames>Chunmei</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>DehazeNet: An End-to-End System for Single Image Haze Removal</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single image haze removal is a challenging ill-posed problem. Existing
methods use various constraints/priors to get plausible dehazing solutions. The
key to achieve haze removal is to estimate a medium transmission map for an
input hazy image. In this paper, we propose a trainable end-to-end system
called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy
image as input, and outputs its medium transmission map that is subsequently
used to recover a haze-free image via atmospheric scattering model. DehazeNet
adopts Convolutional Neural Networks (CNN) based deep architecture, whose
layers are specially designed to embody the established assumptions/priors in
image dehazing. Specifically, layers of Maxout units are used for feature
extraction, which can generate almost all haze-relevant features. We also
propose a novel nonlinear activation function in DehazeNet, called Bilateral
Rectified Linear Unit (BReLU), which is able to improve the quality of
recovered haze-free image. We establish connections between components of the
proposed DehazeNet and those used in existing methods. Experiments on benchmark
images show that DehazeNet achieves superior performance over existing methods,
yet keeps efficient and easy to use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07670</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07670</id><created>2016-01-28</created><updated>2016-01-29</updated><authors><author><keyname>Tanimura</keyname><forenames>Yuka</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Deterministic sub-linear space LCE data structures with efficient
  construction</title><categories>cs.DS</categories><comments>updated title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a string $S$ of $n$ symbols, a longest common extension query
$\mathsf{LCE}(i,j)$ asks for the length of the longest common prefix of the
$i$th and $j$th suffixes of $S$. LCE queries have several important
applications in string processing, perhaps most notably to suffix sorting.
Recently, Bille et al. (J. Discrete Algorithms 25:42-50, 2014, Proc. CPM 2015:
65-76) described several data structures for answering LCE queries that offers
a space-time trade-off between data structure size and query time. In
particular, for a parameter $1 \leq \tau \leq n$, their best deterministic
solution is a data structure of size $O(n/\tau)$ which allows LCE queries to be
answered in $O(\tau)$ time. However, the construction time for all
deterministic versions of their data structure is quadratic in $n$. In this
paper, we propose a deterministic solution that achieves a similar space-time
trade-off of $O(\tau\min\{\log\tau,\log\frac{n}{\tau}\})$ query time using
$O(n/\tau)$ space, but significantly improve the construction time to
$O(n\tau)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07678</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07678</id><created>2016-01-28</created><authors><author><keyname>Sakai</keyname><forenames>Yuta</forenames></author><author><keyname>Iwata</keyname><forenames>Ken-ichi</forenames></author></authors><title>Extremal Relations Between Shannon Entropy and $\ell_{\alpha}$-Norm</title><categories>cs.IT math.IT</categories><comments>a short version was submitted to ISIT'2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper examines relationships between the Shannon entropy and the
$\ell_{\alpha}$-norm for $n$-ary probability vectors, $n \ge 2$. More
precisely, we investigate the tight bounds of the $\ell_{\alpha}$-norm with a
fixed Shannon entropy, and vice versa. As applications of the results, we
derive the tight bounds between the Shannon entropy and several information
measures which are determined by the $\ell_{\alpha}$-norm, e.g., R\'{e}nyi
entropy, Tsallis entropy, the $R$-norm information, and some diversity indices.
Moreover, we apply these results to uniformly focusing channels. Then, we show
the tight bounds of Gallager's $E_{0}$ functions with a fixed mutual
information under a uniform input distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07679</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07679</id><created>2016-01-28</created><authors><author><keyname>Traag</keyname><forenames>V. A.</forenames></author></authors><title>Complex Contagion of Campaign Donations</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Money is central in US politics, and most campaign contributions stem from a
tiny, wealthy elite. Like other political acts, campaign donations are known to
be socially contagious. We study how campaign donations diffuse through a
network of more than 50 000 elites and examine how connectivity among previous
donors reinforces contagion. We find the diffusion of donations to be driven by
independent reinforcement contagion: people are more likely to donate when
exposed to donors from different social groups than when they are exposed to
equally many donors from the same group. Counter-intuitively, being exposed to
one side may increase donations to the other side. Although the effect is weak,
simultaneous cross-cutting exposure makes donation somewhat less likely.
Finally, the independence of donors in the beginning of a campaign predicts the
amount of money that is raised throughout a campaign. We theorize that people
infer population-wide estimates from their local observations, with elites
assessing the viability of candidates, possibly opposing candidates in response
to local support. Our findings suggest that theories of complex contagions need
refinement and that political campaigns should target multiple communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07686</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07686</id><created>2016-01-28</created><authors><author><keyname>Yoon</keyname><forenames>Seokhyun</forenames></author></authors><title>Convergence and Density Evolution of a Low-Complexity MIMO Detector
  based on Forward-Backward Recursion over a Ring</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convergence and density evolution of a low complexity, iterative MIMO
detection based on belief propagation (BP) over a ring-type pair-wise graph are
presented in this paper. The detection algorithm to be considered is
effectively a forward-backward recursion and was originally proposed in [13],
where the link level performance and the convergence for Gaussian input were
analyzed. Presented here are the convergence proof for discrete alphabet and
the density evolution framework for binary input to give an asymptotic
performance in terms of average SINR and bit error rate (BER) without channel
coding. The BER curve obtained via density evolution shows a good match with
simulation results, verifying the effectiveness of the density evolution
analysis and the performance of the detection algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07699</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07699</id><created>2016-01-28</created><updated>2016-01-30</updated><authors><author><keyname>Carneiro</keyname><forenames>Mario</forenames></author></authors><title>Models for Metamath</title><categories>math.LO cs.LO</categories><comments>15 pages, 0 figures; submitted to CICM 2016</comments><msc-class>03C95 (Primary), 03B22, 03B70 (Secondary)</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although some work has been done on the metamathematics of Metamath, there
has not been a clear definition of a model for a Metamath formal system. We
define the collection of models of an arbitrary Metamath formal system, both
for tree-based and string-based representations. This definition is
demonstrated with examples for propositional calculus, \textsf{ZFC} set theory
with classes, and Hofstadter's MIU system, with applications for proving that
statements are not provable, showing consistency of the main Metamath database
(assuming \textsf{ZFC} has a model), developing new independence proofs, and
proving a form of G\&quot;{o}del's completeness theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07700</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07700</id><created>2016-01-28</created><authors><author><keyname>Luo</keyname><forenames>Shi-Long</forenames></author><author><keyname>Gong</keyname><forenames>Kai</forenames></author><author><keyname>Kang</keyname><forenames>Li</forenames></author></authors><title>Identifying Influential Spreaders of Epidemics on Community Networks</title><categories>physics.soc-ph cs.SI stat.ME</categories><comments>10 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient strategy for the identification of influential spreaders that
could be used to control epidemics within populations would be of considerable
importance. Generally, populations are characterized by its community
structures and by the heterogeneous distributions of weak ties among nodes
bridging over communities. A strategy for community networks capable of
identifying influential spreaders that accelerate the spread of disease is here
proposed. In this strategy, influential spreaders serve as target nodes. This
is based on the idea that, in k-shell decomposition, weak ties and strong ties
are processed separately. The strategy was used on empirical networks
constructed from online social networks, and results indicated that this
strategy is more accurate than other strategies. Its effectiveness stems from
the patterns of connectivity among neighbors, and it successfully identified
the important nodes. In addition, the performance of the strategy remained
robust even when there were errors in the structure of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07701</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07701</id><created>2016-01-28</created><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Qi</keyname><forenames>Chenhao</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Near-Optimal Signal Detector Based on Structured Compressive Sensing for
  Massive SM-MIMO</title><categories>cs.IT math.IT</categories><comments>7 pages 7 figures, submitted to IEEE Transactions on Vehicular
  Technology. Keywords: Spatial modulation (SM), massive MIMO, signal
  detection, structured compressive sensing (SCS), signal interleaving</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive spatial modulation (SM)-MIMO, which employs massive low-cost antennas
but few power-hungry transmit radio frequency (RF) chains at the transmitter,
is recently proposed to provide both high spectrum efficiency and energy
efficiency for future green communications. However, in massive SM-MIMO, the
optimal maximum likelihood (ML) detector has the prohibitively high complexity,
while state-of-the-art low-complexity detectors for conventional small-scale
SM-MIMO suffer from an obvious performance loss. In this paper, by exploiting
the structured sparsity of multiple SM signals, we propose a low-complexity
signal detector based on structured compressive sensing (SCS) to improve the
signal detection performance. Specifically, we first propose the grouped
transmission scheme at the transmitter, where multiple SM signals in several
continuous time slots are grouped to carry the common spatial constellation
symbol to introduce the desired structured sparsity. Accordingly, a structured
subspace pursuit (SSP) algorithm is proposed at the receiver to jointly detect
multiple SM signals by leveraging the structured sparsity. In addition, we also
propose the SM signal interleaving to permute SM signals in the same
transmission group, whereby the channel diversity can be exploited to further
improve the signal detection performance. Theoretical analysis quantifies the
performance gain from SM signal interleaving, and simulation results
demonstrate the near-optimal performance of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07702</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07702</id><created>2016-01-28</created><updated>2016-02-17</updated><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Correlated- and Coarse- equilibria of Single-item auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study correlated equilibria and coarse equilibria of simple first-price
single-item auctions in the simplest auction model of full information. Nash
equilibria are known to always yield full efficiency and a revenue that is at
least the second-highest value. We prove that the same is true for all
correlated equilibria, even those in which agents overbid -- i.e., bid above
their values.
  Coarse equilibria, in contrast, may yield lower efficiency and revenue. We
show that the revenue can be as low as $26\%$ of the second-highest value in a
coarse equilibrium, even if agents are assumed not to overbid, and this is
tight. We also show that when players do not overbid, the worst-case bound on
social welfare at coarse equilibrium improves from 63% of the highest value to
81%, and this bound is tight as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07709</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07709</id><created>2016-01-28</created><authors><author><keyname>Banerjee</keyname><forenames>Archi</forenames></author><author><keyname>Sanyal</keyname><forenames>Shankha</forenames></author><author><keyname>Guhathakurata</keyname><forenames>Tarit</forenames></author><author><keyname>Sengupta</keyname><forenames>Ranjan</forenames></author><author><keyname>Ghosh</keyname><forenames>Dipak</forenames></author></authors><title>Categorization of Stringed Instruments with Multifractal Detrended
  Fluctuation Analysis</title><categories>cs.SD nlin.CD physics.data-an</categories><comments>6 pages, 1 figures; Presented in Frontiers of Research in Speech and
  Music, held at IIT Kharagpur, 23-24 November 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorization is crucial for content description in archiving of music
signals. On many occasions, human brain fails to classify the instruments
properly just by listening to their sounds which is evident from the human
response data collected during our experiment. Some previous attempts to
categorize several musical instruments using various linear analysis methods
required a number of parameters to be determined. In this work, we attempted to
categorize a number of string instruments according to their mode of playing
using latest-state-of-the-art robust non-linear methods. For this, 30 second
sound signals of 26 different string instruments from all over the world were
analyzed with the help of non linear multifractal analysis (MFDFA) technique.
The spectral width obtained from the MFDFA method gives an estimate of the
complexity of the signal. From the variation of spectral width, we observed
distinct clustering among the string instruments according to their mode of
playing. Also there is an indication that similarity in the structural
configuration of the instruments is playing a major role in the clustering of
their spectral width. The observations and implications are discussed in
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07714</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07714</id><created>2016-01-28</created><authors><author><keyname>Mohtashemi</keyname><forenames>Brian</forenames></author><author><keyname>Ketseoglou</keyname><forenames>Thomas</forenames></author></authors><title>Log-Normal Matrix Completion for Large Scale Link Prediction</title><categories>cs.SI cs.LG stat.ML</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ubiquitous proliferation of online social networks has led to the
widescale emergence of relational graphs expressing unique patterns in link
formation and descriptive user node features. Matrix Factorization and
Completion have become popular methods for Link Prediction due to the low rank
nature of mutual node friendship information, and the availability of parallel
computer architectures for rapid matrix processing. Current Link Prediction
literature has demonstrated vast performance improvement through the
utilization of sparsity in addition to the low rank matrix assumption. However,
the majority of research has introduced sparsity through the limited L1 or
Frobenius norms, instead of considering the more detailed distributions which
led to the graph formation and relationship evolution. In particular, social
networks have been found to express either Pareto, or more recently discovered,
Log Normal distributions. Employing the convexity-inducing Lovasz Extension, we
demonstrate how incorporating specific degree distribution information can lead
to large scale improvements in Matrix Completion based Link prediction. We
introduce Log-Normal Matrix Completion (LNMC), and solve the complex
optimization problem by employing Alternating Direction Method of Multipliers.
Using data from three popular social networks, our experiments yield up to 5%
AUC increase over top-performing non-structured sparsity based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07721</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07721</id><created>2016-01-28</created><authors><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author><author><keyname>Zhong</keyname><forenames>Peilin</forenames></author></authors><title>Distributed Low Rank Approximation of Implicit Functions of a Matrix</title><categories>cs.NA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed low rank approximation in which the matrix to be
approximated is only implicitly represented across the different servers. For
example, each of $s$ servers may have an $n \times d$ matrix $A^t$, and we may
be interested in computing a low rank approximation to $A = f(\sum_{t=1}^s
A^t)$, where $f$ is a function which is applied entrywise to the matrix
$\sum_{t=1}^s A^t$. We show for a wide class of functions $f$ it is possible to
efficiently compute a $d \times d$ rank-$k$ projection matrix $P$ for which
$\|A - AP\|_F^2 \leq \|A - [A]_k\|_F^2 + \varepsilon \|A\|_F^2$, where $AP$
denotes the projection of $A$ onto the row span of $P$, and $[A]_k$ denotes the
best rank-$k$ approximation to $A$ given by the singular value decomposition.
The communication cost of our protocols is $d \cdot (sk/\varepsilon)^{O(1)}$,
and they succeed with high probability. Our framework allows us to efficiently
compute a low rank approximation to an entry-wise softmax, to a Gaussian kernel
expansion, and to $M$-Estimators applied entrywise (i.e., forms of robust low
rank approximation). We also show that our additive error approximation is best
possible, in the sense that any protocol achieving relative error for these
problems requires significantly more communication. Finally, we experimentally
validate our algorithms on real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07723</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07723</id><created>2016-01-28</created><authors><author><keyname>Barcucci</keyname><forenames>Elena</forenames></author><author><keyname>Bernini</keyname><forenames>Antonio</forenames></author><author><keyname>Bilotta</keyname><forenames>Stefano</forenames></author><author><keyname>Pinzani</keyname><forenames>Renzo</forenames></author></authors><title>Non-overlapping matrices</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two matrices are said non-overlapping if one of them can not be put on the
other one in a way such that the corresponding entries coincide. We provide a
set of non-overlapping binary matrices and a formula to enumerate it which
involves the $k$-generalized Fibonacci numbers. Moreover, the generating
function for the enumerating sequence is easily seen to be rational.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07724</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07724</id><created>2016-01-28</created><authors><author><keyname>Bernardy</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Jansson</keyname><forenames>Patrik</forenames></author></authors><title>Certified Context-Free Parsing: A formalisation of Valiant's Algorithm
  in Agda</title><categories>cs.LO</categories><acm-class>F.4.1; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valiant (1975) has developed an algorithm for recognition of context free
languages. As of today, it remains the algorithm with the best asymptotic
complexity for this purpose. In this paper, we present an algebraic
specification, implementation, and proof of correctness of a generalisation of
Valiant's algorithm. The generalisation can be used for recognition, parsing or
generic calculation of the transitive closure of upper triangular matrices. The
proof is certified by the Agda proof assistant. The certification is
representative of state-of-the-art methods for specification and proofs in
proof assistants based on type-theory. As such, this paper can be read as a
tutorial for the Agda system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07741</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07741</id><created>2016-01-28</created><authors><author><keyname>Bassolas</keyname><forenames>Aleix</forenames></author><author><keyname>Lenormand</keyname><forenames>Maxime</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Tugores</keyname><forenames>Ant&#xf2;nia</forenames></author><author><keyname>Ramasco</keyname><forenames>Jos&#xe9; J.</forenames></author></authors><title>Touristic site attractiveness seen through Twitter</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages and 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tourism is a significant contributor to medium and long range travels in an
increasingly globalized world. Leisure travel has an important impact on the
local and global economy and on the environment as well. The study of touristic
trips is thus raising a considerable interest. In this work, we apply a method
to assess the attractiveness of 20 of the most popular touristic sites
worldwide using geolocated tweets as a proxy for human mobility. We first rank
the touristic sites according to the spatial distribution of their visitors'
place of residence. The Taj Mahal, the Pisa Tower and the Eiffel Tower appear
consistently in the top 5 in these rankings. We then consider a coarser scale
and classify the travelers by country of residence. Touristic site's visiting
figures are then studied by country of residence showing that the Eiffel Tower,
Times Square and the London Tower welcome the majority of the visitors of each
country. Finally, we build a network linking sites whenever a user has been
detected in more than one site. This allows us to unveil relations between
touristic sites and find which ones are more tightly interconnected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07742</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07742</id><created>2016-01-28</created><authors><author><keyname>AL-msie'deen</keyname><forenames>Ra'Fat</forenames></author></authors><title>Visualizing Object-oriented Software for Understanding and Documentation</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Science and Information Security
  (IJCSIS), Vol. 13, No. 5, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding or comprehending source code is one of the core activities of
software engineering. Understanding object-oriented source code is essential
and required when a programmer maintains, migrates, reuses, documents or
enhances source code. The source code that is not comprehended cannot be
changed. The comprehension of object-oriented source code is a difficult
problem solving process. In order to document object-oriented software system
there are needs to understand its source code. To do so, it is necessary to
mine source code dependencies in addition to quantitative information in source
code such as the number of classes. This paper proposes an automatic approach,
which aims to document object-oriented software by visualizing its source code.
The design of the object-oriented source code and its main characteristics are
represented in the visualization. Package content, class information,
relationships between classes, dependencies between methods and software
metrics is displayed. The extracted views are very helpful to understand and
document the object-oriented software. The novelty of this approach is the
exploiting of code dependencies and quantitative information in source code to
document object-oriented software efficiently by means of a set of graphs. To
validate the approach, it has been applied to several case studies. The results
of this evaluation showed that most of the object-oriented software systems
have been documented correctly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07754</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07754</id><created>2016-01-28</created><authors><author><keyname>Podlesnaya</keyname><forenames>Anna</forenames></author><author><keyname>Podlesnyy</keyname><forenames>Sergey</forenames></author></authors><title>Deep Learning Based Semantic Video Indexing and Retrieval</title><categories>cs.IR</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We share the implementation details and testing results for video retrieval
system based exclusively on features extracted by convolutional neural
networks. We show that deep learned features might serve as universal signature
for semantic content of video useful in many search and retrieval tasks. We
further show that graph-based storage structure for video index allows to
efficiently retrieving the content with complicated spatial and temporal search
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07765</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07765</id><created>2016-01-28</created><authors><author><keyname>Santoni</keyname><forenames>Christian</forenames></author><author><keyname>Calabrese</keyname><forenames>Claudio</forenames></author><author><keyname>Di Renzo</keyname><forenames>Francesco</forenames></author><author><keyname>Pellacini</keyname><forenames>Fabio</forenames></author></authors><title>SculptStat: Statistical Analysis of Digital Sculpting Workflows</title><categories>cs.GR</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Targeted user studies are often employed to measure how well artists can
perform specific tasks. But these studies cannot properly describe editing
workflows as wholes, since they guide the artists both by choosing the tasks
and by using simplified interfaces. In this paper, we investigate digital
sculpting workflows used to produce detailed models. In our experiment design,
artists can choose freely what and how to model. We recover whole-workflow
trends with sophisticated statistical analyzes and validate these trends with
goodness-of-fits measures. We record brush strokes and mesh snapshots by
instrumenting a sculpting program and analyze the distribution of these
properties and their spatial and temporal characteristics. We hired expert
artists that can produce relatively sophisticated models in short time, since
their workflows are representative of best practices. We analyze 13 meshes
corresponding to roughly 25 thousand strokes in total. We found that artists
work mainly with short strokes, with average stroke length dependent on model
features rather than the artist itself. Temporally, artists do not work
coarse-to-fine but rather in bursts. Spatially, artists focus on some selected
regions by dedicating different amounts of edits and by applying different
techniques. Spatio-temporally, artists return to work on the same area multiple
times without any apparent periodicity. We release the entire dataset and all
code used for the analyzes as reference for the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07768</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07768</id><created>2016-01-28</created><authors><author><keyname>Larsson</keyname><forenames>Peter</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author><author><keyname>Al-Zubaidy</keyname><forenames>Hussein</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Effective Capacity of Retransmission Schemes - A Recurrence Relation
  Approach</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures, submitted to Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the effective capacity performance of retransmission schemes that
can involve any combination of multiple transmissions per packet, multiple
communication states, or multiple packet communication. We present a novel
unified analytical approach, based on a recurrence relation formulation, and
give an exact effective capacity expression. The expression is based on the
spectral radius of a special block companion matrix given in terms of the state
transition probabilities and renewal rate(s). We apply this approach to analyze
the performance of hybrid automatic repeat request (HARQ), network-coded ARQ
(NC-ARQ), and ARQ operating in a block Gilbert-Elliot channel. We extend the
analysis of HARQ to wireless block fading channels, specified by a probability
density function, and give closed-form effective capacity expressions of HARQ
in general, and of repetition redundancy-HARQ in Rayleigh fading in particular.
Finally, we determine the optimal effective capacity, wrt the initial rate, of
ARQ, as well as the effective capacity of multilayer-ARQ. Apart from handling a
wide class of retransmission schemes, the recurrence relation framework
developed for computing the moment-generating function, or moments, may be
applicable to other fields of time-series analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07776</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07776</id><created>2016-01-28</created><authors><author><keyname>Antoci</keyname><forenames>Angelo</forenames></author><author><keyname>Delfino</keyname><forenames>Alexia</forenames></author><author><keyname>Paglieri</keyname><forenames>Fabio</forenames></author><author><keyname>Sabatini</keyname><forenames>Fabio</forenames></author></authors><title>The ecology of social interactions in online and offline environments</title><categories>cs.SI physics.soc-ph q-fin.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise in online social networking has brought about a revolution in social
relations. However, its effects on offline interactions and its implications
for collective well-being are still not clear and are under-investigated. We
study the ecology of online and offline interaction in an evolutionary game
framework where individuals can adopt different strategies of socialization.
Our main result is that the spreading of self-protective behaviors to cope with
hostile social environments can lead the economy to non-socially optimal
stationary states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07783</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07783</id><created>2016-01-27</created><authors><author><keyname>Khan</keyname><forenames>Sohail</forenames></author><author><keyname>Shahzad</keyname><forenames>Mohsin</forenames></author><author><keyname>Habib</keyname><forenames>Usman</forenames></author><author><keyname>Gawlik</keyname><forenames>Wolfgang</forenames></author><author><keyname>Palensky</keyname><forenames>Peter</forenames></author></authors><title>Stochastic Battery Model for Aggregation of Thermostatically Controlled
  Loads</title><categories>cs.SY</categories><comments>IEEE ICIT 2016 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential of demand side as a frequency reserve proposes interesting
opportunity in handling imbalances due to intermittent renewable energy
sources. This paper proposes a novel approach for computing the parameters of a
stochastic battery model representing the aggregation of Thermostatically
Controlled Loads (TCLs). A hysteresis based non-disruptive control is used
using priority stack algorithm to track the reference regulation signal. The
parameters of admissible ramp-rate and the charge limits of the battery are
dynamically calculated using the information from TCLs that is the status
(on/off), availability and relative temperature distance till the switching
boundary. The approach builds on and improves on the existing research work by
providing a straight-forward mechanism for calculation of stochastic parameters
of equivalent battery model. The effectiveness of proposed approach is
demonstrated by a test case having a large number of residential TCLs tracking
a scaled down real frequency regulation signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07789</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07789</id><created>2016-01-26</created><authors><author><keyname>Anderson</keyname><forenames>Andrew</forenames></author><author><keyname>Gregg</keyname><forenames>David</forenames></author></authors><title>Vectorization of Multibyte Floating Point Data Formats</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a scheme for reduced-precision representation of floating point
data on a continuum between IEEE-754 floating point types. Our scheme enables
the use of lower precision formats for a reduction in storage space
requirements and data transfer volume. We describe how our scheme can be
accelerated using existing hardware vector units on a general-purpose
processor. Exploiting native vector hardware allows us to support reduced
precision floating point with low overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07790</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07790</id><created>2016-01-28</created><authors><author><keyname>Dereniowski</keyname><forenames>Dariusz</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Topology Recognition and Leader Election in Colored Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topology recognition and leader election are fundamental tasks in distributed
computing in networks. The first of them requires each node to find a labeled
isomorphic copy of the network, while the result of the second one consists in
a single node adopting the label 1 (leader), with all other nodes adopting the
label 0 and learning a path to the leader. We consider both these problems in
networks whose nodes are equipped with not necessarily distinct labels called
colors, and ports at each node of degree $d$ are arbitrarily numbered
$0,1,\dots, d-1$. Colored networks are generalizations both of labeled networks
and anonymous networks.
  In colored networks, topology recognition and leader election are not always
feasible. Hence we study two more general problems. The aim of the problem TOP
(resp. LE), for a colored network and for input $I$ given to its nodes, is to
solve topology recognition (resp. leader election) in this network, if this is
possible under input $I$, and to have all nodes answer &quot;unsolvable&quot; otherwise.
  We show that nodes of a network can solve problems TOP and LE, if they are
given, as input $I$, an upper bound $k$ on the number of nodes of a given
color, called the size of this color. On the other hand we show that, if the
nodes are given an input that does not bound the size of any color, then the
answer to TOP and LE must be &quot;unsolvable&quot;, even for the class of rings.
  Under the assumption that nodes are given an upper bound $k$ on the size of a
given color, we study the time of solving problems TOP and LE in the $LOCAL$.
We give an algorithm to solve each of these problems in arbitrary $n$-node
networks of diameter $D$ in time $O(kD+D\log(n/D))$. We also show that this
time is optimal, by exhibiting classes of networks in which every algorithm
solving problems TOP or LE must use time $\Omega(kD+D\log(n/D))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07792</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07792</id><created>2016-01-28</created><authors><author><keyname>Nay</keyname><forenames>John J.</forenames></author><author><keyname>Vorobeychik</keyname><forenames>Yevgeniy</forenames></author></authors><title>Predicting Human Cooperation</title><categories>cs.GT q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Prisoner's Dilemma has been a subject of extensive research due to its
importance in understanding the ever-present tension between individual
self-interest and social benefit. A strictly dominant strategy in a Prisoner's
Dilemma (defection), when played by both players, is mutually harmful.
Repetition of the Prisoner's Dilemma can give rise to cooperation as an
equilibrium, but defection is as well, and this ambiguity is difficult to
resolve. The numerous behavioral experiments investigating the Prisoner's
Dilemma highlight that players often cooperate, but the level of cooperation
varies significantly with the specifics of the experimental predicament. We
present the first computational model of human behavior in repeated Prisoner's
Dilemma games that unifies the diversity of experimental observations in a
systematic and quantitatively reliable manner. Our model relies on data we
integrated from many experiments, comprising 168,386 individual decisions. The
computational model is composed of two pieces: the first predicts the
first-period action using solely the structural game parameters, while the
second predicts dynamic actions using both game parameters and history of play.
Our model is extremely successful not merely at fitting the data, but in
predicting behavior at multiple scales in experimental designs not used for
calibration, using only information about the game structure. We demonstrate
the power of our approach through a simulation analysis revealing how to best
promote human cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07793</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07793</id><created>2016-01-27</created><authors><author><keyname>Kanabar</keyname><forenames>Vijay</forenames></author><author><keyname>Temkin</keyname><forenames>Anatoly</forenames></author></authors><title>Computer Science Programs, Goals, Student Learning Outcomes and their
  Assessment</title><categories>cs.CY</categories><comments>20 pages, 5 figure, Proceedings of the 11th Annual CSECS Conference,
  Boston, USA. ISSN 1313-8624</comments><acm-class>K.3.2, K.3.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the process used by MET Computer Science to
identify programs, goals, and student learning outcomes for all our programs
and graduate certificates. We illustrate how we started the process of
assessing learning outcomes for our programs and present actual assessment
results from the data collected for two programs that went through
accreditation--it describes sample direct and indirect assessment data from two
of our programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07795</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07795</id><created>2016-01-27</created><authors><author><keyname>Maghsudi</keyname><forenames>Setareh</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Distributed User Association in Energy Harvesting Small Cell Networks: A
  Probabilistic Model</title><categories>cs.IT cs.LG math.IT</categories><comments>27 Pages, Single-Column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a distributed downlink user association problem in a small cell
network, where small cells obtain the required energy for providing wireless
services to users through ambient energy harvesting. Since energy harvesting is
opportunistic in nature, the amount of harvested energy is a random variable,
without any a priori known characteristics. Moreover, since users arrive in the
network randomly and require different wireless services, the energy
consumption is a random variable as well. In this paper, we propose a
probabilistic framework to mathematically model and analyze the random behavior
of energy harvesting and energy consumption in dense small cell networks.
Furthermore, as acquiring (even statistical) channel and network knowledge is
very costly in a distributed dense network, we develop a bandit-theoretical
formulation for distributed user association when no information is available
at users
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07797</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07797</id><created>2016-01-28</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Roditty</keyname><forenames>Liam</forenames></author><author><keyname>Seiferth</keyname><forenames>Paul</forenames></author></authors><title>Reachability Oracles for Directed Transmission Graphs</title><categories>cs.CG</categories><comments>A preliminary version appeared in SoCG 2015</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P \subset \mathbb{R}^d$ be a set of $n$ points in the $d$ dimensions
such that each point $p \in P$ has an associated radius $r_p &gt; 0$. The
transmission graph $G$ for $P$ is the directed graph with vertex set $P$ such
that there is an edge from $p$ to $q$ if and only if $d(p, q) \leq r_p$, for
any $p, q \in P$.
  A reachability oracle is a data structure that decides for any two vertices
$p, q \in G$ whether $G$ has a path from $p$ to $q$. The quality of the oracle
is measured by the space requirement $S(n)$, the query time $Q(n)$, and the
preprocessing time. For transmission graphs of one-dimensional point sets, we
can construct in $O(n \log n)$ time an oracle with $Q(n) = O(1)$ and $S(n) =
O(n)$. For planar point sets, the ratio $\Psi$ between the largest and the
smallest associated radius turns out to be an important parameter. We present
three data structures whose quality depends on $\Psi$: the first works only for
$\Psi &lt; \sqrt{3}$ and achieves $Q(n) = O(1)$ with $S(n) = O(n)$ and
preprocessing time $O(n\log n)$; the second data structure gives $Q(n) =
O(\Psi^3 \sqrt{n})$ and $S(n) = O(\Psi^5 n^{3/2})$; the third data structure is
randomized with $Q(n) = O(n^{2/3}\log^{1/3} \Psi \log^{2/3} n)$ and $S(n) =
O(n^{5/3}\log^{1/3} \Psi \log^{2/3} n)$ and answers queries correctly with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07798</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07798</id><created>2016-01-28</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Roditty</keyname><forenames>Liam</forenames></author><author><keyname>Seiferth</keyname><forenames>Paul</forenames></author></authors><title>Spanners for Directed Transmission Graphs</title><categories>cs.CG</categories><comments>A preliminary version appeared in SoCG 2015</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P \subset \mathbb{R}^2$ be a planar $n$-point set such that each point
$p \in P$ has an associated radius $r_p &gt; 0$. The transmission graph $G$ for
$P$ is the directed graph with vertex set $P$ such that for any $p, q \in P$,
there is an edge from $p$ to $q$ if and only if $d(p, q) \leq r_p$.
  Let $t &gt; 1$ be a constant. A $t$-spanner for $G$ is a subgraph $H \subseteq
G$ with vertex set $P$ so that for any two vertices $p,q \in P$, we have
$d_H(p, q) \leq t d_G(p, q)$, where $d_H$ and $d_G$ denote the shortest path
distance in $H$ and $G$, respectively (with Euclidean edge lengths). We show
how to compute a $t$-spanner for $G$ with $O(n)$ edges in $O(n (\log n + \log
\Psi))$ time, where $\Psi$ is the ratio of the largest and smallest radius of a
point in $P$. Using more advanced data structures, we obtain a construction
that runs in $O(n \log^6 n)$ time, independent of $\Psi$.
  We give two applications for our spanners. First, we show how to use our
spanner to find a BFS tree from any given start vertex in $O(n \log n)$ time
(in addition to the time it takes to build the spanner). Second, we show how to
use our spanner to extend a reachability oracle to answer geometric
reachability queries. In a geometric reachability query we ask whether a vertex
$p$ in $G$ can &quot;reach&quot; a target $q$ which is an arbitrary point the plane
(rather than restricted to be another vertex $q$ of $G$ in a standard
reachability query). Our spanner allows the reachability oracle to answer
geometric reachability queries with an additive overhead of $O(\log n\log
\Psi)$ to the query time and $O(n \log \Psi)$ to the space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07800</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07800</id><created>2016-01-28</created><authors><author><keyname>Hollander</keyname><forenames>Gabriel</forenames></author><author><keyname>Dreesen</keyname><forenames>Philippe</forenames></author><author><keyname>Ishteva</keyname><forenames>Mariya</forenames></author><author><keyname>Schoukens</keyname><forenames>Johan</forenames></author></authors><title>Weighted tensor decomposition for approximate decoupling of multivariate
  polynomials</title><categories>math.OC cs.NA</categories><comments>17 pages, 8 figures. General results were presented at the Tensor
  Decompositions and Applications workshop in January 2016, Leuven, Belgium,
  organized by ESAT, Katholieke Universiteit Leuven</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multivariate polynomials arise in many different disciplines. Representing
such a polynomial as a vector of univariate polynomials can offer useful
insight, as well as more intuitive understanding. For this, techniques based on
tensor methods are known, but these have only been studied in the exact case.
In this paper, we generalize an existing method to the noisy case, by
introducing a weight factor in the tensor decomposition. Finally, we apply the
proposed weighted decoupling algorithm in the domain of system identification,
and observe smaller model errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07804</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07804</id><created>2016-01-28</created><authors><author><keyname>Ding</keyname><forenames>Xin</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wassell</keyname><forenames>Ian J.</forenames></author></authors><title>Joint Sensing Matrix and Sparsifying Dictionary Optimization for Tensor
  Compressive Sensing</title><categories>cs.LG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor Compressive Sensing (TCS) is a multidimensional framework of
Compressive Sensing (CS), and it is advantageous in terms of reducing the
amount of storage, easing hardware implementations and preserving
multidimensional structures of signals in comparison to a conventional CS
system. In a TCS system, instead of using a random sensing matrix and a
predefined dictionary, the average-case performance can be further improved by
employing an optimized multidimensional sensing matrix and a learned
multilinear sparsifying dictionary. In this paper, we propose a joint
optimization approach of the sensing matrix and dictionary for a TCS system.
For the sensing matrix design in TCS, an extended separable approach with a
closed form solution and a novel iterative non-separable method are proposed
when the multilinear dictionary is fixed. In addition, a multidimensional
dictionary learning method that takes advantages of the multidimensional
structure is derived, and the influence of sensing matrices is taken into
account in the learning process. A joint optimization is achieved via
alternately iterating the optimization of the sensing matrix and dictionary.
Numerical experiments using both synthetic data and real images demonstrate the
superiority of the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07810</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07810</id><created>2016-01-28</created><authors><author><keyname>N&#xfc;&#xdf;ing</keyname><forenames>Andreas</forenames></author><author><keyname>Wolters</keyname><forenames>Carsten H.</forenames></author><author><keyname>Brinck</keyname><forenames>Heinrich</forenames></author><author><keyname>Engwer</keyname><forenames>Christian</forenames></author></authors><title>The Unfitted Discontinuous Galerkin Method for Solving the EEG Forward
  Problem</title><categories>cs.CE math.NA q-bio.NC</categories><msc-class>35J25, 35J75, 35Q92, 65N30, 68U20, 92C50</msc-class><acm-class>G.1.8; G.1.10; I.6.0; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: The purpose of this study is to introduce and evaluate the
unfitted discontinuous Galerkin finite element method (UDG-FEM) for solving the
electroencephalography (EEG) forward problem. Methods: This new approach for
source analysis does not use a geometry conforming volume triangulation, but
instead uses a structured mesh that does not resolve the geometry. The geometry
is described using level set functions and is incorporated implicitly in its
mathematical formulation. As no triangulation is necessary, the complexity of a
simulation pipeline and the need for manual interaction for patient specific
simulations can be reduced and is comparable with that of the FEM for
hexahedral meshes. In addition it maintains conservation laws on a discrete
level. Here, we present the theory for UDG-FEM forward modeling, its validation
using quasianalytical solutions in multi-layer sphere models and an evaluation
of the new method in a comparison with a discontinuous Galerkin (DG-FEM) method
on hexahedral and on conforming tetrahedral meshes. We furthermore apply the
UDG-FEM forward approach in a realistic head model simulation study. Results:
The given results show convergence and indicate a good overall accuracy of the
UDG-FEM approach. UDG-FEM performs comparable or even better than DG-FEM on a
conforming tetrahedral mesh while providing a less complex simulation pipeline.
When compared to DG-FEM on hexahedral meshes, an overall better accuracy is
achieved. Conclusion: The UDG-FEM approach is an accurate, flexible and
promising method to solve the EEG forward problem. Significance: This study
shows the first application of the UDG-FEM approach to the EEG forward problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07811</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07811</id><created>2016-01-28</created><authors><author><keyname>Lee</keyname><forenames>Yong Chan</forenames></author><author><keyname>Jang</keyname><forenames>Won Chol</forenames></author><author><keyname>Choe</keyname><forenames>Un Kyong</forenames></author><author><keyname>Leem</keyname><forenames>Gyong Chol</forenames></author></authors><title>The Pilot Alignment Pattern Design in OFDM Systems</title><categories>cs.OH</categories><comments>11 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose optimal pilot pattern of downlink OFDM (Orthogonal
Frequency Division Multiplexing) communication system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07815</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07815</id><created>2016-01-28</created><authors><author><keyname>Yavits</keyname><forenames>L.</forenames></author><author><keyname>Morad</keyname><forenames>A.</forenames></author><author><keyname>Ginosar</keyname><forenames>R.</forenames></author><author><keyname>Weiser</keyname><forenames>U.</forenames></author></authors><title>Convex Optimization of Real Time SoC</title><categories>cs.DC cs.PF</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex optimization methods are employed to optimize a real-time (RT)
system-on-chip (SoC) under a variety of physical resource-driven constraints,
demonstrated on an industry MPEG2 encoder SoC. The power optimization is
compared to conventional performance-optimization framework, showing a factor
of two and a half saving in power. Convex optimization is shown to be very
efficient in a high-level early stage design exploration, guiding computer
architects as to the choice of area, voltage, and frequency of the individual
components of the Chip Multiprocessor (CMP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07843</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07843</id><created>2016-01-28</created><authors><author><keyname>Mishra</keyname><forenames>Nabin K.</forenames></author><author><keyname>Celebi</keyname><forenames>M. Emre</forenames></author></authors><title>An Overview of Melanoma Detection in Dermoscopy Images Using Image
  Processing and Machine Learning</title><categories>cs.CV stat.ML</categories><comments>15 pages, 3 figures</comments><acm-class>I.4; I.5.4; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The incidence of malignant melanoma continues to increase worldwide. This
cancer can strike at any age; it is one of the leading causes of loss of life
in young persons. Since this cancer is visible on the skin, it is potentially
detectable at a very early stage when it is curable. New developments have
converged to make fully automatic early melanoma detection a real possibility.
First, the advent of dermoscopy has enabled a dramatic boost in clinical
diagnostic ability to the point that melanoma can be detected in the clinic at
the very earliest stages. The global adoption of this technology has allowed
accumulation of large collections of dermoscopy images of melanomas and benign
lesions validated by histopathology. The development of advanced technologies
in the areas of image processing and machine learning have given us the ability
to allow distinction of malignant melanoma from the many benign mimics that
require no biopsy. These new technologies should allow not only earlier
detection of melanoma, but also reduction of the large number of needless and
costly biopsy procedures. Although some of the new systems reported for these
technologies have shown promise in preliminary trials, widespread
implementation must await further technical progress in accuracy and
reproducibility. In this paper, we provide an overview of computerized
detection of melanoma in dermoscopy images. First, we discuss the various
aspects of lesion segmentation. Then, we provide a brief overview of clinical
feature segmentation. Finally, we discuss the classification stage where
machine learning algorithms are applied to the attributes generated from the
segmented features to predict the existence of melanoma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07858</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07858</id><created>2016-01-28</created><authors><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn S.</forenames></author><author><keyname>Thompson</keyname><forenames>Donna M.</forenames></author><author><keyname>Chyla</keyname><forenames>Roman</forenames></author><author><keyname>Holachek</keyname><forenames>Alexandra</forenames></author><author><keyname>Elliott</keyname><forenames>Jonathan</forenames></author></authors><title>Aggregation and Linking of Observational Metadata in the ADS</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages, Proceedings of the ADASS XXV conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss current efforts behind the curation of observing proposals,
archive bibliographies, and data links in the NASA Astrophysics Data System
(ADS). The primary data in the ADS is the bibliographic content from scholarly
articles in Astronomy and Physics, which ADS aggregates from publishers, arXiv
and conference proceeding sites. This core bibliographic information is then
further enriched by ADS via the generation of citations and usage data, and
through the aggregation of external resources from astronomy data archives and
libraries. Important sources of such additional information are the metadata
describing observing proposals and high level data products, which, once
ingested in ADS, become easily discoverable and citeable by the science
community. Bibliographic studies have shown that the integration of links
between data archives and the ADS provides greater visibility to data products
and increased citations to the literature associated with them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07862</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07862</id><created>2016-01-28</created><updated>2016-02-04</updated><authors><author><keyname>Merelo</keyname><forenames>JJ</forenames></author></authors><title>Evolution of the number of GitHub users in Spain</title><categories>cs.CY</categories><comments>Report to support FLOSSMETRICS flash talks The third version updated
  to the end of January</comments><report-no>Geneura-2016-1</report-no><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Since we started measuring the community of GitHub users in Spain, it has
kept increasing in numbers in such a way that it has grown almost 50%, to the
current 12000, in less than one year. However, the reasons for this are not
clear. In this paper we will try to find out what are the different components
in this growth, or at least those that can be measured, in order to find out
which ones are due to the measurement itself and which might be due to other
reasons. In this paper we make an advance towards finding out the reasons for
this growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07865</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07865</id><created>2016-01-28</created><authors><author><keyname>Mao</keyname><forenames>Yuyi</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Grid Energy Consumption and QoS Tradeoff in Hybrid Energy Supply
  Wireless Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 7 figures, to appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid energy supply (HES) wireless networks have recently emerged as a new
paradigm to enable green networks, which are powered by both the electric grid
and harvested renewable energy. In this paper, we will investigate two critical
but conflicting design objectives of HES networks, i.e., the grid energy
consumption and quality of service (QoS). Minimizing grid energy consumption by
utilizing the harvested energy will make the network environmentally friendly,
but the achievable QoS may be degraded due to the intermittent nature of energy
harvesting. To investigate the tradeoff between these two aspects, we introduce
the total service cost as the performance metric, which is the weighted sum of
the grid energy cost and the QoS degradation cost. Base station assignment and
power control is adopted as the main strategy to minimize the total service
cost, while both cases with non-causal and causal side information are
considered. With non-causal side information, a Greedy Assignment algorithm
with low complexity and near-optimal performance is proposed. With causal side
information, the design problem is formulated as a discrete Markov decision
problem. Interesting solution structures are derived, which shall help to
develop an efficient monotone backward induction algorithm. To further reduce
complexity, a Look-Ahead policy and a Threshold-based Heuristic policy are also
proposed. Simulation results shall validate the effectiveness of the proposed
algorithms and demonstrate the unique grid energy consumption and QoS tradeoff
in HES networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07883</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07883</id><created>2016-01-28</created><authors><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author><author><keyname>Chen</keyname><forenames>Jun-Cheng</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajeev</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Swami</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Castillo</keyname><forenames>Carlos D.</forenames></author></authors><title>Towards the Design of an End-to-End Automated System for Image and
  Video-based Recognition</title><categories>cs.CV</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over many decades, researchers working in object recognition have longed for
an end-to-end automated system that will simply accept 2D or 3D image or videos
as inputs and output the labels of objects in the input data. Computer vision
methods that use representations derived based on geometric, radiometric and
neural considerations and statistical and structural matchers and artificial
neural network-based methods where a multi-layer network learns the mapping
from inputs to class labels have provided competing approaches for image
recognition problems. Over the last four years, methods based on Deep
Convolutional Neural Networks (DCNNs) have shown impressive performance
improvements on object detection/recognition challenge problems. This has been
made possible due to the availability of large annotated data, a better
understanding of the non-linear mapping between image and class labels as well
as the affordability of GPUs. In this paper, we present a brief history of
developments in computer vision and artificial neural networks over the last
forty years for the problem of image-based recognition. We then present the
design details of a deep learning system for end-to-end unconstrained face
verification/recognition. Some open issues regarding DCNNs for object
recognition problems are then discussed. We caution the readers that the views
expressed in this paper are from the authors and authors only!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07884</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07884</id><created>2016-01-28</created><authors><author><keyname>Li</keyname><forenames>Xinchao</forenames></author><author><keyname>Larson</keyname><forenames>Martha A.</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author></authors><title>Geo-distinctive Visual Element Matching for Location Estimation of
  Images</title><categories>cs.MM cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose an image representation and matching approach that substantially
improves visual-based location estimation for images. The main novelty of the
approach, called distinctive visual element matching (DVEM), is its use of
representations that are specific to the query image whose location is being
predicted. These representations are based on visual element clouds, which
robustly capture the connection between the query and visual evidence from
candidate locations. We then maximize the influence of visual elements that are
geo-distinctive because they do not occur in images taken at many other
locations. We carry out experiments and analysis for both geo-constrained and
geo-unconstrained location estimation cases using two large-scale,
publicly-available datasets: the San Francisco Landmark dataset with $1.06$
million street-view images and the MediaEval '15 Placing Task dataset with
$5.6$ million geo-tagged images from Flickr. We present examples that
illustrate the highly-transparent mechanics of the approach, which are based on
common sense observations about the visual patterns in image collections. Our
results show that the proposed method delivers a considerable performance
improvement compared to the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07885</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07885</id><created>2016-01-28</created><authors><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Blind Interference Alignment for Private Information Retrieval</title><categories>cs.IT cs.CR cs.IR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind interference alignment (BIA) refers to interference alignment schemes
that are designed only based on channel coherence pattern knowledge at the
transmitters (the &quot;blind&quot; transmitters do not know the exact channel values).
Private information retrieval (PIR) refers to the problem where a user
retrieves one out of K messages from N non-communicating databases (each holds
all K messages) without revealing anything about the identity of the desired
message index to any individual database. In this paper, we identify an
intriguing connection between PIR and BIA. Inspired by this connection, we
characterize the information theoretic optimal download cost of PIR, when we
have K = 2 messages and the number of databases, N, is arbitrary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07888</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07888</id><created>2016-01-28</created><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Okano</keyname><forenames>Kunihisa</forenames></author><author><keyname>Hespanha</keyname><forenames>Joao P.</forenames></author></authors><title>Stabilization of systems with asynchronous sensors and controllers</title><categories>cs.SY</categories><comments>28 pages, 5 figures. This paper was partially presented at the 2015
  American Control Conference, July 1-3, 2015, the USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the stabilization of networked control systems with asynchronous
sensors and controllers. Clock offsets between the sensor and the controller
are unknown and modeled as parametric uncertainty. First we consider MIMO
systems and provide a sufficient condition for the existence of controllers
that are capable of stabilizing the closed-loop system for every clock offset
in a given range. For scalar systems, we next obtain the maximum length of the
offset range for which the system can be stabilized by a linear time-invariant
controller. Finally, this bound is compared with the offset bounds that would
be allowed if we restricted our attention to static output feedback
controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07913</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07913</id><created>2016-01-28</created><authors><author><keyname>Baldi</keyname><forenames>Pierre</forenames></author><author><keyname>Cranmer</keyname><forenames>Kyle</forenames></author><author><keyname>Faucett</keyname><forenames>Taylor</forenames></author><author><keyname>Sadowski</keyname><forenames>Peter</forenames></author><author><keyname>Whiteson</keyname><forenames>Daniel</forenames></author></authors><title>Parameterized Machine Learning for High-Energy Physics</title><categories>hep-ex cs.LG hep-ph</categories><comments>For submission to PRD</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a new structure for machine learning classifiers applied to
problems in high-energy physics by expanding the inputs to include not only
measured features but also physics parameters. The physics parameters represent
a smoothly varying learning task, and the resulting parameterized classifier
can smoothly interpolate between them and replace sets of classifiers trained
at individual values. This simplifies the training process and gives improved
performance at intermediate values, even for complex problems requiring deep
learning. Applications include tools parameterized in terms of theoretical
model parameters, such as the mass of a particle, which allow for a single
network to provide improved discrimination across a range of masses. This
concept is simple to implement and allows for optimized interpolatable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07925</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07925</id><created>2016-01-28</created><authors><author><keyname>Olson</keyname><forenames>Randal S.</forenames></author><author><keyname>Urbanowicz</keyname><forenames>Ryan J.</forenames></author><author><keyname>Andrews</keyname><forenames>Peter C.</forenames></author><author><keyname>Lavender</keyname><forenames>Nicole A.</forenames></author><author><keyname>Kidd</keyname><forenames>La Creis</forenames></author><author><keyname>Moore</keyname><forenames>Jason H.</forenames></author></authors><title>Automating biomedical data science through tree-based pipeline
  optimization</title><categories>cs.LG cs.NE</categories><comments>16 pages, 5 figures, to appear in EvoBIO 2016 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, data science and machine learning has grown from a
mysterious art form to a staple tool across a variety of fields in academia,
business, and government. In this paper, we introduce the concept of tree-based
pipeline optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement a Tree-based Pipeline Optimization
Tool (TPOT) and demonstrate its effectiveness on a series of simulated and
real-world genetic data sets. In particular, we show that TPOT can build
machine learning pipelines that achieve competitive classification accuracy and
discover novel pipeline operators---such as synthetic feature
constructors---that significantly improve classification accuracy on these data
sets. We also highlight the current challenges to pipeline optimization, such
as the tendency to produce pipelines that overfit the data, and suggest future
research paths to overcome these challenges. As such, this work represents an
early step toward fully automating machine learning pipeline design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07929</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07929</id><created>2016-01-28</created><updated>2016-02-01</updated><authors><author><keyname>Plajner</keyname><forenames>Martin</forenames></author><author><keyname>Vomlel</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Probabilistic Models for Computerized Adaptive Testing: Experiments</title><categories>cs.AI</categories><comments>9 pages, v2: language corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper follows previous research we have already performed in the area of
Bayesian networks models for CAT. We present models using Item Response Theory
(IRT - standard CAT method), Bayesian networks, and neural networks. We
conducted simulated CAT tests on empirical data. Results of these tests are
presented for each model separately and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07932</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07932</id><created>2016-01-28</created><authors><author><keyname>Park</keyname><forenames>Keehwan</forenames></author><author><keyname>Honorio</keyname><forenames>Jean</forenames></author></authors><title>Information-Theoretic Lower Bounds for Recovery of Diffusion Network
  Structures</title><categories>cs.LG cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the information-theoretic lower bound of the sample complexity of
the correct recovery of diffusion network structures. We introduce a
discrete-time diffusion model based on the Independent Cascade model for which
we obtain a lower bound of order $\Omega(k \log p)$, for directed graphs of $p$
nodes, and at most $k$ parents per node. Next, we introduce a continuous-time
diffusion model, for which a similar lower bound of order $\Omega(k \log p)$ is
obtained. Our results show that the algorithm of Pouget-Abadie et. at. is
statistically optimal for the discrete-time regime. Our work also opens the
question of whether it is possible to devise optimal algorithms for the
continuous-time regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07941</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07941</id><created>2016-01-28</created><authors><author><keyname>Vandekerckhove</keyname><forenames>Steven</forenames></author><author><keyname>Wells</keyname><forenames>Garth N.</forenames></author><author><keyname>De Gersem</keyname><forenames>Herbert</forenames></author><author><keyname>Abeele</keyname><forenames>Koen Van Den</forenames></author></authors><title>Automatic calibration of damping layers in finite element time domain
  simulations</title><categories>cs.CE cs.NA</categories><msc-class>35L05, 65M60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matched layers are commonly used in numerical simulations of wave propagation
to model (semi-)infinite domains. Attenuation functions describe the damping in
layers, and provide a matching of the wave impedance at the interface between
the domain of interest and the absorbing region. Selecting parameters in the
attenuation functions is non-trivial. In this work, an optimisation procedure
for automatically calibrating matched layers is presented. The procedure is
based on solving optimisation problems constrained by partial differential
equations with polynomial and piecewise-constant attenuation functions. We show
experimentally that, for finite element time domain simulations,
piecewise-constant attenuation function are at least as efficient as quadratic
attenuation functions. This observation leads us to introduce consecutive
matched layers as an alternative to perfectly matched layers, which can easily
be employed for problems with arbitrary geometries. Moreover, the use of
consecutive matched layers leads to a reduction in computational cost compared
to perfectly matched layers. Examples are presented for acoustic, elastodynamic
and electromagnetic problems. Numerical simulations are performed with the
libraries FEniCS/DOLFIN and dolfin-adjoint, and the computer code to reproduce
all numerical examples is made freely available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07944</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07944</id><created>2016-01-28</created><authors><author><keyname>Fuhry</keyname><forenames>Martin</forenames></author><author><keyname>Giuliani</keyname><forenames>Andrew</forenames></author><author><keyname>Krivodonova</keyname><forenames>Lilia</forenames></author></authors><title>Discontinuous Galerkin methods on graphics processing units for
  nonlinear hyperbolic conservation laws</title><categories>cs.DC cs.MS physics.comp-ph</categories><comments>36 pages, 14 figures, 6 tables</comments><journal-ref>International Journal for Numerical Methods in Fluids, 76(12),
  982-1003 (2014)</journal-ref><doi>10.1002/fld.3963</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel implementation of the modal discontinuous Galerkin (DG)
method for hyperbolic conservation laws in two dimensions on graphics
processing units (GPUs) using NVIDIA's Compute Unified Device Architecture
(CUDA). Both flexible and highly accurate, DG methods accommodate parallel
architectures well as their discontinuous nature produces element-local
approximations. High performance scientific computing suits GPUs well, as these
powerful, massively parallel, cost-effective devices have recently included
support for double-precision floating point numbers. Computed examples for
Euler equations over unstructured triangle meshes demonstrate the effectiveness
of our implementation on an NVIDIA GTX 580 device. Profiling of our method
reveals performance comparable to an existing nodal DG-GPU implementation for
linear problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07947</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07947</id><created>2016-01-28</created><authors><author><keyname>Sheikholeslami</keyname><forenames>Fateme</forenames></author><author><keyname>Berberidis</keyname><forenames>Dimitris</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Large-scale Kernel-based Feature Extraction via Budgeted Nonlinear
  Subspace Tracking</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel-based methods enjoy powerful generalization capabilities in handling a
variety of learning tasks. When such methods are provided with sufficient
training data, broadly-applicable classes of nonlinear functions can be
approximated with desired accuracy. Nevertheless, inherent to the nonparametric
nature of kernel-based estimators are computational and memory requirements
that become prohibitive with large-scale datasets. In response to this
formidable challenge, the present work puts forward a low-rank, kernel-based,
feature extraction approach that is particularly tailored for online operation,
where data streams need not be stored in memory. A novel generative model is
introduced to approximate high-dimensional (possibly infinite) features via a
low-rank nonlinear subspace, the learning of which leads to a direct kernel
function approximation. Offline and online solvers are developed for the
subspace learning task, along with affordable versions, in which the number of
stored data vectors is confined to a predefined budget. Analytical results
provide performance bounds on how well the kernel matrix as well as
kernel-based classification and regression tasks can be approximated by
leveraging budgeted online subspace learning and feature extraction schemes.
Tests on synthetic and real datasets demonstrate and benchmark the efficiency
of the proposed method when linear classification and regression is applied to
the extracted features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07950</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07950</id><created>2016-01-28</created><authors><author><keyname>Kumar</keyname><forenames>Amit</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajeev</forenames></author><author><keyname>Patel</keyname><forenames>Vishal</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Face Alignment by Local Deep Descriptor Regression</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for extracting key-point descriptors using deep
convolutional neural networks (CNN). Unlike many existing deep CNNs, our model
computes local features around a given point in an image. We also present a
face alignment algorithm based on regression using these local descriptors. The
proposed method called Local Deep Descriptor Regression (LDDR) is able to
localize face landmarks of varying sizes, poses and occlusions with high
accuracy. Deep Descriptors presented in this paper are able to uniquely and
efficiently describe every pixel in the image and therefore can potentially
replace traditional descriptors such as SIFT and HOG. Extensive evaluations on
five publicly available unconstrained face alignment datasets show that our
deep descriptor network is able to capture strong local features around a given
landmark and performs significantly better than many competitive and
state-of-the-art face alignment algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07953</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07953</id><created>2016-01-28</created><authors><author><keyname>Jacobson</keyname><forenames>Alec</forenames></author></authors><title>Boolean Operations using Generalized Winding Numbers</title><categories>cs.GR</categories><comments>1 page, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized winding number function measures insideness for arbitrary
oriented triangle meshes. Exploiting this, I similarly generalize binary
boolean operations to act on such meshes. The resulting operations for union,
intersection, difference, etc. avoid volumetric discretization or
pre-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07962</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07962</id><created>2016-01-28</created><authors><author><keyname>Liu</keyname><forenames>Tongping</forenames></author><author><keyname>Curtsinger</keyname><forenames>Charlie</forenames></author><author><keyname>Berger</keyname><forenames>Emery D.</forenames></author></authors><title>DoubleTake: Fast and Precise Error Detection via Evidence-Based Dynamic
  Analysis</title><categories>cs.SE</categories><comments>Pre-print, accepted to appear at ICSE 2016</comments><acm-class>D.2.5; D.2.4; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents evidence-based dynamic analysis, an approach that enables
lightweight analyses--under 5% overhead for these bugs--making it practical for
the first time to perform these analyses in deployed settings. The key insight
of evidence-based dynamic analysis is that for a class of errors, it is
possible to ensure that evidence that they happened at some point in the past
remains for later detection. Evidence-based dynamic analysis allows execution
to proceed at nearly full speed until the end of an epoch (e.g., a heavyweight
system call). It then examines program state to check for evidence that an
error occurred at some time during that epoch. If so, it rolls back execution
and re-executes the code with instrumentation activated to pinpoint the error.
  We present DoubleTake, a prototype evidence-based dynamic analysis framework.
DoubleTake is practical and easy to deploy, requiring neither custom hardware,
compiler, nor operating system support. We demonstrate DoubleTake's generality
and efficiency by building dynamic analyses that find buffer overflows, memory
use-after-free errors, and memory leaks. Our evaluation shows that DoubleTake
is efficient, imposing just 4% overhead on average, making it the fastest such
system to date. It is also precise: DoubleTake pinpoints the location of these
errors to the exact line and memory addresses where they occur, providing
valuable debugging information to programmers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07965</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07965</id><created>2016-01-28</created><updated>2016-02-27</updated><authors><author><keyname>Polevoy</keyname><forenames>Gleb</forenames></author><author><keyname>de Weerdt</keyname><forenames>Mathijs</forenames></author><author><keyname>Jonker</keyname><forenames>Catholijn</forenames></author></authors><title>Towards Decision Support in Reciprocation</title><categories>cs.GT</categories><comments>An extended abstract is published at AAMAS'16(forthcoming)</comments><msc-class>91</msc-class><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People often interact repeatedly: with relatives, through file sharing, in
politics, etc. Many such interactions are reciprocal: reacting to the actions
of the other. In order to facilitate decisions regarding reciprocal
interactions, we analyze the development of reciprocation over time. To this
end, we propose a model for such interactions that is simple enough to enable
formal analysis, but is sufficient to predict how such interactions will
evolve. Inspired by existing models of international interactions and arguments
between spouses, we suggest a model with two reciprocating attitudes where an
agent's action is a weighted combination of the others' last actions (reacting)
and either i) her innate kindness, or ii) her own last action (inertia). We
analyze a network of repeatedly interacting agents, each having one of these
attitudes, and prove that their actions converge to specific limits.
Convergence means that the interaction stabilizes, and the limits indicate the
behavior after the stabilization. For two agents, we describe the interaction
process and find the limit values. For a general connected network, we find
these limit values if all the agents employ the second attitude, and show that
the agents' actions then all become equal. In the other cases, we study the
limit values using simulations. We discuss how these results predict the
development of the interaction and can be used to help agents decide on their
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07969</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07969</id><created>2016-01-28</created><authors><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Reagan</keyname><forenames>Andrew J.</forenames></author><author><keyname>Alajajian</keyname><forenames>Sharon E.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Selection models of language production support informed text
  partitioning: an intuitive and practical, bag-of-phrases framework for text
  analysis</title><categories>cs.CL</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of text segmentation, or 'chunking,' may occur at many levels in
text analysis, depending on whether it is most beneficial to break it down by
paragraphs of a book, sentences of a paragraph, etc. Here, we focus on a
fine-grained segmentation task, which we refer to as text partitioning, where
we apply methodologies to segment sentences or clauses into phrases, or lexical
constructions of one or more words. In the past, we have explored (uniform)
stochastic text partitioning---a process on the gaps between words whereby each
space assumes one from a binary state of fixed (word binding) or broken (word
separating) by some probability. In that work, we narrowly explored perhaps the
most naive version of this process: random, or, uniform stochastic
partitioning, where all word-word gaps are prescribed a uniformly-set breakage
probability, q. Under this framework, the breakage probability is a tunable
parameter, and was set to be pure-uniform: q = 1/2. In this work, we explore
phrase frequency distributions under variation of the parameter q, and define
non-uniform, or informed stochastic partitions, where q is a function of
surrounding information. Using a crude but effective function for q, we go on
to apply informed partitions to over 20,000 English texts from the Project
Gutenberg eBooks database. In these analyses, we connect selection models to
generate a notion of fit goodness for the 'bag-of-terms' (words or phrases)
representations of texts, and find informed (phrase) partitions to be an
improvement over the q = 1 (word) and q = 1/2 (phrase) partitions in most
cases. This, together with the scalability of the methods proposed, suggests
that the bag-of-phrases model should more often than not be implemented in
place of the bag-of-words model, setting the stage for a paradigm shift in
feature selection, which lies at the foundation of text analysis methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07975</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07975</id><created>2016-01-28</created><authors><author><keyname>&#xc1;lvarez</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Becher</keyname><forenames>Ver&#xf3;nica</forenames></author><author><keyname>Ferrari</keyname><forenames>Pablo A.</forenames></author><author><keyname>Yuhjtman</keyname><forenames>Sergio A.</forenames></author></authors><title>Perfect Necklaces</title><categories>math.CO cs.DM math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a variant of de Bruijn words that we call perfect necklaces. Fix
a finite alphabet. Recall that a word is a finite sequence of symbols in the
alphabet and a circular word, or necklace, is the equivalence class of a word
under rotations. For positive integers k and n, we call a necklace
(k,n)-perfect if each word of length k occurs exactly n times at positions
which are different modulo n for any convention on the starting point. We call
a necklace perfect if it is (k,k)-perfect for some k. We prove that every
arithmetic sequence with difference coprime with the alphabet size induces a
perfect necklace. In particular, the concatenation of all words of the same
length in lexicographic order yields a perfect necklace. For each k and n, we
give a closed formula for the number of (k,n)-perfect necklaces. Finally, we
prove that every infinite periodic sequence whose period coincides with some
(k,n)-perfect necklace for any n, passes all statistical tests of size up to k,
but not all larger tests. This last theorem motivated this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07976</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07976</id><created>2016-01-28</created><authors><author><keyname>A</keyname><forenames>Krishna Chaitanya</forenames></author><author><keyname>Mukherji</keyname><forenames>Utpal</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Distributed Algorithms for Complete and Partial Information Games on
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1501.04412</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Gaussian interference channel with independent direct and cross
link channel gains, each of which is independent and identically distributed
across time. Each transmitter-receiver user pair aims to maximize its long-term
average transmission rate subject to an average power constraint. We formulate
a stochastic game for this system in three different scenarios. First, we
assume that each user knows all direct and cross link channel gains. Later, we
assume that each user knows channel gains of only the links that are incident
on its receiver. Lastly, we assume that each user knows only its own direct
link channel gain. In all cases, we formulate the problem of finding a Nash
equilibrium (NE) as a variational inequality (VI) problem. We present a novel
heuristic for solving a VI. We use this heuristic to solve for a NE of power
allocation games with partial information. We also present a lower bound on the
utility for each user at any NE in the case of the games with partial
information. We obtain this lower bound using a water-filling like power
allocation that requires only knowledge of the distribution of a user's own
channel gains and average power constraints of all the users. We also provide a
distributed algorithm to compute Pareto optimal solutions for the proposed
games. Finally, we use Bayesian learning to obtain an algorithm that converges
to an $\epsilon$-Nash equilibrium for the incomplete information game with
direct link channel gain knowledge only without requiring the knowledge of the
power policies of the other users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07977</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07977</id><created>2016-01-29</created><authors><author><keyname>Xie</keyname><forenames>Guo-Sen</forenames></author><author><keyname>Zhang</keyname><forenames>Xu-Yao</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Liu</keyname><forenames>Cheng-Lin</forenames></author></authors><title>Hybrid CNN and Dictionary-Based Models for Scene Recognition and Domain
  Adaptation</title><categories>cs.CV</categories><comments>Accepted by TCSVT on Sep.2015</comments><doi>10.1109/TCSVT.2015.2511543</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural network (CNN) has achieved state-of-the-art performance
in many different visual tasks. Learned from a large-scale training dataset,
CNN features are much more discriminative and accurate than the hand-crafted
features. Moreover, CNN features are also transferable among different domains.
On the other hand, traditional dictionarybased features (such as BoW and SPM)
contain much more local discriminative and structural information, which is
implicitly embedded in the images. To further improve the performance, in this
paper, we propose to combine CNN with dictionarybased models for scene
recognition and visual domain adaptation. Specifically, based on the well-tuned
CNN models (e.g., AlexNet and VGG Net), two dictionary-based representations
are further constructed, namely mid-level local representation (MLR) and
convolutional Fisher vector representation (CFV). In MLR, an efficient
two-stage clustering method, i.e., weighted spatial and feature space spectral
clustering on the parts of a single image followed by clustering all
representative parts of all images, is used to generate a class-mixture or a
classspecific part dictionary. After that, the part dictionary is used to
operate with the multi-scale image inputs for generating midlevel
representation. In CFV, a multi-scale and scale-proportional GMM training
strategy is utilized to generate Fisher vectors based on the last convolutional
layer of CNN. By integrating the complementary information of MLR, CFV and the
CNN features of the fully connected layer, the state-of-the-art performance can
be achieved on scene recognition and domain adaptation problems. An interested
finding is that our proposed hybrid representation (from VGG net trained on
ImageNet) is also complementary with GoogLeNet and/or VGG-11 (trained on
Place205) greatly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07985</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07985</id><created>2016-01-29</created><authors><author><keyname>Zhan</keyname><forenames>Jinchun</forenames></author><author><keyname>Lois</keyname><forenames>Brian</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>Online (and Offline) Robust PCA: Novel Algorithms and Performance
  Guarantees</title><categories>cs.IT math.IT</categories><comments>A shorter version of this work will be presented at AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the online robust principal components' analysis
(RPCA) problem. In recent work, RPCA has been defined as a problem of
separating a low-rank matrix (true data), $L$, and a sparse matrix (outliers),
$S$, from their sum, $M:=L + S$. A more general version of this problem is to
recover $L$ and $S$ from $M:=L + S + W$ where $W$ is the matrix of unstructured
small noise/corruptions. An important application where this problem occurs is
in video analytics in trying to separate sparse foregrounds (e.g., moving
objects) from slowly changing backgrounds. While there has been a large amount
of recent work on solutions and guarantees for the batch RPCA problem, the
online problem is largely open.&quot;Online&quot; RPCA is the problem of doing the above
on-the-fly with the extra assumptions that the initial subspace is accurately
known and that the subspace from which $l_t$ is generated changes slowly over
time. We develop and study a novel &quot;online&quot; RPCA algorithm based on the
recently introduced Recursive Projected Compressive Sensing (ReProCS)
framework. Our algorithm improves upon the original ReProCS algorithm and it
also returns even more accurate offline estimates. The key contribution of this
work is a correctness result (complete performance guarantee) for this
algorithm under reasonably mild assumptions. By using extra assumptions --
accurate initial subspace knowledge, slow subspace change, and clustered
eigenvalues -- we are able to remove one important limitation of batch RPCA
results and two key limitations of a recent result for ReProCS for online RPCA.
To our knowledge, this work is among the first few correctness results for
online RPCA. Most earlier results were only partial results, i.e., they
required an assumption on intermediate algorithm estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07995</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07995</id><created>2016-01-29</created><authors><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>I&#xf1;iguez</keyname><forenames>Gerardo</forenames></author><author><keyname>Kikas</keyname><forenames>Riivo</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Local cascades induced global contagion: How heterogeneous thresholds,
  exogenous effects, and unconcerned behaviour govern online adoption spreading</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>28 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adoption of innovations, products or online services is commonly interpreted
as a spreading process driven to large extent by social influence and
conditioned by the needs and capacities of individuals. To model this process
one usually introduces behavioural threshold mechanisms, which can give rise to
the evolution of global cascades if the system satisfies a set of conditions.
However, these models do not address temporal aspects of the emerging cascades,
which in real systems may evolve through various pathways ranging from slow to
rapid patterns. Here we fill this gap through the analysis and modelling of
product adoption in the world's largest voice over internet service, the social
network of Skype. We provide empirical evidence about the heterogeneous
distribution of fractional behavioural thresholds, which appears to be
independent of the degree of adopting egos. We show that the structure of
real-world adoption clusters is radically different from previous theoretical
expectations, since vulnerable adoptions --induced by a single adopting
neighbour-- appear to be important only locally, while spontaneous adopters
arriving at a constant rate and the involvement of unconcerned individuals
govern the global emergence of social spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.07996</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.07996</id><created>2016-01-29</created><updated>2016-03-04</updated><authors><author><keyname>Li</keyname><forenames>Jundong</forenames></author><author><keyname>Cheng</keyname><forenames>Kewei</forenames></author><author><keyname>Wang</keyname><forenames>Suhang</forenames></author><author><keyname>Morstatter</keyname><forenames>Fred</forenames></author><author><keyname>Trevino</keyname><forenames>Robert P.</forenames></author><author><keyname>Tang</keyname><forenames>Jiliang</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>Feature Selection: A Data Perspective</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection, as a data preprocessing strategy, has been proven to be
effective and efficient in preparing high-dimensional data for data mining and
machine learning problems. The objectives of feature selection include:
building simpler and more comprehensible models, improving data mining
performance, and preparing clean, understandable data. The recent proliferation
of big data has presented some substantial challenges and opportunities of
feature selection algorithms. In this survey, we provide a comprehensive and
structured overview of recent advances in feature selection research. Motivated
by current challenges and opportunities in the big data age, we revisit feature
selection research from a data perspective, and review representative feature
selection algorithms for generic data, structured data, heterogeneous data and
streaming data. Methodologically, to emphasize the differences and similarities
of most existing feature selection algorithms for generic data, we generally
categorize them into four groups: similarity based, information theoretical
based, sparse learning based and statistical based methods. Finally, to
facilitate and promote the research in this community, we also present a
open-source feature selection repository that consists of most of the popular
feature selection algorithms (http://featureselection.asu.edu/). At the end of
this survey, we also have a discussion about some open problems and challenges
that need to be paid more attention in future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08003</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08003</id><created>2016-01-29</created><authors><author><keyname>Jonsson</keyname><forenames>Erik</forenames></author><author><keyname>Felsberg</keyname><forenames>Michael</forenames></author></authors><title>Efficient Robust Mean Value Calculation of 1D Features</title><categories>cs.CV</categories><comments>Presented at the SSBA Symposium 2005, Malm\&quot;o, Sweden</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust mean value is often a good alternative to the standard mean value
when dealing with data containing many outliers. An efficient method for
samples of one-dimensional features and the truncated quadratic error norm is
presented and compared to the method of channel averaging (soft histograms).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08011</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08011</id><created>2016-01-29</created><updated>2016-02-04</updated><authors><author><keyname>Brust</keyname><forenames>Matthias R.</forenames></author><author><keyname>Kiremire</keyname><forenames>Ankunda R.</forenames></author></authors><title>A Concise Network-Centric Survey of IP Traceback Schemes based on
  Probabilistic Packet Marking</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple probabilistic packet marking (PPM) schemes for IP traceback have
been proposed to deal with Distributed Denial of Service (DDoS) attacks by
reconstructing their attack graphs and identifying the attack sources. In this
paper, ten PPM-based IP traceback schemes are compared and analyzed in terms of
features such as convergence time, performance evaluation, underlying
topologies, incremental deployment, re-marking, and upstream graph. Our
analysis shows that the considered schemes exhibit a significant discrepancy in
performance as well as performance assessment. We concisely demonstrate this by
providing a table showing that (a) different metrics are used for many schemes
to measure their performance and, (b) most schemes are evaluated on different
classes of underlying network topologies. Our results reveal that both the
value and arrangement of the PPM-based scheme convergence times vary depending
on exactly the underlying network topology. As a result, this paper shows that
a side-by-side comparison of the scheme performance a complicated and turns out
to be a crucial open problem in this research area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08021</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08021</id><created>2016-01-29</created><authors><author><keyname>Banerjee</keyname><forenames>Soumya</forenames></author></authors><title>A Biologically Inspired Model of Distributed Online Communication
  Supporting Efficient Search and Diffusion of Innovation</title><categories>cs.SI</categories><comments>16 pages, 1 figure</comments><doi>10.7906/indecs.14.1.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We inhabit a world that is not only small but supports efficient
decentralized search - an individual using local information can establish a
line of communication with another completely unknown individual. Here we
augment a hierarchical social network model with communication between and
within communities. We argue that organization into communities would decrease
overall decentralized search times. We take inspiration from the biological
immune system which organizes search for pathogens in a hybrid modular
strategy. Our strategy has relevance in search for rare amounts of information
in online social networks. Our work also has implications for design of
efficient online networks that could have an impact on networks of human
collaboration, scientific collaboration and networks used in targeted manhunts.
Real world systems, like online social networks, have high associated delays
for long-distance links, since they are built on top of physical networks. Such
systems have been shown to densify. Hence such networks will have a
communication cost due to space and the requirement of maintaining connections.
We have incorporated such a non-spatial cost to communication. We introduce the
notion of a community size that increases with the size of the system, which is
shown to reduce the time to search for information in networks. Our final
strategy balances search times and participation costs and is shown to decrease
time to find information in decentralized search in online social networks. Our
strategy also balances strong-ties and weak-ties over long distances and may
ultimately lead to more productive and innovative networks of human
communication and enterprise. We hope that this work will lay the foundation
for strategies aimed at producing global scale human interaction networks that
are sustainable and lead to a more networked, diverse and prosperous society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08027</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08027</id><created>2016-01-29</created><authors><author><keyname>Tian</keyname><forenames>Bin</forenames><affiliation>LIMOS</affiliation></author><author><keyname>Hou</keyname><forenames>K. M.</forenames><affiliation>LIMOS</affiliation></author><author><keyname>Li</keyname><forenames>Jianjin</forenames><affiliation>LIMOS</affiliation></author></authors><title>TrAD: Traffic Adaptive Data Dissemination Protocol for Both Urban and
  Highway VANETs</title><categories>cs.NI</categories><comments>Accepted by the 30-th IEEE International Conference on Advanced
  Information Networking and Applications (AINA-2016)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad hoc Networks (VANETs) aim to improve transportation activities
that include traffic safety, transport efficiency and even infotainment on the
wheels, in which a great number of traffic event-driven messages are needed to
disseminate in a region of interest timely. However, due to the nature of
VANETs, highly dynamic mobility and frequent disconnection, data dissemination
faces great challenges. Inter-Vehicle Communication (IVC) protocols are the key
technology to mitigate this issue. Therefore, we propose an infrastructure-less
Traffic Adaptive data Dissemination (TrAD) protocol that considers road traffic
and network traffic status for both highway and urban scenarios. TrAD is
flexible to fit the irregular road topology and owns double broadcast
suppression techniques. Three state-of-the-art IVC protocols have been compared
with TrAD by means of realistic simulations. The performance of all protocols
is quantitatively evaluated with different real city maps and traffic routes.
Finally, TrAD gets an outstanding overall performance in terms of several
metrics, even though under the worse condition of GPS drift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08030</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08030</id><created>2016-01-29</created><authors><author><keyname>Tour&#xe9;</keyname><forenames>Carine</forenames><affiliation>SICAL, SCP</affiliation></author><author><keyname>Michel</keyname><forenames>Christine</forenames><affiliation>SICAL</affiliation></author><author><keyname>Marty</keyname><forenames>Jean-Charles</forenames><affiliation>SICAL</affiliation></author></authors><title>What if we considered awareness for sustainable Kwowledge Management ?
  Towards a model for self regulated knowledge management systems based on
  acceptance models of technologies and awareness</title><categories>cs.HC</categories><comments>Knowledge Management and Information Sharing (KMIS), Oct 2014, Rome,
  Italy. 2014</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose, in this paper, a model of continuous use of corporate
collaborative KMS. Companies do not always have the guaranty that their KMS
will be continuously used. This statement can constitute an important obstacle
for knowledge management processes. Our work is based on the analysis of
classical models for initial and continuous use of technologies. We also
analyse the regulation concept and explain how it is valuable to support a
continuous use of KMS. We observed that awareness may be a regulation means
that allows taking this problem into account. Awareness is a concept, which has
been profusely used to improve user experience in collaborative environments.
It is an important element for regulation of activity. In our model, we assume
that one can integrate awareness in information systems to positively influence
beliefs about them. The final objective of our work is to refine some concepts
to fit the particularities of collaborative KMS and to propose an awareness
regulation process using the traces of the users' interactions with the
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08031</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08031</id><created>2016-01-29</created><authors><author><keyname>Gurjar</keyname><forenames>Rohit</forenames></author><author><keyname>Korwar</keyname><forenames>Arpita</forenames></author><author><keyname>Saxena</keyname><forenames>Nitin</forenames></author></authors><title>Identity Testing for constant-width, and commutative, read-once
  oblivious ABPs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give improved hitting-sets for two special cases of Read-once Oblivious
Arithmetic Branching Programs (ROABP). First is the case of an ROABP with known
variable order. The best hitting-set known for this case had cost $(nw)^{O(\log
n)}$, where $n$ is the number of variables and $w$ is the width of the ROABP.
Even for a constant-width ROABP, nothing better than a quasi-polynomial bound
was known. We improve the hitting-set complexity for the known-order case to
$n^{O(\log w)}$. In particular, this gives the first polynomial time
hitting-set for constant-width ROABP (known-order). However, our hitting-set
works only over those fields whose characteristic is zero or large enough. To
construct the hitting-set, we use the concept of the rank of partial derivative
matrix. Unlike previous approaches whose basic building block is a monomial
map, we use a polynomial map.
  The second case we consider is that of commutative ROABP. The best known
hitting-set for this case had cost $d^{O(\log w)}(nw)^{O(\log \log w)}$, where
$d$ is the individual degree. We improve this hitting-set complexity to
$(ndw)^{O(\log \log w)}$. We get this by achieving rank concentration more
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08032</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08032</id><created>2016-01-29</created><authors><author><keyname>Tour&#xe9;</keyname><forenames>Carine</forenames><affiliation>SICAL, SCP</affiliation></author><author><keyname>Michel</keyname><forenames>Christine</forenames><affiliation>SICAL</affiliation></author><author><keyname>Marty</keyname><forenames>Jean-Charles</forenames><affiliation>SICAL</affiliation></author></authors><title>Re-designing knowledge management systems : Towards user-centred design
  methods integrating information architecture</title><categories>cs.HC</categories><comments>in Knowledge Management and Information Sharing, Oct 2014, Rome,
  Italy</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presented in this paper focuses on the improvement of corporate
knowledge management systems. For the implementation of such systems, companies
deploy can important means for small gains. Indeed, management services often
notice very limited use compared to what they actually expect. We present a
five-step redesigning approach which takes into account different factors to
increase the use of these systems. We use as an example the knowledge sharing
platform implemented for the employees of Soci{\'e}t{\'e} du Canal de Provence
(SCP). This system was taken into production but very occasionally used. We
describe the reasons for this limited use and we propose a design methodology
adapted to the context. Promoting the effective use of the system, our approach
has been experimented and evaluated with a panel of users working at SCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08039</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08039</id><created>2016-01-29</created><authors><author><keyname>Srivatsa</keyname><forenames>Sharath</forenames></author></authors><title>Analysis of Distributed Snapshot Algorithms</title><categories>cs.DC</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Snapshot recording durations at each process contribute to the overall
efficiency of the algorithm. In this paper we are presenting the observed
variations in snapshot recording durations at processes in a distributed
system. We conclude with key characteristics of a reliable and effective
snapshot algorithm. Simulations were achieved using SimGrid Java API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08046</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08046</id><created>2016-01-29</created><authors><author><keyname>An</keyname><forenames>Hyung-Chan</forenames></author><author><keyname>Yang</keyname><forenames>Hoeseok</forenames></author><author><keyname>Ha</keyname><forenames>Soonhoi</forenames></author></authors><title>A Formal Approach to Power Optimization in CPSs with Delay-Workload
  Dependence Awareness</title><categories>cs.OH</categories><comments>27 pages, 8 figures, 3 tables</comments><acm-class>C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of cyber-physical systems (CPSs) faces various new challenges that
are unheard of in the design of classical real-time systems. Power optimization
is one of the major design goals that is witnessing such new challenges. The
presence of interaction between the cyber and physical components of a CPS
leads to dependence between the time delay of a computational task and the
amount of workload in the next iteration. We demonstrate that it is essential
to take this delay-workload dependence into consideration in order to achieve
low power consumption.
  In this paper, we identify this new challenge, and present the first formal
and comprehensive model to enable rigorous investigations on this topic. We
propose a simple power management policy, and show that this policy achieves a
best possible notion of optimality. In fact, we show that the optimal power
consumption is attained in a &quot;steady-state&quot; operation and a simple policy of
finding and entering this steady state suffices, which can be quite surprising
considering the added complexity of this problem. Finally, we validated the
efficiency of our policy with experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08049</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08049</id><created>2016-01-29</created><updated>2016-02-23</updated><authors><author><keyname>Gorraiz</keyname><forenames>Juan</forenames></author><author><keyname>Wieland</keyname><forenames>Martin</forenames></author><author><keyname>Gumpenberger</keyname><forenames>Christian</forenames></author></authors><title>Individual Bibliometric Assessment @ University of Vienna: From Numbers
  to Multidimensional Profiles</title><categories>cs.DL</categories><comments>Preprint</comments><doi>10.5281/zenodo.45402</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper shows how bibliometric assessment can be implemented at individual
level. This has been successfully done at the University of Vienna carried out
by the Bibliometrics and Publication Strategies Department of the Vienna
University Library. According to the department's philosophy, bibliometrics is
not only a helpful evaluation instrument in order to complement the peer review
system. It is also meant as a compass for researchers in the &quot;publish or
perish&quot; dilemma in order to increase general visibility and to optimize
publication strategies. The individual assessment comprises of an interview
with the researcher under evaluation, the elaboration of a bibliometric report
of the researcher's publication output, the discussion and validation of the
obtained results with the researcher under evaluation as well as further
optional analyses. The produced bibliometric reports are provided to the
researchers themselves and inform them about the quantitative aspects of their
research output. They also serve as a basis for further discussion concerning
their publication strategies. These reports are eventually intended for
informed peer review practices, and are therefore forwarded to the quality
assurance and the rector's office and finally sent to the peers. The most
important feature of the generated bibliometric report is its multidimensional
and individual character. It relies on a variety of basic indicators and
further control parameters in order to foster comprehensibility. Researchers,
administrative staff and peers alike have confirmed the usefulness of this
bibliometric approach. An increasing demand is noticeable. In total, 33
bibliometric reports have been delivered so far. Moreover, similar reports have
also been produced for the bibliometric assessment of two faculties with great
success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08051</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08051</id><created>2016-01-29</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author></authors><title>Minimal Suffix and Rotation of a Substring in Optimal Time</title><categories>cs.DS</categories><msc-class>68W32 (Primary), 68P05, 68R15 (Secondary)</msc-class><acm-class>E.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a text given in advance, the substring minimal suffix queries ask to
determine the lexicographically minimal non-empty suffix of a substring
specified by the location of its occurrence in the text. We develop a data
structure answering such queries optimally: in constant time after linear-time
preprocessing. This improves upon the results of Babenko et al. (CPM 2014),
whose trade-off solution is characterized by $\Theta(n\log n)$ product of these
time complexities. Next, we extend our queries to support concatenations of
$O(1)$ substrings, for which the construction and query time is preserved. We
apply these generalized queries to compute lexicographically minimal and
maximal rotations of a given substring in constant time after linear-time
preprocessing.
  Our data structures mainly rely on properties of Lyndon words and Lyndon
factorizations. We combine them with further algorithmic and combinatorial
tools, such as fusion trees and the notion of order isomorphism of strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08059</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08059</id><created>2016-01-29</created><authors><author><keyname>Bikakis</keyname><forenames>Nikos</forenames></author><author><keyname>Sellis</keyname><forenames>Timos</forenames></author></authors><title>Exploration and Visualization in the Web of Big Linked Data: A Survey of
  the State of the Art</title><categories>cs.HC cs.DB</categories><comments>6th International Workshop on Linked Web Data Management (LWDM 2016)</comments><msc-class>97R50, 68P05, 68P15</msc-class><acm-class>E.1; H.2.8; H.5.2; H.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data exploration and visualization systems are of great importance in the Big
Data era. Exploring and visualizing very large datasets has become a major
research challenge, of which scalability is a vital requirement. In this
survey, we describe the major prerequisites and challenges that should be
addressed by the modern exploration and visualization systems. Considering
these challenges, we present how state-of-the-art approaches from the Database
and Information Visualization communities attempt to handle them. Finally, we
survey the systems developed by Semantic Web community in the context of the
Web of Linked Data, and discuss to which extent these satisfy the contemporary
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08062</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08062</id><created>2016-01-29</created><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>Bar</keyname><forenames>Shahar</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author><author><keyname>Tabrikian</keyname><forenames>Joseph</forenames></author></authors><title>Performance Analysis for Pilot-based 1-bit Channel Estimation with
  Unknown Quantization Threshold</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Shanghai, China, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter estimation using quantized observations is of importance in many
practical applications. Under a symmetric $1$-bit setup, consisting of a
zero-threshold hard-limiter, it is well known that the large sample performance
loss for low signal-to-noise ratios (SNRs) is moderate ($\frac{2}{\pi}$ or
$-1.96$dB). This makes low-complexity analog-to-digital converters (ADCs) with
$1$-bit resolution a promising solution for future wireless communications and
signal processing devices. However, hardware imperfections and external effects
introduce the quantizer with an unknown hard-limiting level different from
zero. In this paper, the performance loss associated with pilot-based channel
estimation, subject to an asymmetric hard limiter with unknown offset, is
studied under two setups. The analysis is carried out via the Cram\'{e}r-Rao
lower bound (CRLB) and an expected CRLB for a setup with random parameter. Our
findings show that the unknown threshold leads to an additional information
loss, which vanishes for low SNR values or when the offset is close to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08067</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08067</id><created>2016-01-29</created><authors><author><keyname>Xiang</keyname><forenames>Zhuolun</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin H.</forenames></author></authors><title>Relaxed Byzantine Vector Consensus</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact Byzantine consensus problem requires that non-faulty processes reach
agreement on a decision (or output) that is in the convex hull of the inputs at
the non-faulty processes. It is well-known that exact consensus is impossible
in an asynchronous system in presence of faults, and in a synchronous system,
n&gt;=3f+1 is tight on the number of processes to achieve exact Byzantine
consensus with scalar inputs, in presence of up to f Byzantine faulty
processes. Recent work has shown that when the inputs are d-dimensional vectors
of reals, n&gt;=max(3f+1,(d+1)f+1) is tight to achieve exact Byzantine consensus
in synchronous systems, and n&gt;= (d+2)f+1 for approximate Byzantine consensus in
asynchronous systems.
  Due to the dependence of the lower bound on vector dimension d, the number of
processes necessary becomes large when the vector dimension is large. With the
hope of reducing the lower bound on n, we consider two relaxed versions of
Byzantine vector consensus: k-Relaxed Byzantine vector consensus and
(delta,p)-Relaxed Byzantine vector consensus. In k-relaxed consensus, the
validity condition requires that the output must be in the convex hull of
projection of the inputs onto any subset of k-dimensions of the vectors. For
(delta,p)-consensus the validity condition requires that the output must be
within distance delta of the convex hull of the inputs of the non-faulty
processes, where L_p norm is used as the distance metric. For
(delta,p)-consensus, we consider two versions: in one version, delta is a
constant, and in the second version, delta is a function of the inputs
themselves.
  We show that for k-relaxed consensus and (delta,p)-consensus with constant
delta&gt;=0, the bound on n is identical to the bound stated above for the
original vector consensus problem. On the other hand, when delta depends on the
inputs, we show that the bound on n is smaller when d&gt;=3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08068</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08068</id><created>2016-01-29</created><authors><author><keyname>Bijl</keyname><forenames>Hildo</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>van Wingerden</keyname><forenames>Jan-Willem</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Online Sparse Gaussian Process Training with Input Noise</title><categories>stat.ML cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian process regression traditionally has three important downsides. (1)
It is computationally intensive, (2) it cannot efficiently implement newly
obtained measurements online, and (3) it cannot deal with stochastic (noisy)
input points. In this paper we present an algorithm tackling all these three
issues simultaneously. The resulting Sparse Online Noisy Input GP (SONIG)
regression algorithm can incorporate new measurements in constant runtime. A
comparison has shown that it is more accurate than similar existing regression
algorithms. In addition, the algorithm can be applied to non-linear black-box
system modeling, where its performance is competitive with non-linear ARX
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08084</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08084</id><created>2016-01-29</created><authors><author><keyname>Hod&#x17e;i&#x107;</keyname><forenames>S.</forenames></author><author><keyname>Pasalic</keyname><forenames>E.</forenames></author></authors><title>Generalized bent functions - sufficient conditions and related
  constructions</title><categories>math.CO cs.IT math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The necessary and sufficient conditions for a class of functions
$f:\mathbb{Z}_2^n \rightarrow \mathbb{Z}_q$, where $q \geq 2$ is an even
positive integer, have been recently identified for $q=4$ and $q=8$. In this
article we give an alternative characterization of the generalized
Walsh-Hadamard transform in terms of the Walsh spectra of the component Boolean
functions of $f$, which then allows us to derive sufficient conditions that $f$
is generalized bent for any even $q$. The case when $q$ is not a power of two,
which has not been addressed previously, is treated separately and a suitable
representation in terms of the component functions is employed. Consequently,
the derived results lead to generic construction methods of this class of
functions. The main remaining task, which is not answered in this article, is
whether the sufficient conditions are also necessary. There are some
indications that this might be true which is also formally confirmed for
generalized bent functions that belong to the class of generalized
Maiorana-McFarland functions (GMMF), but still we were unable to completely
specify (in terms of necessity) gbent conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08091</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08091</id><created>2016-01-29</created><authors><author><keyname>Jacobs</keyname><forenames>Christian T.</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D.</forenames></author><author><keyname>Kramer</keyname><forenames>Stephan C.</forenames></author><author><keyname>Funke</keyname><forenames>Simon W.</forenames></author></authors><title>On the validity of tidal turbine array configurations obtained from
  steady-state adjoint optimisation</title><categories>physics.flu-dyn cs.CE math.OC</categories><comments>Conference paper comprising 15 pages and 13 figures. Submitted to the
  Proceedings of the ECCOMAS Congress 2016 (VII European Congress on
  Computational Methods in Applied Sciences and Engineering), held in Crete,
  Greece on 5-10 June 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extracting the optimal amount of power from an array of tidal turbines
requires an intricate understanding of tidal dynamics and the effects of
turbine placement on the local and regional scale flow. Numerical models have
contributed significantly towards this understanding, and more recently,
adjoint-based modelling has been employed to optimise the positioning of the
turbines in an array in an automated way and improve on simple, regular
man-made configurations. Adjoint-based optimisation of high-resolution and
ideally 3D transient models is generally a very computationally expensive
problem. As a result, existing work on the adjoint optimisation of tidal
turbine placement has been mostly limited to steady-state simulations in which
very high, non-physical values of the background viscosity are required to
ensure that a steady-state solution exists. However, such compromises may
affect the reliability of the modelled turbines, their wakes and interactions,
and thus bring into question the validity of the computed optimal turbine
positions. This work considers a suite of idealised simulations of flow past
tidal turbine arrays in a 2D channel. It compares four regular array
configurations, detailed by Divett et al. (2013), with the configuration found
through adjoint optimisation in a steady-state, high-viscosity setup. The
optimised configuration produces considerably more power. The same
configurations are then used to produce a suite of transient simulations that
do not use constant high-viscosity, and instead use large eddy simulation (LES)
to parameterise the resulting turbulent structures. It is shown that the LES
simulations produce less power than that predicted by the constant
high-viscosity runs. Nevertheless, they still follow the same trends in the
power curve throughout time, with optimised layouts continuing to perform
significantly better than simplified configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08111</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08111</id><created>2016-01-29</created><authors><author><keyname>B&#xf6;hm</keyname><forenames>Martin</forenames></author><author><keyname>Sgall</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>van Stee</keyname><forenames>Rob</forenames></author><author><keyname>Vesel&#xfd;</keyname><forenames>Pavel</forenames></author></authors><title>The Best Two-Phase Algorithm for Bin Stretching</title><categories>cs.DS</categories><comments>Preprint of a journal version. The conference version can be found at
  arXiv:1404.5569</comments><msc-class>68W27, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Bin Stretching is a semi-online variant of bin packing in which the
algorithm has to use the same number of bins as an optimal packing, but is
allowed to slightly overpack the bins. The goal is to minimize the amount of
overpacking, i.e., the maximum size packed into any bin.
  We give an algorithm for Online Bin Stretching with a stretching factor of
$1.5$ for any number of bins. We build on previous algorithms and use a
two-phase approach. We also show that this approach cannot give better results
by proving a matching lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08112</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08112</id><created>2016-01-29</created><authors><author><keyname>Karpuk</keyname><forenames>David A.</forenames></author><author><keyname>Moss</keyname><forenames>Peter</forenames></author></authors><title>Large-Scale Channel Inversion and Transmit Vector Perturbation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behavior of channel inversion and vector perturbation schemes
for large systems, wherein the transmitter has $M$ transmit antennas, and is
transmitting to $K$ single-antenna non-cooperating receivers. We provide
results which predict the signal-to-interference-plus-noise-ratio (SINR) for
MMSE preinversion for large systems (as $K\rightarrow\infty$). We construct a
vector perturbation strategy which maximizes a very sharp estimate of the SINR
of the system, which we deem max-SINR vector perturbation, and similarly
provide results which predict the corresponding expected SINR. We demonstrate
that max-SINR vector perturbation outperforms other methods which minimize
power renormalization constants.
  The complexity of solving integer least squares problems prohibits one from
realizing the potential capacity gains of vector perturbation methods for very
large systems. To that end, we use a sub-ML solver based on a sorted QR matrix
decomposition to perform max-SINR vector perturbation for very large systems.
The resulting SINR is shown to be comparable to the ML method for small $K$,
while retaining large benefits over MMSE inversion for large $K$. We conclude
by experimentally studying the resulting capacity of our scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08116</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08116</id><created>2016-01-29</created><authors><author><keyname>Azevedo</keyname><forenames>Tiago</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Barbosa</keyname><forenames>Jorge G.</forenames></author></authors><title>Densifying the sparse cloud SimSaaS: The need of a synergy among
  agent-directed simulation, SimSaaS and HLA</title><categories>cs.MA cs.DC</categories><journal-ref>Proceedings of the 5th International Conference on Simulation and
  Modeling Methodologies, Technologies and Applications (2015) 172-177</journal-ref><doi>10.5220/0005542801720177</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modelling &amp; Simulation (M&amp;S) is broadly used in real scenarios where making
physical modifications could be highly expensive. With the so-called Simulation
Software-as-a-Service (SimSaaS), researchers could take advantage of the huge
amount of resource that cloud computing provides. Even so, studying and
analysing a problem through simulation may need several simulation tools, hence
raising interoperability issues. Having this in mind, IEEE developed a standard
for interoperability among simulators named High Level Architecture (HLA).
Moreover, the multi-agent system approach has become recognised as a convenient
approach for modelling and simulating complex systems. Despite all the recent
works and acceptance of these technologies, there is still a great lack of work
regarding synergies among them. This paper shows by means of a literature
review this lack of work or, in other words, the sparse Cloud SimSaaS. The
literature review and the resulting taxonomy are the main contributions of this
paper, as they provide a research agenda illustrating future research
opportunities and trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08117</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08117</id><created>2016-01-29</created><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author><author><keyname>Barb&#xe9;</keyname><forenames>Kurt</forenames></author></authors><title>Measurement-driven Quality Assessment of Nonlinear Systems by
  Exponential Replacement</title><categories>cs.IT math.IT</categories><comments>IEEE International Instrumentation and Measurement Technology
  Conference (I2MTC), Taipei, Taiwan, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the problem how to determine the quality of a nonlinear system
with respect to a measurement task. Due to amplification, filtering,
quantization and internal noise sources physical measurement equipment in
general exhibits a nonlinear and random input-to-output behaviour. This usually
makes it impossible to accurately describe the underlying statistical system
model. When the individual operations are all known and deterministic, one can
resort to approximations of the input-to-output function. The problem becomes
challenging when the processing chain is not exactly known or contains
nonlinear random effects. Then one has to approximate the output distribution
in an empirical way. Here we show that by measuring the first two sample
moments of an arbitrary set of output transformations in a calibrated setup,
the output distribution of the actual system can be approximated by an
equivalent exponential family distribution. This method has the property that
the resulting approximation of the statistical system model is guaranteed to be
pessimistic in an estimation theoretic sense. We show this by proving that an
equivalent exponential family distribution in general exhibits a lower Fisher
information measure than the original system model. With various examples and a
model matching step we demonstrate how this estimation theoretic aspect can be
exploited in practice in order to obtain a conservative measurement-driven
quality assessment method for nonlinear measurement systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08123</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08123</id><created>2016-01-29</created><authors><author><keyname>Jacob</keyname><forenames>Swaroop</forenames></author><author><keyname>Narasimhan</keyname><forenames>T. Lakshmi</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Space-Time Index Modulation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new multi-antenna modulation scheme, termed as
{\em space-time index modulation (STIM)}. In STIM, information bits are
conveyed through antenna indexing in the spatial domain, slot indexing in the
time domain, and $M$-ary modulation symbols. A time slot in a given frame can
be used or unused, and the choice of the slots used for transmission conveys
slot index bits. In addition, antenna index bits are conveyed in every used
time slot by activating one among the available antennas. $M$-ary symbols are
sent on the active antenna in a used time slot. We study STIM in a
cyclic-prefixed single-carrier (CPSC) system in frequency-selective fading
channels. It is shown that, for the same spectral efficiency, STIM can achieve
better performance compared to conventional orthogonal frequency division
multiplexing (OFDM). Low-complexity iterative algorithms for the detection of
large-dimensional STIM signals are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08132</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08132</id><created>2016-01-29</created><authors><author><keyname>Kalokidou</keyname><forenames>Vaia</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Piechocki</keyname><forenames>Robert</forenames></author></authors><title>Interference Management in Heterogeneous Networks with Blind
  Transmitters</title><categories>cs.IT math.IT</categories><comments>30 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future multi-tier communication networks will require enhanced network
capacity and reduced overhead. In the absence of Channel State Information
(CSI) at the transmitters, Blind Interference Alignment (BIA) and Topological
Interference Management (TIM) can achieve optimal Degrees of Freedom (DoF),
minimising network's overhead. In addition, Non-Orthogonal Multiple Access
(NOMA) can increase the sum rate of the network, compared to orthogonal radio
access techniques currently adopted by 4G networks. Our contribution is two
interference management schemes, BIA and a hybrid TIM-NOMA scheme, employed in
heterogeneous networks by applying user-pairing and Kronecker Product
representation. BIA manages inter- and intra-cell interference by antenna
selection and appropriate message scheduling. The hybrid scheme manages
intra-cell interference based on NOMA and inter-cell interference based on TIM.
We show that both schemes achieve at least double the rate of TDMA. The hybrid
scheme always outperforms TDMA and BIA in terms of Degrees of Freedom (DoF).
Comparing the two proposed schemes, BIA achieves more DoF than TDMA under
certain restrictions, and provides better Bit-Error-Rate (BER) and sum rate
performance to macrocell users, whereas the hybrid scheme improves the
performance of femtocell users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08139</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08139</id><created>2016-01-29</created><authors><author><keyname>Abidi</keyname><forenames>Abdessalem</forenames></author><author><keyname>Wang</keyname><forenames>Qianxue</forenames></author><author><keyname>Bouall&#xe8;gue</keyname><forenames>Belgacem</forenames></author><author><keyname>Machhout</keyname><forenames>Mohsen</forenames></author><author><keyname>Gyeux</keyname><forenames>Christophe</forenames></author></authors><title>Quantitative Evaluation of Chaotic CBC Mode of Operation</title><categories>cs.CR</categories><comments>in International Conference on Advanced Technologies for Signal &amp;
  Images Processing ATSIP'2016 , Mar 2016, Monastir, Tunisia</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cipher block chaining (CBC) block cipher mode of operation presents a
very popular way of encrypting which is used in various applications. In
previous research work, we have mathematically proven that, under some
conditions, this mode of operation can admit a chaotic behavior according to
Devaney. Proving that CBC mode is chaotic is only the beginning of the study of
its security. The next step, which is the purpose of this paper, is to develop
the quantitative study of the chaotic CBC mode of operation by evaluating the
level of sensibility and expansivity for this mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08154</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08154</id><created>2016-01-29</created><authors><author><keyname>Azevedo</keyname><forenames>Tiago</forenames></author><author><keyname>de Ara&#xfa;jo</keyname><forenames>Paulo J. M.</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Rocha</keyname><forenames>Ana Paula C.</forenames></author></authors><title>JADE, TraSMAPI and SUMO: A tool-chain for simulating traffic light
  control</title><categories>cs.MA</categories><journal-ref>Proceedings of the 8th International Workshop on Agents in Traffic
  and Transportation, ATT'14, held at the Thirteenth International Joint
  Conference on Autonomous Agents and Multiagent Systems, AAMAS'14 (2014) 8-15</journal-ref><doi>10.13140/2.1.2739.4886</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Increased stress, fuel consumption, air pollution, accidents and delays are
some of the consequences of traffic congestion usually incurring in tremendous
economic impacts, which society aims to remedy in order to leverage a
sustainable development. Recently, unconventional means for modeling and
controlling such complex traffic systems relying on multi-agent systems have
arisen. This paper contributes to the understanding of such complex and highly
dynamic systems by proposing an open-source tool-chain to implement
multi-agent-based solutions in traffic and transportation. The proposed
approach relies on two very popular tools in both domains, with focus on
traffic light control. This tool-chain consists in combining JADE (Java Agent
DEvelopment Framework), for the implementation of multi-agent systems, with
SUMO (Simulation of Urban MObility), for the microscopic simulation of traffic
interactions. TraSMAPI (Traffic Simulation Manager Application Programming
Interface) is used to combine JADE and SUMO allowing communication between
them. A demonstration of the concept is presented to illustrate the main
features of this tool-chain, using Q-Learning as the reinforcement learning
method for each traffic light agent in a simulated network. Results demonstrate
the feasibility of the proposed framework as a practical means to experiment
with different agent-based designs of intelligent transportation solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08158</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08158</id><created>2016-01-29</created><authors><author><keyname>Mart&#xed;nez-G&#xf3;mez</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Morell</keyname><forenames>Vicente</forenames></author><author><keyname>Cazorla</keyname><forenames>Miguel</forenames></author><author><keyname>Garc&#xed;a-Varea</keyname><forenames>Ismael</forenames></author></authors><title>Semantic Localization in the PCL library</title><categories>cs.RO</categories><comments>19 pages, 6 figures, ISSN 0921-8890</comments><journal-ref>Robotics and Autonomous Systems, Volume 75, Part B, January 2016,
  Pages 641-648</journal-ref><doi>10.1016/j.robot.2015.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic localization problem in robotics consists in determining the
place where a robot is located by means of semantic categories. The problem is
usually addressed as a supervised classification process, where input data
correspond to robot perceptions while classes to semantic categories, like
kitchen or corridor.
  In this paper we propose a framework, implemented in the PCL library, which
provides a set of valuable tools to easily develop and evaluate semantic
localization systems. The implementation includes the generation of 3D global
descriptors following a Bag-of-Words approach. This allows the generation of
dimensionality-fixed descriptors from any type of keypoint detector and feature
extractor combinations. The framework has been designed, structured and
implemented in order to be easily extended with different keypoint detectors,
feature extractors as well as classification models.
  The proposed framework has also been used to evaluate the performance of a
set of already implemented descriptors, when used as input for a specific
semantic localization system. The results obtained are discussed paying special
attention to the internal parameters of the BoW descriptor generation process.
Moreover, we also review the combination of some keypoint detectors with
different 3D descriptor generation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08162</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08162</id><created>2016-01-29</created><authors><author><keyname>Azevedo</keyname><forenames>Tiago</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Barbosa</keyname><forenames>Jorge G.</forenames></author></authors><title>A State-of-the-art Integrated Transportation Simulation Platform</title><categories>cs.MA cs.DC</categories><journal-ref>Proceedings of the 4th International Conference on Models and
  Technologies for Intelligent Transportation Systems (2015) 340-347</journal-ref><doi>10.1109/MTITS.2015.7223277</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Nowadays, universities and companies have a huge need for simulation and
modelling methodologies. In the particular case of traffic and transportation,
making physical modifications to the real traffic networks could be highly
expensive, dependent on political decisions and could be highly disruptive to
the environment. However, while studying a specific domain or problem,
analysing a problem through simulation may not be trivial and may need several
simulation tools, hence raising interoperability issues. To overcome these
problems, we propose an agent-directed transportation simulation platform,
through the cloud, by means of services. We intend to use the IEEE standard HLA
(High Level Architecture) for simulators interoperability and agents for
controlling and coordination. Our motivations are to allow multiresolution
analysis of complex domains, to allow experts to collaborate on the analysis of
a common problem and to allow co-simulation and synergy of different
application domains. This paper will start by presenting some preliminary
background concepts to help better understand the scope of this work. After
that, the results of a literature review is shown. Finally, the general
architecture of a transportation simulation platform is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08165</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08165</id><created>2016-01-29</created><authors><author><keyname>Nguyen</keyname><forenames>Thien Bao</forenames></author><author><keyname>Olivetti</keyname><forenames>Emanuele</forenames></author><author><keyname>Avesani</keyname><forenames>Paolo</forenames></author></authors><title>Mapping Tractography Across Subjects</title><categories>stat.ML cs.CV q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion magnetic resonance imaging (dMRI) and tractography provide means to
study the anatomical structures within the white matter of the brain. When
studying tractography data across subjects, it is usually necessary to align,
i.e. to register, tractographies together. This registration step is most often
performed by applying the transformation resulting from the registration of
other volumetric images (T1, FA). In contrast with registration methods that
&quot;transform&quot; tractographies, in this work, we try to find which streamline in
one tractography correspond to which streamline in the other tractography,
without any transformation. In other words, we try to find a &quot;mapping&quot; between
the tractographies. We propose a graph-based solution for the tractography
mapping problem and we explain similarities and differences with the related
well-known graph matching problem. Specifically, we define a loss function
based on the pairwise streamline distance and reformulate the mapping problem
as combinatorial optimization of that loss function. We show preliminary
promising results where we compare the proposed method, implemented with
simulated annealing, against a standard registration techniques in a task of
segmentation of the corticospinal tract.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08169</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08169</id><created>2016-01-29</created><authors><author><keyname>Kir&#xe1;ly</keyname><forenames>Franz J</forenames></author><author><keyname>Oberhauser</keyname><forenames>Harald</forenames></author></authors><title>Kernels for sequentially ordered data</title><categories>stat.ML cs.DM cs.LG math.ST stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework for kernel learning with sequential data of any
kind, such as time series, sequences of graphs, or strings. Our approach is
based on signature features which can be seen as an ordered variant of sample
(cross-)moments; it allows to obtain a &quot;sequentialized&quot; version of any static
kernel. The sequential kernels are efficiently computable for discrete
sequences and are shown to approximate a continuous moment form in a sampling
sense.
  A number of known kernels for sequences arise as &quot;sequentializations&quot; of
suitable static kernels: string kernels may be obtained as a special case, and
alignment kernels are closely related up to a modification that resolves their
open non-definiteness issue. Our experiments indicate that our signature-based
sequential kernel framework may be a promising approach to learning with
sequential data, such as time series, that allows to avoid extensive manual
pre-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08176</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08176</id><created>2016-01-29</created><authors><author><keyname>Yazdi</keyname><forenames>S. M. Hossein Tabatabaei</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Weakly Mutually Uncorrelated Codes</title><categories>cs.IT math.IT</categories><comments>1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of weakly mutually uncorrelated (WMU) sequences,
motivated by applications in DNA-based storage systems and synchronization
protocols. WMU sequences are characterized by the property that no sufficiently
long suffix of one sequence is the prefix of the same or another sequence. In
addition, WMU sequences used in DNA-based storage systems are required to have
balanced compositions of symbols and to be at large mutual Hamming distance
from each other. We present a number of constructions for balanced,
error-correcting WMU codes using Dyck paths, Knuth's balancing principle,
prefix synchronized and cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08179</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08179</id><created>2016-01-29</created><authors><author><keyname>Huismann</keyname><forenames>Immo</forenames></author><author><keyname>Stiller</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Fr&#xf6;hlich</keyname><forenames>Jochen</forenames></author></authors><title>Factorizing the factorization - a spectral-element solver for elliptic
  equations with linear operation count</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-order methods gain more and more attention in computational fluid
dynamics. However, the potential advantage of these methods depends critically
on the availability of efficient elliptic solvers. With spectral-element
methods, static condensation is a common approach to reduce the number of
degree of freedoms and to improve the condition of the algebraic equations. The
resulting system is block-structured and the face-based operator well suited
for matrix-matrix multiplications. However, a straight-forward implementation
scales super-linearly with the number of unknowns and, therefore, prohibits the
application to high polynomial degrees. This paper proposes a novel
factorization technique, which yields a linear operation count of just 13N
multiplications, where N is the total number of unknowns. In comparison to
previous work it saves a factor larger than 3 and clearly outpaces unfactored
variants for all polynomial degrees. Using the new technique as a building
block for a preconditioned conjugate gradient method resulted in a runtime
scaling linearly with N for polynomial degrees $2 \leq p \leq 32$ . Moreover
the solver proved remarkably robust for aspect ratios up to 128.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08188</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08188</id><created>2016-01-29</created><authors><author><keyname>Wand</keyname><forenames>Michael</forenames></author><author><keyname>Koutn&#xed;k</keyname><forenames>Jan</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Lipreading with Long Short-Term Memory</title><categories>cs.CV cs.CL</categories><comments>Accepted for publication at ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lipreading, i.e. speech recognition from visual-only recordings of a
speaker's face, can be achieved with a processing pipeline based solely on
neural networks, yielding significantly better accuracy than conventional
methods. Feed-forward and recurrent neural network layers (namely Long
Short-Term Memory; LSTM) are stacked to form a single structure which is
trained by back-propagating error gradients through all the layers. The
performance of such a stacked network was experimentally evaluated and compared
to a standard Support Vector Machine classifier using conventional computer
vision features (Eigenlips and Histograms of Oriented Gradients). The
evaluation was performed on data from 19 speakers of the publicly available
GRID corpus. With 51 different words to classify, we report a best word
accuracy on held-out evaluation speakers of 79.6% using the end-to-end neural
network-based solution (11.6% improvement over the best feature-based solution
evaluated).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08189</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08189</id><created>2016-01-29</created><updated>2016-03-04</updated><authors><author><keyname>Chen</keyname><forenames>Jiahao</forenames></author><author><keyname>Zhang</keyname><forenames>Weijian</forenames></author></authors><title>The Right Way to Search Evolving Graphs</title><categories>cs.SI cs.DS math.NA</categories><comments>10 pages, 5 figures</comments><msc-class>05C82, 91D30</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolving graphs arise in problems where interrelations between data change
over time. We present a breadth first search (BFS) algorithm for evolving
graphs that computes the most direct influences between nodes at two different
times. Using simple examples, we show that naive unfoldings of adjacency
matrices miscount the number of temporal paths. By mapping an evolving graph to
an adjacency matrix of an equivalent static graph, we prove that our
generalization of the BFS algorithm correctly accounts for paths that traverse
both space and time. Finally, we demonstrate how the BFS over evolving graphs
can be applied to mine citation networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08190</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08190</id><created>2016-01-29</created><authors><author><keyname>Krishnan</keyname><forenames>M. Nikhil</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>On MBR codes with replication</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An early paper by Rashmi et. al. presented the construction of an
$(n,k,d=n-1)$ MBR regenerating code featuring the inherent double replication
of all code symbols and repair-by-transfer (RBT), both of which are important
in practice. We first show that no MBR code can contain even a single code
symbol that is replicated more than twice. We then go on to present two new
families of MBR codes which feature double replication of all systematic
message symbols. The codes also possess a set of $d$ nodes whose contents
include the message symbols and which can be repaired through help-by-transfer
(HBT). As a corollary, we obtain systematic RBT codes for the case $d=(n-1)$
that possess inherent double replication of all code symbols and having a field
size of $O(n)$ in comparison with the general, $O(n^2)$ field size requirement
of the earlier construction by Rashmi et. al. For the cases $(k=d=n-2)$ or
$(k+1=d=n-2)$, the field size can be reduced to $q=2$, and hence the codes can
be binary. We also give a necessary and sufficient condition for the existence
of MBR codes having double replication of all code symbols and also suggest
techniques which will enable an arbitrary MBR code to be converted to one with
double replication of all code symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08201</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08201</id><created>2016-01-29</created><authors><author><keyname>Odinaka</keyname><forenames>Ikenna</forenames></author><author><keyname>Kaganovsky</keyname><forenames>Yan</forenames></author><author><keyname>Greenberg</keyname><forenames>Joel A.</forenames></author><author><keyname>Hassan</keyname><forenames>Mehadi</forenames></author><author><keyname>Politte</keyname><forenames>David G.</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Joseph A.</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author><author><keyname>Brady</keyname><forenames>David J.</forenames></author></authors><title>Spectrally Grouped Total Variation Reconstruction for Scatter Imaging
  Using ADMM</title><categories>math.NA cs.CV</categories><comments>Presented at IEEE Nuclear Science Symposium and Medical Imaging
  Conference (NSS/MIC) 2015. 4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider X-ray coherent scatter imaging, where the goal is to reconstruct
momentum transfer profiles (spectral distributions) at each spatial location
from multiplexed measurements of scatter. Each material is characterized by a
unique momentum transfer profile (MTP) which can be used to discriminate
between different materials. We propose an iterative image reconstruction
algorithm based on a Poisson noise model that can account for photon-limited
measurements as well as various second order statistics of the data. To improve
image quality, previous approaches use edge-preserving regularizers to promote
piecewise constancy of the image in the spatial domain while treating each
spectral bin separately. Instead, we propose spectrally grouped regularization
that promotes piecewise constant images along the spatial directions but also
ensures that the MTPs of neighboring spatial bins are similar, if they contain
the same material. We demonstrate that this group regularization results in
improvement of both spectral and spatial image quality. We pursue an
optimization transfer approach where convex decompositions are used to lift the
problem such that all hyper-voxels can be updated in parallel and in
closed-form. The group penalty introduces a challenge since it is not directly
amendable to these decompositions. We use the alternating directions method of
multipliers (ADMM) to replace the original problem with an equivalent sequence
of sub-problems that are amendable to convex decompositions, leading to a
highly parallel algorithm. We demonstrate the performance on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08221</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08221</id><created>2016-01-29</created><updated>2016-02-28</updated><authors><author><keyname>Marcus</keyname><forenames>Ryan</forenames></author><author><keyname>Papaemmanouil</keyname><forenames>Olga</forenames></author></authors><title>WiSeDB: A Learning-based Workload Management Advisor for Cloud Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Workload management for cloud databases must deal with the tasks of resource
provisioning, query placement and query scheduling in a manner that meets the
application's performance goals while minimizing the cost of using cloud
resources. Existing solutions have approached these three challenges in
isolation, and with only a particular type of performance goal in mind. In this
paper, we introduce WiSeDB, a learning-based framework for generating holistic
workload management solutions customized to application-defined performance
metrics and workload characteristics. Our approach relies on supervised
learning to train cost-effective decision tree models for guiding query
placement, scheduling, and resource provisioning decisions. Applications can
use these models for both batch and online scheduling of incoming workloads. A
unique feature of our system is that it can adapt its offline model to
stricter/looser performance goals with minimal re-training. This allows us to
present alternative workload management strategies that address the typical
performance vs. cost trade-off of cloud services. Experimental results show
that our approach has very low training overhead while offering low cost
strategies for a variety of performance goals and workload characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08224</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08224</id><created>2016-01-29</created><authors><author><keyname>Erd&#x151;s</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Toroczkai</keyname><forenames>Zolt&#xe1;n</forenames></author></authors><title>New classes of degree sequences with fast mixing swap Markov chain
  sampling</title><categories>math.CO cs.DM</categories><comments>submitted</comments><msc-class>05C81, 68R10, 05C07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In network modeling of complex systems one is often required to sample random
realizations of networks that obey a given set of constraints, often in form of
graph measures. A much studied class of problems targets uniform sampling of
simple graphs with given degree sequence or also with given degree correlations
expressed in the form of a joint degree matrix. One approach is to use Markov
chains based on edge switches (swaps) that preserve the constraints, are
irreducible (ergodic) and fast mixing. In 1999, Kannan, Tetali and Vempala
(KTV) proposed a simple swap Markov chain for sampling graphs with given degree
sequence and conjectured that it mixes rapidly (in poly-time) for arbitrary
degree sequences. While the conjecture is still open for the general case, it
was proven for special degree sequences, in particular, for those of undirected
and directed regular simple graphs (Cooper, Dyer, Greenhill, 2007; Greenhill,
2011), of half-regular bipartite graphs (Mikl\'os, Erd\H{o}s, Soukup, 2013),
and of graphs with certain bounded maximum degrees (Greenhill, 2015). Here we
prove the fast mixing KTV conjecture for novel, exponentially large classes of
irregular degree sequences. Our method is based on a canonical decomposition of
degree sequences into split graph degree sequences due to Tyshkevich (1984,
2000), a structural theorem for the space of graph realizations by Barrus
(2015) and on a factorization theorem for Markov chains (Erd\H{o}s, Mikl\'os,
Toroczkai, 2015). After introducing bipartite splitted degree sequences, we
also generalize Tyshkevich's decomposition for bipartite and directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08227</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08227</id><created>2016-01-29</created><authors><author><keyname>M&#xe1;rquez-Corbella</keyname><forenames>Irene</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Using Reed-Solomon codes in the $\left( U\mid U+V\right)$ construction
  and an application to cryptography</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a modification of Reed-Solomon codes that beats the
Guruwami-Sudan $1-\sqrt{R}$ decoding radius of Reed-Solomon codes at low rates
$R$. The idea is to choose Reed-Solomon codes $U$ and $V$ with appropriate
rates in a $\left( U\mid U+V\right)$ construction and to decode them with the
Koetter-Vardy soft information decoder. We suggest to use a slightly more
general version of these codes (but which has the same decoding performances as
the $\left( U\mid U+V\right)$-construction) for code-based cryptography, namely
to build a McEliece scheme. The point is here that these codes not only perform
nearly as well (or even better in the low rate regime) as Reed-Solomon codes,
their structure seems to avoid the Sidelnikov-Shestakov attack which broke a
previous McEliece proposal based on generalized Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08229</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08229</id><created>2016-01-29</created><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author><author><keyname>Micha&#x142;ek</keyname><forenames>Mateusz</forenames></author></authors><title>On the geometry of border rank algorithms for matrix multiplication and
  other tensors with symmetry</title><categories>math.AG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish basic information about border rank algorithms for the matrix
multiplication tensor and other tensors with symmetry. We prove that border
rank algorithms for tensors with symmetry (such as matrix multiplication and
the determinant polynomial) come in families that include representatives with
normal forms. These normal forms will be useful both to develop new efficient
algorithms and to prove lower complexity bounds. We derive a border rank
version of the substitution method used in proving lower bounds for tensor
rank. We use this border-substitution method and a normal form to improve the
lower bound on the border rank of matrix multiplication by one, to 2n^2- n+1.
We also point out difficulties that will be formidable obstacles to future
progress on lower complexity bounds for tensors because of the &quot;wild&quot; structure
of the Hilbert scheme of points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1601.08237</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1601.08237</id><created>2016-01-29</created><authors><author><keyname>Almeida</keyname><forenames>J.</forenames></author><author><keyname>Kl&#xed;ma</keyname><forenames>O.</forenames></author><author><keyname>Kunc</keyname><forenames>M.</forenames></author></authors><title>The omega-inequality problem for concatenation hierarchies of star-free
  languages</title><categories>math.GR cs.FL</categories><msc-class>Primary 20M05, 20M07, Secondary 20M35, 68Q70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem considered in this paper is whether an inequality of omega-terms
is valid in a given level of a concatenation hierarchy of star-free languages.
The main result shows that this problem is decidable for all (integer and half)
levels of the Straubing-Th\'erien hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00020</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00020</id><created>2016-01-29</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Wang</keyname><forenames>Yinong</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Burns</keyname><forenames>Joseph E.</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Deep convolutional networks for automated detection of posterior-element
  fractures on spine CT</title><categories>cs.CV</categories><comments>To be presented at SPIE Medical Imaging, 2016, San Diego</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Injuries of the spine, and its posterior elements in particular, are a common
occurrence in trauma patients, with potentially devastating consequences.
Computer-aided detection (CADe) could assist in the detection and
classification of spine fractures. Furthermore, CAD could help assess the
stability and chronicity of fractures, as well as facilitate research into
optimization of treatment paradigms.
  In this work, we apply deep convolutional networks (ConvNets) for the
automated detection of posterior element fractures of the spine. First, the
vertebra bodies of the spine with its posterior elements are segmented in spine
CT using multi-atlas label fusion. Then, edge maps of the posterior elements
are computed. These edge maps serve as candidate regions for predicting a set
of probabilities for fractures along the image edges using ConvNets in a 2.5D
fashion (three orthogonal patches in axial, coronal and sagittal planes). We
explore three different methods for training the ConvNet using 2.5D patches
along the edge maps of 'positive', i.e. fractured posterior-elements and
'negative', i.e. non-fractured elements.
  An experienced radiologist retrospectively marked the location of 55
displaced posterior-element fractures in 18 trauma patients. We randomly split
the data into training and testing cases. In testing, we achieve an
area-under-the-curve of 0.857. This corresponds to 71% or 81% sensitivities at
5 or 10 false-positives per patient, respectively. Analysis of our set of
trauma patients demonstrates the feasibility of detecting posterior-element
fractures in spine CT images using computer vision techniques such as deep
convolutional networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00023</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00023</id><created>2016-01-29</created><authors><author><keyname>Barbay</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author></authors><title>Optimal Prefix Free Codes With Partial Sorting</title><categories>cs.DS</categories><comments>13 pages, no figures. arXiv admin note: text overlap with
  arXiv:1204.5801</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm computing an optimal prefix free code for $n$
unsorted positive weights in time within $O(n(1+\lg \alpha))\subseteq O(n\lg
n)$, where the alternation $\alpha\in[1..n-1]$ measures the amount of sorting
required by the computation. This asymptotical complexity is within a constant
factor of the optimal in the algebraic decision tree computational model, in
the worst case over all instances of size $n$ and alternation $\alpha$. Such
results refine the state of the art complexity of $\Theta(n\lg n)$ in the worst
case over instances of size $n$ in the same computational model, a landmark in
compression and coding since 1952, by the mere combination of van Leeuwen's
algorithm to compute optimal prefix free codes from sorted weights (known since
1976), with Deferred Data Structures to partially sort a multiset depending on
the queries on it (known since 1988).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00032</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00032</id><created>2016-01-29</created><updated>2016-02-02</updated><authors><author><keyname>Ye</keyname><forenames>Chengxi</forenames></author><author><keyname>Yang</keyname><forenames>Yezhou</forenames></author><author><keyname>Fermuller</keyname><forenames>Cornelia</forenames></author><author><keyname>Aloimonos</keyname><forenames>Yiannis</forenames></author></authors><title>What Can I Do Around Here? Deep Functional Scene Understanding for
  Cognitive Robots</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For robots that have the capability to interact with the physical environment
through their end effectors, understanding the surrounding scenes is not merely
a task of image classification or object recognition. To perform actual tasks,
it is critical for the robot to have a functional understanding of the visual
scene. Here, we address the problem of localizing and recognition of functional
areas from an arbitrary indoor scene, formulated as a two-stage deep learning
based detection pipeline. A new scene functionality testing-bed, which is
complied from two publicly available indoor scene datasets, is used for
evaluation. Our method is evaluated quantitatively on the new dataset,
demonstrating the ability to perform efficient recognition of functional areas
from arbitrary indoor scenes. We also demonstrate that our detection model can
be generalized onto novel indoor scenes by cross validating it with the images
from two different datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00033</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00033</id><created>2016-01-29</created><authors><author><keyname>Lin</keyname><forenames>Chunbin</forenames></author><author><keyname>Mandel</keyname><forenames>Benjamin</forenames></author><author><keyname>Papakonstantinou</keyname><forenames>Yannis</forenames></author><author><keyname>Springer</keyname><forenames>Matthias</forenames></author></authors><title>Fast In-Memory SQL Analytics on Relationships between Entities</title><categories>cs.DB</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study relationship queries on graph databases with binary
relationship. A relationship query is a graph reachability query bounded on
primary-keys and foreign-keys with aggregation on single output attribute. Both
row stores and column stores miss key opportunities towards the efficient
execution of relationship queries, making them unacceptably slow in real-world
OLAP scenarios.
  We present the FastR in-memory analytics engine that utilizes a new form of
bottom-up fully pipelined query processing execution strategy. The plans run on
a novel data organization that combines salient features of column-based
organization, indexing and compression. Furthermore, FastR compiles its query
plans into query-aware executable C++ source codes. Besides achieving runtime
efficiency, FastR also reduces main memory requirements because, unlike column
databases, FastR selectively allows more dense forms of compression including
heavy-weighted compressions, which do not support random access.
  In our experiments, we used FastR to accelerate queries for two OLAP
dashboards in the biomedical field. The first dashboard runs queries on the
PubMed dataset and the second one on the SemMedDB dataset. FastR outperforms
Postgres by 2-4 orders of magnitude and MonetDB by 1-3 orders of magnitude,
when all of them are running on RAM. Our experiments dissect the FastR
advantage between (i) the novel FastR execution strategy and associated data
structures and (ii) the use of compiled code. We also provide an analysis and
experiments illustrating space savings due to appropriate use of compression
methods. Finally, we investigate the beneficial effect of the increasing CPU
cycles / RAM transfer rate ratio on the space-time tradeoff for various
compression methods used in FastR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00036</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00036</id><created>2016-01-29</created><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>On the Automorphism Groups of the Z2Z4-Linear 1-Perfect and
  Preparata-Like Codes</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>10pp, subm. to Des. Codes Crypt</comments><msc-class>94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the symmetry group of a $Z_2Z_4$-linear code with parameters of a
$1$-perfect, extended $1$-perfect, or Preparata-like code. We show that,
provided the code length is greater than $16$, this group consists only of
symmetries that preserve the $Z_2Z_4$ structure. We find the orders of the
symmetry groups of the $Z_2Z_4$-linear (extended) $1$-perfect codes. Keywords:
additive codes, $Z_2Z_4$-linear codes, $1$-perfect codes, Preparata-like codes,
automorphism group, symmetry group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00043</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00043</id><created>2016-01-29</created><authors><author><keyname>Diaz</keyname><forenames>Mario</forenames></author></authors><title>On the Symmetries and the Capacity Achieving Input Covariance Matrices
  of Multiantenna Channels</title><categories>cs.IT math.IT</categories><comments>10 pages, submitted to the 2016 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the capacity achieving input covariance matrices of a
single user multiantenna channel based solely on the group of symmetries of its
matrix of propagation coefficients. Our main result, which unifies and improves
the techniques used in a variety of classical capacity theorems, uses the Haar
(uniform) measure on the group of symmetries to establish the existence of a
capacity achieving input covariance matrix in a very particular subset of the
covariance matrices. This result allows us to provide simple proofs for old and
new capacity theorems. Among other results, we show that for channels with two
or more standard symmetries, the isotropic input is optimal. In overall, this
paper provides a precise explanation of why the capacity achieving input
covariance matrices of a channel depend more on the symmetries of the matrix of
propagation coefficients than any other distributional assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00057</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00057</id><created>2016-01-29</created><authors><author><keyname>Bari</keyname><forenames>Mohammad</forenames></author><author><keyname>Doroslovacki</keyname><forenames>Milos</forenames></author></authors><title>Separation of Signals Consisting of Amplitude and Instantaneous
  Frequency RRC Pulses Using SNR Uniform Training</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents sample mean and sample variance based features that
distinguish continuous phase FSK from QAM and PSK modulations. Root raised
cosine pulses are used for signal generation. Support vector machines are
employed for signals separation. They are trained for only one value of SNR and
used to classify the signals from a wide range of SNR. A priori information
about carrier amplitude, carrier phase, carrier offset, roll-off factor and
initial symbol phase is relaxed. Effectiveness of the method is tested by
observing the joint effects of AWGN, carrier offset, lack of symbol and
sampling synchronization, and fast fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00061</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00061</id><created>2016-01-29</created><authors><author><keyname>Kong</keyname><forenames>Weihao</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Spectrum Estimation from Samples</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximating the set of eigenvalues of the
covariance matrix of a multivariate distribution (equivalently, the problem of
approximating the &quot;population spectrum&quot;), given access to samples drawn from
the distribution. The eigenvalues of the covariance of a distribution contain
basic information about the distribution, including the presence or lack of
structure in the distribution, the effective dimensionality of the
distribution, and the applicability of higher-level machine learning and
multivariate statistical tools. We consider this fundamental recovery problem
in the regime where the number of samples is comparable, or even sublinear in
the dimensionality of the distribution in question. First, we propose a
theoretically optimal and computationally efficient algorithm for recovering
the moments of the eigenvalues---the \emph{Schatten} $p$-norms of the
population covariance matrix. We then leverage this accurate moment recovery,
via an earthmover argument, to show that the vector of eigenvalues can be
accurately recovered. In addition to our theoretical results, we show that our
approach performs well in practice for a broad range of distributions and
sample sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00066</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00066</id><created>2016-01-29</created><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Xiao</keyname><forenames>Zhiping</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Shi</keyname><forenames>Shuyu</forenames></author><author><keyname>Li</keyname><forenames>Rui</forenames></author><author><keyname>Ji</keyname><forenames>Yusheng</forenames></author></authors><title>Skolem Sequence Based Self-adaptive Broadcast Protocol in Cognitive
  Radio Networks</title><categories>cs.NI</categories><comments>A full version with technical proofs. Accepted by IEEE VTC 2016
  Spring</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The base station (BS) in a multi-channel cognitive radio (CR) network has to
broadcast to secondary (or unlicensed) receivers/users on more than one
broadcast channels via channel hopping (CH), because a single broadcast channel
can be reclaimed by the primary (or licensed) user, leading to broadcast
failures. Meanwhile, a secondary receiver needs to synchronize its clock with
the BS's clock to avoid broadcast failures caused by the possible clock drift
between the CH sequences of the secondary receiver and the BS. In this paper,
we propose a CH-based broadcast protocol called SASS, which enables a BS to
successfully broadcast to secondary receivers over multiple broadcast channels
via channel hopping. Specifically, the CH sequences are constructed on basis of
a mathematical construct---the Self-Adaptive Skolem sequence. Moreover, each
secondary receiver under SASS is able to adaptively synchronize its clock with
that of the BS without any information exchanges, regardless of any amount of
clock drift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00067</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00067</id><created>2016-01-29</created><authors><author><keyname>Dayaratne</keyname><forenames>T. T.</forenames></author></authors><title>A Framework to Prevent QR Code Based Phishing Attacks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though the rapid development and spread of Information and Communication
Technology (ICT) making people's life much more easier, on the other hand it
causing some serious threats to the society. Phishing is one of the most common
cyber threat, that most users falls in. This research investigate on QR code
based phishing attacks which is a newly adopted intrusive method and how to
enhance the awareness and avoidance behavior of QR based phishing attacks
through the user centric security education approaches using game based
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00069</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00069</id><created>2016-01-29</created><authors><author><keyname>Zong</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Ji-Feng</forenames></author></authors><title>Consensus conditions of continuous-time multi-agent systems with
  time-delays and measurement noises</title><categories>cs.SY</categories><comments>32 pages, 6 figures. This paper was submitted to Automatica on July
  12, 2015</comments><msc-class>93E03, 93E15, 60H10, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is concerned with stochastic consensus conditions of multi-agent
systems with both time-delays and measurement noises. For the case of additive
noises, we develop some necessary conditions and sufficient conditions for the
stochastic weak consensus by estimating the differential resolvent function for
functional equations. By the martingale convergence theorem, we obtain the
necessary conditions and the sufficient conditions for the stochastic strong
consensus and prove that the small time-delay does not affect the agents to
reach the stochastic strong consensus. For the case of multiplicative noises,
we consider two kinds of time-delays, appeared in the measurement term and the
noise term, respectively. A fundamental theorem is developed to show that the
stochastic weak consensus with an exponential pairwise convergence rate implies
the stochastic strong consensus. By constructing Lyapunov functional, we show
that for any bounded time-delay in the noise term, the stochastic consensus can
be achieved by carefully choosing the control gain according to the time-delay
in the measurement term.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00070</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00070</id><created>2016-01-29</created><authors><author><keyname>Zhang</keyname><forenames>Jian-Xiong</forenames></author><author><keyname>Chen</keyname><forenames>Duan-Bing</forenames></author><author><keyname>Dong</keyname><forenames>Qiang</forenames></author><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author></authors><title>Identifying a set of influential spreaders in complex networks</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 6 Figures, 37 references</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Identifying a set of influential spreaders in complex networks plays a
crucial role in effective information spreading. A simple strategy is to choose
top-$r$ ranked nodes as spreaders according to influence ranking method such as
PageRank, ClusterRank and $k$-shell decomposition. Besides, some heuristic
methods such as hill-climbing, SPIN, degree discount and independent set based
are also proposed. However, these approaches suffer from a possibility that
some spreaders are so close together that they overlap sphere of influence or
time consuming. In this report, we present a simply yet effectively iterative
method named VoteRank to identify a set of decentralized spreaders with the
best spreading ability. In this approach, all nodes vote in a spreader in each
turn, and the voting ability of neighbors of elected spreader will be decreased
in subsequent turn. Experimental results on four real networks show that under
Susceptible-Infected-Recovered (SIR) model, VoteRank outperforms the
traditional benchmark methods on both spreading speed and final affected scale.
What's more, VoteRank is also superior to other group-spreader identifying
methods on computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00078</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00078</id><created>2016-01-30</created><authors><author><keyname>Talmon</keyname><forenames>Ronen</forenames></author><author><keyname>Wu</keyname><forenames>Hau-tieng</forenames></author></authors><title>Latent common manifold learning with alternating diffusion: analysis and
  applications</title><categories>physics.data-an cs.DS math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of data sets arising from multiple sensors has drawn significant
research attention over the years. Traditional methods, including kernel-based
methods, are typically incapable of capturing nonlinear geometric structures.
We introduce a latent common manifold model underlying multiple sensor
observations for the purpose of multimodal data fusion. A method based on
alternating diffusion is presented and analyzed; we provide theoretical
analysis of the method under the latent common manifold model. To exemplify the
power of the proposed framework, experimental results in several applications
are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00079</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00079</id><created>2016-01-30</created><authors><author><keyname>Sundar</keyname><forenames>Kaarthik</forenames></author><author><keyname>Nagarajan</keyname><forenames>Harsha</forenames></author><author><keyname>Lubin</keyname><forenames>Miles</forenames></author><author><keyname>Roald</keyname><forenames>Line</forenames></author><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Bienstock</keyname><forenames>Daniel</forenames></author></authors><title>Unit Commitment with N-1 Security and Wind Uncertainty</title><categories>cs.SY math.OC</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As renewable wind energy penetration rates continue to increase, one of the
major challenges facing grid operators is the question of how to control
transmission grids in a reliable and a cost-efficient manner. The stochastic
nature of wind forces an alteration of traditional methods for solving
day-ahead and look-ahead unit commitment and dispatch. In particular,
uncontrollable wind generation increases the risk of random component failures.
To address these questions, we present an N-1 Security and Chance-Constrained
Unit Commitment (SCCUC) that includes the modeling of generation reserves that
respond to wind fluctuations and tertiary reserves to account for single
component outages. The basic formulation is reformulated as a mixed-integer
second-order cone problem to limit the probability of failure. We develop three
different algorithms to solve the problem to optimality and present a detailed
case study on the IEEE RTS-96 single area system. The case study assesses the
economic impacts due to contingencies and various degrees of wind power
penetration into the system and also corroborates the effectiveness of the
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00093</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00093</id><created>2016-01-30</created><updated>2016-02-19</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author></authors><title>Network nestedness as generalized core-periphery structures</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>7 pages, 8 figures</comments><journal-ref>Phys. Rev. E 93, 022306 (2016)</journal-ref><doi>10.1103/PhysRevE.93.022306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of nestedness, in particular for ecological and economical
networks, has been introduced as a structural characteristic of real
interacting systems. We suggest that the nestedness is in fact another way to
express a mesoscale network property called the core-periphery structure. With
real ecological mutualistic networks and synthetic model networks, we reveal
the strong correlation between the nestedness and core-periphery-ness (likeness
to the core-periphery structure), by defining the network-level measures for
nestedness and core-periphery-ness in the case of weighted and bipartite
networks. However, at the same time, via more sophisticated null-model
analysis, we also discover that the degree (the number of connected neighbors
of a node) distribution poses quite severe restrictions on the possible
nestedness and core-periphery parameter space. Therefore, there must exist
structurally interwoven properties in more fundamental levels of network
formation, behind this seemingly obvious relation between nestedness and
core-periphery structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00095</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00095</id><created>2016-01-30</created><authors><author><keyname>Lu</keyname><forenames>Yi</forenames></author></authors><title>Walsh Sampling with Incomplete Noisy Signals</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1502.06221</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of massive data outputs at a regular rate, admittedly, signal
processing technology plays an increasingly key role. Nowadays, signals are not
merely restricted to physical sources, they have been extended to digital
sources as well. Under the general assumption of discrete statistical signal
sources, we propose a practical problem of sampling incomplete noisy signals
for which we do not know a priori and the sample size is bounded. We approach
this sampling problem by Shannon's channel coding theorem. Our main results
demonstrate that it is the large Walsh coefficient(s) that characterize(s)
discrete statistical signals, regardless of the signal sources. By the
connection of Shannon's theorem, we establish the necessary and sufficient
condition for our generic sampling problem for the first time. Our generic
sampling results find practical and powerful applications in not only
cryptanalysis, but software system performance optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00097</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00097</id><created>2016-01-30</created><authors><author><keyname>Han</keyname><forenames>Zhenhua</forenames></author><author><keyname>Tan</keyname><forenames>Haisheng</forenames></author><author><keyname>Chen</keyname><forenames>Guihai</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Chen</keyname><forenames>Yifan</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>Dynamic Virtual Machine Management via Approximate Markov Decision
  Process</title><categories>cs.NI</categories><comments>Full version for the paper appeared in INFOCOM'16 with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient virtual machine (VM) management can dramatically reduce energy
consumption in data centers. Existing VM management algorithms fall into two
categories based on whether the VMs' resource demands are assumed to be static
or dynamic. The former category fails to maximize the resource utilization as
they cannot adapt to the dynamic nature of VMs' resource demands. Most
approaches in the latter category are heuristical and lack theoretical
performance guarantees. In this work, we formulate dynamic VM management as a
large-scale Markov Decision Process (MDP) problem and derive an optimal
solution. Our analysis of real-world data traces supports our choice of the
modeling approach. However, solving the large-scale MDP problem suffers from
the curse of dimensionality. Therefore, we further exploit the special
structure of the problem and propose an approximate MDP-based dynamic VM
management method, called MadVM. We prove the convergence of MadVM and analyze
the bound of its approximation error. Moreover, MadVM can be implemented in a
distributed system, which should suit the needs of real data centers. Extensive
simulations based on two real-world workload traces show that MadVM achieves
significant performance gains over two existing baseline approaches in power
consumption, resource shortage and the number of VM migrations. Specifically,
the more intensely the resource demands fluctuate, the more MadVM outperforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00101</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00101</id><created>2016-01-30</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Anupam</forenames></author><author><keyname>Baksi</keyname><forenames>Anubhab</forenames></author></authors><title>Reversible Logic Circuit Complexity Analysis via Functional
  Decomposition</title><categories>cs.ET</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible computation is gaining increasing relevance in the context of
several post-CMOS technologies, the most prominent of those being Quantum
computing. One of the key theoretical problem pertaining to reversible logic
synthesis is the upper bound of the gate count. Compared to the known bounds,
the results obtained by optimal synthesis methods are significantly less. In
this paper, we connect this problem with the multiplicative complexity analysis
of classical Boolean functions. We explore the possibility of relaxing the
ancilla and if that approach makes the upper bound tighter. Our results are
negative. The ancilla-free synthesis methods by using transformations and by
starting from an Exclusive Sum-of-Product (ESOP) formulation remain,
theoretically, the synthesis methods for achieving least gate count for the
cases where the number of variables $n$ is $&lt; 8$ and otherwise, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00104</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00104</id><created>2016-01-30</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author></authors><title>Extracting Keyword for Disambiguating Name Based on the Overlap
  Principle</title><categories>cs.IR cs.CL</categories><comments>7 pages, Proceeding of International Conference on Information
  Technology and Engineering Application (4-th ICIBA), Book 1, 119-125,
  February 20-21, 2015. arXiv admin note: text overlap with arXiv:1212.3023</comments><acm-class>F.2.2; I.2.7</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Name disambiguation has become one of the main themes in the Semantic Web
agenda. The semantic web is an extension of the current Web in which
information is not only given well-defined meaning, but also has many purposes
that contain the ambiguous naturally or a lot of thing came with the overlap,
mainly deals with the persons name. Therefore, we develop an approach to
extract keywords from web snippet with utilizing the overlap principle, a
concept to understand things with ambiguous, whereby features of person are
generated for dealing with the variety of web, the web is steadily gaining
ground in the semantic research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00110</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00110</id><created>2016-01-30</created><authors><author><keyname>Cresci</keyname><forenames>Stefano</forenames></author><author><keyname>Di Pietro</keyname><forenames>Roberto</forenames></author><author><keyname>Petrocchi</keyname><forenames>Marinella</forenames></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames></author><author><keyname>Tesconi</keyname><forenames>Maurizio</forenames></author></authors><title>DNA-inspired online behavioral modeling and its application to spambot
  detection</title><categories>cs.SI cs.CR cs.LG</categories><acm-class>H.2.8.d; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a strikingly novel, simple, and effective approach to model online
user behavior: we extract and analyze digital DNA sequences from user online
actions and we use Twitter as a benchmark to test our proposal. We obtain an
incisive and compact DNA-inspired characterization of user actions. Then, we
apply standard DNA analysis techniques to discriminate between genuine and
spambot accounts on Twitter. An experimental campaign supports our proposal,
showing its effectiveness and viability. To the best of our knowledge, we are
the first ones to identify and adapt DNA-inspired techniques to online user
behavioral modeling. While Twitter spambot detection is a specific use case on
a specific social media, our proposed methodology is platform and technology
agnostic, hence paving the way for diverse behavioral characterization tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00132</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00132</id><created>2016-01-30</created><authors><author><keyname>Huo</keyname><forenames>Qiang</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Source and Physical-Layer Network Coding for Correlated Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>15 pages, 6 figures. IET Communications, 2016</comments><doi>10.1049/iet-com.2015.0572</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a half-duplex two-way relay channel (TWRC) with
correlated sources exchanging bidirectional information. In the case, when both
sources have the knowledge of correlation statistics, a source compression with
physical-layer network coding (SCPNC) scheme is proposed to perform the
distributed compression at each source node. When only the relay has the
knowledge of correlation statistics, we propose a relay compression with
physical-layer network coding (RCPNC) scheme to compress the bidirectional
messages at the relay. The closed-form block error rate (BLER) expressions of
both schemes are derived and verified through simulations. It is shown that the
proposed schemes achieve considerable improvements in both error performance
and throughput compared with the conventional non-compression scheme in
correlated two-way relay networks (CTWRNs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00133</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00133</id><created>2016-01-30</created><updated>2016-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Shen-Yi</forenames></author><author><keyname>Xiang</keyname><forenames>Ru</forenames></author><author><keyname>Shi</keyname><forenames>Ying-Hao</forenames></author><author><keyname>Gao</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Wu-jun</forenames></author></authors><title>SCOPE: Scalable Composite Optimization for Learning on Spark</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning models, such as lo- gistic regression (LR) and support
vector ma- chine (SVM), can be formulated as compos- ite optimization problems.
Recently, many dis- tributed stochastic optimization (DSO) methods have been
proposed to solve the large-scale com- posite optimization problems, which have
shown better performance than traditional batch meth- ods. However, most of
these DSO methods might not be scalable enough. In this paper, we propose a
novel DSO method, called scalable composite optimization for learning (SCOPE),
and implement it on the fault-tolerant distributed platform Spark. SCOPE is
both computation- efficient and communication-efficient. Theoret- ical analysis
shows that SCOPE is convergent with linear convergence rate when the objective
function is strongly convex. Furthermore, em- pirical results on real datasets
show that SCOPE can outperform other state-of-the-art distributed learning
methods on Spark, including both batch learning methods and DSO methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00134</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00134</id><created>2016-01-30</created><updated>2016-02-01</updated><authors><author><keyname>Wei</keyname><forenames>Shih-En</forenames></author><author><keyname>Ramakrishna</keyname><forenames>Varun</forenames></author><author><keyname>Kanade</keyname><forenames>Takeo</forenames></author><author><keyname>Sheikh</keyname><forenames>Yaser</forenames></author></authors><title>Convolutional Pose Machines</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pose Machines provide a powerful modular framework for articulated pose
estimation. The sequential prediction framework allows for the learning of rich
implicit spatial models, but currently relies on manually designed features for
representing image and spatial context. In this work, we incorporate a
convolutional network architecture into the pose machine framework allowing the
learning of representations for both image and spatial context directly from
data. The contribution of this paper is a systematic approach to composing
convolutional networks with large receptive fields for pose estimation tasks.
Our approach addresses the characteristic difficulty of vanishing gradients
during training by providing a natural learning objective function that
enforces intermediate supervision, thereby replenishing backpropagated
gradients and conditioning the learning procedure. We demonstrate
state-of-the-art performance and outperform competing methods on standard
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00139</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00139</id><created>2016-01-30</created><authors><author><keyname>B</keyname><forenames>Srinivasulu</forenames></author><author><keyname>Bhaintwal</keyname><forenames>Maheshanand</forenames></author></authors><title>One generator quasi-cyclic codes over F2 + uF2 + vF2 + uvF2</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the structure of 1-generator quasi-cyclic codes over
the ring R = F2 + uF2 + vF2 + uvF2, with u2 = v2 = 0 and uv = vu. We determine
the minimal spanning sets for these codes. As a generalization of these codes,
we also investigate the structure of 1-generator generalized quasi-cyclic codes
over R and determine a BCH type bound for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00145</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00145</id><created>2016-01-30</created><authors><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Chalise</keyname><forenames>Batu K.</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author></authors><title>Throughput Analysis and Optimization of Wireless-Powered Multiple
  Antenna Full-Duplex Relay Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a full-duplex (FD) decode-and-forward system in which the
time-switching protocol is employed by the multi-antenna relay to receive
energy from the source and transmit information to the destination. The
instantaneous throughput is maximized by optimizing receive and transmit
beamformers at the relay and the time-split parameter. We study both optimum
and suboptimum schemes. The reformulated problem in the optimum scheme achieves
closed-form solutions in terms of transmit beamformer for some scenarios. In
other scenarios, the optimization problem is formulated as a semi-definite
relaxation problem and a rank-one optimum solution is always guaranteed. In the
suboptimum schemes, the beamformers are obtained using maximum ratio combining,
zero-forcing, and maximum ratio transmission. When beamformers have closed-form
solutions, the achievable instantaneous and delay-constrained throughput are
analytically characterized. Our results reveal that, beamforming increases both
the energy harvesting and loop interference suppression capabilities at the FD
relay. Moreover, simulation results demonstrate that the choice of the linear
processing scheme as well as the time-split plays a critical role in
determining the FD gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00148</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00148</id><created>2016-01-30</created><updated>2016-02-29</updated><authors><author><keyname>Katis</keyname><forenames>Andreas</forenames></author><author><keyname>Whalen</keyname><forenames>Michael W.</forenames></author><author><keyname>Gacek</keyname><forenames>Andrew</forenames></author></authors><title>Towards Synthesis from Assume-Guarantee Contracts involving Infinite
  Theories: A Preliminary Report</title><categories>cs.SE</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, we have introduced a contract-based real- izability
checking algorithm for assume-guarantee contracts involving infinite theories,
such as linear integer/real arith- metic and uninterpreted functions over
infinite domains. This algorithm can determine whether or not it is possible to
con- struct a realization (i.e. an implementation) of an assume- guarantee
contract. The algorithm is similar to k-induction model checking, but involves
the use of quantifiers to deter- mine implementability. While our work on
realizability is inherently useful for vir- tual integration in determining
whether it is possible for sup- pliers to build software that meets a contract,
it also provides the foundations to solving the more challenging problem of
component synthesis. In this paper, we provide an initial synthesis algorithm
for assume-guarantee contracts involv- ing infinite theories. To do so, we take
advantage of our realizability checking procedure and a skolemization solver
for forall-exists formulas, called AE-VAL. We show that it is possible to
immediately adapt our existing algorithm towards syn- thesis by using this
solver, using a demonstration example. We then discuss challenges towards
creating a more robust synthesis algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00162</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00162</id><created>2016-01-30</created><authors><author><keyname>Sontag</keyname><forenames>Eduardo D.</forenames></author></authors><title>A remark on incoherent feedforward circuits as change detectors and
  feedback controllers</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note analyzes incoherent feedforward loops in signal processing and
control. It studies the response properties of IFFL's to exponentially growing
inputs, both for a standard version of the IFFL and for a variation in which
the output variable has a positive self-feedback term. It also considers a
negative feedback configuration, using such a device as a controller. It
uncovers a somewhat surprising phenomenon in which stabilization is only
possible in disconnected regions of parameter space, as the controlled system's
growth rate is varied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00163</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00163</id><created>2016-01-30</created><authors><author><keyname>Zoghlami</keyname><forenames>Manel</forenames></author><author><keyname>Aridhi</keyname><forenames>Sabeur</forenames></author><author><keyname>Sghaier</keyname><forenames>Haitham</forenames></author><author><keyname>Maddouri</keyname><forenames>Mondher</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>A multiple instance learning approach for sequence data with across bag
  dependencies</title><categories>cs.LG</categories><comments>Submitted to Data Mining and Knowledge Discovery Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Multiple Instance Learning (MIL) problem for sequence data, the learning
data consist of a set of bags where each bag contains a set of
instances/sequences. In many real world applications such as bioinformatics,
web mining, and text mining, comparing a random couple of sequences makes no
sense. In fact, each instance of each bag may have structural and/or temporal
relation with other instances in other bags. Thus, the classification task
should take into account the relation between semantically related instances
across bags. In this paper, we present two novel MIL approaches for sequence
data classification: (1) ABClass and (2) ABSim. In ABClass, each sequence is
represented by one vector of attributes. For each sequence of the unknown bag,
a discriminative classifier is applied in order to compute a partial
classification result. Then, an aggregation method is applied to these partial
results in order to generate the final result. In ABSim, we use a similarity
measure between each sequence of the unknown bag and the corresponding
sequences in the learning bags. An unknown bag is labeled with the bag that
presents more similar sequences. We applied both approaches to the problem of
bacterial Ionizing Radiation Resistance (IRR) prediction. We evaluated and
discussed the proposed approaches on well known Ionizing Radiation Resistance
Bacteria (IRRB) and Ionizing Radiation Sensitive Bacteria (IRSB) represented by
primary structure of basal DNA repair proteins. The experimental results show
that both ABClass and ABSim approaches are efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00165</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00165</id><created>2016-01-30</created><authors><author><keyname>Yadav</keyname><forenames>Amulya</forenames></author><author><keyname>Chan</keyname><forenames>Hau</forenames></author><author><keyname>Jiang</keyname><forenames>Albert</forenames></author><author><keyname>Xu</keyname><forenames>Haifeng</forenames></author><author><keyname>Rice</keyname><forenames>Eric</forenames></author><author><keyname>Tambe</keyname><forenames>Milind</forenames></author></authors><title>Using Social Networks to Aid Homeless Shelters: Dynamic Influence
  Maximization under Uncertainty - An Extended Version</title><categories>cs.AI cs.CY cs.SI</categories><comments>This is an extended version of our AAMAS 2016 paper (with the same
  name) with full proofs of all our theorems included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents HEALER, a software agent that recommends sequential
intervention plans for use by homeless shelters, who organize these
interventions to raise awareness about HIV among homeless youth. HEALER's
sequential plans (built using knowledge of social networks of homeless youth)
choose intervention participants strategically to maximize influence spread,
while reasoning about uncertainties in the network. While previous work
presents influence maximizing techniques to choose intervention participants,
they do not address three real-world issues: (i) they completely fail to scale
up to real-world sizes; (ii) they do not handle deviations in execution of
intervention plans; (iii) constructing real-world social networks is an
expensive process. HEALER handles these issues via four major contributions:
(i) HEALER casts this influence maximization problem as a POMDP and solves it
using a novel planner which scales up to previously unsolvable real-world
sizes; (ii) HEALER allows shelter officials to modify its recommendations, and
updates its future plans in a deviation-tolerant manner; (iii) HEALER
constructs social networks of homeless youth at low cost, using a Facebook
application. Finally, (iv) we show hardness results for the problem that HEALER
solves. HEALER will be deployed in the real world in early Spring 2016 and is
currently undergoing testing at a homeless shelter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00169</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00169</id><created>2016-01-30</created><authors><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author></authors><title>A Linearithmic Time Algorithm for a Shortest Vector Problem in
  Compute-and-Forward Design</title><categories>cs.IT math.IT</categories><comments>It has been submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm with expected complexity of $\bigO(n\log n)$
arithmetic operations to solve a special shortest vector problem arising in
computer-and-forward design, where $n$ is the dimension of the channel vector.
This algorithm is more efficient than the best known algorithms with proved
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00172</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00172</id><created>2016-01-30</created><authors><author><keyname>Glauner</keyname><forenames>Patrick O.</forenames></author></authors><title>Deep Learning For Smile Recognition</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by recent successes of deep learning in computer vision, we propose
a novel application of deep convolutional neural networks to facial expression
recognition, in particular smile recognition. A smile recognition test accuracy
of 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action
(DISFA) database, significantly outperforming existing approaches based on
hand-crafted features with accuracies ranging from 65.55% to 79.67%. The
novelty of this approach includes a comprehensive model selection of the
architecture parameters, allowing to find an appropriate architecture for each
expression such as smile. This is feasible because all experiments were run on
a Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations
on a CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00173</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00173</id><created>2016-01-30</created><authors><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Land</keyname><forenames>Ingmar</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Wireless Caching: Technical Misconceptions and Business Barriers</title><categories>cs.IT cs.NI math.IT</categories><comments>a version of this paper has been submitted to IEEE Communications
  Magazine, Special Issue on Communications, Caching, and Computing for
  Content-Centric Mobile Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is a hot research topic and poised to develop into a key technology
for the upcoming 5G wireless networks. The successful implementation of caching
techniques however, crucially depends on joint research developments in
different scientific domains such as networking, information theory, machine
learning, and wireless communications. Moreover, there exist business barriers
related to the complex interactions between the involved stakeholders, the
users, the cellular operators, and the Internet content providers. In this
article we discuss several technical misconceptions with the aim to uncover
enabling research directions for caching in wireless systems. Ultimately we
make a speculative stakeholder analysis for wireless caching in 5G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00174</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00174</id><created>2016-01-30</created><authors><author><keyname>Gozalvez</keyname><forenames>J.</forenames></author><author><keyname>Coll-Perales</keyname><forenames>B.</forenames></author></authors><title>Experimental Evaluation of Multihop Cellular Networks using Mobile
  Relays</title><categories>cs.NI</categories><comments>15 pages, 7 figures</comments><journal-ref>IEEE Communications Magazine, vol. 51, no. 7, pp. 122-129, July
  2013</journal-ref><doi>10.1109/MCOM.2013.6553688</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless networks are expected to provide high bandwidth multimedia
services in extended areas with homogeneous Quality of Service (QoS) levels.
Conventional cellular architectures might not be able to satisfy these
requirements due to the effect of surrounding obstacles and the signal
attenuation with distance, in particular under Non Line of Sight (NLOS)
propagation conditions. Recent studies have investigated the potential of
Multi-hop Cellular Networks (MCNs) to overcome the traditional cellular
architecture limitations through the integration of cellular and ad-hoc
networking technologies. However, these studies are generally analytical or
simulation-based. On the other hand, this paper reports the first experimental
field tests that validate and quantify the benefits that MCNs using mobile
relays can provide over traditional cellular systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00177</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00177</id><created>2016-01-30</created><authors><author><keyname>Eppel</keyname><forenames>Sagi</forenames></author></authors><title>Tracing liquid level and material boundaries in transparent vessels
  using the graph cut computer vision approach</title><categories>cs.CV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Detection of boundaries of materials stored in transparent vessels is
essential for identifying properties such as liquid level and phase boundaries,
which are vital for controlling numerous processes in the industry and
chemistry laboratory. This work presents a computer vision method for
identifying the boundary of materials in transparent vessels using the
graph-cut algorithm. The method receives an image of a transparent vessel
containing a material and the contour of the vessel in the image. The boundary
of the material in the vessel is found by the graph cut method. In general the
method uses the vessel region of the image to create a graph, where pixels are
vertices, and the cost of an edge between two pixels is inversely correlated
with their intensity difference. The bottom 10% of the vessel region in the
image is assumed to correspond to the material phase and defined as the graph
and source. The top 10% of the pixels in the vessels are assumed to correspond
to the air phase and defined as the graph sink. The minimal cut that splits the
resulting graph between the source and sink (hence, material and air) is traced
using the max-flow/min-cut approach. This cut corresponds to the boundary of
the material in the image. The method gave high accuracy in boundary
recognition for a wide range of liquid, solid, granular and powder materials in
various glass vessels from everyday life and the chemistry laboratory, such as
bottles, jars, Glasses, Chromotography colums and separatory funnels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00180</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00180</id><created>2016-01-30</created><authors><author><keyname>Kim</keyname><forenames>Nicolas</forenames></author><author><keyname>Wilburne</keyname><forenames>Dane</forenames></author><author><keyname>Petrovi&#x107;</keyname><forenames>Sonja</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author></authors><title>On the Geometry and Extremal Properties of the Edge-Degeneracy Model</title><categories>math.ST cs.DM stat.TH</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The edge-degeneracy model is an exponential random graph model that uses the
graph degeneracy, a measure of the graph's connection density, and number of
edges in a graph as its sufficient statistics. We show this model is relatively
well-behaved by studying the statistical degeneracy of this model through the
geometry of the associated polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00192</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00192</id><created>2016-01-30</created><updated>2016-02-01</updated><authors><author><keyname>Qiu</keyname><forenames>Xian</forenames></author><author><keyname>Kern</keyname><forenames>Walter</forenames></author></authors><title>On the Factor Revealing LP Approach for Facility Location with Penalties</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the uncapacitated facility location problem with (linear) penalty
function and show that a modified JMS algorithm, combined with a randomized LP
rounding technique due to Byrka-Aardal[1], Li[14] and Li et al.[16] yields
1.488 approximation, improving the factor 1.5148 due to Li et al.[16]. This
closes the current gap between the classical facility location problem and this
penalized variant. Main ingredient is a straightforward adaptation of the JMS
algorithm to the penalty setting plus a consistent use of the upper bounding
technique for factor revealing LPs due to Fernandes et al.[7]. In contrast to
the bounds in [12], our factor revealing LP is monotone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00193</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00193</id><created>2016-01-30</created><authors><author><keyname>Li</keyname><forenames>Zhuqi</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Bai</keyname><forenames>Yichong</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Zhou</keyname><forenames>Pan</forenames></author></authors><title>On Diffusion-restricted Social Network: A Measurement Study of WeChat
  Moments</title><categories>cs.CY cs.SI</categories><comments>Accepted by IEEE International Conference on Communications (IEEE ICC
  2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WeChat is a mobile messaging application that has 549 million active users as
of Q1 2015, and &quot;WeChat Moments&quot; (WM) serves as its social-networking function
that allows users to post/share links of web pages). WM differs from the other
social networks as it imposes many restrictions on the information diffusion
process to mitigate the information overload. In this paper, we conduct a
measurement study on information diffusion in the WM network by crawling and
analyzing the spreading statistics of more than 160,000 pages that involve
approximately 40 million users. Specifically, we identify the relationship of
the number of posted pages and the number of views, the diffusion path lengths,
the similarity and distribution of users' locations as well as their
connections to the GDP of the user's province. For each individual WM page, we
measure its temporal characteristics (e.g., the life time, the popularity
within a time period); for each individual user, we evaluate how many of, or
how likely, one's friends will view his posted pages. Our results will help the
business to decide when and how to release the marketing pages over WM for
better publicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00198</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00198</id><created>2016-01-30</created><authors><author><keyname>Xiong</keyname><forenames>Chuyu</forenames></author></authors><title>Discussion on Mechanical Learning and Learning Machine</title><categories>cs.AI</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mechanical learning is a computing system that is based on a set of simple
and fixed rules, and can learn from incoming data. A learning machine is a
system that realizes mechanical learning. Importantly, we emphasis that it is
based on a set of simple and fixed rules, contrasting to often called machine
learning that is sophisticated software based on very complicated mathematical
theory, and often needs human intervene for software fine tune and manual
adjustments. Here, we discuss some basic facts and principles of such system,
and try to lay down a framework for further study. We propose 2 directions to
approach mechanical learning, just like Church-Turing pair: one is trying to
realize a learning machine, another is trying to well describe the mechanical
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00203</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00203</id><created>2016-01-31</created><authors><author><keyname>Tariyal</keyname><forenames>Snigdha</forenames></author><author><keyname>Majumdar</keyname><forenames>Angshul</forenames></author><author><keyname>Singh</keyname><forenames>Richa</forenames></author><author><keyname>Vatsa</keyname><forenames>Mayank</forenames></author></authors><title>Greedy Deep Dictionary Learning</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose a new deep learning tool called deep dictionary
learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at
a time. This requires solving a simple (shallow) dictionary learning problem,
the solution to this is well known. We apply the proposed technique on some
benchmark deep learning datasets. We compare our results with other deep
learning tools like stacked autoencoder and deep belief network; and state of
the art supervised dictionary learning tools like discriminative KSVD and label
consistent KSVD. Our method yields better results than all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00206</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00206</id><created>2016-01-31</created><authors><author><keyname>Xia</keyname><forenames>Zhaoqiang</forenames></author><author><keyname>Feng</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Peng</keyname><forenames>Jinye</forenames></author><author><keyname>Hadid</keyname><forenames>Abdenour</forenames></author></authors><title>Unsupervised Deep Hashing for Large-scale Visual Search</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning based hashing plays a pivotal role in large-scale visual search.
However, most existing hashing algorithms tend to learn shallow models that do
not seek representative binary codes. In this paper, we propose a novel hashing
approach based on unsupervised deep learning to hierarchically transform
features into hash codes. Within the heterogeneous deep hashing framework, the
autoencoder layers with specific constraints are considered to model the
nonlinear mapping between features and binary codes. Then, a Restricted
Boltzmann Machine (RBM) layer with constraints is utilized to reduce the
dimension in the hamming space. Extensive experiments on the problem of visual
search demonstrate the competitiveness of our proposed approach compared to
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00212</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00212</id><created>2016-01-31</created><updated>2016-02-08</updated><authors><author><keyname>Sulam</keyname><forenames>Jeremias</forenames></author><author><keyname>Ophir</keyname><forenames>Boaz</forenames></author><author><keyname>Zibulevsky</keyname><forenames>Michael</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Trainlets: Dictionary Learning in High Dimensions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representations has shown to be a very powerful model for real world
signals, and has enabled the development of applications with notable
performance. Combined with the ability to learn a dictionary from signal
examples, sparsity-inspired algorithms are often achieving state-of-the-art
results in a wide variety of tasks. Yet, these methods have traditionally been
restricted to small dimensions mainly due to the computational constraints that
the dictionary learning problem entails. In the context of image processing,
this implies handling small image patches. In this work we show how to
efficiently handle bigger dimensions and go beyond the small patches in
sparsity-based signal and image processing methods. We build our approach based
on a new cropped wavelet decomposition, which enables a multi-scale analysis
with virtually no border effects. We then employ this as the base dictionary
within a double sparsity model to enable the training of adaptive dictionaries.
To cope with the increase of training data, while at the same time improving
the training performance, we present an Online Sparse Dictionary Learning
(OSDL) algorithm to train this model effectively, enabling it to handle
millions of examples. This work shows that dictionary learning can be up-scaled
to tackle a new level of signal dimensions, obtaining large adaptable atoms
that we call trainlets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00214</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00214</id><created>2016-01-31</created><authors><author><keyname>Laparra</keyname><forenames>Valero</forenames></author><author><keyname>Malo</keyname><forenames>Jesus</forenames></author><author><keyname>Camps-Valls</keyname><forenames>Gustau</forenames></author></authors><title>Dimensionality Reduction via Regression in Hyperspectral Imagery</title><categories>stat.ML cs.CV</categories><comments>12 pages, 6 figures, 62 references</comments><journal-ref>J. Sel. Topics Signal Processing 9(6): 1026-1036 (2015)</journal-ref><doi>10.1109/JSTSP.2015.2417833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new unsupervised method for dimensionality reduction
via regression (DRR). The algorithm belongs to the family of invertible
transforms that generalize Principal Component Analysis (PCA) by using
curvilinear instead of linear features. DRR identifies the nonlinear features
through multivariate regression to ensure the reduction in redundancy between
he PCA coefficients, the reduction of the variance of the scores, and the
reduction in the reconstruction error. More importantly, unlike other nonlinear
dimensionality reduction methods, the invertibility, volume-preservation, and
straightforward out-of-sample extension, makes DRR interpretable and easy to
apply. The properties of DRR enable learning a more broader class of data
manifolds than the recently proposed Non-linear Principal Components Analysis
(NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performance
of the representation in reducing the dimensionality of remote sensing data. In
particular, we tackle two common problems: processing very high dimensional
spectral information such as in hyperspectral image sounding data, and dealing
with spatial-spectral image patches of multispectral images. Both settings pose
collinearity and ill-determination problems. Evaluation of the expressive power
of the features is assessed in terms of truncation error, estimating
atmospheric variables, and surface land cover classification error. Results
show that DRR outperforms linear PCA and recently proposed invertible
extensions based on neural networks (NLPCA) and univariate regressions (PPA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00216</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00216</id><created>2016-01-31</created><updated>2016-03-07</updated><authors><author><keyname>Golay</keyname><forenames>Jean</forenames></author><author><keyname>Leuenberger</keyname><forenames>Michael</forenames></author><author><keyname>Kanevski</keyname><forenames>Mikhail</forenames></author></authors><title>Feature Selection for Regression Problems Based on the Morisita
  Estimator of Intrinsic Dimension</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data acquisition, storage and management have been improved, while the key
factors of many phenomena are not well known. Consequently, irrelevant and
redundant features artificially increase the size of datasets, which
complicates learning tasks, such as regression. To address this problem,
feature selection methods have been proposed. This paper introduces a new
supervised filter based on the Morisita estimator of intrinsic dimension. It
can identify relevant features and distinguish between redundant, irrelevant
and missing information. Besides, it offers a clear graphical representation of
the results and it can be easily implemented in different programming
languages. Comprehensive numerical experiments are conducted using simulated
datasets characterized by different levels of complexity, sample sizes and
noise. The suggested algorithm is also successfully tested on a selection of
real world applications and compared with RReliefF using extreme learning
machine. In addition, a new measure of relevance is presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00217</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00217</id><created>2016-01-31</created><authors><author><keyname>Laparra</keyname><forenames>Valero</forenames></author><author><keyname>Guti&#xe9;rrez</keyname><forenames>Juan</forenames></author><author><keyname>Camps-Valls</keyname><forenames>Gustavo</forenames></author><author><keyname>Malo</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>Image Denoising with Kernels based on Natural Image Relations</title><categories>cs.CV stat.ML</categories><journal-ref>Journal of Machine Learning Research 11: 873-903 (2010)</journal-ref><doi>10.1145/1756006.1756035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A successful class of image denoising methods is based on Bayesian approaches
working in wavelet representations. However, analytical estimates can be
obtained only for particular combinations of analytical models of signal and
noise, thus precluding its straightforward extension to deal with other
arbitrary noise sources. In this paper, we propose an alternative non-explicit
way to take into account the relations among natural image wavelet coefficients
for denoising: we use support vector regression (SVR) in the wavelet domain to
enforce these relations in the estimated signal. Since relations among the
coefficients are specific to the signal, the regularization property of SVR is
exploited to remove the noise, which does not share this feature. The specific
signal relations are encoded in an anisotropic kernel obtained from mutual
information measures computed on a representative image database. Training
considers minimizing the Kullback-Leibler divergence (KLD) between the
estimated and actual probability functions of signal and noise in order to
enforce similarity. Due to its non-parametric nature, the method can eventually
cope with different noise sources without the need of an explicit
re-formulation, as it is strictly necessary under parametric Bayesian
formalisms. Results under several noise levels and noise sources show that: (1)
the proposed method outperforms conventional wavelet methods that assume
coefficient independence, (2) it is similar to state-of-the-art methods that do
explicitly include these relations when the noise source is Gaussian, and (3)
it gives better numerical and visual performance when more complex, realistic
noise sources are considered. Therefore, the proposed machine learning approach
can be seen as a more flexible (model-free) alternative to the explicit
description of wavelet coefficient relations for image denoising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00223</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00223</id><created>2016-01-31</created><authors><author><keyname>Luo</keyname><forenames>Luo</forenames></author><author><keyname>Chen</keyname><forenames>Zihao</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author></authors><title>Variance-Reduced Second-Order Methods</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the problem of minimizing the sum of two convex
functions: a smooth function plus a non-smooth function. Further, the smooth
part can be expressed by the average of a large number of smooth component
functions, and the non-smooth part is equipped with a simple proximal mapping.
We propose a proximal stochastic second-order method, which is efficient and
scalable. It incorporates the Hessian in the smooth part of the function and
exploits multistage scheme to reduce the variance of the stochastic gradient.
We prove that our method can achieve linear rate of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00224</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00224</id><created>2016-01-31</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shen</keyname><forenames>Heng Tao</forenames></author></authors><title>Order-aware Convolutional Pooling for Video Based Action Recognition</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most video based action recognition approaches create the video-level
representation by temporally pooling the features extracted at each frame. The
pooling methods that they adopt, however, usually completely or partially
neglect the dynamic information contained in the temporal domain, which may
undermine the discriminative power of the resulting video representation since
the video sequence order could unveil the evolution of a specific event or
action. To overcome this drawback and explore the importance of incorporating
the temporal order information, in this paper we propose a novel temporal
pooling approach to aggregate the frame-level features. Inspired by the
capacity of Convolutional Neural Networks (CNN) in making use of the internal
structure of images for information abstraction, we propose to apply the
temporal convolution operation to the frame-level representations to extract
the dynamic information. However, directly implementing this idea on the
original high-dimensional feature would inevitably result in parameter
explosion.
  To tackle this problem, we view the temporal evolution of the feature value
at each feature dimension as a 1D signal and learn a unique convolutional
filter bank for each of these 1D signals. We conduct experiments on two
challenging video-based action recognition datasets, HMDB51 and UCF101; and
demonstrate that the proposed method is superior to the conventional pooling
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00225</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00225</id><created>2016-01-31</created><authors><author><keyname>Vishwakarma</keyname><forenames>Sanjay</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Transmitter Optimization in Slow Fading MISO Wiretap Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the transmitter optimization problem in slow
fading multiple-input-single-output (MISO) wiretap channel. The source
transmits a secret message intended for $K$ users in the presence of $J$
non-colluding eavesdroppers, and operates under a total power constraint. The
channels between the source and all users and eavesdroppers are assumed to be
slow fading, and only statistical channel state information (CSI) is known at
the source. For a given code rate and secrecy rate pair of the wiretap code,
denoted by $(R_{D}, R_{s})$, we define the non-outage event as the joint event
of the link information rates to $K$ users be greater than or equal to $R_{D}$
and the link information rates to $J$ eavesdroppers be less than or equal to
$(R_{D} - R_{s})$. We minimize the transmit power subject to the total power
constraint and satisfying the probability of the non-outage event to be greater
than or equal to a desired threshold $(1-\epsilon)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00233</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00233</id><created>2016-01-31</created><authors><author><keyname>Cheng</keyname><forenames>Hao-Chung</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>Characterisations of Matrix and Operator-Valued $\Phi$-Entropies, and
  Operator Efron-Stein Inequalities</title><categories>math-ph cs.IT math.IT math.MP math.PR quant-ph</categories><comments>21 pages. Text partially overlaps with arXiv:1506.06801. Accepted in
  Proceedings of the Royal Society A: Mathematical, Physical &amp; Engineering
  Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive new characterisations of the matrix $\mathrm{\Phi}$-entropy
functionals introduced in [Electron.~J.~Probab., 19(20): 1--30, 2014]. Notably,
all known equivalent characterisations of the classical $\Phi$-entropies have
their matrix correspondences. Next, we propose an operator-valued
generalisation of the matrix $\Phi$-entropy functionals, and prove their
subadditivity under L\&quot;owner partial ordering. Our results demonstrate that the
subadditivity of operator-valued $\Phi$-entropies is equivalent to the
convexity of various related functions. This result can be used to demonstrate
an interesting result in quantum information theory: the matrix $\Phi$-entropy
of a quantum ensemble is monotone under unital quantum channels. Finally, we
derive the operator Efron-Stein inequality to bound the operator-valued
variance of a random matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00234</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00234</id><created>2016-01-31</created><authors><author><keyname>Hedges</keyname><forenames>Mark</forenames></author><author><keyname>Jordanous</keyname><forenames>Anna</forenames></author><author><keyname>Lawrence</keyname><forenames>K. Faith</forenames></author><author><keyname>Rouech&#xe9;</keyname><forenames>Charlotte</forenames></author><author><keyname>Tupman</keyname><forenames>Charlotte</forenames></author></authors><title>Computer-Assisted Processing of Intertextuality in Ancient Languages</title><categories>cs.DL</categories><comments>19 pages, 2 figures, submission to overlay journal Journal of Data
  Mining and Digital Humanities (jdmdh.episciences.org)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The production of digital critical editions of texts using TEI is now a
widely-adopted procedure within digital humanities. The work described in this
paper extends this approach to the publication of gnomologia (anthologies of
wise sayings), which formed a widespread literary genre in many cultures of the
medieval Mediterranean. These texts are challenging because they were rarely
copied straightforwardly; rather, sayings were selected, reorganised, modified
or re-attributed between manuscripts, resulting in a highly interconnected
corpus for which a standard approach to digital publication is insufficient.
Focusing on Greek and Arabic collections, we address this challenge using
semantic web techniques to create an ecosystem of texts, relationships and
annotations, and consider a new model - organic, collaborative, interconnected,
and open-ended - of what constitutes an edition. This semantic web-based
approach allows scholars to add their own materials and annotations to the
network of information and to explore the conceptual networks that arise from
these interconnected sayings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00238</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00238</id><created>2016-01-31</created><authors><author><keyname>Thorn</keyname><forenames>Jacob</forenames></author><author><keyname>Pizarro</keyname><forenames>Rodrigo</forenames></author><author><keyname>Spanlang</keyname><forenames>Bernhard</forenames></author><author><keyname>Bermell-Garcia</keyname><forenames>Pablo</forenames></author><author><keyname>Gonzalez-Franco</keyname><forenames>Mar</forenames></author></authors><title>Assessing 3D scan quality through paired-comparisons psychophysics test</title><categories>cs.HC cs.MM</categories><comments>9 pages, 10 figures, video: https://youtu.be/vC3Tx07szXU</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consumer 3D scanners and depth cameras are increasingly being used to
generate content and avatars for Virtual Reality (VR) environments and avoid
the inconveniences of hand modeling; however, it is sometimes difficult to
evaluate quantitatively the mesh quality at which 3D scans should be exported,
and whether the object perception might be affected by its shading. We propose
using a paired-comparisons test based on psychophysics of perception to do that
evaluation. As psychophysics is not subject to opinion, skill level, mental
state, or economic situation it can be considered a quantitative way to measure
how people perceive the mesh quality. In particular, we propose using the
psychophysical measure for the comparison of four different levels of mesh
quality (1K, 5K, 10K and 20K triangles). We present two studies within
subjects: in one we investigate the quality perception variations of seeing an
object in a regular screen monitor against an stereoscopic Head Mounted Display
(HMD); while in the second experiment we aim at detecting the effects of
shading into quality perception. At each iteration of the pair-test comparisons
participants pick the mesh that they think had higher quality; by the end of
the experiment we compile a preference matrix. The matrix evidences the
correlation between real quality and assessed quality. Regarding the shading
mode, we find an interaction with quality and shading when the model has high
definition. Furthermore, we assess the subjective realism of the most/least
preferred scans using an Immersive Augmented Reality (IAR) video-see-through
setup. Results show higher levels of realism were perceived through the HMD
than when using a monitor, although the quality was similarly perceived in both
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00242</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00242</id><created>2016-01-31</created><authors><author><keyname>Chen</keyname><forenames>Miao</forenames></author></authors><title>Technical Report: Representing SES Cases Using Ontology</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Socio-ecological System (SES) research studies the interaction between
environment, users, and governance of environment resources. Data produced
during the research cycle can be both long-tail (e.g. heterogeneous) and
longitudinal data. For example, the IFRI (International Forestry Resources and
Institutions) data set contains studies carried out over a period of 20 years.
Given the complexity of a SES system, case studies that are accumulated over
time and from different sites (e.g. site visit cases) are highly valuable in
the understanding of new SES system behavior for instance. We, as a group of
informatics researchers collaborating with personnel from the Workshop in
Political Theory and Policy Analysis at Indiana University, are developing
informatics approaches to facilitating SES scholars' research.
  Here we focus on presenting our work on representing SES cases using
ontology. An ontology for the SES field can help organize concepts in the
field, describe resources such as data and publications using a shared
vocabulary, and also facilitate data use and query for researchers. We develop
a core SES ontology, which contains core concepts and resources in the field
and can be used to describe actual concept and resource instances, and also a
tool for contributing instances by drawing graphs, called Cmap2SES.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00243</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00243</id><created>2016-01-31</created><authors><author><keyname>Danilov</keyname><forenames>Vladimir G.</forenames></author><author><keyname>Turuntaev</keyname><forenames>Ilya S.</forenames></author></authors><title>Reliability of Checking an Answer Given by a Mathematical Expression in
  Interactive Learning Systems</title><categories>cs.OH</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we address the problem of automatic answer checking in
interactive learning systems that support mathematical notation. This problem
consists of the problem of establishing identities in formal mathematical
systems and hence is formally unsolvable. However, there is a way to cope with
the issue. We suggest to reinforce the standard algorithm for function
comparison with an additional pointwise checking procedure. An error might
appear in this case. The article provides a detailed analysis of the
probability of this error. It appears that the error probability is extremely
low in most common cases. Generally speaking, this means that such an
additional checking procedure can be quite successfully used in order to
support standard algorithms for functions comparison. The results, obtained in
this article, help avoiding some sudden effects of the identity problem, and
provide a way to estimate the reliability of answer checking procedure in
interactive learning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00244</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00244</id><created>2016-01-31</created><authors><author><keyname>Lairez</keyname><forenames>Pierre</forenames></author><author><keyname>Vaccon</keyname><forenames>Tristan</forenames></author></authors><title>On p-adic differential equations with separation of variables</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several algorithms in computer algebra involve the computation of a power
series solution of a given ordinary differential equation. Over finite fields,
the problem is often lifted in an approximate $p$-adic setting to be
well-posed. This raises precision concerns: how much precision do we need on
the input to compute the output accurately? In the case of ordinary
differential equation with separation of variables, we apply the recent
technique of differential precision to obtain optimal bounds on the stability
of the Newton iteration. It applies, for example, to algorithms for
manipulating algebraic numbers over finite fields, for computing isogenies
between elliptic curves or for deterministically finding roots of polynomials
in finite fields. The new bounds lead to significant speedups in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00248</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00248</id><created>2016-01-31</created><authors><author><keyname>Kucharski</keyname><forenames>Adam J.</forenames></author></authors><title>Modelling the transmission dynamics of online social contagion</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 6 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  During 2014-15, there were several outbreaks of nominated-based online social
contagion. These infections, which were transmitted from one individual to
another via posts on social media, included games such as 'neknomination', 'ice
bucket challenge', 'no make up selfies', and Facebook users re-posting their
first profile pictures. Fitting a mathematical model of infectious disease
transmission to outbreaks of these four games in the United Kingdom, I
estimated the basic reproduction number, $R_0$, and generation time of each
infection. Median estimates for $R_0$ ranged from 1.9-2.5 across the four
outbreaks, and the estimated generation times were between 1.0 and 2.0 days.
Tests using out-of-sample data from Australia suggested that the model had
reasonable predictive power, with $R^2$ values between 0.52-0.70 across the
four Australian datasets. Further, the relatively low basic reproduction
numbers for the infections suggests that only 48-60% of index cases in
nomination-based games may subsequently generate major outbreaks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00251</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00251</id><created>2016-01-31</created><authors><author><keyname>Bakhtiyari</keyname><forenames>Kaveh</forenames></author></authors><title>Do we have privacy in digital world?</title><categories>cs.CR cs.HC cs.IR</categories><doi>10.13140/RG.2.1.2492.5203/1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not really.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00252</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00252</id><created>2016-01-31</created><authors><author><keyname>Stattner</keyname><forenames>Erick</forenames></author></authors><title>Comment Diffusons-nous sur les R\'eseaux Sociaux ?</title><categories>cs.SI</categories><comments>12 pages, in French. Colloque international et interdisciplinaire
  pour les enjeux et usages des technologies de l'information et de la
  communication, 2015 (EUTIC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of new communication media such as blogs, online newspapers and
social networks allow us to go further in the understanding of human behavior.
Indeed, these public exchange spaces are now firmly planted in our modern
society and appear to be powerful sensors of social behavior and opinion
movements. In this paper, we focus on information spreading and attempt to
understand what are the conditions in which a person decides to speak on a
subject. For this purpose, we propose a set of measures that aim to
characterize the diffusion behavior. Our measures have been used on messages
related to two events that took place in January 2015: presentation by
Microsoft of a new virtual reality headset and the election of a political
party of radical left in Greece.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00269</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00269</id><created>2016-01-31</created><authors><author><keyname>R</keyname><forenames>Sarath P</forenames></author><author><keyname>Mandhan</keyname><forenames>Sunil</forenames></author><author><keyname>Niwa</keyname><forenames>Yoshiki</forenames></author></authors><title>Numerical Atrribute Extraction from Clinical Texts</title><categories>cs.AI</categories><comments>6 Pages</comments><report-no>Submission 42, CLEF 2015</report-no><msc-class>68T50</msc-class><doi>10.13140/RG.2.1.4763.3365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes about information extraction system, which is an
extension of the system developed by team Hitachi for &quot;Disease/Disorder
Template filling&quot; task organized by ShARe/CLEF eHealth Evolution Lab 2014. In
this extension module we focus on extraction of numerical attributes and values
from discharge summary records and associating correct relation between
attributes and values. We solve the problem in two steps. First step is
extraction of numerical attributes and values, which is developed as a Named
Entity Recognition (NER) model using Stanford NLP libraries. Second step is
correctly associating the attributes to values, which is developed as a
relation extraction module in Apache cTAKES framework. We integrated Stanford
NER model as cTAKES pipeline component and used in relation extraction module.
Conditional Random Field (CRF) algorithm is used for NER and Support Vector
Machines (SVM) for relation extraction. For attribute value relation
extraction, we observe 95% accuracy using NER alone and combined accuracy of
87% with NER and SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00276</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00276</id><created>2016-01-31</created><authors><author><keyname>Chen</keyname><forenames>Zitan</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>The Capacity of Online (Causal) $q$-ary Error-Erasure Channels</title><categories>cs.IT math.IT</categories><comments>This is a new version of the binary case, which can be found at
  arXiv:1412.6376</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the $q$-ary online (or &quot;causal&quot;) channel coding model, a sender wishes to
communicate a message to a receiver by transmitting a codeword $\mathbf{x}
=(x_1,\ldots,x_n) \in \{0,1,\ldots,q-1\}^n$ symbol by symbol via a channel
limited to at most $pn$ errors and/or $p^{*} n$ erasures. The channel is
&quot;online&quot; in the sense that at the $i$th step of communication the channel
decides whether to corrupt the $i$th symbol or not based on its view so far,
i.e., its decision depends only on the transmitted symbols $(x_1,\ldots,x_i)$.
This is in contrast to the classical adversarial channel in which the
corruption is chosen by a channel that has a full knowledge on the sent
codeword $\mathbf{x}$.
  In this work we study the capacity of $q$-ary online channels for a combined
corruption model, in which the channel may impose at most $pn$ {\em errors} and
at most $p^{*} n$ {\em erasures} on the transmitted codeword. The online
channel (in both the error and erasure case) has seen a number of recent
studies which present both upper and lower bounds on its capacity. In this
work, we give a full characterization of the capacity as a function of $q,p$,
and $p^{*}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00287</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00287</id><created>2016-01-31</created><authors><author><keyname>Kandasamy</keyname><forenames>Kirthevasan</forenames></author><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author></authors><title>Additive Approximations in High Dimensional Nonparametric Regression via
  the SALSA</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High dimensional nonparametric regression is an inherently difficult problem
with known lower bounds depending exponentially in dimension. A popular
strategy to alleviate this curse of dimensionality has been to use additive
models of \emph{first order}, which model the regression function as a sum of
independent functions on each dimension. Though useful in controlling the
variance of the estimate, such models are often too restrictive in practical
settings. Between non-additive models which often have large variance and first
order additive models which have large bias, there has been little work to
exploit the trade-off in the middle via additive models of intermediate order.
In this work, we propose SALSA, which bridges this gap by allowing interactions
between variables, but controls model capacity by limiting the order of
interactions. SALSA minimises the residual sum of squares with squared RKHS
norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression
with an additive kernel. When the regression function is additive, the excess
risk is only polynomial in dimension. Using the Girard-Newton formulae, we
efficiently sum over a combinatorial number of terms in the additive expansion.
Via a comparison on $16$ real datasets, we show that our method is competitive
against $21$ other alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00293</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00293</id><created>2016-01-31</created><authors><author><keyname>Maity</keyname><forenames>Suman Kalyan</forenames></author><author><keyname>Sarda</keyname><forenames>Chaitanya</forenames></author><author><keyname>Chaudhary</keyname><forenames>Anshit</forenames></author><author><keyname>Patil</keyname><forenames>Abhijeet</forenames></author><author><keyname>Kumar</keyname><forenames>Shraman</forenames></author><author><keyname>Mondal</keyname><forenames>Akash</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>WASSUP? LOL : Characterizing Out-of-Vocabulary Words in Twitter</title><categories>cs.CL cs.SI</categories><comments>4 pages, 1 figure, CSCW '16</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Language in social media is mostly driven by new words and spellings that are
constantly entering the lexicon thereby polluting it and resulting in high
deviation from the formal written version. The primary entities of such
language are the out-of-vocabulary (OOV) words. In this paper, we study various
sociolinguistic properties of the OOV words and propose a classification model
to categorize them into at least six categories. We achieve 81.26% accuracy
with high precision and recall. We observe that the content features are the
most discriminative ones followed by lexical and context features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00296</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00296</id><created>2016-01-31</created><authors><author><keyname>Heinle</keyname><forenames>Albert</forenames></author><author><keyname>Levandovskyy</keyname><forenames>Viktor</forenames></author></authors><title>A Factorization Algorithm for G-Algebras and Applications</title><categories>math.RA cs.SC math.OA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been recently discovered by Bell, Heinle and Levandovskyy that a large
class of algebras, including the ubiquitous $G$-algebras, are finite
factorization domains (FFD for short).
  Utilizing this result, we contribute an algorithm to find all distinct
factorizations of a given element $f \in \mathcal{G}$, where $\mathcal{G}$ is
any $G$-algebra, with minor assumptions on the underlying field.
  Moreover, the property of being an FFD, in combination with the factorization
algorithm, enables us to propose an analogous description of the factorized
Gr\&quot;obner basis algorithm for $G$-algebras. This algorithm is useful for
various applications, e.g. in analysis of solution spaces of systems of linear
partial functional equations with polynomial coefficients, coming from
$\mathcal{G}$. Additionally, it is possible to include inequality constraints
for ideals in the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00307</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00307</id><created>2016-01-31</created><authors><author><keyname>Alismail</keyname><forenames>Hatem</forenames></author><author><keyname>Browning</keyname><forenames>Brett</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Bit-Planes: Dense Subpixel Alignment of Binary Descriptors</title><categories>cs.CV</categories><comments>10 pages. In submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary descriptors have been instrumental in the recent evolution of
computationally efficient sparse image alignment algorithms. Increasingly,
however, the vision community is interested in dense image alignment methods,
which are more suitable for estimating correspondences from high frame rate
cameras as they do not rely on exhaustive search. However, classic dense
alignment approaches are sensitive to illumination change. In this paper, we
propose an easy to implement and low complexity dense binary descriptor, which
we refer to as bit-planes, that can be seamlessly integrated within a
multi-channel Lucas &amp; Kanade framework. This novel approach combines the
robustness of binary descriptors with the speed and accuracy of dense alignment
methods. The approach is demonstrated on a template tracking problem achieving
state-of-the-art robustness and faster than real-time performance on consumer
laptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+
fps on an iPad Air 2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00309</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00309</id><created>2016-01-31</created><authors><author><keyname>Glassner</keyname><forenames>Yonatan</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Bandits meet Computer Architecture: Designing a Smartly-allocated Cache</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many embedded systems, such as imaging sys- tems, the system has a single
designated purpose, and same threads are executed repeatedly. Profiling thread
behavior, allows the system to allocate each thread its resources in a way that
improves overall system performance. We study an online resource al-
locationproblem,wherearesourcemanagersimulta- neously allocates resources
(exploration), learns the impact on the different consumers (learning) and im-
proves allocation towards optimal performance (ex- ploitation). We build on the
rich framework of multi- armed bandits and present online and offline algo-
rithms. Through extensive experiments with both synthetic data and real-world
cache allocation to threads we show the merits and properties of our al-
gorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00310</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00310</id><created>2016-01-31</created><authors><author><keyname>Vu</keyname><forenames>Tiep H.</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author></authors><title>Learning a low-rank shared dictionary for object classification</title><categories>cs.CV</categories><comments>4 page + 1 reference page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the fact that different objects possess distinct class-specific
features, they also usually share common patterns. Inspired by this
observation, we propose a novel method to explicitly and simultaneously learn a
set of common patterns as well as class-specific features for classification.
Our dictionary learning framework is hence characterized by both a shared
dictionary and particular (class-specific) dictionaries. For the shared
dictionary, we enforce a low-rank constraint, i.e. claim that its spanning
subspace should have low dimension and the coefficients corresponding to this
dictionary should be similar. For the particular dictionaries, we impose on
them the well-known constraints stated in the Fisher discrimination dictionary
learning (FDDL). Further, we propose a new fast and accurate algorithm to solve
the sparse coding problems in the learning step, accelerating its convergence.
The said algorithm could also be applied to FDDL and its extensions.
Experimental results on widely used image databases establish the advantages of
our method over state-of-the-art dictionary learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00328</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00328</id><created>2016-01-31</created><authors><author><keyname>Rematas</keyname><forenames>Konstantinos</forenames></author><author><keyname>Nguyen</keyname><forenames>Chuong</forenames></author><author><keyname>Ritschel</keyname><forenames>Tobias</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Novel Views of Objects from a Single Image</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Taking an image of an object is at its core a lossy process. The rich
information about the three-dimensional structure of the world is flattened to
an image plane and decisions such as viewpoint and camera parameters are final
and not easily revertible. As a consequence, possibilities of changing
viewpoint are limited. Given a single image depicting an object, novel-view
synthesis is the task of generating new images that render the object from a
different viewpoint than the one given. The main difficulty is to synthesize
the parts that are disoccluded; disocclusion occurs when parts of an object are
hidden by the object itself under a specific viewpoint. In this work, we show
how to improve novel-view synthesis by making use of the correlations observed
in 3D models and applying them to new image instances. We propose a technique
to use the structural information extracted from a 3D model that matches the
image object in terms of viewpoint and shape. For the latter part, we propose
an efficient 2D-to-3D alignment method that associates precisely the image
appearance with the 3D model geometry with minimal user interaction. Our
technique is able to simulate plausible viewpoint changes for a variety of
object classes within seconds. Additionally, we show that our synthesized
images can be used as additional training data that improves the performance of
standard object detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00329</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00329</id><created>2016-01-31</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Kempa</keyname><forenames>Dominik</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Lempel-Ziv Decoding in External Memory</title><categories>cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple and fast decoding is one of the main advantages of LZ77-type text
encoding used in many popular file compressors such as gzip and 7zip. With the
recent introduction of external memory algorithms for Lempel-Ziv factorization
there is a need for external memory LZ77 decoding but the standard algorithm
makes random accesses to the text and cannot be trivially modified for external
memory computation. We describe the first external memory algorithms for LZ77
decoding, prove that their I/O complexity is optimal, and demonstrate that they
are very fast in practice, only about three times slower than in-memory
decoding (when reading input and writing output is included in the time).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00339</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00339</id><created>2016-01-31</created><authors><author><keyname>Gu</keyname><forenames>Yifan</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>Distributed Multi-Relay Selection in Accumulate-then-Forward Energy
  Harvesting Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a wireless-powered cooperative network (WPCN)
consisting of one source-destination pair and multiple decode-and-forward
relays. Contrary to conventional cooperative networks, we consider the scenario
that all relays have no embedded energy supply, but they are equipped with
energy harvesting units and rechargeable batteries. As such, they can
accumulate sufficient energy harvested from source's signals before helping
forward its information to destination. Each relay will adaptively switch
between two basic modes, information forwarding mode and energy harvesting
mode. A natural yet challenging question for the considered system is &quot;how to
determine the operation mode for each relay and select the energy harvesting
relays to efficiently assist the source's information transmission?&quot;. Motivated
by this, we develop an energy threshold based multi-relay selection (ETMRS)
scheme for the considered WPCN. The proposed ETMRS scheme can be implemented in
a fully distributed manner as the relays only needs local information to switch
between energy harvesting and information forwarding modes. By modeling the
charging/discharging of the finite-capacity battery at each relay as a
finite-state Markov Chain, we derive closed-form expressions for the system
outage probability and packet error rate (PER) of the proposed ETMRS scheme
over mixed Nakagami-m and Rayleigh fading channels. To gain some useful
insights for practical relay design, we also derive the upper bounds for system
outage probability and PER corresponding to the case that all relays are
equipped with infinite-capacity batteries. Numerical results validate our
theoretical analysis and show that the proposed ETMRS scheme outperforms the
existing single-relay selection scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00345</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00345</id><created>2016-01-31</created><authors><author><keyname>Luckow</keyname><forenames>Andre</forenames></author><author><keyname>Paraskevakos</keyname><forenames>Ioannis</forenames></author><author><keyname>Chantzialexiou</keyname><forenames>George</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>Hadoop on HPC: Integrating Hadoop and Pilot-based Dynamic Resource
  Management</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-performance computing platforms such as supercomputers have
traditionally been designed to meet the compute demands of scientific
applications. Consequently, they have been architected as producers and not
consumers of data. The Apache Hadoop ecosystem has evolved to meet the
requirements of data processing applications and has addressed many of the
limitations of HPC platforms. There exist a class of scientific applications
however, that need the collective capabilities of traditional high-performance
computing environments and the Apache Hadoop ecosystem. For example, the
scientific domains of bio-molecular dynamics, genomics and network science need
to couple traditional computing with Hadoop/Spark based analysis. We
investigate the critical question of how to present the capabilities of both
computing environments to such scientific applications. Whereas this questions
needs answers at multiple levels, we focus on the design of resource management
middleware that might support the needs of both. We propose extensions to the
Pilot-Abstraction to provide a unifying resource management layer. This is an
important step that allows applications to integrate HPC stages (e.g.
simulations) to data analytics. Many supercomputing centers have started to
officially support Hadoop environments, either in a dedicated environment or in
hybrid deployments using tools such as myHadoop. This typically involves many
intrinsic, environment-specific details that need to be mastered, and often
swamp conceptual issues like: How best to couple HPC and Hadoop application
stages? How to explore runtime trade-offs (data localities vs. data movement)?
This paper provides both conceptual understanding and practical solutions to
the integrated use of HPC and Hadoop environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00349</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00349</id><created>2016-01-31</created><authors><author><keyname>Hor&#xe1;&#x10d;ek</keyname><forenames>Jaroslav</forenames></author><author><keyname>Hlad&#xed;k</keyname><forenames>Milan</forenames></author><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Michal</forenames></author></authors><title>Interval Linear Algebra and Computational Complexity</title><categories>cs.CC</categories><comments>Submitted to Mat Triad 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work connects two mathematical fields - computational complexity and
interval linear algebra. It introduces the basic topics of interval linear
algebra - regularity and singularity, full column rank, solving a linear
system, deciding solvability of a linear system, computing inverse matrix,
eigenvalues, checking positive (semi)definiteness or stability. We discuss
these problems and relations between them from the view of computational
complexity. Many problems in interval linear algebra are intractable, hence we
emphasize subclasses of these problems that are easily solvable or decidable.
The aim of this work is to provide a basic insight into this field and to
provide materials for further reading and research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00351</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00351</id><created>2016-01-31</created><authors><author><keyname>Ding</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Peilin</forenames></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames></author><author><keyname>Ong</keyname><forenames>Yew-Soon</forenames></author></authors><title>Adaptive Subgradient Methods for Online AUC Maximization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning for maximizing AUC performance is an important research problem in
Machine Learning and Artificial Intelligence. Unlike traditional batch learning
methods for maximizing AUC which often suffer from poor scalability, recent
years have witnessed some emerging studies that attempt to maximize AUC by
single-pass online learning approaches. Despite their encouraging results
reported, the existing online AUC maximization algorithms often adopt simple
online gradient descent approaches that fail to exploit the geometrical
knowledge of the data observed during the online learning process, and thus
could suffer from relatively larger regret. To address the above limitation, in
this work, we explore a novel algorithm of Adaptive Online AUC Maximization
(AdaOAM) which employs an adaptive gradient method that exploits the knowledge
of historical gradients to perform more informative online learning. The new
adaptive updating strategy of the AdaOAM is less sensitive to the parameter
settings and maintains the same time complexity as previous non-adaptive
counterparts. Additionally, we extend the algorithm to handle high-dimensional
sparse data (SAdaOAM) and address sparsity in the solution by performing lazy
gradient updating. We analyze the theoretical bounds and evaluate their
empirical performance on various types of data sets. The encouraging empirical
results obtained clearly highlighted the effectiveness and efficiency of the
proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00354</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00354</id><created>2016-01-31</created><authors><author><keyname>Dasarathy</keyname><forenames>Gautam</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Park</keyname><forenames>Jong Hyuk</forenames></author></authors><title>Active Learning Algorithms for Graphical Model Selection</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>26 pages, 3 figures. Preliminary version to appear in AI &amp; Statistics
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of learning the structure of a high dimensional graphical model
from data has received considerable attention in recent years. In many
applications such as sensor networks and proteomics it is often expensive to
obtain samples from all the variables involved simultaneously. For instance,
this might involve the synchronization of a large number of sensors or the
tagging of a large number of proteins. To address this important issue, we
initiate the study of a novel graphical model selection problem, where the goal
is to optimize the total number of scalar samples obtained by allowing the
collection of samples from only subsets of the variables. We propose a general
paradigm for graphical model selection where feedback is used to guide the
sampling to high degree vertices, while obtaining only few samples from the
ones with the low degrees. We instantiate this framework with two specific
active learning algorithms, one of which makes mild assumptions but is
computationally expensive, while the other is more computationally efficient
but requires stronger (nevertheless standard) assumptions. Whereas the sample
complexity of passive algorithms is typically a function of the maximum degree
of the graph, we show that the sample complexity of our algorithms is provable
smaller and that it depends on a novel local complexity measure that is akin to
the average degree of the graph. We finally demonstrate the efficacy of our
framework via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00357</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00357</id><created>2016-01-31</created><authors><author><keyname>Pham</keyname><forenames>Trang</forenames></author><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>DeepCare: A Deep Dynamic Memory Model for Predictive Medicine</title><categories>stat.ML cs.LG</categories><comments>Under review at TKDE journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalized predictive medicine necessitates the modeling of patient illness
and care processes, which inherently have long-term temporal dependencies.
Healthcare observations, recorded in electronic medical records, are episodic
and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural
network that reads medical records, stores previous illness history, infers
current illness states and predicts future medical outcomes. At the data level,
DeepCare represents care episodes as vectors in space, models patient health
state trajectories through explicit memory of historical records. Built on Long
Short-Term Memory (LSTM), DeepCare introduces time parameterizations to handle
irregular timed events by moderating the forgetting and consolidation of memory
cells. DeepCare also incorporates medical interventions that change the course
of illness and shape future medical risk. Moving up to the health state level,
historical and present health states are then aggregated through multiscale
temporal pooling, before passing through a neural network that estimates future
outcomes. We demonstrate the efficacy of DeepCare for disease progression
modeling, intervention recommendation, and future risk prediction. On two
important cohorts with heavy social and economic burden -- diabetes and mental
health -- the results show improved modeling and risk prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00363</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00363</id><created>2016-01-31</created><updated>2016-02-02</updated><authors><author><keyname>Li</keyname><forenames>Chuanwen</forenames></author><author><keyname>Gu</keyname><forenames>Yu</forenames></author><author><keyname>Qi</keyname><forenames>Jianzhong</forenames></author><author><keyname>Yu</keyname><forenames>Ge</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Deng</keyname><forenames>Qingxu</forenames></author></authors><title>INSQ: An Influential Neighbor Set Based Moving kNN Query Processing
  System</title><categories>cs.DB</categories><comments>Accepted to appear in ICDE 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the moving k nearest neighbor (MkNN) query, which computes one's k
nearest neighbor set and maintains it while at move. Existing MkNN algorithms
are mostly safe region based, which lack efficiency due to either computing
small safe regions with a high recomputation frequency or computing larger safe
regions but with a high cost for each computation. In this demonstration, we
showcase a system named INSQ that adopts a novel algorithm called the
Influential Neighbor Set (INS) algorithm to process the MkNN query in both
two-dimensional Euclidean space and road networks. This algorithm uses a small
set of safe guarding objects instead of safe regions. As long as the the
current k nearest neighbors are closer to the query object than the safe
guarding objects are, the current k nearest neighbors stay valid and no
recomputation is required. Meanwhile, the region defined by the safe guarding
objects is the largest possible safe region. This means that the recomputation
frequency is also minimized and hence, the INS algorithm achieves high overall
query processing efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00366</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00366</id><created>2016-01-31</created><authors><author><keyname>Thanh</keyname><forenames>Tan Le</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Multi-Channel MAC Protocol for Full-Duplex Cognitive Radio Networks with
  Optimized Access Control and Load Balancing</title><categories>cs.IT cs.DC cs.NI math.IT math.ST stat.AP stat.TH</categories><comments>To appear in 2016 IEEE International Conference on Communications
  (IEEE ICC 2016). arXiv admin note: text overlap with arXiv:1512.03839</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multi-channel full-duplex Medium Access Control
(MAC) protocol for cognitive radio networks (MFDC-MAC). Our design exploits the
fact that full-duplex (FD) secondary users (SUs) can perform spectrum sensing
and access simultaneously, and we employ the randomized dynamic channel
selection for load balancing among channels and the standard backoff mechanism
for contention resolution on each available channel. Then, we develop a
mathematical model to analyze the throughput performance of the proposed
MFDC-MAC protocol. Furthermore, we study the protocol configuration
optimization to maximize the network throughput where we show that this
optimization can be performed in two steps, namely optimization of access and
transmission parameters on each channel and optimization of channel selection
probabilities of the users. Such optimization aims at achieving efficient
self-interference management for FD transceivers, sensing overhead control, and
load balancing among the channels. Numerical results demonstrate the impacts of
different protocol parameters and the importance of parameter optimization on
the throughput performance as well as the significant performance gain of the
proposed design compared to traditional design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00367</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00367</id><created>2016-01-31</created><authors><author><keyname>Xiao</keyname><forenames>Yijun</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author></authors><title>Efficient Character-level Document Classification by Combining
  Convolution and Recurrent Layers</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document classification tasks were primarily tackled at word level. Recent
research that works with character-level inputs shows several benefits over
word-level approaches such as natural incorporation of morphemes and better
handling of rare words. We propose a neural network architecture that utilizes
both convolution and recurrent layers to efficiently encode character inputs.
We validate the proposed model on eight large scale document classification
tasks and compare with character-level convolution-only models. It achieves
comparable performances with much less parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00370</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00370</id><created>2016-01-31</created><authors><author><keyname>Tang</keyname><forenames>Jian</forenames></author><author><keyname>Liu</keyname><forenames>Jingzhou</forenames></author><author><keyname>Zhang</keyname><forenames>Ming</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author></authors><title>Visualization Large-scale and High-dimensional Data</title><categories>cs.LG cs.HC</categories><comments>WWW 2016</comments><doi>10.1145/2872427.2883041</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We study the problem of visualizing large-scale and high-dimensional data in
a low-dimensional (typically 2D or 3D) space. Much success has been reported
recently by techniques that first compute a similarity structure of the data
points and then project them into a low-dimensional space with the structure
preserved. These two steps suffer from considerable computational costs,
preventing the state-of-the-art methods such as the t-SNE from scaling to
large-scale and high-dimensional data (e.g., millions of data points and
hundreds of dimensions). We propose the LargeVis, a technique that first
constructs an accurately approximated K-nearest neighbor graph from the data
and then layouts the graph in the low-dimensional space. Comparing to t-SNE,
LargeVis significantly reduces the computational cost of the graph construction
step and employs a principled probabilistic model for the visualization step,
the objective of which can be effectively optimized through asynchronous
stochastic gradient descent with a linear time complexity. The whole procedure
thus easily scales to millions of high-dimensional data points. Experimental
results on real-world data sets demonstrate that the LargeVis outperforms the
state-of-the-art methods in both efficiency and effectiveness. The
hyper-parameters of LargeVis are also much more stable over different data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00374</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00374</id><created>2016-01-31</created><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Moon</keyname><forenames>Kyeong H.</forenames></author><author><keyname>Hsu</keyname><forenames>William</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>ConfidentCare: A Clinical Decision Support System for Personalized
  Breast Cancer Screening</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Breast cancer screening policies attempt to achieve timely diagnosis by the
regular screening of apparently healthy women. Various clinical decisions are
needed to manage the screening process; those include: selecting the screening
tests for a woman to take, interpreting the test outcomes, and deciding whether
or not a woman should be referred to a diagnostic test. Such decisions are
currently guided by clinical practice guidelines (CPGs), which represent a
one-size-fits-all approach that are designed to work well on average for a
population, without guaranteeing that it will work well uniformly over that
population. Since the risks and benefits of screening are functions of each
patients features, personalized screening policies that are tailored to the
features of individuals are needed in order to ensure that the right tests are
recommended to the right woman. In order to address this issue, we present
ConfidentCare: a computer-aided clinical decision support system that learns a
personalized screening policy from the electronic health record (EHR) data.
ConfidentCare operates by recognizing clusters of similar patients, and
learning the best screening policy to adopt for each cluster. A cluster of
patients is a set of patients with similar features (e.g. age, breast density,
family history, etc.), and the screening policy is a set of guidelines on what
actions to recommend for a woman given her features and screening test scores.
ConfidentCare algorithm ensures that the policy adopted for every cluster of
patients satisfies a predefined accuracy requirement with a high level of
confidence. We show that our algorithm outperforms the current CPGs in terms of
cost-efficiency and false positive rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00376</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00376</id><created>2016-01-31</created><authors><author><keyname>Yang</keyname><forenames>Shengtian</forenames></author><author><keyname>Honold</keyname><forenames>Thomas</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Beyond Countable Alphabets: An Extension of the Information-Spectrum
  Approach</title><categories>cs.IT math.IT</categories><comments>v0.3.1.7687f2, 8 pages, a short version was submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general approach, as an extension of the information-spectrum approach, is
established for deriving one-shot performance bounds for information-theoretic
problems whose alphabets are Polish. It is mainly based on the quantization
idea and a novel form of &quot;likelihood ratio&quot;. As an example, one-shot lower and
upper bounds for random number generation from correlated sources on Polish
alphabets are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00377</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00377</id><created>2016-01-31</created><updated>2016-02-24</updated><authors><author><keyname>Akhoundi</keyname><forenames>Farhad</forenames></author><author><keyname>Jamali</keyname><forenames>Mohammad Vahid</forenames></author><author><keyname>Banihassan</keyname><forenames>Navid</forenames></author><author><keyname>Beyranvand</keyname><forenames>Hamzeh</forenames></author><author><keyname>Minoofar</keyname><forenames>Amir</forenames></author><author><keyname>Salehi</keyname><forenames>Jawad A.</forenames></author></authors><title>Cellular Underwater Wireless Optical CDMA Network: Potentials and
  Challenges</title><categories>cs.NI</categories><comments>11 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Underwater wireless optical communications is an emerging solution to the
expanding demand for broadband links in oceans and seas. In this paper, a
cellular underwater wireless optical code division multiple-access (UW-OCDMA)
network is proposed to provide broadband links for commercial and military
applications. The optical orthogonal codes (OOC) are employed as signature
codes of underwater mobile users. Fundamental key aspects of the network such
as its backhaul architecture, its potential applications and its design
challenges are presented. In particular, the proposed network is used as
infrastructure of centralized, decentralized and relay-assisted underwater
sensor networks for high-speed real-time monitoring. Furthermore, a promising
underwater localization and positioning scheme based on this cellular network
is presented. Finally, probable design challenges such as cell edge coverage,
blockage avoidance, power control and increasing the network capacity are
addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00382</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00382</id><created>2016-01-31</created><authors><author><keyname>Sahu</keyname><forenames>Anit Kumar</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Moura</keyname><forenames>Jose' M. F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Distributed Constrained Recursive Nonlinear Least-Squares Estimation:
  Algorithms and Asymptotics</title><categories>math.OC cs.IT math.IT math.PR math.ST stat.TH</categories><comments>27 pages. Submitted for journal publication. Initial Submission: Feb.
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the problem of recursive nonlinear least squares
parameter estimation in multi-agent networks, in which the individual agents
observe sequentially over time an independent and identically distributed
(i.i.d.) time-series consisting of a nonlinear function of the true but unknown
parameter corrupted by noise. A distributed recursive estimator of the
\emph{consensus} + \emph{innovations} type, namely $\mathcal{CIWNLS}$, is
proposed, in which the agents update their parameter estimates at each
observation sampling epoch in a collaborative way by simultaneously processing
the latest locally sensed information~(\emph{innovations}) and the parameter
estimates from other agents~(\emph{consensus}) in the local neighborhood
conforming to a pre-specified inter-agent communication topology. Under rather
weak conditions on the connectivity of the inter-agent communication and a
\emph{global observability} criterion, it is shown that at every network agent,
the proposed algorithm leads to consistent parameter estimates. Furthermore,
under standard smoothness assumptions on the local observation functions, the
distributed estimator is shown to yield order-optimal convergence rates, i.e.,
as far as the order of pathwise convergence is concerned, the local parameter
estimates at each agent are as good as the optimal centralized nonlinear least
squares estimator which would require access to all the observations across all
the agents at all times. In order to benchmark the performance of the proposed
distributed $\mathcal{CIWNLS}$ estimator with that of the centralized nonlinear
least squares estimator, the asymptotic normality of the estimate sequence is
established and the asymptotic covariance of the distributed estimator is
evaluated. Finally, simulation results are presented which illustrate and
verify the analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00386</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00386</id><created>2016-01-31</created><authors><author><keyname>Siva</keyname><forenames>Parthipan</forenames></author><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Jamieson</keyname><forenames>Mike</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Scene Invariant Crowd Segmentation and Counting Using Scale-Normalized
  Histogram of Moving Gradients (HoMG)</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of automated crowd segmentation and counting has garnered
significant interest in the field of video surveillance. This paper proposes a
novel scene invariant crowd segmentation and counting algorithm designed with
high accuracy yet low computational complexity in mind, which is key for
widespread industrial adoption. A novel low-complexity, scale-normalized
feature called Histogram of Moving Gradients (HoMG) is introduced for highly
effective spatiotemporal representation of individuals and crowds within a
video. Real-time crowd segmentation is achieved via boosted cascade of weak
classifiers based on sliding-window HoMG features, while linear SVM regression
of crowd-region HoMG features is employed for real-time crowd counting.
Experimental results using multi-camera crowd datasets show that the proposed
algorithm significantly outperform state-of-the-art crowd counting algorithms,
as well as achieve very promising crowd segmentation results, thus
demonstrating the efficacy of the proposed method for highly-accurate,
real-time video-driven crowd analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00389</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00389</id><created>2016-01-31</created><updated>2016-02-02</updated><authors><author><keyname>Sun</keyname><forenames>Yu</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Xue</keyname><forenames>Andy Yuan</forenames></author><author><keyname>Qi</keyname><forenames>Jianzhong</forenames></author><author><keyname>Du</keyname><forenames>Xiaoyong</forenames></author></authors><title>Reverse Nearest Neighbor Heat Maps: A Tool for Influence Exploration</title><categories>cs.DB</categories><comments>Accepted to appear in ICDE 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing a reverse nearest neighbor (RNN) heat
map by finding the RNN set of every point in a two-dimensional space. Based on
the RNN set of a point, we obtain a quantitative influence (i.e., heat) for the
point. The heat map provides a global view on the influence distribution in the
space, and hence supports exploratory analyses in many applications such as
marketing and resource management. To construct such a heat map, we first
reduce it to a problem called Region Coloring (RC), which divides the space
into disjoint regions within which all the points have the same RNN set. We
then propose a novel algorithm named CREST that efficiently solves the RC
problem by labeling each region with the heat value of its containing points.
In CREST, we propose innovative techniques to avoid processing expensive RNN
queries and greatly reduce the number of region labeling operations. We perform
detailed analyses on the complexity of CREST and lower bounds of the RC
problem, and prove that CREST is asymptotically optimal in the worst case.
Extensive experiments with both real and synthetic data sets demonstrate that
CREST outperforms alternative algorithms by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00398</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00398</id><created>2016-02-01</created><authors><author><keyname>Lee</keyname><forenames>Seunghoon</forenames></author></authors><title>A Short Note on Improved Logic Circuits in a Hexagonal Minesweeper</title><categories>cs.CC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to present an advanced version of PP-hardness proof of
Minesweeper by Bondt. The advancement includes improved Minesweeper
configurations for 'logic circuits' in a hexagonal Minesweeper. To do so, I
demonstrate logical uncertainty in Minesweeper, which ironically allows a
possibility to make some Boolean operators.
  The fact that existing hexagonal logic circuits did not clearly distinguish
the true and false signal needs an improved form of a hexagonal wire. I
introduce new forms of logic circuits such as NOT, AND, OR gates, a curve and a
splitter of wires. Moreover, these new logic circuits complement Bondt's proof
for PP-hardness of Minesweeper by giving a new figure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00399</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00399</id><created>2016-02-01</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>The Price of Order</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present tight bounds on the spanning ratio of a large family of ordered
$\theta$-graphs. A $\theta$-graph partitions the plane around each vertex into
$m$ disjoint cones, each having aperture $\theta = 2 \pi/m$. An ordered
$\theta$-graph is constructed by inserting the vertices one by one and
connecting each vertex to the closest previously-inserted vertex in each cone.
We show that for any integer $k \geq 1$, ordered $\theta$-graphs with $4k + 4$
cones have a tight spanning ratio of $1 + 2 \sin(\theta/2) / (\cos(\theta/2) -
\sin(\theta/2))$. We also show that for any integer $k \geq 2$, ordered
$\theta$-graphs with $4k + 2$ cones have a tight spanning ratio of $1 / (1 - 2
\sin(\theta/2))$. We provide lower bounds for ordered $\theta$-graphs with $4k
+ 3$ and $4k + 5$ cones. For ordered $\theta$-graphs with $4k + 2$ and $4k + 5$
cones these lower bounds are strictly greater than the worst case spanning
ratios of their unordered counterparts. These are the first results showing
that ordered $\theta$-graphs have worse spanning ratios than unordered
$\theta$-graphs. Finally, we show that, unlike their unordered counterparts,
the ordered $\theta$-graphs with 4, 5, and 6 cones are not spanners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00401</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00401</id><created>2016-02-01</created><authors><author><keyname>Cui</keyname><forenames>Shawn X</forenames></author><author><keyname>Ji</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author><author><keyname>Zeng</keyname><forenames>Bei</forenames></author></authors><title>Quantum Capacities for Entanglement Networks</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss quantum capacities for two types of entanglement networks:
$\mathcal{Q}$ for the quantum repeater network with free classical
communication, and $\mathcal{R}$ for the tensor network as the rank of the
linear operation represented by the tensor network. We find that $\mathcal{Q}$
always equals $\mathcal{R}$ in the regularized case for the samenetwork graph.
However, the relationships between the corresponding one-shot capacities
$\mathcal{Q}_1$ and $\mathcal{R}_1$ are more complicated, and the min-cut upper
bound is in general not achievable. We show that the tensor network can be
viewed as a stochastic protocol with the quantum repeater network, such that
$\mathcal{R}_1$ is a natural upper bound of $\mathcal{Q}_1$. We analyze the
possible gap between $\mathcal{R}_1$ and $\mathcal{Q}_1$ for certain networks,
and compare them with the one-shot classical capacity of the corresponding
classical network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00412</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00412</id><created>2016-02-01</created><updated>2016-02-17</updated><authors><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Efficient Frequent Directions Algorithm for Sparse Matrices</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes Sparse Frequent Directions, a variant of Frequent
Directions for sketching sparse matrices. It resembles the original algorithm
in many ways: both receive the rows of an input matrix $A^{n \times d}$ one by
one in the streaming setting and compute a small sketch $B \in R^{\ell \times
d}$. Both share the same strong (provably optimal) asymptotic guarantees with
respect to the space-accuracy tradeoff in the streaming setting. However,
unlike Frequent Directions which runs in $O(nd\ell)$ time regardless of the
sparsity of the input matrix $A$, Sparse Frequent Directions runs in $\tilde{O}
(nnz(A)\ell + n\ell^2)$ time. Our analysis loosens the dependence on computing
the Singular Value Decomposition (SVD) as a black box within the Frequent
Directions algorithm. Our bounds require recent results on the properties of
fast approximate SVD computations. Finally, we empirically demonstrate that
these asymptotic improvements are practical and significant on real and
synthetic data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00413</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00413</id><created>2016-02-01</created><authors><author><keyname>Lai</keyname><forenames>Ching-Yi</forenames></author></authors><title>Linear Programming Bounds for Entanglement-Assisted Quantum
  Error-Correcting Codes by Split Weight Enumerators</title><categories>cs.IT math.IT quant-ph</categories><comments>17 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear programming approaches have been applied to derive upper bounds on the
size of classical codes and quantum codes. In this paper, we derive similar
results for general quantum codes with entanglement assistance, including
nonadditive codes, by considering a type of split weight enumerators. After
deriving the MacWilliams identities for these split weight enumerators, we are
able to prove algebraic linear programming bounds, such as the Singleton bound,
the Hamming bound, and the first linear programming bound.
  On the other hand, we obtain additional constraints on the size of Pauli
subgroups for quantum codes, which allow us to improve the linear programming
bounds on the minimum distance of small quantum codes. In particular, we show
that there is no [[27,15,5]] quantum stabilizer code. We also discuss the
existence of some entanglement-assisted quantum stabilizer codes with maximal
entanglement. As a result, the upper and lower bounds on the minimum distance
of maximal-entanglement quantum stabilizer codes with length up to 15 are
significantly improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00417</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00417</id><created>2016-02-01</created><authors><author><keyname>Alikhanov</keyname><forenames>Jumabek</forenames></author><author><keyname>SeungYon</keyname><forenames>Ko</forenames></author><author><keyname>Geun-Sik</keyname><forenames>Jo</forenames></author></authors><title>Obtaining Better Image Representations by Combining Complementary
  Activation Features of Multiple ConvNet Layers for Transfer Learning</title><categories>cs.CV</categories><comments>5 pages 3 tables, under review of IEEE World Congress on
  Computational Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Networks (ConvNets) are powerful models that learn hierarchies
of visual features, which could also be used as an image representations in
transfer learning. The Basic pipeline for transfer learning is to first train a
ConvNet on a large dataset (source task) and then use feed-forward units
activation of the trained network, at a certain layer, as a generic
representation of an input image for another smaller dataset (target task). Our
key contribution is to introduce a notion of better transfer learning using
multiple ConvNet layer features. Idea is based on the difference of ConvNet
Layer features in describing images when transfer learning. We demonstrate this
idea by achieving better performance in classification against single ConvNet
layer features for three standard datasets. When we combine multiple ConvNet
layer features by simply concatenating them, most of the resulting features
would become repetitive. We use AdaBoost.MH classifier to implicitly select
distinct features from among the combined ConvNet Layer features. Our
experiments intends to show this complementary nature of ConvNet layer features
when doing transfer learning. We use AdaBoost.MH classifier to show that one
can achieve superior results with Combined ConvNet Layer features against
individual ConvNet Layer features. To emphasize the importance of careful
feature selection among combined ConvNet Layer features we also train linear
SVM classifier. In our experiments with AdaBoost.MH being a classifier,
combined multiple ConvNet layer features always outperform Single ConvNet layer
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00419</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00419</id><created>2016-02-01</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Is collaboration among scientists related to the citation impact of
  papers because their quality increases with collaboration? An analysis based
  on data from F1000Prime and normalized citation scores</title><categories>cs.DL physics.soc-ph</categories><comments>Accepted for publication in the Journal of the Association for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the relationship of collaboration among scientists and the
citation impact of papers have been frequently investigated. Most of the
studies show that the two variables are closely related: an increasing
collaboration activity (measured in terms of number of authors, number of
affiliations, and number of countries) is associated with an increased citation
impact. However, it is not clear whether the increased citation impact is based
on the higher quality of papers which profit from more than one scientist
giving expert input or other (citation-specific) factors. Thus, the current
study addresses this question by using two comprehensive datasets with
publications (in the biomedical area) including quality assessments by experts
(F1000Prime member scores) and citation data for the publications. The study is
based on nearly 10,000 papers. Robust regression models are used to investigate
the relationship between number of authors, number of affiliations, and number
of countries, respectively, and citation impact - controlling for the papers'
quality (measured by F1000Prime expert ratings). The results point out that the
effect of collaboration activities on impact is largely independent of the
papers' quality. The citation advantage is apparently not quality-related;
citation specific factors (e.g. self-citations) seem to be important here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00422</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00422</id><created>2016-02-01</created><authors><author><keyname>Takagi</keyname><forenames>Takuya</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Sadakane</keyname><forenames>Kunihiko</forenames></author><author><keyname>Arimura</keyname><forenames>Hiroki</forenames></author></authors><title>Packed Compact Tries: A Fast and Efficient Data Structure for Online
  String Processing</title><categories>cs.DS</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new data structure called the packed compact trie
(packed c-trie) which stores a set $S$ of $k$ strings of total length $n$ in $n
\log\sigma + O(k \log n)$ bits of space and supports fast pattern matching
queries and updates, where $\sigma$ is the size of an alphabet. Assume that
$\alpha = \log_\sigma n$ letters are packed in a single machine word on the
standard word RAM model, and let $f(k,n)$ denote the query and update times of
the dynamic predecessor/successor data structure of our choice which stores $k$
integers from universe $[1,n]$ in $O(k \log n)$ bits of space. Then, given a
string of length $m$, our packed c-tries support pattern matching queries and
insert/delete operations in $O(\frac{m}{\alpha} f(k,n))$ worst-case time and in
$O(\frac{m}{\alpha} + f(k,n))$ expected time. Our experiments show that our
packed c-tries are faster than the standard compact tries (a.k.a. Patricia
trees) on real data sets. As an application of our packed c-trie, we show that
the sparse suffix tree for a string of length $n$ over prefix codes with $k$
sampled positions, such as evenly-spaced and word delimited sparse suffix
trees, can be constructed online in $O((\frac{n}{\alpha} + k) f(k,n))$
worst-case time and $O(\frac{n}{\alpha} + k f(k,n))$ expected time with $n \log
\sigma + O(k \log n)$ bits of space. When $k = O(\frac{n}{\alpha})$, by using
the state-of-the-art dynamic predecessor/successor data structures, we obtain
sub-linear time construction algorithms using only $O(\frac{n}{\alpha})$ bits
of space in both cases. We also discuss an application of our packed c-tries to
online LZD factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00424</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00424</id><created>2016-02-01</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author></authors><title>Reduction-Based Creative Telescoping for Algebraic Functions</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuing a series of articles in the past few years on creative telescoping
using reductions, we develop a new algorithm to construct minimal telescopers
for algebraic functions. This algorithm is based on Trager's Hermite reduction
and on polynomial reduction, which was originally designed for hyperexponential
functions and extended to the algebraic case in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00426</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00426</id><created>2016-02-01</created><authors><author><keyname>Chung</keyname><forenames>Cheng-Tao</forenames></author><author><keyname>Tsai</keyname><forenames>Cheng-Yu</forenames></author><author><keyname>Lu</keyname><forenames>Hsiang-Hung</forenames></author><author><keyname>Liu</keyname><forenames>Chia-Hsiang</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>An Iterative Deep Learning Framework for Unsupervised Discovery of
  Speech Features and Linguistic Units with Applications on Spoken Term
  Detection</title><categories>cs.CL cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1506.02327</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we aim to discover high quality speech features and linguistic
units directly from unlabeled speech data in a zero resource scenario. The
results are evaluated using the metrics and corpora proposed in the Zero
Resource Speech Challenge organized at Interspeech 2015. A Multi-layered
Acoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets
of acoustic tokens from the given corpus. Each acoustic token set is specified
by a set of hyperparameters that describe the model configuration. These sets
of acoustic tokens carry different characteristics fof the given corpus and the
language behind, thus can be mutually reinforced. The multiple sets of token
labels are then used as the targets of a Multi-target Deep Neural Network
(MDNN) trained on low-level acoustic features. Bottleneck features extracted
from the MDNN are then used as the feedback input to the MAT and the MDNN
itself in the next iteration. We call this iterative deep learning framework
the Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which
generates both high quality speech features for the Track 1 of the Challenge
and acoustic tokens for the Track 2 of the Challenge. In addition, we performed
extra experiments on the same corpora on the application of query-by-example
spoken term detection. The experimental results showed the iterative deep
learning framework of MAT-DNN improved the detection performance due to better
underlying speech features and acoustic tokens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00430</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00430</id><created>2016-02-01</created><authors><author><keyname>Sun</keyname><forenames>Biao</forenames></author><author><keyname>Zhao</keyname><forenames>Wenfeng</forenames></author><author><keyname>Zhu</keyname><forenames>Xinshan</forenames></author></authors><title>Compressed Sensing for Implantable Neural Recordings Using Co-sparse
  Analysis Model and Weighted $\ell_1$-Optimization</title><categories>cs.IT math.IT</categories><comments>22 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable and energy-efficient wireless data transmission remains a major
challenge in resource-constrained wireless neural recording tasks, where data
compression is generally adopted to relax the burdens on the wireless data
link. Recently, Compressed Sensing (CS) theory has successfully demonstrated
its potential in neural recording application. The main limitation of CS,
however, is that the neural signals have no good sparse representation with
commonly used dictionaries and learning a reliable dictionary is often data
dependent and computationally demanding. In this paper, a novel CS approach for
implantable neural recording is proposed. The main contributions are: 1) The
co-sparse analysis model is adopted to enforce co-sparsity of the neural
signals, therefore overcoming the drawbacks of conventional synthesis model and
enhancing the reconstruction performance. 2) A multi-fractional-order
difference matrix is constructed as the analysis dictionary, thus avoiding the
dictionary learning procedure and reducing the need for previously acquired
data and computational resources. 3) By exploiting the statistical priors of
the analysis coefficients, a weighted analysis $\ell_1$-minimization (WALM)
algorithm is proposed to reconstruct the neural signals. Experimental results
on Leicester neural signal database reveal that the proposed approach
outperforms the state-of-the-art CS-based methods. On the challenging high
compression ratio task, the proposed approach still achieves high
reconstruction performance and spike classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00431</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00431</id><created>2016-02-01</created><authors><author><keyname>Naldi</keyname><forenames>Simone</forenames></author></authors><title>Solving rank-constrained semidefinite programs in exact arithmetic</title><categories>cs.SY cs.SC</categories><comments>21 pages, 1 appendix</comments><msc-class>14Q20, 52B55</msc-class><acm-class>F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of minimizing a linear function over an affine
section of the cone of positive semidefinite matrices, with the additional
constraint that the feasible matrix has prescribed rank. When the rank
constraint is active, this is a non-convex optimization problem, otherwise it
is a semidefinite program. Both find numerous applications especially in
systems control theory and combinatorial optimization, but even in more general
contexts such as polynomial optimization or real algebra. While numerical
algorithms exist for solving this problem, such as interior-point or
Newton-like algorithms, in this paper we propose an approach based on symbolic
computation. We design an exact algorithm for solving rank-constrained
semidefinite programs, whose complexity is essentially quadratic on natural
degree bounds associated to the given optimization problem: for subfamilies of
the problem where the size of the feasible matrix is fixed, the complexity is
polynomial in the number of variables. The algorithm works under assumptions on
the input data: we prove that these assumptions are generically satisfied. We
also implement it in Maple and discuss practical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00435</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00435</id><created>2016-02-01</created><authors><author><keyname>Gasieniec</keyname><forenames>Leszek</forenames></author><author><keyname>Levcopoulos</keyname><forenames>Christos</forenames></author><author><keyname>Lingas</keyname><forenames>Andrzej</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Tokuyama</keyname><forenames>Takeshi</forenames></author></authors><title>Efficiently Correcting Matrix Products</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of efficiently correcting an erroneous product of two
$n\times n$ matrices over a ring. Among other things, we provide a randomized
algorithm for correcting a matrix product with at most $k$ erroneous entries
running in $\tilde{O}(n^2+kn)$ time and a deterministic $\tilde{O}(kn^2)$-time
algorithm for this problem (where the notation $\tilde{O}$ suppresses
polylogarithmic terms in $n$ and $k$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00446</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00446</id><created>2016-02-01</created><updated>2016-02-02</updated><authors><author><keyname>Ota</keyname><forenames>Takahiro</forenames></author><author><keyname>Manada</keyname><forenames>Akiko</forenames></author><author><keyname>Morita</keyname><forenames>Hiroyoshi</forenames></author></authors><title>A Graph Representation for Two-Dimensional Finite Type Constrained
  Systems</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand of two-dimensional source coding and constrained coding has been
getting higher these days, but compared to the one-dimensional case, many
problems have remained open as the analysis is cumbersome. A main reason for
that would be because there are no graph representations discovered so far. In
this paper, we focus on a two-dimensional finite type constrained system, a set
of two-dimensional blocks characterized by a finite number of two-dimensional
constraints, and propose its graph representation. We then show how to generate
an element of the two-dimensional finite type constrained system from the graph
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00447</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00447</id><created>2016-02-01</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Wale&#x144;</keyname><forenames>Tomasz</forenames></author></authors><title>Faster Longest Common Extension Queries in Strings over General
  Alphabets</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Longest common extension queries (often called longest common prefix queries)
constitute a fundamental building block in multiple string algorithms, for
example computing runs and approximate pattern matching. We show that a
sequence of $q$ LCE queries for a string of size $n$ over a general ordered
alphabet can be realized in $O(q \log \log n+n\log^*n)$ time making only
$O(q+n)$ symbol comparisons. Consequently, all runs in a string over a general
ordered alphabet can be computed in $O(n \log \log n)$ time making $O(n)$
symbol comparisons. Our results improve upon a solution by Kosolobov
(Information Processing Letters, 2016), who gave an algorithm with $O(n
\log^{2/3} n)$ running time and conjectured that $O(n)$ time is possible. We
make a significant progress towards resolving this conjecture. Our techniques
extend to the case of general unordered alphabets, when the time increases to
$O(q\log n + n\log^*n)$. The main tools are difference covers and a variant of
the disjoint-sets data structure by La Poutr\'e (SODA 1990).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00448</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00448</id><created>2016-02-01</created><authors><author><keyname>Hammami</keyname><forenames>Seif eddine</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Marot</keyname><forenames>Michel</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author></authors><title>Network planning tool based on network classification and load
  prediction</title><categories>cs.NI</categories><comments>The article has 6 pages, 8 figures and is accepted to be presented at
  WCNC'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real Call Detail Records (CDR) are analyzed and classified based on Support
Vector Machine (SVM) algorithm. The daily classification results in three
traffic classes. We use two different algorithms, K-means and SVM to check the
classification efficiency. A second support vector regression (SVR) based
algorithm is built to make an online prediction of traffic load using the
history of CDRs. Then, these algorithms will be integrated to a network
planning tool which will help cellular operators on planning optimally their
access network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00453</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00453</id><created>2016-02-01</created><authors><author><keyname>Boshkovska</keyname><forenames>Elena</forenames></author><author><keyname>Morsi</keyname><forenames>Rania</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Power Allocation and Scheduling for SWIPT Systems with Non-linear Energy
  Harvesting Model</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation at the IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a resource allocation algorithm for multiuser
simultaneous wireless information and power transfer systems for a realistic
non-linear energy harvesting (EH) model. In particular, the algorithm design is
formulated as a non-convex optimization problem for the maximization of the
long-term average total harvested power at EH receivers subject to quality of
service requirements for information decoding receivers. To obtain a tractable
solution, we transform the corresponding non-convex sum-of-ratios objective
function into an equivalent objective function in parametric subtractive form.
This leads to a computationally efficient iterative resource allocation
algorithm. Numerical results reveal a significant performance gain that can be
achieved if the resource allocation algorithm design is based on the non-linear
EH model instead of the traditional linear model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00454</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00454</id><created>2016-02-01</created><authors><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Paule</keyname><forenames>Peter</forenames></author></authors><title>Holonomic Tools for Basic Hypergeometric Functions</title><categories>cs.SC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the exception of q-hypergeometric summation, the use of computer algebra
packages implementing Zeilberger's &quot;holonomic systems approach&quot; in a broader
mathematical sense is less common in the field of q-series and basic
hypergeometric functions. A major objective of this article is to popularize
the usage of such tools also in these domains. Concrete case studies showing
software in action introduce to the basic techniques. An application highlight
is a new computer-assisted proof of the celebrated Ismail-Zhang formula, an
important q-analog of a classical expansion formula of plane waves in terms of
Gegenbauer polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00458</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00458</id><created>2016-02-01</created><authors><author><keyname>Alberti</keyname><forenames>Francesco</forenames></author><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames></author><author><keyname>Pagani</keyname><forenames>Elena</forenames></author></authors><title>Counting Constraints in Flat Array Fragments</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify a fragment of Presburger arithmetic enriched with free function
symbols and cardinality constraints for interpreted sets, which is amenable to
automated analysis. We establish decidability and complexity results for such a
fragment and we implement our algorithms. The experiments run in discharging
proof obligations coming from invariant checking and bounded model-checking
benchmarks show the practical feasibility of our decision procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00462</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00462</id><created>2016-02-01</created><authors><author><keyname>Yakovlev</keyname><forenames>Konstantin</forenames></author><author><keyname>Khithov</keyname><forenames>Vsevolod</forenames></author><author><keyname>Loginov</keyname><forenames>Maxim</forenames></author><author><keyname>Petrov</keyname><forenames>Alexander</forenames></author></authors><title>Distributed control and navigation system for quadrotor UAVs in
  GPS-denied environments</title><categories>cs.RO</categories><comments>Camera-ready as submitted (and accepted) to the 7th IEEE
  International Conference Intelligent Systems IS'2014, September 24-26, 2014,
  Warsaw, Poland</comments><doi>10.1007/978-3-319-11310-4_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of developing distributed control and navigation system for
quadrotor UAVs operating in GPS-denied environments is addressed in the paper.
Cooperative navigation, marker detection and mapping task solved by a team of
multiple unmanned aerial vehicles is chosen as demo example. Developed
intelligent control system complies with on 4D\RCS reference model and its
implementation is based on ROS framework. Custom implementation of EKF-based
map building algorithm is used to solve marker detection and map building task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00476</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00476</id><created>2016-02-01</created><authors><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Lasota</keyname><forenames>S&#x142;awomir</forenames></author><author><keyname>Mayr</keyname><forenames>Richard</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Simulation Problems Over One-Counter Nets</title><categories>cs.LO</categories><acm-class>F.1.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-counter nets (OCN) are finite automata equipped with a counter that can
store non-negative integer values, and that cannot be tested for zero.
Equivalently, these are exactly 1-dimensional vector addition systems with
states. We show that both strong and weak simulation preorder on OCN are
PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00477</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00477</id><created>2016-02-01</created><authors><author><keyname>Englert</keyname><forenames>Matthias</forenames></author><author><keyname>Lazi&#x107;</keyname><forenames>Ranko</forenames></author><author><keyname>Totzke</keyname><forenames>Patrick</forenames></author></authors><title>Reachability in Two-Dimensional Unary Vector Addition Systems with
  States is NL-Complete</title><categories>cs.LO</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blondin et al. showed at LICS 2015 that two-dimensional vector addition
systems with states have reachability witnesses of length exponential in the
number of states and polynomial in the norm of vectors. The resulting
guess-and-verify algorithm is optimal (PSPACE), but only if the input vectors
are given in binary. We answer positively the main question left open by their
work, namely establish that reachability witnesses of pseudo-polynomial length
always exist. Hence, when the input vectors are given in unary, the improved
guess-and-verify algorithm requires only logarithmic space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00479</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00479</id><created>2016-02-01</created><authors><author><keyname>Nawaz</keyname><forenames>Waqas</forenames></author><author><keyname>Khan</keyname><forenames>Kifayat-Ullah</forenames></author><author><keyname>Lee</keyname><forenames>Young-Koo</forenames></author></authors><title>A Multi-User Perspective for Personalized Email Communities</title><categories>cs.SI</categories><comments>46 pages, 14 images, Accepted in Expert Systems with Applications
  Journal</comments><doi>10.1016/j.eswa.2016.01.046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Email classification and prioritization expert systems have the potential to
automatically group emails and users as communities based on their
communication patterns, which is one of the most tedious tasks. The exchange of
emails among users along with the time and content information determine the
pattern of communication. The intelligent systems extract these patterns from
an email corpus of single or all users and are limited to statistical analysis.
However, the email information revealed in those methods is either constricted
or widespread, i.e. single or all users respectively, which limits the
usability of the resultant communities. In contrast to extreme views of the
email information, we relax the aforementioned restrictions by considering a
subset of all users as multi-user information in an incremental way to extend
the personalization concept. Accordingly, we propose a multi-user personalized
email community detection method to discover the groupings of email users based
on their structural and semantic intimacy. We construct a social graph using
multi-user personalized emails. Subsequently, the social graph is uniquely
leveraged with expedient attributes, such as semantics, to identify user
communities through collaborative similarity measure. The multi-user
personalized communities, which are evaluated through different quality
measures, enable the email systems to filter spam or malicious emails and
suggest contacts while composing emails. The experimental results over two
randomly selected users from email network, as constrained information, unveil
partial interaction among 80% email users with 14% search space reduction where
we notice 25% improvement in the clustering coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00481</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00481</id><created>2016-02-01</created><authors><author><keyname>Bouyer</keyname><forenames>Patricia</forenames></author><author><keyname>Colange</keyname><forenames>Maximilien</forenames></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames></author></authors><title>Symbolic Optimal Reachability in Weighted Timed Automata</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted timed automata have been defined in the early 2000's for modelling
resource-consumption or -allocation problems in real-time systems. Optimal
reachability is decidable in weighted timed automata, and a symbolic forward
algorithm has been developed to solve that problem. This algorithm uses
so-called priced zones, an extension of standard zones with cost functions. In
order to ensure termination, the algorithm requires clocks to be bounded. For
unpriced timed automata, much work has been done to develop sound abstractions
adapted to the forward exploration of timed automata, ensuring termination of
the model-checking algorithm without bounding the clocks. In this paper, we
take advantage of recent developments on abstractions for timed automata, and
propose an algorithm allowing for symbolic analysis of all weighted timed
automata, without requiring bounded clocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00482</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00482</id><created>2016-02-01</created><authors><author><keyname>Roy</keyname><forenames>Sayan Basu</forenames></author><author><keyname>Bhasin</keyname><forenames>Shubhendu</forenames></author><author><keyname>Kar</keyname><forenames>Indra Narayan</forenames></author></authors><title>Memory-Based Data-Driven MRAC Architecture Ensuring Parameter
  Convergence</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convergence of controller parameters in standard model reference adaptive
control (MRAC) requires the system states to be persistently exciting (PE), a
restrictive condition to be verified online. A recent data-driven approach,
concurrent learning, uses information-rich past data concurrently with the
standard parameter update laws to guarantee parameter convergence without the
need of the PE condition. This method guarantees exponential convergence of
both the tracking and the controller parameter estimation errors to zero,
whereas, the classical MRAC merely ensures asymptotic convergence of tracking
error to zero. However, the method requires knowledge of the state derivative,
at least at the time instances when the state values are stored in memory. The
method further assumes knowledge of the control allocation matrix. This paper
addresses these limitations by using a memory-based finite-time system
identifier in conjunction with a data-driven approach, leading to convergence
of both the tracking and the controller parameter estimation errors without the
PE condition and knowledge of the system matrices and the state derivative. A
Lyapunov based stability proof is included to justify the validity of the
proposed data-driven approach. Simulation results demonstrate the efficacy of
the suggested method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00484</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00484</id><created>2016-02-01</created><authors><author><keyname>Roman</keyname><forenames>Rodrigo</forenames></author><author><keyname>Lopez</keyname><forenames>Javier</forenames></author><author><keyname>Mambo</keyname><forenames>Masahiro</forenames></author></authors><title>Mobile Edge Computing, Fog et al.: A Survey and Analysis of Security
  Threats and Challenges</title><categories>cs.CR cs.NI</categories><comments>Preprint (28 pages, 1 figure, 3 tables) submitted to a journal</comments><acm-class>A.1; D.4.6; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For various reasons, the cloud computing paradigm is unable to meet certain
requirements (e.g. low latency and jitter, context awareness, mobility support)
that are crucial for several applications (e.g. vehicular networks, augmented
reality). To fulfil these requirements, various paradigms, such as mobile edge
computing, fog computing, and mobile cloud computing, have emerged in recent
years. While these edge paradigms share several features, most of the existing
research is compartmentalised; no synergies have been explored. This is
especially true in the field of security, where most analyses focus only on one
edge paradigm, while ignoring the others. The main goal of this study is to
holistically analyse the security threats, challenges, and mechanisms inherent
in all edge paradigms, while highlighting potential synergies and venues of
collaboration. In our results, we will show that all edge paradigms should
consider the advances in other paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00487</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00487</id><created>2016-02-01</created><authors><author><keyname>Francois</keyname><forenames>Frederic</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Towards a Cognitive Routing Engine for Software Defined Networks</title><categories>cs.NI cs.AI</categories><comments>This is a non-final version of the paper submitted to IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most Software Defined Networks (SDN) traffic engineering applications use
excessive and frequent global monitoring in order to find the optimal
Quality-of-Service (QoS) paths for the current state of the network. In this
work, we present the motivations, architecture and initial evaluation of a SDN
application called Cognitive Routing Engine (CRE) which is able to find
near-optimal paths for a user-specified QoS while using a very small monitoring
overhead compared to global monitoring which is required to guarantee that
optimal paths are found. Smaller monitoring overheads bring the advantage of
smaller response time for the SDN controllers and switches. The initial
evaluation of CRE on a SDN representation of the GEANT academic network shows
that it is possible to find near-optimal paths with a small optimality gap of
1.65% while using 9.5 times less monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00489</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00489</id><created>2016-02-01</created><updated>2016-02-19</updated><authors><author><keyname>Dubin</keyname><forenames>Ran</forenames></author><author><keyname>Dvir</keyname><forenames>Amit</forenames></author><author><keyname>Pele</keyname><forenames>Ofir</forenames></author><author><keyname>Hadar</keyname><forenames>Ofer</forenames></author><author><keyname>Richman</keyname><forenames>Itay</forenames></author><author><keyname>Trabelsi</keyname><forenames>Ofir</forenames></author></authors><title>Real Time Video Quality Representation Classification of Encrypted HTTP
  Adaptive Video Streaming - the Case of Safari</title><categories>cs.MM cs.CR cs.LG cs.NI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing popularity of HTTP adaptive video streaming services has
dramatically increased bandwidth requirements on operator networks, which
attempt to shape their traffic through Deep Packet Inspection (DPI). However,
Google and certain content providers have started to encrypt their video
services. As a result, operators often encounter difficulties in shaping their
encrypted video traffic via DPI. This highlights the need for new traffic
classification methods for encrypted HTTP adaptive video streaming to enable
smart traffic shaping. These new methods will have to effectively estimate the
quality representation layer and playout buffer. We present a new method and
show for the first time that video quality representation classification for
(YouTube) encrypted HTTP adaptive streaming is possible. We analyze the
performance of this classification method with Safari over HTTPS. Based on a
large number of offline and online traffic classification experiments, we
demonstrate that it can independently classify, in real time, every video
segment into one of the quality representation layers with 97.18% average
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00490</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00490</id><created>2016-02-01</created><authors><author><keyname>Dubin</keyname><forenames>Ran</forenames></author><author><keyname>Dvir</keyname><forenames>Amit</forenames></author><author><keyname>Pele</keyname><forenames>Ofir</forenames></author><author><keyname>Hadar</keyname><forenames>Ofer</forenames></author></authors><title>I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video
  Streaming Title Classification</title><categories>cs.MM cs.LG cs.NI</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1602.00489</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous research has shown that information can be extracted from encrypted
multimedia streams. This includes video titles classification of non HTTP
adaptive streams (non-HAS). This paper presents an algorithm for
\emph{encrypted HTTP adaptive video streaming title classification}. We
evaluated our algorithm on a new YouTube popular videos dataset that was
collected from the internet under real-world network conditions. We provide the
dataset and the crawler for future research. Our algorithm's classification
accuracy is 98\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00503</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00503</id><created>2016-02-01</created><authors><author><keyname>Ghrab</keyname><forenames>Amine</forenames></author><author><keyname>Romero</keyname><forenames>Oscar</forenames></author><author><keyname>Skhiri</keyname><forenames>Sabri</forenames></author><author><keyname>Vaisman</keyname><forenames>Alejandro</forenames></author><author><keyname>Zim&#xe1;nyi</keyname><forenames>Esteban</forenames></author></authors><title>GRAD: On Graph Database Modeling</title><categories>cs.DB</categories><comments>28 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph databases have emerged as the fundamental technology underpinning
trendy application domains where traditional databases are not well-equipped to
handle complex graph data. However, current graph databases support basic graph
structures and integrity constraints with no standard algebra. In this paper,
we introduce GRAD, a native and generic graph database model. GRAD goes beyond
traditional graph database models, which support simple graph structures and
constraints. Instead, GRAD presents a complete graph database model supporting
advanced graph structures, a set of well-defined constraints over these
structures and a powerful graph analysis-oriented algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00508</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00508</id><created>2016-02-01</created><authors><author><keyname>Gopalkrishnan</keyname><forenames>Manoj</forenames></author><author><keyname>Kandula</keyname><forenames>Varshith</forenames></author><author><keyname>Sriram</keyname><forenames>Praveen</forenames></author><author><keyname>Deshpande</keyname><forenames>Abhishek</forenames></author><author><keyname>Muralidharan</keyname><forenames>Bhaskaran</forenames></author></authors><title>A Bayesian view of Single-Qubit Clocks, and an Energy versus Accuracy
  tradeoff</title><categories>cond-mat.stat-mech cond-mat.mes-hall cs.IT math.IT physics.ins-det quant-ph</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We bring a Bayesian viewpoint to the analysis of clocks. Using exponential
distributions as priors for clocks, we analyze the case of a single precessing
spin. We find that, at least with a single qubit, quantum mechanics does not
allow exact timekeeping, in contrast to classical mechanics which does. We find
the optimal ratio of angular velocity of precession to rate of the exponential
distribution that leads to maximum accuracy. Further, we find an energy versus
accuracy tradeoff --- the energy cost is at least $k_BT$ times the improvement
in accuracy as measured by the entropy reduction in going from the prior
distribution to the posterior distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00515</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00515</id><created>2016-02-01</created><updated>2016-02-02</updated><authors><author><keyname>Milosevic</keyname><forenames>Nikola</forenames></author></authors><title>Marvin: Semantic annotation using multiple knowledge sources</title><categories>cs.AI cs.CL</categories><comments>9 pages, 4 figures, keywords: Semantic annotation, text
  normalization, semantic web, linked data, information management, text
  mining, information extraction, data curation</comments><acm-class>D.3.2; K.2; H.2.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  People are producing more written material then anytime in the history. The
increase is so high that professionals from the various fields are no more able
to cope with this amount of publications. Text mining tools can offer tools to
help them and one of the tools that can aid information retrieval and
information extraction is semantic text annotation. In this report we present
Marvin, a text annotator written in Java, which can be used as a command line
tool and as a Java library. Marvin is able to annotate text using multiple
sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00542</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00542</id><created>2016-02-01</created><updated>2016-03-01</updated><authors><author><keyname>Srinath</keyname><forenames>K. Pavan</forenames></author><author><keyname>Venkataramanan</keyname><forenames>Ramji</forenames></author></authors><title>Cluster-Seeking James-Stein Estimators</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>36 Pages, 5 figures, submitted in part to ISIT 2016. The latest
  version includes a few corrections, mainly to Lemma 2, Lemma 11, and the
  proof of Theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of estimating a high-dimensional vector of
parameters $\boldsymbol{\theta} \in \mathbb{R}^n$ from a noisy observation. The
noise vector is i.i.d. Gaussian with known variance. For a squared-error loss
function, the James-Stein (JS) estimator is known to dominate the simple
maximum-likelihood (ML) estimator when the dimension $n$ exceeds two. The
JS-estimator shrinks the observed vector towards the origin, and the risk
reduction over the ML-estimator is greatest for $\boldsymbol{\theta}$ that lie
close to the origin. JS-estimators can be generalized to shrink the data
towards any target subspace. Such estimators also dominate the ML-estimator,
but the risk reduction is significant only when $\boldsymbol{\theta}$ lies
close to the subspace. This leads to the question: in the absence of prior
information about $\boldsymbol{\theta}$, how do we design estimators that give
significant risk reduction over the ML-estimator for a wide range of
$\boldsymbol{\theta}$?
  In this paper, we propose shrinkage estimators that attempt to infer the
structure of $\boldsymbol{\theta}$ from the observed data in order to construct
a good attracting subspace. In particular, the components of the observed
vector are separated into clusters, and the elements in each cluster shrunk
towards a common attractor. The number of clusters and the attractor for each
cluster are determined from the observed vector. We provide concentration
results for the squared-error loss and convergence results for the risk of the
proposed estimators. The results show that the estimators give significant risk
reduction over the ML-estimator for a wide range of $\boldsymbol{\theta}$,
particularly for large $n$. Simulation results are provided to support the
theoretical claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00545</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00545</id><created>2016-02-01</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Christol</keyname><forenames>Gilles</forenames></author><author><keyname>Dumas</keyname><forenames>Philippe</forenames></author></authors><title>Fast computation of the $N$th term of an algebraic series in positive
  characteristic</title><categories>cs.SC math.NT</categories><msc-class>11Y16, 68W30, 13F25, 11B85</msc-class><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question of computing one selected term of an algebraic power
series. In characteristic zero, the best known algorithm for computing the
$N$th coefficient of an algebraic series uses differential equations and has
arithmetic complexity quasi-linear in $\sqrt{N}$. We show that in positive
characteristic $p$, the complexity can be lowered to $O(\log N)$. The
mathematical basis of this dramatic improvement is a classical theorem stating
that a formal power series with coefficients in a finite field is algebraic if
and only if the sequence of its coefficients can be generated by an automaton.
We revisit and enhance two constructive proofs of this result. The first proof
uses Mahler equations; their size appear to be prohibitively large. The second
proof relies on diagonals of rational functions; we turn it into an efficient
algorithm, of complexity linear in $\log N$ and quasi-linear in $p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00546</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00546</id><created>2016-01-21</created><authors><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Gra&#xe7;a</keyname><forenames>Daniel</forenames></author><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author></authors><title>On the Functions Generated by the General Purpose Analog Computer</title><categories>cs.CC math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the General Purpose Analog Computer (GPAC), introduced by Claude
Shannon in 1941 as a mathematical model of Differential Analysers, that is to
say as a model of continuous-time analog (mechanical, and later one electronic)
machines of that time. We extend the model properly to a model of computation
not restricted to univariate functions (i.e. functions $f: \mathbb{R} \to
\mathbb{R}$) but also to the multivariate case of (i.e. functions $f:
\mathbb{R}^n \to \mathbb{R}^m$), and establish some basic properties. In
particular, we prove that a very wide class of (continuous and discontinuous)
functions can be uniformly approximated over their full domain. Technically: we
generalize some known results about the GPAC to the multidimensional case: we
extend naturally the notion of \emph{generable} function, from the
unidimensional to the multidimensional case. We prove a few stability
properties of this class, mostly stability by arithmetic operations,
composition and ODE solving. We establish that generable functions are always
analytic. We prove that generable functions include some basic (useful)
generable functions, and that we can (uniformly) approximate a wide range of
functions this way. This extends some of the results from \cite{Sha41} to the
multidimensional case, and this also strengths the approximation result from
\cite{Sha41} over a compact domain to a uniform approximation result over
unbounded domains. We also discuss the issue of constants, and we prove that
involved constants can basically assumed to always be polynomial time
computable numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00547</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00547</id><created>2016-02-01</created><authors><author><keyname>Alamir</keyname><forenames>Mazen</forenames></author></authors><title>A New Contraction-Based NMPC Formulation Without Stability-Related
  terminal Constraints</title><categories>cs.SY</categories><comments>Submitted to IFAC Nolcos 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contraction-Based Nonlinear Model Predictive Control (NMPC) formulations are
attractive because of the generally short prediction horizons they require and
the needless use of terminal set computation that are commonly necessary to
guarantee stability. However, the inclusion of the contraction constraint in
the definition of the underlying optimization problem often leads to non
standard features such as the need for multi-step open-loop application of
control sequences or the use of multi-step memorization of the contraction
level that may induce unfeasibility in presence of unexpected disturbance. This
paper proposes a new formulation of contraction-based NMPC in which no
contraction constraint is explicitly involved. Convergence of the resulting
closed-loop behavior is proved under mild assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00554</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00554</id><created>2016-02-01</created><authors><author><keyname>Weghenkel</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author><author><keyname>Wiskott</keyname><forenames>Laurenz</forenames></author></authors><title>Graph-based Predictable Feature Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for the unsupervised extraction of predictable
features from high-dimensional time-series, where high predictability is
understood very generically as low variance in the distribution of the next
data point given the current one. We show how this objective can be understood
in terms of graph embedding as well as how it corresponds to the
information-theoretic measure of excess entropy in special cases.
Experimentally, we compare the approach to two other algorithms for the
extraction of predictable features, namely ForeCA and PFA, and show how it is
able to outperform them in certain settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00557</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00557</id><created>2016-02-01</created><authors><author><keyname>Ugalde</keyname><forenames>Unai</forenames></author><author><keyname>Anduaga</keyname><forenames>Javier</forenames></author><author><keyname>Martinez</keyname><forenames>Fernando</forenames></author><author><keyname>Iturrospe</keyname><forenames>Aitzol</forenames></author></authors><title>A SHM method for detecting damage with incomplete observations based on
  VARX modelling and Granger causality</title><categories>cs.SY</categories><comments>9 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A SHM method is proposed that minimises the required number of sensors for
detecting damage. The damage detection method consists of two steps. In an
initial characterization step, substructuring approach is applied to the
healthy structure in order to isolate the substructures of interest and later,
each substructure is identified by a Vector Auto Regressive with eXogenous
inputs (VARX) model measuring all DOFs. Then, pairwise conditional Granger
causality analysis is carried out with data measured from substructural DOFs to
evaluate the information loss when measurements from all DOFs are not
available. This analysis allows selecting those accelerometers that can be
suppressed minimising the information loss. In the evaluation phase, vibration
data from the reduced set of sensors is compared to the estimated data obtained
from the healthy substructure's VARX model, and as a result a damage indicator
is computed. The proposed detection method is validated by finite element
simulations in a lattice structure model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00563</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00563</id><created>2016-02-01</created><updated>2016-02-14</updated><authors><author><keyname>Bonifati</keyname><forenames>Angela</forenames></author><author><keyname>Ileana</keyname><forenames>Ioana</forenames></author><author><keyname>Linardi</keyname><forenames>Michele</forenames></author></authors><title>Functional Dependencies Unleashed for Scalable Data Exchange</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of efficiently evaluating target con- straints in the
data exchange process. In particular, we are interested in dealing with key
constraints and functional de- pendencies on the target schema, which can be
expressed as equality-generating dependencies (egds). The support of these
dependencies is limited in the state-of-the-art data ex- change engines, mainly
because considering them does not let leverage the performance of pure SQL.
Indeed, exist- ing approaches either rely on an all-SQL approach, further
assuming a set of source dependencies to rewrite the set of target functional
dependencies and source-to-target (s-t) dependencies into s-t dependencies, or,
if they go beyond SQL, they provide custom chase variants with limited scal-
ability. In this paper, we present a novel chase-based al- gorithm that can
efficiently handle arbitrary functional de- pendencies on the target. Our
approach essentially relies on a careful choice of ordering of the chase steps,
interleav- ing egds and (chosen) tgds. As a consequence, it leads to
considering smaller intermediate results during the chase as well as to
interesting parallelization opportunities, thus en- suring significant
scalability gains. We provide an efficient implementation of our chase-based
algorithm and an exper- imental study aiming at gauging its scalability with
respect to a number of parameters, among which the size of source instances and
the number of dependencies of each tested sce- nario. Finally, we empirically
compare with the latest data exchange engines, and show that our algorithm
outperforms them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00566</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00566</id><created>2016-02-01</created><authors><author><keyname>Katsaros</keyname><forenames>Konstantinos V.</forenames></author><author><keyname>Sourlas</keyname><forenames>Vasilis</forenames></author><author><keyname>Psaras</keyname><forenames>Ioannis</forenames></author><author><keyname>Rene</keyname><forenames>Sergi</forenames></author><author><keyname>Pavlou</keyname><forenames>George</forenames></author></authors><title>Information-Centric Connectivity</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices are often presented with multiple connectivity options usually
making a selection either randomly or based on load/wireless conditions
metrics, as is the case of current offloading schemes. In this paper we claim
that link-layer connectivity can be associated with information-availability
and in this respect connectivity decisions should be information-aware. This
constitutes a next step for the Information-Centric Networking paradigm,
realizing the concept of Information-Centric Connectivity (ICCON). We elaborate
on different types of information availability and connectivity decisions in
the context of ICCON, present specific use cases and discuss emerging
opportunities, challenges and technical approaches. We illustrate the potential
benefits of ICCON through preliminary simulation and numerical results in an
example use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00569</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00569</id><created>2016-02-01</created><authors><author><keyname>Kuhn</keyname><forenames>Nicolas</forenames></author><author><keyname>Ros</keyname><forenames>David</forenames></author></authors><title>Improving PIE's performance over high-delay paths</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bufferbloat is excessive latency due to over- provisioned network buffers.
PIE and CoDel are two recently proposed Active Queue Management (AQM)
algorithms, designed to tackle bufferbloat by lowering the queuing delay
without degrading the bottleneck utilization. PIE uses a proportional integral
controller to maintain the average queuing delay at a desired level; however,
large Round Trip Times (RTT) result in large spikes in queuing delays, which
induce high dropping probability and low utilization. To deal with this
problem, we propose Maximum and Average queuing Delay with PIE (MADPIE).
Loosely based on the drop policy used by CoDel to keep queuing delay bounded,
MADPIE is a simple extension to PIE that adds deterministic packet drops at
controlled intervals. By means of simulations, we observe that our proposed
change does not affect PIE's performance when RTT &lt; 100 ms. The deterministic
drops are more dominant when the RTT increases, which results in lower maximum
queuing delays and better performance for VoIP traffic and small file
downloads, with no major impact on bulk transfers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00572</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00572</id><created>2016-02-01</created><authors><author><keyname>Romero</keyname><forenames>Daniel M.</forenames></author><author><keyname>Uzzi</keyname><forenames>Brian</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author></authors><title>Social Networks Under Stress</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>12 pages, 8 figures, Proceedings of the 25th ACM International World
  Wide Web Conference (WWW) 2016</comments><doi>10.1145/2872427.2883063l</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network research has begun to take advantage of fine-grained
communications regarding coordination, decision-making, and knowledge sharing.
These studies, however, have not generally analyzed how external events are
associated with a social network's structure and communicative properties.
Here, we study how external events are associated with a network's change in
structure and communications. Analyzing a complete dataset of millions of
instant messages among the decision-makers in a large hedge fund and their
network of outside contacts, we investigate the link between price shocks,
network structure, and change in the affect and cognition of decision-makers
embedded in the network. When price shocks occur the communication network
tends not to display structural changes associated with adaptiveness. Rather,
the network &quot;turtles up&quot;. It displays a propensity for higher clustering,
strong tie interaction, and an intensification of insider vs. outsider
communication. Further, we find changes in network structure predict shifts in
cognitive and affective processes, execution of new transactions, and local
optimality of transactions better than prices, revealing the important
predictive relationship between network structure and collective behavior
within a social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00575</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00575</id><created>2016-02-01</created><authors><author><keyname>Li</keyname><forenames>Qunwei</forenames></author><author><keyname>Vempaty</keyname><forenames>Aditya</forenames></author><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Multi-object Classification via Crowdsourcing with a Reject Option</title><categories>cs.LG</categories><comments>single column, 33 pages, 8 figures, submitted to IEEE Trans. Signal
  Process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the design of an effective crowdsourcing system for an $M$-ary
classification task. Crowd workers complete simple binary microtasks whose
results are aggregated to give the final decision. We consider the novel
scenario where the workers have a reject option so that they are allowed to
skip microtasks when they are unable to or choose not to respond to binary
microtasks. For example, in mismatched speech transcription, crowd workers who
do not know the language may not be able to respond to certain microtasks
outside their categorical perception. We present an aggregation approach using
a weighted majority voting rule, where each worker's response is assigned an
optimized weight to maximize crowd's classification performance. We evaluate
system performance in both exact and asymptotic forms. Further, we consider the
case where there may be a set of greedy workers in the crowd for whom reward is
very important. Greedy workers may respond to a microtask even when they are
unable to perform it reliably. We consider an oblivious and an expurgation
strategy for crowdsourcing with greedy workers, and develop an algorithm to
adaptively switch between the two, based on the estimated fraction of greedy
workers in the anonymous crowd. Simulation results show performance improvement
of the proposed approaches compared with conventional majority voting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00577</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00577</id><created>2016-02-01</created><authors><author><keyname>Pan</keyname><forenames>Hengyue</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author></authors><title>A Deep Learning Based Fast Image Saliency Detection Algorithm</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.01173</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a fast deep learning method for object saliency
detection using convolutional neural networks. In our approach, we use a
gradient descent method to iteratively modify the input images based on the
pixel-wise gradients to reduce a pre-defined cost function, which is defined to
measure the class-specific objectness and clamp the class-irrelevant outputs to
maintain image background. The pixel-wise gradients can be efficiently computed
using the back-propagation algorithm. We further apply SLIC superpixels and LAB
color based low level saliency features to smooth and refine the gradients. Our
methods are quite computationally efficient, much faster than other deep
learning based saliency methods. Experimental results on two benchmark tasks,
namely Pascal VOC 2012 and MSRA10k, have shown that our proposed methods can
generate high-quality salience maps, at least comparable with many slow and
complicated deep learning methods. Comparing with the pure low-level methods,
our approach excels in handling many difficult images, which contain complex
background, highly-variable salient objects, multiple objects, and/or very
small salient objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00585</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00585</id><created>2016-02-01</created><authors><author><keyname>Wang</keyname><forenames>Yinong</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Burns</keyname><forenames>Joseph E.</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Improving Vertebra Segmentation through Joint Vertebra-Rib Atlases</title><categories>cs.CV</categories><comments>Manuscript to be presented at SPIE Medical Imaging 2016, 27 February
  - 3 March, 2016, San Diego, California, USA</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Accurate spine segmentation allows for improved identification and
quantitative characterization of abnormalities of the vertebra, such as
vertebral fractures. However, in existing automated vertebra segmentation
methods on computed tomography (CT) images, leakage into nearby bones such as
ribs occurs due to the close proximity of these visibly intense structures in a
3D CT volume. To reduce this error, we propose the use of joint vertebra-rib
atlases to improve the segmentation of vertebrae via multi-atlas joint label
fusion. Segmentation was performed and evaluated on CTs containing 106 thoracic
and lumbar vertebrae from 10 pathological and traumatic spine patients on an
individual vertebra level basis. Vertebra atlases produced errors where the
segmentation leaked into the ribs. The use of joint vertebra-rib atlases
produced a statistically significant increase in the Dice coefficient from 92.5
$\pm$ 3.1% to 93.8 $\pm$ 2.1% for the left and right transverse processes and a
decrease in the mean and max surface distance from 0.75 $\pm$ 0.60mm and 8.63
$\pm$ 4.44mm to 0.30 $\pm$ 0.27mm and 3.65 $\pm$ 2.87mm, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00586</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00586</id><created>2016-02-01</created><authors><author><keyname>Ferro</keyname><forenames>Mariza</forenames></author><author><keyname>Mury</keyname><forenames>Antonio R.</forenames></author><author><keyname>Schulze</keyname><forenames>Bruno</forenames></author></authors><title>A Gain Function for Architectural Decision-Making in Scientific
  Computing</title><categories>cs.DC cs.PF</categories><comments>25 pages, 1 figure, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific Computing typically requires large computational needs which have
been addressed with High Performance Distributed Computing. It is essential to
efficiently deploy a number of complex scientific applications, which have
different characteristics, and so require distinct computational resources too.
However, in many research laboratories, this high performance architecture is
not dedicated. So, the architecture must be shared to execute a set of
scientific applications, with so many different execution times and relative
importance to research. Also, the high performance architectures have different
characteristics and costs. When a new infrastructure has to be acquired to meet
the needs of this scenario, the decision-making is hard and complex. In this
work, we present a Gain Function as a model of an utility function, with which
it is possible a decision-making with confidence. With the function is possible
to evaluate the best architectural option taking into account aspects of
applications and architectures, including the executions time, cost of
architecture, the relative importance of each application and also the relative
importance of performance and cost on the final evaluation. This paper presents
the Gain Function, examples, and a real case showing their applicabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00591</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00591</id><created>2016-02-01</created><authors><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author></authors><title>NEXT: In-Network Nonconvex Optimization</title><categories>cs.DC cs.SY math.OC</categories><comments>To appear on IEEE Transactions on Signal and Information Processing
  over Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study nonconvex distributed optimization in multi-agent networks with
time-varying (nonsymmetric) connectivity. We introduce the first algorithmic
framework for the distributed minimization of the sum of a smooth (possibly
nonconvex and nonseparable) function - the agents' sum-utility - plus a convex
(possibly nonsmooth and nonseparable) regularizer. The latter is usually
employed to enforce some structure in the solution, typically sparsity. The
proposed method hinges on successive convex approximation techniques while
leveraging dynamic consensus as a mechanism to distribute the computation among
the agents: each agent first solves (possibly inexactly) a local convex
approximation of the nonconvex original problem, and then performs local
averaging operations. Asymptotic convergence to (stationary) solutions of the
nonconvex problem is established. Our algorithmic framework is then customized
to a variety of convex and nonconvex problems in several fields, including
signal processing, communications, networking, and machine learning. Numerical
results show that the new method compares favorably to existing distributed
algorithms on both convex and nonconvex problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00596</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00596</id><created>2016-02-01</created><authors><author><keyname>Mokhtari</keyname><forenames>Aryan</forenames></author><author><keyname>Shi</keyname><forenames>Wei</forenames></author><author><keyname>Ling</keyname><forenames>Qing</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>A Decentralized Second-Order Method with Exact Linear Convergence Rate
  for Consensus Optimization</title><categories>math.OC cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers decentralized consensus optimization problems where
different summands of a global objective function are available at nodes of a
network that can communicate with neighbors only. The proximal method of
multipliers is considered as a powerful tool that relies on proximal primal
descent and dual ascent updates on a suitably defined augmented Lagrangian. The
structure of the augmented Lagrangian makes this problem non-decomposable,
which precludes distributed implementations. This problem is regularly
addressed by the use of the alternating direction method of multipliers. The
exact second order method (ESOM) is introduced here as an alternative that
relies on: (i) The use of a separable quadratic approximation of the augmented
Lagrangian. (ii) A truncated Taylor's series to estimate the solution of the
first order condition imposed on the minimization of the quadratic
approximation of the augmented Lagrangian. The sequences of primal and dual
variables generated by ESOM are shown to converge linearly to their optimal
arguments when the aggregate cost function is strongly convex and its gradients
are Lipschitz continuous. Numerical results demonstrate advantages of ESOM
relative to decentralized alternatives in solving least squares and logistic
regression problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00602</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00602</id><created>2016-02-01</created><authors><author><keyname>Barrett</keyname><forenames>Edd</forenames></author><author><keyname>Bolz</keyname><forenames>Carl Friedrich</forenames></author><author><keyname>Killick</keyname><forenames>Rebecca</forenames></author><author><keyname>Knight</keyname><forenames>Vincent</forenames></author><author><keyname>Mount</keyname><forenames>Sarah</forenames></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames></author></authors><title>Virtual Machine Warmup Blows Hot and Cold</title><categories>cs.PL</categories><comments>13 pages, 8 figures</comments><acm-class>D.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Virtual Machines (VMs) with Just-In-Time (JIT) compilers are traditionally
thought to execute programs in two phases: first the warmup phase determines
which parts of a program would most benefit from dynamic compilation; after
compilation has occurred the program is said to be at peak performance. When
measuring the performance of JIT compiling VMs, data collected during the
warmup phase is generally discarded, placing the focus on peak performance. In
this paper we run a number of small, deterministic benchmarks on a variety of
well known VMs. In our experiment, less than one quarter of the benchmark/VM
pairs conform to the traditional notion of warmup, and none of the VMs we
tested consistently warms up in the traditional notion. This raises a number of
questions about VM benchmarking, which are of interest to both VM authors and
end users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00615</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00615</id><created>2016-01-29</created><authors><author><keyname>Woodruff</keyname><forenames>William</forenames></author></authors><title>EMFS: Repurposing SMTP and IMAP for Data Storage and Synchronization</title><categories>cs.NI</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud storage has become a massive and lucrative business, with companies
like Apple, Microsoft, Google, and Dropbox providing hundreds of millions of
clients with synchronized and redundant storage. These services often command
price-to-storage ratios significantly higher than the market rate for physical
storage, as well as increase the surface area for data leakage. In place of
this consumer-unfriendly status quo, I propose using widely available, well
standardized email protocols like SMTP and IMAP in conjunction with free email
service providers to store, synchronize, and share files across discrete
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00621</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00621</id><created>2016-02-01</created><authors><author><keyname>Nicolae</keyname><forenames>Marius</forenames></author><author><keyname>Rajasekaran</keyname><forenames>Sanguthevar</forenames></author></authors><title>On pattern matching with k mismatches and few don't cares</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>I.5.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider pattern matching with $k$ mismatches, with don't care symbols in
the pattern. Given a pattern $P$ of length $m$ and a text $T$ of length $n$, we
want to find all occurrences of $P$ in $T$ that have no more than $k$
mismatches. Without don't cares, the best known algorithm has a runtime of
$O(n\sqrt{k \log k})$. With don't cares in the pattern, the best deterministic
algorithm has a runtime of $O(n\sqrt[3]{mk\log^2m})$. Therefore, there is a
considerable gap between the best known runtime for the case without don't
cares and the case with don't cares.
  In this paper we reduce the gap between the two versions by giving an
algorithm whose runtime increases with the number of don't cares. We define an
island to be a maximal length substring of $P$ that does not contain don't
cares. Let $q$ be the number of islands in $P$. We present an algorithm that
runs in $O(n\sqrt{k\log m}+n\min\{\sqrt[3]{qk\log^2 m},\sqrt{q\log m}\})$ time.
If the number of islands $q$ is $O(k)$ this runtime becomes $O(n\sqrt{k\log
m})$, which essentially matches the best known runtime for pattern matching
with $k$ mismatches without don't cares. If the number of islands $q$ is
$o(m)$, this algorithm is asymptotically faster than the previous best
algorithm for pattern matching with $k$ mismatches with don't cares in the
pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00639</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00639</id><created>2016-02-01</created><authors><author><keyname>Lee</keyname><forenames>Gilsoo</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Mehbodniya</keyname><forenames>Abolfazl</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Online Ski Rental for ON/OFF Scheduling of Energy Harvesting, Millimeter
  Wave Base Stations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The co-existence of millimeter wave (mmW) small cell base stations (SBSs)
with conventional microwave macrocell base stations is a promising approach to
boost the capacity and coverage of cellular networks. However, densifying the
network with a viral deployment of mmW SBSs can significantly increase energy
consumption. To reduce the reliance on unsustainable energy sources, one can
adopt self-powered, mmW SBSs that rely solely on energy harvesting. Due to the
uncertainty of energy arrival and the finite capacity of energy storage
systems, self-powered SBSs must smartly optimize their ON and OFF schedule. In
this paper, the problem of ON/OFF scheduling of self-powered mmW SBSs is
studied, in the presence of energy harvesting uncertainty with the goal of
minimizing the power consumption and transmission delay of a network. To solve
this problem, a novel approach based on the ski rental framework, a powerful
online optimization tool, is proposed. Using this approach, each SBS can
effectively decide on its ON/OFF schedule autonomously, without any prior
information on future energy arrivals. By using competitive analysis, a
deterministic algorithm (DOA) and a randomized online algorithm (ROA) are
developed. The proposed ROA is then shown to achieve the optimal competitive
ratio. Simulation results show that, compared to the DOA and a baseline
approach, the ROA can yield significant performance gains reaching,
respectively, up to 31.8% and 25.3% in terms of reduced power consumption and
up to 41.5% and 36.8%, in terms of delay reduction. The results also shed light
on the factors that affect the ON time of SBSs while demonstrating how the
proposed ROA can eliminate up to 96.2% of the ON/OFF switching overhead
compared to a baseline approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00646</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00646</id><created>2016-02-01</created><updated>2016-02-22</updated><authors><author><keyname>Hoffmann</keyname><forenames>Ruth</forenames></author><author><keyname>Ireland</keyname><forenames>Murray</forenames></author><author><keyname>Miller</keyname><forenames>Alice</forenames></author><author><keyname>Norman</keyname><forenames>Gethin</forenames></author><author><keyname>Veres</keyname><forenames>Sandor</forenames></author></authors><title>Autonomous Agent Behaviour Modelled in PRISM -- A Case Study</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal verification of agents representing robot behaviour is a growing area
due to the demand that autonomous systems have to be proven safe. In this paper
we present an abstract definition of autonomy which can be used to model
autonomous scenarios and propose the use of small-scale simulation models
representing abstract actions to infer quantitative data. To demonstrate the
applicability of the approach we build and verify a model of an unmanned aerial
vehicle (UAV) in an exemplary autonomous scenario, utilising this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00648</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00648</id><created>2016-02-01</created><authors><author><keyname>Rajatheva</keyname><forenames>Namal</forenames></author><author><keyname>Sousa</keyname><forenames>Elvino</forenames></author></authors><title>Hybrid Beamforming for Wireless Backhaul in Two-Tier Networks (Working
  Title)</title><categories>cs.IT math.IT</categories><comments>In process of submission for Globecomm, prior work done on thesis at
  https://tspace.library.utoronto.ca/handle/1807/70521</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The uplink where both the transmitter and receiver can use a large antenna
array is studied, proposed as a method of antenna offloading (active relay) and
connecting small cell access points (SCAP) in a Two-Tier cellular network. Due
to having a limited number of RF-chains, hybrid beamformers are designed where
phase-only processing is done at the RF-band, followed by traditional digital
processing at the baseband. For the receiver, a row combiner that clusters
groups of antenna elements sufficiently correlated, and compared against a
random projection scheme that follows a Discrete Fourier Transform matrix
structure. The analogue to the row combiner is a column spreader which is
dependent on the transmit correlation and repeats the transmitted signal over
antenna elements that are correlated. A key benefit of this approach is to
reduce the number of phase shifters used. When only the transmitter has
correlation and RF-chain limited, the baseband precoding vectors are the
eigenvectors of the effective transmit correlation matrix. Depending on the
correlation levels, this matrix can be approximated to have a tridiagonal
Toeplitz structure. The eigenvalues and associated eigenvectors have a closed
form solution, which leads to simplified calculation of the precoding vectors
and resulting sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00651</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00651</id><created>2016-02-01</created><authors><author><keyname>Jeannerod</keyname><forenames>Claude-Pierre</forenames></author><author><keyname>Neiger</keyname><forenames>Vincent</forenames></author><author><keyname>Schost</keyname><forenames>Eric</forenames></author><author><keyname>Villard</keyname><forenames>Gilles</forenames></author></authors><title>Fast computation of minimal interpolation bases in Popov form for
  arbitrary shifts</title><categories>cs.SC</categories><comments>19 pages, 4 figures (problems and algorithms)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute minimal bases of solutions for a general interpolation problem, a
particular case of which is Hermite-Pad\'e approximation. The problem asks to
find univariate polynomial relations between $m$ vectors of size $\sigma$;
these relations should have small degree with respect to an input degree shift
$s \in \mathbb{Z}^m$. For an arbitrary input shift, we propose an algorithm for
the computation of an interpolation basis in shifted Popov form with a cost of
$O\tilde{~}( m^{w-1} \sigma )$ operations in the base field, where $w$ is the
exponent of matrix multiplication, and the notation $O\tilde{~}(\cdot)$ omits
logarithmic terms.
  Earlier works, in the case of Hermite-Pad\'e approximation [Zhou and Labahn,
2012] and in the general interpolation case [Jeannerod et al., 2015], compute
non-normalized bases. Since for arbitrary shifts such bases may have size
$\Theta( m^2 \sigma )$, the cost bound $O\tilde{~}( m^{w-1} \sigma )$ was
feasible only with restrictive assumptions on $s$ for ensuring small output
sizes. The question of handling arbitrary shifts with the same complexity bound
was left open.
  To obtain the target cost for any shift, we strengthen the properties of the
output bases, and of those obtained during the course of the algorithm: all the
bases are computed in shifted Popov form, whose size is always $O( m \sigma )$.
Then, we design a divide-and-conquer scheme. We recursively reduce the initial
interpolation problem to sub-problems with more convenient shifts by first
computing information on the degrees of the intermediary bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00661</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00661</id><created>2016-02-01</created><authors><author><keyname>De Ridder</keyname><forenames>Simon</forenames></author><author><keyname>Vandermarliere</keyname><forenames>Benjamin</forenames></author><author><keyname>Ryckebusch</keyname><forenames>Jan</forenames></author></authors><title>Detection and localization of change points in temporal networks with
  the aid of stochastic block models</title><categories>cs.SI physics.data-an physics.soc-ph stat.ME</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework based on generalized hierarchical random graphs (GHRGs) for the
detection of change points in the structure of temporal networks has recently
been developed by Peel and Clauset [1]. We build on this methodology and extend
it to also include the versatile stochastic block models (SBMs) as a parametric
family for reconstructing the empirical networks. We use five different
techniques for change point detection on three prototypical empirical temporal
networks. We find that none of the considered methods can consistently
outperform the others when it comes to detecting and locating the change
points. With respect to the precision and the recall of the results of the
change points, we find that the method based on a degree-corrected SBM has
better recall properties than other dedicated methods, especially for sparse
networks and smaller sliding time window widths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00678</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00678</id><created>2016-02-01</created><authors><author><keyname>Balasubramanian</keyname><forenames>Vivekanandan</forenames></author><author><keyname>Treikalis</keyname><forenames>Antons</forenames></author><author><keyname>Weidner</keyname><forenames>Ole</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>EnsembleMD Toolkit: Scalable and Flexible Execution of Ensembles of
  Molecular Simulations</title><categories>cs.DC</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many applications such as molecular science simulations that
require scalable task-level parallelism and support for the flexible execution
and coupling of ensembles of simulations. Most high-performance system software
and middleware however, are designed to support the execution and optimization
of single tasks. Motivated by the missing capabilities of these computing
systems and the increasing importance of task-level parallelism, we introduce
the Ensemble-MD toolkit which has the following application development
features: (i) abstractions that enable the expression of ensembles as primary
entities, and (ii) support for ensemble-based execution patterns that capture
the majority of application scenarios. Ensemble-MD toolkit uses a scalable
pilot-based runtime system that, (i) decouples workload execution and resource
management details from the expression of the application, and (ii) enables the
efficient and dynamic execution of ensembles on heterogeneous computing
resources. We investigate three execution patterns and characterize the
scalability and overhead of Ensemble-MD toolkit for these patterns. We
investigate scaling properties for up to O(1000) concurrent ensembles and
O(1000) cores and find linear weak and strong scaling behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00700</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00700</id><created>2016-02-01</created><authors><author><keyname>Brake</keyname><forenames>Daniel A.</forenames></author><author><keyname>Hauenstein</keyname><forenames>Jonathan D.</forenames></author><author><keyname>Liddell</keyname><forenames>Alan C.</forenames></author></authors><title>Numerically validating the completeness of the real solution set of a
  system of polynomial equations</title><categories>math.NA cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the real solutions to a system of polynomial equations is a
challenging problem, particularly verifying that all solutions have been
computed. We describe an approach that combines numerical algebraic geometry
and sums of squares programming to test whether a given set is &quot;complete&quot; with
respect to the real solution set. Specifically, we test whether the Zariski
closure of that given set is indeed equal to the solution set of the real
radical of the ideal generated by the given polynomials. Examples with finitely
and infinitely many real solutions are provided, along with an example having
polynomial inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00709</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00709</id><created>2016-01-29</created><authors><author><keyname>da Silva</keyname><forenames>Adenilton J.</forenames></author><author><keyname>Ludermir</keyname><forenames>Teresa B.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Wilson R.</forenames></author></authors><title>Quantum perceptron over a field and neural network architecture
  selection in a quantum computer</title><categories>quant-ph cs.NE</categories><journal-ref>Neural Networks, Volume 76, April 2016, Pages 55-64</journal-ref><doi>10.1016/j.neunet.2016.01.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a quantum neural network named quantum perceptron
over a field (QPF). Quantum computers are not yet a reality and the models and
algorithms proposed in this work cannot be simulated in actual (or classical)
computers. QPF is a direct generalization of a classical perceptron and solves
some drawbacks found in previous models of quantum perceptrons. We also present
a learning algorithm named Superposition based Architecture Learning algorithm
(SAL) that optimizes the neural network weights and architectures. SAL searches
for the best architecture in a finite set of neural network architectures with
linear time over the number of patterns in the training set. SAL is the first
learning algorithm to determine neural network architectures in polynomial
time. This speedup is obtained by the use of quantum parallelism and a
non-linear quantum operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00710</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00710</id><created>2016-02-01</created><authors><author><keyname>Neiger</keyname><forenames>Vincent</forenames></author></authors><title>Fast computation of shifted Popov forms of polynomial matrices via
  systems of modular polynomial equations</title><categories>cs.SC</categories><comments>22 pages, 6 figures (problems and algorithms)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a Las Vegas algorithm which computes the shifted Popov form of a
nonsingular polynomial matrix $A \in \mathbb{K}^{ m \times m }$ in expected
$O\tilde{~}( m^w \lceil \sigma(A) / m \rceil ) \subseteq O\tilde{~}( m^w deg(A)
)$ operations in $\mathbb{K}$, where $deg(A)$ is the degree of $A$, $\sigma(A)$
is some quantity such that $\sigma(A) / m$ is bounded from above by both the
average row degree and the average column degree of $A$, $w$ is the exponent of
matrix multiplication, and $O\tilde{~}( \cdot )$ indicates that logarithmic
factors are omitted. This improves upon the cost bound of the fastest known
algorithms for row reduction and Hermite form computation, which are
deterministic. This is the first algorithm for shifted row reduction with cost
bound $O\tilde{~}( m^w deg(A) )$ for an arbitrary shift.
  This algorithm uses partial linearization to reduce to the case $deg(A) \le
\sigma(A)$, and builds a system of modular equations whose solution set is the
row space of $A$. It remains to find the basis in shifted Popov form of this
solution set: we give a deterministic algorithm for this problem in
$O\tilde{~}( m^{w-1} \sigma )$ operations, where $m$ is the number of unknowns
and $\sigma$ is the sum of the degrees of the moduli. This extends previous
results with the same cost bound in the specific cases of order basis
computation and M-Pad\'e approximation, in which the moduli are products of
known linear factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00715</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00715</id><created>2016-02-01</created><authors><author><keyname>Chan</keyname><forenames>Stanley H.</forenames></author></authors><title>Algorithm-Induced Prior for Image Restoration</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a type of image priors that are constructed implicitly
through the alternating direction method of multiplier (ADMM) algorithm, called
the algorithm-induced prior. Different from classical image priors which are
defined before running the reconstruction algorithm, algorithm-induced priors
are defined by the denoising procedure used to replace one of the two modules
in the ADMM algorithm. Since such prior is not explicitly defined, analyzing
the performance has been difficult in the past.
  Focusing on the class of symmetric smoothing filters, this paper presents an
explicit expression of the prior induced by the ADMM algorithm. The new prior
is reminiscent to the conventional graph Laplacian but with stronger
reconstruction performance. It can also be shown that the overall
reconstruction has an efficient closed-form implementation if the associated
symmetric smoothing filter is low rank. The results are validated with
experiments on image inpainting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00721</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00721</id><created>2016-02-01</created><authors><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>Concentration of measure without independence: a unified approach via
  the martingale method</title><categories>math.PR cs.IT math.IT</categories><comments>Draft of a chapter for IMA Volume &quot;Discrete Structures: Analysis and
  Applications&quot; (Springer)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concentration of measure phenomenon may be summarized as follows: a
function of many weakly dependent random variables that is not too sensitive to
any of its individual arguments will tend to take values very close to its
expectation. This phenomenon is most completely understood when the arguments
are mutually independent random variables, and there exist several powerful
complementary methods for proving concentration inequalities, such as the
martingale method, the entropy method, and the method of transportation
inequalities. The setting of dependent arguments is much less well understood.
This chapter focuses on the martingale method for deriving concentration
inequalities without independence assumptions. In particular, we use the
machinery of so-called Wasserstein matrices to show that the Azuma-Hoeffding
concentration inequality for martingales with almost surely bounded
differences, when applied in a sufficiently abstract setting, is powerful
enough to recover and sharpen several known concentration results for
nonproduct measures. Wasserstein matrices provide a natural formalism for
capturing the interplay between the metric and the probabilistic structures,
which is fundamental to the concentration phenomenon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00722</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00722</id><created>2016-02-01</created><authors><author><keyname>Chang</keyname><forenames>Kevin K.</forenames></author><author><keyname>Loh</keyname><forenames>Gabriel H.</forenames></author><author><keyname>Thottethodi</keyname><forenames>Mithuna</forenames></author><author><keyname>Eckert</keyname><forenames>Yasuko</forenames></author><author><keyname>O'Connor</keyname><forenames>Mike</forenames></author><author><keyname>Manne</keyname><forenames>Srilatha</forenames></author><author><keyname>Hsu</keyname><forenames>Lisa</forenames></author><author><keyname>Subramanian</keyname><forenames>Lavanya</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Enabling Efficient Dynamic Resizing of Large DRAM Caches via A Hardware
  Consistent Hashing Mechanism</title><categories>cs.AR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Die-stacked DRAM has been proposed for use as a large, high-bandwidth,
last-level cache with hundreds or thousands of megabytes of capacity. Not all
workloads (or phases) can productively utilize this much cache space, however.
Unfortunately, the unused (or under-used) cache continues to consume power due
to leakage in the peripheral circuitry and periodic DRAM refresh. Dynamically
adjusting the available DRAM cache capacity could largely eliminate this energy
overhead. However, the current proposed DRAM cache organization introduces new
challenges for dynamic cache resizing. The organization differs from a
conventional SRAM cache organization because it places entire cache sets and
their tags within a single bank to reduce on-chip area and power overhead.
Hence, resizing a DRAM cache requires remapping sets from the powered-down
banks to active banks.
  In this paper, we propose CRUNCH (Cache Resizing Using Native Consistent
Hashing), a hardware data remapping scheme inspired by consistent hashing, an
algorithm originally proposed to uniformly and dynamically distribute Internet
traffic across a changing population of web servers. CRUNCH provides a
load-balanced remapping of data from the powered-down banks alone to the active
banks, without requiring sets from all banks to be remapped, unlike naive
schemes to achieve load balancing. CRUNCH remaps only sets from the
powered-down banks, so it achieves this load balancing with low bank
power-up/down transition latencies. CRUNCH's combination of good load balancing
and low transition latencies provides a substrate to enable efficient DRAM
cache resizing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00729</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00729</id><created>2016-02-01</created><authors><author><keyname>Luo</keyname><forenames>Yixin</forenames></author><author><keyname>Govindan</keyname><forenames>Sriram</forenames></author><author><keyname>Sharma</keyname><forenames>Bikash</forenames></author><author><keyname>Santaniello</keyname><forenames>Mark</forenames></author><author><keyname>Meza</keyname><forenames>Justin</forenames></author><author><keyname>Kansal</keyname><forenames>Aman</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Khessib</keyname><forenames>Badriddine</forenames></author><author><keyname>Vaid</keyname><forenames>Kushagra</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Heterogeneous-Reliability Memory: Exploiting Application-Level Memory
  Error Tolerance</title><categories>cs.DC</categories><comments>4 pages, 4 figures, summary report for DSN 2014 paper:
  &quot;Characterizing Application Memory Error Vulnerability to Optimize Datacenter
  Cost via Heterogeneous-Reliability Memory&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory devices represent a key component of datacenter total cost of
ownership (TCO), and techniques used to reduce errors that occur on these
devices increase this cost. Existing approaches to providing reliability for
memory devices pessimistically treat all data as equally vulnerable to memory
errors. Our key insight is that there exists a diverse spectrum of tolerance to
memory errors in new data-intensive applications, and that traditional
one-size-fits-all memory reliability techniques are inefficient in terms of
cost. For example, we found that while traditional error protection increases
memory system cost by 12.5%, some applications can achieve 99.00% availability
on a single server with a large number of memory errors without any error
protection. This presents an opportunity to greatly reduce server hardware cost
by provisioning the right amount of memory reliability for different
applications.
  Toward this end, in this paper, we make three main contributions to enable
highly-reliable servers at low datacenter cost. First, we develop a new
methodology to quantify the tolerance of applications to memory errors. Second,
using our methodology, we perform a case study of three new data-intensive
workloads (an interactive web search application, an in-memory key-value store,
and a graph mining framework) to identify new insights into the nature of
application memory error vulnerability. Third, based on our insights, we
propose several new hardware/software heterogeneous-reliability memory system
designs to lower datacenter cost while achieving high reliability and discuss
their trade-offs. We show that our new techniques can reduce server hardware
cost by 4.7% while achieving 99.90% single server availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00733</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00733</id><created>2016-02-01</created><authors><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Raymond</keyname><forenames>Jean-Florent</forenames></author><author><keyname>Trunck</keyname><forenames>Th&#xe9;ophile</forenames></author></authors><title>Well-quasi-ordering H-contraction-free graphs</title><categories>math.CO cs.DM</categories><comments>16 pages</comments><msc-class>06A07</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-quasi-order is an order which contains no infinite decreasing sequence
and no infinite collection of incomparable elements. In this paper, we consider
graph classes defined by excluding one graph as contraction. More precisely, we
give a complete characterization of graphs H such that the class of
H-contraction-free graphs is well-quasi-ordered by the contraction relation.
This result is the contraction analogue on the previous dichotomy theorems of
Damsaschke [Induced subgraphs and well-quasi-ordering, Journal of Graph Theory,
14(4):427-435, 1990] on the induced subgraph relation, Ding [Subgraphs and
well-quasi-ordering, Journal of Graph Theory, 16(5):489-502, 1992] on the
subgraph relation, and B{\l}asiok et al. [Induced minors and
well-quasi-ordering, ArXiv e-prints, 1510.07135, 2015] on the induced minor
relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00734</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00734</id><created>2016-02-01</created><authors><author><keyname>Li</keyname><forenames>Yen-Huan</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Learning Data Triage: Linear Decoding Works for Compressive MRI</title><categories>cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard approach to compressive sampling considers recovering an unknown
deterministic signal with certain known structure, and designing the
sub-sampling pattern and recovery algorithm based on the known structure. This
approach requires looking for a good representation that reveals the signal
structure, and solving a non-smooth convex minimization problem (e.g., basis
pursuit). In this paper, another approach is considered: We learn a good
sub-sampling pattern based on available training signals, without knowing the
signal structure in advance, and reconstruct an accordingly sub-sampled signal
by computationally much cheaper linear reconstruction. We provide a theoretical
guarantee on the recovery error, and show via experiments on real-world MRI
data the effectiveness of the proposed compressive MRI scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00739</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00739</id><created>2016-02-01</created><authors><author><keyname>Bergomi</keyname><forenames>Mattia G.</forenames></author><author><keyname>Barat&#xe9;</keyname><forenames>Adriano</forenames></author><author><keyname>Di Fabio</keyname><forenames>Barbara</forenames></author></authors><title>Towards a topological fingerprint of music</title><categories>cs.SD cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can music be represented as a meaningful geometric and topological object? In
this paper, we propose a strategy to describe some music features as a
polyhedral surface obtained by a simplicial interpretation of the
\textit{Tonnetz}. The \textit{Tonnetz} is a graph largely used in computational
musicology to describe the harmonic relationships of notes in equal tuning. In
particular, we use persistent homology in order to describe the
\textit{persistent} properties of music encoded in the aforementioned model.
Both the relevance and the characteristics of this approach are discussed by
analyzing some paradigmatic compositional styles. Eventually, the task of
automatic music style classification is addressed by computing the hierarchical
clustering of the topological fingerprints associated with some collections of
compositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00749</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00749</id><created>2016-02-01</created><authors><author><keyname>Wang</keyname><forenames>Pichao</forenames></author><author><keyname>Li</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Hou</keyname><forenames>Yonghong</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author></authors><title>Combining ConvNets with Hand-Crafted Features for Action Recognition
  Based on an HMM-SVM Classifier</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new framework for RGB-D-based action recognition that
takes advantages of hand-designed features from skeleton data and deeply
learned features from depth maps, and exploits effectively both the local and
global temporal information. Specifically, depth and skeleton data are firstly
augmented for deep learning and making the recognition insensitive to view
variance. Secondly, depth sequences are segmented using the hand-crafted
features based on skeleton joints motion histogram to exploit the local
temporal information. All training se gments are clustered using an Infinite
Gaussian Mixture Model (IGMM) through Bayesian estimation and labelled for
training Convolutional Neural Networks (ConvNets) on the depth maps. Thus, a
depth sequence can be reliably encoded into a sequence of segment labels.
Finally, the sequence of labels is fed into a joint Hidden Markov Model and
Support Vector Machine (HMM-SVM) classifier to explore the global temporal
information for final recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00753</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00753</id><created>2016-02-01</created><authors><author><keyname>Bagherinezhad</keyname><forenames>Hessam</forenames></author><author><keyname>Hajishirzi</keyname><forenames>Hannaneh</forenames></author><author><keyname>Choi</keyname><forenames>Yejin</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author></authors><title>Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects</title><categories>cs.AI cs.CV</categories><comments>To appear in AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human vision greatly benefits from the information about sizes of objects.
The role of size in several visual reasoning tasks has been thoroughly explored
in human perception and cognition. However, the impact of the information about
sizes of objects is yet to be determined in AI. We postulate that this is
mainly attributed to the lack of a comprehensive repository of size
information. In this paper, we introduce a method to automatically infer object
sizes, leveraging visual and textual information from web. By maximizing the
joint likelihood of textual and visual observations, our method learns reliable
relative size estimates, with no explicit human supervision. We introduce the
relative size dataset and show that our method outperforms competitive textual
and visual baselines in reasoning about size comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00761</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00761</id><created>2016-02-01</created><authors><author><keyname>Ranganathan</keyname><forenames>Sudarsan V. S.</forenames></author><author><keyname>Mu</keyname><forenames>Tong</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Optimality and Rate-Compatibility for Erasure-Coded Packet Transmissions
  when Fading Channel Diversity Increases with Packet Length</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A message composed of packets is transmitted using erasure and channel coding
over a fading channel with no feedback. For this scenario, the paper explores
the trade-off between the redundancies allocated to the packet-level erasure
code and the channel code, along with an objective of a low probability of
failure to recover the message.
  To this end, we consider a fading model that we term proportional-diversity
block fading (PD block fading). For a fixed overall code rate and transmit
power, we formulate an optimization problem to numerically find the optimal
channel-coding rate (and thus the optimal erasure-coding rate) that minimizes
the probability of failure for various approximations of the problem.
  Furthermore, an interpretation of the results from an incremental redundancy
point of view shows how rate-compatibility affects the possible trajectories of
the failure probability as a function of the overall code rate. Our numerical
results suggest that an optimal, rateless, hybrid coding scheme for a
single-user wireless system over the PD block-fading channel should have the
rate of the erasure code approach one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00763</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00763</id><created>2016-02-01</created><authors><author><keyname>Bewley</keyname><forenames>Alex</forenames></author><author><keyname>Ge</keyname><forenames>Zongyuan</forenames></author><author><keyname>Ott</keyname><forenames>Lionel</forenames></author><author><keyname>Ramos</keyname><forenames>Fabio</forenames></author><author><keyname>Upcroft</keyname><forenames>Ben</forenames></author></authors><title>Simple Online and Realtime Tracking</title><categories>cs.CV</categories><comments>5 pages, 1 figure, short paper, formatted for the international
  conference on image processing ICIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores a pragmatic approach to multiple object tracking where
the main focus is to associate objects efficiently for online and realtime
applications. To this end, detection quality is identified as a key factor
influencing tracking performance, where changing the detector can improve
tracking by up to 18.9%. Despite only using a rudimentary combination of
familiar techniques such as the Kalman Filter and Hungarian algorithm for the
tracking components, this approach achieves an accuracy comparable to
state-of-the-art online trackers. Furthermore, due to the simplicity of our
tracking method, the tracker updates at a rate of 260 Hz which is over 20x
faster than other state-of-the-art trackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00766</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00766</id><created>2016-02-01</created><authors><author><keyname>Yan</keyname><forenames>Shi</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author></authors><title>User Access Mode Selection in Fog Computing Based Radio Access Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fog computing based radio access network is a promising paradigm for the
fifth generation wireless communication system to provide high spectral and
energy efficiency. With the help of the new designed fog computing based access
points (F-APs), the user-centric objectives can be achieved through the
adaptive technique and will relieve the load of fronthaul and alleviate the
burden of base band unit pool. In this paper, we derive the coverage
probability and ergodic rate for both F-AP users and device-to-device users by
taking into account the different nodes locations, cache sizes as well as user
access modes. Particularly, the stochastic geometry tool is used to derive
expressions for above performance metrics. Simulation results validate the
accuracy of our analysis and we obtain interesting trade-offs that depend on
the effect of the cache size, user node density, and the quality of service
constrains on the different performance metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00767</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00767</id><created>2016-02-01</created><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>de Berg</keyname><forenames>Mark</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Roeloffzen</keyname><forenames>Marcel</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author></authors><title>Distance-Sensitive Planar Point Location</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathcal{S}$ be a connected planar polygonal subdivision with $n$ edges
that we want to preprocess for point-location queries, and where we are given
the probability $\gamma_i$ that the query point lies in a polygon $P_i$ of
$\mathcal{S}$. We show how to preprocess $\mathcal{S}$ such that the query time
for a point~$p\in P_i$ depends on~$\gamma_i$ and, in addition, on the distance
from $p$ to the boundary of~$P_i$---the further away from the boundary, the
faster the query. More precisely, we show that a point-location query can be
answered in time $O\left(\min \left(\log n, 1 + \log
\frac{\mathrm{area}(P_i)}{\gamma_i \Delta_{p}^2}\right)\right)$, where
$\Delta_{p}$ is the shortest Euclidean distance of the query point~$p$ to the
boundary of $P_i$. Our structure uses $O(n)$ space and $O(n \log n)$
preprocessing time. It is based on a decomposition of the regions of
$\mathcal{S}$ into convex quadrilaterals and triangles with the following
property: for any point $p\in P_i$, the quadrilateral or triangle
containing~$p$ has area $\Omega(\Delta_{p}^2)$. For the special case where
$\mathcal{S}$ is a subdivision of the unit square and
$\gamma_i=\mathrm{area}(P_i)$, we present a simpler solution that achieves a
query time of $O\left(\min \left(\log n, \log
\frac{1}{\Delta_{p}^2}\right)\right)$. The latter solution can be extended to
convex subdivisions in three dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00773</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00773</id><created>2016-02-01</created><authors><author><keyname>Moffitt</keyname><forenames>Vera Zaychik</forenames></author><author><keyname>Stoyanovich</keyname><forenames>Julia</forenames></author></authors><title>Portal: A Query Language for Evolving Graphs</title><categories>cs.DB cs.DC</categories><comments>12 pages plus appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are used to represent a plethora of phenomena, from the Web and social
networks, to biological pathways, to semantic knowledge bases. Arguably the
most interesting and important questions one can ask about graphs have to do
with their evolution. Which Web pages are showing an increasing popularity
trend? How does influence propagate in social networks? How does knowledge
evolve?
  Much research and engineering effort today goes into developing sophisticated
graph analytics and their efficient implementations, both stand-alone and in
scope of data processing platforms. Yet, systematic support for scalable
querying and analytics over evolving graphs still lacks.
  In this paper we present Portal, a declarative language that supports
efficient querying and exploratory analysis of evolving graphs, and an
implementation of Portal in scope of Apache Spark, an open-source distributed
data processing framework. Our language supports a variety of operations
including temporal selection, join and aggregation, and a rich class of
analytics. We develop multiple physical representations and partitioning
strategies and study the trade-offs between structural and temporal locality.
We provide an extensive experimental evaluation of our system, demonstrating
that careful engineering can lead to good performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00786</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00786</id><created>2016-02-01</created><authors><author><keyname>&#x10c;ern&#xfd;</keyname><forenames>Pavol</forenames><affiliation>University of Colorado Boulder</affiliation></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames><affiliation>EPFL</affiliation></author><author><keyname>Parthasarathy</keyname><forenames>Madhusudan</forenames><affiliation>University of Illinois Urbana-Champaign</affiliation></author></authors><title>Proceedings Fourth Workshop on Synthesis</title><categories>cs.PL cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 202, 2016</journal-ref><doi>10.4204/EPTCS.202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SYNT workshop aims to bring together researchers interested in the broad
area of synthesis of computing systems. The goal is to foster the development
of frontier techniques in automating the development of computing system.
Contributions of interest include algorithms, complexity and decidability
analysis, as well as reproducible heuristics, implemented tools, and
experimental evaluation. Application domains include software, hardware,
embedded, and cyberphysical systems. Computation models include functional,
reactive, hybrid and timed systems. Identifying, formalizing, and evaluating
synthesis in particular application domains is encouraged.
  The fourth iteration of the workshop took place in San Francisco, CA, USA. It
was co-located with the 27th International Conference on Computer Aided
Verification. The workshop included five contributed talks and two invited
talks. In addition, it featured a special session about the Syntax-Guided
Synthesis Competition (SyGuS) and the SyntComp Synthesis competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00795</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00795</id><created>2016-02-02</created><authors><author><keyname>Way</keyname><forenames>Samuel F.</forenames></author><author><keyname>Larremore</keyname><forenames>Daniel B.</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Gender, Productivity, and Prestige in Computer Science Faculty Hiring
  Networks</title><categories>cs.SI cs.CY physics.soc-ph stat.AP</categories><comments>11 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Women are dramatically underrepresented in computer science at all levels in
academia and account for just 15% of tenure-track faculty. Understanding the
causes of this gender imbalance would inform both policies intended to rectify
it and employment decisions by departments and individuals. Progress in this
direction, however, is complicated by the complexity and decentralized nature
of faculty hiring and the non-independence of hires. Using comprehensive data
on both hiring outcomes and scholarly productivity for 2659 tenure-track
faculty across 205 Ph.D.-granting departments in North America, we investigate
the multi-dimensional nature of gender inequality in computer science faculty
hiring through a network model of the hiring process. Overall, we find that
hiring outcomes are most directly affected by (i) the relative prestige between
hiring and placing institutions and (ii) the scholarly productivity of the
candidates. After including these, and other features, the addition of gender
did not significantly reduce modeling error. However, gender differences do
exist, e.g., in scholarly productivity, postdoctoral training rates, and in
career movements up the rankings of universities, suggesting that the effects
of gender are indirectly incorporated into hiring decisions through gender's
covariates. Furthermore, we find evidence that more highly ranked departments
recruit female faculty at higher than expected rates, which appears to inhibit
similar efforts by lower ranked departments. These findings illustrate the
subtle nature of gender inequality in faculty hiring networks and provide new
insights to the underrepresentation of women in computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00798</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00798</id><created>2016-02-02</created><authors><author><keyname>Hui</keyname><forenames>David Shui Wing</forenames><affiliation>Huawei Technologies Co. Ltd</affiliation></author><author><keyname>Chen</keyname><forenames>Yi-Chao</forenames><affiliation>Huawei Technologies Co. Ltd</affiliation></author><author><keyname>Zhang</keyname><forenames>Gong</forenames><affiliation>Huawei Technologies Co. Ltd</affiliation></author><author><keyname>Wu</keyname><forenames>Weijie</forenames><affiliation>Huawei Technologies Co. Ltd</affiliation></author><author><keyname>Chen</keyname><forenames>Guanrong</forenames><affiliation>City University of Hong Kong</affiliation></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames><affiliation>The Chinese University of Hong Kong</affiliation></author><author><keyname>Li</keyname><forenames>Yingtao</forenames><affiliation>Huawei Technologies Co. Ltd</affiliation></author></authors><title>A Unified Framework for Information Consumption Based on Markov Chains</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes a Markov chain model as a unified framework for
understanding information consumption processes in complex networks, with clear
implications to the Internet and big-data technologies. In particular, the
proposed model is the first one to address the formation mechanism of the
&quot;trichotomy&quot; in observed probability density functions from empirical data of
various social and technical networks. Both simulation and experimental results
demonstrate a good match of the proposed model with real datasets, showing its
superiority over the classical power-law models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00801</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00801</id><created>2016-02-02</created><updated>2016-02-03</updated><authors><author><keyname>Li</keyname><forenames>Zhiyan</forenames></author></authors><title>A Novel Human Computer Interaction Platform based College Mathematical
  Education Methodology</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes the analysis on novel human computer interaction (HCI)
platform based college mathematical education methodology. Above for the
application of virtual reality technology in teaching the problems in the
study, only through the organization focus on the professional and technical
personnel, and constantly improve researchers in development process of
professional knowledge, close to the actual needs of the teaching can we
achieve the satisfactory result. To obtain better education output, we combine
the Kinect to form the HCI based teaching environment. We firstly review the
latest HCI technique and principles of college math courses, then we introduce
basic components of the Kinect including the gesture segmentation, systematic
implementation and the primary characteristics of the platform. As the further
step, we implement the system with the re-write of script code to build up the
personalized HCI assisted education scenario. The verification and simulation
proves the feasibility of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00802</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00802</id><created>2016-02-02</created><authors><author><keyname>Hessar</keyname><forenames>Farzad</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author></authors><title>Spectrum Sharing Between A Surveillance Radar and Secondary Wi-Fi
  Networks</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Aerospace and Electronic Systems, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Co-existence between unlicensed networks that share spectrum
spatio-temporally with terrestrial (e.g. Air Traffic Control) and shipborne
radars in 3-GHz band is attracting significant interest. Similar to every
primary-secondary coexistence scenario, interference from unlicensed devices to
a primary receiver must be within acceptable bounds. In this work, we formulate
the spectrum sharing problem between a pulsed, search radar (primary) and
802.11 WLAN as the secondary. We compute the protection region for such a
search radar for a) a single secondary user (initially) as well as b) a random
spatial distribution of multiple secondary users. Furthermore, we also analyze
the interference to the WiFi devices from the radar's transmissions to estimate
the impact on achievable WLAN throughput as a function of distance to the
primary radar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00804</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00804</id><created>2016-02-02</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Zhiyan</forenames></author></authors><title>Research on Information Security Enhancement Approaches and the
  Applications on HCI Systems</title><categories>cs.HC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With rapid development of computer techniques, the human computer interaction
scenarios are becoming more and more frequent. The development history of the
human-computer interaction is from a person to adapt to the computer to the
computer and continually adapt to the rapid development. Facing the process of
human-computer interaction, information system daily operation to produce huge
amounts of data, how to ensure human-computer interaction interface clear,
generated data safe and reliable, has become a problem to be solved in the
world of information. To deal with the challenging, we propose the information
security enhancement approaches and the core applications on HCI systems.
Through reviewing the other state-of-the-art methods, we propose the data
encryption system to deal with the issues that uses mixed encryption system to
make full use of the symmetric cipher algorithm encryption speed and encryption
intensity is high while the encryption of large amounts of data efficiently.
Our method could enhance the general safety of the HCI system, the experimental
result verities the feasibility and general robustness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00805</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00805</id><created>2016-02-02</created><authors><author><keyname>von Sivers</keyname><forenames>I.</forenames></author><author><keyname>Templeton</keyname><forenames>A.</forenames></author><author><keyname>K&#xfc;nzner</keyname><forenames>F.</forenames></author><author><keyname>K&#xf6;ster</keyname><forenames>G.</forenames></author><author><keyname>Drury</keyname><forenames>J.</forenames></author><author><keyname>Philippides</keyname><forenames>A.</forenames></author><author><keyname>Neckel</keyname><forenames>T.</forenames></author><author><keyname>Bungartz</keyname><forenames>H. -J.</forenames></author></authors><title>Modelling social identification and helping in evacuation simulation</title><categories>physics.soc-ph cs.MA math.DS</categories><comments>submitted to Safety Science, 34 pages (incl. bibliography)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social scientists have criticised computer models of pedestrian streams for
their treatment of psychological crowds as mere aggregations of individuals.
Indeed most models for evacuation dynamics use analogies from physics where
pedestrians are considered as particles. Although this ensures that the results
of the simulation match important physical phenomena, such as the deceleration
of the crowd with increasing density, social phenomena such as group processes
are ignored. In particular, people in a crowd have social identities and share
those social identities with the others in the crowd. The process of self
categorisation determines norms within the crowd and influences how people will
behave in evacuation situations. We formulate the application of social
identity in pedestrian simulation algorithmically. The goal is to examine
whether it is possible to carry over the psychological model to computer models
of pedestrian motion so that simulation results correspond to observations from
crowd psychology. That is, we quantify and formalise empirical research on and
verbal descriptions of the effect of group identity on behaviour. We use
uncertainty quantification to analyse the model's behaviour when we vary
crucial model parameters. In this first approach we restrict ourselves to a
specific scenario that was thoroughly investigated by crowd psychologists and
where some quantitative data is available: the bombing and subsequent
evacuation of a London underground tube carriage on July 7th 2005.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00810</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00810</id><created>2016-02-02</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Kaltofen</keyname><forenames>Erich</forenames><affiliation>NCSU</affiliation></author><author><keyname>Thom&#xe9;</keyname><forenames>Emmanuel</forenames><affiliation>CARAMEL</affiliation></author><author><keyname>Villard</keyname><forenames>Gilles</forenames><affiliation>ARIC, LIP</affiliation></author></authors><title>Linear Time Interactive Certificates for the Minimal Polynomial and the
  Determinant of a Sparse Matrix</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certificates to a linear algebra computation are additional data structures
for each output, which can be used by a-possibly randomized- verification
algorithm that proves the correctness of each output. In this paper, we give an
algorithm that compute a certificate for the minimal polynomial of sparse or
structured n x n matrices over an abstract field, of sufficiently large
cardinality, whose Monte Carlo verification complexity requires a single
matrix-vector multiplication and a linear number of extra field operations. We
also propose a novel preconditioner that ensures irreducibility of the
characteristic polynomial of the preconditioned matrix. This preconditioner
takes linear time to be applied and uses only two random entries. We then
combine these two techniques to give algorithms that compute certificates for
the determinant, and thus for the characteristic polynomial, whose Monte Carlo
verification complexity is therefore also linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00812</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00812</id><created>2016-02-02</created><authors><author><keyname>Moot</keyname><forenames>Richard</forenames><affiliation>LaBRI, CNRS</affiliation></author></authors><title>The Grail theorem prover: Type theory for syntax and semantics</title><categories>cs.CL</categories><comments>Modern Perspectives in Type Theoretical Semantics, Springer, 2016</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the name suggests, type-logical grammars are a grammar formalism based on
logic and type theory. From the prespective of grammar design, type-logical
grammars develop the syntactic and semantic aspects of linguistic phenomena
hand-in-hand, letting the desired semantics of an expression inform the
syntactic type and vice versa. Prototypical examples of the successful
application of type-logical grammars to the syntax-semantics interface include
coordination, quantifier scope and extraction.This chapter describes the Grail
theorem prover, a series of tools for designing and testing grammars in various
modern type-logical grammars which functions as a tool . All tools described in
this chapter are freely available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00828</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00828</id><created>2016-02-02</created><authors><author><keyname>Rahmani</keyname><forenames>Hossein</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author><author><keyname>Shah</keyname><forenames>Mubarak</forenames></author></authors><title>Learning a Deep Model for Human Action Recognition from Novel Viewpoints</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognizing human actions from unknown and unseen (novel) views is a
challenging problem. We propose a Robust Non-Linear Knowledge Transfer Model
(R-NKTM) for human action recognition from novel views. The proposed R-NKTM is
a deep fully-connected neural network that transfers knowledge of human actions
from any unknown view to a shared high-level virtual view by finding a
non-linear virtual path that connects the views. The R-NKTM is learned from
dense trajectories of synthetic 3D human models fitted to real motion capture
data and generalizes to real videos of human actions. The strength of our
technique is that we learn a single R-NKTM for all actions and all viewpoints
for knowledge transfer of any real human action video without the need for
re-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to
incorporate new action classes. R-NKTM is learned with dummy labels and does
not require knowledge of the camera viewpoint at any stage. Experiments on
three benchmark cross-view human action datasets show that our method
outperforms existing state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00831</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00831</id><created>2016-02-02</created><authors><author><keyname>Issartel</keyname><forenames>Paul</forenames><affiliation>LIMSI</affiliation></author><author><keyname>Gu&#xe9;niat</keyname><forenames>Florimond</forenames><affiliation>FSU</affiliation></author><author><keyname>Coquillart</keyname><forenames>Sabine</forenames><affiliation>PRIMA</affiliation></author><author><keyname>Ammi</keyname><forenames>Mehdi</forenames><affiliation>LIMSI</affiliation></author></authors><title>Perceiving Mass in Mixed Reality through Pseudo-Haptic Rendering of
  Newton's Third Law</title><categories>cs.HC</categories><proxy>ccsd</proxy><doi>10.1109/VR.2015.7223322</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mixed reality, real objects can be used to interact with virtual objects.
However, unlike in the real world, real objects do not encounter any opposite
reaction force when pushing against virtual objects. The lack of reaction force
during manipulation prevents users from perceiving the mass of virtual objects.
Although this could be addressed by equipping real objects with force-feedback
devices, such a solution remains complex and impractical.In this work, we
present a technique to produce an illusion of mass without any active
force-feedback mechanism. This is achieved by simulating the effects of this
reaction force in a purely visual way. A first study demonstrates that our
technique indeed allows users to differentiate light virtual objects from heavy
virtual objects. In addition, it shows that the illusion is immediately
effective, with no prior training. In a second study, we measure the lowest
mass difference (JND) that can be perceived with this technique. The
effectiveness and ease of implementation of our solution provides an
opportunity to enhance mixed reality interaction at no additional cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00833</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00833</id><created>2016-02-02</created><authors><author><keyname>Boshkovska</keyname><forenames>Elena</forenames></author></authors><title>Practical Non-Linear Energy Harvesting Model and Resource Allocation in
  SWIPT Systems</title><categories>cs.IT math.IT</categories><comments>Master thesis, Institute for Digital Communications, University of
  Erlangen-Nuremberg, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous wireless information and power transfer (SWIPT) is a promising
solution for enabling long-life, and self-sustainable wireless networks. In
this thesis, we propose a practical non-linear energy harvesting (EH) model and
design a resource allocation algorithm for SWIPT systems. In particular, the
algorithm design is formulated as a non-convex optimization problem for the
maximization of the total harvested power at the EH receivers subject to
quality of service (QoS) constraints for the information decoding (ID)
receivers. To circumvent the non-convexity of the problem, we transform the
corresponding non-convex sum-of-ratios objective function into an equivalent
objective function in parametric subtractive form. Furthermore, we design a
computationally efficient iterative resource allocation algorithm to obtain the
globally optimal solution. Numerical results illustrate significant performance
gain in terms of average total harvested power for the proposed non-linear EH
receiver model, when compared to the traditional linear model.\
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00836</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00836</id><created>2016-02-02</created><authors><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author><author><keyname>Storjohann</keyname><forenames>Arne</forenames></author></authors><title>Algorithms for Simultaneous Pad\'e Approximations</title><categories>cs.SC</categories><comments>Submitted to ISSAC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe how to solve simultaneous Pad\'e approximations over a power
series ring $K[[x]]$ for a field $K$ using $O~(n^{\omega - 1} d)$ operations in
$K$, where $d$ is the sought precision and $n$ is the number of power series to
approximate. We develop two algorithms using different approaches and with
different advantages. Both algorithms return reduced bases for the complete set
of solutions to the input approximation problem. Our results are made possible
by recent breakthroughs in fast computations of minimal approximant bases and
Hermite Pad\'e approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00841</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00841</id><created>2016-02-02</created><authors><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author></authors><title>On Physical Web models</title><categories>cs.CY cs.NI</categories><comments>a paper for Sibcon 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Physical Web is a generic term describes interconnection of physical
objects and web. The Physical Web lets present physical objects in a web. There
are different ways to do that and we will discuss them in our paper. Usually,
the web presentation for a physical object could be implemented with the help
of mobile devices. The basic idea behind the Physical Web is to navigate and
control physical objects in the world surrounding mobile devices with the help
of web technologies. Of course, there are different ways to identify and
enumerate physical objects. In this paper, we describe the existing models as
well as related challenges. In our analysis, we will target objects enumeration
and navigation as well as data retrieving and programming for the Physical Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00844</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00844</id><created>2016-02-02</created><authors><author><keyname>Miyoshi</keyname><forenames>Naoto</forenames></author><author><keyname>Shirai</keyname><forenames>Tomoyuki</forenames></author></authors><title>A sufficient condition for the tail asymptotics of SIR distribution in
  downlink cellular networks</title><categories>cs.IT math.IT math.PR</categories><msc-class>60G55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the spatial stochastic model of single-tier downlink cellular
networks, where the wireless base stations are deployed according to a general
stationary point process on the plane with general i.i.d. propagation effects.
Recently, Ganti &amp; Haenggi (2015) consider the same general cellular network
model and, as one of many significant results, derive the tail asymptotics of
the signal-to-interference ratio (SIR) distribution. However, they do not
mention any conditions under which the result holds. In this paper, we
compensate their result for the lack of the condition and expose a sufficient
condition for the asymptotic result to be valid. We further illustrate some
examples satisfying such a sufficient condition and indicate the corresponding
asymptotic results for the example models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00848</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00848</id><created>2016-02-02</created><authors><author><keyname>Renault</keyname><forenames>Gu&#xe9;na&#xeb;l</forenames><affiliation>PolSys</affiliation></author><author><keyname>Vaccon</keyname><forenames>Tristan</forenames></author></authors><title>On the p-adic stability of the FGLM algorithm</title><categories>cs.SC math.AC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, many strategies to solve polynomial systems use the computation of
a Gr{\&quot;o}bner basis for the graded reverse lexicographical ordering, followed
by a change of ordering algorithm to obtain a Gr{\&quot;o}bner basis for the
lexicographical ordering. The change of ordering algorithm is crucial for these
strategies. We study the p-adic stability of the main change of ordering
algorithm, FGLM. We show that FGLM is stable and give explicit upper bound on
the loss of precision occuring in its execution. The variant of FGLM designed
to pass from the grevlex ordering to a Gr{\&quot;o}bner basis in shape position is
also stable. Our study relies on the application of Smith Normal Form
computations for linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00849</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00849</id><created>2016-02-02</created><updated>2016-02-03</updated><authors><author><keyname>Izzo</keyname><forenames>Dario</forenames></author><author><keyname>Hennes</keyname><forenames>Daniel</forenames></author><author><keyname>M&#xe4;rtens</keyname><forenames>Marcus</forenames></author><author><keyname>Getzner</keyname><forenames>Ingmar</forenames></author><author><keyname>Nowak</keyname><forenames>Krzysztof</forenames></author><author><keyname>Heffernan</keyname><forenames>Anna</forenames></author><author><keyname>Campagnola</keyname><forenames>Stefano</forenames></author><author><keyname>Yam</keyname><forenames>Chit Hong</forenames></author><author><keyname>Ozaki</keyname><forenames>Naoya</forenames></author><author><keyname>Sugimoto</keyname><forenames>Yoshihide</forenames></author></authors><title>GTOC8: Results and Methods of ESA Advanced Concepts Team and JAXA-ISAS</title><categories>physics.space-ph astro-ph.IM cs.DS cs.SY</categories><comments>Presented at the 26th AAS/AIAA Space Flight Mechanics Meeting, Napa,
  CA. Paper AAS 16-275</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the interplanetary trajectory design problem posed by the 8th
edition of the Global Trajectory Optimization Competition and present the
end-to-end strategy developed by the team ACT-ISAS (a collaboration between the
European Space Agency's Advanced Concepts Team and JAXA's Institute of Space
and Astronautical Science). The resulting interplanetary trajectory won 1st
place in the competition, achieving a final mission value of $J=146.33$ [Mkm].
Several new algorithms were developed in this context but have an interest that
go beyond the particular problem considered, thus, they are discussed in some
detail. These include the Moon-targeting technique, allowing one to target a
Moon encounter from a low Earth orbit; the 1-$k$ and 2-$k$ fly-by targeting
techniques, enabling one to design resonant fly-bys while ensuring a targeted
future formation plane% is acquired at some point after the manoeuvre ; the
distributed low-thrust targeting technique, admitting one to control the
spacecraft formation plane at 1,000,000 [km]; and the low-thrust optimization
technique, permitting one to enforce the formation plane's orientations as path
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00860</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00860</id><created>2016-02-02</created><authors><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author><author><keyname>Robshaw</keyname><forenames>M. J. B.</forenames></author></authors><title>On the security of the Algebraic Eraser tag authentication protocol</title><categories>cs.CR math.GR</categories><comments>15 pages</comments><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Algebraic Eraser has been gaining prominence as SecureRF, the company
commercializing the algorithm, increases its marketing reach. The scheme is
claimed to be well-suited to IoT applications but a lack of detail in available
documentation has hampered peer-review. Recently more details of the system
have emerged after a tag authentication protocol built using the Algebraic
Eraser was proposed for standardization in ISO/IEC SC31 and SecureRF provided
an open public description of the protocol. In this paper we describe a range
of attacks on this protocol that include very efficient and practical tag
impersonation as well as partial, and total, tag secret key recovery. Most of
these results have been practically verified, they contrast with the 80-bit
security that is claimed for the protocol, and they emphasize the importance of
independent public review for any cryptographic proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00875</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00875</id><created>2016-02-02</created><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Converse Bounds for Noisy Group Testing with Arbitrary Measurement
  Matrices</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the group testing problem, in which one seeks to identify a
subset of defective items within a larger set of items based on a number of
noisy tests. While matching achievability and converse bounds are known in
several cases of interest for i.i.d.~measurement matrices, less is known
regarding converse bounds for arbitrary matrices, except in some specific
scenarios. We partially close this gap by presenting two new converse bounds
for arbitrary matrices and general noise models. First, we provide a strong
converse bound ($\mathbb{P}[\mathrm{error}] \to 1$) that matches existing
achievability bounds in several cases of interest. Second, we provide a weak
converse bound ($\mathbb{P}[\mathrm{error}] \not\to 0$) that matches existing
achievability bounds in greater generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00877</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00877</id><created>2016-02-02</created><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Partial Recovery Bounds for the Sparse Stochastic Block Model</title><categories>cs.IT cs.SI math.IT stat.ML</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the information-theoretic limits of community
detection in the symmetric two-community stochastic block model, with
intra-community and inter-community edge probabilities $\frac{a}{n}$ and
$\frac{b}{n}$ respectively. We consider the sparse setting, in which $a$ and
$b$ do not scale with $n$, and provide upper and lower bounds on the proportion
of community labels recovered on average. We provide a numerical example for
which the bounds are near-matching for moderate values of $a - b$, and matching
in the limit as $a-b$ grows large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00878</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00878</id><created>2016-02-02</created><authors><author><keyname>Fahs</keyname><forenames>Jihad</forenames></author><author><keyname>Abou-Faycal</keyname><forenames>Ibrahim</forenames></author></authors><title>Input Constraints and Noise Density Functions: A Simple Relation for
  Bounded-Support and Discrete Capacity-Achieving Inputs</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory on
  11-June-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classical problem of characterizing the channel capacity and its
achieving distribution in a generic fashion. We derive a simple relation
between three parameters: the input-output function, the input cost function
and the noise probability density function, one which dictates the type of the
optimal input. In Layman terms we prove that the support of the optimal input
is bounded whenever the cost grows faster than a cut-off rate equal to the
logarithm of the noise PDF evaluated at the input-output function. Furthermore,
we prove a converse statement that says whenever the cost grows slower than the
cut-off rate, the optimal input has necessarily an unbounded support. In
addition, we show how the discreteness of the optimal input is guaranteed
whenever the triplet satisfy some analyticity properties. We argue that a
suitable cost function to be imposed on the channel input is one that grows
similarly to the cut-off rate. Our results are valid for any cost function that
is super-logarithmic. They summarize a large number of previous channel
capacity results and give new ones for a wide range of communication channel
models, such as Gaussian mixtures, generalized-Gaussians and heavy-tailed noise
models, that we state along with numerical computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00883</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00883</id><created>2016-02-02</created><authors><author><keyname>Kapoor</keyname><forenames>Sakshi</forenames></author><author><keyname>Sreekumar</keyname><forenames>Sreejith</forenames></author><author><keyname>Pillai</keyname><forenames>Sibi Raj B</forenames></author></authors><title>Distributed Scheduling in Multiple Access with Bursty Arrivals and Delay
  Constraints</title><categories>cs.IT math.IT</categories><comments>39 pages, 16 figures, presented in part at ISIT 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a distributed multiple access system with bursty arrivals. The
transmissions are grouped into slots and the users are frame-synchronized. At
the start of each time slot, variable sized packets independently arrive at
each of the transmitting terminals. The packets are to be delivered to a common
receiver within a certain number of slots specified by a maximum delay
constraint on each packet. The key assumption is that each terminal knows only
its own arrival process, i.e. the arrivals at the rest of the terminals are
unknown to each transmitter. For this interesting distributed multiple access
model, we design novel power efficient communication schemes which transport
the arriving data without any outage, while ensuring the delay constraints. The
proposed schemes are not only optimal from the average transmit sum-power
perspective, but also considerably outperform conventional schemes like TDMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00895</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00895</id><created>2016-02-02</created><authors><author><keyname>Khernane</keyname><forenames>Nesrine</forenames><affiliation>NPA</affiliation></author><author><keyname>Potop-Butucaru</keyname><forenames>Maria</forenames><affiliation>NPA</affiliation></author><author><keyname>Chaudet</keyname><forenames>Claude</forenames></author></authors><title>BANZKP: a Secure Authentication Scheme Using Zero Knowledge Proof for
  WBANs</title><categories>cs.CR cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -Wireless body area network(WBAN) has shown great potential in improving
healthcare quality not only for patients but also for medical staff. However,
security and privacy are still an important issue in WBANs especially in
multi-hop architectures. In this paper, we propose and present the design and
the evaluation of a secure lightweight and energy efficient authentication
scheme BANZKP based on an efficient cryptographic protocol, Zero Knowledge
Proof (ZKP) and a commitment scheme. ZKP is used to confirm the identify of the
sensor nodes, with small computational requirement, which is favorable for body
sensors given their limited resources, while the commitment scheme is used to
deal with replay attacks and hence the injection attacks by committing a
message and revealing the key later. Our scheme reduces the memory requirement
by 56.13 % compared to TinyZKP [13], the comparable alternative so far for Body
Area Networks, and uses 10 % less energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00904</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00904</id><created>2016-02-02</created><updated>2016-02-03</updated><authors><author><keyname>Oikonomou</keyname><forenames>Vangelis P.</forenames></author><author><keyname>Liaros</keyname><forenames>Georgios</forenames></author><author><keyname>Georgiadis</keyname><forenames>Kostantinos</forenames></author><author><keyname>Chatzilari</keyname><forenames>Elisavet</forenames></author><author><keyname>Adam</keyname><forenames>Katerina</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Spiros</forenames></author><author><keyname>Kompatsiaris</keyname><forenames>Ioannis</forenames></author></authors><title>Comparative evaluation of state-of-the-art algorithms for SSVEP-based
  BCIs</title><categories>cs.HC cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain-computer interfaces (BCIs) have been gaining momentum in making
human-computer interaction more natural, especially for people with
neuro-muscular disabilities. Among the existing solutions the systems relying
on electroencephalograms (EEG) occupy the most prominent place due to their
non-invasiveness. However, the process of translating EEG signals into computer
commands is far from trivial, since it requires the optimization of many
different parameters that need to be tuned jointly. In this report, we focus on
the category of EEG-based BCIs that rely on Steady-State-Visual-Evoked
Potentials (SSVEPs) and perform a comparative evaluation of the most promising
algorithms existing in the literature. More specifically, we define a set of
algorithms for each of the various different parameters composing a BCI system
(i.e. filtering, artifact removal, feature extraction, feature selection and
classification) and study each parameter independently by keeping all other
parameters fixed. The results obtained from this evaluation process are
provided together with a dataset consisting of the 256-channel, EEG signals of
11 subjects, as well as a processing toolbox for reproducing the results and
supporting further experimentation. In this way, we manage to make available
for the community a state-of-the-art baseline for SSVEP-based BCIs that can be
used as a basis for introducing novel methods and approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00910</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00910</id><created>2016-02-02</created><authors><author><keyname>Bodinier</keyname><forenames>Quentin</forenames></author><author><keyname>Farhang</keyname><forenames>Arman</forenames></author><author><keyname>Bader</keyname><forenames>Faouzi</forenames></author><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author><author><keyname>Palicot</keyname><forenames>Jacques</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz A.</forenames></author></authors><title>5G Waveforms for Overlay D2D Communications: Effects of Time-Frequency
  Misalignment</title><categories>cs.IT math.IT</categories><comments>7 pages, 7 figures, Accepted at IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses a scenario where a Device-To-Device (D2D) pair coexists
with an Orthogonal Frequency Division Multiplexing (OFDM) based incumbent
network. D2D transmitter communicates in parts of spectrum left free by
cellular users, while respecting a given spectral mask. The D2D pair is
misaligned in time and frequency with the cellular users. Furthermore, the D2D
pair utilizes alternative waveforms to OFDM proposed for 5G. In this study, we
show that it is not worth synchronising the D2D pair in time with respect to
the cellular users. Indeed, the interference injected into the incumbent
network has small variations with respect to time misalignment. We provide
interference tables that encompass both time and frequency misalignment. We use
them to analyse the maximum rate achievable by the D2D pair when it uses
different waveforms. Then, we present numerical results showing what waveform
should be utilized by the D2D pair according to the time-frequency resources
that are not used by the incumbent network. Our results show that the delay
induced by linearly convolved waveforms make them hardly applicable to short
time windows, but that they dominate OFDM for long transmissions, mainly in the
case where cellular users are very sensitive to interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00912</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00912</id><created>2016-02-02</created><authors><author><keyname>Arif</keyname><forenames>M. Fareed</forenames></author></authors><title>From $\mu$-Calculus to Alternating Tree Automata using Parity Games</title><categories>cs.LO cs.FL</categories><comments>17 pages, 1 figure</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  $\mu$-Calculus and automata on infinite trees are complementary ways of
describing infinite tree languages. The correspondence between $\mu$-Calculus
and alternating tree automaton is used to solve the satisfiability and model
checking problems by compiling the modal $\mu$-Calculus formula into an
alternating tree automata. Thus advocating an automaton model specially
tailored for working with modal $\mu$-Calculus. The advantage of the automaton
model is its ability to deal with arbitrary branching in a much simpler way as
compare to the one proposed by Janin and Walukiewicz. Both problems (i.e.,
model checking and satisfiability) are solved by reduction to the corresponding
problems of alternating tree automata, namely to the acceptance and the
non-emptiness problems, respectively. These problems, in turn, are solved using
parity games where semantics of alternating tree automata is translated to a
winning strategy in an appropriate parity game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00914</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00914</id><created>2016-02-02</created><authors><author><keyname>Li</keyname><forenames>Fei</forenames></author><author><keyname>Yan</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Qiuyan</forenames></author><author><keyname>Yan</keyname><forenames>Tongjiang</forenames></author></authors><title>Binary linear codes with at most 4 weights</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past decades, linear codes with few weights have been widely studied,
since they have applications in space communications, data storage and
cryptography. In this paper, a class of binary linear codes is constructed and
their weight distribution is determined. Results show that they are at most
4-weight linear codes. Additionally, these codes can be used in secret sharing
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00917</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00917</id><created>2016-02-02</created><authors><author><keyname>Bytev</keyname><forenames>V.</forenames></author><author><keyname>Kniehl</keyname><forenames>B.</forenames></author></authors><title>HYPERgeometric functions DIfferential REduction: Mathematica-based
  packages for the differential reduction of generalizedhypergeometric
  functions: Fc hypergeometric function of three variables</title><categories>math-ph cs.SC hep-ph hep-th math.MP</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a further development of the HYPERDIRE project: a set of
Mathematica-based program packages for manipulations with Horn-type
hypergeometric functions on the basis of differential equations. Specifically,
we present the implementation of differential reduction for the Lauricella
function $F_C$ of three variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00928</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00928</id><created>2016-02-02</created><authors><author><keyname>Gorce</keyname><forenames>Jean-Marie</forenames><affiliation>SOCRATE</affiliation></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Kelif</keyname><forenames>Jean-Marc</forenames></author></authors><title>Spatial Continuum Extensions of Asymmetric Gaussian Channels (Multiple
  Access and Broadcast)</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new model called \emph{spatial continuum asymmetric
channels} to study the channel capacity region of asymmetric scenarios in which
either one source transmits to a spatial density of receivers or a density of
transmitters transmit to a unique receiver.This approach is built upon the
classical broadcast channel (BC) and multiple access channel (MAC). For the
sake of consistency, the study is limited to Gaussian channels with power
constraints and is restricted to the asymptotic regime (zero-error
capacity).The reference scenario comprises one base station (BS) in Tx or Rx
mode, a spatial random distribution of nodes (resp. in Rx or Tx mode)
characterized by a probability spatial density $u(x)$ and a request for a
quantity of information with no delay constraint. This system is modeled as an
$\infty-$user asymmetric channel (BC or MAC). To derive the properties of this
model, a spatial discretization is performed and the equivalence with either a
BC or MAC is established. A discretization sequence is then defined to refine
infinitely the approximation. Achievability and capacity results are obtained
in the limit of this sequence. The uniform capacity is then defined as the
maximal symmetric achievable rate at which the distributed users can
transmit/receive with no delay constraint.The capacity region is also
established as the set of information distributions that are achievable. The
tightness of these limits and their practical interest are briefly illustrated
and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00932</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00932</id><created>2016-02-02</created><authors><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Addendum to Pentapods with Mobility 2</title><categories>cs.RO math.AG</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a foregoing publication the authors studied pentapods with mobility 2,
where neither all platform anchor points nor all base anchor points are located
on a line. It turned out that the given classification is incomplete. This
addendum is devoted to the discussion of the missing cases resulting in
additional solutions already known to Duporcq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00955</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00955</id><created>2016-02-02</created><updated>2016-02-04</updated><authors><author><keyname>Dai</keyname><forenames>Dengxin</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Unsupervised High-level Feature Learning by Ensemble Projection for
  Semi-supervised Image Classification and Image Clustering</title><categories>cs.CV</categories><comments>22 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of image classification with limited or
no annotations, but abundant unlabeled data. The setting exists in many tasks
such as semi-supervised image classification, image clustering, and image
retrieval. Unlike previous methods, which develop or learn sophisticated
regularizers for classifiers, our method learns a new image representation by
exploiting the distribution patterns of all available data for the task at
hand. Particularly, a rich set of visual prototypes are sampled from all
available data, and are taken as surrogate classes to train discriminative
classifiers; images are projected via the classifiers; the projected values,
similarities to the prototypes, are stacked to build the new feature vector.
The training set is noisy. Hence, in the spirit of ensemble learning we create
a set of such training sets which are all diverse, leading to diverse
classifiers. The method is dubbed Ensemble Projection (EP). EP captures not
only the characteristics of individual images, but also the relationships among
images. It is conceptually simple and computationally efficient, yet effective
and flexible. Experiments on eight standard datasets show that: (1) EP
outperforms previous methods for semi-supervised image classification; (2) EP
produces promising results for self-taught image classification, where
unlabeled samples are a random collection of images rather than being from the
same distribution as the labeled ones; and (3) EP improves over the original
features for image clustering. The code of the method is available on the
project page.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00963</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00963</id><created>2016-02-02</created><authors><author><keyname>Vella</keyname><forenames>Flavio</forenames></author><author><keyname>Carbone</keyname><forenames>Giancarlo</forenames></author><author><keyname>Bernaschi</keyname><forenames>Massimo</forenames></author></authors><title>Algorithms and Heuristics for Scalable Betweenness Centrality
  Computation on Multi-GPU Systems</title><categories>cs.DC cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Betweenness Centrality (BC) is steadily growing in popularity as a metrics of
the influence of a vertex in a graph. The BC score of a vertex is proportional
to the number of all-pairs-shortest-paths passing through it. However, complete
and exact BC computation for a large-scale graph is an extraordinary challenge
that requires high performance computing techniques to provide results in a
reasonable amount of time. Our approach combines bi-dimensional (2-D)
decomposition of the graph and multi-level parallelism together with a suitable
data-thread mapping that overcomes most of the difficulties caused by the
irregularity of the computation on GPUs. Furthermore, we propose novel
heuristics which exploit the topology information of the graph in order to
reduce time and space requirements of BC computation. Experimental results on
synthetic and real-world graphs show that the proposed techniques allow the BC
computation of graphs which are too large to fit in the memory of a single
computational node along with a significant reduction of the computing time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00970</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00970</id><created>2016-02-02</created><authors><author><keyname>Napoletano</keyname><forenames>Paolo</forenames></author></authors><title>Visual descriptors for content-based retrieval of remote sensing images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an extensive evaluation of visual descriptors for
the content-based retrieval of remote sensing images. The evaluation includes
global, local, and Convolutional Neural Network (CNNs) features coupled with
three different Content-Based Image Retrieval schemas. We conducted all the
experiments on two publicly available datasets: the 21-class UC Merced Land
Use/Land Cover data set and 19-class High-resolution Satellite Scene dataset.
Results demonstrate that features extracted from CNNs are the best performing
whatever is the retrieval schema adopted. Local descriptors perform better than
CNN-based descriptors only when dealing with images that contain fine-grained
textures or objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00975</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00975</id><created>2016-02-02</created><authors><author><keyname>Davis</keyname><forenames>Clayton A.</forenames></author><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author></authors><title>BotOrNot: A System to Evaluate Social Bots</title><categories>cs.SI</categories><comments>2 pages, 2 figures, WWW Developers Day</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While most online social media accounts are controlled by humans, these
platforms also host automated agents called social bots or sybil accounts.
Recent literature reported on cases of social bots imitating humans to
manipulate discussions, alter the popularity of users, pollute content and
spread misinformation, and even perform terrorist propaganda and recruitment
actions. Here we present BotOrNot, a publicly-available service that leverages
more than one thousand features to evaluate the extent to which a Twitter
account exhibits similarity to the known characteristics of social bots. Since
its release in May 2014, BotOrNot has served over one million requests via our
website and APIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00981</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00981</id><created>2016-02-02</created><updated>2016-02-05</updated><authors><author><keyname>Bra&#x10d;evac</keyname><forenames>Oliver</forenames></author><author><keyname>Erdweg</keyname><forenames>Sebastian</forenames></author><author><keyname>Salvaneschi</keyname><forenames>Guido</forenames></author><author><keyname>Mezini</keyname><forenames>Mira</forenames></author></authors><title>CPL: A Core Language for Cloud Computing -- Technical Report</title><categories>cs.PL</categories><comments>Technical report accompanying the MODULARITY '16 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Running distributed applications in the cloud involves deployment. That is,
distribution and configuration of application services and middleware
infrastructure. The considerable complexity of these tasks resulted in the
emergence of declarative JSON-based domain-specific deployment languages to
develop deployment programs. However, existing deployment programs unsafely
compose artifacts written in different languages, leading to bugs that are hard
to detect before run time. Furthermore, deployment languages do not provide
extension points for custom implementations of existing cloud services such as
application-specific load balancing policies.
  To address these shortcomings, we propose CPL (Cloud Platform Language), a
statically-typed core language for programming both distributed applications as
well as their deployment on a cloud platform. In CPL, application services and
deployment programs interact through statically typed, extensible interfaces,
and an application can trigger further deployment at run time. We provide a
formal semantics of CPL and demonstrate that it enables type-safe, composable
and extensible libraries of service combinators, such as load balancing and
fault tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00984</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00984</id><created>2016-02-02</created><authors><author><keyname>Pereira</keyname><forenames>Rui</forenames></author><author><keyname>Couto</keyname><forenames>Marco</forenames></author><author><keyname>Cunha</keyname><forenames>J&#xe1;come</forenames></author><author><keyname>Fernandes</keyname><forenames>Jo&#xe3;o Paulo</forenames></author><author><keyname>Saraiva</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>The Influence of the Java Collection Framework on Overall Energy
  Consumption</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a detailed study of the energy consumption of the
different Java Collection Framework (JFC) implementations. For each method of
an implementation in this framework, we present its energy consumption when
handling different amounts of data. Knowing the greenest methods for each
implementation, we present an energy optimization approach for Java programs:
based on calls to JFC methods in the source code of a program, we select the
greenest implementation. Finally, we present preliminary results of optimizing
a set of Java programs where we obtained 6.2% energy savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00985</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00985</id><created>2016-02-02</created><authors><author><keyname>Bashivan</keyname><forenames>Pouya</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Heisig</keyname><forenames>Steve</forenames></author></authors><title>Mental State Recognition via Wearable EEG</title><categories>cs.CV cs.HC</categories><journal-ref>Proceedings of 5th NIPS workshop on Machine Learning and
  Interpretation in Neuroimaging (MLINI15) (2015) 5-1</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing quality and affordability of consumer electroencephalogram
(EEG) headsets make them attractive for situations where medical grade devices
are impractical. Predicting and tracking cognitive states is possible for tasks
that were previously not conducive to EEG monitoring. For instance, monitoring
operators for states inappropriate to the task (e.g. drowsy drivers), tracking
mental health (e.g. anxiety) and productivity (e.g. tiredness) are among
possible applications for the technology. Consumer grade EEG headsets are
affordable and relatively easy to use, but they lack the resolution and quality
of signal that can be achieved using medical grade EEG devices. Thus, the key
questions remain: to what extent are wearable EEG devices capable of mental
state recognition, and what kind of mental states can be accurately recognized
with these devices? In this work, we examined responses to two different types
of input: instructional (logical) versus recreational (emotional) videos, using
a range of machine-learning methods. We tried SVMs, sparse logistic regression,
and Deep Belief Networks, to discriminate between the states of mind induced by
different types of video input, that can be roughly labeled as logical vs.
emotional. Our results demonstrate a significant potential of wearable EEG
devices in differentiating cognitive states between situations with large
contextual but subtle apparent differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00991</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00991</id><created>2016-02-02</created><authors><author><keyname>Ondruska</keyname><forenames>Peter</forenames></author><author><keyname>Posner</keyname><forenames>Ingmar</forenames></author></authors><title>Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks</title><categories>cs.LG cs.AI cs.CV cs.NE cs.RO</categories><comments>Published in The Thirtieth AAAI Conference on Artificial Intelligence
  (AAAI-16) Video: https://youtu.be/cdeWCpfUGWc Code:
  http://mrg.robots.ox.ac.uk/mrg_people/peter-ondruska/</comments><acm-class>I.2.6; I.2.9; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents to the best of our knowledge the first end-to-end object
tracking approach which directly maps from raw sensor input to object tracks in
sensor space without requiring any feature engineering or system identification
in the form of plant or sensor models. Specifically, our system accepts a
stream of raw sensor data at one end and, in real-time, produces an estimate of
the entire environment state at the output including even occluded objects. We
achieve this by framing the problem as a deep learning task and exploit
sequence models in the form of recurrent neural networks to learn a mapping
from sensor measurements to object tracks. In particular, we propose a learning
method based on a form of input dropout which allows learning in an
unsupervised manner, only based on raw, occluded sensor data without access to
ground-truth annotations. We demonstrate our approach using a synthetic dataset
designed to mimic the task of tracking objects in 2D laser data -- as commonly
encountered in robotics applications -- and show that it learns to track many
dynamic objects despite occlusions and the presence of sensor noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00994</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00994</id><created>2016-02-02</created><authors><author><keyname>Zhao</keyname><forenames>Kai</forenames></author><author><keyname>Prasath</keyname><forenames>C Mohan</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author></authors><title>Automatic City Region Analysis for Urban Routing</title><categories>cs.SI physics.soc-ph</categories><comments>In proceedings of the IEEE International Conference on Data Mining
  (ICDM) workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are different functional regions in cities such as tourist attractions,
shopping centers, workplaces and residential places. The human mobility
patterns for different functional regions are different, e.g., people usually
go to work during daytime on weekdays, and visit shopping centers after work.
In this paper, we analyse urban human mobility patterns and infer the functions
of the regions in three cities. The analysis is based on three large taxi GPS
datasets in Rome, San Francisco and Beijing containing 21 million, 11 million
and 17 million GPS points respectively. We categorized the city regions into
four kinds of places, workplaces, entertainment places, residential places and
other places. First, we provide a new quad-tree region division method based on
the taxi visits. Second, we use the association rule to infer the functional
regions in these three cities according to temporal human mobility patterns.
Third, we show that these identified functional regions can help us deliver
data in network applications, such as urban Delay Tolerant Networks (DTNs),
more efficiently. The new functional-regions-based DTNs algorithm achieves up
to 183% improvement in terms of delivery ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00997</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00997</id><created>2016-02-02</created><authors><author><keyname>Kumar</keyname><forenames>Amit</forenames></author><author><keyname>Bindal</keyname><forenames>Rishabh</forenames></author><author><keyname>Indela</keyname><forenames>Soumya</forenames></author><author><keyname>Rotkowitz</keyname><forenames>Michael</forenames></author></authors><title>Head Pose Estimation of Occluded Faces using Regularized Regression</title><categories>cs.CV</categories><comments>Submitted to ICIP'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents regression methods for estimation of head pose from
occluded 2-D face images. The process primarily involves reconstructing a face
from its occluded image, followed by classification. Typical methods for
reconstruction assume that the pixel errors of the occluded regions are
independent. However, such an assumption is not true in the case of occlusion,
because of its inherent contiguous nature. Hence, we use nuclear norm as a
metric that can describe well the structure of the error. We also use LASSO
Regression based l1 - regularization to improve reconstruction. Next, we
implement Nuclear Norm Regularized Regression (NR), and also our proposed
method, for reconstruction and subsequent classification. Finally, we compare
the performance of the methods in terms of accuracy of head pose estimation of
occluded faces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.00998</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.00998</id><created>2016-02-02</created><authors><author><keyname>Hooshmandasl</keyname><forenames>M. R.</forenames></author><author><keyname>Meybodi</keyname><forenames>M. Alambardar</forenames></author><author><keyname>Goharshady</keyname><forenames>A. K.</forenames></author><author><keyname>Shakiba</keyname><forenames>A.</forenames></author></authors><title>A combinatorial approach to certain topological spaces based on minimum
  complement S-approximation spaces</title><categories>math.AT cs.DM</categories><comments>This paper is presented in 8th International Seminar on Geometry and
  Topology at Amirkabir University of Technology (Iran)</comments><msc-class>54F99, 68P01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An S-approximation space is a novel approach to study systems with
uncertainty that are not expressible in terms of inclusion relations. In this
work, we further examined these spaces, mostly from a topological point of view
by a combinatorial approach. This work also identifies a subclass of these
approximation spaces, called $S_\mathcal{MC}$-approximations. Topological
properties of this subclass are investigated and finally, the topologies formed
by $S_\mathcal{MC}$-approximations are enumerated up to homeomorphism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01003</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01003</id><created>2016-02-02</created><authors><author><keyname>Kandhway</keyname><forenames>Kundan</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Using Node Centrality and Optimal Control to Maximize Information
  Diffusion in Social Networks</title><categories>cs.SI cs.MA cs.SY math.OC physics.soc-ph</categories><comments>12 pages, 11 figures. Author's version of an article accepted for
  publication in IEEE Transactions on Systems, Man, and Cybernetics: Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model information dissemination as a susceptible-infected epidemic process
and formulate a problem to jointly optimize seeds for the epidemic and time
varying resource allocation over the period of a fixed duration campaign
running on a social network with a given adjacency matrix. Individuals in the
network are grouped according to their centrality measure and each group is
influenced by an external control function---implemented through
advertisements---during the campaign duration. The aim is to maximize an
objective function which is a linear combination of the reward due to the
fraction of informed individuals at the deadline, and the aggregated cost of
applying controls (advertising) over the campaign duration. We also study a
problem variant with a fixed budget constraint. We set up the optimality system
using Pontryagin's Maximum Principle from optimal control theory and solve it
numerically using the forward-backward sweep technique. Our formulation allows
us to compare the performance of various centrality measures (pagerank, degree,
closeness and betweenness) in maximizing the spread of a message in the optimal
control framework. We find that degree---a simple and local measure---performs
well on the three social networks used to demonstrate results: scientific
collaboration, Slashdot and Facebook. The optimal strategy targets central
nodes when the resource is scarce, but non-central nodes are targeted when the
resource is in abundance. Our framework is general and can be used in similar
studies for other disease or information spread models---that can be modeled
using a system of ordinary differential equations---for a network with a known
adjacency matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01006</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01006</id><created>2016-02-02</created><authors><author><keyname>Isack</keyname><forenames>Hossam</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author><author><keyname>Veksler</keyname><forenames>Olga</forenames></author></authors><title>A-expansion for multiple &quot;hedgehog&quot; shapes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overlapping colors and cluttered or weak edges are common segmentation
problems requiring additional regularization. For example, star-convexity is
popular for interactive single object segmentation due to simplicity and
amenability to exact graph cut optimization. This paper proposes an approach to
multiobject segmentation where objects could be restricted to separate
&quot;hedgehog&quot; shapes. We show that a-expansion moves are submodular for our
multi-shape constraints. Each &quot;hedgehog&quot; shape has its surface normals
constrained by some vector field, e.g. gradients of a distance transform for
user scribbles. Tight constraint give an extreme case of a shape prior
enforcing skeleton consistency with the scribbles. Wider cones of allowed
normals gives more relaxed hedgehog shapes. A single click and +/-90 degrees
normal orientation constraints reduce our hedgehog prior to star-convexity. If
all hedgehogs come from single clicks then our approach defines multi-star
prior. Our general method has significantly more applications than standard
one-star segmentation. For example, in medical data we can separate multiple
non-star organs with similar appearances and weak or noisy edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01007</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01007</id><created>2016-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Ren</keyname><forenames>Kan</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Optimal Real-Time Bidding Frameworks Discussion</title><categories>cs.GT</categories><comments>4 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This note is a complementary material for the solution of optimal real-time
bidding function in paper &quot;Optimal Real-Time Bidding for Display Advertising,
KDD 2014&quot;, where the estimated cost is taken as the bid price, i.e., the upper
bound of the true cost. Here we discuss a more general bid optimisation
framework with various utility and cost functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01013</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01013</id><created>2016-02-02</created><authors><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Hofman</keyname><forenames>Jake M.</forenames></author><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Anderson</keyname><forenames>Ashton</forenames></author><author><keyname>Watts</keyname><forenames>Duncan J.</forenames></author></authors><title>Exploring limits to prediction in complex social systems</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>12 pages, 6 figures, Proceedings of the 25th ACM International World
  Wide Web Conference (WWW) 2016</comments><doi>10.1145/2872427.2883001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How predictable is success in complex social systems? In spite of a recent
profusion of prediction studies that exploit online social and information
network data, this question remains unanswered, in part because it has not been
adequately specified. In this paper we attempt to clarify the question by
presenting a simple stylized model of success that attributes prediction error
to one of two generic sources: insufficiency of available data and/or models on
the one hand; and inherent unpredictability of complex social systems on the
other. We then use this model to motivate an illustrative empirical study of
information cascade size prediction on Twitter. Despite an unprecedented volume
of information about users, content, and past performance, our best performing
models can explain less than half of the variance in cascade sizes. In turn,
this result suggests that even with unlimited data predictive performance would
be bounded well below deterministic accuracy. Finally, we explore this
potential bound theoretically using simulations of a diffusion process on a
random scale free network similar to Twitter. We show that although higher
predictive power is possible in theory, such performance requires a homogeneous
system and perfect ex-ante knowledge of it: even a small degree of uncertainty
in estimating product quality or slight variation in quality across products
leads to substantially more restrictive bounds on predictability. We conclude
that realistic bounds on predictive accuracy are not dissimilar from those we
have obtained empirically, and that such bounds for other complex social
systems for which data is more difficult to obtain are likely even lower.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01016</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01016</id><created>2016-02-02</created><authors><author><keyname>Dinh</keyname><forenames>Thang N.</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Thai</keyname><forenames>My T.</forenames></author></authors><title>Network Clustering via Maximizing Modularity: Approximation Algorithms
  and Theoretical Limits</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>Appeared in IEEE ICDM 2015</comments><doi>10.1109/ICDM.2015.139</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many social networks and complex systems are found to be naturally divided
into clusters of densely connected nodes, known as community structure (CS).
Finding CS is one of fundamental yet challenging topics in network science. One
of the most popular classes of methods for this problem is to maximize Newman's
modularity. However, there is a little understood on how well we can
approximate the maximum modularity as well as the implications of finding
community structure with provable guarantees. In this paper, we settle
definitely the approximability of modularity clustering, proving that
approximating the problem within any (multiplicative) positive factor is
intractable, unless P = NP. Yet we propose the first additive approximation
algorithm for modularity clustering with a constant factor. Moreover, we
provide a rigorous proof that a CS with modularity arbitrary close to maximum
modularity QOPT might bear no similarity to the optimal CS of maximum
modularity. Thus even when CS with near-optimal modularity are found, other
verification methods are needed to confirm the significance of the structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01024</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01024</id><created>2016-02-02</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Arora</keyname><forenames>Raman</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>On Deep Multi-View Representation Learning: Objectives and Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning representations (features) in the setting in which we
have access to multiple unlabeled views of the data for learning while only one
view is available for downstream tasks. Previous work on this problem has
proposed several techniques based on deep neural networks, typically involving
either autoencoder-like networks with a reconstruction objective or paired
feedforward networks with a batch-style correlation-based objective. We analyze
several techniques based on prior work, as well as new variants, and compare
them empirically on image, speech, and text tasks. We find an advantage for
correlation-based representation learning, while the best results on most tasks
are obtained with our new variant, deep canonically correlated autoencoders
(DCCAE). We also explore a stochastic optimization procedure for minibatch
correlation-based objectives and discuss the time/performance trade-offs for
kernel-based and neural network-based implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01028</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01028</id><created>2016-02-02</created><authors><author><keyname>Sadraddini</keyname><forenames>Sadra</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>A Provably Correct MPC Approach to Safety Control of Urban Traffic
  Networks</title><categories>cs.SY</categories><comments>16 Pages, single column, shorter version to appear in the proceedings
  of 2016 American Control Conference (ACC), Boston, MA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model predictive control (MPC) is a popular strategy for urban traffic
management that is able to incorporate physical and user defined constraints.
However, the current MPC methods rely on finite horizon predictions that are
unable to guarantee desirable behaviors over long periods of time. In this
paper we design an MPC strategy that is guaranteed to keep the evolution of a
network in a desirable yet arbitrary -safe- set, while optimizing a finite
horizon cost function. Our approach relies on finding a robust controlled
invariant set inside the safe set that provides an appropriate terminal
constraint for the MPC optimization problem. An illustrative example is
included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01038</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01038</id><created>2016-02-02</created><authors><author><keyname>Ashour</keyname><forenames>Mahmoud</forenames></author><author><keyname>El-Keyi</keyname><forenames>Amr</forenames></author></authors><title>Interactive Multiple Model Estimation of Doubly-Selective Channels for
  OFDM systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an algorithm for channel estimation, acquisition
and tracking, for orthogonal frequency division multiplexing (OFDM) systems.
The proposed algorithm is suitable for vehicular communications that encounter
very high mobility. A preamble sequence is used to derive an initial estimate
of the channel using least squares (LS). The temporal variation of the channel
within one OFDM symbol is approximated by two complex exponential basis
expansion models (CE-BEM). One of the Fourier-based BEMs is intended to capture
the low frequencies in the channel (slow variations corresponding to low
Doppler), while the other is destined to capture high frequencies (fast
variations corresponding to high Doppler). Kalman filtering is employed to
track the BEM coefficients iteratively on an OFDM symbol-by-symbol basis. An
interactive multiple model (IMM) estimator is implemented to dynamically mix
the estimates obtained by the two Kalman filters, each of which matched to one
of the BEMs. Extensive numerical simulations are conducted to signify the gain
obtained by the proposed combining technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01040</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01040</id><created>2016-02-02</created><authors><author><keyname>Kim</keyname><forenames>HyeongSik</forenames></author><author><keyname>Anyanwu</keyname><forenames>Kemafor</forenames></author></authors><title>Scalable Ontological Query Processing over Semantically Integrated Life
  Science Datasets using MapReduce</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To address the requirement of enabling a comprehensive perspective of
life-sciences data, Semantic Web technologies have been adopted for
standardized representations of data and linkages between data. This has
resulted in data warehouses such as UniProt, Bio2RDF, and Chem2Bio2RDF, that
integrate different kinds of biological and chemical data using ontologies.
Unfortunately, the ability to process queries over ontologically-integrated
collections remains a challenge, particularly when data is large. The reason is
that besides the traditional challenges of processing graph-structured data,
complete query answering requires inferencing to explicate implicitly
represented facts. Since traditional inferencing techniques like forward
chaining are difficult to scale up, and need to be repeated each time data is
updated, recent focus has been on inferencing that can be supported using
database technologies via query rewriting. However, due to the richness of most
biomedical ontologies relative to other domain ontologies, the queries
resulting from the query rewriting technique are often more complex than
existing query optimization techniques can cope with. This is particularly so
when using the emerging class of cloud data processing platforms for big data
processing due to some additional overhead which they introduce. In this paper,
we present an approach for dealing such complex queries on big data using
MapReduce, along with an evaluation on existing real-world datasets and
benchmark queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01042</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01042</id><created>2016-02-02</created><authors><author><keyname>Cullina</keyname><forenames>Daniel</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Improved Achievability and Converse Bounds for Erd\H{o}s-R\'enyi Graph
  Matching</title><categories>cs.IT cs.LG math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of perfectly recovering the vertex correspondence
between two correlated Erd\H{o}s-R\'enyi (ER) graphs. For a pair of correlated
graphs on the same vertex set, the correspondence between the vertices can be
obscured by randomly permuting the vertex labels of one of the graphs. In some
cases, the structural information in the graphs allow this correspondence to be
recovered. We investigate the information-theoretic threshold for exact
recovery, i.e. the conditions under which the entire vertex correspondence can
be correctly recovered given unbounded computational resources.
  Pedarsani and Grossglauser provided an achievability result of this type.
Their result establishes the scaling dependence of the threshold on the number
of vertices. We improve on their achievability bound. We also provide a
converse bound, establishing conditions under which exact recovery is
impossible. Together, these establish the scaling dependence of the threshold
on the level of correlation between the two graphs. The converse and
achievability bounds differ by a factor of two for sparse, significantly
correlated graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01052</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01052</id><created>2016-02-02</created><authors><author><keyname>Schulz</keyname><forenames>Eric</forenames></author><author><keyname>Huys</keyname><forenames>Quentin J. M.</forenames></author><author><keyname>Bach</keyname><forenames>Dominik R.</forenames></author><author><keyname>Speekenbrink</keyname><forenames>Maarten</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Better safe than sorry: Risky function exploitation through safe
  optimization</title><categories>stat.AP cs.LG stat.ML</categories><comments>6 pages, submitted to Cognitive Science Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploration-exploitation of functions, that is learning and optimizing a
mapping between inputs and expected outputs, is ubiquitous to many real world
situations. These situations sometimes require us to avoid certain outcomes at
all cost, for example because they are poisonous, harmful, or otherwise
dangerous. We test participants' behavior in scenarios in which they have to
find the optimum of a function while at the same time avoid outputs below a
certain threshold. In two experiments, we find that Safe-Optimization, a
Gaussian Process-based exploration-exploitation algorithm, describes
participants' behavior well and that participants seem to care firstly whether
a point is safe and then try to pick the optimal point from all such safe
points. This means that their trade-off between exploration and exploitation
can be seen as an intelligent, approximate, and homeostasis-driven strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01059</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01059</id><created>2016-02-02</created><authors><author><keyname>Bonzon</keyname><forenames>Elise</forenames><affiliation>LIPADE</affiliation></author><author><keyname>Delobelle</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>CRIL</affiliation></author><author><keyname>Konieczny</keyname><forenames>S&#xe9;bastien</forenames><affiliation>CRIL</affiliation></author><author><keyname>Maudet</keyname><forenames>Nicolas</forenames><affiliation>LIP6</affiliation></author></authors><title>A Comparative Study of Ranking-based Semantics for Abstract
  Argumentation</title><categories>cs.AI</categories><comments>Proceedings of the 30th AAAI Conference on Artificial Intelligence
  (AAAI-2016), Feb 2016, Phoenix, United States</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Argumentation is a process of evaluating and comparing a set of arguments. A
way to compare them consists in using a ranking-based semantics which
rank-order arguments from the most to the least acceptable ones. Recently, a
number of such semantics have been proposed independently, often associated
with some desirable properties. However, there is no comparative study which
takes a broader perspective. This is what we propose in this work. We provide a
general comparison of all these semantics with respect to the proposed
properties. That allows to underline the differences of behavior between the
existing semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01061</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01061</id><created>2016-02-02</created><authors><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Waveform Optimization for SWIPT with Nonlinear Energy Harvester Modeling</title><categories>cs.IT cs.NI math.IT</categories><comments>to be presented at 20th International ITG Workshop on Smart Antennas
  (WSA 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous Wireless Information and Power Transfer (SWIPT) has attracted
significant attention in the communication community. The problem of waveform
design for SWIPT has however never been addressed so far. In this paper, a
novel SWIPT transceiver architecture is introduced relying on the superposition
of multisine and OFDM waveforms at the transmitter and a power-splitter
receiver equipped with an energy harvester and an information decoder capable
of cancelling the multisine waveforms. The SWIPT multisine/OFDM waveforms are
optimized so as to maximize the rate-energy region of the whole system. They
are adaptive to the channel state information and result from a posynomial
maximization problem that originates from the non-linearity of the energy
harvester. Numerical results illustrate the performance of the derived
waveforms and SWIPT architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01064</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01064</id><created>2016-02-02</created><updated>2016-02-09</updated><authors><author><keyname>Metzen</keyname><forenames>Jan Hendrik</forenames></author></authors><title>Minimum Regret Search for Single- and Multi-Task Optimization</title><categories>stat.ML cs.IT cs.LG cs.RO math.IT</categories><comments>Revising Section 3.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose minimum regret search (MRS), a novel acquisition function for
Bayesian optimization. MRS bears similarities with information-theoretic
approaches such as entropy search (ES). However, while ES aims in each query at
maximizing the information gain with respect to the global maximum, MRS aims at
minimizing the expected immediate regret of its ultimate recommendation for the
optimum. While empirically ES and MRS perform similar in most of the cases, MRS
produces fewer outliers with high regret than ES. We provide empirical results
both for a synthetic single-task optimization problem as well as for a
simulated multi-task robotic control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01065</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01065</id><created>2016-02-02</created><authors><author><keyname>Bramas</keyname><forenames>Quentin</forenames><affiliation>NPA,LIP6,UPMC,LINCS</affiliation></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames><affiliation>Department of Information and Computer sciences Osaka University</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>NPA,LIP6,UPMC,LINCS</affiliation></author></authors><title>Distributed Online Data Aggregation in Dynamic Graphs</title><categories>cs.DC cs.CC cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of aggregating data in a dynamic graph, that is,
aggregating the data that originates from all nodes in the graph to a specific
node, the sink. We are interested in giving lower bounds for this problem,
under different kinds of adversaries. In our model, nodes are endowed with
unlimited memory and unlimited computational power. Yet, we assume that
communications between nodes are carried out with pairwise interactions, where
nodes can exchange control information before deciding whether they transmit
their data or not, given that each node is allowed to transmit its data at most
once. When a node receives a data from a neighbor, the node may aggregate it
with its own data. We consider three possible adversaries: the online adaptive
adversary, the oblivious adversary , and the randomized adversary that chooses
the pairwise interactions uniformly at random. For the online adaptive and the
oblivious adversary, we give impossibility results when nodes have no knowledge
about the graph and are not aware of the future. Also, we give several tight
bounds depending on the knowledge (be it topology related or time related) of
the nodes. For the randomized adversary, we show that the Gathering algorithm,
which always commands a node to transmit, is optimal if nodes have no knowledge
at all. Also, we propose an algorithm called Waiting Greedy, where a node
either waits or transmits depending on some parameter, that is optimal when
each node knows its future pairwise interactions with the sink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01066</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01066</id><created>2016-02-02</created><authors><author><keyname>Boshkovska</keyname><forenames>Elena</forenames></author><author><keyname>Koelpin</keyname><forenames>Alexander</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Robust Beamforming for SWIPT Systems with Non-linear Energy Harvesting
  Model</title><categories>cs.IT math.IT</categories><comments>Invited paper, submitted to IEEE SPAWC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates resource allocation for simultaneous wireless
information and power transfer (SWIPT) downlink systems based on a non-linear
energy harvesting model. The resource allocation algorithm design is formulated
as a non-convex optimization problem for the maximization of the total
harvested power. The proposed problem formulation not only takes into account
imperfect channel state information (CSI) but also guarantees the
quality-of-service (QoS) of information transfer. A novel iterative algorithm
is proposed to obtain the globally optimal solution of the considered
non-convex optimization problem. In each iteration, a rank-constrained
semidefinite program (SDP) is solved optimally by SDP relaxation. Simulation
results demonstrate the significant gains in harvested power and the robustness
against CSI imperfection for the proposed optimal resource allocation, compared
to a baseline scheme designed for perfect CSI and the conventional linear
energy harvesting model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01075</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01075</id><created>2016-02-02</created><authors><author><keyname>Afshari</keyname><forenames>Mehrdad</forenames></author><author><keyname>Su</keyname><forenames>Zhendong</forenames></author></authors><title>Toward Rapid Transformation of Ideas into Software</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key mission of computer science is to enable people realize their creative
ideas as naturally and painlessly as possible. Software engineering is at the
center of this mission -- software technologies enable reification of ideas
into working systems. As computers become ubiquitous, both in availability and
the aspects of human lives they touch, the quantity and diversity of ideas also
rapidly grow. Our programming systems and technologies need to evolve to make
this reification process -- transforming ideas to software -- as quick and
accessible as possible.
  The goal of this paper is twofold. First, it advocates and highlights the
&quot;transforming ideas to software&quot; mission as a moonshot for software engineering
research. This is a long-term direction for the community, and there is no
silver bullet that can get us there. To make this mission a reality, as a
community, we need to improve the status quo across many dimensions. Thus, the
second goal is to outline a number of directions to modernize our contemporary
programming technologies for decades to come, describe work that has been
undertaken along those vectors, and pinpoint critical challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01084</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01084</id><created>2016-02-02</created><authors><author><keyname>Morgan</keyname><forenames>Alex J. L.</forenames></author><author><keyname>Barrow</keyname><forenames>David A.</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Hanczyc</keyname><forenames>Martin M.</forenames></author></authors><title>Simple fluidic digital half-adder</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fluidic one-bit half-adder is made of five channels which intersect at a
junction. Two channels are inputs, two channels are outputs and one channel is
the drain. The channels direct fluid from input fragments to output fragments
and the streams of fluid interact at the junctions. Binary signals are
represented by water droplets introduced in the input channels: presence of a
droplet in an input or output segments symbolises logical {\sc True}, absence
--- {\sc False}. The droplets travel along channels by following a path of
least resistance unless deflected at the junction. We demonstrate the function
of the half-adder in both computer modelling and laboratory experiments, and
propose a design of a one-bit full adder based on simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01103</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01103</id><created>2016-02-02</created><updated>2016-02-06</updated><authors><author><keyname>Tan</keyname><forenames>Chenhao</forenames></author><author><keyname>Niculae</keyname><forenames>Vlad</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>Winning Arguments: Interaction Dynamics and Persuasion Strategies in
  Good-faith Online Discussions</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>12 pages, 10 figures, to appear in Proceedings of WWW 2016, data and
  more at https://chenhaot.com/pages/changemyview.html (v2 made a minor
  correction on submission rules in ChangeMyView.)</comments><doi>10.1145/2872427.2883081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Changing someone's opinion is arguably one of the most important challenges
of social interaction. The underlying process proves difficult to study: it is
hard to know how someone's opinions are formed and whether and how someone's
views shift. Fortunately, ChangeMyView, an active community on Reddit, provides
a platform where users present their own opinions and reasoning, invite others
to contest them, and acknowledge when the ensuing discussions change their
original views. In this work, we study these interactions to understand the
mechanisms behind persuasion.
  We find that persuasive arguments are characterized by interesting patterns
of interaction dynamics, such as participant entry-order and degree of
back-and-forth exchange. Furthermore, by comparing similar counterarguments to
the same opinion, we show that language factors play an essential role. In
particular, the interplay between the language of the opinion holder and that
of the counterargument provides highly predictive cues of persuasiveness.
Finally, since even in this favorable setting people may not be persuaded, we
investigate the problem of determining whether someone's opinion is susceptible
to being changed at all. For this more difficult task, we show that stylistic
choices in how the opinion is expressed carry predictive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01107</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01107</id><created>2016-02-02</created><authors><author><keyname>Cheng</keyname><forenames>Justin</forenames></author><author><keyname>Adamic</keyname><forenames>Lada A</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>Do Cascades Recur?</title><categories>cs.SI physics.soc-ph stat.ML</categories><comments>WWW 2016</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascades of information-sharing are a primary mechanism by which content
reaches its audience on social media, and an active line of research has
studied how such cascades, which form as content is reshared from person to
person, develop and subside. In this paper, we perform a large-scale analysis
of cascades on Facebook over significantly longer time scales, and find that a
more complex picture emerges, in which many large cascades recur, exhibiting
multiple bursts of popularity with periods of quiescence in between. We
characterize recurrence by measuring the time elapsed between bursts, their
overlap and proximity in the social network, and the diversity in the
demographics of individuals participating in each peak. We discover that
content virality, as revealed by its initial popularity, is a main driver of
recurrence, with the availability of multiple copies of that content helping to
spark new bursts. Still, beyond a certain popularity of content, the rate of
recurrence drops as cascades start exhausting the population of interested
individuals. We reproduce these observed patterns in a simple model of content
recurrence simulated on a real social network. Using only characteristics of a
cascade's initial burst, we demonstrate strong performance in predicting
whether it will recur in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01116</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01116</id><created>2016-02-02</created><authors><author><keyname>Barton</keyname><forenames>Carl</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author></authors><title>Efficient Index for Weighted Sequences</title><categories>cs.DS</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding factors of a text string which are identical or
similar to a given pattern string is a central problem in computer science. A
generalised version of this problem consists in implementing an index over the
text to support efficient on-line pattern queries. We study this problem in the
case where the text is weighted: for every position of the text and every
letter of the alphabet a probability of occurrence of this letter at this
position is given. Sequences of this type, also called position weight
matrices, are commonly used to represent imprecise or uncertain data. A
weighted sequence may represent many different strings, each with probability
of occurrence equal to the product of probabilities of its letters at
subsequent positions. Given a probability threshold $1/z$, we say that a
pattern string $P$ matches a weighted text at position $i$ if the product of
probabilities of the letters of $P$ at positions $i,\ldots,i+|P|-1$ in the text
is at least $1/z$. In this article, we present an $O(nz)$-time construction of
an $O(nz)$-sized index that can answer pattern matching queries in a weighted
text in optimal time improving upon the state of the art by a factor of $z \log
z$. Other applications of this data structure include an $O(nz)$-time
construction of the weighted prefix table and an $O(nz)$-time computation of
all covers of a weighted sequence, which improve upon the state of the art by
the same factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01125</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01125</id><created>2016-02-02</created><authors><author><keyname>Bas</keyname><forenames>Anil</forenames></author><author><keyname>Smith</keyname><forenames>William A. P.</forenames></author><author><keyname>Bolkart</keyname><forenames>Timo</forenames></author><author><keyname>Wuhrer</keyname><forenames>Stefanie</forenames></author></authors><title>Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and
  Soft Correspondences</title><categories>cs.CV</categories><comments>5 pages, 5 figures. Submitted to IEEE International Conference on
  Image Processing 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a fully automatic method for fitting a 3D morphable model to
single face images in arbitrary pose and lighting. Our approach relies on
geometric features (edges and landmarks) and, inspired by the iterated closest
point algorithm, is based on computing hard correspondences between model
vertices and edge pixels. We demonstrate that this is superior to previous work
that uses soft correspondences to form an edge-derived cost surface that is
minimised by nonlinear optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01128</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01128</id><created>2016-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Sai</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author><author><keyname>Banavar</keyname><forenames>Mahesh K.</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Max Consensus in Sensor Networks: Non-linear Bounded Transmission and
  Additive Noise</title><categories>cs.SY</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed consensus algorithm for estimating the maximum value of the
initial measurements in a sensor network with communication noise is proposed.
In the absence of communication noise, max estimation can be done by updating
the state value with the largest received measurements in every iteration at
each sensor. In the presence of communication noise, however, the maximum
estimate will incorrectly drift and the estimate at each sensor will diverge.
As a result, a soft-max approximation together with a non-linear consensus
algorithm is introduced herein. A design parameter controls the trade-off
between the soft-max error and convergence speed. An analysis of this trade-off
gives a guideline towards how to choose the design parameter for the max
estimate. We also show that if some prior knowledge of the initial measurements
is available, the consensus process can converge faster by using an optimal
step size in the iterative algorithm. A shifted non-linear bounded transmit
function is also introduced for faster convergence when sensor nodes have some
prior knowledge of the initial measurements. Simulation results corroborating
the theory are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01130</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01130</id><created>2016-02-02</created><authors><author><keyname>Harshaw</keyname><forenames>Christopher R.</forenames></author><author><keyname>Bridges</keyname><forenames>Robert A.</forenames></author><author><keyname>Iannacone</keyname><forenames>Michael D.</forenames></author><author><keyname>Reed</keyname><forenames>Joel W.</forenames></author><author><keyname>Goodall</keyname><forenames>John R.</forenames></author></authors><title>GraphPrints: Towards a Graph Analytic Method for Network Anomaly
  Detection</title><categories>cs.CR stat.ML</categories><comments>4 pages submitted to Cyber &amp; Information Security Research Conference
  2016, ACM</comments><doi>10.1145/1235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel graph-analytic approach for detecting anomalies
in network flow data called GraphPrints. Building on foundational
network-mining techniques, our method represents time slices of traffic as a
graph, then counts graphlets -- small induced subgraphs that describe local
topology. By performing outlier detection on the sequence of graphlet counts,
anomalous intervals of traffic are identified, and furthermore, individual IPs
experiencing abnormal behavior are singled-out. Initial testing of GraphPrints
is performed on real network data with an implanted anomaly. Evaluation shows
false positive rates bounded by 2.84% at the time-interval level, and 0.05% at
the IP-level with 100% true positive rates at both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01132</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01132</id><created>2016-02-02</created><updated>2016-02-14</updated><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Hess</keyname><forenames>Tom</forenames></author></authors><title>Interactive algorithms: From pool to stream</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider interactive algorithms in the pool-based setting, and in the
stream-based setting. Interactive algorithms observe suggested elements
(representing actions or queries), and interactively select some of them and
receive responses. Stream-based algorithms are not allowed to select suggested
elements after more elements have been observed, while pool-based algorithms
can select elements at any order. We assume that the available elements are
generated independently according to some distribution, and design stream-based
algorithms that emulate black-box pool-based interactive algorithms. We provide
two such emulating algorithms. The first algorithm can emulate any pool-based
algorithm, but the number of suggested elements that need to be observed might
be exponential in the number of selected elements. The second algorithm applies
to the class of utility-based interactive algorithms, and the number of
suggested elements that it observes is linear in the number of selected
elements. For the case of utility-based emulation, we also provide a lower
bound showing that near-linearity is necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01137</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01137</id><created>2016-02-02</created><authors><author><keyname>Mitra</keyname><forenames>Bhaskar</forenames></author><author><keyname>Nalisnick</keyname><forenames>Eric</forenames></author><author><keyname>Craswell</keyname><forenames>Nick</forenames></author><author><keyname>Caruana</keyname><forenames>Rich</forenames></author></authors><title>A Dual Embedding Space Model for Document Ranking</title><categories>cs.IR</categories><comments>This paper is an extended evaluation and analysis of the model
  proposed in a poster to appear in WWW'16, April 11 - 15, 2016, Montreal,
  Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental goal of search engines is to identify, given a query, documents
that have relevant text. This is intrinsically difficult because the query and
the document may use different vocabulary, or the document may contain query
words without being relevant. We investigate neural word embeddings as a source
of evidence in document ranking. We train a word2vec embedding model on a large
unlabelled query corpus, but in contrast to how the model is commonly used, we
retain both the input and the output projections, allowing us to leverage both
the embedding spaces to derive richer distributional relationships. During
ranking we map the query words into the input space and the document words into
the output space, and compute a query-document relevance score by aggregating
the cosine similarities across all the query-document word pairs.
  We postulate that the proposed Dual Embedding Space Model (DESM) captures
evidence on whether a document is about a query term in addition to what is
modelled by traditional term-frequency based approaches. Our experiments show
that the DESM can re-rank top documents returned by a commercial Web search
engine, like Bing, better than a term-matching based signal like TF-IDF.
However, when ranking a larger set of candidate documents, we find the
embeddings-based approach is prone to false positives, retrieving documents
that are only loosely related to the query. We demonstrate that this problem
can be solved effectively by ranking based on a linear mixture of the DESM and
the word counting features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01139</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01139</id><created>2016-02-02</created><authors><author><keyname>Jacobsson</keyname><forenames>Sven</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Coldrey</keyname><forenames>Mikael</forenames></author><author><keyname>Gustavsson</keyname><forenames>Ulf</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Massive MIMO with Low-Resolution ADCs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the information-theoretic throughput that is achievable over a
fading communication link when the receiver is equipped with low-resolution
analog-to-digital converters (ADCs). We focus on the case where neither the
transmitter, nor the receiver have any a priori channel state information. This
means that the fading realizations have to be learned through pilot
transmission followed by channel estimation at the receiver, based on coarsely
quantized observations. For the extreme case of one-bit ADCs and for the
single-user single-input single-output case, we show that least squares (LS)
estimation combined with joint pilot-data processing is optimal in terms of the
achievable rate. We also investigate the uplink throughput achievable by a
massive multiple-input multiple-output system in which the base station is
equipped with low-resolution ADCs. We show that for the one-bit quantized case,
LS estimation together with maximal ratio combing or zero-forcing detection is
sufficient to support reliable multi-user transmission with high-order
constellations. Numerical results show that the rates achievable in the
infinite-precision (no quantization) case can be approached using ADCs with
only a few bits of resolution. The robustness of the low-resolution ADC system
against receive power imbalances between the different users, caused for
example by imperfect power control, is also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01149</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01149</id><created>2016-02-02</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Vellambi</keyname><forenames>Badri N.</forenames></author><author><keyname>Yeoh</keyname><forenames>Phee Lep</forenames></author><author><keyname>Kliewer</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Secure Index Coding: Existence and Construction</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the construction of secure index codes for a sender
transmitting to multiple receivers with side information in the presence of an
eavesdropper. We derive a sufficient and necessary condition for the existence
of index codes that are secure against an eavesdropper with access to any
subset of messages of cardinality $t$, for any fixed $t$. In contrast to secure
network coding, we show that random keys are not useful for three classes of
index-coding instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01154</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01154</id><created>2016-02-02</created><authors><author><keyname>Ghosh</keyname><forenames>Arnob</forenames></author><author><keyname>Sarkar</keyname><forenames>Saswati</forenames></author><author><keyname>Berry</keyname><forenames>Randall</forenames></author></authors><title>Secondary Spectrum Market: To acquire or not to acquire Side
  Information?</title><categories>cs.GT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a secondary spectrum market primaries set prices for their unused channels
to the secondaries. The payoff of a primary depends on the channel state
information (CSI) of its competitors. We consider a model where a primary can
acquire its competitors CSI at a cost. We formulate a game between two
primaries where each primary decides whether to acquire its competitor's CSI or
not and then selects its price based on that. Our result shows that no primary
will decide to acquire its competitor's CSI with probability $1$. When the cost
of acquiring the CSI is above a threshold, there is a unique Nash Equilibrium
(NE) where both the primaries remain uninformed of their respective
competitor's CSI. When the cost is below the threshold, in the unique NE each
primary randomizes between its decision to acquire the CSI or not. Our result
reveals that irrespective of the cost of acquiring the CSI, the expected payoff
of a primary remains the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01161</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01161</id><created>2016-02-02</created><authors><author><keyname>Azari</keyname><forenames>Amin</forenames></author></authors><title>Energy Efficient Scheduling for Grouped M2M Communications over Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>Accepted in Elsevier Ad Hoc Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, energy-efficient scheduling for grouped machine-type devices
deployed in cellular networks is investigated. We introduce a scheduling-based
cooperation incentive scheme which enables machine nodes to organize themselves
locally, create machine groups, and communicate through group representatives
to the base station. This scheme benefits from a novel scheduler design which
takes into account the cooperation level of each node, reimburses the extra
energy consumptions of group representatives, and maximizes the network
lifetime. As reusing cellular uplink resources for communications inside the
groups degrades the Quality of Service (QoS) of the primary users, analytical
results are provided which present a tradeoff between maximum allowable number
of simultaneously active machine groups in a given cell and QoS of the primary
users. Furthermore, we extend our derived solutions for the existing cellular
networks, propose a cooperation-incentive LTE scheduler, and present our
simulation results in the context of LTE. The simulation results show that the
proposed solutions significantly prolong the network lifetime. Also, it is
shown that under certain circumstances, reusing uplink resource by machine
devices can degrade the outage performance of the primary users significantly,
and hence, coexistence management of machine devices and cellular users is of
paramount importance for next generations of cellular networks in order to
enable group-based machine-type communications while guaranteeing QoS for the
primary users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01164</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01164</id><created>2016-02-02</created><authors><author><keyname>Miranda</keyname><forenames>Conrado S.</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando J.</forenames></author></authors><title>Single-Solution Hypervolume Maximization and its use for Improving
  Generalization of Neural Networks</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the hypervolume maximization with a single solution as
an alternative to the mean loss minimization. The relationship between the two
problems is proved through bounds on the cost function when an optimal solution
to one of the problems is evaluated on the other, with a hyperparameter to
control the similarity between the two problems. This same hyperparameter
allows higher weight to be placed on samples with higher loss when computing
the hypervolume's gradient, whose normalized version can range from the mean
loss to the max loss. An experiment on MNIST with a neural network is used to
validate the theory developed, showing that the hypervolume maximization can
behave similarly to the mean loss minimization and can also provide better
performance, resulting on a 20% reduction of the classification error on the
test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01168</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01168</id><created>2016-02-02</created><authors><author><keyname>Jiang</keyname><forenames>Zhuolin</forenames></author><author><keyname>Wang</keyname><forenames>Yaming</forenames></author><author><keyname>Davis</keyname><forenames>Larry</forenames></author><author><keyname>Andrews</keyname><forenames>Walt</forenames></author><author><keyname>Rozgic</keyname><forenames>Viktor</forenames></author></authors><title>Learning Discriminative Features via Label Consistent Neural Network</title><categories>cs.CV cs.LG cs.MM cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Convolutional Neural Network (CNN) enforces supervised information only
at the output layer, and hidden layers are trained by back propagating the
prediction error from the output layer without explicit supervision. We propose
a supervised feature learning approach, Label Consistency Neural Network, which
enforces direct supervision in late hidden layers. We associate each neuron in
a hidden layer with a particular class label and encourage it to be activated
for input signals from the same class. More specifically, we introduce a label
consistency regularization called &quot;discriminative representation error&quot; loss
for late hidden layers and combine it with classification error loss to build
our overall objective function. This label consistency constraint not only
alleviates the common problem of gradient vanishing and tends to faster
convergence, but also makes the features derived from late hidden layers
discriminative enough for classification even using a simple classifier such as
$k$-NN classifier, since input signals from the same class will have very
similar representations. Experimental results demonstrate that our approach can
achieve state-of-the-art performances on several public benchmarks for action
and object category recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01170</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01170</id><created>2016-02-02</created><authors><author><keyname>Alur</keyname><forenames>Rajeev</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Fisman</keyname><forenames>Dana</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Singh</keyname><forenames>Rishabh</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames><affiliation>Massachusetts Institute of Technology</affiliation></author></authors><title>Results and Analysis of SyGuS-Comp'15</title><categories>cs.PL cs.SE</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><acm-class>I.2.2, D.2.4, F.3.1;</acm-class><journal-ref>EPTCS 202, 2016, pp. 3-26</journal-ref><doi>10.4204/EPTCS.202.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an
implementation f that meets both a semantic constraint given by a logical
formula $\varphi$ in a background theory T, and a syntactic constraint given by
a grammar G, which specifies the allowed set of candidate implementations. Such
a synthesis problem can be formally defined in SyGuS-IF, a language that is
built on top of SMT-LIB.
  The Syntax-Guided Synthesis Competition (SyGuS-comp) is an effort to
facilitate, bring together and accelerate research and development of efficient
solvers for SyGuS by providing a platform for evaluating different synthesis
techniques on a comprehensive set of benchmarks. In this year's competition we
added two specialized tracks: a track for conditional linear arithmetic, where
the grammar need not be specified and is implicitly assumed to be that of the
LIA logic of SMT-LIB, and a track for invariant synthesis problems, with
special constructs conforming to the structure of an invariant synthesis
problem. This paper presents and analyzes the results of SyGuS-comp'15.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01171</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01171</id><created>2016-02-02</created><authors><author><keyname>Jacobs</keyname><forenames>Swen</forenames><affiliation>Saarland University, Saarbr&#xfc;cken, Germany</affiliation></author><author><keyname>Bloem</keyname><forenames>Roderick</forenames><affiliation>Graz University of Technology, Austria</affiliation></author><author><keyname>Brenguier</keyname><forenames>Romain</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Brussels, Belgium</affiliation></author><author><keyname>K&#xf6;nighofer</keyname><forenames>Robert</forenames><affiliation>Graz University of Technology, Austria</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Brussels, Belgium</affiliation></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Brussels, Belgium</affiliation></author><author><keyname>Ryzhyk</keyname><forenames>Leonid</forenames><affiliation>NICTA, Sydney, Australia and Carnegie Mellon University, Pittsburgh, USA</affiliation></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames><affiliation>Universit&#xe9; Libre de Bruxelles, Brussels, Belgium</affiliation></author><author><keyname>Seidl</keyname><forenames>Martina</forenames><affiliation>Johannes-Kepler-University, Linz, Austria</affiliation></author><author><keyname>Tentrup</keyname><forenames>Leander</forenames><affiliation>Saarland University, Saarbr&#xfc;cken, Germany</affiliation></author><author><keyname>Walker</keyname><forenames>Adam</forenames><affiliation>NICTA, Sydney, Australia</affiliation></author></authors><title>The Second Reactive Synthesis Competition (SYNTCOMP 2015)</title><categories>cs.LO</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 202, 2016, pp. 27-57</journal-ref><doi>10.4204/EPTCS.202.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on the design and results of the second reactive synthesis
competition (SYNTCOMP 2015). We describe our extended benchmark library, with 6
completely new sets of benchmarks, and additional challenging instances for 4
of the benchmark sets that were already used in SYNTCOMP 2014. To enhance the
analysis of experimental results, we introduce an extension of our benchmark
format with meta-information, including a difficulty rating and a reference
size for solutions. Tools are evaluated on a set of 250 benchmarks, selected to
provide a good coverage of benchmarks from all classes and difficulties. We
report on changes of the evaluation scheme and the experimental setup. Finally,
we describe the entrants into SYNTCOMP 2015, as well as the results of our
experimental evaluation. In our analysis, we emphasize progress over the tools
that participated last year.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01172</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01172</id><created>2016-02-02</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames><affiliation>Tel Aviv University</affiliation></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames><affiliation>Tel Aviv University</affiliation></author></authors><title>Synthesizing a Lego Forklift Controller in GR(1): A Case Study</title><categories>cs.SE</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 202, 2016, pp. 58-72</journal-ref><doi>10.4204/EPTCS.202.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive synthesis is an automated procedure to obtain a
correct-by-construction reactive system from a given specification. GR(1) is a
well-known fragment of linear temporal logic (LTL) where synthesis is possible
using a polynomial symbolic algorithm. We conducted a case study to learn about
the challenges that software engineers may face when using GR(1) synthesis for
the development of a reactive robotic system. In the case study we developed
two variants of a forklift controller, deployed on a Lego robot. The case study
employs LTL specification patterns as an extension of the GR(1) specification
language, an examination of two specification variants for execution
scheduling, traceability from the synthesized controller to constraints in the
specification, and generated counter strategies to support understanding
reasons for unrealizability. We present the specifications we developed, our
observations, and challenges faced during the case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01173</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01173</id><created>2016-02-02</created><authors><author><keyname>Filippidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Murray</keyname><forenames>Richard M.</forenames></author><author><keyname>Holzmann</keyname><forenames>Gerard J.</forenames></author></authors><title>A multi-paradigm language for reactive synthesis</title><categories>cs.LO cs.PL cs.SY</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 202, 2016, pp. 73-97</journal-ref><doi>10.4204/EPTCS.202.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a language for describing reactive synthesis problems
that integrates imperative and declarative elements. The semantics is defined
in terms of two-player turn-based infinite games with full information.
Currently, synthesis tools accept linear temporal logic (LTL) as input, but
this description is less structured and does not facilitate the expression of
sequential constraints. This motivates the use of a structured programming
language to specify synthesis problems. Transition systems and guarded commands
serve as imperative constructs, expressed in a syntax based on that of the
modeling language Promela. The syntax allows defining which player controls
data and control flow, and separating a program into assumptions and
guarantees. These notions are necessary for input to game solvers. The
integration of imperative and declarative paradigms allows using the paradigm
that is most appropriate for expressing each requirement. The declarative part
is expressed in the LTL fragment of generalized reactivity(1), which admits
efficient synthesis algorithms, extended with past LTL. The implementation
translates Promela to input for the Slugs synthesizer and is written in Python.
The AMBA AHB bus case study is revisited and synthesized efficiently,
identifying the need to reorder binary decision diagrams during strategy
construction, in order to prevent the exponential blowup observed in previous
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01174</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01174</id><created>2016-02-02</created><authors><author><keyname>Brenguier</keyname><forenames>Romain</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames><affiliation>Universit&#xe9; Libre de Bruxelles</affiliation></author></authors><title>Compositional Algorithms for Succinct Safety Games</title><categories>cs.LO</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><acm-class>F.3.1,B.1.2,B.6.3</acm-class><journal-ref>EPTCS 202, 2016, pp. 98-111</journal-ref><doi>10.4204/EPTCS.202.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the synthesis of circuits for succinct safety specifications given
in the AIG format. We show how AIG safety specifications can be decomposed
automatically into sub specifications. Then we propose symbolic compositional
algorithms to solve the synthesis problem compositionally starting for the
sub-specifications. We have evaluated the compositional algorithms on a set of
benchmarks including those proposed for the first synthesis competition
organised in 2014 by the Synthesis Workshop affiliated to the CAV conference.
We show that a large number of benchmarks can be decomposed automatically and
solved more efficiently with the compositional algorithms that we propose in
this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01175</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01175</id><created>2016-02-02</created><authors><author><keyname>Khalimov</keyname><forenames>Ayrat</forenames><affiliation>Graz University of Technology, Austria</affiliation></author></authors><title>Specification Format for Reactive Synthesis Problems</title><categories>cs.LO</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 202, 2016, pp. 112-119</journal-ref><doi>10.4204/EPTCS.202.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic synthesis from a given specification automatically constructs
correct implementation. This frees the user from the mundane implementation
work, but still requires the specification. But is specifying easier than
implementing? In this paper, we propose a user-friendly format to ease the
specification work, in particularly, that of specifying partial
implementations. Also, we provide scripts to convert specifications in the new
format into the SYNTCOMP format, thus benefiting from state of the art
synthesizers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01176</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01176</id><created>2016-02-02</created><authors><author><keyname>Huang</keyname><forenames>Xiaowei</forenames><affiliation>UNSW Australia</affiliation></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames><affiliation>UNSW Australia</affiliation></author></authors><title>The complexity of approximations for epistemic synthesis (extended
  abstract)</title><categories>cs.LO cs.MA</categories><comments>In Proceedings SYNT 2015, arXiv:1602.00786</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 202, 2016, pp. 120-137</journal-ref><doi>10.4204/EPTCS.202.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epistemic protocol specifications allow programs, for settings in which
multiple agents act with incomplete information, to be described in terms of
how actions are related to what the agents know. They are a variant of the
knowledge-based programs of Fagin et al [Distributed Computing, 1997],
motivated by the complexity of synthesizing implementations in that framework.
The paper proposes an approach to the synthesis of implementations of epistemic
protocol specifications, that reduces the problem of finding an implementation
to a sequence of model checking problems in approximations of the ultimate
system being synthesized. A number of ways to construct such approximations is
considered, and these are studied for the complexity of the associated model
checking problems. The outcome of the study is the identification of the best
approximations with the property of being PTIME implementable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01178</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01178</id><created>2016-02-02</created><authors><author><keyname>Cambria</keyname><forenames>Erik</forenames></author><author><keyname>Nguyen</keyname><forenames>Tam V.</forenames></author><author><keyname>Cheng</keyname><forenames>Brian</forenames></author><author><keyname>Kwok</keyname><forenames>Kenneth</forenames></author><author><keyname>Sepulveda</keyname><forenames>Jose</forenames></author></authors><title>GECKA3D: A 3D Game Engine for Commonsense Knowledge Acquisition</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commonsense knowledge representation and reasoning is key for tasks such as
artificial intelligence and natural language understanding. Since commonsense
consists of information that humans take for granted, gathering it is an
extremely difficult task. In this paper, we introduce a novel 3D game engine
for commonsense knowledge acquisition (GECKA3D) which aims to collect
commonsense from game designers through the development of serious games.
GECKA3D integrates the potential of serious games and games with a purpose.
This provides a platform for the acquisition of re-usable and multi-purpose
knowledge, and also enables the development of games that can provide
entertainment value and teach players something meaningful about the actual
world they live in.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01197</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01197</id><created>2016-02-03</created><authors><author><keyname>Huang</keyname><forenames>Chen</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Discriminative Sparse Neighbor Approximation for Imbalanced Learning</title><categories>cs.CV</categories><comments>11 pages, 10 figures, In submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data imbalance is common in many vision tasks where one or more classes are
rare. Without addressing this issue conventional methods tend to be biased
toward the majority class with poor predictive accuracy for the minority class.
These methods further deteriorate on small, imbalanced data that has a large
degree of class overlap. In this study, we propose a novel discriminative
sparse neighbor approximation (DSNA) method to ameliorate the effect of
class-imbalance during prediction. Specifically, given a test sample, we first
traverse it through a cost-sensitive decision forest to collect a good subset
of training examples in its local neighborhood. Then we generate from this
subset several class-discriminating but overlapping clusters and model each as
an affine subspace. From these subspaces, the proposed DSNA iteratively seeks
an optimal approximation of the test sample and outputs an unbiased prediction.
We show that our method not only effectively mitigates the imbalance issue, but
also allows the prediction to extrapolate to unseen data. The latter capability
is crucial for achieving accurate prediction on small dataset with limited
samples. The proposed imbalanced learning method can be applied to both
classification and regression tasks at a wide range of imbalance levels. It
significantly outperforms the state-of-the-art methods that do not possess an
imbalance handling mechanism, and is found to perform comparably or even better
than recent deep learning methods by using hand-crafted features only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01198</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01198</id><created>2016-02-03</created><updated>2016-02-12</updated><authors><author><keyname>Nock</keyname><forenames>Richard</forenames></author><author><keyname>Canyasse</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Boreli</keyname><forenames>Roksana</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>k-variates++: more pluses in the k-means++</title><categories>cs.LG</categories><acm-class>H.3.3; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  k-means++ seeding has become a de facto standard for hard clustering
algorithms. In this paper, our first contribution is a two-way generalisation
of this seeding, k-variates++, that includes the sampling of general densities
rather than just a discrete set of Dirac densities anchored at the point
locations, and a generalisation of the well known Arthur-Vassilvitskii (AV)
approximation guarantee, in the form of a bias+variance approximation bound of
the global optimum. This approximation exhibits a reduced dependency on the
&quot;noise&quot; component with respect to the optimal potential --- actually
approaching the statistical lower bound. We show that k-variates++ reduces to
efficient (biased seeding) clustering algorithms tailored to specific
frameworks; these include distributed, streaming and on-line clustering, with
direct approximation results for these algorithms. Finally, we present a novel
application of k-variates++ to differential privacy. For either the specific
frameworks considered here, or for the differential privacy setting, there is
little to no prior results on the direct application of k-means++ and its
approximation bounds --- state of the art contenders appear to be significantly
more complex and / or display less favorable (approximation) properties. We
stress that our algorithms can still be run in cases where there is \textit{no}
closed form solution for the population minimizer. We demonstrate the
applicability of our analysis via experimental evaluation on several domains
and settings, displaying competitive performances vs state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01202</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01202</id><created>2016-02-03</created><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Sharma</keyname><forenames>Abhishek A.</forenames></author><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author><author><keyname>Song</keyname><forenames>Seung-Hwan</forenames></author><author><keyname>Bandic</keyname><forenames>Zvonimir Z.</forenames></author><author><keyname>Bain</keyname><forenames>James A.</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Locally rewritable codes for resistive memories</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE International Conference on Communications (ICC)
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose locally rewritable codes (LWC) for resistive memories inspired by
locally repairable codes (LRC) for distributed storage systems. Small values of
repair locality of LRC enable fast repair of a single failed node since the
lost data in the failed node can be recovered by accessing only a small
fraction of other nodes. By using rewriting locality, LWC can improve endurance
limit and power consumption which are major challenges for resistive memories.
We point out the duality between LRC and LWC, which indicates that existing
construction methods of LRC can be applied to construct LWC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01205</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01205</id><created>2016-02-03</created><authors><author><keyname>Mir</keyname><forenames>Tariq Ahmad</forenames></author></authors><title>Citations to articles citing Benford's law: a Benford analysis</title><categories>cs.DL physics.soc-ph</categories><comments>10 pages, 1 figure, 4 tables, 37 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The observation that in large data the occurrence of first significant digits
of numbers is often governed by a logarithmically decreasing distribution,
quite far from the ordinarily expected uniform distribution, is called
Benford's law (BL). It was first reported by S. Newcomb and many decades later
independently by F. Benford. Due to its counter-intuitiveness the law was
ignored for decades as a mere curious observation. However, huge swell in
number of publications which the law has seen lately is an indication of its
remarkable resurgence. The law has come a long way, from obscurity to now being
a regular subject of books, peer reviewed papers, patents, blogs and news.
Here, we use Google Scholar (GS) to collect the data on the number of citations
received by the articles citing the original papers of Newcomb and Benford, and
then investigate whether the leading digits of this citations data are
distributed according to the law they discovered. We find that the monthly
citations data of the articles citing Benford's paper remarkably follow the law
but that of Newcomb's paper in some cases does/does not follow the law. On the
other hand, the yearly citations data corresponding to both Newcomb's and
Benford's paper follow the law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01208</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01208</id><created>2016-02-03</created><authors><author><keyname>Taniguchi</keyname><forenames>Akira</forenames></author><author><keyname>Taniguchi</keyname><forenames>Tadahiro</forenames></author><author><keyname>Inamura</keyname><forenames>Tetsunari</forenames></author></authors><title>Spatial Concept Acquisition for a Mobile Robot that Integrates
  Self-Localization and Unsupervised Word Discovery from Spoken Sentences</title><categories>cs.AI cs.CL cs.RO</categories><comments>Draft submitted to IEEE Transactions on Autonomous Mental Development
  (TAMD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel unsupervised learning method for the
lexical acquisition of words related to places visited by robots, from human
continuous speech signals. We address the problem of learning novel words by a
robot that has no prior knowledge of these words except for a primitive
acoustic model. Further, we propose a method that allows a robot to effectively
use the learned words and their meanings for self-localization tasks. The
proposed method is nonparametric Bayesian spatial concept acquisition method
(SpCoA) that integrates the generative model for self-localization and the
unsupervised word segmentation in uttered sentences via latent variables
related to the spatial concept. We implemented the proposed method SpCoA on
SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile
robot in a real environment. Further, we conducted experiments for evaluating
the performance of SpCoA. The experimental results showed that SpCoA enabled
the robot to acquire the names of places from speech sentences. They also
revealed that the robot could effectively utilize the acquired spatial concepts
and reduce the uncertainty in self-localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01218</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01218</id><created>2016-02-03</created><updated>2016-02-28</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>On the Accuracy of Interference Models in Wireless Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages, 3 figures, accepted in IEEE ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new framework for measuring and comparing the accuracy of any
wireless interference models used in the analysis and design of wireless
networks. Our approach is based on a new index that assesses the ability of the
interference model to correctly predict harmful interference events, i.e., link
outages. We use this new index to quantify the accuracy of various interference
models used in the literature, under various scenarios such as Rayleigh fading
wireless channels, directional antennas, and blockage (impenetrable obstacles)
in the network. Our analysis reveals that in highly directional antenna
settings with obstructions, even simple interference models (e.g., the
classical protocol model) are accurate, while with omnidirectional antennas,
more sophisticated and complex interference models (e.g., the classical
physical model) are necessary. Our new approach makes it possible to adopt the
appropriate interference model of adequate accuracy and simplicity in different
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01224</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01224</id><created>2016-02-03</created><authors><author><keyname>L&#xe1;vi&#x10d;ka</keyname><forenames>Miroslav</forenames></author><author><keyname>&#x160;&#xed;r</keyname><forenames>Zbyn&#x11b;k</forenames></author><author><keyname>Vr&#x161;ek</keyname><forenames>Jan</forenames></author></authors><title>Smooth surface interpolation using patches with rational offsets</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for the interpolation of given data points and
associated normals with surface parametric patches with rational normal fields.
We give some arguments why a dual approach is the most convenient for these
surfaces, which are traditionally called Pythagorean normal vector (PN)
surfaces. Our construction is based on the isotropic model of the dual space to
which the original data are pushed. Then the bicubic Coons patches are
constructed in the isotropic space and then pulled back to the standard three
dimensional space. As a result we obtain the patch construction which is
completely local and produces surfaces with the global G1~continuity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01225</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01225</id><created>2016-02-03</created><authors><author><keyname>Hackl</keyname><forenames>Christoph M.</forenames></author><author><keyname>Schechner</keyname><forenames>Korbinian</forenames></author></authors><title>Non-ideal torque control of wind turbine systems: Impacts on annual
  energy production</title><categories>cs.SY</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss non-ideal torque control in wind turbine systems (WTS). Most
high-level controllers generate a reference torque which is then send to the
underlying electrical drive system (generator+inverter) of the WTS to steer the
turbine/generator to its optimal operation point (depending on the wind speed).
The energy production heavily depends on the mechanical power (i.e. the product
of rotational speed and generator torque). However, since torque sensors in the
MW range are not available or extremely expensive, the torque controllers are
implemented as feedforward controllers and, therefore, are inherently sensitive
to parameter variations/uncertainties. Based on real wind data and a dynamical
WTS model, we discuss causes and impacts of non-ideal (feedforward) torque
control on the energy production and the gross earnings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01226</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01226</id><created>2016-02-03</created><authors><author><keyname>Al&#xe9;groth</keyname><forenames>Emil</forenames></author><author><keyname>Feldt</keyname><forenames>Robert</forenames></author><author><keyname>Kolstr&#xf6;m</keyname><forenames>Pirjo</forenames></author></authors><title>Maintenance of Automated Test Suites in Industry: An Empirical study on
  Visual GUI Testing</title><categories>cs.SE</categories><report-no>INFSOF5687</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Verification and validation (V&amp;V) activities make up 20 to 50
percent of the total development costs of a software system in practice. Test
automation is proposed to lower these V&amp;V costs but available research only
provides limited empirical data from industrial practice about the maintenance
costs of automated tests and what factors affect these costs. In particular,
these costs and factors are unknown for automated GUI-based testing.
  Objective: This paper addresses this lack of knowledge through analysis of
the costs and factors associated with the maintenance of automated GUI-based
tests in industrial practice.
  Method: An empirical study at two companies, Siemens and Saab, is reported
where interviews about, and empirical work with, Visual GUI Testing is
performed to acquire data about the technique's maintenance costs and
feasibility.
  Results: 13 factors are observed that affect maintenance, e.g. tester
knowledge/experience and test case complexity. Further, statistical analysis
shows that developing new test scripts is costlier than maintenance but also
that frequent maintenance is less costly than infrequent, big bang maintenance.
In addition a cost model, based on previous work, is presented that estimates
the time to positive return on investment (ROI) of test automation compared to
manual testing.
  Conclusions: It is concluded that test automation can lower overall software
development costs of a project whilst also having positive effects on software
quality. However, maintenance costs can still be considerable and the less time
a company currently spends on manual testing, the more time is required before
positive, economic, ROI is reached after automation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01228</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01228</id><created>2016-02-03</created><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>Image and Information</title><categories>cs.CV</categories><comments>9 pages, 7 figures. to be published in french by Belin publisher for
  a collaborative book project on &quot;Image and Communication&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-known old adage says that {\em &quot;A picture is worth a thousand words!&quot;}
(attributed to the Chinese philosopher Confucius ca 500 years BC). But more
precisely, what do we mean by information in images? And how can it be
retrieved effectively by machines? We briefly highlight these puzzling
questions in this column. But first of all, let us start by defining more
precisely what is meant by an &quot;Image.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01237</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01237</id><created>2016-02-03</created><authors><author><keyname>Zhang</keyname><forenames>Shanshan</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Omran</keyname><forenames>Mohamed</forenames></author><author><keyname>Hosang</keyname><forenames>Jan</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>How Far are We from Solving Pedestrian Detection?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encouraged by the recent progress in pedestrian detection, we investigate the
gap between current state-of-the-art methods and the &quot;perfect single frame
detector&quot;. We enable our analysis by creating a human baseline for pedestrian
detection (over the Caltech dataset), and by manually clustering the recurrent
errors of a top detector. Our results characterize both localization and
background-versus-foreground errors. To address localization errors we study
the impact of training annotation noise on the detector performance, and show
that we can improve even with a small portion of sanitized training data. To
address background/foreground discrimination, we study convnets for pedestrian
detection, and discuss which factors affect their performance. Other than our
in-depth analysis, we report top performance on the Caltech dataset, and
provide a new sanitized set of training and test annotations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01242</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01242</id><created>2016-02-03</created><updated>2016-02-19</updated><authors><author><keyname>Tabue</keyname><forenames>A. Fotue</forenames></author><author><keyname>Mart&#xed;nez-Moro</keyname><forenames>E.</forenames></author><author><keyname>Mouaha</keyname><forenames>C.</forenames></author></authors><title>Galois Correspondence on Linear Codes over Finite Chain Rings</title><categories>cs.IT math.IT</categories><msc-class>51E22, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $\texttt{S}|\texttt{R}$ a finite Galois extension of finite chain rings
and $\mathcal{B}$ an $\texttt{S}$-linear code we define two Galois operators,
the closure operator and the interior operator. We proof that a linear code is
Galois invariant if and only if the row standard form of its generator matrix
has all entries in the fixed ring by the Galois group and show a Galois
correspondence in the class of $\texttt{S}$-linear codes. As applications some
improvements of upper and lower bounds for the rank of the restriction and
trace code are given and some applications to $\texttt{S}$-linear cyclic codes
are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01246</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01246</id><created>2016-02-03</created><authors><author><keyname>Pernet</keyname><forenames>Clement</forenames><affiliation>UGA, ARIC</affiliation></author></authors><title>Computing with quasiseparable matrices</title><categories>cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of quasiseparable matrices is defined by a pair of bounds, called
the quasiseparable orders, on the ranks of the maximal sub-matrices entirely
located in their strictly lower and upper triangular parts. These arise
naturally in applications, as e.g. the inverse of band matrices, and are widely
used for they admit structured representations allowing to compute with them in
time linear in the dimension and quadratic with the quasiseparable order. We
show, in this paper, the connection between the notion of quasisepa-rability
and the rank profile matrix invariant, presented in [Dumas \&amp; al. ISSAC'15].
This allows us to propose an algorithm computing the quasiseparable orders (rL,
rU) in time O(n^2 s^($\omega$--2)) where s = max(rL, rU) and $\omega$ the
exponent of matrix multiplication. We then present two new structured
representations, a binary tree of PLUQ decompositions, and the Bruhat
generator, using respectively O(ns log n/s) and O(ns) field elements instead of
O(ns^2) for the previously known generators. We present algorithms computing
these representations in time O(n^2 s^($\omega$--2)). These representations
allow a matrix-vector product in time linear in the size of their
representation. Lastly we show how to multiply two such structured matrices in
time O(n^2 s^($\omega$--2)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01248</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01248</id><created>2016-02-03</created><authors><author><keyname>Nodarakis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Sioutas</keyname><forenames>Spyros</forenames></author><author><keyname>Tsakalidis</keyname><forenames>Athanasios</forenames></author><author><keyname>Tzimas</keyname><forenames>Giannis</forenames></author></authors><title>Using Hadoop for Large Scale Analysis on Twitter: A Technical Report</title><categories>cs.DB cs.CL cs.IR</categories><comments>8 pages, 3 tables, 3 figures</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis (or opinion mining) on Twitter data has attracted much
attention recently. One of the system's key features, is the immediacy in
communication with other users in an easy, user-friendly and fast way.
Consequently, people tend to express their feelings freely, which makes Twitter
an ideal source for accumulating a vast amount of opinions towards a wide
diversity of topics. This amount of information offers huge potential and can
be harnessed to receive the sentiment tendency towards these topics. However,
since none can invest an infinite amount of time to read through these tweets,
an automated decision making approach is necessary. Nevertheless, most existing
solutions are limited in centralized environments only. Thus, they can only
process at most a few thousand tweets. Such a sample, is not representative to
define the sentiment polarity towards a topic due to the massive number of
tweets published daily. In this paper, we go one step further and develop a
novel method for sentiment learning in the MapReduce framework. Our algorithm
exploits the hashtags and emoticons inside a tweet, as sentiment labels, and
proceeds to a classification procedure of diverse sentiment types in a parallel
and distributed manner. Moreover, we utilize Bloom filters to compact the
storage size of intermediate data and boost the performance of our algorithm.
Through an extensive experimental evaluation, we prove that our solution is
efficient, robust and scalable and confirm the quality of our sentiment
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01255</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01255</id><created>2016-02-03</created><authors><author><keyname>van Noord</keyname><forenames>Nanne</forenames></author><author><keyname>Postma</keyname><forenames>Eric</forenames></author></authors><title>Learning scale-variant and scale-invariant features for deep image
  classification</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1506.05929</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Convolutional Neural Network (CNN) trained on a corpus of images consists
of filters tuned to visual features relevant to the task at hand. Variations in
the resolution of the images and in the size of the objects and patterns
depicted, require the filters to both ignore task-irrelevant scale variations
(for recognizing a face, the size of the face is irrelevant) and to respond to
task-relevant features at a specific scale (given a scale, the shape and size
of the nose are relevant). Previous work focused on developing scale-invariant
filters in CNNs. This paper addresses the combined development of
scale-invariant and scale-variant filters. We propose a multi-scale CNN method
to encourage the development of both types of filters and evaluate it on a
challenging image classification task involving task-relevant characteristics
at multiple scales. The results show the multi-scale CNN to outperform
single-scale CNNs. This leads to the conclusion that encouraging the combined
development of scale-invariant and scale-variant filters in CNNs is beneficial
to image recognition performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01258</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01258</id><created>2016-02-03</created><authors><author><keyname>Grandi</keyname><forenames>Umberto</forenames></author><author><keyname>Turrini</keyname><forenames>Paolo</forenames></author></authors><title>A network-based rating system and its resistance to bribery</title><categories>cs.SI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a rating system in which a set of individuals (e.g., the customers
of a restaurant) evaluate a given service (e.g, the restaurant), with their
aggregated opinion determining the probability of all individuals to use the
service and thus its generated revenue. We explicitly model the influence
relation by a social network, with individuals being influenced by the
evaluation of their trusted peers. On top of that we allow a malicious service
provider (e.g., the restaurant owner) to bribe some individuals, i.e., to
invest a part of his or her expected income to modify their opinion, therefore
influencing his or her final gain. We analyse the effect of bribing strategies
under various constraints, and we show under what conditions the system is
bribery-proof, i.e., no bribing strategy yields a strictly positive expected
gain to the service provider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01261</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01261</id><created>2016-02-03</created><authors><author><keyname>Rahulamathavan</keyname><forenames>Yogachandran</forenames></author></authors><title>User Collusion Avoidance Scheme for Privacy-Preserving Decentralized
  Key-Policy Attribute-Based Encryption -- Full Version</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent trend towards cloud computing paradigm, smart devices and 4G wireless
technologies has enabled seamless data sharing among users. Cloud computing
environment is distributed and untrusted, hence data owners have to encrypt
their data to enforce data confidentiality. The data confidentiality in a
distributed environment can be achieved by using attribute-based encryption
technique. Decentralized attribute-based encryption technique is a variant of
multiple authority based attribute-based encryption whereby any attribute
authority can independently join and leave the system without collaborating
with the existing attribute authorities. In this paper, we propose a
privacy-preserving decentralized key-policy attribute-based encryption scheme.
The scheme preserves the user privacy when users interact with multiple
authorities to obtain decryption keys while mitigating the well-known user
collusion security vulnerability. We showed that our scheme relies on
decisional bilinear Diffie-Hellman standard complexity assumption in contrast
to the previous nonstandard complexity assumptions such as $q-$decisional
Diffie-Hellman inversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01265</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01265</id><created>2016-02-03</created><authors><author><keyname>Quax</keyname><forenames>Rick</forenames></author><author><keyname>Har-Shemesh</keyname><forenames>Omri</forenames></author><author><keyname>Sloot</keyname><forenames>Peter M. A.</forenames></author></authors><title>Quantifying synergistic information using intermediate stochastic
  variables</title><categories>cs.IT math.IT</categories><acm-class>E.4; H.1.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantifying synergy among stochastic variables is an important open problem
in information theory. Information synergy occurs when multiple sources
together predict an outcome variable better than the sum of single-source
predictions. It is an essential phenomenon in biology such as in neuronal
networks and cellular regulatory processes, where different information flows
integrate to produce a single response, but also in social cooperation
processes as well as in statistical inference tasks in machine learning. Here
we propose a metric of synergistic entropy and synergistic information from
first principles. The proposed measure relies on so-called synergistic random
variables (SRVs) which are constructed to have zero mutual information about
individual source variables but non-zero mutual information about the complete
set of source variables. We prove several basic and desired properties of our
measure, including bounds and additivity properties. In addition, we prove
several important consequences of our measure, including the fact that
different types of synergistic information may co-exist between the same sets
of variables. A numerical implementation is provided, which we use to
demonstrate that synergy is associated with resilience to noise. Our measure
may be a marked step forward in the study of multivariate information theory
and its numerous applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01286</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01286</id><created>2016-02-03</created><updated>2016-03-01</updated><authors><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Constructing Dominating Sets in Circulant Graphs</title><categories>math.CO cs.DM math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an efficient construction of a reasonably small dominating set in a
circulant graph on $n$ notes and $k$ distinct chord lengths. This result is
based on bounds on some double exponential sums. .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01295</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01295</id><created>2016-02-03</created><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author></authors><title>How proofs are prepared at Camelot</title><categories>cs.DS cs.CC</categories><comments>42 pp</comments><acm-class>F.2.2; G.2.2; G.3; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a design framework for robust, independently verifiable, and
workload-balanced distributed algorithms working on a common input. An
algorithm based on the framework is essentially a distributed encoding
procedure for a Reed--Solomon code, which enables (a) robustness against
byzantine failures with intrinsic error-correction and identification of failed
nodes, and (b) independent randomized verification to check the entire
computation for correctness, which takes essentially no more resources than
each node individually contributes to the computation. The framework builds on
recent Merlin--Arthur proofs of batch evaluation of Williams~[{\em Electron.\
Colloq.\ Comput.\ Complexity}, Report TR16-002, January 2016] with the
observation that {\em Merlin's magic is not needed} for batch evaluation---mere
Knights can prepare the proof, in parallel, and with intrinsic
error-correction.
  The contribution of this paper is to show that in many cases the verifiable
batch evaluation framework admits algorithms that match in total resource
consumption the best known sequential algorithm for solving the problem. As our
main result, we show that the $k$-cliques in an $n$-vertex graph can be counted
{\em and} verified in per-node $O(n^{(\omega+\epsilon)k/6})$ time and space on
$O(n^{(\omega+\epsilon)k/6})$ compute nodes, for any constant $\epsilon&gt;0$ and
positive integer $k$ divisible by $6$, where $2\leq\omega&lt;2.3728639$ is the
exponent of matrix multiplication. This matches in total running time the best
known sequential algorithm, due to Ne{\v{s}}et{\v{r}}il and Poljak [{\em
Comment.~Math.~Univ.~Carolin.}~26 (1985) 415--419], and considerably improves
its space usage and parallelizability. Further results include novel algorithms
for counting triangles in sparse graphs, computing the chromatic polynomial of
a graph, and computing the Tutte polynomial of a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01298</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01298</id><created>2016-02-03</created><authors><author><keyname>Silva</keyname><forenames>Ana</forenames></author><author><keyname>Linhares-Sales</keyname><forenames>Cl&#xe1;udia</forenames></author></authors><title>Graphs with Large Girth are b-continuous</title><categories>math.CO cs.DM</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A b-coloring of the vertices of a graph is a proper coloring where each color
class contains a vertex which is adjacent to each other color class. The
b-chromatic number of $G$ is the maximum integer $b(G)$ for which $G$ has a
b-coloring with $b(G)$ colors. A graph $G$ is b-continuous if $G$ has a
b-coloring with $k$ colors, for every integer $k$ in the interval
$[\chi(G),b(G)]$. It is known that not all graphs are b-continuous. Here, we
show that if $G$ has girth at least 10, then $G$ is b-continuous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01303</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01303</id><created>2016-02-03</created><authors><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Roe</keyname><forenames>David</forenames></author><author><keyname>Vaccon</keyname><forenames>Tristan</forenames></author></authors><title>Division and Slope Factorization of p-Adic Polynomials</title><categories>math.NT cs.SC</categories><proxy>ccsd</proxy><doi>10.1145/1235</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two important operations on polynomials defined over complete
discrete valuation fields: Euclidean division and factorization. In particular,
we design a simple and efficient algorithm for computing slope factorizations,
based on Newton iteration. One of its main features is that we avoid working
with fractional exponents. We pay particular attention to stability, and
analyze the behavior of the algorithm using several precision models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01304</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01304</id><created>2016-02-03</created><authors><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Neum&#xfc;ller</keyname><forenames>Martin</forenames></author><author><keyname>Radu</keyname><forenames>Silviu</forenames></author></authors><title>Inverse Inequality Estimates with Symbolic Computation</title><categories>cs.SC math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the convergence analysis of numerical methods for solving partial
differential equations (such as finite element methods) one arrives at certain
generalized eigenvalue problems, whose maximal eigenvalues need to be estimated
as accurate as possible. We apply symbolic computation methods to the situation
of square elements and are able to improve the previously known upper bound by
a factor of 8. More precisely, we try to evaluate the corresponding determinant
using the holonomic ansatz, which is a powerful tool for dealing with
determinants, proposed by Zeilberger in 2007. However, it turns out that this
method does not succeed on the problem at hand. As a solution we present a
variation of the original holonomic ansatz that is applicable to a larger class
of determinants, including the one we are dealing with here. We obtain an
explicit closed form for the determinant, whose special form enables us to
derive new and tight upper resp. lower bounds on the maximal eigenvalue, as
well as its asymptotic behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01321</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01321</id><created>2016-02-03</created><authors><author><keyname>Godfrey</keyname><forenames>Luke B.</forenames></author><author><keyname>Gashler</keyname><forenames>Michael S.</forenames></author></authors><title>A continuum among logarithmic, linear, and exponential functions, and
  its potential to improve generalization in neural networks</title><categories>cs.NE</categories><comments>6 pages, 8 figures, conference, In Proceedings of Knowledge Discovery
  and Information Retrieval (KDIR) 2015, Lisbon, Portugal, December 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the soft exponential activation function for artificial neural
networks that continuously interpolates between logarithmic, linear, and
exponential functions. This activation function is simple, differentiable, and
parameterized so that it can be trained as the rest of the network is trained.
We hypothesize that soft exponential has the potential to improve neural
network learning, as it can exactly calculate many natural operations that
typical neural networks can only approximate, including addition,
multiplication, inner product, distance, polynomials, and sinusoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01323</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01323</id><created>2016-02-03</created><authors><author><keyname>McCollum</keyname><forenames>Joey</forenames></author><author><keyname>Brown</keyname><forenames>Stephen</forenames></author></authors><title>Biclustering Readings and Manuscripts via Non-negative Matrix
  Factorization, with Application to the Text of Jude</title><categories>cs.LG</categories><comments>31 pages, 2 figures, 42 tables</comments><msc-class>6U815</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The text-critical practice of grouping witnesses into families or texttypes
often faces two obstacles: Contamination in the manuscript tradition, and
co-dependence in identifying characteristic readings and manuscripts. We
introduce non-negative matrix factorization (NMF) as a simple, unsupervised,
and efficient way to cluster large numbers of manuscripts and readings
simultaneously while summarizing contamination using an easy-to-interpret
mixture model. We apply this method to an extensive collation of the New
Testament epistle of Jude and show that the resulting clusters correspond to
human-identified textual families from existing research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01327</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01327</id><created>2016-02-03</created><authors><author><keyname>Alghamdi</keyname><forenames>Wael</forenames></author><author><keyname>Abediseid</keyname><forenames>Walid</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On The Construction of Capacity-Achieving Lattice Gaussian Codes</title><categories>cs.IT math.IT</categories><comments>9 pages, a short version of this paper has been submitted to the 2016
  IEEE International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new approach to proving results regarding channel
coding schemes based on construction-A lattices for the Additive White Gaussian
Noise (AWGN) channel that yields new characterizations of the code construction
parameters, i.e., the primes and dimensions of the codes, as functions of the
block-length. The approach we take introduces an averaging argument that
explicitly involves the considered parameters. This averaging argument is
applied to a generalized Loeliger ensemble to provide a more practical proof of
the existence of AWGN-good lattices, and to characterize suitable parameters
for the lattice Gaussian coding scheme proposed by Ling and Belfiore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01329</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01329</id><created>2016-02-03</created><authors><author><keyname>Yavits</keyname><forenames>Leonid</forenames></author><author><keyname>Morad</keyname><forenames>Amir</forenames></author><author><keyname>Ginosar</keyname><forenames>Ran</forenames></author></authors><title>Effect of Data Sharing on Private Cache Design in Chip Multiprocessors</title><categories>cs.AR</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multithreaded applications with high degree of data sharing, the miss rate
of private cache is shown to exhibit a compulsory miss component. It manifests
because at least some of the shared data originates from other cores and can
only be accessed in a shared cache. The compulsory component does not change
with the private cache size, causing its miss rate to diminish slower as the
cache size grows. As a result, the peak performance of a Chip Multiprocessor
(CMP) for workloads with high degree of data sharing is achieved with a smaller
private cache, compared to workloads with no data sharing. The CMP performance
can be improved by reassigning some of the constrained area or power resource
from private cache to core. Alternatively, the area or power budget of a CMP
can be reduced without a performance hit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01342</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01342</id><created>2016-02-03</created><authors><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Friedetzky</keyname><forenames>Tom</forenames></author><author><keyname>Kling</keyname><forenames>Peter</forenames></author><author><keyname>Mallmann-Trenn</keyname><forenames>Frederik</forenames></author><author><keyname>Wastell</keyname><forenames>Chris</forenames></author></authors><title>Plurality Consensus via Shuffling: Lessons Learned from Load Balancing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider \emph{plurality consensus} in a network of $n$ nodes. Initially,
each node has one of $k$ opinions. The nodes execute a (randomized) distributed
protocol to agree on the plurality opinion (the opinion initially supported by
the most nodes). Nodes in such networks are often quite cheap and simple, and
hence one seeks protocols that are not only fast but also simple and space
efficient. Typically, protocols depend heavily on the employed communication
mechanism, which ranges from sequential (only one pair of nodes communicates at
any time) to fully parallel (all nodes communicate with all their neighbors at
once) communication and everything in-between.
  We propose a framework to design protocols for a multitude of communication
mechanisms. We introduce protocols that solve the plurality consensus problem
and are with probability 1-o(1) both time and space efficient. Our protocols
are based on an interesting relationship between plurality consensus and
distributed load balancing. This relationship allows us to design protocols
that generalize the state of the art for a large range of problem parameters.
In particular, we obtain the same bounds as the recent result of Alistarh et
al. (who consider only two opinions on a clique) using a much simpler protocol
that generalizes naturally to general graphs and multiple opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01346</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01346</id><created>2016-02-03</created><updated>2016-02-15</updated><authors><author><keyname>Fulek</keyname><forenames>Radoslav</forenames></author></authors><title>C-planarity of Embedded Cyclic c-Graphs</title><categories>cs.CG</categories><comments>a revised version, title changed, 14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that c-planarity is solvable in quadratic time for flat clustered
graphs with three clusters if the combinatorial embedding of the underlying
abstract graph is fixed. In simpler graph-theoretical terms our result can be
viewed as follows. Given a graph $G$ with the vertex set partitioned into three
parts embedded on a 2-sphere, our algorithm decides if we can augment $G$ by
adding edges without creating an edge-crossing so that in the resulting
spherical graph the vertices of each part induce a connected sub-graph. We
proceed by a reduction to the problem of testing the existence of a perfect
matching in planar bipartite graphs. We formulate our result in a slightly more
general setting of cyclic clustered graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01348</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01348</id><created>2016-02-03</created><authors><author><keyname>Vijaykumar</keyname><forenames>Nandita</forenames></author><author><keyname>Pekhimenko</keyname><forenames>Gennady</forenames></author><author><keyname>Jog</keyname><forenames>Adwait</forenames></author><author><keyname>Ghose</keyname><forenames>Saugata</forenames></author><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Ausavarangnirun</keyname><forenames>Rachata</forenames></author><author><keyname>Das</keyname><forenames>Chita</forenames></author><author><keyname>Kandemir</keyname><forenames>Mahmut</forenames></author><author><keyname>Mowry</keyname><forenames>Todd C.</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>A Framework for Accelerating Bottlenecks in GPU Execution with Assist
  Warps</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Graphics Processing Units (GPUs) are well provisioned to support the
concurrent execution of thousands of threads. Unfortunately, different
bottlenecks during execution and heterogeneous application requirements create
imbalances in utilization of resources in the cores. For example, when a GPU is
bottlenecked by the available off-chip memory bandwidth, its computational
resources are often overwhelmingly idle, waiting for data from memory to
arrive.
  This work describes the Core-Assisted Bottleneck Acceleration (CABA)
framework that employs idle on-chip resources to alleviate different
bottlenecks in GPU execution. CABA provides flexible mechanisms to
automatically generate &quot;assist warps&quot; that execute on GPU cores to perform
specific tasks that can improve GPU performance and efficiency.
  CABA enables the use of idle computational units and pipelines to alleviate
the memory bandwidth bottleneck, e.g., by using assist warps to perform data
compression to transfer less data from memory. Conversely, the same framework
can be employed to handle cases where the GPU is bottlenecked by the available
computational units, in which case the memory pipelines are idle and can be
used by CABA to speed up computation, e.g., by performing memoization using
assist warps.
  We provide a comprehensive design and evaluation of CABA to perform effective
and flexible data compression in the GPU memory hierarchy to alleviate the
memory bandwidth bottleneck. Our extensive evaluations show that CABA, when
used to implement data compression, provides an average performance improvement
of 41.7% (as high as 2.6X) across a variety of memory-bandwidth-sensitive GPGPU
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01352</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01352</id><created>2016-02-03</created><authors><author><keyname>Martiel</keyname><forenames>Simon</forenames></author><author><keyname>Martin</keyname><forenames>Bruno</forenames></author></authors><title>Universality of causal graph dynamics</title><categories>cs.DM cs.DC</categories><comments>long version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal Graph Dynamics generalize Cellular Automata, extending them to bounded
degree, time varying graphs. The dynamics rewrite the graph at each time step
with respect to two physics-like symmetries: causality (bounded speed of
information) and homogeneity (the rewriting acts the same everywhere on the
graph, at every time step). Universality is the ability simulating every other
instances of another (or the same) model of computation. In this work, we study
three different notions of simulation for Causal Graph Dynamics, each of them
leading to a definition of universality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01358</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01358</id><created>2016-02-03</created><authors><author><keyname>Zamani</keyname><forenames>Majid</forenames></author><author><keyname>Tkachev</keyname><forenames>Ilya</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Towards Scalable Synthesis of Stochastic Control Systems</title><categories>cs.SY cs.FL math.OC</categories><comments>22 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1407.2730</comments><msc-class>93E03, 68Q60, 93C10</msc-class><acm-class>D.2.4; B.1.2; B.5.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal control synthesis approaches over stochastic systems have received
significant attention in the past few years, in view of their ability to
provide provably correct controllers for complex logical specifications in an
automated fashion. Examples of complex specifications of interest include
properties expressed as formulae in linear temporal logic (LTL) or as automata
on infinite strings. A general methodology to synthesize controllers for such
properties resorts to symbolic abstractions of the given stochastic systems.
Symbolic models are discrete abstractions of the given concrete systems with
the property that a controller designed on the abstraction can be refined (or
implemented) into a controller on the original system. Although the recent
development of techniques for the construction of symbolic models has been
quite encouraging, the general goal of formal synthesis over stochastic control
systems is by no means solved. A fundamental issue with the existing techniques
is the known &quot;curse of dimensionality,&quot; which is due to the need to discretize
state and input sets and that results in an exponential complexity over the
number of state and input variables in the concrete system. In this work we
propose a novel abstraction technique for incrementally stable stochastic
control systems, which does not require state-space discretization but only
input set discretization, and that can be potentially more efficient (and thus
scalable) than existing approaches. We elucidate the effectiveness of the
proposed approach by synthesizing a schedule for the coordination of two
traffic lights under some safety and fairness requirements for a road traffic
model. Further we argue that this 5-dimensional linear stochastic control
system cannot be studied with existing approaches based on state-space
discretization due to the very large number of generated discrete states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01365</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01365</id><created>2016-02-03</created><authors><author><keyname>Kavvos</keyname><forenames>G. A.</forenames></author></authors><title>On the Semantics of Intensional Recursion</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the concept of intensionality and its relevance for computer
science, as well as the ways in which `intension' can be turned into a logical
construct. We describe the need for bicategories in that endeavour. Then,
inspired by the categorical semantics of modal logic, we introduce exposures, a
new 2-categorical construct that is meant to abstractly capture intensional
constructions, e.g. G\&quot;odel numberings. In our new framework, the classic
results of Kleene, G\&quot;odel, Tarski, and Rice are very simple to reproduce.
Moreover, it is easy to isolate the expressive power required to reproduce
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01366</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01366</id><created>2016-02-03</created><authors><author><keyname>Barcelo</keyname><forenames>Pablo</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Pieris</keyname><forenames>Andreas</forenames></author></authors><title>Semantic Acyclicity Under Constraints</title><categories>cs.DB cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conjunctive query (CQ) is semantically acyclic if it is equivalent to an
acyclic one. Semantic acyclicity has been studied in the constraint-free case,
and deciding whether a query enjoys this property is NP-complete. However, in
case the database is subject to constraints such as tuple-generating
dependencies (tgds) that can express, e.g., inclusion dependencies, or
equality-generating dependencies (egds) that capture, e.g., functional
dependencies, a CQ may turn out to be semantically acyclic under the
constraints while not semantically acyclic in general. This opens avenues to
new query optimization techniques. In this paper, we initiate and develop the
theory of semantic acyclicity under constraints. We study the following natural
problem: Given a CQ and a set of constraints, is the query semantically acyclic
under the constraints, or, in other words, is the query equivalent to an
acyclic one over all those databases that satisfy the set of constraints?
  We show that decidability of CQ containment is a necessary but not sufficient
condition for the decidability of semantic acyclicity. In particular, we show
that semantic acyclicity is undecidable in presence of full tgds (i.e., Datalog
rules). In view of this fact, we focus on the main classes of tgds for which CQ
containment is decidable, and do not capture the class of full tgds, namely
guarded, non-recursive and sticky tgds. For these classes we show that semantic
acyclicity is decidable, and its complexity coincides with the complexity of CQ
containment. In the case of egds, we show that semantic acyclicity is
undecidable even over unary and binary predicates. When restricted to keys the
problem becomes decidable (NP-complete) over such schemas. We finally consider
the problem of evaluating a semantically acyclic query over a database that
satisfies a set of constraints. For guarded tgds the evaluation problem is
tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01376</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01376</id><created>2016-02-03</created><authors><author><keyname>Yu</keyname><forenames>Chenhan D.</forenames></author><author><keyname>March</keyname><forenames>William B.</forenames></author><author><keyname>Xiao</keyname><forenames>Bo</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>Inv-ASKIT: A Parallel Fast Diret Solver for Kernel Matrices</title><categories>cs.NA cs.DS cs.MS</categories><comments>11 pages, 2 figures, to appear in IPDPS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel algorithm for computing the approximate factorization
of an $N$-by-$N$ kernel matrix. Once this factorization has been constructed
(with $N \log^2 N $ work), we can solve linear systems with this matrix with $N
\log N $ work. Kernel matrices represent pairwise interactions of points in
metric spaces. They appear in machine learning, approximation theory, and
computational physics. Kernel matrices are typically dense (matrix
multiplication scales quadratically with $N$) and ill-conditioned (solves can
require 100s of Krylov iterations). Thus, fast algorithms for matrix
multiplication and factorization are critical for scalability.
  Recently we introduced ASKIT, a new method for approximating a kernel matrix
that resembles N-body methods. Here we introduce INV-ASKIT, a factorization
scheme based on ASKIT. We describe the new method, derive complexity estimates,
and conduct an empirical study of its accuracy and scalability. We report
results on real-world datasets including &quot;COVTYPE&quot; ($0.5$M points in 54
dimensions), &quot;SUSY&quot; ($4.5$M points in 8 dimensions) and &quot;MNIST&quot; (2M points in
784 dimensions) using shared and distributed memory parallelism. In our largest
run we approximately factorize a dense matrix of size 32M $\times$ 32M
(generated from points in 64 dimensions) on 4,096 Sandy-Bridge cores. To our
knowledge these results improve the state of the art by several orders of
magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01385</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01385</id><created>2016-02-03</created><authors><author><keyname>Fluschnik</keyname><forenames>Till</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>The Minimum Shared Edges Problem on Planar Graphs</title><categories>cs.CC</categories><comments>7 pages, 3 figures</comments><msc-class>68Q17, 68Q25, 68R10, 05C10</msc-class><acm-class>F.1.3; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Minimum Shared Edges problem introduced by Omran et al. [Journal
of Combinatorial Optimization, 2015] on planar graphs: Planar MSE asks, given a
planar graph G = (V,E), two distinct vertices s,t in V , and two integers p, k,
whether there are p s-t paths in G that share at most k edges, where an edges
is called shared if it appears in at least two of the p s-t paths. We show that
Planar MSE is NP-hard by reduction from Vertex Cover. We make use of a
grid-like structure, where the alignment (horizontal/vertical) of the edges in
the grid correspond to selection and validation gadgets respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01387</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01387</id><created>2016-02-03</created><updated>2016-02-10</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author></authors><title>True State Complexity of Binary Operations on Regular Languages</title><categories>cs.FL</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I study the state complexity of binary operations on regular languages over
different alphabets. It is well known that if $L'_m$ and $L_n$ are languages
over the same alphabet with $m$ and $n$ quotients, respectively, the state
complexity of any binary boolean operation on $L'_m$ and $L_n$ is $mn$, and
that of the product (concatenation) is $(m-1)2^n +2^{n-1}$. In contrast to
this, I show that if $L'_m$ and $L_n$ are over their own different alphabets,
the state complexity of union and symmetric difference is $mn+m+n+1$, that of
intersection is $mn+1$, that of difference is $mn+m+1$, and that of the product
is $m2^n+2^{n-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01388</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01388</id><created>2016-02-03</created><authors><author><keyname>Holovatch</keyname><forenames>Yurij</forenames></author><author><keyname>Mryglod</keyname><forenames>Olesya</forenames></author><author><keyname>Szell</keyname><forenames>Michael</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Analyses of a Virtual World</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 7 figures. To appear in: &quot;Maths Meets Myths:
  Complexity-science approaches to folktales, myths, sagas, and histories.&quot;
  Editors: R. Kenna, M. Mac Carron, P. Mac Carron. (Springer, 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an overview of a series of results obtained from the analysis of
human behavior in a virtual environment. We focus on the massive multiplayer
online game (MMOG) Pardus which has a worldwide participant base of more than
400,000 registered players. We provide evidence for striking statistical
similarities between social structures and human-action dynamics in the real
and virtual worlds. In this sense MMOGs provide an extraordinary way for
accurate and falsifiable studies of social phenomena. We further discuss
possibilities to apply methods and concepts developed in the course of these
studies to analyse oral and written narratives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01396</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01396</id><created>2016-02-03</created><authors><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author><author><keyname>Michon</keyname><forenames>Gerard P.</forenames></author></authors><title>Making Walks Count: From Silent Circles to Hamiltonian Cycles</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We illustrate the application of the matrix-transfer method for a number of
enumeration problems concerning the party game &quot;silent circles&quot;, Hamiltonian
cycles in the antiprism graphs, and simple paths and cycles of a fixed length
in arbitrary graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01398</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01398</id><created>2016-02-03</created><authors><author><keyname>Habib</keyname><forenames>Usman</forenames></author><author><keyname>Zucker</keyname><forenames>Gerhard</forenames></author></authors><title>Finding the different patterns in buildings data using bag of words
  representation with clustering</title><categories>cs.AI</categories><doi>10.1109/FIT.2015.60</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The understanding of the buildings operation has become a challenging task
due to the large amount of data recorded in energy efficient buildings. Still,
today the experts use visual tools for analyzing the data. In order to make the
task realistic, a method has been proposed in this paper to automatically
detect the different patterns in buildings. The K Means clustering is used to
automatically identify the ON (operational) cycles of the chiller. In the next
step the ON cycles are transformed to symbolic representation by using Symbolic
Aggregate Approximation (SAX) method. Then the SAX symbols are converted to bag
of words representation for hierarchical clustering. Moreover, the proposed
technique is applied to real life data of adsorption chiller. Additionally, the
results from the proposed method and dynamic time warping (DTW) approach are
also discussed and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01404</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01404</id><created>2016-02-03</created><authors><author><keyname>Vlachou</keyname><forenames>C.</forenames></author><author><keyname>Rodrigues</keyname><forenames>J.</forenames></author><author><keyname>Mateus</keyname><forenames>P.</forenames></author><author><keyname>Paunkovi&#x107;</keyname><forenames>N.</forenames></author><author><keyname>Souto</keyname><forenames>A.</forenames></author></authors><title>Quantum walks public key cryptographic system</title><categories>quant-ph cs.CR</categories><journal-ref>International Journal of Quantum Information, Vol. 13, No. 6
  (2015) 1550050</journal-ref><doi>10.1142/S0219749915500501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum Cryptography is a rapidly developing field of research that benefits
from the properties of Quantum Mechanics in performing cryptographic tasks.
Quantum walks are a powerful model for quantum computation and very promising
for quantum information processing. In this paper, we present a quantum
public-key cryptographic system based on quantum walks. In particular, in the
proposed protocol the public key is given by a quantum state generated by
performing a quantum walk. We show that the protocol is secure and analyze the
complexity of public-key generation and encryption/decryption procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01407</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01407</id><created>2016-02-03</created><authors><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Martens</keyname><forenames>James</forenames></author></authors><title>A Kronecker-factored approximate Fisher matrix for convolution layers</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Second-order optimization methods such as natural gradient descent have the
potential to speed up training of neural networks by correcting for the
curvature of the loss function. Unfortunately, the exact natural gradient is
impractical to compute for large models, and most approximations either require
an expensive iterative procedure or make crude approximations to the curvature.
We present Kronecker Factors for Convolution (KFC), a tractable approximation
to the Fisher matrix for convolutional networks based on a structured
probabilistic model for the distribution over backpropagated derivatives.
Similarly to the recently proposed Kronecker-Factored Approximate Curvature
(K-FAC), each block of the approximate Fisher matrix decomposes as the
Kronecker product of small matrices, allowing for efficient inversion. KFC
captures important curvature information while still yielding comparably
efficient updates to stochastic gradient descent (SGD). We show that the
updates are invariant to commonly used reparameterizations, such as centering
of the activations. In our experiments, approximate natural gradient descent
with KFC was able to train convolutional networks several times faster than
carefully tuned SGD. Furthermore, it was able to train the networks in 10-20
times fewer iterations than SGD, suggesting its potential applicability in a
distributed setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01409</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01409</id><created>2016-02-03</created><authors><author><keyname>Onaran</keyname><forenames>Efe</forenames></author><author><keyname>Garg</keyname><forenames>Siddharth</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Optimal De-Anonymization in Random Graphs with Community Structure</title><categories>cs.SI</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymized social network graphs published for academic or advertisement
purposes are subject to de-anonymization attacks by leveraging side information
in the form of a second, public social network graph correlated with the
anonymized graph. This is because the two are from the same underlying graph of
true social relationships. In this paper, we (i) characterize the maximum a
posteriori (MAP) estimates of user identities for the anonymized graph and (ii)
provide sufficient conditions for successful de-anonymization for underlying
graphs with community structure. Our results generalize prior work that assumed
underlying graphs of Erd\H{o}s-R\'enyi type, in addition to proving the
optimality of the attack strategy adopted in the prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01410</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01410</id><created>2016-02-03</created><authors><author><keyname>Sim&#xf5;es</keyname><forenames>Miguel</forenames></author><author><keyname>Almeida</keyname><forenames>Luis B.</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author></authors><title>A General Framework for Fast Image Deconvolution with Incomplete
  Observations. Applications to Unknown Boundaries, Inpainting,
  Superresolution, and Demosaicing</title><categories>cs.CV stat.ML</categories><comments>13 pages, 12 figures. Submitted. MATLAB code to be made available
  soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image deconvolution problems, the diagonalization of the underlying
operators by means of the FFT usually yields very large speedups. When there
are incomplete observations (e.g., in the case of unknown boundaries), standard
deconvolution techniques normally involve non-diagonalizable
operators---resulting in rather slow methods---or, otherwise, use inexact
convolution models, resulting in the occurrence of artifacts in the enhanced
images. In this paper, we propose a new deconvolution framework for images with
incomplete observations that allows us to work with diagonalized convolution
operators, and therefore is very fast. We iteratively alternate the estimation
of the unknown pixels and of the deconvolved image, using, e.g., a FFT-based
deconvolution method. In principle, any fast deconvolution method can be used.
We give an example in which a published method that assumes periodic boundary
conditions is extended, through the use of this framework, to unknown boundary
conditions. Furthermore, we propose an implementation of this framework, based
on the alternating direction method of multipliers (ADMM). We provide a proof
of convergence for the resulting algorithm, which can be seen as a &quot;partial&quot;
ADMM, in which not all variables are dualized. We report experimental
comparisons with other primal-dual methods, in which the proposed one performed
at the level of the state of the art. Four different kinds of applications were
tested in the experiments: deconvolution, deconvolution with inpainting,
superresolution, and demosaicing, all with unknown boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01412</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01412</id><created>2016-02-03</created><authors><author><keyname>Qu</keyname><forenames>Hang</forenames></author><author><keyname>Mashayekhi</keyname><forenames>Omid</forenames></author><author><keyname>Terei</keyname><forenames>David</forenames></author><author><keyname>Levis</keyname><forenames>Philip</forenames></author></authors><title>Canary: A Scheduling Architecture for High Performance Cloud Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Canary, a scheduling architecture that allows high performance
analytics workloads to scale out to run on thousands of cores. Canary is
motivated by the observation that a central scheduler is a bottleneck for high
performance codes: a handful of multicore workers can execute tasks faster than
a controller can schedule them.
  The key insight in Canary is to reverse the responsibilities between
controllers and workers. Rather than dispatch tasks to workers, which then
fetch data as necessary, in Canary the controller assigns data partitions to
workers, which then spawn and schedule tasks locally.
  We evaluate three benchmark applications in Canary on up to 64 servers and
1,152 cores on Amazon EC2. Canary achieves up to 9-90X speedup over Spark and
up to 4X speedup over GraphX, a highly optimized graph analytics engine. While
current centralized schedulers can schedule 2,500 tasks/second, each Canary
worker can schedule 136,000 tasks/second per core and experiments show this
scales out linearly, with 64 workers scheduling over 120 million tasks per
second, allowing Canary to support optimized jobs running on thousands of
cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01416</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01416</id><created>2016-02-03</created><authors><author><keyname>Congiu</keyname><forenames>Roberto</forenames></author><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Santucci</keyname><forenames>Fortunato</forenames></author></authors><title>On the Relay-Fallback Tradeoff in Millimeter Wave Wireless System</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, accepted in IEEE INFOCOM mmNet Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) communications systems are promising candidate to
support extremely high data rate services in future wireless networks. MmWave
communications exhibit high penetration loss (blockage) and require directional
transmissions to compensate for severe channel attenuations and for high noise
powers. When blockage occurs, there are at least two simple prominent options:
1) switching to the conventional microwave frequencies (fallback option) and 2)
using an alternative non-blocked path (relay option). However, currently it is
not clear under which conditions and network parameters one option is better
than the other. To investigate the performance of the two options, this paper
proposes a novel blockage model that allows deriving maximum achievable
throughput and delay performance of both options. A simple criterion to decide
which option should be taken under which network condition is provided. By a
comprehensive performance analysis, it is shown that the right option depends
on the payload size, beam training overhead, and blockage probability. For a
network with light traffic and low probability of blockage in the direct link,
the fallback option is throughput- and delay-optimal. For a network with heavy
traffic demands and semi-static topology (low beam-training overhead), the
relay option is preferable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01421</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01421</id><created>2016-02-03</created><updated>2016-02-26</updated><authors><author><keyname>Zheng</keyname><forenames>Da</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua</forenames></author><author><keyname>Priebe</keyname><forenames>Carey E.</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author></authors><title>An SSD-based eigensolver for spectral analysis on billion-node graphs</title><categories>cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many eigensolvers such as ARPACK and Anasazi have been developed to compute
eigenvalues of a large sparse matrix. These eigensolvers are limited by the
capacity of RAM. They run in memory of a single machine for smaller eigenvalue
problems and require the distributed memory for larger problems.
  In contrast, we develop an SSD-based eigensolver framework called FlashEigen,
which extends Anasazi eigensolvers to SSDs, to compute eigenvalues of a graph
with hundreds of millions or even billions of vertices in a single machine.
FlashEigen performs sparse matrix multiplication in a semi-external memory
fashion, i.e., we keep the sparse matrix on SSDs and the dense matrix in
memory. We store the entire vector subspace on SSDs and reduce I/O to improve
performance through caching the most recent dense matrix. Our result shows that
FlashEigen is able to achieve 40%-60% performance of its in-memory
implementation and has performance comparable to the Anasazi eigensolvers on a
machine with 48 CPU cores. Furthermore, it is capable of scaling to a graph
with 3.4 billion vertices and 129 billion edges. It takes about four hours to
compute eight eigenvalues of the billion-node graph using 120 GB memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01425</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01425</id><created>2016-02-03</created><authors><author><keyname>Fan</keyname><forenames>Ping</forenames></author><author><keyname>Zhou</keyname><forenames>Ri-Gui</forenames></author><author><keyname>Jing</keyname><forenames>Naihuan</forenames></author><author><keyname>Li</keyname><forenames>Hai-Sheng</forenames></author></authors><title>Geometric transformations of multidimensional color images based on NASS</title><categories>quant-ph cs.IT math.IT</categories><comments>32 pages</comments><journal-ref>Information Sciences 340-341 (2016), 191-208</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present quantum algorithms to realize geometric transformations (two-point
swappings, symmetric flips, local flips, orthogonal rotations, and
translations) based on an $n$-qubit normal arbitrary superposition state
(NASS). These transformations are implemented using quantum circuits consisting
of basic quantum gates, which are constructed with polynomial numbers of
single-qubit and two-qubit gates. Complexity analysis shows that the global
operators (symmetric flips, local flips, orthogonal rotations) can be
implemented with $O(n)$ gates. The proposed geometric transformations are used
to facilitate applications of quantum images with low complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01428</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01428</id><created>2016-02-03</created><authors><author><keyname>Dou</keyname><forenames>Jason</forenames></author><author><keyname>Sun</keyname><forenames>Ni</forenames></author><author><keyname>Zou</keyname><forenames>Xiaojun</forenames></author></authors><title>&quot;Draw My Topics&quot;: Find Desired Topics fast from large scale of Corpus</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the &quot;Draw My Topics&quot; toolkit, which provides a fast way to
incorporate social scientists' interest into standard topic modelling. Instead
of using raw corpus with primitive processing as input, an algorithm based on
Vector Space Model and Conditional Entropy are used to connect social
scientists' willingness and unsupervised topic models' output. Space for users'
adjustment on specific corpus of their interest is also accommodated. We
demonstrate the toolkit's use on the Diachronic People's Daily Corpus in
Chinese.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01441</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01441</id><created>2016-02-03</created><authors><author><keyname>Alagic</keyname><forenames>Gorjan</forenames></author><author><keyname>Broadbent</keyname><forenames>Anne</forenames></author><author><keyname>Fefferman</keyname><forenames>Bill</forenames></author><author><keyname>Gagliardoni</keyname><forenames>Tommaso</forenames></author><author><keyname>Schaffner</keyname><forenames>Christian</forenames></author><author><keyname>Jules</keyname><forenames>Michael St.</forenames></author></authors><title>Computational Security of Quantum Encryption</title><categories>quant-ph cs.CR</categories><comments>31 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum-mechanical devices have the potential to transform cryptography. Most
research in this area has focused either on the information-theoretic
advantages of quantum protocols or on the security of classical cryptographic
schemes against quantum attacks. In this work, we initiate the study of another
relevant topic: the encryption of quantum data in the computational setting.
  In this direction, we establish quantum versions of several fundamental
classical results. First, we develop natural definitions for private-key and
public-key encryption schemes for quantum data. We then define notions of
semantic security and indistinguishability, and, in analogy with the classical
work of Goldwasser and Micali, show that these notions are equivalent. Finally,
we construct secure quantum encryption schemes from basic primitives. In
particular, we show that quantum-secure one-way functions imply IND-CCA1-secure
symmetric-key quantum encryption, and that quantum-secure trapdoor one-way
permutations imply semantically-secure public-key quantum encryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01443</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01443</id><created>2016-02-03</created><authors><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Some Pairs Problems</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common form of MapReduce application involves discovering relationships
between certain pairs of inputs. Similarity joins serve as a good example of
this type of problem, which we call a &quot;some-pairs&quot; problem. In the framework of
Afrati et al. (VLDB 2013), algorithms are measured by the tradeoff between
reducer size (maximum number of inputs a reducer can handle) and the
replication rate (average number of reducers to which an input must be sent.
There are two obvious approaches to solving some-pairs problems in general. We
show that no general-purpose MapReduce algorithm can beat both of these two
algorithms in the worst case. We then explore a recursive algorithm for solving
some-pairs problems and heuristics for beating the lower bound on common
instances of the some-pairs class of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01449</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01449</id><created>2016-02-02</created><authors><author><keyname>MacGahan</keyname><forenames>Christopher J.</forenames></author><author><keyname>Kupinski</keyname><forenames>Matthew A.</forenames></author><author><keyname>Hilton</keyname><forenames>Nathan R.</forenames></author><author><keyname>Brubaker</keyname><forenames>Erik M.</forenames></author><author><keyname>Johnson</keyname><forenames>William C.</forenames></author></authors><title>Development of an Ideal Observer that Incorporates Nuisance Parameters
  and Processes List-Mode Data</title><categories>physics.data-an cs.CV</categories><report-no>SAND2016-0849J</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observer models were developed to process data in list-mode format in order
to perform binary discrimination tasks for use in an arms-control-treaty
context. Data used in this study was generated using GEANT4 Monte Carlo
simulations for photons using custom models of plutonium inspection objects and
a radiation imaging system. Observer model performance was evaluated and
presented using the area under the receiver operating characteristic curve. The
ideal observer was studied under both signal-known-exactly conditions and in
the presence of unknowns such as object orientation and absolute count-rate
variability; when these additional sources of randomness were present, their
incorporation into the observer yielded superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01458</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01458</id><created>2016-02-03</created><authors><author><keyname>Tajeddine</keyname><forenames>Razan</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author></authors><title>Private Information Retrieval from MDS Coded Data in Distributed Storage
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of providing privacy, in the private information
retrieval (PIR) sense, to users requesting data from a distributed storage
system (DSS). The DSS uses a Maximum Distance Separable (MDS) code to store the
data reliably on unreliable storage nodes. Some of these nodes can be spies
which report to a third party, such as an oppressive regime, which data is
being requested by the user. An information theoretic PIR scheme ensures that a
user can satisfy its request while revealing, to the spy nodes, no information
on which data is being requested. A user can achieve PIR by downloading all the
data in the DSS. However, this is not a feasible solution due to its high
communication cost. We construct PIR schemes with low download communication
cost. When there is one spy node, we construct PIR schemes with download cost
$\frac{1}{1-R}$ per unit of requested data ($R$ is the code rate), achieving
the information theoretic limit for linear schemes. An important property of
these schemes is their universality since they depend on the code rate, but not
on the MDS code itself. When there are two spy nodes for rate $R\leq 1/2$
rates, we devise PIR schemes that have download cost independent of the total
size of the data in the DSS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01464</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01464</id><created>2016-02-03</created><authors><author><keyname>Kouskouridas</keyname><forenames>Rigas</forenames></author><author><keyname>Tejani</keyname><forenames>Alykhan</forenames></author><author><keyname>Doumanoglou</keyname><forenames>Andreas</forenames></author><author><keyname>Tang</keyname><forenames>Danhang</forenames></author><author><keyname>Kim</keyname><forenames>Tae-Kyun</forenames></author></authors><title>Latent-Class Hough Forests for 6 DoF Object Pose Estimation</title><categories>cs.CV</categories><comments>PAMI submission, project page:
  http://www.iis.ee.ic.ac.uk/rkouskou/research/LCHF.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present Latent-Class Hough Forests, a method for object
detection and 6 DoF pose estimation in heavily cluttered and occluded
scenarios. We adapt a state of the art template matching feature into a
scale-invariant patch descriptor and integrate it into a regression forest
using a novel template-based split function. We train with positive samples
only and we treat class distributions at the leaf nodes as latent variables.
During testing we infer by iteratively updating these distributions, providing
accurate estimation of background clutter and foreground occlusions and, thus,
better detection rate. Furthermore, as a by-product, our Latent-Class Hough
Forests can provide accurate occlusion aware segmentation masks, even in the
multi-instance scenario. In addition to an existing public dataset, which
contains only single-instance sequences with large amounts of clutter, we have
collected two, more challenging, datasets for multiple-instance detection
containing heavy 2D and 3D clutter as well as foreground occlusions. We provide
extensive experiments on the various parameters of the framework such as patch
size, number of trees and number of iterations to infer class distributions at
test time. We also evaluate the Latent-Class Hough Forests on all datasets
where we outperform state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01483</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01483</id><created>2016-02-03</created><authors><author><keyname>Fullmer</keyname><forenames>Daniel</forenames></author><author><keyname>Wang</keyname><forenames>Lili</forenames></author><author><keyname>Morse</keyname><forenames>A. Stephen</forenames></author></authors><title>A Distributed Algorithm for Computing a Common Fixed Point of a Family
  of Paracontractions</title><categories>math.OC cs.MA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed algorithm is described for finding a common fixed point of a
family of $m&gt;1$ nonlinear maps $M_i : \mathbb{R}^n \rightarrow \mathbb{R}^n$
assuming that each map is a paracontraction. The common fixed point is
simultaneously computed by $m$ agents assuming each agent $i$ knows only $M_i$,
the current estimates of the fixed point generated by its neighbors, and
nothing more. Each agent recursively updates its estimate of the fixed point by
utilizing the current estimates generated by each of its neighbors. Neighbor
relations are characterized by a time-dependent directed graph $\mathbb{N}(t)$
whose vertices correspond to agents and whose arcs depict neighbor relations.
It is shown that for any family of paracontractions $M_i, i \in
\{1,2,\ldots,m\}$ which has at least one common fixed point, and any sequence
of strongly connected neighbor graphs $\mathbb{N}(t)$, $t=1,2,\ldots$, the
algorithm causes all agent estimates to converge a common fixed point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01506</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01506</id><created>2016-02-03</created><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Burke</keyname><forenames>James V.</forenames></author><author><keyname>Drusvyatskiy</keyname><forenames>Dmitriy</forenames></author><author><keyname>Friedlander</keyname><forenames>Michael P.</forenames></author><author><keyname>Roy</keyname><forenames>Scott</forenames></author></authors><title>Level-set methods for convex optimization</title><categories>math.OC cs.NA</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex optimization problems arising in applications often have favorable
objective functions and complicated constraints, thereby precluding first-order
methods from being immediately applicable. We describe an approach that
exchanges the roles of the objective and constraint functions, and instead
approximately solves a sequence of parametric level-set problems. A
zero-finding procedure, based on inexact function evaluations and possibly
inexact derivative information, leads to an efficient solution scheme for the
original problem. We describe the theoretical and practical properties of this
approach for a broad range of problems, including low-rank semidefinite
optimization, sparse optimization, and generalized linear models for inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01509</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01509</id><created>2016-02-03</created><updated>2016-03-07</updated><authors><author><keyname>Luan</keyname><forenames>Tom H.</forenames></author><author><keyname>Gao</keyname><forenames>Longxiang</forenames></author><author><keyname>Li</keyname><forenames>Zhi</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author><author><keyname>We</keyname><forenames>Guiyi</forenames></author><author><keyname>Sun</keyname><forenames>Limin</forenames></author></authors><title>A View of Fog Computing from Networking Perspective</title><categories>cs.NI</categories><comments>The manuscript is an update version of arXiv:1502.01815 and has
  substantial text overlap with arXiv:1502.01815. It is therefore requested to
  be withdrawn to avoid duplication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With smart devices, particular smartphones, becoming our everyday companions,
the ubiquitous mobile Internet and computing applications pervade people's
daily lives. With the surge demand on high-quality mobile services at anywhere,
how to address the ubiquitous user demand and accommodate the explosive growth
of mobile traffics is the key issue of the next generation mobile networks. The
Fog computing is a promising solution towards this goal. Fog computing extends
cloud computing by providing virtualized resources and engaged location-based
services to the edge of the mobile networks so as to better serve mobile
traffics. Therefore, Fog computing is a lubricant of the combination of cloud
computing and mobile applications. In this article, we outline the main
features of Fog computing and describe its concept, architecture and design
goals. Lastly, we discuss some of the future research issues from the
networking perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01510</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01510</id><created>2016-02-03</created><authors><author><keyname>Panda</keyname><forenames>Priyadarshini</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Unsupervised Regenerative Learning of Hierarchical Features in Spiking
  Deep Networks for Object Recognition</title><categories>cs.NE</categories><comments>8 pages, 9 figures, &lt;Under review in IJCNN 2016&gt;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a spike-based unsupervised regenerative learning scheme to train
Spiking Deep Networks (SpikeCNN) for object recognition problems using
biologically realistic leaky integrate-and-fire neurons. The training
methodology is based on the Auto-Encoder learning model wherein the
hierarchical network is trained layer wise using the encoder-decoder principle.
Regenerative learning uses spike-timing information and inherent latencies to
update the weights and learn representative levels for each convolutional layer
in an unsupervised manner. The features learnt from the final layer in the
hierarchy are then fed to an output layer. The output layer is trained with
supervision by showing a fraction of the labeled training dataset and performs
the overall classification of the input. Our proposed methodology yields
0.92%/29.84% classification error on MNIST/CIFAR10 datasets which is comparable
with state-of-the-art results. The proposed methodology also introduces
sparsity in the hierarchical feature representations on account of event-based
coding resulting in computationally efficient learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01511</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01511</id><created>2016-02-03</created><authors><author><keyname>Tang</keyname><forenames>Chunming</forenames></author><author><keyname>Xiang</keyname><forenames>Can</forenames></author><author><keyname>Feng</keyname><forenames>Keqin</forenames></author></authors><title>Linear codes with a few weights from inhomogeneous quadratic functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes with few weights have been an interesting subject of study for
many years, as these codes have applications in secrete sharing, authentication
codes, association schemes, and strongly regular graphs. In this paper, linear
codes with a few weights are constructed from inhomogeneous quadratic functions
over the finite field $\gf(p)$, where $p$ is an odd prime. They include some
earlier linear codes as special cases. The weight distributions of these linear
codes are also determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01516</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01516</id><created>2016-02-03</created><authors><author><keyname>Parhizi</keyname><forenames>Sina</forenames></author><author><keyname>Khodaei</keyname><forenames>Amin</forenames></author></authors><title>Market-based Microgrid Optimal Scheduling</title><categories>cs.SY</categories><comments>Appeared in 6th IEEE International Conference on Smart Grid
  Communications (SmartGridComm 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an optimal scheduling model for a microgrid participating
in the electricity distribution market in interaction with the Distribution
Market Operator (DMO). The DMO is a concept proposed here, which administers
the established electricity market in the distribution level, i.e., similar to
the role of Independent System Operator (ISO) in the wholesale electricity
market, sets electricity prices, determines the amounts of the power exchange
between market participators, and interacts with the ISO. Considering a
predetermined main grid power transfer to the microgrid, the microgrid
scheduling problem will aim at balancing the power supply and demand while
taking financial objectives into account. A stochastic programming method is
employed to model prevailing uncertainties in the microgrid grid-connected and
islanded operations. Numerical simulations exhibit the application and the
effectiveness of the proposed market-based microgrid scheduling model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01517</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01517</id><created>2016-02-03</created><authors><author><keyname>Nogueira</keyname><forenames>Keiller</forenames></author><author><keyname>Penatti</keyname><forenames>Ot&#xe1;vio A. B.</forenames></author><author><keyname>Santos</keyname><forenames>Jefersson A. dos</forenames></author></authors><title>Towards Better Exploiting Convolutional Neural Networks for Remote
  Sensing Scene Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analysis of three possible strategies for exploiting the power
of existing convolutional neural networks (ConvNets) in different scenarios
from the ones they were trained: full training, fine tuning, and using ConvNets
as feature extractors. In many applications, especially including remote
sensing, it is not feasible to fully design and train a new ConvNet, as this
usually requires a considerable amount of labeled data and demands high
computational costs. Therefore, it is important to understand how to obtain the
best profit from existing ConvNets. We perform experiments with six popular
ConvNets using three remote sensing datasets. We also compare ConvNets in each
strategy with existing descriptors and with state-of-the-art baselines. Results
point that fine tuning tends to be the best performing strategy. In fact, using
the features from the fine-tuned ConvNet with linear SVM obtains the best
results. We also achieved state-of-the-art results for the three datasets used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01520</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01520</id><created>2016-02-03</created><authors><author><keyname>Parhizi</keyname><forenames>Sina</forenames></author><author><keyname>Khodaei</keyname><forenames>Amin</forenames></author></authors><title>Investigating the Necessity of Distribution Markets in Accomodating High
  Penetration Microgrids</title><categories>cs.SY</categories><comments>To appear in &quot;IEEE PES Transmission and Distribution Conference,
  Dallas, TX, 2016.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increased need for reliable, resilient, and high quality power combined
with a falling cost of distributed generation technologies has resulted in a
rapid growth of microgrid in power systems. Although providing multitude of
benefits, the microgrid power transfer with the main grid, which is commonly
obtained using economy and reliability consideration, may result in major
operational drawbacks, most notably, a large mismatch between actual and
forecasted system loads. This paper investigates the impact of high penetration
microgrids on the power system net load, and further proposes three paradigms
that can be adopted to address the emerging operational issues. The IEEE 6-bus
test system is used for numerical studies and to further support the
discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01528</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01528</id><created>2016-02-03</created><authors><author><keyname>Han</keyname><forenames>Song</forenames></author><author><keyname>Liu</keyname><forenames>Xingyu</forenames></author><author><keyname>Mao</keyname><forenames>Huizi</forenames></author><author><keyname>Pu</keyname><forenames>Jing</forenames></author><author><keyname>Pedram</keyname><forenames>Ardavan</forenames></author><author><keyname>Horowitz</keyname><forenames>Mark A.</forenames></author><author><keyname>Dally</keyname><forenames>William J.</forenames></author></authors><title>EIE: Efficient Inference Engine on Compressed Deep Neural Network</title><categories>cs.CV cs.AR</categories><comments>External links to this paper: TheNextPlatform:
  http://www.nextplatform.com/2015/12/08/emergent-chip-vastly-accelerates-deep-neural-networks/
  O'Reilly:
  https://www.oreilly.com/ideas/compressed-representations-in-the-age-of-big-data
  Hacker News: https://news.ycombinator.com/item?id=10881683 Video:
  https://www.youtube.com/watch?v=hfFkS_vHslI&amp;feature=youtu.be</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-of-the art deep neural networks (DNNs) have hundreds of millions of
connections and are both computationally and memory intensive, making them
difficult to deploy on embedded systems with limited hardware resources and
power budgets. While custom hardware can help the computation, fetching the
weights from DRAM can be as much as two orders of magnitude more expansive than
ALU operation, and dominates the required power. Previously proposed
compression makes it possible to fit state-of-the-art DNNs (AlexNet with 60
million parameters, VGG-16 with 130 million parameters) fully in on-chip SRAM.
This compression is achieved by pruning the redundant connections and having
multiple connections share the same weight. We propose an energy efficient
inference engine (EIE) that performs inference on this compressed network model
and accelerates the inherent modified sparse matrix-vector multiplication.
Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to
CPU and GPU implementations of the DNN without compression. EIE with processing
power of 102~GOPS at only 600mW is also 24,000x and 3,000x more energy
efficient than a CPU and GPU respectively. The EIE resulted into no loss of
accuracy on AlexNet and VGG-16 outputs on the ImageNet dataset, which
represents the state-of-the-art model and the largest computer vision
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01530</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01530</id><created>2016-02-03</created><updated>2016-02-18</updated><authors><author><keyname>Cheng</keyname><forenames>Kuan</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Randomness Extraction in AC0 and with Small Locality</title><categories>cs.CC</categories><comments>50 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two variants of seeded randomness extractors. The first one, as
studied by Goldreich et al. \cite{goldreich2015randomness}, is seeded
extractors that can be computed by AC0 circuits. The second one, as introduced
by Bogdanov and Guo \cite{bogdanov2013sparse}, is (strong) extractor families
that consist of sparse transformations, i.e., functions that have a small
number of overall input-output dependencies (called \emph{sparse extractor
families}).
  In the AC0 extractor case, our main results substantially improve the
positive results in \cite{goldreich2015randomness}. We give constructions of
strong seeded extractors for $k=\delta n \geq n/poly(\log n)$, with seed length
$d=O(\log n)$, output length $m=k^{\Omega(1)}$, and error any $1/poly(n)$. We
can then boost the output length to $\Omega(\delta k)$ with seed length
$d=O(\log n)$, or to $(1-\gamma)k$ for any constant $0&lt;\gamma&lt;1$ with
$d=O(\frac{1}{\delta}\log n)$.
  In the case of sparse extractor families,for min-entropy $k=\Omega(\log^2 n)$
and error $\epsilon \geq 2^{-k^{\Omega(1)}}$, we give a strong seeded extractor
with seed length $d = O(k)$, $m = (1-\gamma)k$ and locality $\frac{n}{k}\log^2
(1/\epsilon) (\log n)poly(\log k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01532</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01532</id><created>2016-02-03</created><authors><author><keyname>Mozaffari</keyname><forenames>Mohammad</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Optimal Transport Theory for Power-Efficient Deployment of Unmanned
  Aerial Vehicles</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the optimal deployment of multiple unmanned aerial vehicles
(UAVs) acting as flying base stations is investigated. Considering the downlink
scenario, the goal is to minimize the total required transmit power of UAVs
while satisfying the users' rate requirements. To this end, the optimal
locations of UAVs as well as the cell boundaries of their coverage areas are
determined. To find those optimal parameters, the problem is divided into two
sub-problems that are solved iteratively. In the first sub-problem, given the
cell boundaries corresponding to each UAV, the optimal locations of the UAVs
are derived using the facility location framework. In the second sub-problem,
the locations of UAVs are assumed to be fixed, and the optimal cell boundaries
are obtained using tools from optimal transport theory. The analytical results
show that the total required transmit power is significantly reduced by
determining the optimal coverage areas for UAVs. These results also show that,
moving the UAVs based on users' distribution, and adjusting their altitudes can
lead to a minimum power consumption. Finally, it is shown that the proposed
deployment approach, can improve the system's power efficiency by a factor of
20 compared to the classical Voronoi cell association technique with fixed UAVs
locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01537</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01537</id><created>2016-02-03</created><authors><author><keyname>Dave</keyname><forenames>Vachik S.</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author></authors><title>TopCom: Index for Shortest Distance Query in Directed Graph</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding shortest distance between two vertices in a graph is an important
problem due to its numerous applications in diverse domains, including
geo-spatial databases, social network analysis, and information retrieval.
Classical algorithms (such as, Dijkstra) solve this problem in polynomial time,
but these algorithms cannot provide real-time response for a large number of
bursty queries on a large graph. So, indexing based solutions that pre-process
the graph for efficiently answering (exactly or approximately) a large number
of distance queries in real-time is becoming increasingly popular. Existing
solutions have varying performance in terms of index size, index building time,
query time, and accuracy. In this work, we propose T OP C OM , a novel
indexing-based solution for exactly answering distance queries. Our experiments
with two of the existing state-of-the-art methods (IS-Label and TreeMap) show
the superiority of T OP C OM over these two methods considering scalability and
query time. Besides, indexing of T OP C OM exploits the DAG (directed acyclic
graph) structure in the graph, which makes it significantly faster than the
existing methods if the SCCs (strongly connected component) of the input graph
are relatively small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01541</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01541</id><created>2016-02-03</created><authors><author><keyname>Aguerrebere</keyname><forenames>Cecilia</forenames></author><author><keyname>Delbracio</keyname><forenames>Mauricio</forenames></author><author><keyname>Bartesaghi</keyname><forenames>Alberto</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Fundamental Limits in Multi-image Alignment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of multi-image alignment, bringing different images into one
coordinate system, is critical in many applications with varied signal-to-noise
ratio (SNR) conditions. A great amount of effort is being invested into
developing methods to solve this problem. Several important questions thus
arise, including: Which are the fundamental limits in multi-image alignment
performance? Does having access to more images improve the alignment?
Theoretical bounds provide a fundamental benchmark to compare methods and can
help establish whether improvements can be made. In this work, we tackle the
problem of finding the performance limits in image registration when multiple
shifted and noisy observations are available. We derive and analyze the
Cram\'er-Rao and Ziv-Zakai lower bounds under different statistical models for
the underlying image. The accuracy of the derived bounds is experimentally
assessed through a comparison to the maximum likelihood estimator. We show the
existence of different behavior zones depending on the difficulty level of the
problem, given by the SNR conditions of the input images. We find that
increasing the number of images is only useful below a certain SNR threshold,
above which the pairwise MLE estimation proves to be optimal. The analysis we
present here brings further insight into the fundamental limitations of the
multi-image alignment problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01545</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01545</id><created>2016-02-03</created><authors><author><keyname>Ashikhmin</keyname><forenames>Alexei</forenames></author><author><keyname>Lai</keyname><forenames>Ching-Yi</forenames></author><author><keyname>Brun</keyname><forenames>Todd</forenames></author></authors><title>Correction of Data and Syndrome Errors by Stabilizer Codes</title><categories>cs.IT math.IT quant-ph</categories><comments>2 figures. This is a short version of our full paper (in preparation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing active quantum error correction to protect fragile quantum states
highly depends on the correctness of error information--error syndromes. To
obtain reliable error syndromes using imperfect physical circuits, we propose
the idea of quantum data-syndrome (DS) codes that are capable of correcting
both data qubits and syndrome bits errors. We study fundamental properties of
quantum DS codes and provide several CSS-type code constructions of quantum DS
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01547</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01547</id><created>2016-02-03</created><authors><author><keyname>Fund</keyname><forenames>Fraida</forenames></author><author><keyname>Hosseini</keyname><forenames>S. Amir</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra S.</forenames></author></authors><title>Under a cloud of uncertainty: Legal questions affecting Internet storage
  and transmission of copyright-protected video content</title><categories>cs.NI cs.CY</categories><comments>Accepted in IEEE Network Special Issue on Smart Data Pricing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of multimedia consumption has triggered technical, economic,
and business innovations that improve the quality and accessibility of content.
It has also opened new markets, promising large revenues for industry players.
However, new technologies also pose new questions regarding the legal aspects
of content delivery, which are often resolved through litigation between
copyright owners and content distributors. The precedents set by these cases
will act as a game changer in the content delivery industry and will shape the
existing offerings in the market in terms of how new technologies can be
deployed and what kind of pricing strategies can be associated with them. In
this paper, we offer a tutorial on key copyright and communications laws and
decisions related to storage and transmission of video content over the
Internet. We summarize legal limitations on the deployment of new technologies
and pricing mechanisms, and explain the implications of recent lawsuits.
Understanding these concerns is essential for engineers engaged in designing
the technical and economic aspects of video delivery systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01557</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01557</id><created>2016-02-03</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author><author><keyname>Raziperchikolaei</keyname><forenames>Ramin</forenames></author></authors><title>An ensemble diversity approach to supervised binary hashing</title><categories>cs.LG cs.CV math.OC stat.ML</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary hashing is a well-known approach for fast approximate nearest-neighbor
search in information retrieval. Much work has focused on affinity-based
objective functions involving the hash functions or binary codes. These
objective functions encode neighborhood information between data points and are
often inspired by manifold learning algorithms. They ensure that the hash
functions differ from each other through constraints or penalty terms that
encourage codes to be orthogonal or dissimilar across bits, but this couples
the binary variables and complicates the already difficult optimization. We
propose a much simpler approach: we train each hash function (or bit)
independently from each other, but introduce diversity among them using
techniques from classifier ensembles. Surprisingly, we find that not only is
this faster and trivially parallelizable, but it also improves over the more
complex, coupled objective function, and achieves state-of-the-art precision
and recall in experiments with image retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01560</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01560</id><created>2016-02-04</created><authors><author><keyname>Deshmukh</keyname><forenames>Aditya</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Online energy efficient packet scheduling for a common deadline with and
  without energy harvesting</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of online packet scheduling to minimize the required conventional
grid energy for transmitting a fixed number of packets given a common deadline
is considered. The total number of packets arriving within the deadline is
known, but the packet arrival times are unknown, and can be arbitrary. The
proposed algorithm tries to finish the transmission of each packet assuming all
future packets are going to arrive at equal time intervals within the left-over
time. The proposed online algorithm is shown to have competitive ratio that is
logarithmic in the number of packet arrivals. The hybrid energy paradigm is
also considered, where in addition to grid energy, energy is also available via
extraction from renewable sources. The objective here is to minimize the grid
energy use. A suitably modified version of the previous algorithm is also shown
to have competitive ratio that is logarithmic in the number of packet arrivals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01565</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01565</id><created>2016-02-04</created><authors><author><keyname>Semiari</keyname><forenames>Omid</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author></authors><title>Context-Aware Scheduling of Joint Millimeter Wave and Microwave
  Resources for Dual-Mode Base Stations</title><categories>cs.IT math.IT</categories><comments>In Proc. of the IEEE International Conference on Communications
  (ICC), Mobile and Wireless Networks Symposium, Kualalumpur, Malaysia, May
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most promising approaches to overcome the drastic channel
variations of millimeter wave (mmW) communications is to deploy dual-mode base
stations that integrate both mmW and microwave (\muW) frequencies. Reaping the
benefits of a dual-mode operation requires scheduling mechanisms that can
allocate resources efficiently and jointly at both frequency bands. In this
paper, a novel resource allocation framework is proposed that exploits users'
context, in terms of user application (UA) delay requirements, to maximize the
quality-of-service (QoS) of a dual-mode base station. In particular, such a
context-aware approach enables the network to dynamically schedule UAs, instead
of users, thus providing more precise delay guarantees and a more efficient
exploitation of the mmW resources. The scheduling of UAs is formulated as a
one-to-many matching problem between UAs and resources and a novel algorithm is
proposed to solve it. The proposed algorithm is shown to converge to a
two-sided stable matching between UAs and network resources. Simulation results
show that the proposed approach outperforms classical CSI-based scheduling in
terms of the per UA QoS, yielding up to 36% improvement. The results also show
that exploiting mmW resources provides significant traffic offloads reaching up
to 43% from \muW band.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01567</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01567</id><created>2016-02-04</created><authors><author><keyname>Saginbekov</keyname><forenames>Sain</forenames></author><author><keyname>Shakenov</keyname><forenames>Chingiz</forenames></author></authors><title>Testing Wireless Sensor Networks with Hybrid Simulators</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development for Wireless Sensor Networks (WSNs) is challenging due
to characteristics of sensor nodes and the environment they are deployed in.
Testing software in a real WSN testbed allows users to get reliable test
results. However, real testbeds become more expensive as the number of sensor
nodes in the network grows. Simulation tools are alternatives to real testbeds.
They are cheaper, faster and repeatable. However, simulation results are not
reliable as that of testbeds. Therefore, there is a need for a testing tool
that can leverage the advantages of testbeds and simulation tools. These tools
are usually called hybrid simulators. In this survey, we discuss several hybrid
simulators that use real sensor motes integrated with a simulator to make
software development cheaper, repeatable and to make the results more reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01569</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01569</id><created>2016-02-04</created><authors><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Pan</keyname><forenames>Jiaxian</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author></authors><title>Unraveling the Rank-One Solution Mystery of Robust MISO Downlink
  Transmit Optimization: A Verifiable Sufficient Condition via a New Duality
  Result</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concentrates on a robust transmit optimization problem for the
multiuser multi-input single-output (MISO) downlink scenario and under
inaccurate channel state information (CSI). This robust problem deals with a
general-rank transmit covariance design, and it follows a safe rate-constrained
formulation under spherically bounded CSI uncertainties. Curiously, simulation
results in previous works suggested that the robust problem admits rank-one
optimal transmit covariances in most cases. Such a numerical finding is
appealing because transmission with rank-one covariances can be easily realized
by single-stream transmit beamforming. This gives rise to a fundamentally
important question, namely, whether we can theoretically identify conditions
under which the robust problem admits a rank-one solution. In this paper, we
identify one such condition. Simply speaking, we show that the robust problem
is guaranteed to admit a rank-one solution if the CSI uncertainties are not too
large and the multiuser channel is not too poorly conditioned. To establish the
aforementioned condition, we develop a novel duality framework, through which
an intimate relationship between the robust problem and a related maximin
problem is revealed. Our condition involves only a simple expression with
respect to the multiuser channel and other system parameters. Thus, it is
verifiable, unlike the sufficient rank-one conditions that have appeared in the
literature. The application of our analysis framework to several other CSI
uncertainty models is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01576</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01576</id><created>2016-02-04</created><authors><author><keyname>Iyer</keyname><forenames>Anantharaman Palacode Narayana</forenames></author></authors><title>A Factorized Recurrent Neural Network based architecture for medium to
  large vocabulary Language Modelling</title><categories>cs.CL cs.AI</categories><comments>8 pages</comments><doi>10.1109/ICSC.2016.37</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical language models are central to many applications that use
semantics. Recurrent Neural Networks (RNN) are known to produce state of the
art results for language modelling, outperforming their traditional n-gram
counterparts in many cases. To generate a probability distribution across a
vocabulary, these models require a softmax output layer that linearly increases
in size with the size of the vocabulary. Large vocabularies need a
commensurately large softmax layer and training them on typical laptops/PCs
requires significant time and machine resources. In this paper we present a new
technique for implementing RNN based large vocabulary language models that
substantially speeds up computation while optimally using the limited memory
resources. Our technique, while building on the notion of factorizing the
output layer by having multiple output layers, improves on the earlier work by
substantially optimizing on the individual output layer size and also
eliminating the need for a multistep prediction process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01577</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01577</id><created>2016-02-04</created><updated>2016-03-04</updated><authors><author><keyname>Kwak</keyname><forenames>Heeyoul</forenames></author><author><keyname>Jun</keyname><forenames>Bohwan</forenames></author><author><keyname>Yang</keyname><forenames>Pilwoong</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>Overlapped Circular SC-LDPC Codes and Their Modified Coupled Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, overlapped circular spatially coupled low-density parity-check
codes (OC-LDPC) are proposed and their modified coupled codes are derived,
which show more improved performance compared with the spatially coupled
low-density parity-check (SC-LDPC) codes. It is known that the SC-LDPC codes
show asymptotically good performance when the chain length is sufficiently
large. However, a large chain length requires a large number of iterations for
decoding, which is often ignored in asymptotic analysis but it causes large
decoding complexity in the decoding process. The proposed OC-LDPC codes reduce
the decoding complexity while maintaining good asymptotic properties of the
SC-LDPC codes. The improvement in the decoding complexity comes from that the
proposed code is split into two SC-LDPC codes with shorter chain length during
the decoding process. Thus, the OC-LDPC code can be decoded with lower
iteration numbers required for decoding of two splitted SC-LDPC codes. Also, we
design finite-length protograph-based codes for the proposed codes and it is
shown that the proposed codes have superior finite-length code performances
than the SC-LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01580</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01580</id><created>2016-02-04</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Ben-Zrihem</keyname><forenames>Nir</forenames></author><author><keyname>Cohen</keyname><forenames>Aviad</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>Long-term Planning by Short-term Prediction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider planning problems, that often arise in autonomous driving
applications, in which an agent should decide on immediate actions so as to
optimize a long term objective. For example, when a car tries to merge in a
roundabout it should decide on an immediate acceleration/braking command, while
the long term effect of the command is the success/failure of the merge. Such
problems are characterized by continuous state and action spaces, and by
interaction with multiple agents, whose behavior can be adversarial. We argue
that dual versions of the MDP framework (that depend on the value function and
the $Q$ function) are problematic for autonomous driving applications due to
the non Markovian of the natural state space representation, and due to the
continuous state and action spaces. We propose to tackle the planning task by
decomposing the problem into two phases: First, we apply supervised learning
for predicting the near future based on the present. We require that the
predictor will be differentiable with respect to the representation of the
present. Second, we model a full trajectory of the agent using a recurrent
neural network, where unexplained factors are modeled as (additive) input
nodes. This allows us to solve the long-term planning problem using supervised
learning techniques and direct optimization over the recurrent neural network.
Our approach enables us to learn robust policies by incorporating adversarial
elements to the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01581</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01581</id><created>2016-02-04</created><authors><author><keyname>Olsen</keyname><forenames>Martin</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Molinero</keyname><forenames>Xavier</forenames></author></authors><title>On the Construction of High Dimensional Simple Games</title><categories>cs.GT cs.IT math.CO math.IT</categories><comments>9 pages, 1 table</comments><msc-class>91B12, 91A12, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every simple game can be written as the intersection of a finite number of
weighted games. The smallest possible such number is the dimension of a simple
game. Taylor and Zwicker have constructed simple games with $n$ players and
dimension at least $2^{\frac{n}{2}-1}$. By using theory on error correcting
codes, we construct simple games with dimension $2^{n-o(n)}$. Moreover, we show
that there are no simple games with dimension $n$ times higher than our games.
Our results hold for all $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01582</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01582</id><created>2016-02-04</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>SDCA without Duality, Regularization, and Individual Convexity</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Dual Coordinate Ascent is a popular method for solving regularized
loss minimization for the case of convex losses. We describe variants of SDCA
that do not require explicit regularization and do not rely on duality. We
prove linear convergence rates even if individual loss functions are
non-convex, as long as the expected loss is strongly convex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01585</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01585</id><created>2016-02-04</created><authors><author><keyname>He</keyname><forenames>Ruining</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author></authors><title>Ups and Downs: Modeling the Visual Evolution of Fashion Trends with
  One-Class Collaborative Filtering</title><categories>cs.AI cs.IR</categories><comments>11 pages, 5 figures</comments><doi>10.1145/2872427.2883037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building a successful recommender system depends on understanding both the
dimensions of people's preferences as well as their dynamics. In certain
domains, such as fashion, modeling such preferences can be incredibly
difficult, due to the need to simultaneously model the visual appearance of
products as well as their evolution over time. The subtle semantics and
non-linear dynamics of fashion evolution raise unique challenges especially
considering the sparsity and large scale of the underlying datasets. In this
paper we build novel models for the One-Class Collaborative Filtering setting,
where our goal is to estimate users' fashion-aware personalized ranking
functions based on their past feedback. To uncover the complex and evolving
visual factors that people consider when evaluating products, our method
combines high-level visual features extracted from a deep convolutional neural
network, users' past feedback, as well as evolving trends within the community.
Experimentally we evaluate our method on two large real-world datasets from
Amazon.com, where we show it to outperform state-of-the-art personalized
ranking measures, and also use it to visualize the high-level fashion trends
across the 11-year span of our dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01595</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01595</id><created>2016-02-04</created><updated>2016-03-02</updated><authors><author><keyname>Ammar</keyname><forenames>Waleed</forenames></author><author><keyname>Mulcaire</keyname><forenames>George</forenames></author><author><keyname>Ballesteros</keyname><forenames>Miguel</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Many Languages, One Parser</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We train one model for dependency parsing and use it to parse competitively
in several languages. The parsing model uses multilingual word clusters and
multilingual word embeddings alongside learned and specified typological
information, enabling generalization based on linguistic universals and
typological similarities. Our model can also incorporate language-specific
features (e.g., fine POS tags), enabling still letting the parser to learn
language-specific behaviors. Our parser compares favorably to strong baselines
in a range of data scenarios, including when the target language has a large
treebank, a small treebank, or no treebank for training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01599</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01599</id><created>2016-02-04</created><authors><author><keyname>Carvajal</keyname><forenames>Johanna</forenames></author><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>McCool</keyname><forenames>Chris</forenames></author><author><keyname>Lovell</keyname><forenames>Brian</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Comparative Evaluation of Action Recognition Methods via Riemannian
  Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions</title><categories>cs.CV</categories><acm-class>I.4; I.5; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a comparative evaluation of various techniques for action
recognition while keeping as many variables as possible controlled. We employ
two categories of Riemannian manifolds: symmetric positive definite matrices
and linear subspaces. For both categories we use their corresponding nearest
neighbour classifiers, kernels, and recent kernelised sparse representations.
We compare against traditional action recognition techniques based on Gaussian
mixture models and Fisher vectors (FVs). We evaluate these action recognition
techniques under ideal conditions, as well as their sensitivity in more
challenging conditions (variations in scale and translation). Despite recent
advancements for handling manifolds, manifold based techniques obtain the
lowest performance and their kernel representations are more unstable in the
presence of challenging conditions. The FV approach obtains the highest
accuracy under ideal conditions. Moreover, FV best deals with moderate scale
and translation changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01600</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01600</id><created>2016-02-04</created><authors><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Design of geometric molecular bonds</title><categories>cs.IT cs.ET math.IT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An example of a nonspecific molecular bond is the affinity of any positive
charge for any negative charge (like-unlike), or of nonpolar material for
itself when in aqueous solution (like-like). This contrasts specific bonds such
as the affinity of the DNA base A for T, but not for C, G, or another A. Recent
experimental breakthroughs in DNA nanotechnology demonstrate that a particular
nonspecific like-like bond (&quot;blunt-end DNA stacking&quot; that occurs between the
ends of any pair of DNA double-helices) can be used to create specific
&quot;macrobonds&quot; by careful geometric arrangement of many nonspecific blunt ends,
motivating the need for sets of macrobonds that are orthogonal: two macrobonds
not intended to bind should have relatively low binding strength, even when
misaligned.
  To address this need, we introduce geometric orthogonal codes that abstractly
model the engineered DNA macrobonds as two-dimensional binary codewords. While
motivated by completely different applications, geometric orthogonal codes
share similar features to the optical orthogonal codes studied by Chung,
Salehi, and Wei. The main technical difference is the importance of 2D geometry
in defining codeword orthogonality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01601</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01601</id><created>2016-02-04</created><authors><author><keyname>Carvajal</keyname><forenames>Johanna</forenames></author><author><keyname>McCool</keyname><forenames>Chris</forenames></author><author><keyname>Lovell</keyname><forenames>Brian</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Joint Recognition and Segmentation of Actions via Probabilistic
  Integration of Spatio-Temporal Fisher Vectors</title><categories>cs.CV</categories><acm-class>I.2.10; I.4; I.5; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a hierarchical approach to multi-action recognition that performs
joint classification and segmentation. A given video (containing several
consecutive actions) is processed via a sequence of overlapping temporal
windows. Each frame in a temporal window is represented through selective
low-level spatio-temporal features which efficiently capture relevant local
dynamics. Features from each window are represented as a Fisher vector, which
captures first and second order statistics. Instead of directly classifying
each Fisher vector, it is converted into a vector of class probabilities. The
final classification decision for each frame is then obtained by integrating
the class probabilities at the frame level, which exploits the overlapping of
the temporal windows. Experiments were performed on two datasets: s-KTH (a
stitched version of the KTH dataset to simulate multi-actions), and the
challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an
accuracy of 85.0%, significantly outperforming two recent approaches based on
GMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, the
proposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMM
approaches which obtained 33.7% and 38.4%, respectively. Furthermore, the
proposed system is on average 40 times faster than the GMM based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01608</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01608</id><created>2016-02-04</created><updated>2016-02-09</updated><authors><author><keyname>Mandal</keyname><forenames>Bappaditya</forenames></author></authors><title>Appearance Based Robot and Human Activity Recognition System</title><categories>cs.RO cs.CV</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present an appearance based human activity recognition
system. It uses background modeling to segment the foreground object and
extracts useful discriminative features for representing activities performed
by humans and robots. Subspace based method like principal component analysis
is used to extract low dimensional features from large voluminous activity
images. These low dimensional features are then used to classify an activity.
An apparatus is designed using a webcam, which watches a robot replicating a
human fall under indoor environment. In this apparatus, a robot performs
various activities (like walking, bending, moving arms) replicating humans,
which also includes a sudden fall. Experimental results on robot performing
various activities and standard human activity recognition databases show the
efficacy of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01614</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01614</id><created>2016-02-04</created><authors><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author></authors><title>Connectivity Scaling Laws in Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>4 pages, 1 figure</comments><doi>10.1109/LWC.2015.2476488</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present scaling laws that dictate both local and global connectivity
properties of bounded wireless networks. These laws are defined with respect to
the key system parameters of per-node transmit power and the number of antennas
exploited for diversity coding and/or beamforming at each node. We demonstrate
that the local probability of connectivity scales like $\mathcal{O}(z^\mathcal
C)$ in these parameters, where $\mathcal C$ is the ratio of the dimension of
the network domain to the path loss exponent, thus enabling efficient boundary
effect mitigation and network topology control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01616</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01616</id><created>2016-02-04</created><authors><author><keyname>Park</keyname><forenames>Jinhwan</forenames></author><author><keyname>Sung</keyname><forenames>Wonyong</forenames></author></authors><title>Fpga Based Implementation of Deep Neural Networks Using On-chip Memory
  Only</title><categories>cs.AR cs.NE</categories><comments>To be published in ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNNs) demand a very large amount of computation and
weight storage, and thus efficient implementation using special purpose
hardware is highly desired. In this work, we have developed an FPGA based
fixed-point DNN system using only on-chip memory not to access external DRAM.
The execution time and energy consumption of the developed system is compared
with a GPU based implementation. Since the capacity of memory in FPGA is
limited, only 3-bit weights are used for this implementation, and training
based fixed-point weight optimization is employed. The implementation using
Xilinx XC7Z045 is tested for the MNIST handwritten digit recognition benchmark
and a phoneme recognition task on TIMIT corpus. The obtained speed is about one
quarter of a GPU based implementation and much better than that of a PC based
one. The power consumption is less than 5 Watt at the full speed operation
resulting in much higher efficiency compared to GPU based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01619</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01619</id><created>2016-02-04</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Bocus</keyname><forenames>Mohammud Z.</forenames></author><author><keyname>Wang</keyname><forenames>Shanshan</forenames></author></authors><title>Distributed Power Allocation and Channel Access Probability Assignment
  for Cognitive Radio</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 4 figures, Published in Proceedings of IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a framework for distributively optimizing the
transmission strategies of secondary users in an ad hoc cognitive radio
network. In particular, the proposed approach allows secondary users to set
their transmit powers and channel access probabilities such that, on average,
the quality of service of both the primary and secondary networks are
satisfied. The system under consideration assumes several primary and secondary
transceiver pairs and assumes no cooperation or information exchange between
neither primary and secondary users nor among secondary users. The outage
probability, and consequently the connection probability, is derived for the
system and is used in defining a new performance metric in the optimization
problem using tools from stochastic geometry. We refer to this metric as the
spatial density of successful transmission. We corroborate our derivations
through numerical evaluations. We further demonstrate that even in the absence
of any form of cooperation, an acceptable quality of service can be attained in
the cognitive radio environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01620</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01620</id><created>2016-02-04</created><authors><author><keyname>Akbar</keyname><forenames>Rabia</forenames></author><author><keyname>Azim</keyname><forenames>Tahir</forenames></author></authors><title>A Green Enterprise Computing Architecture for Developing Countries</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing countries often have access to limited energy resources, which
frequently results in power cuts and failures. During these power cuts,
enterprises rely on backup sources for power such as uninterruptible power
supplies (UPS) and electric generators. This paper proposes AnywareDC, an
architecture that builds on the recent work on Anyware to reduce energy
utilization in the presence of such intermittent power supplies. Anyware
reduces energy usage by providing enterprise users laptops instead of desktops,
while maintaining performance using a central compute cluster. Our basic
insight is that in the presence of power cuts, only the routers and the cluster
needs to be provided power: the laptops can continue to run on their own
batteries. This reduces both energy usage and UPS load allowing it to supply
power for longer, thus also saving generator fuel costs. Simulations show that
this architecture reduces energy usage by up to 80% compared to one not using
Anyware, and by up to 20% compared to Anyware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01623</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01623</id><created>2016-02-04</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Kalogridis</keyname><forenames>Georgios</forenames></author><author><keyname>Yassine</keyname><forenames>Hachem</forenames></author><author><keyname>Denic</keyname><forenames>Stojan</forenames></author></authors><title>Connectivity of Cooperative Ad hoc Networks</title><categories>cs.NI</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connectivity properties of ad hoc networks have been extensively studied
over the past few years, from local observables, to global network properties.
In this paper we introduce a novel layer of network dynamics which lives and
evolves on top of the ad hoc network. Nodes are assumed selfish and a
snow-drift type game is defined dictating the way nodes decide to allocate
their cooperative resource efforts towards other nodes in the network. The
dynamics are strongly coupled with the physical network causing the cooperation
network topology to converge towards a stable equilibrium state, a global
maximum of the total pay-off. We study this convergence from a connectivity
perspective and analyse the inherent parameter dependence. Moreover, we show
that direct reciprocity can be an efficient incentive to promote cooperation
within the network and discuss the analogies between our simple yet tractable
framework with D2D proximity based services such as LTE-Direct. We argue that
cooperative network dynamics have many application in ICT, not just ad hoc
networks, and similar models as the one described herein can be devised and
studied in their own right.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01625</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01625</id><created>2016-02-04</created><authors><author><keyname>Hwang</keyname><forenames>Sangheum</forenames></author><author><keyname>Kim</keyname><forenames>Hyo-Eun</forenames></author></authors><title>Self-Transfer Learning for Fully Weakly Supervised Object Localization</title><categories>cs.CV</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances of deep learning have achieved remarkable performances in
various challenging computer vision tasks. Especially in object localization,
deep convolutional neural networks outperform traditional approaches based on
extraction of data/task-driven features instead of hand-crafted features.
Although location information of region-of-interests (ROIs) gives good prior
for object localization, it requires heavy annotation efforts from human
resources. Thus a weakly supervised framework for object localization is
introduced. The term &quot;weakly&quot; means that this framework only uses image-level
labeled datasets to train a network. With the help of transfer learning which
adopts weight parameters of a pre-trained network, the weakly supervised
learning framework for object localization performs well because the
pre-trained network already has well-trained class-specific features. However,
those approaches cannot be used for some applications which do not have
pre-trained networks or well-localized large scale images. Medical image
analysis is a representative among those applications because it is impossible
to obtain such pre-trained networks. In this work, we present a &quot;fully&quot; weakly
supervised framework for object localization (&quot;semi&quot;-weakly is the counterpart
which uses pre-trained filters for weakly supervised localization) named as
self-transfer learning (STL). It jointly optimizes both classification and
localization networks simultaneously. By controlling a supervision level of the
localization network, STL helps the localization network focus on correct ROIs
without any types of priors. We evaluate the proposed STL framework using two
medical image datasets, chest X-rays and mammograms, and achieve signiticantly
better localization performance compared to previous weakly supervised
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01626</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01626</id><created>2016-02-04</created><authors><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Speck</keyname><forenames>Robert</forenames></author></authors><title>Spectral deferred corrections with fast-wave slow-wave splitting</title><categories>math.NA cs.NA</categories><msc-class>65M70, 65M20, 65L05, 65L04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates a variant of semi-implicit spectral deferred
corrections (SISDC) in which the stiff, fast dynamics correspond to fast
propagating waves (&quot;fast-wave slow-wave problem&quot;). It is shown that the
fast-wave slow-wave SDC (FWSW-SDC) iteration will converge for small enough
time steps and that in the limit of infinitely fast waves the convergence rate
of the non-split version is retained. Stability function and discrete
dispersion relation are derived and show that the method is stable for
essentially arbitrary fast-wave CFL numbers as long as the slow dynamics are
resolved. The method causes little numerical diffusion and its semi-discrete
phase speed is accurate also for large wave number modes. Performance is
studied for an acoustic-advection problem and for the linearised Boussinesq
equations, describing compressible, stratified flow. FWSW-SDC is compared to a
diagonally implicit Runge-Kutta (DIRK) and IMEX Runge-Kutta (IMEX) method and
found to be competitive in terms of both accuracy and cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01628</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01628</id><created>2016-02-04</created><updated>2016-02-16</updated><authors><author><keyname>Terletskyi</keyname><forenames>D. A.</forenames></author><author><keyname>Provotar</keyname><forenames>A. I.</forenames></author></authors><title>Fuzzy Object-Oriented Dynamic Networks. II</title><categories>cs.AI</categories><comments>2 figures</comments><acm-class>I.2.4; D.1.5; D.3.3; F.4.1; E.2</acm-class><journal-ref>Cybernetics and Systems Analysis, 2016, Volume 52, Issue 1, pp
  38-45</journal-ref><doi>10.1007/s10559-016-9797-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article generalizes object-oriented dynamic networks to the fuzzy case,
which allows one to represent knowledge on objects and classes of objects that
are fuzzy by nature and also to model their changes in time. Within the
framework of the approach described, a mechanism is proposed that makes it
possible to acquire new knowledge on the basis of basic knowledge and
considerably differs from well-known methods used in existing models of
knowledge representation. The approach is illustrated by an example of
construction of a concrete fuzzy object-oriented dynamic network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01629</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01629</id><created>2016-02-04</created><authors><author><keyname>Leguay</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Draief</keyname><forenames>Moez</forenames></author><author><keyname>Chouvardas</keyname><forenames>Symeon</forenames></author><author><keyname>Paris</keyname><forenames>Stefano</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios S.</forenames></author><author><keyname>Maggi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Qi</keyname><forenames>Meiyu</forenames></author></authors><title>Online and Global Network Optimization: Towards the Next-Generation of
  Routing Platforms</title><categories>cs.NI</categories><comments>16 pages, 6 figures, Under submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The computation power of SDN controllers fosters the development of a new
generation of control plane that uses compute-intensive operations to automate
and optimize the network configuration across layers. From now on, cutting-edge
optimization and machine learning algorithms can be used to control networks in
real-time. This formidable opportunity transforms the way routing systems
should be conceived and designed. This paper presents a candidate architecture
for the next generation of routing platforms built on three main pillars for
admission control, re-routing and monitoring that would have not been possible
in legacy control planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01635</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01635</id><created>2016-02-04</created><authors><author><keyname>Hedges</keyname><forenames>Jules</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>A Generalised Quantifier Theory of Natural Language in Categorical
  Compositional Distributional Semantics with Bialgebras</title><categories>cs.CL cs.AI math.CT</categories><msc-class>cs.CL, cs.AI, math.CT</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorical compositional distributional semantics is a model of natural
language; it combines the statistical vector space models of words with the
compositional models of grammar. We formalise in this model the generalised
quantifier theory of natural language, due to Barwise and Cooper. The
underlying setting is a compact closed category with bialgebras. We start from
a generative grammar formalisation and develop an abstract categorical
compositional semantics for it, then instantiate the abstract setting to sets
and relations and to finite dimensional vector spaces and linear maps. We prove
the equivalence of the relational instantiation to the truth theoretic
semantics of generalized quantifiers. The vector space instantiation formalises
the statistical usages of words and enables us to, for the first time, reason
about quantified phrases and sentences compositionally in distributional
semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01641</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01641</id><created>2016-02-04</created><authors><author><keyname>Gioan</keyname><forenames>Emeric</forenames></author><author><keyname>Sol</keyname><forenames>Kevin</forenames></author><author><keyname>Subsol</keyname><forenames>G&#xe9;rard</forenames></author></authors><title>Orientations of Simplices Determined by Orderings on the Coordinates of
  their Vertices</title><categories>cs.DM math.CO</categories><comments>Full length paper submitted to a journal. A short conference version
  has been published [5]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provided n points in an (n-1)-dimensional affine space, and one ordering of
the points for each coordinate, we address the problem of testing whether these
orderings determine if the points are the vertices of a simplex (i.e. are
affinely independent), regardless of the real values of the coordinates. We
also attempt to determine the orientation of this simplex. In other words,
given a matrix whose columns correspond to affine points, we want to know when
the sign (or the non-nullity) of its determinant is implied by orderings given
to each row for the values of the row. We completely solve the problem in
dimensions 2 and 3. We provide a direct combinatorial characterization, along
with a formal calculus method. It can also be viewed as a decision algorithm,
and is based on testing the existence of a suitable inductive cofactor
expansion of the determinant. We conjecture that our method generalizes in
higher dimensions. This work aims to be part of a study on how oriented
matroids encode shapes of 3-dimensional landmark-based objects. Specifically,
applications include the analysis of anatomical data for physical anthropology
and clinical research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01644</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01644</id><created>2016-02-04</created><authors><author><keyname>Chen</keyname><forenames>Xiaojun</forenames></author><author><keyname>Xu</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Yue</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author></authors><title>A semi-automatic computer-aided method for surgical template design</title><categories>cs.GR cs.CG cs.CV</categories><comments>18 pages, 16 figures, 2 tables, 36 references</comments><journal-ref>Scientific Reports 6, Article number: 20280, 2016</journal-ref><doi>10.1038/srep20280</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a generalized integrated framework of semi-automatic
surgical template design. Several algorithms were implemented including the
mesh segmentation, offset surface generation, collision detection, ruled
surface generation, etc., and a special software named TemDesigner was
developed. With a simple user interface, a customized template can be semi-
automatically designed according to the preoperative plan. Firstly, mesh
segmentation with signed scalar of vertex is utilized to partition the inner
surface from the input surface mesh based on the indicated point loop. Then,
the offset surface of the inner surface is obtained through contouring the
distance field of the inner surface, and segmented to generate the outer
surface. Ruled surface is employed to connect inner and outer surfaces.
Finally, drilling tubes are generated according to the preoperative plan
through collision detection and merging. It has been applied to the template
design for various kinds of surgeries, including oral implantology, cervical
pedicle screw insertion, iliosacral screw insertion and osteotomy,
demonstrating the efficiency, functionality and generality of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01648</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01648</id><created>2016-02-04</created><authors><author><keyname>Bollauf</keyname><forenames>Maiara F.</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Uniformity Properties of Construction C</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Construction C (also known as Forney's multi-level code formula) forms a
Euclidean code for the additive white Gaussian noise (AWGN) channel from L
binary code components. If the component codes are linear, then the minimum
distance and kissing number are the same for all the points. However, while in
the single level (L=1) case it reduces to lattice Construction A, a multi-level
Construction C is in general not a lattice. We show that a two-level (L=2)
Construction C satisfies Forney's definition for a geometrically uniform
constellation. Specifically, every point sees the same configuration of
neighbors, up to a reflection of the coordinates in which the lower level code
is equal to 1. In contrast, for three levels and up (L&gt;= 3), we construct
examples where the distance spectrum varies between the points, hence the
constellation is not geometrically uniform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01652</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01652</id><created>2016-02-04</created><authors><author><keyname>Tsvetkova</keyname><forenames>Milena</forenames></author><author><keyname>Garc&#xed;a-Gavilanes</keyname><forenames>Ruth</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author></authors><title>Dynamics of Disagreement: Large-Scale Temporal Network Analysis Reveals
  Negative Interactions in Online Collaboration</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disagreement and conflict are a fact of social life and considerably affect
our well-being and productivity. Such negative interactions are rarely
explicitly declared and recorded and this makes them hard for scientists to
study. We overcome this challenge by investigating the patterns in the timing
and configuration of contributions to a large online collaboration community.
We analyze sequences of reverts of contributions to Wikipedia, the largest
online encyclopedia, and investigate how often and how fast they occur compared
to a null model that randomizes the order of actions to remove any systematic
clustering. We find evidence that individuals systematically attack the same
person and attack back their attacker; both of these interactions occur at a
faster response rate than expected. We also establish that individuals come to
defend an attack victim but we do not find evidence that attack victims &quot;pay it
forward&quot; or that attackers collude to attack the same individual. We further
find that high-status contributors are more likely to attack many others
serially, status equals are more likely to revenge attacks back, while attacks
by lower-status contributors trigger attacks forward; yet, it is the
lower-status contributors who also come forward to defend third parties. The
method we use can be applied to other large-scale temporal communication and
collaboration networks to identify the existence of negative social
interactions and other social processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01659</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01659</id><created>2016-02-04</created><authors><author><keyname>Dahlum</keyname><forenames>Jakob</forenames></author><author><keyname>Lamm</keyname><forenames>Sebastian</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author><author><keyname>Strash</keyname><forenames>Darren</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Accelerating Local Search for the Maximum Independent Set Problem</title><categories>cs.DS</categories><comments>17 pages, 2 figures, 3 tables</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing high-quality independent sets quickly is an important problem in
combinatorial optimization. Several recent algorithms have shown that
kernelization techniques can be used to find exact maximum independent sets in
medium-sized sparse graphs, as well as high-quality independent sets in huge
sparse graphs that are intractable for exact (exponential-time) algorithms.
However, a major drawback of these algorithms is that they require significant
preprocessing overhead, and therefore cannot be used to find a high-quality
independent set quickly.
  In this paper, we show that performing simple kernelization techniques in an
online fashion significantly boosts the performance of local search, and is
much faster than pre-computing a kernel using advanced techniques. In addition,
we show that cutting high-degree vertices can boost local search performance
even further, especially on huge (sparse) complex networks. Our experiments
show that we can drastically speed up the computation of large independent sets
compared to other state-of-the-art algorithms, while also producing results
that are very close to the best known solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01665</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01665</id><created>2016-02-04</created><authors><author><keyname>Cummins</keyname><forenames>Ronan</forenames></author></authors><title>Improved Query Topic Models via Pseudo-Relevant P\'olya Document Models</title><categories>cs.IR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query-expansion via pseudo-relevance feedback is a popular method of
overcoming the problem of vocabulary mismatch and of increasing average
retrieval effectiveness. In this paper, we develop a new method that estimates
a query topic model from a set of pseudo-relevant documents using a new
language modelling framework.
  We assume that documents are generated via a mixture of multivariate Polya
distributions, and we show that by identifying the topical terms in each
document, we can appropriately select terms that are likely to belong to the
query topic model. The results of experiments on several TREC collections show
that the new approach compares favourably to current state-of-the-art expansion
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01690</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01690</id><created>2016-02-04</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Wexler</keyname><forenames>Yonatan</forenames></author></authors><title>Minimizing the Maximal Loss: How and Why?</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A commonly used learning rule is to approximately minimize the \emph{average}
loss over the training set. Other learning algorithms, such as AdaBoost and
hard-SVM, aim at minimizing the \emph{maximal} loss over the training set. The
average loss is more popular, particularly in deep learning, due to three main
reasons. First, it can be conveniently minimized using online algorithms, that
process few examples at each iteration. Second, it is often argued that there
is no sense to minimize the loss on the training set too much, as it will not
be reflected in the generalization loss. Last, the maximal loss is not robust
to outliers. In this paper we describe and analyze an algorithm that can
convert any online algorithm to a minimizer of the maximal loss. We prove that
in some situations better accuracy on the training set is crucial to obtain
good performance on unseen examples. Last, we propose robust versions of the
approach that can handle outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01700</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01700</id><created>2016-02-04</created><authors><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>Semerjian</keyname><forenames>Guilhem</forenames></author><author><keyname>Zdeborova</keyname><forenames>Lenka</forenames></author></authors><title>The large deviations of the whitening process in random constraint
  satisfaction problems</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.DM math.PR</categories><comments>53 pages, 32 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random constraint satisfaction problems undergo several phase transitions as
the ratio between the number of constraints and the number of variables is
varied. When this ratio exceeds the satisfiability threshold no more solutions
exist; the satisfiable phase, for less constrained problems, is itself divided
in an unclustered regime and a clustered one. In the latter solutions are
grouped in clusters of nearby solutions separated in configuration space from
solutions of other clusters. In addition the rigidity transition signals the
appearance of so-called frozen variables in typical solutions: beyond this
threshold most solutions belong to clusters with an extensive number of
variables taking the same values in all solutions of the cluster. In this paper
we refine the description of this phenomenon by estimating the location of the
freezing transition, corresponding to the disappearance of all unfrozen
solutions (not only typical ones). From a technical point of view we
characterize atypical solutions with a number of frozen variables different
from the typical value via a large deviation study of the dynamics of a
stripping process (whitening) that unveils the frozen variables of a solution,
building upon recent works on atypical trajectories of the bootstrap
percolation dynamics. Our results also bear some relevance from an algorithmic
perspective, previous numerical studies having shown that heuristic algorithms
of various kinds usually output unfrozen solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01711</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01711</id><created>2016-02-04</created><authors><author><keyname>Bagnall</keyname><forenames>Anthony</forenames></author><author><keyname>Bostrom</keyname><forenames>Aaron</forenames></author><author><keyname>Large</keyname><forenames>James</forenames></author><author><keyname>Lines</keyname><forenames>Jason</forenames></author></authors><title>The Great Time Series Classification Bake Off: An Experimental
  Evaluation of Recently Proposed Algorithms. Extended Version</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last five years there have been a large number of new time series
classification algorithms proposed in the literature. These algorithms have
been evaluated on subsets of the 47 data sets in the University of California,
Riverside time series classification archive. The archive has recently been
expanded to 85 data sets, over half of which have been donated by researchers
at the University of East Anglia. Aspects of previous evaluations have made
comparisons between algorithms difficult. For example, several different
programming languages have been used, experiments involved a single train/test
split and some used normalised data whilst others did not. The relaunch of the
archive provides a timely opportunity to thoroughly evaluate algorithms on a
larger number of datasets. We have implemented 18 recently proposed algorithms
in a common Java framework and compared them against two standard benchmark
classifiers (and each other) by performing 100 resampling experiments on each
of the 85 datasets. We use these results to test several hypotheses relating to
whether the algorithms are significantly more accurate than the benchmarks and
each other. Our results indicate that only 9 of these algorithms are
significantly more accurate than both benchmarks and that one classifier, the
Collective of Transformation Ensembles, is significantly more accurate than all
of the others. All of our experiments and results are reproducible: we release
all of our code, results and experimental details and we hope these experiments
form the basis for more rigorous testing of new algorithms in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01716</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01716</id><created>2016-02-04</created><authors><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author><author><keyname>Koppel</keyname><forenames>Alec</forenames></author><author><keyname>Mokhtari</keyname><forenames>Aryan</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Decentralized Prediction-Correction Methods for Networked Time-Varying
  Convex Optimization</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop algorithms that find and track the optimal solution trajectory of
time-varying convex optimization problems which consist of local and
network-related objectives. The methods we propose are derived from the
prediction-correction methodology, which corresponds to a strategy where the
time-varying problem is sampled at discrete time instances and then a sequence
is generated via alternatively executing predictions on how the optimizers at
the next time sample are changing and corrections on how they actually have
changed. Prediction is based on keeping track of the residual dynamics of the
optimality conditions, while correction is based on a gradient or Newton
method, leading to Decentralized Prediction-Correction Gradient (DPC-G) and
Decentralized Prediction-Correction Newton (DPC-N). We also extend these
methods to cases where the knowledge on how the optimization programs are
changing in time is only approximate and propose Decentralized Approximate
Prediction-Correction Gradient (DAPC-G) and Decentralized Approximate
Prediction-Correction Newton (DAPC-N). These methods use a first-order backward
approximation to estimate the time variation of the functions. We also study
the convergence properties of the proposed methods. We next show an application
of a resource allocation problem in a wireless network, and observe that the
proposed methods outperform existing running algorithms by orders of magnitude.
Moreover, numerical results showcase a trade-off between convergence accuracy,
sampling period, and network communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01718</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01718</id><created>2016-02-04</created><authors><author><keyname>Kamali</keyname><forenames>Maryam</forenames></author><author><keyname>Dennis</keyname><forenames>Louise A.</forenames></author><author><keyname>McAree</keyname><forenames>Owen</forenames></author><author><keyname>Fisher</keyname><forenames>Michael</forenames></author><author><keyname>Veres</keyname><forenames>Sandor M.</forenames></author></authors><title>Formal Verification of Autonomous Vehicle Platooning</title><categories>cs.AI cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coordination of multiple autonomous vehicles into convoys or platoons is
expected on our highways in the near future. However, before such platoons can
be deployed, the new autonomous behaviors of the vehicles in these platoons
must be certified. An appropriate representation for vehicle platooning is as a
multi-agent system in which each agent captures the &quot;autonomous decisions&quot;
carried out by each vehicle. In order to ensure that these autonomous
decision-making agents in vehicle platoons never violate safety requirements,
we use formal verification. However, as the formal verification technique used
to verify the agent code does not scale to the full system and as the global
verification technique does not capture the essential verification of
autonomous behavior, we use a combination of the two approaches. This mixed
strategy allows us to verify safety requirements not only of a model of the
system, but of the actual agent code used to program the autonomous vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01728</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01728</id><created>2016-02-04</created><authors><author><keyname>Shafiee</keyname><forenames>M. J.</forenames></author><author><keyname>Siva</keyname><forenames>P.</forenames></author><author><keyname>Scharfenberger</keyname><forenames>C.</forenames></author><author><keyname>Fieguth</keyname><forenames>P.</forenames></author><author><keyname>Wong</keyname><forenames>A.</forenames></author></authors><title>NeRD: a Neural Response Divergence Approach to Visual Salience Detection</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel approach to visual salience detection via Neural
Response Divergence (NeRD) is proposed, where synaptic portions of deep neural
networks, previously trained for complex object recognition, are leveraged to
compute low level cues that can be used to compute image region
distinctiveness. Based on this concept , an efficient visual salience detection
framework is proposed using deep convolutional StochasticNets. Experimental
results using CSSD and MSRA10k natural image datasets show that the proposed
NeRD approach can achieve improved performance when compared to
state-of-the-art image saliency approaches, while the attaining low
computational complexity necessary for near-real-time computer vision
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01729</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01729</id><created>2016-02-04</created><authors><author><keyname>Zhu</keyname><forenames>Fei</forenames></author><author><keyname>Halimi</keyname><forenames>Abderrahim</forenames></author><author><keyname>Honeine</keyname><forenames>Paul</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Zheng</keyname><forenames>Nanning</forenames></author></authors><title>Correntropy Maximization via ADMM - Application to Robust Hyperspectral
  Unmixing</title><categories>stat.ML cs.CV cs.NE</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hyperspectral images, some spectral bands suffer from low signal-to-noise
ratio due to noisy acquisition and atmospheric effects, thus requiring robust
techniques for the unmixing problem. This paper presents a robust supervised
spectral unmixing approach for hyperspectral images. The robustness is achieved
by writing the unmixing problem as the maximization of the correntropy
criterion subject to the most commonly used constraints. Two unmixing problems
are derived: the first problem considers the fully-constrained unmixing, with
both the non-negativity and sum-to-one constraints, while the second one deals
with the non-negativity and the sparsity-promoting of the abundances. The
corresponding optimization problems are solved efficiently using an alternating
direction method of multipliers (ADMM) approach. Experiments on synthetic and
real hyperspectral images validate the performance of the proposed algorithms
for different scenarios, demonstrating that the correntropy-based unmixing is
robust to outlier bands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01731</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01731</id><created>2016-02-04</created><authors><author><keyname>Chandhar</keyname><forenames>Prabhu</forenames></author><author><keyname>Das</keyname><forenames>Suvra Sekhar</forenames></author></authors><title>Multi-Objective Framework for Dynamic Optimization of OFDMA Cellular
  Systems</title><categories>cs.IT math.IT</categories><comments>Journal paper, 21 pages, 24 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Green cellular networking has become an important research area in recent
years due to environmental and economical concerns. Switching off
under-utilized BSs during off-peak traffic load conditions is a promising
approach to reduce energy consumption in cellular networks. In practice, during
initial cell planning, the BS locations and RAN parameters are optimized to
meet the basic system design requirements like coverage, capacity, overlap, QoS
etc. As these metrics are tightly coupled with each other due to co-channel
interference, switching off certain BSs may affect the system requirements.
Therefore, identifying a subset of large number of BSs which are to be put into
sleep mode, is a challenging dynamic optimization problem. In this work, we
develop a multiobjective framework for dynamic optimization framework for OFDMA
based cellular systems. The objective is to identify the appropriate set of
active sectors and RAN parameters that maximize coverage and area spectral
efficiency while minimizing overlap and area power consumption without
violating the QoS requirements for a given traffic demand density. The
objective functions and constraints are obtained using appropriate analytical
models which capture the traffic characteristics, propagation characteristics
(pathloss, shadowing, and small scale fading) as well as load condition in
neighbouring cells. A low complexity evolutionary algorithm is used for
identifying the global Pareto optimal solutions at a faster convergence rate.
The inter-relationships between the system objectives are studied and
guidelines are provided to find an appropriate network configuration that
provides the best achievable trade-offs. The results show that using the
proposed framework, significant amount of energy saving can be achieved and
with a low computational complexity while maintaining good trade-offs among the
other objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01732</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01732</id><created>2016-02-04</created><authors><author><keyname>Mifdaoui</keyname><forenames>Ahlem</forenames></author><author><keyname>Ayed</keyname><forenames>Hamdi</forenames></author></authors><title>Buffer-aware Worst Case Timing Analysis of Wormhole Network On Chip</title><categories>cs.NI cs.DC cs.PF</categories><comments>ISAE Technical report done during the master Thesis of Hamdi Ayed at
  ISAE at 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A buffer-aware worst-case timing analysis of wormhole NoC is proposed in this
paper to integrate the impact of buffer size on the different dependencies
relationship between flows, i.e. direct and indirect blocking flows, and
consequently the timing performance. First, more accurate definitions of direct
and indirect blocking flows sets have been introduced to take into account the
buffer size impact. Then, the modeling and worst-case timing analysis of
wormhole NoC have been detailed, based on Network Calculus formalism and the
newly defined blocking flows sets. This introduced approach has been
illustrated in the case of a realistic NoC case study to show the trade off
between latency and buffer size. The comparative analysis of our proposed
Buffer-aware timing analysis with conventional approaches is conducted and
noticeable enhancements in terms of maximum latency have been proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01739</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01739</id><created>2016-02-04</created><authors><author><keyname>Fluschnik</keyname><forenames>Till</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>The Parameterized Complexity of the Minimum Shared Edges Problem</title><categories>cs.CC</categories><comments>35 pages, 16 figures</comments><msc-class>68Q17, 68Q25, 68R10</msc-class><acm-class>F.1.3; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the NP-complete Minimum Shared Edges (MSE) problem. Given an
undirected graph, a source and a sink vertex, and two integers p and k, the
question is whether there are p paths in the graph connecting the source with
the sink and sharing at most k edges. Herein, an edge is shared if it appears
in at least two paths. We show that MSE is W[1]-hard when parameterized by the
treewidth of the input graph and the number k of shared edges combined. We show
that MSE is fixed-parameter tractable with respect to p, but does not admit a
polynomial-size kernel (unless NP is contained in coNP/poly). In the proof of
the fixed-parameter tractability of MSE parameterized by p, we employ the
treewidth reduction technique due to Marx, O'Sullivan, and Razgon [ACM TALG
2013].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01764</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01764</id><created>2016-02-04</created><authors><author><keyname>Gilbert</keyname><forenames>Hugo</forenames></author><author><keyname>Spanjaard</keyname><forenames>Olivier</forenames></author></authors><title>A game theoretic bound for minmax regret optimization problems with
  interval data</title><categories>cs.DS</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a generic anytime lower bounding procedure for
minmax regret optimization problems. We show that the lower bound obtained is
always at least as accurate as the lower bound recently proposed by Chassein
and Goerigk (2015). The validity of the bound is based on game theoretic
arguments and its computation is performed via a double oracle algorithm
(McMahan et al., 2003) that we specify. The lower bound can be efficiently
computed for any minmax regret optimization problem whose standard version is
&quot;easy&quot;. We describe how to efficiently embed this lower bound in a branch and
bound procedure. Finally we apply our approach to the robust shortest path
problem. Our numerical results show a significant gain in the computation
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01768</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01768</id><created>2016-02-04</created><updated>2016-02-28</updated><authors><author><keyname>Gower</keyname><forenames>Robert M.</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Randomized Quasi-Newton Updates are Linearly Convergent Matrix Inversion
  Algorithms</title><categories>math.NA cs.NA</categories><comments>34 pages, 5 figures, 2 tables</comments><msc-class>15A09, 15B52, 15A24, 65F10, 65F08, 68W20, 65Y20, 68Q25, 68W40,
  90C20,</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop and analyze a broad family of stochastic/randomized algorithms for
inverting a matrix. We also develop a specialized variant which maintains
symmetry or positive definiteness of the iterates. All methods in the family
converge globally and linearly (i.e., the error decays exponentially), with
explicit rates. In special cases, we obtain stochastic block variants of
several quasi-Newton updates, including bad Broyden (BB), good Broyden (GB),
Powell-symmetric-Broyden (PSB), Davidon-Fletcher-Powell (DFP) and
Broyden-Fletcher-Goldfarb-Shanno (BFGS). Ours are the first stochastic versions
of these updates shown to converge to an inverse of a fixed matrix. Through a
dual viewpoint we uncover a fundamental link between quasi-Newton updates and
approximate inverse preconditioning. Further, we develop an adaptive variant of
randomized block BFGS, where we modify the distribution underlying the
stochasticity of the method throughout the iterative process to achieve faster
convergence. By inverting several matrices from varied applications, we
demonstrate that AdaRBFGS is highly competitive when compared to the well
established Newton-Schulz and minimal residual methods. In particular, on
large-scale problems our method outperforms the standard methods by orders of
magnitude. Development of efficient methods for estimating the inverse of very
large matrices is a much needed tool for preconditioning and variable metric
methods in the advent of the big data era.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01771</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01771</id><created>2016-02-04</created><authors><author><keyname>Alagic</keyname><forenames>Gorjan</forenames></author><author><keyname>Fefferman</keyname><forenames>Bill</forenames></author></authors><title>On Quantum Obfuscation</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encryption of data is fundamental to secure communication in the modern
world. Beyond encryption of data lies obfuscation, i.e., encryption of
functionality. It is well-known that the most powerful means of obfuscating
classical programs, so-called ``black-box obfuscation',' is provably impossible
[Barak et al '12]. However, several recent results have yielded candidate
schemes that satisfy a definition weaker than black-box, and yet still have
numerous applications.
  In this work, we initialize the rigorous study of obfuscating programs via
quantum-mechanical means. We define notions of quantum obfuscation which
encompass several natural variants. The input to the obfuscator can describe
classical or quantum functionality, and the output can be a circuit description
or a quantum state. The obfuscator can also satisfy one of a number of
obfuscation conditions: black-box, information-theoretic black-box,
indistinguishability, and best possible; the last two conditions come in three
variants: perfect, statistical, and computational. We discuss many
applications, including CPA-secure quantum encryption, quantum
fully-homomorphic encryption, and public-key quantum money.
  We then prove several impossibility results, extending a number of
foundational papers on classical obfuscation to the quantum setting. We prove
that quantum black-box obfuscation is impossible in a setting where adversaries
can possess more than one output of the obfuscator. In particular, generic
transformation of quantum circuits into black-box-obfuscated quantum circuits
is impossible. We also show that statistical indistinguishability obfuscation
is impossible, up to an unlikely complexity-theoretic collapse. Our proofs
involve a new tool: chosen-ciphertext-secure encryption of quantum data, which
was recently shown to be possible assuming quantum-secure one-way functions
exist [Alagic et al '16].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01777</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01777</id><created>2016-02-04</created><authors><author><keyname>Baum</keyname><forenames>Moritz</forenames></author><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Gemsa</keyname><forenames>Andreas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author><author><keyname>Wegner</keyname><forenames>Franziska</forenames></author></authors><title>Scalable Isocontour Visualization in Road Networks via Minimum-Link
  Paths</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Isocontours in road networks represent the area that is reachable from a
source within a given resource limit. We study the problem of computing
accurate isocontours in realistic, large-scale networks. We propose polygons
with minimum number of segments that separate reachable and unreachable
components of the network. Since the resulting problem is not known to be
solvable in polynomial time, we introduce several heuristics that are simple
enough to be implemented in practice. A key ingredient is a new practical
linear-time algorithm for minimum-link paths in simple polygons. Experiments in
a challenging realistic setting show excellent performance of our algorithms in
practice, answering queries in a few milliseconds on average even for long
ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01783</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01783</id><created>2016-02-04</created><authors><author><keyname>Mnih</keyname><forenames>Volodymyr</forenames></author><author><keyname>Badia</keyname><forenames>Adri&#xe0; Puigdom&#xe8;nech</forenames></author><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Graves</keyname><forenames>Alex</forenames></author><author><keyname>Lillicrap</keyname><forenames>Timothy P.</forenames></author><author><keyname>Harley</keyname><forenames>Tim</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Asynchronous Methods for Deep Reinforcement Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a conceptually simple and lightweight framework for deep
reinforcement learning that uses asynchronous gradient descent for optimization
of deep neural network controllers. We present asynchronous variants of four
standard reinforcement learning algorithms and show that parallel
actor-learners have a stabilizing effect on training allowing all four methods
to successfully train neural network controllers. The best performing method,
an asynchronous variant of actor-critic, surpasses the current state-of-the-art
on the Atari domain while training for half the time on a single multi-core CPU
instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds
on a wide variety of continuous motor control problems as well as on a new task
involving finding rewards in random 3D mazes using a visual input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01792</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01792</id><created>2016-02-04</created><authors><author><keyname>Kim</keyname><forenames>Kunho</forenames></author><author><keyname>Khabsa</keyname><forenames>Madian</forenames></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>Random Forest DBSCAN for USPTO Inventor Name Disambiguation</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inventor name disambiguation is a task that distinguishes each unique
inventor from all other inventor records in patent database. This task is
essential for processing person name queries in order to get information
related to certain inventor, e.g. list of all patents invented. We present a
scalable machine learning based inventor name disambiguation algorithm. We
train random forest classifier to classify whether each pair of inventor
records is from same person. We use DBSCAN algorithm for clustering, and its
distance function which is derived from a random forest classifier. For
scalability, it is important to use blocking functions and parallelize the
algorithm to run each block simultaneously. Our algorithm tested on the USPTO
patent database disambiguated 12 million inventor records in 6.5 hours.
Evaluation is on labeled datasets from USPTO PatentsView inventor name
disambiguation competition and showed our algorithm outperforms all algorithms
submitted to the competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01804</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01804</id><created>2016-02-04</created><authors><author><keyname>Herrmann</keyname><forenames>Dominik</forenames></author><author><keyname>Lindemann</keyname><forenames>Jens</forenames></author></authors><title>Obtaining personal data and asking for erasure: Do app vendors and
  website owners honour your privacy rights?</title><categories>cs.CY</categories><comments>This paper has been accepted for publication at Sicherheit 2016. The
  preprint is a slightly extended version of the conference paper: The preprint
  contains the list of the selected apps and websites</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EU Directive 95/46/EC and the upcoming EU General Data Protection Regulation
grant Europeans the right of access to data pertaining to them. Consumers can
approach their service providers to obtain all personal data stored and
processed there. Furthermore, they can demand erasure (or correction) of their
data. We conducted an undercover field study to determine whether these rights
can be exerted in practice. We assessed the behaviour of the vendors of 150
smartphone apps and 120 websites that are popular in Germany. Our deletion
requests were fulfilled in 52 to 57% of the cases and less than half of the
data provision requests were answered satisfactorily. Further, we observed
instances of carelessness: About 20% of website owners would have disclosed our
personal data to impostors. The results indicate that exerting privacy rights
that have been introduced two decades ago is still a frustrating endeavour most
of the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01818</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01818</id><created>2016-02-04</created><authors><author><keyname>Chung</keyname><forenames>A. G.</forenames></author><author><keyname>Shafiee</keyname><forenames>M. J.</forenames></author><author><keyname>Wong</keyname><forenames>A.</forenames></author></authors><title>Random Feature Maps via a Layered Random Projection (LaRP) Framework for
  Object Classification</title><categories>cs.CV cs.LG stat.ML</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The approximation of nonlinear kernels via linear feature maps has recently
gained interest due to their applications in reducing the training and testing
time of kernel-based learning algorithms. Current random projection methods
avoid the curse of dimensionality by embedding the nonlinear feature space into
a low dimensional Euclidean space to create nonlinear kernels. We introduce a
Layered Random Projection (LaRP) framework, where we model the linear kernels
and nonlinearity separately for increased training efficiency. The proposed
LaRP framework was assessed using the MNIST hand-written digits database and
the COIL-100 object database, and showed notable improvement in object
classification performance relative to other state-of-the-art random projection
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01819</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01819</id><created>2016-02-04</created><authors><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author></authors><title>A short note on Merlin-Arthur protocols for subset sum</title><categories>cs.CC cs.DS</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the subset sum problem we are given n positive integers along with a
target integer t. A solution is a subset of these integers summing to t. In
this short note we show that for a given subset sum instance there is a proof
of size $O^*(\sqrt{t})$ of what the number of solutions is that can be
constructed in $O^*(t)$ time and can be probabilistically verified in time
$O^*(\sqrt{t})$ with at most constant error probability. Here, the $O^*()$
notation omits factors polynomial in the input size $n\log(t)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01827</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01827</id><created>2016-02-04</created><updated>2016-02-05</updated><authors><author><keyname>Zhong</keyname><forenames>Yang</forenames></author><author><keyname>Sullivan</keyname><forenames>Josephine</forenames></author><author><keyname>Li</keyname><forenames>Haibo</forenames></author></authors><title>Face Attribute Prediction with classification CNN</title><categories>cs.CV</categories><comments>in submission to IEEE Conferences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting facial attributes from faces in the wild is very challenging due
to pose and lighting variations in the real world. The key to this problem is
to build proper feature representations to cope with these unfavorable
conditions. Given the success of convolutional neural network (CNN) in image
classification, the high-level CNN feature as an intuitive and reasonable
choice has been widely utilized for this problem. In this paper, however, we
consider the mid-level CNN features as an alternative to the high-level ones
for attribute prediction. This is based on the observation that face attributes
are different: some of them are locally oriented while others are globally
defined. Our investigations reveal that the mid-level deep representations
outperform the prediction accuracy achieved by the high-level abstractions. We
demonstrate that the mid-level representations achieve state-of-the-art
prediction performance on CelebA and LFWA datasets. Our investigations also
show that by utilizing the mid-level representations one can employ a single
deep network to achieve both face recognition and attribute prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01867</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01867</id><created>2016-02-04</created><authors><author><keyname>Furmanczyk</keyname><forenames>H.</forenames></author><author><keyname>Kubale</keyname><forenames>M.</forenames></author></authors><title>Scheduling of unit-length jobs with bipartite incompatibility graphs on
  four uniform machines</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we consider the problem of scheduling $n$ identical jobs on 4
uniform machines with speeds $s_1 \geq s_2 \geq s_3 \geq s_4,$ respectively.
Our aim is to find a schedule with a minimum possible length. We assume that
jobs are subject to some kind of mutual exclusion constraints modeled by a
bipartite incompatibility graph of degree $\Delta$, where two incompatible jobs
cannot be processed on the same machine. We show that the problem is NP-hard
even if $s_1=s_2=s_3$. If, however, $\Delta \leq 4$ and $s_1 \geq 12 s_2$,
$s_2=s_3=s_4$, then the problem can be solved to optimality in time
$O(n^{1.5})$. The same algorithm returns a solution of value at most 2 times
optimal provided that $s_1 \geq 2s_2$. Finally, we study the case $s_1 \geq s_2
\geq s_3=s_4$ and give an $O(n^{1.5})$-time $32/15$-approximation algorithm in
all such situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01868</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01868</id><created>2016-02-04</created><authors><author><keyname>Baxter</keyname><forenames>Paul</forenames></author><author><keyname>Trafton</keyname><forenames>J. Gregory</forenames></author><author><keyname>Lemaignan</keyname><forenames>Severin</forenames></author></authors><title>2nd Workshop on Cognitive Architectures for Social Human-Robot
  Interaction 2016 (CogArch4sHRI 2016)</title><categories>cs.RO</categories><comments>Index for conference proceedings CogArch4sHRI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume is the proceedings of the 2nd workshop on Cognitive Architectures
for Social Human-Robot Interaction, held at the ACM/IEEE HRI 2016 conference,
which took place on Monday 7th March 2016, in Christchurch, New Zealand.
  Organised by Paul Baxter (Plymouth University, U.K.), J. Gregory Trafton
(Naval Research Laboratory, USA), and Severin Lemaignan (Plymouth University,
U.K.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01870</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01870</id><created>2016-02-04</created><authors><author><keyname>Sasoglu</keyname><forenames>Eren</forenames></author><author><keyname>Tal</keyname><forenames>Ido</forenames></author></authors><title>Polar Coding for Processes with Memory</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study polar coding over channels and sources with memory. We show that
$\psi$-mixing processes polarize under the standard transform, and that the
rate of polarization to deterministic distributions is roughly
$O(2^{-\sqrt{N}})$ as in the memoryless case, where $N$ is the blocklength.
This implies that the error probability guarantees of polar channel and source
codes extend to a large class of models with memory, including finite-order
Markov sources and finite-state channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01871</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01871</id><created>2016-02-04</created><updated>2016-03-03</updated><authors><author><keyname>Huang</keyname><forenames>Jiamin</forenames></author><author><keyname>Mozafari</keyname><forenames>Barzan</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author><author><keyname>Wenisch</keyname><forenames>Thomas</forenames></author></authors><title>Identifying the Major Sources of Variance in Transaction Latencies:
  Towards More Predictable Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decades of research have sought to improve transaction processing performance
and scalability in database management systems (DBMSs). However, significantly
less attention has been dedicated to the predictability of performance: how
often individual transactions exhibit execution latency far from the mean?
Performance predictability is vital when transaction processing lies on the
critical path of a complex enterprise software or an interactive web service,
as well as in emerging database-as-a-service markets where customers contract
for guaranteed levels of performance. In this paper, we take several steps
towards achieving more predictable database systems. First, we propose a
profiling framework called VProfiler that, given the source code of a DBMS, is
able to identify the dominant sources of variance in transaction latency.
VProfiler automatically instruments the DBMS source code to deconstruct the
overall variance of transaction latencies into variances and covariances of the
execution time of individual functions, which in turn provide insight into the
root causes of variance. Second, we use VProfiler to analyze MySQL and Postgres
- two of the most popular and complex open-source database systems. Our case
studies reveal that the primary causes of variance in MySQL and Postgres are
lock scheduling and centralized logging, respectively. Finally, based on
VProfiler's findings, we further focus on remedying the performance variance of
MySQL by (1) proposing a new lock scheduling algorithm, called Variance-Aware
Transaction Scheduling (VATS), (2) enhancing the buffer pool replacement
policy, and (3) identifying tuning parameters that can reduce variance
significantly. Our experimental results show that our schemes reduce overall
transaction latency variance by 37% on average (and up to 64%) without
compromising throughput or mean latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01883</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01883</id><created>2016-02-04</created><authors><author><keyname>Ghosh</keyname><forenames>Shromona</forenames></author><author><keyname>Sadigh</keyname><forenames>Dorsa</forenames></author><author><keyname>Nuzzo</keyname><forenames>Pierluigi</forenames></author><author><keyname>Raman</keyname><forenames>Vasumathi</forenames></author><author><keyname>Donze</keyname><forenames>Alexandre</forenames></author><author><keyname>Sangiovanni-Vincentelli</keyname><forenames>Alberto</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>Diagnosis and Repair for Synthesis from Signal Temporal Logic
  Specifications</title><categories>cs.SY cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of diagnosing and repairing specifications for hybrid
systems formalized in signal temporal logic (STL). Our focus is on the setting
of automatic synthesis of controllers in a model predictive control (MPC)
framework. We build on recent approaches that reduce the controller synthesis
problem to solving one or more mixed integer linear programs (MILPs), where
infeasibility of a MILP usually indicates unrealizability of the controller
synthesis problem. Given an infeasible STL synthesis problem, we present
algorithms that provide feedback on the reasons for unrealizability, and
suggestions for making it realizable. Our algorithms are sound and complete,
i.e., they provide a correct diagnosis, and always terminate with a non-trivial
specification that is feasible using the chosen synthesis method, when such a
solution exists. We demonstrate the effectiveness of our approach on the
synthesis of controllers for various cyber-physical systems, including an
autonomous driving application and an aircraft electric power system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01887</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01887</id><created>2016-02-04</created><updated>2016-02-17</updated><authors><author><keyname>Wang</keyname><forenames>Shu</forenames></author><author><keyname>Zhang</keyname><forenames>Shaoting</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Metaxas</keyname><forenames>Dimitris N.</forenames></author></authors><title>Visual Tracking via Reliable Memories</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose a novel visual tracking framework that
intelligently discovers reliable patterns from a wide range of video to resist
drift error for long-term tracking tasks. First, we design a Discrete Fourier
Transform (DFT) based tracker which is able to exploit a large number of
tracked samples while still ensures real-time performance. Second, we propose a
clustering method with temporal constraints to explore and memorize consistent
patterns from previous frames, named as reliable memories. By virtue of this
method, our tracker can utilize uncontaminated information to alleviate
drifting issues. Experimental results show that our tracker performs favorably
against other state of-the-art methods on benchmark datasets. Furthermore, it
is significantly competent in handling drifts and able to robustly track
challenging long videos over 4000 frames, while most of others lose track at
early frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01890</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01890</id><created>2016-02-04</created><authors><author><keyname>Bency</keyname><forenames>Archith J.</forenames></author><author><keyname>Karthikeyan</keyname><forenames>S.</forenames></author><author><keyname>De Leo</keyname><forenames>Carter</forenames></author><author><keyname>Sunderrajan</keyname><forenames>Santhoshkumar</forenames></author><author><keyname>Manjunath</keyname><forenames>B. S.</forenames></author></authors><title>Search Tracker: Human-derived object tracking in-the-wild through
  large-scale search and retrieval</title><categories>cs.CV cs.MM</categories><comments>Under review with the IEEE Transactions on Circuits and Systems for
  Video Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans use context and scene knowledge to easily localize moving objects in
conditions of complex illumination changes, scene clutter and occlusions. In
this paper, we present a method to leverage human knowledge in the form of
annotated video libraries in a novel search and retrieval based setting to
track objects in unseen video sequences. For every video sequence, a document
that represents motion information is generated. Documents of the unseen video
are queried against the library at multiple scales to find videos with similar
motion characteristics. This provides us with coarse localization of objects in
the unseen video. We further adapt these retrieved object locations to the new
video using an efficient warping scheme. The proposed method is validated on
in-the-wild video surveillance datasets where we outperform state-of-the-art
appearance-based trackers. We also introduce a new challenging dataset with
complex object appearance changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01891</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01891</id><created>2016-02-04</created><authors><author><keyname>Franchi</keyname><forenames>Antonio</forenames></author><author><keyname>Petitti</keyname><forenames>Antonio</forenames></author><author><keyname>Rizzo</keyname><forenames>Alessandro</forenames></author></authors><title>Distributed Estimation for Cooperative Mobile Manipulation</title><categories>cs.RO cs.MA cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed method for the estimation of the kinematic
parameters, the dynamic parameters, and the kinematic state of an unknown body
manipulated by a decentralized team of mobile ground (planar) robots. The
proposed approach relies on the geometry of the rigid body kinematics, the
rigid body dynamics, on nonlinear observation, and on consensus algorithms. The
only three requirements are that each robot is able to control the 2D wrench
exerted locally on the load, it can measure the velocity of its contact point,
and the communication graph is connected. The finite time convergence of the
strategy is proven and all the robots agree on the same estimated quantities at
the end of the procedure. We present also two basic distributed control
strategies that are proven to satisfy nonlinear observability conditions needed
for the estimation accomplishment. Finally, a numerical test that demonstrates
the evolution of the estimation algorithm is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01895</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01895</id><created>2016-02-04</created><authors><author><keyname>Tang</keyname><forenames>Shijian</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author></authors><title>Generate Image Descriptions based on Deep RNN and Memory Cells for
  Images Features</title><categories>cs.CV cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating natural language descriptions for images is a challenging task.
The traditional way is to use the convolutional neural network (CNN) to extract
image features, followed by recurrent neural network (RNN) to generate
sentences. In this paper, we present a new model that added memory cells to
gate the feeding of image features to the deep neural network. The intuition is
enabling our model to memorize how much information from images should be fed
at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed
that our model outperforms other state-of-the-art models with higher BLEU
scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01896</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01896</id><created>2016-02-04</created><authors><author><keyname>Li</keyname><forenames>Yuqian</forenames></author><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author><author><keyname>Korzhyk</keyname><forenames>Dmytro</forenames></author></authors><title>Catcher-Evader Games</title><categories>cs.GT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for computing game-theoretic solutions have recently been applied
to a number of security domains. However, many of the techniques developed for
compact representations of security games do not extend to {\em Bayesian}
security games, which allow us to model uncertainty about the attacker's type.
In this paper, we introduce a general framework of {\em catcher-evader} games
that can capture Bayesian security games as well as other game families of
interest. We show that computing Stackelberg strategies is NP-hard, but give an
algorithm for computing a Nash equilibrium that performs well in simulations.
We also prove that the Nash equilibria of these games satisfy the {\em
interchangeability} property, so that equilibrium selection is not an issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01904</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01904</id><created>2016-02-04</created><authors><author><keyname>Pradhan</keyname><forenames>Dinesh</forenames></author><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Pandit</keyname><forenames>Saswata</forenames></author><author><keyname>Nandi</keyname><forenames>Subrata</forenames></author></authors><title>On the Discovery of Success Trajectories of Authors</title><categories>cs.DL</categories><comments>2 pages, 1 figure in 25rd International World Wide Web Conference WWW
  2016</comments><acm-class>H.3.7; H.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the qualitative patterns of research endeavor of scientific
authors in terms of publication count and their impact (citation) is important
in order to quantify success trajectories. Here, we examine the career profile
of authors in computer science and physics domains and discover at least six
different success trajectories in terms of normalized citation count in
longitudinal scale. Initial observations of individual trajectories lead us to
characterize the authors in each category. We further leverage this trajectory
information to build a two-stage stratification model to predict future success
of an author at the early stage of her career. Our model outperforms the
baseline with an average improvement of 15.68% for both the datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01906</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01906</id><created>2016-02-04</created><authors><author><keyname>Akhlaq</keyname><forenames>Assad</forenames></author><author><keyname>McKilliam</keyname><forenames>Robby</forenames></author><author><keyname>Subramanian</keyname><forenames>Ramanan</forenames></author><author><keyname>Pollok</keyname><forenames>Andre</forenames></author></authors><title>Selecting wavelengths for least squares range estimation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the distance, or range, between two
locations by measuring the phase of multiple sinusoidal signals transmitted
between the locations. Traditional estimators developed for optical
interferometry include the beat wavelength and excess fractions methods. More
recently, estimators based on the Chinese remainder theorem (CRT) and least
squares have appeared. Recent research suggests the least squares estimator to
be most accurate in many cases. The accuracy of all of these range estimators
depends upon the wavelengths chosen. This leads to the problem of selecting
wavelengths that maximise accuracy. Procedures for selecting wavelengths for
the beat wavelength and excess fractions methods have previously been
described, but procedures for the CRT and least squares estimators are yet to
be developed. In this paper we develop an algorithm to automatically select
wavelengths for use with the least square range estimator. The algorithm
minimises an optimisation criterion connected with the mean square error.
Interesting properties of a particular class of lattices simplify the criterion
allowing minimisation by depth first search. Monte-Carlo simulations indicate
that wavelengths that minimise the criterion can result is considerably more
accurate range estimates than wavelengths selected by ad hoc means.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01910</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01910</id><created>2016-02-04</created><authors><author><keyname>Hou</keyname><forenames>Yangyang</forenames></author><author><keyname>Whang</keyname><forenames>Joyce Jiyoung</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit S.</forenames></author></authors><title>Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping
  Clustering</title><categories>cs.LG</categories><comments>9 pages. 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is one of the most fundamental and important tasks in data mining.
Traditional clustering algorithms, such as K-means, assign every data point to
exactly one cluster. However, in real-world datasets, the clusters may overlap
with each other. Furthermore, often, there are outliers that should not belong
to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive,
Overlapping K-Means) objective as a way to address both issues in an integrated
fashion. Optimizing this discrete objective is NP-hard, and even though there
is a convex relaxation of the objective, straightforward convex optimization
approaches are too expensive for large datasets. A practical alternative is to
use a low-rank factorization of the solution matrix in the convex formulation.
The resulting optimization problem is non-convex, and we can locally optimize
the objective function using an augmented Lagrangian method. In this paper, we
consider two fast multiplier methods to accelerate the convergence of an
augmented Lagrangian scheme: a proximal method of multipliers and an
alternating direction method of multipliers (ADMM). For the proximal augmented
Lagrangian or proximal method of multipliers, we show a convergence result for
the non-convex case with bound-constrained subproblems. These methods are up to
13 times faster---with no change in quality---compared with a standard
augmented Lagrangian method on problems with over 10,000 variables and bring
runtimes down from over an hour to around 5 minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01911</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01911</id><created>2016-02-04</created><authors><author><keyname>Shirani</keyname><forenames>Farhad</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>An Achievable Rate-Distortion Region for Multiple Descriptions Source
  Coding Based on Coset Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of multiple descriptions (MD) source coding and
propose new coding strategies involving both unstructured and structured coding
layers. Previously, the most general achievable rate-distortion (RD) region for
the $l$-descriptions problem was the Combinatorial Message Sharing with Binning
(CMSB) region. The CMSB scheme utilizes unstructured quantizers and
unstructured binning. In the first part of the paper, we show that this
strategy can be improved upon using more general unstructured quantizers and a
more general unstructured binning method. In the second part, structured coding
strategies are considered. First, structured coding strategies are developed by
considering specific MD examples involving three or more descriptions. We show
that application of structured quantizers results in strict RD improvements
when there are more than two descriptions. Furthermore, we show that structured
binning also yields improvements. These improvements are in addition to the
ones derived in the first part of the paper. This suggests that structured
coding is essential when coding over more than two descriptions. Using the
ideas developed through these examples we provide a new unified coding strategy
by considering several structured coding layers. Finally, we characterize its
performance in the form of an inner bound to the optimal rate-distortion region
using computable single-letter information quantities. The new RD region
strictly contains all of the previous known achievable regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01913</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01913</id><created>2016-02-04</created><authors><author><keyname>Yang</keyname><forenames>Ming</forenames></author><author><keyname>Chao</keyname><forenames>Hongyang</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author><author><keyname>Guo</keyname><forenames>Jun</forenames></author><author><keyname>Yuan</keyname><forenames>Lu</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Effective Clipart Image Vectorization Through Direct Optimization of
  Bezigons</title><categories>cs.GR</categories><comments>18 pages, 16 figures</comments><journal-ref>IEEE Transactions on Visualization and Computer Graphics (TVCG),
  Volume 22 Issue 2, February 2016, Pages 1063-1075</journal-ref><doi>10.1109/TVCG.2015.2440273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bezigons, i.e., closed paths composed of B\'ezier curves, have been widely
employed to describe shapes in image vectorization results. However, most
existing vectorization techniques infer the bezigons by simply approximating an
intermediate vector representation (such as polygons). Consequently, the
resultant bezigons are sometimes imperfect due to accumulated errors, fitting
ambiguities, and a lack of curve priors, especially for low-resolution images.
In this paper, we describe a novel method for vectorizing clipart images. In
contrast to previous methods, we directly optimize the bezigons rather than
using other intermediate representations; therefore, the resultant bezigons are
not only of higher fidelity compared with the original raster image but also
more reasonable because they were traced by a proficient expert. To enable such
optimization, we have overcome several challenges and have devised a
differentiable data energy as well as several curve-based prior terms. To
improve the efficiency of the optimization, we also take advantage of the local
control property of bezigons and adopt an overlapped piecewise optimization
strategy. The experimental results show that our method outperforms both the
current state-of-the-art method and commonly used commercial software in terms
of bezigon quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01921</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01921</id><created>2016-02-04</created><authors><author><keyname>Lee</keyname><forenames>Haanvid</forenames></author><author><keyname>Jung</keyname><forenames>Minju</forenames></author><author><keyname>Tani</keyname><forenames>Jun</forenames></author></authors><title>Characteristics of Visual Categorization of Long-Concatenated and
  Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent
  Neural Network Model</title><categories>cs.CV cs.AI cs.LG</categories><comments>20 pages, 7 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current paper proposes a novel dynamic neural network model, multiple
spatio-temporal scales recurrent neural network (MSTRNN) used for
categorization of complex human action pattern in video image. The MSTRNN has
been developed by newly introducing recurrent connectivity to a prior-proposed
model, multiple spatio-temporal scales neural network (MSTNN) [1] such that the
model can learn to extract latent spatio-temporal structures more effectively
by developing adequate recurrent contextual dynamics. The MSTRNN was evaluated
by conducting a set of simulation experiments on learning to categorize human
action visual patterns. The first experiment on categorizing a set of
long-concatenated human movement patterns showed that MSTRNN outperforms MSTNN
in the capability of learning to extract long-ranged correlation in video
image. The second experiment on categorizing a set of object-directed actions
showed that the MSTRNN can learn to extract structural relationship between
actions and directed-objects. Our analysis on the characteristics of
miscategorization in both cases of object-directed action and pantomime actions
indicated that the model network developed the categorical memories by
organizing relational structure among them. Development of such relational
structure is considered to be beneficial for gaining generalization in
categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01925</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01925</id><created>2016-02-04</created><authors><author><keyname>Ammar</keyname><forenames>Waleed</forenames></author><author><keyname>Mulcaire</keyname><forenames>George</forenames></author><author><keyname>Tsvetkov</keyname><forenames>Yulia</forenames></author><author><keyname>Lample</keyname><forenames>Guillaume</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Massively Multilingual Word Embeddings</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new methods for estimating and evaluating embeddings of words
from dozens of languages in a single shared embedding space. Our estimation
methods, multiCluster and multiCCA, use dictionaries and monolingual data; they
do not require parallel data. Our new evaluation method, multiQVEC+, is shown
to correlate better than previous ones with two downstream tasks (text
categorization and parsing). On this evaluation and others, our estimation
methods outperform existing ones. We also describe a web portal for evaluation
that will facilitate further research in this area, along with open-source
releases of all our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01927</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01927</id><created>2016-02-05</created><authors><author><keyname>Khan</keyname><forenames>Zanobya N.</forenames></author><author><keyname>Qureshi</keyname><forenames>Rashid Jalal</forenames></author><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author></authors><title>On Feature based Delaunay Triangulation for Palmprint Recognition</title><categories>cs.CV</categories><journal-ref>Journal of Platform Technology, 3(4), 9-18 (2015)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Authentication of individuals via palmprint based biometric system is
becoming very popular due to its reliability as it contains unique and stable
features. In this paper, we present a novel approach for palmprint recognition
and its representation. To extract the palm lines, local thresholding technique
Niblack binarization algorithm is adopted. The endpoints of these lines are
determined and a connection is created among them using the Delaunay
triangulation thereby generating a distinct topological structure of each
palmprint. Next, we extract different geometric as well as quantitative
features from the triangles of the Delaunay triangulation that assist in
identifying different individuals. To ensure that the proposed approach is
invariant to rotation and scaling, features were made relative to topological
and geometrical structure of the palmprint. The similarity of the two
palmprints is computed using the weighted sum approach and compared with the
k-nearest neighbor. The experimental results obtained reflect the effectiveness
of the proposed approach to discriminate between different palmprint images and
thus achieved a recognition rate of 90% over large databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01929</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01929</id><created>2016-02-05</created><authors><author><keyname>Lee</keyname><forenames>Kong Aik</forenames></author><author><keyname>Hautam&#xe4;ki</keyname><forenames>Ville</forenames></author><author><keyname>Larcher</keyname><forenames>Anthony</forenames></author><author><keyname>Rao</keyname><forenames>Wei</forenames></author><author><keyname>Sun</keyname><forenames>Hanwu</forenames></author><author><keyname>Nguyen</keyname><forenames>Trung Hieu</forenames></author><author><keyname>Wang</keyname><forenames>Guangsen</forenames></author><author><keyname>Sizov</keyname><forenames>Aleksandr</forenames></author><author><keyname>Kukanov</keyname><forenames>Ivan</forenames></author><author><keyname>Poorjam</keyname><forenames>Amir</forenames></author><author><keyname>Trong</keyname><forenames>Trung Ngo</forenames></author><author><keyname>Xiao</keyname><forenames>Xiong</forenames></author><author><keyname>Xu</keyname><forenames>Cheng-Lin</forenames></author><author><keyname>Xu</keyname><forenames>Hai-Hua</forenames></author><author><keyname>Ma</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author><author><keyname>Meignier</keyname><forenames>Sylvain</forenames></author></authors><title>Fantastic 4 system for NIST 2015 Language Recognition Evaluation</title><categories>cs.CL</categories><comments>Technical report for NIST LRE 2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes the systems jointly submitted by Institute for
Infocomm (I$^2$R), the Laboratoire d'Informatique de l'Universit\'e du Maine
(LIUM), Nanyang Technology University (NTU) and the University of Eastern
Finland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The
submitted system is a fusion of nine sub-systems based on i-vectors extracted
from different types of features. Given the i-vectors, several classifiers are
adopted for the language detection task including support vector machines
(SVM), multi-class logistic regression (MCLR), Probabilistic Linear
Discriminant Analysis (PLDA) and Deep Neural Networks (DNN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01930</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01930</id><created>2016-02-05</created><authors><author><keyname>Xu</keyname><forenames>Yuedong</forenames></author><author><keyname>Xiao</keyname><forenames>Zhujun</forenames></author><author><keyname>Ni</keyname><forenames>Tianyu</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Efficiency of Adversarial Timeline Competition in Online Social Networks</title><categories>cs.SI cs.GT</categories><comments>21</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Targeted online advertising elicits a potential threat. A commercial agent
has a chance to mitigate the visibility of his opponents because their sales or
services are of similar types. In this paper, we consider the competition for
attention in popular online social networks (OSNs) that usually employ a
timeline-based homepage to sort messages chronologically in a limited visible
region. A non-cooperative Tullock-like game model is formulated that consists
of a finite amount of \emph{benign} agents and one \emph{malicious} agent. By
paying to the OSN, each benign agent seeks to maximize his utility of
visibility, while the malicious one aims to reduce the utilities of benign
agents. Our primary purposes are to quantify how robust the overall performance
of benign agents is against the malicious action, and how the OSN's revenue is
influenced. We derive the upper and the lower bounds of six fundamental
measures with regard to the total utility and the total net utility of benign
agents and the OSN's revenue under three different scenarios: with and without
the malicious agent, and the maximum. They capture the worst and the best
performances of the benign agents as well as the OSN. Our study reveals two
important insights: i) the performance bounds are very sensitive to the
malicious agent's willingness to pay at certain ranges; ii) the OSN acquires
more revenues from this malicious action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01937</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01937</id><created>2016-02-05</created><authors><author><keyname>Ghazinour</keyname><forenames>Kambiz</forenames></author><author><keyname>Matwin</keyname><forenames>Stan</forenames></author><author><keyname>Sokolova</keyname><forenames>Marina</forenames></author></authors><title>YOURPRIVACYPROTECTOR, A recommender system for privacy settings in
  social networks</title><categories>cs.CR cs.CY cs.IR cs.SI</categories><comments>15 pages, International journal of security, privacy and trust
  management. (IJSPTM) Volume 2, No 4, Aug. 2013</comments><journal-ref>International journal of security, privacy and trust management.
  (IJSPTM) Volume 2, No 4, Aug. 2013</journal-ref><doi>10.5121/ijsptm.2013.2402</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Ensuring privacy of users of social networks is probably an unsolvable
conundrum. At the same time, an informed use of the existing privacy options by
the social network participants may alleviate - or even prevent - some of the
more drastic privacy-averse incidents. Unfortunately, recent surveys show that
an average user is either not aware of these options or does not use them,
probably due to their perceived complexity. It is therefore reasonable to
believe that tools assisting users with two tasks: 1) understanding their
social net behavior in terms of their privacy settings and broad privacy
categories, and 2)recommending reasonable privacy options, will be a valuable
tool for everyday privacy practice in a social network context. This paper
presents YourPrivacyProtector, a recommender system that shows how simple
machine learning techniques may provide useful assistance in these two tasks to
Facebook users. We support our claim with empirical results of application of
YourPrivacyProtector to two groups of Facebook users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01940</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01940</id><created>2016-02-05</created><authors><author><keyname>Liu</keyname><forenames>Liangchen</forenames></author><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Chen</keyname><forenames>Shaokang</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Automatic and Quantitative evaluation of attribute discovery methods</title><categories>cs.CV</categories><comments>9 pages, WACV 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many automatic attribute discovery methods have been developed to extract a
set of visual attributes from images for various tasks. However, despite good
performance in some image classification tasks, it is difficult to evaluate
whether these methods discover meaningful attributes and which one is the best
to find the attributes for image descriptions. An intuitive way to evaluate
this is to manually verify whether consistent identifiable visual concepts
exist to distinguish between positive and negative images of an attribute. This
manual checking is tedious, labor intensive and expensive and it is very hard
to get quantitative comparisons between different methods. In this work, we
tackle this problem by proposing an attribute meaningfulness metric, that can
perform automatic evaluation on the meaningfulness of attribute sets as well as
achieving quantitative comparisons. We apply our proposed metric to recent
automatic attribute discovery methods and popular hashing methods on three
attribute datasets. A user study is also conducted to validate the
effectiveness of the metric. In our evaluation, we gleaned some insights that
could be beneficial in developing automatic attribute discovery methods to
generate meaningful attributes. To the best of our knowledge, this is the first
work to quantitatively measure the semantic content of automatically discovered
attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01944</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01944</id><created>2016-02-05</created><authors><author><keyname>Gonzalez-Franco</keyname><forenames>Mar</forenames></author><author><keyname>Cermeron</keyname><forenames>Julio</forenames></author><author><keyname>Li</keyname><forenames>Katie</forenames></author><author><keyname>Pizarro</keyname><forenames>Rodrigo</forenames></author><author><keyname>Thorn</keyname><forenames>Jacob</forenames></author><author><keyname>Hannah</keyname><forenames>Paul</forenames></author><author><keyname>Hutabarat</keyname><forenames>Windo</forenames></author><author><keyname>Tiwari</keyname><forenames>Ashutosh</forenames></author><author><keyname>Bermell-Garcia</keyname><forenames>Pablo</forenames></author></authors><title>Immersive Augmented Reality Training for Complex Manufacturing Scenarios</title><categories>cs.HC</categories><comments>6 pages, 4 figures, Video: https://youtu.be/xCbvmGkL6Y4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the complex manufacturing sector a considerable amount of resources are
focused on developing new skills and training workers. In that context,
increasing the effectiveness of those processes and reducing the investment
required is an outstanding issue. In this paper we present an experiment that
shows how modern Human Computer Interaction (HCI) metaphors such as
collaborative mixed-reality can be used to transmit procedural knowledge and
could eventually replace other forms of face-to-face training. We implement a
real-time Immersive Augmented Reality (IAR) setup with see-through cameras that
allows for collaborative interactions that can simulate conventional forms of
training. The obtained results indicate that people who took the IAR training
achieved the same performance than people in the conventional face-to-face
training condition. These results, their implications for future training and
the use of HCI paradigms in this context are discussed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01947</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01947</id><created>2016-02-05</created><authors><author><keyname>Dongale</keyname><forenames>T. D.</forenames></author><author><keyname>Khot</keyname><forenames>K. V.</forenames></author><author><keyname>Mohite</keyname><forenames>S. V.</forenames></author><author><keyname>Desai</keyname><forenames>N. K.</forenames></author><author><keyname>Shinde</keyname><forenames>S. S.</forenames></author><author><keyname>Moholkar</keyname><forenames>A. V.</forenames></author><author><keyname>Rajpure</keyname><forenames>K. Y.</forenames></author><author><keyname>Bhosale</keyname><forenames>P. N.</forenames></author><author><keyname>Patil</keyname><forenames>P. S.</forenames></author><author><keyname>Gaikwad</keyname><forenames>P. K.</forenames></author><author><keyname>Kamat</keyname><forenames>R. K.</forenames></author></authors><title>Investigating Reliability Aspects of Memristor based RRAM with Reference
  to Write Voltage and Frequency</title><categories>cs.ET</categories><comments>Pages-11, figures-6</comments><msc-class>00A72, 68M1</msc-class><acm-class>B.3.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we report the effect of write voltage and frequency on
memristor based Resistive Random Access Memory (RRAM). The above said
parameters have been investigated on the linear drift model of memristor. With
a variation of write voltage from 0.2V to 1.2V and a subsequent frequency
modulation from 1, 2, 4, 10, 100 and 200 Hz the corresponding effects on memory
window, Low Resistance State (LRS) and High Resistance State (HRS) have been
reported. Thus the lifetime ({\tau}) reliability analysis of memristor based
RRAM is carried out using above results. It is found that, the HRS is
independent of write voltage, whereas LRS shows dependency on write voltage and
frequency. The simulation results showcase that the memristor possess higher
memory window and lifetime ({\tau}) in the higher voltage with lower frequency
region, which has been attributed to the fewer data losses in the memory
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01949</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01949</id><created>2016-02-05</created><updated>2016-02-08</updated><authors><author><keyname>Geisert</keyname><forenames>Mathieu</forenames><affiliation>LAAS-GEPETTO</affiliation></author><author><keyname>Mansard</keyname><forenames>Nicolas</forenames><affiliation>LAAS-GEPETTO</affiliation></author></authors><title>Trajectory Generation for Quadrotor Based Systems using Numerical
  Optimal Control</title><categories>cs.RO math.OC</categories><comments>in ICRA, May 2016, Stockholm, Sweden. 2016</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent works on quadrotor have focused on more and more challenging tasks
on increasingly complex systems. Systems are often augmented with slung loads,
inverted pendulums or arms, and accomplish complex tasks such as going through
a window, grasping, throwing or catching. Usually, controllers are designed to
accomplish a specific task on a specific system using analytic solutions, so
each application needs long preparations. On the other hand, the direct
multiple shooting approach is able to solve complex problems without any
analytic development, by using on-the-shelf optimization solver. In this paper,
we show that this approach is able to solve a wide range of problems relevant
to quadrotor systems, from on-line trajectory generation for quadrotors, to
going through a window for a quadrotor-and-pendulum system, through
manipulation tasks for a aerial manipulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01950</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01950</id><created>2016-02-05</created><updated>2016-02-09</updated><authors><author><keyname>Comins</keyname><forenames>Jordan A.</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>RPYS i/o: A web-based tool for the historiography and visualization of
  citation classics, sleeping beauties, and research fronts</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reference Publication Year Spectroscopy (RPYS) and Multi-RPYS provide
algorithmic approaches to reconstructing the intellectual histories of
scientific fields. With this brief communication, we describe a technical
advancement for developing research historiographies by introducing RPYS i/o,
an online tool for performing standard RPYS and Multi-RPYS analyses
interactively (at http://comins.leydesdorff.net/). The tool enables users to
explore seminal works underlying a research field and to plot the influence of
these seminal works over time. This suite of visualizations offers the
potential to analyze and visualize the myriad of temporal dynamics of
scientific influence, such as citation classics, sleeping beauties, and the
dynamics of research fronts. We demonstrate the features of the tool by
analyzing--as an example--the references in documents published in the journal
Philosophy of Science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01956</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01956</id><created>2016-02-05</created><authors><author><keyname>Manlove</keyname><forenames>David F.</forenames></author><author><keyname>McBride</keyname><forenames>Iain</forenames></author></authors><title>&quot;Almost-stable&quot; matchings in the Hospitals / Residents problem with
  Couples: An Integer Programming approach</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1308.4534</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hospitals / Residents problem with Couples ({\sc hrc}) models the
allocation of intending junior doctors to hospitals where couples are allowed
to submit joint preference lists over pairs of (typically geographically close)
hospitals. It is known that a stable matching need not exist, so we consider
{\sc min bp hrc}, the problem of finding a matching that admits the minimum
number of blocking pairs (i.e., is &quot;as stable as possible&quot;). We show that this
problem is NP-hard and difficult to approximate even if each couple finds only
one hospital pair acceptable. However if we further assume that the preference
list of each single resident and hospital is of length at most 2, we give a
polynomial-time algorithm for this case. We then show how to adapt an earlier
Integer Programming (IP) model for {\sc hrc} to yield an IP formulation for
{\sc min bp hrc}. Finally, we discuss an empirical evaluation of the IP model
applied to randomly-generated instances of {\sc min bp hrc}. Our main finding
is that the number of blocking pairs admitted by a solution is very small,
i.e., usually at most 1, and never more than 2, for the (28,000) instances
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01958</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01958</id><created>2016-02-05</created><authors><author><keyname>Dalal</keyname><forenames>Gal</forenames></author><author><keyname>Gilboa</keyname><forenames>Elad</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Distributed Scenario-Based Optimization for Asset Management in a
  Hierarchical Decision Making Environment</title><categories>cs.SY</categories><comments>Accepted to IEEE PES PSCC2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asset management attempts to keep the power system in working conditions. It
requires much coordination between multiple entities and long term planning
often months in advance. In this work we introduce a mid-term asset management
formulation as a stochastic optimization problem, that includes three
hierarchical layers of decision making, namely the mid-term, short-term and
real-time. We devise a tractable scenario approximation technique for
efficiently assessing the complex implications a maintenance schedule inflicts
on a power system. This is done using efficient Monte-Carlo simulations that
trade-off between accuracy and tractability. We then present our implementation
of a distributed scenario-based optimization algorithm for solving our
formulation, and use an updated PJM 5-bus system to show a solution that is
cheaper than other maintenance heuristics that are likely to be considered by
TSOs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01959</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01959</id><created>2016-02-05</created><authors><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Shi</keyname><forenames>Xuanhua</forenames></author><author><keyname>Zhou</keyname><forenames>Yongluan</forenames></author><author><keyname>Zhang</keyname><forenames>Xiong</forenames></author><author><keyname>Jin</keyname><forenames>Hai</forenames></author><author><keyname>Pei</keyname><forenames>Cheng</forenames></author><author><keyname>He</keyname><forenames>Ligang</forenames></author><author><keyname>Geng</keyname><forenames>Yuanzhen</forenames></author></authors><title>Lifetime-Based Memory Management for Distributed Data Processing Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-memory caching of intermediate data and eager combining of data in shuffle
buffers have been shown to be very effective in minimizing the re-computation
and I/O cost in distributed data processing systems like Spark and Flink.
However, it has also been widely reported that these techniques would create a
large amount of long-living data objects in the heap, which may quickly
saturate the garbage collector, especially when handling a large dataset, and
hence would limit the scalability of the system. To eliminate this problem, we
propose a lifetime-based memory management framework, which, by automatically
analyzing the user-defined functions and data types, obtains the expected
lifetime of the data objects, and then allocates and releases memory space
accordingly to minimize the garbage collection overhead. In particular, we
present Deca, a concrete implementation of our proposal on top of Spark, which
transparently decomposes and groups objects with similar lifetimes into byte
arrays and releases their space altogether when their lifetimes come to an end.
An extensive experimental study using both synthetic and real datasets shows
that, in comparing to Spark, Deca is able to 1) reduce the garbage collection
time by up to 99.9%, 2) to achieve up to 22.7x speed up in terms of execution
time in cases without data spilling and 41.6x speedup in cases with data
spilling, and 3) to consume up to 46.6% less memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01963</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01963</id><created>2016-02-05</created><authors><author><keyname>Vester</keyname><forenames>Steen</forenames></author></authors><title>Winning Cores in Parity Games</title><categories>cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the novel notion of winning cores in parity games and develop a
deterministic polynomial-time under-approximation algorithm for solving parity
games based on winning core approximation. Underlying this algorithm are a
number properties about winning cores which are interesting in their own right.
In particular, we show that the winning core and the winning region for a
player in a parity game are equivalently empty. Moreover, the winning core
contains all fatal attractors but is not necessarily a dominion itself.
Experimental results are very positive both with respect to quality of
approximation and running time. It outperforms existing state-of-the-art
algorithms significantly on most benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01970</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01970</id><created>2016-02-05</created><authors><author><keyname>Mikkelsen</keyname><forenames>Kaare B.</forenames></author><author><keyname>Bach</keyname><forenames>Lars A.</forenames></author></authors><title>Threshold games and cooperation on multiplayer graphs</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>in PLOS ONE, 4th Feb 2016</comments><msc-class>91A06, 91A12, 91A15, 91A22, 91A43</msc-class><doi>10.1371/journal.pone.0147207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: The study investigates the effect on cooperation in multiplayer
games, when the population from which all individuals are drawn is structured -
i.e. when a given individual is only competing with a small subset of the
entire population.
  Method: To optimize the focus on multiplayer effects, a class of games were
chosen for which the payoff depends nonlinearly on the number of cooperators -
this ensures that the game cannot be represented as a sum of pair-wise
interactions, and increases the likelihood of observing behaviour different
from that seen in two-player games. The chosen class of games are named
&quot;threshold games&quot;, and are defined by a threshold, $M &gt; 0$, which describes the
minimal number of cooperators in a given match required for all the
participants to receive a benefit. The model was studied primarily through
numerical simulations of large populations of individuals, each with
interaction neighbourhoods described by various classes of networks.
  Results: When comparing the level of cooperation in a structured population
to the mean-field model, we find that most types of structure lead to a
decrease in cooperation. This is both interesting and novel, simply due to the
generality and breadth of relevance of the model - it is likely that any model
with similar payoff structure exhibits related behaviour.
  More importantly, we find that the details of the behaviour depends to a
large extent on the size of the immediate neighbourhoods of the individuals, as
dictated by the network structure. In effect, the players behave as if they are
part of a much smaller, fully mixed, population, which we suggest an expression
for.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01971</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01971</id><created>2016-02-05</created><authors><author><keyname>Andresen</keyname><forenames>Erik</forenames></author><author><keyname>Haensel</keyname><forenames>David</forenames></author><author><keyname>Chraibi</keyname><forenames>Mohcine</forenames></author><author><keyname>Seyfried</keyname><forenames>Armin</forenames></author></authors><title>Wayfinding and cognitive maps for pedestrian models</title><categories>cs.AI</categories><comments>8 pages, 3 figures, TGF'15 Conference, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually, routing models in pedestrian dynamics assume that agents have
fulfilled and global knowledge about the building's structure. However, they
neglect the fact that pedestrians possess no or only parts of information about
their position relative to final exits and possible routes leading to them. To
get a more realistic description we introduce the systematics of gathering and
using spatial knowledge. A new wayfinding model for pedestrian dynamics is
proposed. The model defines for every pedestrian an individual knowledge
representation implying inaccuracies and uncertainties. In addition,
knowledge-driven search strategies are introduced. The presented concept is
tested on a fictive example scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01982</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01982</id><created>2016-02-05</created><updated>2016-02-08</updated><authors><author><keyname>Mampilly</keyname><forenames>Antony V.</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author></authors><title>On the Capacity of the Half-Duplex MIMO Gaussian Diamond Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2016</comments><msc-class>94A15</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we analyze the 2-relay multiple-input multiple-output (MIMO)
Gaussian diamond channel. We show that a multihopping decode-and-forward with
multiple access (MDF-MAC) protocol achieves rates within a constant gap from
capacity when a channel parameter $\Delta$ is greater than zero. We also
identify the transmit covariance matrices to be used by each relay in the
multiple-access (MAC) state of the MDF-MAC protocol. As done for the
single-antenna 2-relay Gaussian diamond channel, the channel parameter $\Delta$
is defined to be the difference between the product of the capacities of the
links from the source to the two relays and the product of the capacities of
the links from the two relays to the destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.01995</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.01995</id><created>2016-02-05</created><authors><author><keyname>Marina</keyname><forenames>Ninoslav</forenames></author><author><keyname>Velkoska</keyname><forenames>Aneta</forenames></author><author><keyname>Paunkoska</keyname><forenames>Natasa</forenames></author></authors><title>Efficient distribution and improved security for reliable cloud storage
  system</title><categories>cs.IT math.IT</categories><comments>22 pages, 9 figures</comments><doi>10.1109/ICUMT.2015.7382437</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distributed data storage systems are constructed by large number of nodes
which are interconnected over a network. Each node in such peer-to-peer network
is vulnerable and at a potential risk for attack. The attackers can eavesdrop
the nodes and possibly modify their data. Hence distributed storage systems
should be secure apart from satisfying the reconstruction and repair
requirements. We constructed a distributed storage system, Twin MDS code
framework which is more efficient than the regenerating codes based storage
systems. We prove that this Twin MDS code framework gives better performance
than MBR codes and equal with MSR codes in the distribution process and
investigate its security performance comparing with the security of the MBR and
MSR codes. Such Twin MDS code framework is examined in an eavesdropper model
where passive attackers can access to the stored data or/and downloaded data
during the repair process. We demonstrate that the Twin MDS code framework
manages better results than MBR and MSR codes regarding the security in the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02004</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02004</id><created>2016-02-05</created><authors><author><keyname>Rivera</keyname><forenames>Victor</forenames></author></authors><title>Code Generation for Event-B</title><categories>cs.SE</categories><comments>PhD thesis. Supervisor: Nestor Catano</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stepwise refinement and Design-by-Contract are two formal approaches for
modelling systems. These approaches are widely used in the development of
systems. Both approaches have (dis-)advantages. This thesis aims to answer, is
it possible to combine both approaches in the development of systems, providing
the user with the benefits of both? We answer this question by translating the
stepwise refinement method with Event-B to Design-by-Contract with Java and
JML, so users can take full advantage of both formal approaches without losing
their benefits. This thesis presents a set of syntactic rules that translates
Event-B to JML-annotated Java code. It also presents the implementation of the
syntactic rules as the EventB2Java tool. We used the tool to translate several
Event-B models. It generated JML-annotated Java code for all the considered
models that serve as initial implementation. We also used EventB2Java for the
development of two software applications. Additionally, we compared EventB2Java
against two other tools for Event-B code generation. EventB2Java enables users
to start the software development process in Event-B, where users can model the
system and prove its consistency, to then transition to JML-annotated Java
code, where users can continue the development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02009</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02009</id><created>2016-02-05</created><authors><author><keyname>Dytckov</keyname><forenames>Sergei</forenames></author><author><keyname>Daneshtalab</keyname><forenames>Masoud</forenames></author></authors><title>Computing with hardware neurons: spiking or classical? Perspectives of
  applied Spiking Neural Networks from the hardware side</title><categories>cs.NE</categories><comments>In review for IJCNN 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While classical neural networks take a position of a leading method in the
machine learning community, spiking neuromorphic systems bring attention and
large projects in neuroscience. Spiking neural networks were shown to be able
to substitute networks of classical neurons in applied tasks. This work
explores recent hardware designs focusing on perspective applications (like
convolutional neural networks) for both neuron types from the energy efficiency
side to analyse whether there is a possibility for spiking neuromorphic
hardware to grow up for a wider use. Our comparison shows that spiking hardware
is at least on the same level of energy efficiency or even higher than
non-spiking on a level of basic operations. However, on a system level, spiking
systems are outmatched and consume much more energy due to inefficient data
representation with a long series of spikes. If spike-driven applications,
minimizing an amount of spikes, are developed, spiking neural systems may reach
the energy efficiency level of classical neural systems. However, in the near
future, both type of neuromorphic systems may benefit from emerging memory
technologies, minimizing the energy consumption of computation and memory for
both neuron types. That would make infrastructure and data transfer energy
dominant on the system level. We expect that spiking neurons have some
benefits, which would allow achieving better energy results. Still the problem
of an amount of spikes will still be the major bottleneck for spiking hardware
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02018</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02018</id><created>2016-02-05</created><authors><author><keyname>Tremblay</keyname><forenames>Nicolas</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Gribonval</keyname><forenames>Remi</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Compressive Spectral Clustering</title><categories>cs.DS cs.LG stat.ML</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral clustering has become a popular technique due to its high
performance in many contexts. It comprises three main steps: create a
similarity graph between N objects to cluster, compute the first k eigenvectors
of its Laplacian matrix to define a feature vector for each object, and run
k-means on these features to separate objects into k classes. Each of these
three steps becomes computationally intensive for large N and/or k. We propose
to speed up the last two steps based on recent results in the emerging field of
graph signal processing: graph filtering of random signals, and random sampling
of bandlimited graph signals. We prove that our method, with a gain in
computation time that can reach several orders of magnitude, is in fact an
approximation of spectral clustering, for which we are able to control the
error. We test the performance of our method on artificial and real-world
network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02022</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02022</id><created>2016-02-05</created><authors><author><keyname>Zukic</keyname><forenames>Dzenan</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Bauer</keyname><forenames>Miriam H. A.</forenames></author><author><keyname>Kuhnt</keyname><forenames>Daniela</forenames></author><author><keyname>Carl</keyname><forenames>Barbara</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Kolb</keyname><forenames>Andreas</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author></authors><title>Preoperative Volume Determination for Pituitary Adenoma</title><categories>cs.CV cs.CG cs.GR</categories><comments>7 pages, 6 figures, 1 table, 16 references in Proc. SPIE 7963,
  Medical Imaging 2011: Computer-Aided Diagnosis, 79632T (9 March 2011). arXiv
  admin note: text overlap with arXiv:1103.1778</comments><doi>10.1117/12.877660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most common sellar lesion is the pituitary adenoma, and sellar tumors are
approximately 10-15% of all intracranial neoplasms. Manual slice-by-slice
segmentation takes quite some time that can be reduced by using the appropriate
algorithms. In this contribution, we present a segmentation method for
pituitary adenoma. The method is based on an algorithm that we have applied
recently to segmenting glioblastoma multiforme. A modification of this scheme
is used for adenoma segmentation that is much harder to perform, due to lack of
contrast-enhanced boundaries. In our experimental evaluation, neurosurgeons
performed manual slice-by-slice segmentation of ten magnetic resonance imaging
(MRI) cases. The segmentations were compared to the segmentation results of the
proposed method using the Dice Similarity Coefficient (DSC). The average DSC
for all datasets was 75.92% +/- 7.24%. A manual segmentation took about four
minutes and our algorithm required about one second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02023</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02023</id><created>2016-02-05</created><authors><author><keyname>Robertini</keyname><forenames>Nadia</forenames></author><author><keyname>De Aguiar</keyname><forenames>Edilson</forenames></author><author><keyname>Helten</keyname><forenames>Thomas</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Efficient Multi-view Performance Capture of Fine-Scale Surface Detail</title><categories>cs.CV cs.GR</categories><comments>3D Vision (3DV), 2014 2nd International Conference on</comments><doi>10.1109/3DV.2014.46</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new effective way for performance capture of deforming meshes
with fine-scale time-varying surface detail from multi-view video. Our method
builds up on coarse 4D surface reconstructions, as obtained with commonly used
template-based methods. As they only capture models of coarse-to-medium scale
detail, fine scale deformation detail is often done in a second pass by using
stereo constraints, features, or shading-based refinement. In this paper, we
propose a new effective and stable solution to this second step. Our framework
creates an implicit representation of the deformable mesh using a dense
collection of 3D Gaussian functions on the surface, and a set of 2D Gaussians
for the images. The fine scale deformation of all mesh vertices that maximizes
photo-consistency can be efficiently found by densely optimizing a new
model-to-image consistency energy on all vertex positions. A principal
advantage is that our problem formulation yields a smooth closed form energy
with implicit occlusion handling and analytic derivatives. Error-prone
correspondence finding, or discrete sampling of surface displacement values are
also not needed. We show several reconstructions of human subjects wearing
loose clothing, and we qualitatively and quantitatively show that we robustly
capture more detail than related methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02026</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02026</id><created>2016-02-05</created><authors><author><keyname>Regts</keyname><forenames>Guus</forenames></author><author><keyname>Sevenster</keyname><forenames>Bart</forenames></author></authors><title>Graph parameters from symplectic group invariants</title><categories>math.CO cs.DM</categories><comments>20 pages, 1 figure</comments><msc-class>05C45, 15A72, Secondary 05C25, 05C31</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce, and characterize, a class of graph parameters
obtained from tensor invariants of the symplectic group. These parameters are
similar to partition functions of vertex models, as introduced by de la Harpe
and Jones, [P. de la Harpe, V.F.R. Jones, Graph invariants related to
statistical mechanical models: examples and problems, {\sl Journal of
Combinatorial Theory}, Series B {\bf 57} (1993) 207--227]. Yet they give a
completely different class of graph invariants. We moreover show that certain
evaluations of the cycle partition polynomial give examples of graph parameters
that can be obtained this way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02030</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02030</id><created>2016-02-05</created><authors><author><keyname>Dubin</keyname><forenames>Ran</forenames></author><author><keyname>Dvir</keyname><forenames>Amit</forenames></author><author><keyname>Pele</keyname><forenames>Ofir</forenames></author><author><keyname>Hadar</keyname><forenames>Ofer</forenames></author><author><keyname>Katz</keyname><forenames>Itay</forenames></author><author><keyname>Mashiach</keyname><forenames>Ori</forenames></author></authors><title>Adaptation Logic for HTTP Dynamic Adaptive Streaming using
  Geo-Predictive Crowdsourcing</title><categories>cs.MM</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand for video streaming services with high Quality of
Experience (QoE) has prompted a lot of research on client-side adaptation logic
approaches. However, most algorithms use the client's previous download
experience and do not use a crowd knowledge database generated by users of a
professional service. We propose a new crowd algorithm that maximizes the QoE.
Additionally, we show how crowd information can be integrated into existing
algorithms and illustrate this with two state-of-the-art algorithms. We
evaluate our algorithm and state-of-the-art algorithms (including our modified
algorithms) on a large, real-life crowdsourcing dataset that contains 336,551
samples on network performance. The dataset was provided by WeFi LTD. Our new
algorithm outperforms all other methods in terms of QoS (eMOS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02032</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02032</id><created>2016-02-05</created><authors><author><keyname>Bogdanovi&#x107;</keyname><forenames>Nikola</forenames></author><author><keyname>Driessen</keyname><forenames>Hans</forenames></author><author><keyname>Yarovoy</keyname><forenames>Alexander</forenames></author></authors><title>Track selection in Multifunction Radars for Multi-target tracking: an
  Anti-Coordination game</title><categories>cs.MA cs.IT math.IT</categories><comments>Accepted for ICASSP 2016 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a track selection problem for multi-target tracking in a
multifunction radar network is studied using the concepts from game theory. The
problem is formulated as a non-cooperative game, and specifically as an
anti-coordination game, where each player aims to differ from what other
players do. The players' utilities are modeled using a proper tracking accuracy
criterion and, under different assumptions on the structure of these utilities,
the corresponding Nash equilibria are characterized. To find an equilibrium, a
distributed algorithm based on the best-response dynamics is proposed. Finally,
computer simulations are carried out to verify the effectiveness of the
proposed algorithm in a multi-target tracking scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02036</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02036</id><created>2016-02-05</created><authors><author><keyname>Karumanchi</keyname><forenames>Siddharth</forenames></author><author><keyname>Mancini</keyname><forenames>Stefano</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author><author><keyname>Yang</keyname><forenames>Dong</forenames></author></authors><title>Classical capacities of quantum channels with environment assistance</title><categories>quant-ph cs.IT math.IT</categories><comments>28 pages, 9 figures. Comments are welcome!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum channel physically is a unitary interaction between the information
carrying system and an environment, which is initialized in a pure state before
the interaction. Conventionally, this state, as also the parameters of the
interaction, is assumed to be fixed and known to the sender and receiver. Here,
following the model introduced by us earlier [Karumanchi et al.,
arXiv[quant-ph]:1407.8160], we consider a benevolent third party, i.e. a
helper, controlling the environment state, and how the helper's presence
changes the communication game. In particular, we define and study the
classical capacity of a unitary interaction with helper, indeed two variants,
one where the helper can only prepare separable states across many channel
uses, and one without this restriction. Furthermore, the two even more powerful
scenarios of pre-shared entanglement between helper and receiver, and of
classical communication between sender and helper (making them conferencing
encoders) are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02040</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02040</id><created>2016-02-05</created><authors><author><keyname>Lyu</keyname><forenames>Jiangbin</forenames></author><author><keyname>Chew</keyname><forenames>Yong Huat</forenames></author><author><keyname>Wong</keyname><forenames>Wai-Choong</forenames></author></authors><title>Efficient and Scalable Distributed Autonomous Spatial Aloha Networks via
  Local Leader Election</title><categories>cs.IT math.IT</categories><comments>32 pages, 10 figures, accepted for publication in IEEE Transactions
  on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses a spatial Aloha model to describe a distributed autonomous
wireless network in which a group of transmit-receive pairs (users) shares a
common collision channel via slotted-Aloha-like random access. The objective of
this study is to develop an intelligent algorithm to be embedded into the
transceivers so that all users know how to self-tune their medium access
probability (MAP) to achieve overall Pareto optimality in terms of network
throughput under spatial reuse while maintaining network stability. While the
optimal solution requires each user to have complete information about the
network, our proposed algorithm only requires users to have local information.
The fundamental of our algorithm is that the users will first self-organize
into a number of non-overlapping neighborhoods, and the user with the maximum
node degree in each neighborhood is elected as the local leader (LL). Each LL
then adjusts its MAP according to a parameter R which indicates the radio
intensity level in its neighboring region, whereas the remaining users in the
neighborhood simply follow the same MAP value. We show that by ensuring R less
than or equal to 2 at the LLs, the stability of the entire network can be
assured even when each user only has partial network information. For practical
implementation, we propose each LL to use R=2 as the constant reference signal
to its built-in proportional and integral controller. The settings of the
control parameters are discussed and we validate through simulations that the
proposed method is able to achieve close-to-Pareto-front throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02041</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02041</id><created>2016-02-05</created><authors><author><keyname>Javid</keyname><forenames>Alireza Mahdavi</forenames></author><author><keyname>Setayesh</keyname><forenames>Mehdi</forenames></author><author><keyname>Farhadi</keyname><forenames>Farzaneh</forenames></author><author><keyname>Ashtiani</keyname><forenames>Farid</forenames></author></authors><title>Analysis of Network Coding in a Slotted ALOHA-based Two-Way Relay
  Network</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a two-way relay network (TWRN) based on a slotted ALOHA
protocol which utilizes network coding to exchange the packets. We proposed an
analytical approach to study the behavior of such networks and the effects of
network coding on the throughput, power, and queueing delay of the relay node.
In addition, when end nodes are not saturated, our approach enables us to
achieve the stability region of the network in different situations. Finally,
we carry out some simulation to confirm the validity of the proposed analytical
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02045</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02045</id><created>2016-02-05</created><authors><author><keyname>Cohen</keyname><forenames>Isaac J.</forenames></author><author><keyname>Wetz</keyname><forenames>David A.</forenames></author><author><keyname>Veiga</keyname><forenames>Stepfanie</forenames></author><author><keyname>Dong</keyname><forenames>Qing</forenames></author><author><keyname>Heinzel</keyname><forenames>John</forenames></author></authors><title>Fuzzy Logic Control of a Hybrid Energy Storage Module for Naval Pulsed
  Power Applications</title><categories>math.OC cs.SY</categories><journal-ref>International Journal of Fuzzy Logic Systems, vol. 6, no. 1, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is need for an energy storage device capable of transferring high power
in transient situations aboard naval vessels. Currently, batteries are used to
accomplish this task, but previous research has shown that when utilized at
high power rates, these devices deteriorate over time causing a loss in
lifespan. It has been shown that a hybrid energy storage configuration is
capable of meeting such a demand while reducing the strain placed on individual
components. While designing a custom converter capable of controlling the power
to and from a battery would be ideal for this application, it can be costly to
develop when compared to purchasing commercially available products.
Commercially available products offer limited controllability in exchange for
their proven performance and lower cost point - often times only allowing a
system level control input without any way to interface with low level controls
that are frequently used in controller design. This paper proposes the use of
fuzzy logic control in order to provide a system level control to the
converters responsible for limiting power to and from the battery. A system
will be described mathematically, modeled in MATLAB/Simulink, and a fuzzy logic
controller will be compared with a typical controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02046</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02046</id><created>2016-02-05</created><authors><author><keyname>Parra-Arnau</keyname><forenames>Javier</forenames></author><author><keyname>Achara</keyname><forenames>Jagdish Prasad</forenames></author><author><keyname>Castelluccia</keyname><forenames>Claude</forenames></author></authors><title>MyAdChoices: Bringing Transparency and Control to Online Advertising</title><categories>cs.CY cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intrusiveness and the increasing invasiveness of online advertising have,
in the last few years, raised serious concerns regarding user privacy and Web
usability. As a reaction to these concerns, we have witnessed the emergence of
a myriad of ad-blocking and anti-tracking tools, whose aim is to return control
to users over advertising. The problem with these technologies, however, is
that they are extremely limited and radical in their approach: users can only
choose either to block or allow all ads. With around 200 million people
regularly using these tools, the economic model of the Web ---in which users
get content free in return for allowing advertisers to show them ads--- is at
serious peril. In this paper, we propose a smart Web technology that aims at
bringing transparency to online advertising, so that users can make an informed
and equitable decision regarding ad blocking. The proposed technology is
implemented as a Web-browser extension and enables users to exert fine-grained
control over advertising, thus providing them with certain guarantees in terms
of privacy and browsing experience, while preserving the Internet economic
model. Experimental results in a real environment demonstrate the suitability
and feasibility of our approach, and provide preliminary findings on behavioral
targeting from real user browsing profiles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02047</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02047</id><created>2016-02-05</created><authors><author><keyname>Pontes</keyname><forenames>Elvys Linhares</forenames></author></authors><title>Utiliza\c{c}\~ao de Grafos e Matriz de Similaridade na Sumariza\c{c}\~ao
  Autom\'atica de Documentos Baseada em Extra\c{c}\~ao de Frases</title><categories>cs.CL cs.IR</categories><comments>Dissertation, 83 pages, in Portuguese. in Disserta\c{c}\~ao de
  Mestrado, Universidade Federal do Cear\'a, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The internet increased the amount of information available. However, the
reading and understanding of this information are costly tasks. In this
scenario, the Natural Language Processing (NLP) applications enable very
important solutions, highlighting the Automatic Text Summarization (ATS), which
produce a summary from one or more source texts. Automatically summarizing one
or more texts, however, is a complex task because of the difficulties inherent
to the analysis and generation of this summary. This master's thesis describes
the main techniques and methodologies (NLP and heuristics) to generate
summaries. We have also addressed and proposed some heuristics based on graphs
and similarity matrix to measure the relevance of judgments and to generate
summaries by extracting sentences. We used the multiple languages (English,
French and Spanish), CSTNews (Brazilian Portuguese), RPM (French) and DECODA
(French) corpus to evaluate the developped systems. The results obtained were
quite interesting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02049</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02049</id><created>2016-02-05</created><authors><author><keyname>Labahn</keyname><forenames>George</forenames></author><author><keyname>Zhou</keyname><forenames>Wei</forenames></author></authors><title>A fast, deterministic algorithm for computing a Hermite Normal Form of a
  polynomial matrix</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a square, nonsingular matrix of univariate polynomials $\mathbf{F} \in
\mathbb{K}[x]^{n \times n}$ over a field $\mathbb{K}$, we give a fast,
deterministic algorithm for finding the Hermite normal form of $\mathbf{F}$
with complexity $O^{\sim}\left(n^{\omega}d\right)$ where $d$ is the degree of
$\mathbf{F}$. Here soft-$O$ notation is Big-$O$ with log factors removed and
$\omega$ is the exponent of matrix multiplication. The method relies of a fast
algorithm for determining the diagonal entries of its Hermite normal form,
having as cost $O^{\sim}\left(n^{\omega}s\right)$ operations with $s$ the
average of the column degrees of $\mathbf{F}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02052</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02052</id><created>2016-02-05</created><updated>2016-02-16</updated><authors><author><keyname>Medeiros</keyname><forenames>Fl&#xe1;vio</forenames></author><author><keyname>K&#xe4;stner</keyname><forenames>Christian</forenames></author><author><keyname>Ribeiro</keyname><forenames>M&#xe1;rcio</forenames></author><author><keyname>Gheyi</keyname><forenames>Rohit</forenames></author><author><keyname>Apel</keyname><forenames>Sven</forenames></author></authors><title>A Comparison of 10 Sampling Algorithms for Configurable Systems</title><categories>cs.SE</categories><comments>An extended version of our ICSE 2016 paper, entitled: A Comparison of
  10 Sampling Algorithms for Configurable Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost every software system provides configuration options to tailor the
system to the target platform and application scenario. Often, this
configurability renders the analysis of every individual system configuration
infeasible. To address this problem, researchers have proposed a diverse set of
sampling algorithms. We present a comparative study of 10 state-of-the-art
sampling algorithms regarding their fault-detection capability and size of
sample sets. The former is important to improve software quality and the latter
to reduce the time of analysis. In a nutshell, we found that sampling
algorithms with larger sample sets are able to detect higher numbers of faults,
but simple algorithms with small sample sets, such as most-enabled-disabled,
are the most efficient in most contexts. Furthermore, we observed that the
limiting assumptions made in previous work influence the number of detected
faults, the size of sample sets, and the ranking of algorithms. Finally, we
have identified a number of technical challenges when trying to avoid the
limiting assumptions, which questions the practicality of certain sampling
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02057</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02057</id><created>2016-02-05</created><authors><author><keyname>Maronidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Chatzilari</keyname><forenames>Elisavet</forenames></author><author><keyname>Nikolopoulos</keyname><forenames>Spiros</forenames></author><author><keyname>Kompatsiaris</keyname><forenames>Ioannis</forenames></author></authors><title>A Generalised Differential Framework for Measuring Signal Sparsity</title><categories>cs.IT math.IT</categories><comments>14 pages, 4 figures, The abstract field cannot be longer than 1,920
  characters: The abstract appearing here is slightly shorter than the one in
  the pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of signal sparsity has been gaining increasing interest in
information theory and signal processing communities. As a consequence, a
plethora of sparsity metrics has been presented in the literature. The
appropriateness of these metrics is typically evaluated against a set of
objective criteria that has been proposed for assessing the credibility of any
sparsity metric. In this paper, we propose a Generalised Differential Sparsity
(GDS) framework for generating novel sparsity metrics whose functionality is
based on the concept that sparsity is encoded in the differences among the
signal coefficients. We rigorously prove that every metric generated using GDS
satisfies all the aforementioned criteria and we provide a computationally
efficient formula that makes GDS suitable for high-dimensional signals. The
great advantage of GDS is its flexibility to offer sparsity metrics that can be
well-tailored to certain requirements stemming from the nature of the data and
the problem to be solved. This is in contrast to current state-of-the-art
sparsity metrics like Gini Index (GI), which is actually proven to be only a
specific instance of GDS, demonstrating the generalisation power of our
framework. In verifying our claims, we have incorporated GDS in a stochastic
signal recovery algorithm and experimentally investigated its efficacy in
reconstructing randomly projected sparse signals. As a result, it is proven
that GDS, in comparison to GI, both loosens the bounds of the assumed sparsity
of the original signals and reduces the minimum number of projected dimensions,
required to guarantee an almost perfect reconstruction of heavily compressed
signals. The superiority of GDS over GI in conjunction with the fact that the
latter is considered as a standard in numerous scientific domains, prove the
great potential of GDS as a general purpose framework for measuring sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02063</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02063</id><created>2016-02-05</created><updated>2016-02-24</updated><authors><author><keyname>Jin</keyname><forenames>Kai</forenames></author><author><keyname>Tang</keyname><forenames>Pingzhong</forenames></author><author><keyname>Chen</keyname><forenames>Shiteng</forenames></author></authors><title>On the power of dominated players in team competitions</title><categories>cs.GT cs.MA</categories><comments>8pages, AAMAS2016</comments><msc-class>91A06</msc-class><acm-class>I.2.11; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate multi-round team competitions between two teams, where each
team selects one of its players simultaneously in each round and each player
can play at most once. The competition defines an extensive-form game with
perfect recall and can be solved efficiently by standard methods. We are
interested in the properties of the subgame perfect equilibria of this game.
  We first show that uniformly random strategy is a subgame perfect equilibrium
strategy for both teams when there are no redundant players (i.e., the number
of players in each team equals to the number of rounds of the competition).
Secondly, a team can safely abandon its weak players if it has redundant
players and the strength of players is transitive.
  We then focus on the more interesting case where there are redundant players
and the strength of players is not transitive. In this case, we obtain several
counterintuitive results. First of all, a player might help improve the payoff
of its team, even if it is dominated by the entire other team. We give a
necessary condition for a dominated player to be useful. We also study the
extent to which the dominated players can increase the payoff.
  These results bring insights into playing and designing general team
competitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02066</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02066</id><created>2016-02-05</created><authors><author><keyname>Eksin</keyname><forenames>Ceyhun</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Distributed Fictitious Play for Optimal Behavior of Multi-Agent Systems
  with Incomplete Information</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-agent system operates in an uncertain environment about which agents
have different and time varying beliefs that, as time progresses, converge to a
common belief. A global utility function that depends on the realized state of
the environment and actions of all the agents determines the system's optimal
behavior. We define the asymptotically optimal action profile as an equilibrium
of the potential game defined by considering the expected utility with respect
to the asymptotic belief. At finite time, however, agents have not entirely
congruous beliefs about the state of the environment and may select conflicting
actions. This paper proposes a variation of the fictitious play algorithm which
is proven to converge to equilibrium actions if the state beliefs converge to a
common distribution at a rate that is at least linear. In conventional
fictitious play, agents build beliefs on others' future behavior by computing
histograms of past actions and best respond to their expected payoffs
integrated with respect to these histograms. In the variations developed here
histograms are built using knowledge of actions taken by nearby nodes and best
responses are further integrated with respect to the local beliefs on the state
of the environment. We exemplify the use of the algorithm in coordination and
target covering games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02068</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02068</id><created>2016-02-05</created><updated>2016-02-08</updated><authors><author><keyname>Martins</keyname><forenames>Andr&#xe9; F. T.</forenames></author><author><keyname>Astudillo</keyname><forenames>Ram&#xf3;n Fernandez</forenames></author></authors><title>From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label
  Classification</title><categories>cs.CL cs.LG stat.ML</categories><comments>Minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose sparsemax, a new activation function similar to the traditional
softmax, but able to output sparse probabilities. After deriving its
properties, we show how its Jacobian can be efficiently computed, enabling its
use in a network trained with backpropagation. Then, we propose a new smooth
and convex loss function which is the sparsemax analogue of the logistic loss.
We reveal an unexpected connection between this new loss and the Huber
classification loss. We obtain promising empirical results in multi-label
classification problems and in attention-based neural networks for natural
language inference. For the latter, we achieve a similar performance as the
traditional softmax, but with a selective, more compact, attention focus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02070</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02070</id><created>2016-02-05</created><authors><author><keyname>Shahid</keyname><forenames>Nauman</forenames></author><author><keyname>Perraudin</keyname><forenames>Nathanael</forenames></author><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Compressive PCA on Graphs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized algorithms reduce the complexity of low-rank recovery methods only
w.r.t dimension p of a big dataset $Y \in \Re^{p \times n}$. However, the case
of large n is cumbersome to tackle without sacrificing the recovery. The
recently introduced Fast Robust PCA on Graphs (FRPCAG) approximates a recovery
method for matrices which are low-rank on graphs constructed between their rows
and columns. In this paper we provide a novel framework, Compressive PCA on
Graphs (CPCA) for an approximate recovery of such data matrices from sampled
measurements. We introduce a RIP condition for low-rank matrices on graphs
which enables efficient sampling of the rows and columns to perform FRPCAG on
the sampled matrix. Several efficient, parallel and parameter-free decoders are
presented along with their theoretical analysis for the low-rank recovery and
clustering applications of PCA. On a single core machine, CPCA gains a speed up
of p/k over FRPCAG, where k &lt;&lt; p is the subspace dimension. Numerically, CPCA
can efficiently cluster 70,000 MNIST digits in less than a minute and recover a
low-rank matrix of size 10304 X 1000 in 15 secs, which is 6 and 100 times
faster than FRPCAG and exact recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02086</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02086</id><created>2016-02-05</created><authors><author><keyname>Lin</keyname><forenames>Peng</forenames></author><author><keyname>Neil</keyname><forenames>Martin</forenames></author><author><keyname>Fenton</keyname><forenames>Norman</forenames></author></authors><title>Region Based Approximation for High Dimensional Bayesian Network Models</title><categories>cs.AI cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Performing efficient inference on Bayesian Networks (BNs), with large numbers
of densely connected variables is challenging. With exact inference methods,
such as the Junction Tree algorithm, clustering complexity can grow
exponentially with the number of nodes and so computation becomes intractable.
This paper presents a general purpose approximate inference algorithm called
Triplet Region Construction (TRC) that reduces the clustering complexity for
factorized models from worst case exponential to polynomial. We employ graph
factorization to reduce connection complexity and produce clusters of limited
size. Unlike MCMC algorithms TRC is guaranteed to converge and we present
experiments that show that TRC achieves accurate results when compared with
exact solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02089</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02089</id><created>2016-02-05</created><authors><author><keyname>Lewis</keyname><forenames>Martha</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>Harmonic Grammar in a DisCo Model of Meaning</title><categories>cs.AI cs.CL</categories><comments>Abstract, Advances in Distributional Semantics, IWCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of cognition developed in (Smolensky and Legendre, 2006) seeks to
unify two levels of description of the cognitive process: the connectionist and
the symbolic. The theory developed brings together these two levels into the
Integrated Connectionist/Symbolic Cognitive architecture (ICS). Clark and
Pulman (2007) draw a parallel with semantics where meaning may be modelled on
both distributional and symbolic levels, developed by Coecke et al, 2010 into
the Distributional Compositional (DisCo) model of meaning. In the current work,
we revisit Smolensky and Legendre (S&amp;L)'s model. We describe the DisCo
framework, summarise the key ideas in S&amp;L's architecture, and describe how
their description of harmony as a graded measure of grammaticality may be
applied in the DisCo model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02091</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02091</id><created>2016-02-04</created><authors><author><keyname>Tripathy</keyname><forenames>Malay Ranjan</forenames></author><author><keyname>Ranjan</keyname><forenames>Priya</forenames></author></authors><title>Towards Innovative Physical Layer Design for Mobile WSN Platforms</title><categories>cs.NI</categories><comments>Accepted for Publication at International Conference on IoT and Cloud
  Computing (ICC 2016)-http://icc-conference.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We today live in the era of dynamic and mobile wireless enabled platforms.
This kind of stringent communication capability in the face of volatile and
turbulent mobility demands a fresh look at physical layer in general and
antenna design in particular. The dimension of the antenna is 30x35x1.6 mm^3.
Multiplicity of bands is very useful for compatibility purposes where legacy
robotic platforms generally operate in MHz range while latest robotic platforms
are capable to handle GHz communication regimes and can pump data very at much
greater speeds. Seven frequency bands are obtained at 700 MHz, 2.4 GHz, 3.6
GHz, 4.37 GHz, 5.8 GHz, 6.93 GHz and 7.7 GHz with bandwidth of 1.1 GHz, 0.7
GHz, 0.8 GHz, 0.23 GHz, 0.90 GHz, 0.19 GHz and 0.12 GHz respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02098</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02098</id><created>2016-02-05</created><authors><author><keyname>Chowdhury</keyname><forenames>N. R.</forenames></author><author><keyname>Morarescu</keyname><forenames>I. -C.</forenames></author><author><keyname>Martin</keyname><forenames>S.</forenames></author><author><keyname>Srikant</keyname><forenames>S.</forenames></author></authors><title>Continuous opinions and discrete actions in social networks: a
  multi-agent system approach</title><categories>math.DS cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes and analyzes a novel multi-agent opinion dynamics model
in which agents have access to actions which are quantized version of the
opinions of their neighbors. The model produces different behaviors observed in
social networks such as disensus, clustering, oscillations, opinion
propagation, even when the communication network is connected. The main results
of the paper provides the characterization of preservation and diffusion of
actions under general communication topologies. A complete analysis allowing
the opinion forecasting is given in the particular cases of complete and ring
communication graphs. Numerical examples illustrate the main features of this
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02101</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02101</id><created>2016-02-05</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author></authors><title>Variance-Reduced and Projection-Free Stochastic Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Frank-Wolfe optimization algorithm has recently regained popularity for
machine learning applications due to its projection-free property and its
ability to handle structured constraints. However, in the stochastic learning
setting, it is still relatively understudied compared to the gradient descent
counterpart. In this work, leveraging a recent variance reduction technique, we
propose two stochastic Frank-Wolfe variants which substantially improve
previous results in terms of the number of stochastic gradient evaluations
needed to achieve $1-\epsilon$ accuracy. For example, we improve from
$O(\frac{1}{\epsilon})$ to $O(\ln\frac{1}{\epsilon})$ if the objective function
is smooth and strongly convex, and from $O(\frac{1}{\epsilon^2})$ to
$O(\frac{1}{\epsilon^{1.5}})$ if the objective function is smooth and
Lipschitz. The theoretical improvement is also observed in experiments on
real-world datasets for a multiclass classification application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02102</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02102</id><created>2016-02-05</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>The Spacey Random Walk: a Stochastic Process for Higher-order Data</title><categories>cs.NA cs.SI math.DS math.NA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random walks are a fundamental model in applied mathematics and are a common
example of a Markov chain. The limiting stationary distribution of the Markov
chain represents the fraction of the time spent in each state during the
stochastic process. A standard way to compute this distribution for a random
walk on a finite set of states is to compute the Perron vector of the
associated transition matrix. There are algebraic analogues of this Perron
vector in terms of probability transition tensors of higher-order Markov
chains. These vectors are nonnegative, have dimension equal to the dimension of
the state space, and sum to one. These were derived by making an algebraic
substitution in the equation for the joint-stationary distribution of a
higher-order Markov chains. Here, we present the spacey random walk, a
non-Markovian stochastic process whose stationary distribution is given by the
tensor eigenvector. The process itself is a vertex-reinforced random walk, and
its discrete dynamics are related to a continuous dynamical system. We analyze
the convergence properties of these dynamics and discuss numerical methods for
computing the stationary distribution. Finally, we provide several applications
of the spacey random walk model in population genetics, ranking, and clustering
data, and we use the process to analyze taxi trajectory data in New York. This
example shows definite non-Markovian structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02114</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02114</id><created>2016-02-05</created><authors><author><keyname>Todeschini</keyname><forenames>Adrien</forenames></author><author><keyname>Caron</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Exchangeable Random Measures for Sparse and Modular Graphs with
  Overlapping Communities</title><categories>stat.ME cs.SI physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel statistical model for sparse networks with overlapping
community structure. The model is based on representing the graph as an
exchangeable point process, and naturally generalizes existing probabilistic
models with overlapping block-structure to the sparse regime. Our construction
builds on vectors of completely random measures, and has interpretable
parameters, each node being assigned a vector representing its level of
affiliation to some latent communities. We develop methods for simulating this
class of random graphs, as well as to perform posterior inference. We show that
the proposed approach can recover interpretable structure from two real-world
networks and can handle graphs with thousands of nodes and tens of thousands of
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02120</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02120</id><created>2016-02-05</created><authors><author><keyname>Blelloch</keyname><forenames>Guy</forenames></author><author><keyname>Ferizovic</keyname><forenames>Daniel</forenames></author><author><keyname>Sun</keyname><forenames>Yihan</forenames></author></authors><title>Parallel Ordered Sets Using Join</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ordered set is one of the most important data type in both theoretical
algorithm design and analysis and practical programming. In this paper we study
the set operations on two ordered sets, including Union, Intersect and
Difference, based on four types of balanced Binary Search Trees (BST) including
AVL trees, red-black trees, weight balanced trees and treaps. We introduced
only one subroutine Join that needs to be implemented differently for each
balanced BST, and on top of which we can implement generic, simple and
efficient parallel functions for ordered sets. We first prove the
work-efficiency of these Join-based set functions using a generic proof working
for all the four types of balanced BSTs.
  We also implemented and tested our algorithm on all the four balancing
schemes. Interestingly the implementations on all four data structures and
three set functions perform similarly in time and speedup (more than 45x on 64
cores). We also compare the performance of our implementation to other existing
libraries and algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02123</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02123</id><created>2016-02-05</created><authors><author><keyname>Abramson</keyname><forenames>Myriam</forenames></author></authors><title>Sequence Classification with Neural Conditional Random Fields</title><categories>cs.LG</categories><comments>14th International Conference on Machine Learning and Applications
  (ICMLA) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of sensor devices monitoring human activity generates
voluminous amount of temporal sequences needing to be interpreted and
categorized. Moreover, complex behavior detection requires the personalization
of multi-sensor fusion algorithms. Conditional random fields (CRFs) are
commonly used in structured prediction tasks such as part-of-speech tagging in
natural language processing. Conditional probabilities guide the choice of each
tag/label in the sequence conflating the structured prediction task with the
sequence classification task where different models provide different
categorization of the same sequence. The claim of this paper is that CRF models
also provide discriminative models to distinguish between types of sequence
regardless of the accuracy of the labels obtained if we calibrate the class
membership estimate of the sequence. We introduce and compare different neural
network based linear-chain CRFs and we present experiments on two complex
sequence classification and structured prediction tasks to support this claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02125</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02125</id><created>2016-02-03</created><authors><author><keyname>Liu</keyname><forenames>Na</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Guang-Wei</forenames></author><author><keyname>Mi</keyname><forenames>Zheng-Hui</forenames></author><author><keyname>Lin</keyname><forenames>Hai-Ying</forenames></author><author><keyname>Wang</keyname><forenames>Qun-Yao</forenames></author><author><keyname>Liu</keyname><forenames>Rong</forenames></author><author><keyname>Ma</keyname><forenames>Xin-Peng</forenames></author></authors><title>Tuner control system of spoke012 SRF cavity for C-ADS injector I at IHEP</title><categories>physics.acc-ph cs.SY hep-ex</categories><comments>6 pages,11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new tuner control system of spoke superconducting radio frequency (SRF)
cavity has been developed and applied to cryomodule I (CM1) of C-ADS injector I
at IHEP. We have successfully implemented the tuner controller based on
Programmable Logic Controller (PLC) for the first time and achieved a cavity
tuning phase error of 0.7degrees (about 4 Hz peak to peak) in the presence of
electromechanical coupled resonance. This paper will present the preliminary
experimental results based on PLC tuner controller under proton beam
commissioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02129</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02129</id><created>2016-02-05</created><authors><author><keyname>Borassi</keyname><forenames>Michele</forenames></author></authors><title>A Note on the Complexity of Computing the Number of Reachable Vertices
  in a Digraph</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider the following problem: given a digraph $G=(V,E)$,
for each vertex $v$, we want to compute the number of vertices reachable from
$v$. In other words, we want to compute the out-degree of each vertex in the
transitive closure of $G$. We show that this problem is not solvable in time
$\mathcal{O}\left(|E|^{2-\epsilon}\right)$ for any $\epsilon&gt;0$, unless the
Strong Exponential Time Hypothesis is false. This result still holds if $G$ is
assumed to be acyclic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02130</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02130</id><created>2016-02-05</created><authors><author><keyname>Shakeri</keyname><forenames>Mahsa</forenames><affiliation>CVN, GALEN</affiliation></author><author><keyname>Tsogkas</keyname><forenames>Stavros</forenames><affiliation>CVN, GALEN</affiliation></author><author><keyname>Ferrante</keyname><forenames>Enzo</forenames><affiliation>CVN, GALEN</affiliation></author><author><keyname>Lippe</keyname><forenames>Sarah</forenames><affiliation>CVN, GALEN</affiliation></author><author><keyname>Kadoury</keyname><forenames>Samuel</forenames><affiliation>CVN, GALEN</affiliation></author><author><keyname>Paragios</keyname><forenames>Nikos</forenames><affiliation>CVN, GALEN</affiliation></author><author><keyname>Kokkinos</keyname><forenames>Iasonas</forenames><affiliation>CVN, GALEN</affiliation></author></authors><title>Sub-cortical brain structure segmentation using F-CNN's</title><categories>cs.CV</categories><comments>ISBI 2016: International Symposium on Biomedical Imaging, Apr 2016,
  Prague, Czech Republic</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a deep learning approach for segmenting sub-cortical
structures of the human brain in Magnetic Resonance (MR) image data. We draw
inspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN)
architecture for semantic segmentation of objects in natural images, and adapt
it to our task. Unlike previous CNN-based methods that operate on image
patches, our model is applied on a full blown 2D image, without any alignment
or registration steps at testing time. We further improve segmentation results
by interpreting the CNN output as potentials of a Markov Random Field (MRF),
whose topology corresponds to a volumetric grid. Alpha-expansion is used to
perform approximate inference imposing spatial volumetric homogeneity to the
CNN priors. We compare the performance of the proposed pipeline with a similar
system using Random Forest-based priors, as well as state-of-art segmentation
algorithms, and show promising results on two different brain MRI datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02133</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02133</id><created>2016-02-05</created><authors><author><keyname>Atoum</keyname><forenames>Issa</forenames></author><author><keyname>Otoom</keyname><forenames>Ahmed</forenames></author></authors><title>Mining Software Quality from Software Reviews: Research Trends and Open
  Issues</title><categories>cs.CL cs.IR</categories><comments>11 pages</comments><journal-ref>International Journal of Computer Trends and Technology,Vol. 31,
  No. 2, Jan 2016</journal-ref><doi>10.14445/22312803/IJCTT-V31P114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software review text fragments have considerably valuable information about
users experience. It includes a huge set of properties including the software
quality. Opinion mining or sentiment analysis is concerned with analyzing
textual user judgments. The application of sentiment analysis on software
reviews can find a quantitative value that represents software quality.
Although many software quality methods are proposed they are considered
difficult to customize and many of them are limited. This article investigates
the application of opinion mining as an approach to extract software quality
properties. We found that the major issues of software reviews mining using
sentiment analysis are due to software lifecycle and the diverse users and
teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02136</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02136</id><created>2016-02-05</created><authors><author><keyname>Wang</keyname><forenames>Jialei</forenames></author><author><keyname>Wang</keyname><forenames>Hai</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Reducing Runtime by Recycling Samples</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrary to the situation with stochastic gradient descent, we argue that
when using stochastic methods with variance reduction, such as SDCA, SAG or
SVRG, as well as their variants, it could be beneficial to reuse previously
used samples instead of fresh samples, even when fresh samples are available.
We demonstrate this empirically for SDCA, SAG and SVRG, studying the optimal
sample size one should use, and also uncover be-havior that suggests running
SDCA for an integer number of epochs could be wasteful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02139</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02139</id><created>2016-02-03</created><authors><author><keyname>Chamorro-Posada</keyname><forenames>P.</forenames></author></authors><title>A simple method for estimating the fractal dimension from digital
  images: The compression dimension</title><categories>cs.GR cs.CV physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fractal structure of real world objects is often analyzed using digital
images. In this context, the compression fractal dimension is put forward. It
provides a simple method for the direct estimation of the dimension of fractals
stored as digital image files. The computational scheme can be implemented
using readily available free software. Its simplicity also makes it very
interesting for introductory elaborations of basic concepts of fractal
geometry, complexity, and information theory. A test of the computational
scheme using limited-quality images of well-defined fractal sets obtained from
the Internet and free software has been performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02144</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02144</id><created>2016-02-05</created><authors><author><keyname>Moura</keyname><forenames>Jose</forenames></author><author><keyname>Edwards</keyname><forenames>Christopher</forenames></author></authors><title>Efficient Access of Mobile Flows to Heterogeneous Networks under Flash
  Crowds</title><categories>cs.NI</categories><comments>Submitted to a Journal Special Issue,2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless networks need to offer orders of magnitude more capacity to
address the predicted growth in mobile traffic demand. Operators to enhance the
capacity of cellular networks are increasingly using WiFi to offload traffic
from their core networks. This paper deals with the efficient and flexible
management of a heterogeneous networking environment offering wireless access
to multimode terminals. This wireless access is evaluated under disruptive
usage scenarios, such as flash crowds, which can mean unwanted severe
congestion on a specific operator network whilst the remaining available
capacity from other access technologies is not being used. To address these
issues, we propose a scalable network assisted distributed solution that is
administered by centralized policies, and an embedded reputation system, by
which initially selfish operators are encouraged to cooperate under the threat
of churn. Our solution after detecting a congested technology, including within
its wired backhaul, automatically offloads and balances the flows amongst the
access resources from all the existing technologies, following some quality
metrics. Our results show that the smart integration of access networks can
yield an additional wireless quality for mobile flows up to thirty eight
percent beyond that feasible from the best effort standalone operation of each
wireless access technology. It is also evidenced that backhaul constraints are
conveniently reflected on the way the flow access to wireless media is granted.
Finally, we have analyzed the sensitivity of the handover decision algorithm
running in each terminal agent to consecutive flash crowds, as well as its
centralized feature that controls the connection quality offered by a
heterogeneous access infrastructure owned by distinct operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02148</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02148</id><created>2016-02-05</created><authors><author><keyname>Gupta</keyname><forenames>Boudhayan</forenames></author></authors><title>A replay-attack resistant message authentication scheme using time-based
  keying hash functions and unique message identifiers</title><categories>cs.CR</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Hash-based message authentication codes are an extremely simple yet hugely
effective construction for producing keyed message digests using shared
secrets. HMACs have seen widespread use as ad-hoc digital signatures in many
Internet applications. While messages signed with an HMAC are secure against
sender impersonation and tampering in transit, if used alone they are
susceptible to replay attacks. We propose a construction that extends HMACs to
produce a keyed message digest that has a finite validity period. We then
propose a message signature scheme that uses this time-dependent MAC along with
an unique message identifier to calculate a set of authentication factors using
which a recipient can readily detect and ignore replayed messages, thus
providing perfect resistance against replay attacks. We further analyse
time-based message authentication codes and show that they provide stronger
security guarantees than plain HMACs, even when used independently of the
aforementioned replay attack resistant message signature scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02151</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02151</id><created>2016-02-05</created><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of data available in the world is growing faster and bigger than
our ability to deal with it. However, if we take advantage of the internal
structure, data may become much smaller for machine learning purposes. In this
paper we focus on one of the most fundamental machine learning tasks, empirical
risk minimization (ERM), and provide faster algorithms with the help from the
clustering structure of the data.
  We introduce a simple notion of raw clustering that can be efficiently
obtained with just one pass of the data, and propose two algorithms. Our
variance-reduction based algorithm ClusterSVRG introduces a new gradient
estimator using the clustering information, and our accelerated algorithm
ClusterACDM is built on a novel Haar transformation applied to the dual space
of each cluster. Our algorithms outperform their classical counterparts both in
theory and practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02159</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02159</id><created>2016-02-05</created><authors><author><keyname>Samreen</keyname><forenames>Faiza</forenames></author><author><keyname>Elkhatib</keyname><forenames>Yehia</forenames></author><author><keyname>Rowe</keyname><forenames>Matthew</forenames></author><author><keyname>Blair</keyname><forenames>Gordon S.</forenames></author></authors><title>Daleel: Simplifying Cloud Instance Selection Using Machine Learning</title><categories>cs.DC cs.LG cs.PF</categories><comments>In the IEEE/IFIP Network Operations and Management Symposium (NOMS),
  April 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision making in cloud environments is quite challenging due to the
diversity in service offerings and pricing models, especially considering that
the cloud market is an incredibly fast moving one. In addition, there are no
hard and fast rules, each customer has a specific set of constraints (e.g.
budget) and application requirements (e.g. minimum computational resources).
Machine learning can help address some of the complicated decisions by carrying
out customer-specific analytics to determine the most suitable instance type(s)
and the most opportune time for starting or migrating instances. We employ
machine learning techniques to develop an adaptive deployment policy, providing
an optimal match between the customer demands and the available cloud service
offerings. We provide an experimental study based on extensive set of job
executions over a major public cloud infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02164</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02164</id><created>2016-02-05</created><authors><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author></authors><title>A Note on Alternating Minimization Algorithm for the Matrix Completion
  Problem</title><categories>stat.ML cs.LG cs.NA</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing a low rank matrix from a subset of
its entries and analyze two variants of the so-called Alternating Minimization
algorithm, which has been proposed in the past. We establish that when the
underlying matrix has rank $r=1$, has positive bounded entries, and the graph
$\mathcal{G}$ underlying the revealed entries has bounded degree and diameter
which is at most logarithmic in the size of the matrix, both algorithms succeed
in reconstructing the matrix approximately in polynomial time starting from an
arbitrary initialization. We further provide simulation results which suggest
that the second algorithm which is based on the message passing type updates,
performs significantly better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02169</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02169</id><created>2016-02-05</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author></authors><title>Probabilistic Extension to the Concurrent Constraint Factor Oracle Model
  for Music Improvisation</title><categories>cs.AI</categories><comments>70 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We can program a Real-Time (RT) music improvisation system in C++ without a
formal semantic or we can model it with process calculi such as the
Non-deterministic Timed Concurrent Constraint (ntcc) calculus. &quot;A Concurrent
Constraints Factor Oracle (FO) model for Music Improvisation&quot; (Ccfomi) is an
improvisation model specified on ntcc. Since Ccfomi improvises
non-deterministically, there is no control on choices and therefore little
control over the sequence variation during the improvisation. To avoid this, we
extended Ccfomi using the Probabilistic Non-deterministic Timed Concurrent
Constraint calculus. Our extension to Ccfomi does not change the time and space
complexity of building the FO, thus making our extension compatible with RT.
However, there was not a ntcc interpreter capable of RT to execute Ccfomi. We
developed Ntccrt --a RT capable interpreter for ntcc-- and we executed Ccfomi
on Ntccrt. In the future, we plan to extend Ntccrt to execute our extension to
Ccfomi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02172</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02172</id><created>2016-02-05</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author></authors><title>On Column Selection in Approximate Kernel Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of column selection in large-scale kernel canonical
correlation analysis (KCCA) using the Nystr\&quot;om approximation, where one
approximates two positive semi-definite kernel matrices using &quot;landmark&quot; points
from the training set. When building low-rank kernel approximations in KCCA,
previous work mostly samples the landmarks uniformly at random from the
training set. We propose novel strategies for sampling the landmarks
non-uniformly based on a version of statistical leverage scores recently
developed for kernel ridge regression. We study the approximation accuracy of
the proposed non-uniform sampling strategy, develop an incremental algorithm
that explores the path of approximation ranks and facilitates efficient model
selection, and derive the kernel stability of out-of-sample mapping for our
method. Experimental results on both synthetic and real-world datasets
demonstrate the promise of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02174</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02174</id><created>2016-02-05</created><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Participation Incentives in Randomized Social Choice</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When aggregating preferences of agents via voting, two desirable goals are to
identify outcomes that are Pareto optimal and to incentivize agents to
participate in the voting process. We consider participation notions as
formalized by Brandl, Brandt, and Hofbauer (2015) and study how far efficiency
and participation are achievable by randomized social choice functions in
particular when agents' preferences are downward lexicographic (DL) or satisfy
stochastic dominance (SD). Our results include the followings ones: we prove
formal relations between the participation notions with respect to SD and DL
and we show that the maximal recursive rule satisfies very strong participation
with respect to both SD and DL whereas the egalitarian simultaneous reservation
rule satisfies strong SD-participation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02178</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02178</id><created>2016-02-05</created><authors><author><keyname>Tran</keyname><forenames>Tuyen X.</forenames></author><author><keyname>Hajisami</keyname><forenames>Abolfazl</forenames></author><author><keyname>Pompili</keyname><forenames>Dario</forenames></author></authors><title>Cooperative Hierarchical Caching in 5G Cloud Radio Access Networks
  (C-RANs)</title><categories>cs.IT cs.NI math.IT</categories><comments>a version of this paper has been submitted to IEEE Communications
  Magazine, Special Issue on Communications, Caching, and Computing for
  Content-Centric Mobile Networks, Jan. 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, Cloud Radio Access Network (C-RAN) has arisen as a
transformative architecture for 5G cellular networks that brings the
flexibility and agility of cloud computing to wireless communications. At the
same time, content caching in wireless networks has become an essential
solution to lower the content-access latency and backhaul traffic loading,
which translate into user Quality of Experience (QoE) improvement and network
cost reduction. In this article, a novel Cooperative Hierarchical Caching (CHC)
framework in C-RAN is introduced where contents are jointly cached at the
BaseBand Unit (BBU) and at the Radio Remote Heads (RRHs). Unlike in traditional
approaches, the cache at the BBU, cloud cache, presents a new layer in the
cache hierarchy, bridging the latency/capacity gap between the traditional
edge-based and core-based caching schemes. Trace-driven simulations reveal that
CHC yields up to 80% improvement in cache hit ratio, 21% decrease in average
content-access latency, and 20% reduction in backhaul traffic load compared to
the edge-only caching scheme with the same total cache capacity. Before closing
the article, several challenges and promising opportunities for deploying
content caching in C-RAN are highlighted towards a content-centric mobile
wireless network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02181</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02181</id><created>2016-02-05</created><authors><author><keyname>He</keyname><forenames>He</forenames></author><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author></authors><title>Active Information Acquisition</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for sequential and dynamic acquisition of
useful information in order to solve a particular task. While our goal could in
principle be tackled by general reinforcement learning, our particular setting
is constrained enough to allow more efficient algorithms. In this paper, we
work under the Learning to Search framework and show how to formulate the goal
of finding a dynamic information acquisition policy in that framework. We apply
our formulation on two tasks, sentiment analysis and image recognition, and
show that the learned policies exhibit good statistical performance. As an
emergent byproduct, the learned policies show a tendency to focus on the most
prominent parts of each instance and give harder instances more attention
without explicitly being trained to do so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02191</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02191</id><created>2016-02-05</created><updated>2016-03-03</updated><authors><author><keyname>Azar</keyname><forenames>Mohammad Gheshlaghi</forenames></author><author><keyname>Dyer</keyname><forenames>Eva</forenames></author><author><keyname>Kording</keyname><forenames>Konrad</forenames></author></authors><title>Convex Relaxation Regression: Black-Box Optimization of Smooth Functions
  by Learning Their Convex Envelopes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding efficient and provable methods to solve non-convex optimization
problems is an outstanding challenge in machine learning and optimization
theory. A popular approach used to tackle non-convex problems is to use convex
relaxation techniques to find a convex surrogate for the problem.
Unfortunately, convex relaxations typically must be found on a
problem-by-problem basis. Thus, providing a general-purpose strategy to
estimate a convex relaxation would have a wide reaching impact. Here, we
introduce Convex Relaxation Regression (CoRR), an approach for learning convex
relaxations for a class of smooth functions. The main idea behind our approach
is to estimate the convex envelope of a function $f$ by evaluating $f$ at a set
of $T$ random points and then fitting a convex function to these function
evaluations. We prove that with probability greater than $1-\delta$, the
solution of our algorithm converges to the global optimizer of $f$ with error
$\mathcal{O} \Big( \big(\frac{\log(1/\delta) }{T} \big)^{\alpha} \Big)$ for
some $\alpha&gt; 0$. Our approach enables the use of convex optimization tools to
solve a class of non-convex optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02196</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02196</id><created>2016-02-05</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present efficient algorithms for the problem of contextual bandits with
i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of
policies. Our algorithm BISTRO requires d calls to the empirical risk
minimization (ERM) oracle per round, where d is the number of actions. The
method uses unlabeled data to make the problem computationally simple. When the
ERM problem itself is computationally hard, we extend the approach by employing
multiplicative approximation algorithms for the ERM. The integrality gap of the
relaxation only enters in the regret bound rather than the benchmark. Finally,
we show that the adversarial version of the contextual bandit problem is
learnable (and efficient) whenever the full-information supervised online
learning problem has a non-trivial regret guarantee (and efficient).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02201</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02201</id><created>2016-02-05</created><authors><author><keyname>Kipnis</keyname><forenames>Alon</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Mismatched Multiterminal Source Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiterminal source coding problem in which a random source
signal is estimated from encoded versions of multiple noisy observations. Each
encoded version, however, minimizes a local distortion measure, defined only
with respect to the distribution of the corresponding noisy observation. We
denote the form of encoding as mismatched multiterminal encoding (MME).We
derive a single letter expression for the minimal distortion under MME of an
i.i.d source. We evaluate this expression for the case of a Gaussian source
observed through multiple parallel AWGN channels and quadratic distortion and
in the case of a nonuniform binary i.i.d source observed through multiple
binary symmetric channels under Hamming distortion. For the case of a Gaussian
source, we show that there is no loss of performance compared to the indirect
source coding distortion-rate function due to MME using a centralized encoder
and small code rates, whereas distributed encoding achieves distortion strictly
larger then in an optimal multiterminal source coding scheme. For the case of a
binary source, we show that even with a single observation, the distortion-rate
function under MME is strictly larger than that of indirect source coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02202</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02202</id><created>2016-02-05</created><authors><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Efficient Second Order Online Learning via Sketching</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Sketched Online Newton (SON), an online second order learning
algorithm that enjoys substantially improved regret guarantees for
ill-conditioned data. SON is an enhanced version of the Online Newton Step,
which, via sketching techniques enjoys a linear running time. We further
improve the computational complexity to linear in the number of nonzero entries
by creating sparse forms of the sketching methods (such as Oja's rule) for top
eigenvector extraction. Together, these algorithms eliminate all computational
obstacles in previous second order online learning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02203</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02203</id><created>2016-02-05</created><authors><author><keyname>Davoodi</keyname><forenames>Arash Gholami</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>GDoF of the K user Symmetric MISO BC: Bridging the Gap between Finite
  Precision and Perfect CSIT</title><categories>cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the sum GDoF of the $K $ user symmetric MISO BC where the
direct channels have strengths $\sim SNR$, cross channels have strengths $\sim
SNR^{\alpha}$, and the channel estimation error terms have strengths $\sim
SNR^{-\beta}$. This is a step towards the ultimate goal of unifying recent
advances which focus exclusively on either the diversity of channel strengths
($\alpha$) or the channel uncertainty ($\beta$) aspect, and bridging the gap
between the divergent extremes studied thus far where 1) cross-channels are
either assumed to be as strong as direct channels ($\alpha=1$, as in DoF
studies) or so weak (small $\alpha$) that they can be optimally treated as
noise (TIN); or 2) where channel knowledge is assumed to be either absent
$(\beta=0)$ or essentially perfect (large $\beta$). Restricting
$\alpha\in[0,1]$ and without loss of generality $\beta\in[0,\alpha]$, it is
shown that the $K$ user symmetric MISO BC has
$(\alpha-\beta)+K(1-(\alpha-\beta))$ GDoF. A useful interpretation is that the
power levels effectively split into a fraction $(\alpha-\beta)$ where CSIT is
essentially absent (GDoF $=1$ per power level dimension), and the remaining
$1-(\alpha-\beta)$ fraction where CSIT is essentially perfect (GDoF $=K$ per
power level dimension). For $K=2$ users with arbitrary $\beta_{kl}$ parameters,
the GDoF are shown to be $2-\alpha+\min_{k,l} \beta_{kl}$. Remarkably, the
roles of $\alpha$ and $\beta$ counter each other on equal terms so that only
their difference matters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02205</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02205</id><created>2016-02-05</created><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Shitz</keyname><forenames>Shlomo Shamai</forenames></author></authors><title>On the Capacity of the Dirty Paper Channel with Fast Fading and Discrete
  Channel States</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;writing dirty paper&quot; capacity result crucially dependents on the perfect
channel knowledge at the transmitter as the presence of even a small
uncertainty in the channel realization gravely hampers the ability of the
transmitter to pre-code its transmission against the channel state. This is
particularly disappointing as it implies that interference pre-coding in
practical systems is effective only when the channel estimates at the users
have very high precision, a condition which is generally unattainable in
wireless environments. In this paper we show that substantial improvements are
possible when the state sequence is drawn from a discrete distribution, such as
a constrained input constellation, for which state decoding can be
approximately optimal. We consider the &quot;writing on dirty paper&quot; channel in
which the state sequence is multiplied by a fast fading process and derive
conditions on the fading and state distributions for which state decoding
closely approaches capacity. These conditions intuitively relate to the ability
of the receiver to correctly identify both the input and the state realization
despite of the uncertainty introduced by fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02206</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02206</id><created>2016-02-05</created><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Shitz</keyname><forenames>Shlomo Shamai</forenames></author></authors><title>The Carbon Copy onto Dirty Paper Channel with Statistically Equivalent
  States</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Costa's &quot;writing on dirty paper&quot; capacity result establishes that full state
pre-cancellation can be attained in Gelfand-Pinsker channel with additive state
and additive Gaussian noise. The &quot;carbon copy onto dirty paper&quot; channel is the
extension of Costa's model to the compound setting: M receivers each observe
the sum of the channel input, Gaussian noise and one of M Gaussian state
sequences and attempt to decode the same common message. The state sequences
are all non-causally known at the transmitter which attempts to simultaneously
pre-code its transmission against the channel state affecting each output. In
this correspondence we derive the capacity to within 2.25 bits-per-channel-use
of the carbon copying onto dirty paper channel in which the state sequences are
statistically equivalent, having the same variance and the same pairwise
correlation. For this channel capacity is approached by letting the channel
input be the superposition of two codewords: a base codeword, simultaneously
decoded at each user, and a top codeword which is pre-coded against the state
realization at each user for a portion 1/M of the time. The outer bound relies
on a recursive bounding in which incremental side information is provided at
each receiver. This result represents a significant first step toward
determining the capacity of the most general &quot;carbon copy onto dirty paper&quot;
channel in which state sequences appearing in the different channel outputs
have any jointly Gaussian distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02210</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02210</id><created>2016-02-05</created><authors><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Classification Accuracy as a Proxy for Two Sample Testing</title><categories>cs.LG cs.AI math.ST stat.ML stat.TH</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When data analysts train a classifier and check if its accuracy is
significantly different from random guessing, they are implicitly and
indirectly performing a hypothesis test (two sample testing) and it is of
importance to ask whether this indirect method for testing is statistically
optimal or not. Given that hypothesis tests attempt to maximize statistical
power subject to a bound on the allowable false positive rate, while prediction
attempts to minimize statistical risk on future predictions on unseen data, we
wish to study whether a predictive approach for an ultimate aim of testing is
prudent. We formalize this problem by considering the two-sample mean-testing
setting where one must determine if the means of two Gaussians (with known and
equal covariance) are the same or not, but the analyst indirectly does so by
checking whether the accuracy achieved by Fisher's LDA classifier is
significantly different from chance or not. Unexpectedly, we find that the
asymptotic power of LDA's sample-splitting classification accuracy is actually
minimax rate-optimal in terms of problem-dependent parameters. Since prediction
is commonly thought to be harder than testing, it might come as a surprise to
some that solving a harder problem does not create a information-theoretic
bottleneck for the easier one. On the flip side, even though the power is
rate-optimal, our derivation suggests that it may be worse by a small constant
factor; hence practitioners must be wary of using (admittedly flexible)
prediction methods on disguised testing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02211</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02211</id><created>2016-02-05</created><authors><author><keyname>Halaby</keyname><forenames>Mohamed El</forenames></author><author><keyname>Abdalla</keyname><forenames>Areeg</forenames></author></authors><title>Fuzzy Maximum Satisfiability</title><categories>cs.LO cs.AI cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the Maximum Satisfiability (MaxSAT) problem to
{\L}ukasiewicz logic. The MaxSAT problem for a set of formulae {\Phi} is the
problem of finding an assignment to the variables in {\Phi} that satisfies the
maximum number of formulae. Three possible solutions (encodings) are proposed
to the new problem: (1) Disjunctive Linear Relations (DLRs), (2) Mixed Integer
Linear Programming (MILP) and (3) Weighted Constraint Satisfaction Problem
(WCSP). Like its Boolean counterpart, the extended fuzzy MaxSAT will have
numerous applications in optimization problems that involve vagueness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02215</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02215</id><created>2016-02-05</created><authors><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author><author><keyname>Doherty</keyname><forenames>Ryan</forenames></author><author><keyname>Evans</keyname><forenames>Colin</forenames></author><author><keyname>Waterson</keyname><forenames>Chris</forenames></author></authors><title>Swivel: Improving Embeddings by Noticing What's Missing</title><categories>cs.CL</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Submatrix-wise Vector Embedding Learner (Swivel), a method for
generating low-dimensional feature embeddings from a feature co-occurrence
matrix. Swivel performs approximate factorization of the point-wise mutual
information matrix via stochastic gradient descent. It uses a piecewise loss
with special handling for unobserved co-occurrences, and thus makes use of all
the information in the matrix. While this requires computation proportional to
the size of the entire matrix, we make use of vectorized multiplication to
process thousands of rows and columns at once to compute millions of predicted
values. Furthermore, we partition the matrix into shards in order to
parallelize the computation across many nodes. This approach results in more
accurate embeddings than can be achieved with methods that consider only
observed co-occurrences, and can scale to much larger corpora than can be
handled with sampling methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02216</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02216</id><created>2016-02-06</created><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>Smoothing Brascamp-Lieb Inequalities and Strong Converses for Common
  Randomness Generation</title><categories>cs.IT math.IT</categories><comments>7 pages; first 5 pages submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the infimum of the best constant in a functional inequality, the
Brascamp-Lieb-like inequality, over auxiliary measures within a neighborhood of
a product distribution. In the finite alphabet and the Gaussian cases, such an
infimum converges to the best constant in a mutual information inequality.
Implications for strong converse properties of two common randomness (CR)
generation problems are discussed. In particular, we prove the strong converse
property of the rate region for the omniscient helper CR generation problem in
the discrete and the Gaussian cases. The latter case is perhaps the first
instance of a strong converse for a continuous source when the rate region
involves auxiliary random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02218</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02218</id><created>2016-02-06</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Ghifary</keyname><forenames>Muhammad</forenames></author></authors><title>Strongly-Typed Recurrent Neural Networks</title><categories>cs.LG cs.NE</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks are increasing popular models for sequential
learning. Unfortunately, although the most effective RNN architectures are
perhaps excessively complicated, extensive searches have not found simpler
alternatives. This paper imports ideas from physics and functional programming
into RNN design to provide guiding principles. From physics we introduce type
constraints, analogous to the constraints that disqualify adding meters to
seconds in physics. From functional programming, we require that strongly-typed
architectures factorize into stateless learnware and state-dependent firmware,
thereby ameliorating the impact of side-effects. The features learned by
strongly-typed nets have a simple semantic interpretation via dynamic
average-pooling on one-dimensional convolutions. We also show that
strongly-typed gradients are better behaved than in classical architectures,
and characterize the representational power of strongly-typed nets. Finally,
experiments show that, despite being more constrained, strongly-typed
architectures achieve lower training error and comparable generalization error
to classical architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02220</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02220</id><created>2016-02-06</created><authors><author><keyname>Li</keyname><forenames>Zhe</forenames></author><author><keyname>Gong</keyname><forenames>Boqing</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author></authors><title>Improved Dropout for Shallow and Deep Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dropout has been witnessed with great success in training deep neural
networks by independently zeroing out the outputs of neurons at random. It has
also received a surge of interest for shallow learning, e.g., logistic
regression. However, the independent sampling for dropout could be suboptimal
for the sake of convergence. In this paper, we propose to use multinomial
sampling for dropout, i.e., sampling features or neurons according to a
multinomial distribution with different probabilities for different
features/neurons. To exhibit the optimal dropout probabilities, we analyze the
shallow learning with multinomial dropout and establish the risk bound for
stochastic optimization. By minimizing a sampling dependent factor in the risk
bound, we obtain a distribution-dependent dropout with sampling probabilities
dependent on the second order statistics of the data distribution. To tackle
the issue of evolving distribution of neurons in deep learning, we propose an
efficient adaptive dropout (named \textbf{evolutional dropout}) that computes
the sampling probabilities on-the-fly from a mini-batch of examples. Empirical
studies on several benchmark datasets demonstrate that the proposed dropouts
achieve not only much faster convergence and but also a smaller testing error
than the standard dropout. For example, on the CIFAR-100 data, the evolutional
dropout achieves relative improvements over 10\% on the prediction performance
and over 50\% on the convergence speed compared to the standard dropout.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02235</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02235</id><created>2016-02-06</created><authors><author><keyname>Fan</keyname><forenames>Jihao</forenames></author><author><keyname>Chen</keyname><forenames>Hanwu</forenames></author><author><keyname>Xu</keyname><forenames>Juan</forenames></author></authors><title>Constructions of q-ary entanglement-assisted quantum MDS codes with
  minimum distance greater than q + 1</title><categories>quant-ph cs.IT math.IT</categories><comments>12 pages</comments><journal-ref>Quantum Information and Computation, vol. 16, no. 5&amp;6, pp.
  0423-0434, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entanglement-assisted stabilizer formalism provides a useful framework
for constructing quantum error-correcting codes (QECC), which can transform
arbitrary classical linear codes into entanglement-assisted quantum error
correcting codes (EAQECCs) by using pre-shared entanglement between the sender
and the receiver. In this paper, we construct five classes of
entanglement-assisted quantum MDS (EAQMDS) codes based on classical MDS codes
by exploiting one or more pre-shared maximally entangled states. We show that
these EAQMDS codes have much larger minimum distance than the standard quantum
MDS (QMDS) codes of the same length, and three classes of these EAQMDS codes
consume only one pair of maximally entangled states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02237</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02237</id><created>2016-02-06</created><authors><author><keyname>Atyabi</keyname><forenames>Adham</forenames></author><author><keyname>Luerssena</keyname><forenames>Martin</forenames></author><author><keyname>Fitzgibbon</keyname><forenames>Sean P.</forenames></author><author><keyname>Lewis</keyname><forenames>Trent</forenames></author><author><keyname>Powersa</keyname><forenames>David M. W.</forenames></author></authors><title>Reducing training requirements through evolutionary based dimension
  reduction and subject transfer</title><categories>cs.NE</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Training Brain Computer Interface (BCI) systems to understand the intention
of a subject through Electroencephalogram (EEG) data currently requires
multiple training sessions with a subject in order to develop the necessary
expertise to distinguish signals for different tasks. Conventionally the task
of training the subject is done by introducing a training and calibration stage
during which some feedback is presented to the subject. This training session
can take several hours which is not appropriate for on-line EEG-based BCI
systems. An alternative approach is to use previous recording sessions of the
same person or some other subjects that performed the same tasks (subject
transfer) for training the classifiers. The main aim of this study is to
generate a methodology that allows the use of data from other subjects while
reducing the dimensions of the data. The study investigates several
possibilities for reducing the necessary training and calibration period in
subjects and the classifiers and addresses the impact of i) evolutionary
subject transfer and ii) adapting previously trained methods (retraining) using
other subjects data. Our results suggest reduction to 40% of target subject
data is sufficient for training the classifier. Our results also indicate the
superiority of the approaches that incorporated evolutionary subject transfer
and highlights the feasibility of adapting a system trained on other subjects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02238</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02238</id><created>2016-02-06</created><authors><author><keyname>Quattrone</keyname><forenames>Giovanni</forenames></author><author><keyname>Proserpio</keyname><forenames>Davide</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Capra</keyname><forenames>Licia</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Who Benefits from the &quot;Sharing&quot; Economy of Airbnb?</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>In Proceedings of the 26th International ACM Conference on World Wide
  Web (WWW), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing economy platforms have become extremely popular in the last few
years, and they have changed the way in which we commute, travel, and borrow
among many other activities. Despite their popularity among consumers, such
companies are poorly regulated. For example, Airbnb, one of the most successful
examples of sharing economy platform, is often criticized by regulators and
policy makers. While, in theory, municipalities should regulate the emergence
of Airbnb through evidence-based policy making, in practice, they engage in a
false dichotomy: some municipalities allow the business without imposing any
regulation, while others ban it altogether. That is because there is no
evidence upon which to draft policies. Here we propose to gather evidence from
the Web. After crawling Airbnb data for the entire city of London, we find out
where and when Airbnb listings are offered and, by matching such listing
information with census and hotel data, we determine the socio-economic
conditions of the areas that actually benefit from the hospitality platform.
The reality is more nuanced than one would expect, and it has changed over the
years. Airbnb demand and offering have changed over time, and traditional
regulations have not been able to respond to those changes. That is why,
finally, we rely on our data analysis to envision regulations that are
responsive to real-time demands, contributing to the emerging idea of
&quot;algorithmic regulation&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02241</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02241</id><created>2016-02-06</created><authors><author><keyname>Karmeshu</keyname></author><author><keyname>Patel</keyname><forenames>Sanjeev</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>Adaptive Mean Queue Size and Its Rate of Change: Queue Management with
  Random Dropping</title><categories>cs.NI</categories><comments>17 pages, 19 figures, and submitted to Telecommunication Systems,
  Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Random early detection (RED) active queue management (AQM) scheme uses
the average queue size to calculate the dropping probability in terms of
minimum and maximum thresholds. The effect of heavy load enhances the frequency
of crossing the maximum threshold value resulting in frequent dropping of the
packets. An adaptive queue management with random dropping (AQMRD) algorithm is
proposed which incorporates information not just about the average queue size
but also the rate of change of the same. Introducing an adaptively changing
threshold level that falls in between lower and upper thresholds, our algorithm
demonstrates that these additional features significantly improve the system
performance in terms of throughput, average queue size, utilization and queuing
delay in relation to the existing AQM algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02244</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02244</id><created>2016-02-06</created><authors><author><keyname>Yokota</keyname><forenames>Rio</forenames></author><author><keyname>Ibeid</keyname><forenames>Huda</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>Fast Multipole Method as a Matrix-Free Hierarchical Low-Rank
  Approximation</title><categories>cs.NA</categories><comments>19 pages, 6 figures</comments><msc-class>65Y20, 68Q25</msc-class><acm-class>D.1.3; G.1.0; G.1.2; G.1.3; G.1.4; G.1.8; G.1.9; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a large increase in the amount of work on hierarchical
low-rank approximation methods, where the interest is shared by multiple
communities that previously did not intersect. This objective of this article
is two-fold; to provide a thorough review of the recent advancements in this
field from both analytical and algebraic perspectives, and to present a
comparative benchmark of two highly optimized implementations of contrasting
methods for some simple yet representative test cases. We categorize the recent
advances in this field from the perspective of compute-memory tradeoff, which
has not been considered in much detail in this area. Benchmark tests reveal
that there is a large difference in the memory consumption and performance
between the different methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02249</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02249</id><created>2016-02-06</created><authors><author><keyname>Chougule</keyname><forenames>P. P.</forenames></author><author><keyname>Sen</keyname><forenames>B.</forenames></author><author><keyname>Mukherjee</keyname><forenames>R.</forenames></author><author><keyname>Karade</keyname><forenames>V. C.</forenames></author><author><keyname>Patil</keyname><forenames>P. S.</forenames></author><author><keyname>Dongale</keyname><forenames>T. D.</forenames></author><author><keyname>Kamat</keyname><forenames>R. K.</forenames></author></authors><title>A Processing In-Memory Realization Using QCA: Proposal and
  Implementation</title><categories>cs.ET</categories><comments>4 pages, 3 figures, 3 tables, 1 equation</comments><msc-class>81P45</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Processing in Memory (PIM) is a computing paradigm that promises enormous
gain in processing speed by eradicating latencies in the typical von Neumann
architecture. It has gained popularity owing to its throughput by embedding
storage and computation of data in a single unit. We portray implementation of
Akers array architecture endowed with PIM computation using Quantum-dot
Cellular Automata (QCA). We present the proof of concept of PIM with its
realization in the QCA designer paradigm. We illustrate implementation of Ex-OR
gate with the help of QCA based Akers Array and put forth many interesting
potential possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02250</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02250</id><created>2016-02-06</created><updated>2016-02-10</updated><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Tsai</keyname><forenames>Hong-Cheng</forenames></author></authors><title>On the Limits of Coexisting Coverage and Capacity in Multi-RAT
  Heterogeneous Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>30 pages, 6 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper devises a general modeling and analyzing framework for a
heterogeneous wireless network (HetNet) in which several wireless subnetworks
coexist and use multiple radio access technologies (multi-RATs). The coexisting
coverage and network capacity in such a multi-RAT HetNet are hardly
investigated in prior works. To characterize the coexisting interactions in a
multi-RAT HetNet, in this paper we consider a HetNet consisting of K-tier APs
and two different RATs, RAT-L and RAT-U, are adopted in the HetNet. RAT-L is
adopted by the access points (APs) in the first K-1 tiers and APs in the Kth
tier only use RAT-U. Both noncrossing-RAT and crossing-RAT user association
scenarios are considered. In each scenario, the void probability and channel
access probability of the APs in each tier are first found and then the tight
lower bounds and their lowest limits on the proposed coexisting coverage and
network capacity are derived. We show that multi-RAT networks in general can
achieve higher link coverage and capacity by using opportunistic CSMA/CA that
avoids/alleviates severe interfering between all coexisting APs. Also,
crossing-RAT user association is shown to achieve much higher coexisting
coverage and network capacity than noncrossing-RAT user association. Finally,
numerical simulations for the LTE-U and WiFi networks coexisting in the HetNet
validate our findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02255</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02255</id><created>2016-02-06</created><updated>2016-02-15</updated><authors><author><keyname>Jiang</keyname><forenames>Qing-Yuan</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author></authors><title>Deep Cross-Modal Hashing</title><categories>cs.IR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its low storage cost and fast query speed, cross-modal hashing (CMH)
has been widely used for similarity search in multimedia retrieval
applications. However, almost all existing CMH methods are based on
hand-crafted features which might not be optimally compatible with the
hash-code learning procedure. As a result, existing CMH methods with
handcrafted features may not achieve satisfactory performance. In this paper,
we propose a novel cross-modal hashing method, called deep crossmodal hashing
(DCMH), by integrating feature learning and hash-code learning into the same
framework. DCMH is an end-to-end learning framework with deep neural networks,
one for each modality, to perform feature learning from scratch. Experiments on
two real datasets with text-image modalities show that DCMH can outperform
other baselines to achieve the state-of-the-art performance in cross-modal
retrieval applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02256</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02256</id><created>2016-02-06</created><authors><author><keyname>Hayashi</keyname><forenames>Kohei</forenames></author><author><keyname>Konishi</keyname><forenames>Takuya</forenames></author><author><keyname>Kawamoto</keyname><forenames>Tatsuro</forenames></author></authors><title>A Tractable Fully Bayesian Method for the Stochastic Block Model</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model (SBM) is a generative model revealing macroscopic
structures in graphs. Bayesian methods are used for (i) cluster assignment
inference and (ii) model selection for the number of clusters. In this paper,
we study the behavior of Bayesian inference in the SBM in the large sample
limit. Combining variational approximation and Laplace's method, a consistent
criterion of the fully marginalized log-likelihood is established. Based on
that, we derive a tractable algorithm that solves tasks (i) and (ii)
concurrently, obviating the need for an outer loop to check all model
candidates. Our empirical and theoretical results demonstrate that our method
is scalable in computation, accurate in approximation, and concise in model
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02261</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02261</id><created>2016-02-06</created><authors><author><keyname>Nogueira</keyname><forenames>Rodrigo</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author></authors><title>WebNav: A New Large-Scale Task for Natural Language based Sequential
  Decision Making</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a goal-driven web navigation as a benchmark task for evaluating an
agent with abilities to understand natural language and plan on partially
observed environments. In this challenging task, an agent navigates through a
web site, which is represented as a graph consisting of web pages as nodes and
hyperlinks as directed edges, to find a web page in which a query appears. The
agent is required to have sophisticated high-level reasoning based on natural
languages and efficient sequential decision making capability to succeed. We
release a software tool, called WebNav, that automatically transforms a website
into this goal-driven web navigation task, and as an example, we make WikiNav,
a dataset constructed from the English Wikipedia containing approximately 5
million articles and more than 12 million queries for training. We evaluate two
different agents based on neural networks on the WikiNav and provide the human
performance. Our results show the difficulty of the task for both humans and
machines. With this benchmark, we expect faster progress in developing
artificial agents with natural language understanding and planning skills.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02262</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02262</id><created>2016-02-06</created><authors><author><keyname>Li</keyname><forenames>Yuanzhi</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>Recovery guarantee of weighted low-rank approximation via alternating
  minimization</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require recovering a ground truth low-rank matrix from
noisy observations of the entries. In practice, this is typically formulated as
weighted low-rank approximation problem and solved using non-convex
optimization heuristics such as alternating minimization. Such non-convex
techniques have little guarantees. Even worse, weighted low-rank approximation
is NP-hard for even the most simple case when the ground truth is a rank-1
matrix.
  In this paper, we provide provable recovery guarantee in polynomial time for
a natural class of matrices and weights. In particular, we bound the spectral
norm of the difference between the recovered matrix and the ground truth, by
the spectral norm of the weighted noise plus an additive error term that
decreases exponentially with the number of rounds of alternating minimization.
This provides the first theoretical result for weighted low-rank approximation
via alternating minimization with non-binary deterministic weights. It is a
significant generalization of the results for matrix completion, the special
case with binary weights, since our assumptions are similar or weaker than
those made in existing works.
  The key technical challenge is that under non-binary deterministic weights,
naive alternating minimization steps will destroy the incoherency and spectral
properties of the intermediate solution, which are needed for making progress
towards the ground truth. One of our key technical contributions is a whitening
step that maintains these properties of the intermediate solution after each
round, which may be applied to alternating minimization for other problems and
thus is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02263</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02263</id><created>2016-02-06</created><authors><author><keyname>Tillmann</keyname><forenames>Andreas M.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Mairal</keyname><forenames>Julien</forenames></author></authors><title>DOLPHIn - Dictionary Learning for Phase Retrieval</title><categories>math.OC cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm to learn a dictionary for reconstructing and
sparsely encoding signals from measurements without phase. Specifically, we
consider the task of estimating a two-dimensional image from squared-magnitude
measurements of a complex-valued linear transformation of the original image.
Several recent phase retrieval algorithms exploit underlying sparsity of the
unknown signal in order to improve recovery performance. In this work, we
consider such a sparse signal prior in the context of phase retrieval, when the
sparsifying dictionary is not known in advance. Our algorithm jointly
reconstructs the unknown signal - possibly corrupted by noise - and learns a
dictionary such that each patch of the estimated image can be sparsely
represented. Numerical experiments demonstrate that our approach can obtain
significantly better reconstructions for phase retrieval problems with noise
than methods that cannot exploit such &quot;hidden&quot; sparsity. Moreover, on the
theoretical side, we provide a convergence result for our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02265</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02265</id><created>2016-02-06</created><authors><author><keyname>Sossan</keyname><forenames>Fabrizio</forenames></author><author><keyname>Namor</keyname><forenames>Emil</forenames></author><author><keyname>Cherkaoui</keyname><forenames>Rachid</forenames></author><author><keyname>Paolone</keyname><forenames>Mario</forenames></author></authors><title>Achieving the Dispatchability of Distribution Feeders through Prosumers
  Data Driven Forecasting and Model Predictive Control of Electrochemical
  Storage</title><categories>cs.SY</categories><comments>Submitted for publication, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and experimentally validate a control strategy to dispatch the
operation of a distribution feeder interfacing heterogeneous prosumers by using
a grid-connected battery energy storage system (BESS) as a controllable element
coupled with a minimally invasive monitoring infrastructure. It consists in a
two-stage procedure: day-ahead dispatch planning, where the feeder 5-minute
average power consumption trajectory for the next day of operation (called
\emph{dispatch plan}) is determined, and intra-day/real-time operation, where
the mismatch with respect to the \emph{dispatch plan} is corrected by applying
receding horizon model predictive control (MPC) to decide the BESS
charging/discharging profile while accounting for operational constraints. The
consumption forecast necessary to compute the \emph{dispatch plan} and the
battery model for the MPC algorithm are built by applying adaptive data driven
methodologies. The discussed control framework currently operates on a daily
basis to dispatch the operation of a 20~kV feeder of the EPFL university campus
using a 750~kW/500~kWh lithium titanate BESS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02268</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02268</id><created>2016-02-06</created><authors><author><keyname>Ferrolho</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Alaluna</keyname><forenames>Max</forenames></author><author><keyname>Neves</keyname><forenames>Nuno</forenames></author><author><keyname>Ramos</keyname><forenames>Fernando M. V.</forenames></author></authors><title>Secure and Dependable Virtual Network Embedding</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental problems in network virtualization is Virtual Network
Embedding (VNE). The VNE problem deals with finding an effective mapping of the
virtual nodes &amp; links onto the substrate network. The recent advances in
network virtualization gave cloud operators the ability to extend their cloud
computing offerings with virtual networks. This trend, jointly with the
increasing evidence of incidents in cloud facilities demonstrate that security
and dependability is becoming a critical factor that should be considered by
VNE algorithms. In this abstract we propose a VNE solution that considers
security and dependability as first class citizens. The resiliency properties
of our solution are enhanced by assuming a multiple cloud provider model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02282</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02282</id><created>2016-02-06</created><authors><author><keyname>S&#xf8;nderby</keyname><forenames>Casper Kaae</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Maal&#xf8;e</keyname><forenames>Lars</forenames></author><author><keyname>S&#xf8;nderby</keyname><forenames>S&#xf8;ren Kaae</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>How to Train Deep Variational Autoencoders and Probabilistic Ladder
  Networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variational autoencoders are a powerful framework for unsupervised learning.
However, previous work has been restricted to shallow models with one or two
layers of fully factorized stochastic latent variables, limiting the
flexibility of the latent representation. We propose three advances in training
algorithms of variational autoencoders, for the first time allowing to train
deep models of up to five stochastic layers, (1) using a structure similar to
the Ladder network as the inference model, (2) warm-up period to support
stochastic units staying active in early training, and (3) use of batch
normalization. Using these improvements we show state-of-the-art log-likelihood
results for generative modeling on several benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02283</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02283</id><created>2016-02-06</created><authors><author><keyname>Csiba</keyname><forenames>Dominik</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author></authors><title>Importance Sampling for Minibatches</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minibatching is a very well studied and highly popular technique in
supervised learning, used by practitioners due to its ability to accelerate
training through better utilization of parallel processing power and reduction
of stochastic variance. Another popular technique is importance sampling -- a
strategy for preferential sampling of more important examples also capable of
accelerating the training process. However, despite considerable effort by the
community in these areas, and due to the inherent technical difficulty of the
problem, there is no existing work combining the power of importance sampling
with the strength of minibatching. In this paper we propose the first {\em
importance sampling for minibatches} and give simple and rigorous complexity
analysis of its performance. We illustrate on synthetic problems that for
training data of certain properties, our sampling can lead to several orders of
magnitude improvement in training time. We then test the new sampling on
several popular datasets, and show that the improvement can reach an order of
magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02285</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02285</id><created>2016-02-06</created><authors><author><keyname>Shaham</keyname><forenames>Uri</forenames></author><author><keyname>Cheng</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Dror</keyname><forenames>Omer</forenames></author><author><keyname>Jaffe</keyname><forenames>Ariel</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Chang</keyname><forenames>Joseph</forenames></author><author><keyname>Kluger</keyname><forenames>Yuval</forenames></author></authors><title>A Deep Learning Approach to Unsupervised Ensemble Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how deep learning methods can be applied in the context of
crowdsourcing and unsupervised ensemble learning. First, we prove that the
popular model of Dawid and Skene, which assumes that all classifiers are
conditionally independent, is {\em equivalent} to a Restricted Boltzmann
Machine (RBM) with a single hidden node. Hence, under this model, the posterior
probabilities of the true labels can be instead estimated via a trained RBM.
Next, to address the more general case, where classifiers may strongly violate
the conditional independence assumption, we propose to apply RBM-based Deep
Neural Net (DNN). Experimental results on various simulated and real-world
datasets demonstrate that our proposed DNN approach outperforms other
state-of-the-art methods, in particular when the data violates the conditional
independence assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02293</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02293</id><created>2016-02-06</created><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Neiman</keyname><forenames>Ofer</forenames></author></authors><title>On Efficient Distributed Construction of Near Optimal Routing Schemes</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a distributed network represented by a weighted undirected graph
$G=(V,E)$ on $n$ vertices, and a parameter $k$, we devise a distributed
algorithm that computes a routing scheme in $(n^{1/2+1/k}+D)\cdot n^{o(1)}$
rounds, where $D$ is the hop-diameter of the network. The running time matches
the lower bound of $\tilde{\Omega}(n^{1/2}+D)$ rounds (which holds for any
scheme with polynomial stretch), up to lower order terms. The routing tables
are of size $\tilde{O}(n^{1/k})$, the labels are of size $O(k\log^2n)$, and
every packet is routed on a path suffering stretch at most $4k-5+o(1)$. Our
construction nearly matches the state-of-the-art for routing schemes built in a
centralized sequential manner. The previous best algorithms for building
routing tables in a distributed small messages model were by \cite[STOC
2013]{LP13} and \cite[PODC 2015]{LP15}. The former has similar properties but
suffers from substantially larger routing tables of size $O(n^{1/2+1/k})$,
while the latter has sub-optimal running time of
$\tilde{O}(\min\{(nD)^{1/2}\cdot n^{1/k},n^{2/3+2/(3k)}+D\})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02294</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02294</id><created>2016-02-06</created><authors><author><keyname>Khezeli</keyname><forenames>Kia</forenames></author><author><keyname>Chen</keyname><forenames>Jun</forenames></author></authors><title>A Source-Channel Separation Theorem with Application to the Source
  Broadcast Problem</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A converse method is developed for the source broadcast problem.
Specifically, it is shown that the separation architecture is optimal for a
variant of the source broadcast problem and the associated source-channel
separation theorem can be leveraged, via a reduction argument, to establish a
necessary condition for the original problem, which unifies several existing
results in the literature. Somewhat surprisingly, this method, albeit based on
the source-channel separation theorem, can be used to prove the optimality of
non-separation based schemes and determine the performance limits in certain
scenarios where the separation architecture is suboptimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02296</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02296</id><created>2016-02-06</created><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Choi</keyname><forenames>Sou-Cheng T.</forenames></author><author><keyname>Niemeyer</keyname><forenames>Kyle E.</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Frank</forenames></author><author><keyname>Gunter</keyname><forenames>Dan</forenames></author><author><keyname>Idaszak</keyname><forenames>Ray</forenames></author><author><keyname>Brandt</keyname><forenames>Steven R.</forenames></author><author><keyname>Miller</keyname><forenames>Mark A.</forenames></author><author><keyname>Gesing</keyname><forenames>Sandra</forenames></author><author><keyname>Jones</keyname><forenames>Nick D.</forenames></author><author><keyname>Weber</keyname><forenames>Nic</forenames></author><author><keyname>Marru</keyname><forenames>Suresh</forenames></author><author><keyname>Allen</keyname><forenames>Gabrielle</forenames></author><author><keyname>Penzenstadler</keyname><forenames>Birgit</forenames></author><author><keyname>Venters</keyname><forenames>Colin C.</forenames></author><author><keyname>Davis</keyname><forenames>Ethan</forenames></author><author><keyname>Hwang</keyname><forenames>Lorraine</forenames></author><author><keyname>Todorov</keyname><forenames>Ilian</forenames></author><author><keyname>Patra</keyname><forenames>Abani</forenames></author><author><keyname>de Val-Borro</keyname><forenames>Miguel</forenames></author></authors><title>Report on the Third Workshop on Sustainable Software for Science:
  Practice and Experiences (WSSSPE3)</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This report records and discusses the Third Workshop on Sustainable Software
for Science: Practice and Experiences (WSSSPE3). The report includes a
description of the keynote presentation of the workshop, which served as an
overview of sustainable scientific software. It also summarizes a set of
lightning talks in which speakers highlighted to-the-point lessons and
challenges pertaining to sustaining scientific software. The final and main
contribution of the report is a summary of the discussions, future steps, and
future organization for a set of self-organized working groups on topics
including developing pathways to funding scientific software; constructing
useful common metrics for crediting software stakeholders; identifying
principles for sustainable software engineering design; reaching out to
research software organizations around the world; and building communities for
software sustainability. For each group, we include a point of contact and a
landing page that can be used by those who want to join that group's future
activities. The main challenge left by the workshop is to see if the groups
will execute these activities that they have scheduled, and how the WSSSPE
community can encourage this to happen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02311</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02311</id><created>2016-02-06</created><authors><author><keyname>Li</keyname><forenames>Yingzhen</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Variational Inference with R\'enyi Divergence</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the variational R\'enyi bound (VR) that extends traditional
variational inference to R\'enyi's alpha-divergences. This new family of
variational lower-bounds unifies a number of existing variational methods, and
enables a smooth interpolation from the evidence lower-bound to the log
(marginal) likelihood that is controlled by the value of alpha. The
reparameterization trick, Monte Carlo estimation and stochastic optimisation
methods are deployed to obtain a unified implementation for the VR bound
optimisation. We further consider negative alpha values and propose a novel
variational inference method as a new special case in the proposed framework.
Experiments on variational auto-encoders and Bayesian neural networks
demonstrate the wide applicability of the VR bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02332</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02332</id><created>2016-02-06</created><authors><author><keyname>Puurula</keyname><forenames>Antti</forenames></author></authors><title>Scalable Text Mining with Sparse Generative Models</title><categories>cs.IR cs.AI cs.CL</categories><comments>PhD Thesis, Computer Science, University of Waikato, 2016</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The information age has brought a deluge of data. Much of this is in text
form, insurmountable in scope for humans and incomprehensible in structure for
computers. Text mining is an expanding field of research that seeks to utilize
the information contained in vast document collections. General data mining
methods based on machine learning face challenges with the scale of text data,
posing a need for scalable text mining methods.
  This thesis proposes a solution to scalable text mining: generative models
combined with sparse computation. A unifying formalization for generative text
models is defined, bringing together research traditions that have used
formally equivalent models, but ignored parallel developments. This framework
allows the use of methods developed in different processing tasks such as
retrieval and classification, yielding effective solutions across different
text mining tasks. Sparse computation using inverted indices is proposed for
inference on probabilistic models. This reduces the computational complexity of
the common text mining operations according to sparsity, yielding probabilistic
models with the scalability of modern search engines.
  The proposed combination provides sparse generative models: a solution for
text mining that is general, effective, and scalable. Extensive experimentation
on text classification and ranked retrieval datasets are conducted, showing
that the proposed solution matches or outperforms the leading task-specific
methods in effectiveness, with a order of magnitude decrease in classification
times for Wikipedia article categorization with a million classes. The
developed methods were further applied in two 2014 Kaggle data mining prize
competitions with over a hundred competing teams, earning first and second
places.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02334</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02334</id><created>2016-02-06</created><authors><author><keyname>Bahmani</keyname><forenames>Zeinab</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author><author><keyname>Vasiloglou</keyname><forenames>Nikolaos</forenames></author></authors><title>ERBlox: Combining Matching Dependencies with Machine Learning for Entity
  Resolution</title><categories>cs.DB cs.AI cs.LG</categories><comments>Extended version of arXiv:1508.06013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity resolution (ER), an important and common data cleaning problem, is
about detecting data duplicate representations for the same external entities,
and merging them into single representations. Relatively recently, declarative
rules called &quot;matching dependencies&quot; (MDs) have been proposed for specifying
similarity conditions under which attribute values in database records are
merged. In this work we show the process and the benefits of integrating four
components of ER: (a) Building a classifier for duplicate/non-duplicate record
pairs built using machine learning (ML) techniques; (b) Use of MDs for
supporting the blocking phase of ML; (c) Record merging on the basis of the
classifier results; and (d) The use of the declarative language &quot;LogiQL&quot; -an
extended form of Datalog supported by the &quot;LogicBlox&quot; platform- for all
activities related to data processing, and the specification and enforcement of
MDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02338</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02338</id><created>2016-02-06</created><updated>2016-02-20</updated><authors><author><keyname>Toscano-Palmerin</keyname><forenames>Saul</forenames></author><author><keyname>Frazier</keyname><forenames>Peter I.</forenames></author></authors><title>Stratified Bayesian Optimization</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider derivative-free black-box global optimization of expensive noisy
functions, when most of the randomness in the objective is produced by a few
influential scalar random inputs. We present a new Bayesian global optimization
algorithm, called Stratified Bayesian Optimization (SBO), which uses this
strong dependence to improve performance. Our algorithm is similar in spirit to
stratification, a technique from simulation, which uses strong dependence on a
categorical representation of the random input to reduce variance. We
demonstrate in numerical experiments that SBO outperforms state-of-the-art
Bayesian optimization benchmarks that do not leverage this dependence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02339</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02339</id><created>2016-02-06</created><authors><author><keyname>Grozev</keyname><forenames>Nikolay</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Dynamic Selection of Virtual Machines for Application Servers in Cloud
  Environments</title><categories>cs.DC</categories><report-no>CLOUDS-TR-2016-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autoscaling is a hallmark of cloud computing as it allows flexible
just-in-time allocation and release of computational resources in response to
dynamic and often unpredictable workloads. This is especially important for web
applications whose workload is time dependent and prone to flash crowds. Most
of them follow the 3-tier architectural pattern, and are divided into
presentation, application/domain and data layers. In this work we focus on the
application layer. Reactive autoscaling policies of the type &quot;Instantiate a new
Virtual Machine (VM) when the average server CPU utilisation reaches X%&quot; have
been used successfully since the dawn of cloud computing. But which VM type is
the most suitable for the specific application at the moment remains an open
question. In this work, we propose an approach for dynamic VM type selection.
It uses a combination of online machine learning techniques, works in real time
and adapts to changes in the users' workload patterns, application changes as
well as middleware upgrades and reconfigurations. We have developed a
prototype, which we tested with the CloudStone benchmark deployed on AWS EC2.
Results show that our method quickly adapts to workload changes and reduces the
total cost compared to the industry standard approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02343</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02343</id><created>2016-02-07</created><updated>2016-02-22</updated><authors><author><keyname>Torres</keyname><forenames>Carlos</forenames></author><author><keyname>Fragoso</keyname><forenames>Victor</forenames></author><author><keyname>Hammond</keyname><forenames>Scott D.</forenames></author><author><keyname>Fried</keyname><forenames>Jeffrey C.</forenames></author><author><keyname>Manjunath</keyname><forenames>B. S.</forenames></author></authors><title>Eye-CU: Sleep Pose Classification for Healthcare using Multimodal
  Multiview Data</title><categories>cs.CV</categories><comments>Ten-page manuscript including references and ten figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manual analysis of body poses of bed-ridden patients requires staff to
continuously track and record patient poses. Two limitations in the
dissemination of pose-related therapies are scarce human resources and
unreliable automated systems. This work addresses these issues by introducing a
new method and a new system for robust automated classification of sleep poses
in an Intensive Care Unit (ICU) environment. The new method,
coupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM)
data and finds the set of modality trust values that minimizes the difference
between expected and estimated labels. The new system, Eye-CU, is an affordable
multi-sensor modular system for unobtrusive data collection and analysis in
healthcare. Experimental results indicate that the performance of cc-LS matches
the performance of existing methods in ideal scenarios. This method outperforms
the latest techniques in challenging scenarios by 13% for those with poor
illumination and by 70% for those with both poor illumination and occlusions.
Results also show that a reduced Eye-CU configuration can classify poses
without pressure information with only a slight drop in its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02348</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02348</id><created>2016-02-07</created><updated>2016-02-09</updated><authors><author><keyname>Ivanova</keyname><forenames>Inga</forenames></author><author><keyname>Strand</keyname><forenames>Oivind</forenames></author><author><keyname>Kushnir</keyname><forenames>Duncan</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Economic and Technological Complexity: A Model Study of Indicators of
  Knowledge-based Innovation Systems</title><categories>q-fin.EC cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidalgo &amp; Hausmann's (2009) Economic Complexity Index (ECI) measures the
complexity of national economies in terms of product groups. Analogously to
ECI, we develop the Patent Complexity Index (PatCI) on the basis of a matrix of
nations versus patent classes. Using linear algebra, the three dimensions
countries, product groups, and patent classes can be combined into an
integrated (&quot;Triple Helix&quot;) measure of complexity (THCI). We measure ECI,
PatCI, and THCI during the period 2000-2014 for the 34 OECD member states, the
BRICS countries, and a group of emerging economies (Argentina, Hong Kong,
Indonesia, Malaysia, Romania, and Singapore). The positive correlation between
ECI and average income claimed as an argument for the predictive value of ECI
cannot be confirmed using our data. The three complexity indicators are
significantly correlated between themselves, yet each captures another aspect
of the complexity. THCI adds the trilateral interaction terms among the three
bilateral interactions, and can thus be expected to capture the extent of
systems integration between the global dynamics of markets (ECI) and
technologies (PatCI) in each national system of innovation. Of the world's
major economies, Japan scores highest on all three indicators, while China has
been increasingly successful in combining economic and technological
complexity. Our empirical results raise questions about the interpretation and
empirical fruitfulness of the complexity approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02350</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02350</id><created>2016-02-07</created><authors><author><keyname>Gonen</keyname><forenames>Alon</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Solving Ridge Regression using Sketched Preconditioned SVRG</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel preconditioning method for ridge regression, based on
recent linear sketching methods. By equipping Stochastic Variance Reduced
Gradient (SVRG) with this preconditioning process, we obtain a significant
speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02355</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02355</id><created>2016-02-07</created><updated>2016-02-09</updated><authors><author><keyname>Pedregosa</keyname><forenames>Fabian</forenames></author></authors><title>Hyperparameter optimization with approximate gradient</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most models in machine learning contain at least one hyperparameter to
control for model complexity. Choosing an appropriate set of hyperparameters is
both crucial in terms of model accuracy and computationally challenging. In
this work we propose an algorithm for the optimization of continuous
hyperparameters using inexact gradient information. An advantage of this method
is that hyperparameters can be updated before model parameters have fully
converged. We also give sufficient conditions for the global convergence of
this method, based on regularity conditions of the involved functions and
summability of errors. Finally, we validate the empirical performance of this
method on the estimation of regularization constants of L2-regularized logistic
regression and kernel Ridge regression. Empirical benchmarks indicate that our
approach is highly competitive with respect to state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02358</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02358</id><created>2016-02-07</created><updated>2016-02-15</updated><authors><author><keyname>Zhu</keyname><forenames>Haohan</forenames></author><author><keyname>Meng</keyname><forenames>Xianrui</forenames></author><author><keyname>Kollios</keyname><forenames>George</forenames></author></authors><title>NED: An Inter-Graph Node Metric Based On Edit Distance</title><categories>cs.DB cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node similarity is a fundamental problem in graph analytics. However, node
similarity between nodes in different graphs (inter-graph nodes) has not
received a lot of attention yet. The inter-graph node similarity is important
in learning a new graph based on the knowledge of an existing graph (transfer
learning on graphs) and has applications in biological, communication, and
social networks. In this paper, we propose a novel distance function for
measuring inter-graph node similarity with edit distance, called NED. In NED,
two nodes are compared according to their local neighborhood structures which
are represented as unordered k-adjacent trees, without relying on labels or
other assumptions. Since the computation problem of tree edit distance on
unordered trees is NP-Complete, we propose a modified tree edit distance,
called TED*, for comparing neighborhood trees. TED* is a metric distance, as
the original tree edit distance, but more importantly, TED* is polynomially
computable. As a metric distance, NED admits efficient indexing, provides
interpretable results, and shows to perform better than existing approaches on
a number of data analysis tasks, including graph de-anonymization. Finally, the
efficiency and effectiveness of NED are empirically demonstrated using
real-world graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02362</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02362</id><created>2016-02-07</created><authors><author><keyname>Sergeev</keyname><forenames>Igor S.</forenames></author></authors><title>On the circuit complexity of the standard and the Karatsuba methods of
  multiplying integers</title><categories>cs.DS</categories><comments>6 pages, published in Russian in Proc. XXII Conf. &quot;Information means
  and technology&quot; (Moscow, November 18--20, 2014). Vol. 3. Moscow, MPEI, 2014,
  180--187</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide accurate upper bounds on the Boolean circuit complexity of the
standard and the Karatsuba methods of integer multiplication
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02366</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02366</id><created>2016-02-07</created><authors><author><keyname>Yang</keyname><forenames>Hyun Jong</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author></authors><title>On the Degrees-of-Freedom of the Large-Scale Interfering Two-Way Relay
  Network</title><categories>cs.IT cs.NI math.IT</categories><comments>18 pages, 5 figures, To appear in IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achievable degrees-of-freedom (DoF) of the large-scale interfering two-way
relay network is investigated. The network consists of $K$ pairs of
communication nodes (CNs) and $N$ relay nodes (RNs). It is assumed that $K\ll
N$ and each pair of CNs communicates with each other through one of the $N$
relay nodes without a direct link between them. Interference among RNs is also
considered. Assuming local channel state information (CSI) at each RN, a
distributed and opportunistic RN selection technique is proposed for the
following three promising relaying protocols: amplify--forward,
decode--forward, and compute--forward. As a main result, the asymptotically
achievable DoF is characterized as $N$ increases for the three relaying
protocols. In particular, a sufficient condition on $N$ required to achieve the
certain DoF of the network is analyzed. Through extensive simulations, it is
shown that the proposed RN selection techniques outperform conventional schemes
in terms of achievable rate even in practical communication scenarios. Note
that the proposed technique operates with a distributed manner and requires
only local CSI, leading to easy implementation for practical wireless systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02367</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02367</id><created>2016-02-07</created><authors><author><keyname>Chouvardas</keyname><forenames>Symeon</forenames></author><author><keyname>Draief</keyname><forenames>Moez</forenames></author></authors><title>A Diffusion Kernel LMS algorithm for nonlinear adaptive networks</title><categories>cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a distributed algorithm for nonlinear adaptive learning.
In particular, a set of nodes obtain measurements, sequentially one per time
step, which are related via a nonlinear function; their goal is to collectively
minimize a cost function by employing a diffusion based Kernel Least Mean
Squares (KLMS). The algorithm follows the Adapt Then Combine mode of
cooperation. Moreover, the theoretical properties of the algorithm are studied
and it is proved that under certain assumptions the algorithm suffers a no
regret bound. Finally, comparative experiments verify that the proposed scheme
outperforms other variants of the LMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02373</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02373</id><created>2016-02-07</created><authors><author><keyname>Johnson</keyname><forenames>Rie</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Supervised and Semi-Supervised Text Categorization using One-Hot LSTM
  for Region Embeddings</title><categories>stat.ML cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-hot CNN (convolutional neural network) has been shown to be effective for
text categorization in our previous work. We view it as a special case of a
general framework which jointly trains a linear model with a non-linear feature
generator consisting of `text region embedding + pooling'. Under this
framework, we explore a more sophisticated region embedding method using Long
Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly
large) sizes, whereas the region size needs to be fixed in a CNN. We seek the
best use of LSTM for the purpose in the supervised and semi-supervised
settings, starting with the idea of one-hot LSTM, which eliminates the
customarily used word embedding layer. Our results indicate that on this task,
embeddings of text regions, which can convey higher concepts than single words
in isolation, are more useful than word embeddings. We report performances
exceeding the previous best results on four benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02377</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02377</id><created>2016-02-07</created><authors><author><keyname>Tan</keyname><forenames>Yong</forenames></author></authors><title>Find an Optimal Path in Static System and Dynamical System within
  Polynomial Runtime</title><categories>cs.DS cs.AI cs.DM cs.RO math.DS</categories><comments>27 pages, 9720 words,10 figures,5 trials</comments><msc-class>37HXX, 70Q05, 70G60, 93C85, 91B06,</msc-class><acm-class>F.2.2; G.2.2; I.2.9; J.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an ancient problem that in a static or dynamical system, sought an
optimal path, which the context always means within an extremal condition. In
fact, through those discussions about this theme, we established a universal
essential calculated model to serve for these complex systems. Meanwhile we
utilize the sample space to character the system. These contents in this paper
would involve in several major areas including the geometry, probability, graph
algorithms and some prior approaches, which stands the ultimately subtle linear
algorithm to solve this class problem. Along with our progress, our discussion
would demonstrate more general meaning and robust character, which provides
clear ideas or notion to support our concrete applications, who work in a more
popular complex system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02383</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02383</id><created>2016-02-07</created><authors><author><keyname>Whitney</keyname><forenames>William</forenames></author></authors><title>Disentangled Representations in Neural Models</title><categories>cs.LG cs.NE</categories><comments>MIT Master's of Engineering thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation learning is the foundation for the recent success of neural
network models. However, the distributed representations generated by neural
networks are far from ideal. Due to their highly entangled nature, they are di
cult to reuse and interpret, and they do a poor job of capturing the sparsity
which is present in real- world transformations. In this paper, I describe
methods for learning disentangled representations in the two domains of
graphics and computation. These methods allow neural methods to learn
representations which are easy to interpret and reuse, yet they incur little or
no penalty to performance. In the Graphics section, I demonstrate the ability
of these methods to infer the generating parameters of images and rerender
those images under novel conditions. In the Computation section, I describe a
model which is able to factorize a multitask learning problem into subtasks and
which experiences no catastrophic forgetting. Together these techniques provide
the tools to design a wide range of models that learn disentangled
representations and better model the factors of variation in the real world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02384</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02384</id><created>2016-02-07</created><authors><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author></authors><title>The benefit of a 1-bit jump-start, and the necessity of stochastic
  encoding, in jamming channels</title><categories>cs.IT cs.CR math.IT</categories><comments>21 pages, 4 figures, extended draft of submission to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communicating a message $m$ in the presence of a
malicious jamming adversary (Calvin), who can erase an arbitrary set of up to
$pn$ bits, out of $n$ transmitted bits $(x_1,\ldots,x_n)$. The capacity of such
a channel when Calvin is exactly causal, i.e. Calvin's decision of whether or
not to erase bit $x_i$ depends on his observations $(x_1,\ldots,x_i)$ was
recently characterized to be $1-2p$. In this work we show two (perhaps)
surprising phenomena. Firstly, we demonstrate via a novel code construction
that if Calvin is delayed by even a single bit, i.e. Calvin's decision of
whether or not to erase bit $x_i$ depends only on $(x_1,\ldots,x_{i-1})$ (and
is independent of the &quot;current bit&quot; $x_i$) then the capacity increases to $1-p$
when the encoder is allowed to be stochastic. Secondly, we show via a novel
jamming strategy for Calvin that, in the single-bit-delay setting, if the
encoding is deterministic (i.e. the transmitted codeword is a deterministic
function of the message $m$) then no rate asymptotically larger than $1-2p$ is
possible with vanishing probability of error, hence stochastic encoding (using
private randomness at the encoder) is essential to achieve the capacity of
$1-p$ against a one-bit-delayed Calvin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02386</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02386</id><created>2016-02-07</created><authors><author><keyname>Tang</keyname><forenames>Qingming</forenames></author><author><keyname>Tu</keyname><forenames>Lifu</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>Network Inference by Learned Node-Specific Degree Prior</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method for network inference from partially observed edges
using a node-specific degree prior. The degree prior is derived from observed
edges in the network to be inferred, and its hyper-parameters are determined by
cross validation. Then we formulate network inference as a matrix completion
problem regularized by our degree prior. Our theoretical analysis indicates
that this prior favors a network following the learned degree distribution, and
may lead to improved network recovery error bound than previous work.
Experimental results on both simulated and real biological networks demonstrate
the superior performance of our method in various settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02387</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02387</id><created>2016-02-07</created><authors><author><keyname>Ishii</keyname><forenames>Daisuke</forenames></author><author><keyname>Yonezaki</keyname><forenames>Naoki</forenames></author><author><keyname>Goldsztejn</keyname><forenames>Alexandre</forenames></author></authors><title>Monitoring Temporal Properties using Interval Analysis</title><categories>cs.LO</categories><comments>arXiv admin note: text overlap with arXiv:1506.01762</comments><journal-ref>IEICE Trans. Fundamentals, vol. E99-A, no. 2, pp. 442-453, Feb.
  2016</journal-ref><doi>10.1587/transfun.E99.A.442</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification of temporal logic properties plays a crucial role in proving the
desired behaviors of continuous systems. In this paper, we propose an interval
method that verifies the properties described by a bounded signal temporal
logic. We relax the problem so that if the verification process cannot succeed
at the prescribed precision, it outputs an inconclusive result. The problem is
solved by an efficient and rigorous monitoring algorithm. This algorithm
performs a forward simulation of a continuous-time dynamical system, detects a
set of time intervals in which the atomic propositions hold, and validates the
property by propagating the time intervals. In each step, the continuous state
at a certain time is enclosed by an interval vector that is proven to contain a
unique solution. We experimentally demonstrate the utility of the proposed
method in formal analysis of nonlinear and complex continuous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02389</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02389</id><created>2016-02-07</created><updated>2016-02-09</updated><authors><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Zahavy</keyname><forenames>Tom</forenames></author><author><keyname>Kang</keyname><forenames>Bingyi</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Ensemble Robustness of Deep Learning Algorithms</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question why deep learning algorithms perform so well in practice has
puzzled machine learning theoreticians and practitioners alike. However, most
of well-established approaches, such as hypothesis capacity, robustness or
sparseness, have not provided complete explanations, due to the high complexity
of the deep learning algorithms and their inherent randomness. In this work, we
introduce a new approach -- ensemble robustness -- towards characterizing the
generalization performance of generic deep learning algorithms. Ensemble
robustness concerns robustness of the population of the hypotheses that may be
output by a learning algorithm. Through the lens of ensemble robustness, we
reveal that a stochastic learning algorithm can generalize well as long as its
sensitiveness to adversarial perturbation is bounded in average, or
equivalently, the performance variance of the algorithm is small. Quantifying
the ensemble robustness of various deep learning algorithms may be difficult
analytically. However, extensive simulations for seven common deep learning
algorithms for different network architectures provide supporting evidence for
our claims. In addition, as an example for utilizing ensemble robustness, we
propose a novel semi-supervised learning method that outperforms the
state-of-the-art. Furthermore, our work explains the good performance of
several published deep learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02390</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02390</id><created>2016-02-07</created><authors><author><keyname>Rajakrishnan</keyname><forenames>Shijin</forenames></author><author><keyname>S</keyname><forenames>Sundara Rajan</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod</forenames></author></authors><title>Lower Bounds for Interactive Function Computation via Wyner Common
  Information</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, accepted in NCC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of how much communication is required between collaborating
parties to compute a function of their data is of fundamental importance in the
fields of theoretical computer science and information theory. In this work,
the focus is on coming up with lower bounds on this. The information cost of a
protocol is the amount of information the protocol reveals to Alice and Bob
about each others inputs, and the information complexity of a function is the
infimum of information costs over all valid protocols. For the amortized case,
it is known that the optimal rate for the computation is equal to the
information complexity. Exactly computing this information complexity is not
straight forward however. In this work we lower bound information complexity
for independent inputs in terms of the Wyner common information of a certain
pair of random variables. We show a structural property for the optimal
auxiliary random variable of Wyner common information and exploit this to
exactly compute the Wyner common information in certain cases. The lower bound
obtained through this technique is shown to be tight for a non-trivial example
- equality (EQ) for the ternary alphabet. We also give an example to show that
the lower bound may, in general, not be tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02396</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02396</id><created>2016-02-07</created><authors><author><keyname>Bokslag</keyname><forenames>Wouter</forenames></author></authors><title>The problem of popular primes: Logjam</title><categories>cs.CR</categories><comments>9 pages, 1 figures</comments><msc-class>94A60</msc-class><acm-class>E.3; D.4.6; K.6.5</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper will discuss the Logjam attack on TLS. The Logjam attack allows,
under certain conditions, to defeat the security provided by TLS. This is done
by manipulating server and client into using weak and deprecated export grade
crypto, and subsequently breaking the Diffie-Hellman key exchange. We explore
how the attack works conceptually and how exactly TLS is vulnerable to this
attack. Also, the conditions under which the attack can be mounted are
discussed, and an estimate of the impact of the attack is presented. Lastly,
several mitigations are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02409</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02409</id><created>2016-02-07</created><authors><author><keyname>Eijkhout</keyname><forenames>Victor</forenames></author></authors><title>A mathematical formalization of data parallel operations</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We give a mathematical formalization of `generalized data parallel'
operations, a concept that covers such common scientific kernels as
matrix-vector multiplication, multi-grid coarsening, load distribution, and
many more. We show that from a compact specification such computational aspects
as MPI messages or task dependencies can be automatically derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02410</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02410</id><created>2016-02-07</created><updated>2016-02-11</updated><authors><author><keyname>Jozefowicz</keyname><forenames>Rafal</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Schuster</keyname><forenames>Mike</forenames></author><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author><author><keyname>Wu</keyname><forenames>Yonghui</forenames></author></authors><title>Exploring the Limits of Language Modeling</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore recent advances in Recurrent Neural Networks for
large scale Language Modeling, a task central to language understanding. We
extend current models to deal with two key challenges present in this task:
corpora and vocabulary sizes, and complex, long term structure of language. We
perform an exhaustive study on techniques such as character Convolutional
Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.
Our best single model significantly improves state-of-the-art perplexity from
51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20),
while an ensemble of models sets a new record by improving perplexity from 41.0
down to 23.7. We also release these models for the NLP and ML community to
study and improve upon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02412</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02412</id><created>2016-02-07</created><authors><author><keyname>Martin-Martin</keyname><forenames>Alberto</forenames></author><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayllon</keyname><forenames>Juan M.</forenames></author><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>The counting house: measuring those who count. Presence of
  Bibliometrics, Scientometrics, Informetrics, Webometrics and Altmetrics in
  the Google Scholar Citations, ResearcherID, ResearchGate, Mendeley &amp; Twitter</title><categories>cs.DL</categories><comments>60 pages, 12 tables, 35 figures</comments><report-no>EC3 Working Papers 21</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Following in the footsteps of the model of scientific communication, which
has recently gone through a metamorphosis (from the Gutenberg galaxy to the Web
galaxy), a change in the model and methods of scientific evaluation is also
taking place. A set of new scientific tools are now providing a variety of
indicators which measure all actions and interactions among scientists in the
digital space, making new aspects of scientific communication emerge. In this
work we present a method for capturing the structure of an entire scientific
community (the Bibliometrics, Scientometrics, Informetrics, Webometrics, and
Altmetrics community) and the main agents that are part of it (scientists,
documents, and sources) through the lens of Google Scholar Citations.
  Additionally, we compare these author portraits to the ones offered by other
profile or social platforms currently used by academics (ResearcherID,
ResearchGate, Mendeley, and Twitter), in order to test their degree of use,
completeness, reliability, and the validity of the information they provide. A
sample of 814 authors (researchers in Bibliometrics with a public profile
created in Google Scholar Citations was subsequently searched in the other
platforms, collecting the main indicators computed by each of them. The data
collection was carried out on September, 2015. The Spearman correlation was
applied to these indicators (a total of 31) , and a Principal Component
Analysis was carried out in order to reveal the relationships among metrics and
platforms as well as the possible existence of metric clusters
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02415</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02415</id><created>2016-02-07</created><authors><author><keyname>Poon</keyname><forenames>Clarice</forenames></author></authors><title>On Cartesian line sampling with anisotropic total variation
  regularization</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the use of the anisotropic total variation seminorm to
recover a two dimensional vector $x\in \mathbb{C}^{N\times N}$ from its partial
Fourier coefficients, sampled along Cartesian lines. We prove that if $(x_{k,j}
- x_{k-1,j})_{k,j}$ has at most $s_1$ nonzero coefficients in each column and
$(x_{k,j} - x_{k,j-1})_{k,j}$ has at most $s_2$ nonzero coefficients in each
row, then, up to multiplication by $\log$ factors, one can exactly recover $x$
by sampling along $s_1$ horizontal lines of its Fourier coefficients and along
$s_2$ vertical lines of its Fourier coefficients. Finally, unlike standard
compressed sensing estimates, the $\log$ factors involved are dependent on the
separation distance between the nonzero entries in each row/column of the
gradient of $x$ and not on $N^2$, the ambient dimension of $x$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02422</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02422</id><created>2016-02-07</created><authors><author><keyname>Arbabjolfaei</keyname><forenames>Fatemeh</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>Approximate Capacity of Index Coding for Some Classes of Graphs</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a class of graphs for which the Ramsey number $R(i,j)$ is upper bounded
by $ci^aj^b$, for some constants $a,b,$ and $c$, it is shown that the clique
covering scheme approximates the broadcast rate of every $n$-node index coding
problem in the class within a multiplicative factor of $c^{\frac{1}{a+b+1}}
n^{\frac{a+b}{a+b+1}}$ for every $n$. Using this theorem and some graph
theoretic arguments, it is demonstrated that the broadcast rate of planar
graphs, line graphs and fuzzy circular interval graphs is approximated by the
clique covering scheme within a factor of $n^{\frac{2}{3}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02426</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02426</id><created>2016-02-07</created><updated>2016-02-10</updated><authors><author><keyname>Saveski</keyname><forenames>Martin</forenames></author><author><keyname>Chu</keyname><forenames>Eric</forenames></author><author><keyname>Vosoughi</keyname><forenames>Soroush</forenames></author><author><keyname>Roy</keyname><forenames>Deb</forenames></author></authors><title>Human Atlas: A Tool for Mapping Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>WWW'16 Demonstration, WWW'16 Companion, April 11-15, 2016, Montreal,
  Quebec, Canada</comments><acm-class>H.5.2; H.3.4</acm-class><doi>10.1145/2872518.2890552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most social network analyses focus on online social networks. While these
networks encode important aspects of our lives they fail to capture many
real-world connections. Most of these connections are, in fact, public and
known to the members of the community. Mapping them is a task very suitable for
crowdsourcing: it is easily broken down in many simple and independent
subtasks. Due to the nature of social networks -- presence of highly connected
nodes and tightly knit groups -- if we allow users to map their immediate
connections and the connections between them, we will need few participants to
map most connections within a community. To this end, we built the Human Atlas,
a web-based tool for mapping social networks. To test it, we partially mapped
the social network of the MIT Media Lab. We ran a user study and invited
members of the community to use the tool. In 4.6 man-hours, 22 participants
mapped 984 connections within the lab, demonstrating the potential of the tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02432</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02432</id><created>2016-02-07</created><authors><author><keyname>Gilman</keyname><forenames>Robert H</forenames></author></authors><title>A Finitely presented group whose word problem has sampleable hard
  instances</title><categories>cs.CC math.GR</categories><comments>3 pages</comments><msc-class>68A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hard instances of natural computational problems are often elusive. In this
note we present an example of a natural decision problem, the word problem for
a certain finitely presented group, whose hard instances are easy to find. More
precisely the problem has a complexity core sampleable in linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02434</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02434</id><created>2016-02-07</created><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Screen Content Image Segmentation Using Sparse Decomposition and Total
  Variation Minimization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse decomposition has been widely used for different applications, such as
source separation, image classification, image denoising and more. This paper
presents a new algorithm for segmentation of an image into background and
foreground text and graphics using sparse decomposition and total variation
minimization. The proposed method is designed based on the assumption that the
background part of the image is smoothly varying and can be represented by a
linear combination of a few smoothly varying basis functions, while the
foreground text and graphics can be modeled with a sparse component overlaid on
the smooth background. The background and foreground are separated using a
sparse decomposition framework regularized with a few suitable regularization
terms which promotes the sparsity and connectivity of foreground pixels. This
algorithm has been tested on a dataset of images extracted from HEVC standard
test sequences for screen content coding, and is shown to have superior
performance over some prior methods, including least absolute deviation
fitting, k-means clustering based segmentation in DjVu and shape primitive
extraction and coding (SPEC) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02439</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02439</id><created>2016-02-07</created><updated>2016-02-24</updated><authors><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Repeated Matching Mechanism Design with Moral Hazard and Adverse
  Selection</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In crowdsourcing systems, the mechanism designer aims to repeatedly match a
set of tasks of varying qualities (high quality tasks generate high revenue),
which are known to the designer, to a set of agents of varying qualities (high
quality agents generate high revenue), which are unknown to the designer and to
the agents themselves, such that the overall system performance (e.g. total
revenue) is maximized. However, in any realistic system, the designer can only
observe the output produced by an agent, which is stochastic, and not the
actual quality of the agent. Thus, the designer needs to learn the quality of
the agents and solve an adverse selection problem. Moreover, the expected
values of agents' outputs depend not only on the qualities of the tasks and the
qualities of the agents, but also on the efforts exerted by the agents. This is
because agents are strategic and want to optimally balance the rewards and
costs of exerting effort. Hence, the designer needs to simultaneously learn
about the agents and solve a joint moral hazard and adverse selection problem.
In this paper we develop a first mechanism that learns and repeatedly matches
agents to tasks in a manner that mitigates both adverse selection and moral
hazard. We compute the agents' equilibrium strategies that have a simple
bang-bang structure and also enable the agents to learn their qualities. We
prove that this mechanism achieves in equilibrium high long-run output by
comparing to benchmarks that assume perfect knowledge of the qualities (no
adverse selection) and no strategic behavior from the agents (no moral hazard).
We also define a new metric of long-run stability for the repeated matching
environment and show that our proposed matching is long-run stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02442</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02442</id><created>2016-02-07</created><authors><author><keyname>Defazio</keyname><forenames>Aaron</forenames></author></authors><title>A Simple Practical Accelerated Method for Finite Sums</title><categories>stat.ML cs.LG</categories><comments>Draft</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We describe a novel optimization method for finite sums (such as empirical
risk minimization problems) building on the recently introduced SAGA method.
Our method achieves an accelerated convergence rate on strongly convex smooth
problems, matching the conjectured optimal rate. Our method has only one
parameter (a step size), and is radically simpler than other accelerated
methods for finite sums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02443</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02443</id><created>2016-02-07</created><authors><author><keyname>Finn</keyname><forenames>Danny</forenames></author><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author><author><keyname>Razavi</keyname><forenames>Rouzbeh</forenames></author><author><keyname>Claussen</keyname><forenames>Holger</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz</forenames></author></authors><title>Energy and Spectral Efficiency Gains From Multi-User MIMO-based Small
  Cell Reassignments</title><categories>cs.NI</categories><comments>The paper is presented in IEEE Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the reassignment of User Equipments (UEs) between
adjacent small cells to concurrently enable spatial multiplexing gains through
Multi-User MIMO (MU-MIMO) and reductions in energy consumption though switching
emptied small cells to a sleep state. We consider a case where UEs can be
reassigned between adjacent small cells provided that the targeted neighbouring
cell contains a UE with which the reassigned UE can perform MU-MIMO without
experiencing excessive multi-user interference, and whilst achieving a minimum
expected gain in spectral efficiency over the previous original cell
transmissions as a result. We formulate the selection decision of which UEs to
reassign as a set covering problem with the objective of maximising the number
of small cell base stations to switch to a sleep state. Our results show that,
for both indoor and outdoor LTE small cell scenarios, the proposed
MU-MIMO-based reassignments achieve significant reductions in the required
number of active small cell base stations, whilst simultaneously achieving
increases in spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02445</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02445</id><created>2016-02-07</created><updated>2016-02-28</updated><authors><author><keyname>Wei&#xdf;</keyname><forenames>Armin</forenames></author></authors><title>A Logspace Solution to the Word and Conjugacy problem of Generalized
  Baumslag-Solitar Groups</title><categories>cs.CC cs.DM math.GR</categories><msc-class>20F10, 68Q25</msc-class><acm-class>F.2.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Baumslag-Solitar groups were introduced in 1962 by Baumslag and Solitar as
examples for finitely presented non-Hopfian two-generator groups. Since then,
they served as examples for a wide range of purposes. As Baumslag-Solitar
groups are HNN extensions, there is a natural generalization in terms of graph
of groups.
  Concerning algorithmic aspects of generalized Baumslag-Solitar groups,
several decidability results are known. Indeed, a straightforward application
of standard algorithms leads to a polynomial time solution of the word problem
(the question whether some word over the generators represents the identity of
the group). The conjugacy problem (the question whether two given words
represent conjugate group elements) is more complicated; still decidability has
been established by Anshel and Stebe for ordinary Baumslag-Solitar groups and
for generalized Baumslag-Solitar groups independently by Lockhart and Beeker.
However, up to now no precise complexity estimates have been given.
  In this work, we give a LOGSPACE algorithm for both problems. More precisely,
we describe a uniform TC^0 many-one reduction of the word problem to the word
problem of the free group. Then we refine the known techniques for the
conjugacy problem and show that it can be solved in LOGSPACE. Moreover, for
ordinary Baumslag-Solitar groups also conjugacy is AC^0-Turing-reducible to the
word problem of the free group.
  Finally, we consider uniform versions (where also the graph of groups is part
of the input) of both word and conjugacy problem: while the word problem still
is solvable in LOGSPACE, the conjugacy problem becomes EXPSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02450</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02450</id><created>2016-02-07</created><updated>2016-02-09</updated><authors><author><keyname>Patrini</keyname><forenames>Giorgio</forenames></author><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Nock</keyname><forenames>Richard</forenames></author><author><keyname>Carioni</keyname><forenames>Marcello</forenames></author></authors><title>Loss factorization, weakly supervised learning and label noise
  robustness</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the empirical risk of most well-known loss functions factors
into a linear term aggregating all labels with a term that is label free, and
can further be expressed by sums of the loss. This holds true even for
non-smooth, non-convex losses and in any RKHS. The first term is a (kernel)
mean operator --the focal quantity of this work-- which we characterize as the
sufficient statistic for the labels. The result tightens known generalization
bounds and sheds new light on their interpretation.
  Factorization has a direct application on weakly supervised learning. In
particular, we demonstrate that algorithms like SGD and proximal methods can be
adapted with minimal effort to handle weak supervision, once the mean operator
has been estimated. We apply this idea to learning with asymmetric noisy
labels, connecting and extending prior work. Furthermore, we show that most
losses enjoy a data-dependent (by the mean operator) form of noise robustness,
in contrast with known negative results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02452</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02452</id><created>2016-02-07</created><authors><author><keyname>Dara</keyname><forenames>Sashank</forenames></author><author><keyname>Muralidhara</keyname><forenames>V. N.</forenames></author></authors><title>Privacy Preserving Architectures for Collaborative Intrusion Detection</title><categories>cs.CR</categories><comments>9 Pages, 2 figures, Position paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaboration among multiple organizations is imperative for contemporary
intrusion detection. As modern threats become well sophisticated it is
difficult for organizations to defend with threat context local to their
networks alone. Availability of global \emph{threat intelligence} is must for
organizations to defend against modern advanced persistent threats (APTs). In
order to benefit from such global context of attacks, privacy concerns continue
to be of major hindrance. In this position paper we identify real world privacy
problems as precise use cases, relevant cryptographic technologies and discuss
privacy preserving architectures for collaborative intrusion detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02454</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02454</id><created>2016-02-07</created><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Efficient Algorithms for Adversarial Contextual Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first oracle efficient sublinear regret algorithms for
adversarial versions of the contextual bandit problem. In this problem, the
learner repeatedly makes an action on the basis of a context and receives
reward for the chosen action, with the goal of achieving reward competitive
with a large class of policies. We analyze two settings: i) in the transductive
setting the learner knows the set of contexts a priori, ii) in the small
separator setting, there exists a small set of contexts such that any two
policies behave differently in one of the contexts in the set. Our algorithms
fall into the follow the perturbed leader family \cite{Kalai2005} and achieve
regret $O(T^{3/4}\sqrt{K\log(N)})$ in the transductive setting and $O(T^{2/3}
d^{3/4} K\sqrt{\log(N)})$ in the separator setting, where $K$ is the number of
actions, $N$ is the number of baseline policies, and $d$ is the size of the
separator. We actually solve the more general adversarial contextual
semi-bandit linear optimization problem, whilst in the full information setting
we address the even more general contextual combinatorial optimization. We
provide several extensions and implications of our algorithms, such as
switching regret and efficient learning with predictable sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02473</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02473</id><created>2016-02-08</created><authors><author><keyname>Al-Olimat</keyname><forenames>Hussein S.</forenames></author><author><keyname>Green</keyname><forenames>Robert C.</forenames><suffix>II</suffix></author><author><keyname>Alam</keyname><forenames>Mansoor</forenames></author><author><keyname>Devabhaktuni</keyname><forenames>Vijay</forenames></author><author><keyname>Cheng</keyname><forenames>Wei</forenames></author></authors><title>Particle Swarm Optimized Power Consumption of Trilateration</title><categories>cs.NI cs.AI</categories><comments>19 Pages, 13 Figures, 10 Tables, Journal</comments><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.4, No.4, July 2014</journal-ref><doi>10.5121/ijfcst.2014.4401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trilateration-based localization (TBL) has become a corner stone of modern
technology. This study formulates the concern on how wireless sensor networks
can take advantage of the computational intelligent techniques using both
single- and multi-objective particle swarm optimization (PSO) with an overall
aim of concurrently minimizing the required time for localization, minimizing
energy consumed during localization, and maximizing the number of nodes fully
localized through the adjustment of wireless sensor transmission ranges while
using TBL process. A parameter-study of the applied PSO variants is performed,
leading to results that show algorithmic improvements of up to 32% in the
evaluated objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02481</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02481</id><created>2016-02-08</created><updated>2016-02-09</updated><authors><author><keyname>Choi</keyname><forenames>Sungjoon</forenames></author><author><keyname>Zhou</keyname><forenames>Qian-Yi</forenames></author><author><keyname>Miller</keyname><forenames>Stephen</forenames></author><author><keyname>Koltun</keyname><forenames>Vladlen</forenames></author></authors><title>A Large Dataset of Object Scans</title><categories>cs.CV cs.GR</categories><comments>Added a spacer to the URL in the abstract so that arXiv can correctly
  display the link</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have created a dataset of more than ten thousand 3D scans of real objects.
To create the dataset, we recruited 70 operators, equipped them with
consumer-grade mobile 3D scanning setups, and paid them to scan objects in
their environments. The operators scanned objects of their choosing, outside
the laboratory and without direct supervision by computer vision professionals.
The result is a large and diverse collection of object scans: from shoes, mugs,
and toys to grand pianos, construction vehicles, and large outdoor sculptures.
We worked with an attorney to ensure that data acquisition did not violate
privacy constraints. The acquired data was irrevocably placed in the public
domain and is available freely at http://redwood-data.org/3dscan .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02490</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02490</id><created>2016-02-08</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Gro&#xdf;kopf</keyname><forenames>Stefan</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author></authors><title>Simulation of bifurcated stent grafts to treat abdominal aortic
  aneurysms (AAA)</title><categories>cs.GR cs.CE cs.CG physics.med-ph q-bio.TO</categories><comments>6 pages, 5 figures, 5 equations, 9 references in Proc. SPIE 6509,
  Medical Imaging 2007: Visualization and Image-Guided Procedures, 65091N (22
  March 2007)</comments><doi>10.1117/12.709260</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a method is introduced, to visualize bifurcated stent grafts in
CT-Data. The aim is to improve therapy planning for minimal invasive treatment
of abdominal aortic aneurysms (AAA). Due to precise measurement of the
abdominal aortic aneurysm and exact simulation of the bifurcated stent graft,
physicians are supported in choosing a suitable stent prior to an intervention.
The presented method can be used to measure the dimensions of the abdominal
aortic aneurysm as well as simulate a bifurcated stent graft. Both of these
procedures are based on a preceding segmentation and skeletonization of the
aortic, right and left iliac. Using these centerlines (aortic, right and left
iliac) a bifurcated initial stent is constructed. Through the implementation of
an ACM method the initial stent is fit iteratively to the vessel walls - due to
the influence of external forces (distance- as well as balloonforce). Following
the fitting process, the crucial values for choosing a bifurcated stent graft
are measured, e.g. aortic diameter, right and left common iliac diameter,
minimum diameter of distal neck. The selected stent is then simulated to the
CT-Data - starting with the initial stent. It hereby becomes apparent if the
dimensions of the bifurcated stent graft are exact, i.e. the fitting to the
arteries was done properly and no ostium was covered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02493</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02493</id><created>2016-02-08</created><authors><author><keyname>Bhute</keyname><forenames>Harsha</forenames></author><author><keyname>Chavan</keyname><forenames>G. T.</forenames></author><author><keyname>Bhute</keyname><forenames>Avinash</forenames></author></authors><title>Analysis of Location Management Schemes for MANET using Synthetic
  Mobility Models</title><categories>cs.NI</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the performance evaluation of a protocol for an ad hoc network, the
protocol should be tested under realistic conditions including, but not limited
to, a sensible transmission range, limited buffer space for the storage of
messages, representative data traffic models, and realistic movements of the
mobile users and several mobility models that represent mobile nodes whose
movements are dependent on each other (i.e., group mobility models ).The goal
of this paper is to simulate the movements of mobile nodes within a network and
present a number of mobility models in order to demonstrate its effect on
Location management scheme for mobile ad hoc network or personal communication
services networks. Specifically, to illustrate how the performance results of
an ad hoc network protocol drastically change as a result of changing the
mobility model simulated. Location management is a fundamental problem in
personal communication services network. The current standard of location
management is HLR/VLR scheme. It has been observed that the performance of any
location management scheme depends on space requirements, bandwidth
requirements and time requirements. To avoid certain drawbacks in HLR/VLR
scheme, many approaches including hierarchical approaches have been suggested.
Working set idea is chosen to analyze its performance for location management
in PCS networks. Due to inadequacy of standard network simulators to provide
the output in the format desired, a new location management simulator can be
built. Two variants of working set idea viz. Working set scheme for HLR/VLR
approach and working set scheme for hierarchical approach can be used and then
compare the performance of HLR/VLR scheme and working set scheme using the
results obtained by the simulator with respect to already available mobile
activity traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02499</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02499</id><created>2016-02-08</created><authors><author><keyname>van Leeuwen</keyname><forenames>David A.</forenames></author><author><keyname>Orr</keyname><forenames>Rosemary</forenames></author></authors><title>The &quot;Sprekend Nederland&quot; project and its application to accent location</title><categories>stat.ML cs.CL</categories><comments>Submitted to Speaker and Language Recognition Odyssey 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the data collection effort that is part of the project
Sprekend Nederland (The Netherlands Talking), and discusses its potential use
in Automatic Accent Location. We define Automatic Accent Location as the task
to describe the accent of a speaker in terms of the location of the speaker and
its history. We discuss possible ways of describing accent location, the
consequence these have for the task of automatic accent location, and potential
evaluation metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02504</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02504</id><created>2016-02-08</created><authors><author><keyname>Siivola</keyname><forenames>Eero</forenames></author><author><keyname>Sierla</keyname><forenames>Seppo</forenames></author><author><keyname>Niemist&#xf6;</keyname><forenames>Hannu</forenames></author><author><keyname>Karhela</keyname><forenames>Tommi</forenames></author><author><keyname>Vyatkin</keyname><forenames>Valeriy</forenames></author></authors><title>Requirement verification in simulation-based automation testing</title><categories>cs.SE cs.LO</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of the Industrial Internet results in an increasing number of
complex temporal interdependencies between automation systems and the processes
to be controlled. There is a need for verification methods that scale better
than formal verification methods and which are more rigorous than testing.
Simulation-based runtime verification is proposed as such a method, and an
application of metric temporal logic is presented as a contribution over the
state-of-the-art work by the Modelica community, which is based on linear
temporal logic that cannot capture several kinds of requirements that occur
frequently in industrial process control systems. The practical scalability of
the proposed approach is validated against a production process designed by an
industrial partner, resulting in the discovery of several requirement
violations. The scalability implications of alternative approaches for
requirements formalization are also investigated theoretically from a
computational complexity perspective for the requirements in our case study,
and a broader investigation is proposed as further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02505</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02505</id><created>2016-02-08</created><updated>2016-02-11</updated><authors><author><keyname>Hubara</keyname><forenames>Itay</forenames></author><author><keyname>Soudry</keyname><forenames>Daniel</forenames></author><author><keyname>Yaniv</keyname><forenames>Ran El</forenames></author></authors><title>Binarized Neural Networks</title><categories>cs.LG cs.NE</categories><comments>Fixed typos and figure 4 results analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce a binarized deep neural network (BDNN) model. BDNNs
are trained using a novel binarized back propagation algorithm (BBP), which
uses binary weights and binary neurons during the forward and backward
propagation, while retaining precision of the stored weights in which gradients
are accumulated. At test phase, BDNNs are fully binarized and can be
implemented in hardware with low circuit complexity. The proposed binarized
networks can be implemented using binary convolutions and proxy matrix
multiplications with only standard binary XNOR and population count (popcount)
operations. BBP is expected to reduce energy consumption by at least two orders
of magnitude when compared to the hardware implementation of existing training
algorithms. We obtained near state-of-the-art results with BDNNs on the
permutation-invariant MNIST, CIFAR-10 and SVHN datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02506</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02506</id><created>2016-02-08</created><authors><author><keyname>Steiner</keyname><forenames>Thomas</forenames></author></authors><title>Wikipedia Tools for Google Spreadsheets</title><categories>cs.IR</categories><comments>4 pages, 3 Listings, 4 figures</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the Wikipedia Tools for Google Spreadsheets.
Google Spreadsheets is part of a free, Web-based software office suite offered
by Google within its Google Docs service. It allows users to create and edit
spreadsheets online, while collaborating with other users in realtime.
Wikipedia is a free-access, free-content Internet encyclopedia, whose content
and data is available, among other means, through an API. With the Wikipedia
Tools for Google Spreadsheets, we have created a toolkit that facilitates
working with Wikipedia data from within a spreadsheet context. We make these
tools available as open-source on GitHub
[https://github.com/tomayac/wikipedia-tools-for-google-spreadsheets], released
under the permissive Apache 2.0 license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02509</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02509</id><created>2016-02-08</created><authors><author><keyname>Bokslag</keyname><forenames>Wouter</forenames></author><author><keyname>de Vries</keyname><forenames>Manon</forenames></author></authors><title>Evaluating e-voting: theory and practice</title><categories>cs.CY cs.CR</categories><comments>19 pages</comments><msc-class>91B12, 94A60</msc-class><acm-class>K.4.1; E.3</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In the Netherlands as well as many other countries, the use of electronic
voting solutions is a recurrent topic of discussion. While electronic voting
certainly has advantages over paper voting, there are also important risks
involved. This paper presents an analysis of benefits and risks of electronic
voting, and shows the relevance of these issues by means of three case studies
of real-world implementations. Additionally, techniques that may be employed to
improve upon many of the current systems are presented. We conclude that the
advantages of E-voting do not outweigh the disadvantages, as the resulting
reduced verifiability and transparency seem hard to overcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02514</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02514</id><created>2016-02-08</created><updated>2016-02-25</updated><authors><author><keyname>Newling</keyname><forenames>James</forenames></author><author><keyname>Fleuret</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Fast k-means with accurate bounds</title><categories>stat.ML cs.LG</categories><comments>8 pages + supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel accelerated exact k-means algorithm, which performs better
than the current state-of-the-art low-dimensional algorithm in 18 of 22
experiments, running up to 3 times faster. We also propose a general
improvement of existing state-of-the-art accelerated exact k-means algorithms
through better estimates of the distance bounds used to reduce the number of
distance calculations, and get a speedup in 36 of 44 experiments, up to 1.8
times faster.
  We have conducted experiments with our own implementations of existing
methods to ensure homogeneous evaluation of performance, and we show that our
implementations perform as well or better than existing available
implementations. Finally, we propose simplified variants of standard approaches
and show that they are faster than their fully-fledged counterparts in 59 of 62
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02517</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02517</id><created>2016-02-08</created><authors><author><keyname>Nunez-Yanez</keyname><forenames>Jose</forenames></author><author><keyname>Sun</keyname><forenames>Tom</forenames></author></authors><title>Energy Efficient Video Fusion with Heterogeneous CPU-FPGA Devices</title><categories>cs.AR cs.PF</categories><comments>Presented at HIP3ES, 2016</comments><report-no>HIP3ES/2016/3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a complete video fusion system with hardware acceleration
and investigates the energy trade-offs between computing in the CPU or the FPGA
device. The video fusion application is based on the Dual-Tree Complex Wavelet
Transforms (DT-CWT). In this work the transforms are mapped to a hardware
accelerator using high-level synthesis tools for the FPGA and also vectorized
code for the single instruction multiple data (SIMD) engine available in the
CPU. The accelerated system reduces computation time and energy by a factor of
2. Moreover, the results show a key finding that the FPGA is not always the
best choice for acceleration, and the SIMD engine should be selected when the
wavelet decomposition reduces the frame size below a certain threshold. This
dependency on workload size means that an adaptive system that intelligently
selects between the SIMD engine and the FPGA achieves the most energy and
performance efficiency point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02518</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02518</id><created>2016-02-08</created><authors><author><keyname>Bhadra</keyname><forenames>Sahely</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author><author><keyname>Rousu</keyname><forenames>Juho</forenames></author></authors><title>Multi-view Kernel Completion</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the first method that (1) can complete kernel
matrices with completely missing rows and columns as opposed to individual
missing kernel values, (2) does not require any of the kernels to be complete a
priori, and (3) can tackle non-linear kernels. These aspects are necessary in
practical applications such as integrating legacy data sets, learning under
sensor failures and learning when measurements are costly for some of the
views. The proposed approach predicts missing rows by modelling both
within-view and between-view relationships among kernel values. We show, both
on simulated data and real world data, that the proposed method outperforms
existing techniques in the restricted settings where they are available, and
extends applicability to new settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02522</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02522</id><created>2016-02-08</created><authors><author><keyname>Mirsharif</keyname><forenames>Qazaleh</forenames></author><author><keyname>Sadani</keyname><forenames>Sidharth</forenames></author><author><keyname>Shah</keyname><forenames>Shishir</forenames></author><author><keyname>Yoshida</keyname><forenames>Hanako</forenames></author><author><keyname>Burling</keyname><forenames>Joseph</forenames></author></authors><title>A Semi-Automated Method for Object Segmentation in Infant's Egocentric
  Videos to Study Object Perception</title><categories>cs.CV</categories><comments>Accepted at CVIP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object segmentation in infant's egocentric videos is a fundamental step in
studying how children perceive objects in early stages of development. From the
computer vision perspective, object segmentation in such videos pose quite a
few challenges because the child's view is unfocused, often with large head
movements, effecting in sudden changes in the child's point of view which leads
to frequent change in object properties such as size, shape and illumination.
In this paper, we develop a semi-automated, domain specific, method to address
these concerns and facilitate the object annotation process for cognitive
scientists allowing them to select and monitor the object under segmentation.
The method starts with an annotation from the user of the desired object and
employs graph cut segmentation and optical flow computation to predict the
object mask for subsequent video frames automatically. To maintain accuracy, we
use domain specific heuristic rules to re-initialize the program with new user
input whenever object properties change dramatically. The evaluations
demonstrate the high speed and accuracy of the presented method for object
segmentation in voluminous egocentric videos. We apply the proposed method to
investigate potential patterns in object distribution in child's view at
progressive ages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02523</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02523</id><created>2016-02-08</created><authors><author><keyname>McAllister</keyname><forenames>Rowan</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl Edward</forenames></author></authors><title>Data-Efficient Reinforcement Learning in Continuous-State POMDPs</title><categories>stat.ML cs.LG cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a data-efficient reinforcement learning algorithm resistant to
observation noise. Our method extends the highly data-efficient PILCO algorithm
(Deisenroth &amp; Rasmussen, 2011) into partially observed Markov decision
processes (POMDPs) by considering the filtering process during policy
evaluation. PILCO conducts policy search, evaluating each policy by first
predicting an analytic distribution of possible system trajectories. We
additionally predict trajectories w.r.t. a filtering process, achieving
significantly higher performance than combining a filter with a policy
optimised by the original (unfiltered) framework. Our test setup is the
cartpole swing-up task with sensor noise, which involves nonlinear dynamics and
requires nonlinear control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02524</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02524</id><created>2016-02-08</created><authors><author><keyname>Bijl</keyname><forenames>Hildo</forenames></author><author><keyname>van Wingerden</keyname><forenames>Jan Willem</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author></authors><title>Mean and variance of the LQG cost function</title><categories>cs.SY</categories><journal-ref>Automatica, Volume 67, May 2016, Pages 216-223</journal-ref><doi>10.1016/j.automatica.2016.01.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Quadratic Gaussian (LQG) systems are well-understood and methods to
minimize the expected cost are readily available. Less is known about the
statistical properties of the resulting cost function. The contribution of this
paper is a set of analytic expressions for the mean and variance of the LQG
cost function. These expressions are derived using two different methods, one
using solutions to Lyapunov equations and the other using only matrix
exponentials. Both the discounted and the non-discounted cost function are
considered, as well as the finite-time and the infinite-time cost function. The
derived expressions are successfully applied to an example system to reduce the
probability of the cost exceeding a given threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02527</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02527</id><created>2016-02-08</created><authors><author><keyname>Chen</keyname><forenames>Po-An</forenames></author><author><keyname>Chen</keyname><forenames>Yi-Le</forenames></author><author><keyname>Lu</keyname><forenames>Chi-Jen</forenames></author></authors><title>How Much of a Person Influencing the Others and Being Influenced Matters
  in Opinion Formation Games</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The opinion forming process in a social network could be naturally modeled as
an opinion in influencing and updating dynamics. This already attracted
researchers interest a while ago in mathematical sociology, and recently in
theoretical computer science. In so-called &quot;opinion formation games&quot;, when
underlying networks are directed, a bounded price of anarchy is only known for
weighted Eulerian graphs, which may not be the most general class of directed
graphs that give a bounded price of anarchy. Thus, we aim to bound the price of
anarchy for games with directed graphs more general than weighted Eulerian
graphs in this paper.
  We first bound the price of anarchy for a more general class of directed
graphs with conditions intuitively meaning that each node does not influence
the others more than she is influenced by herself and the others, where the
bounds depend on such influence differences. This generalizes the previous
results on directed graphs, and recovers and matches the previous bounds in
some specific classes of (directed) Eulerian graphs. We then show that there
exists an example that just slightly violates the conditions with an unbounded
price of anarchy. We further propose more directions along this line of
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02543</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02543</id><created>2016-02-08</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh J.</forenames></author></authors><title>Homogeneity of Cluster Ensembles</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expectation and the mean of partitions generated by a cluster ensemble
are not unique in general. This issue poses challenges in statistical inference
and cluster stability. In this contribution, we state sufficient conditions for
uniqueness of expectation and mean. The proposed conditions show that a unique
mean is neither exceptional nor generic. To cope with this issue, we introduce
homogeneity as a measure of how likely is a unique mean for a sample of
partitions. We show that homogeneity is related to cluster stability. This
result points to a possible conflict between cluster stability and diversity in
consensus clustering. To assess homogeneity in a practical setting, we propose
an efficient way to compute a lower bound of homogeneity. Empirical results
using the k-means algorithm suggest that uniqueness of the mean partition is
not exceptional for real-world data. Moreover, for samples of high homogeneity,
uniqueness can be enforced by increasing the number of data points or by
removing outlier partitions. In a broader context, this contribution can be
placed as a further step towards a statistical theory of partitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02574</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02574</id><created>2016-02-08</created><authors><author><keyname>Sevrin</keyname><forenames>Lo&#xef;c</forenames></author><author><keyname>Noury</keyname><forenames>Norbert</forenames></author><author><keyname>Abouchi</keyname><forenames>Nacer</forenames></author><author><keyname>Jumel</keyname><forenames>Fabrice</forenames></author><author><keyname>Massot</keyname><forenames>Bertrand</forenames></author><author><keyname>Saraydaryan</keyname><forenames>Jacques</forenames></author></authors><title>Characterization of a Multi-User Indoor Positioning System Based on Low
  Cost Depth Vision (Kinect) for Monitoring Human Activity in a Smart Home</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of systems use indoor positioning for many scenarios
such as asset tracking, health care, games, manufacturing, logistics, shopping,
and security. Many technologies are available and the use of depth cameras is
becoming more and more attractive as this kind of device becomes affordable and
easy to handle. This paper contributes to the effort of creating an indoor
positioning system based on low cost depth cameras (Kinect). A method is
proposed to optimize the calibration of the depth cameras, to describe the
multi-camera data fusion and to specify a global positioning projection to
maintain the compatibility with outdoor positioning systems.
  The monitoring of the people trajectories at home is intended for the early
detection of a shift in daily activities which highlights disabilities and loss
of autonomy. This system is meant to improve homecare health management at home
for a better end of life at a sustainable cost for the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02575</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02575</id><created>2016-02-08</created><updated>2016-02-12</updated><authors><author><keyname>Wang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Dunson</keyname><forenames>David</forenames></author><author><keyname>Leng</keyname><forenames>Chenlei</forenames></author></authors><title>DECOrrelated feature space partitioning for distributed sparse
  regression</title><categories>stat.ME cs.DC stat.CO stat.ML</categories><comments>Correct legend errors in Figure 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fitting statistical models is computationally challenging when the sample
size or the dimension of the dataset is huge. An attractive approach for
down-scaling the problem size is to first partition the dataset into subsets
and then fit using distributed algorithms. The dataset can be partitioned
either horizontally (in the sample space) or vertically (in the feature space).
While the majority of the literature focuses on sample space partitioning,
feature space partitioning is more effective when $p\gg n$. Existing methods
for partitioning features, however, are either vulnerable to high correlations
or inefficient in reducing the model dimension. In this paper, we solve these
problems through a new embarrassingly parallel framework named DECO for
distributed variable selection and parameter estimation. In DECO, variables are
first partitioned and allocated to $m$ distributed workers. The decorrelated
subset data within each worker are then fitted via any algorithm designed for
high-dimensional problems. We show that by incorporating the decorrelation
step, DECO can achieve consistent variable selection and parameter estimation
on each subset with (almost) no assumptions. In addition, the convergence rate
is nearly minimax optimal for both sparse and weakly sparse models and does NOT
depend on the partition number $m$. Extensive numerical experiments are
provided to illustrate the performance of the new framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02576</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02576</id><created>2016-01-30</created><authors><author><keyname>Delgado</keyname><forenames>Moises</forenames></author><author><keyname>Janwa</keyname><forenames>Heeralal</forenames></author></authors><title>Progress Towards the Conjecture on APN Functions and Absolutely
  Irreducible Polynomials</title><categories>math.NT cs.CR cs.IT math.AG math.IT</categories><msc-class>94A60, 20C05, 05B10, 11T71, 11G20, 11G25, 12E20, 14E15, 14G15,
  14G20, 14H25</msc-class><acm-class>E.4; D.4.6; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost Perfect Nonlinear (APN) functions are very useful in cryptography,
when they are used as S-Boxes, because of their good resistance to differential
cryptanalysis. An APN function $f:\mathbb{F}_{2^n}\rightarrow\mathbb{F}_{2^n}$
is called exceptional APN if it is APN on infinitely many extensions of
$\mathbb{F}_{2^n}$. Aubry, McGuire and Rodier conjectured that the only
exceptional APN functions are the Gold and the Kasami-Welch monomial functions.
They established that a polynomial function of odd degree is not exceptional
APN provided the degree is not a Gold number $(2^k+1)$ or a Kasami-Welch number
$(2^{2k}-2^k+1)$. When the degree of the polynomial function is a Gold number,
several partial results have been obtained [1, 7, 8, 10, 17]. One of the
results in this article is a proof of the relatively primeness of the
multivariate APN polynomial conjecture, in the Gold degree case. This helps us
extend substantially previous results. We prove that Gold degree polynomials of
the form $x^{2^k+1}+h(x)$, where $deg(h)$ is any odd integer (with the natural
exceptions), can not be exceptional APN.
  We also show absolute irreducibility of several classes of multivariate
polynomials over finite fields and discuss their applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02586</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02586</id><created>2016-02-08</created><authors><author><keyname>Tizhoosh</keyname><forenames>Hamid R.</forenames></author><author><keyname>Gangeh</keyname><forenames>Mehrdad J.</forenames></author><author><keyname>Tadayyon</keyname><forenames>Hadi</forenames></author><author><keyname>Czarnota</keyname><forenames>Gregory J.</forenames></author></authors><title>Tumour ROI Estimation in Ultrasound Images via Radon Barcodes in
  Patients with Locally Advanced Breast Cancer</title><categories>cs.CV</categories><comments>To appear in proceedings of The International Symposium on Biomedical
  Imaging (ISBI), April 13-16, 2016, Prague, Czech Republic</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative ultrasound (QUS) methods provide a promising framework that can
non-invasively and inexpensively be used to predict or assess the tumour
response to cancer treatment. The first step in using the QUS methods is to
select a region of interest (ROI) inside the tumour in ultrasound images.
Manual segmentation, however, is very time consuming and tedious. In this
paper, a semi-automated approach will be proposed to roughly localize an ROI
for a tumour in ultrasound images of patients with locally advanced breast
cancer (LABC). Content-based barcodes, a recently introduced binary descriptor
based on Radon transform, were used in order to find similar cases and estimate
a bounding box surrounding the tumour. Experiments with 33 B-scan images
resulted in promising results with an accuracy of $81\%$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02594</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02594</id><created>2016-02-08</created><authors><author><keyname>Matek</keyname><forenames>Tadej</forenames></author><author><keyname>Zebec</keyname><forenames>Svit Timej</forenames></author></authors><title>GitHub open source project recommendation system</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hosting platforms for software projects can form collaborative social
networks and a prime example of this is GitHub which is arguably the most
popular platform of this kind. An open source project recommendation system
could be a major feature for a platform like GitHub, enabling its users to find
relevant projects in a fast and simple manner. We perform network analysis on a
constructed graph based on GitHub data and present a recommendation system that
uses link prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02598</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02598</id><created>2016-02-08</created><authors><author><keyname>Xiang</keyname><forenames>Ji</forenames></author><author><keyname>Li</keyname><forenames>Yanjun</forenames></author><author><keyname>Hill</keyname><forenames>David J.</forenames></author></authors><title>Cooperative output regulation of multi-agent network systems with
  dynamic edges</title><categories>math.DS cs.SY</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a new class of linear multi-agent network systems, in
which nodes are coupled by dynamic edges in the sense that each edge has a
dynamic system attached as well. The outputs of the edge dynamic systems form
the external inputs of the node dynamic systems, which are termed &quot;neighboring
inputs&quot; representing the coupling actions between nodes. The outputs of the
node dynamic systems are the inputs of the edge dynamic systems. Several
cooperative output regulation problems are posed, including output
synchronization, output cooperation and master-slave output cooperation. Output
cooperation is specified as making the neighboring input, a weighted sum of
edge outputs, track a predefined trajectory by cooperation of node outputs.
Distributed cooperative output regulation controllers depending on local state
and neighboring inputs are presented, which are designed by combining feedback
passivity theories and the internal model principle. A simulation example on
the cooperative current control of an electrical network illustrates the
potential applications of the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02601</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02601</id><created>2016-02-02</created><authors><author><keyname>Panko</keyname><forenames>Ray</forenames></author></authors><title>What We Don't Know About Spreadsheet Errors Today: The Facts, Why We
  Don't Believe Them, and What We Need to Do</title><categories>cs.SE</categories><comments>15 Pages, 5 Figures, 3 Tables, some in colour</comments><proxy>Grenville Croll</proxy><journal-ref>Proc. 16th EuSpRIG Conf. &quot;Spreadsheet Risk Management&quot; (2015)
  pp79-93 ISBN: 978-1-905404-52-0</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on spreadsheet errors is substantial, compelling, and unanimous. It
has three simple conclusions. The first is that spreadsheet errors are rare on
a per-cell basis, but in large programs, at least one incorrect bottom-line
value is very likely to be present. The second is that errors are extremely
difficult to detect and correct. The third is that spreadsheet developers and
corporations are highly overconfident in the accuracy of their spreadsheets.
The disconnect between the first two conclusions and the third appears to be
due to the way human cognition works. Most importantly, we are aware of very
few of the errors we make. In addition, while we are proudly aware of errors
that we fix, we have no idea of how many remain, but like Little Jack Horner we
are impressed with our ability to ferret out errors. This paper reviews human
cognition processes and shows first that humans cannot be error free no matter
how hard they try, and second that our intuition about errors and how we can
reduce them is based on appallingly bad knowledge. This paper argues that we
should reject any prescription for reducing errors that has not been rigorously
proven safe and effective. This paper also argues that our biggest need, based
on empirical data, is to do massively more testing than we do now. It suggests
that the code inspection methodology developed in software development is
likely to apply very well to spreadsheet inspection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02610</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02610</id><created>2016-02-08</created><authors><author><keyname>Belmonte</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author></authors><title>Metric Dimension of Bounded Tree-length Graphs</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of resolving sets in a graph was introduced by Slater (1975) and
Harary and Melter (1976) as a way of uniquely identifying every vertex in a
graph. A set of vertices in a graph is a resolving set if for any pair of
vertices x and y there is a vertex in the set which has distinct distances to x
and y. A smallest resolving set in a graph is called a metric basis and its
size, the metric dimension of the graph. The problem of computing the metric
dimension of a graph is a well-known NP-hard problem and while it was known to
be polynomial time solvable on trees, it is only recently that efforts have
been made to understand its computational complexity on various restricted
graph classes. In recent work, Foucaud et al. (2015) showed that this problem
is NP-complete even on interval graphs. They complemented this result by also
showing that it is fixed-parameter tractable (FPT) parameterized by the metric
dimension of the graph. In this work, we show that this FPT result can in fact
be extended to all graphs of bounded tree-length. This includes well-known
classes like chordal graphs, AT-free graphs and permutation graphs. We also
show that this problem is FPT parameterized by the modular-width of the input
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02612</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02612</id><created>2016-02-08</created><authors><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Sign-Compute-Resolve for Tree Splitting Random Access</title><categories>cs.IT math.IT</categories><comments>This is an extended version of arXiv:1409.6902</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to random access that is based on three elements:
physical-layer network coding (PLNC), signature codes and tree splitting. In
presence of a collision, physical-layer network coding enables the receiver to
decode, i.e. compute the sum of the packets that were transmitted by the
individual users. For each user, the packet consists of the user's signature,
as well as the data that the user wants to communicate. As long as no more than
K users collide, their identities can be recovered from the sum of their
signatures. A tree-splitting algorithm is used to deal with the case that more
than K users collide. We demonstrate that our approach achieves throughput that
tends to 1 rapidly as K increases. We also present results on net data-rate of
the system, showing the impact of the overheads of the constituent elements of
the proposed protocol. We compare the performance of our scheme with an upper
bound that is obtained under the assumption that the active users are a priori
known. Also, we consider an upper bound on the net data-rate for any PLNC based
strategy in which one linear equation per slot is decoded. We show that already
at modest packet lengths, the net data-rate of our scheme becomes close to the
second upper bound, i.e. the overhead of the contention resolution algorithm
and the signature codes vanishes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02617</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02617</id><created>2016-02-08</created><authors><author><keyname>Liu</keyname><forenames>Zhun-Ga</forenames><affiliation>Palaiseau</affiliation></author><author><keyname>Pan</keyname><forenames>Quan</forenames><affiliation>Palaiseau</affiliation></author><author><keyname>Dezert</keyname><forenames>Jean</forenames><affiliation>Palaiseau</affiliation></author><author><keyname>Martin</keyname><forenames>Arnaud</forenames><affiliation>DRUID</affiliation></author></authors><title>Adaptive imputation of missing values for incomplete pattern
  classification</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Pattern Recognition, Elsevier, 2016, 52</journal-ref><doi>10.1016/j.patcog.2015.10.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In classification of incomplete pattern, the missing values can either play a
crucial role in the class determination, or have only little influence (or
eventually none) on the classification results according to the context. We
propose a credal classification method for incomplete pattern with adaptive
imputation of missing values based on belief function theory. At first, we try
to classify the object (incomplete pattern) based only on the available
attribute values. As underlying principle, we assume that the missing
information is not crucial for the classification if a specific class for the
object can be found using only the available information. In this case, the
object is committed to this particular class. However, if the object cannot be
classified without ambiguity, it means that the missing values play a main role
for achieving an accurate classification. In this case, the missing values will
be imputed based on the K-nearest neighbor (K-NN) and self-organizing map (SOM)
techniques, and the edited pattern with the imputation is then classified. The
(original or edited) pattern is respectively classified according to each
training class, and the classification results represented by basic belief
assignments are fused with proper combination rules for making the credal
classification. The object is allowed to belong with different masses of belief
to the specific classes and meta-classes (which are particular disjunctions of
several single classes). The credal classification captures well the
uncertainty and imprecision of classification, and reduces effectively the rate
of misclassifications thanks to the introduction of meta-classes. The
effectiveness of the proposed method with respect to other classical methods is
demonstrated based on several experiments using artificial and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02620</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02620</id><created>2016-02-08</created><authors><author><keyname>Pham</keyname><forenames>Ninh</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>Scalability and Total Recall with Fast CoveringLSH</title><categories>cs.DB cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locality-sensitive hashing (LSH) has emerged as the dominant algorithmic
technique for similarity search with strong performance guarantees in
high-dimensional spaces. A drawback of traditional LSH schemes is that they may
have \emph{false negatives}, i.e., the recall is less than 100\%. This limits
the applicability of LSH in settings requiring precise performance guarantees.
Building on the recent theoretical &quot;CoveringLSH&quot; construction that eliminates
false negatives, we propose a fast and practical covering LSH scheme for
Hamming space called \emph{Fast CoveringLSH (fcLSH)}. Inheriting the design
benefits of CoveringLSH our method avoids false negatives and always reports
all near neighbors. Compared to CoveringLSH we achieve an asymptotic
improvement to the hash function computation time from $\mathcal{O}(dL)$ to
$\mathcal{O}(d + L\log{L})$, where $d$ is the dimensionality of data and $L$ is
the number of hash tables. Our experiments on synthetic and real-world data
sets demonstrate that \emph{fcLSH} is comparable (and often superior) to
traditional hashing-based approaches for search radius up to 20 in
high-dimensional Hamming space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02622</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02622</id><created>2016-02-08</created><authors><author><keyname>Zulu</keyname><forenames>Andrew</forenames></author><author><keyname>John</keyname><forenames>Samuel</forenames></author></authors><title>A Review of Control Algorithms for Autonomous Quadrotors</title><categories>cs.SY</categories><comments>10 pages, 6 figures, 1 table</comments><journal-ref>Open Journal of Applied Sciences, 2014, 4, 547-556</journal-ref><doi>10.4236/ojapps.2014.414053</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The quadrotor unmanned aerial vehicle is a great platform for control systems
research as its nonlinear nature and under-actuated configuration make it ideal
to synthesize and analyze control algorithms. After a brief explanation of the
system, several algorithms have been analyzed including their advantages and
disadvantages: PID, Linear Quadratic Regulator (LQR), Sliding mode,
Backstepping, Feedback linearization, Adaptive, Robust, Optimal, L1,
H-infinity, Fuzzy logic and Artificial neutral networks. The conclusion of this
work is a proposal of hybrid systems to be considered as they combine
advantages from more than one control philosophy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02627</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02627</id><created>2016-02-08</created><updated>2016-02-16</updated><authors><author><keyname>Maslov</keyname><forenames>Dmitri</forenames></author></authors><title>Optimal and asymptotically optimal NCT reversible circuits by the gate
  types</title><categories>quant-ph cs.ET</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report optimal and asymptotically optimal reversible circuits composed of
NOT, CNOT, and Toffoli (NCT) gates, keeping the count by the subsets of the
gate types used. This study fine tunes the circuit complexity figures for the
realization of reversible functions via reversible NCT circuits. An important
consequence is a result on the limitation of the use of the $T$-count quantum
circuit metric popular in applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02629</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02629</id><created>2016-02-08</created><authors><author><keyname>Chuzhoy</keyname><forenames>Julia</forenames></author></authors><title>Improved Bounds for the Excluded Grid Theorem</title><categories>cs.DM math.CO</categories><msc-class>05C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Excluded Grid Theorem of Robertson and Seymour. This is a
fundamental result in graph theory, that states that there is some function $f:
Z^+\rightarrow Z^+$, such that for all integers $g&gt;0$, every graph of treewidth
at least $f(g)$ contains the $(g\times g)$-grid as a minor. Until recently, the
best known upper bounds on $f$ were super-exponential in $g$. A recent work of
Chekuri and Chuzhoy provided the first polynomial bound, by showing that
treewidth $f(g)=O(g^{98}\operatorname{poly}\log g)$ is sufficient to ensure the
existence of the $(g\times g)$-grid minor in any graph. In this paper we
improve this bound to $f(g)=O(g^{19}\operatorname{poly}\log g)$. We introduce a
number of new techniques, including a conceptually simple and almost entirely
self-contained proof of the theorem that achieves a polynomial bound on $f(g)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02630</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02630</id><created>2016-02-08</created><authors><author><keyname>Abraham</keyname><forenames>Edo</forenames></author><author><keyname>Stoianov</keyname><forenames>Ivan</forenames></author></authors><title>An efficient null space inexact Newton method for hydraulic simulation
  of water distribution networks</title><categories>cs.SY math.OC</categories><comments>15 pages, 9 figures, Preprint extension of Abraham and Stoianov, 2015
  (https://dx.doi.org/10.1061/(ASCE)HY.1943-7900.0001089), September 2015.
  Includes extended exposition, additional case studies and new simulations and
  analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Null space Newton algorithms are efficient in solving the nonlinear equations
arising in hydraulic analysis of water distribution networks. In this article,
we propose and evaluate an inexact Newton method that relies on partial updates
of the network pipes' frictional headloss computations to solve the linear
systems more efficiently and with numerical reliability. The update set
parameters are studied to propose appropriate values. Different null space
basis generation schemes are analysed to choose methods for sparse and
well-conditioned null space bases resulting in a smaller update set. The Newton
steps are computed in the null space by solving sparse, symmetric positive
definite systems with sparse Cholesky factorizations. By using the constant
structure of the null space system matrices, a single symbolic factorization in
the Cholesky decomposition is used multiple times, reducing the computational
cost of linear solves. The algorithms and analyses are validated using medium
to large-scale water network models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02638</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02638</id><created>2016-02-05</created><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-G.</forenames></author><author><keyname>Khatri</keyname><forenames>Sunil P.</forenames></author><author><keyname>Peper</keyname><forenames>Ferdinand</forenames></author></authors><title>Response to &quot;Comment on 'Zero and negative energy dissipation at
  information-theoretic erasure'&quot;</title><categories>cs.ET</categories><comments>Journal of Computational Electronics (Springer), in press (currently
  web-published)</comments><doi>10.1007/s10825-015-0788-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We prove that statistical information theoretic quantities, such as
information entropy, cannot generally be interrelated with the lower limit of
energy dissipation during information erasure. We also point out that, in
deterministic and error-free computers, the information entropy of memories
does not change during erasure because its value is always zero. On the other
hand, for information-theoretic erasure - i.e., &quot;thermalization&quot; /
randomization of the memory - the originally zero information entropy (with
deterministic data in the memory) changes after erasure to its maximum value, 1
bit / memory bit, while the energy dissipation is still positive, even at
parameters for which the thermodynamic entropy within the memory cell does not
change. Information entropy does not convert to thermodynamic entropy and to
the related energy dissipation; they are quantities of different physical
nature. Possible specific observations (if any) indicating convertibility are
at most fortuitous and due to the disregard of additional processes that are
present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02644</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02644</id><created>2016-02-08</created><updated>2016-02-09</updated><authors><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>Generating Images with Perceptual Similarity Metrics based on Deep
  Networks</title><categories>cs.LG cs.CV cs.NE</categories><comments>minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image-generating machine learning models are typically trained with loss
functions based on distance in the image space. This often leads to
over-smoothed results. We propose a class of loss functions, which we call deep
perceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of
computing distances in the image space, we compute distances between image
features extracted by deep neural networks. This metric better reflects
perceptually similarity of images and thus leads to better results. We show
three applications: autoencoder training, a modification of a variational
autoencoder, and inversion of deep convolutional networks. In all cases, the
generated images look sharp and resemble natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02648</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02648</id><created>2016-02-08</created><authors><author><keyname>Romashchenko</keyname><forenames>Andrei</forenames></author></authors><title>Coding in the fork network in the framework of Kolmogorov complexity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many statements from the classic information theory (the theory of Shannon's
entropy) have natural counterparts in the algorithmic information theory (in
the framework of Kolmogorov complexity). In this paper we discuss one simple
instance of the parallelism between Shannon's and Kolmogorov's theories: we
prove in the setting of Kolmogorov complexity a version of Wolf's
characterization of admissible rates for the fork network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02651</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02651</id><created>2016-02-08</created><authors><author><keyname>Garrido</keyname><forenames>Pablo</forenames></author><author><keyname>Valgaerts</keyname><forenames>Levi</forenames></author><author><keyname>Rehmsen</keyname><forenames>Ole</forenames></author><author><keyname>Thormaehlen</keyname><forenames>Thorsten</forenames></author><author><keyname>Perez</keyname><forenames>Patrick</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Automatic Face Reenactment</title><categories>cs.CV cs.GR</categories><comments>Proceedings of the 2014 IEEE Conference on Computer Vision and
  Pattern Recognition (8 pages)</comments><doi>10.1109/CVPR.2014.537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an image-based, facial reenactment system that replaces the face
of an actor in an existing target video with the face of a user from a source
video, while preserving the original target performance. Our system is fully
automatic and does not require a database of source expressions. Instead, it is
able to produce convincing reenactment results from a short source video
captured with an off-the-shelf camera, such as a webcam, where the user
performs arbitrary facial gestures. Our reenactment pipeline is conceived as
part image retrieval and part face transfer: The image retrieval is based on
temporal clustering of target frames and a novel image matching metric that
combines appearance and motion to select candidate frames from the source
video, while the face transfer uses a 2D warping strategy that preserves the
user's identity. Our system excels in simplicity as it does not rely on a 3D
face model, it is robust under head motion and does not require the source and
target performance to be similar. We show convincing reenactment results for
videos that we recorded ourselves and for low-quality footage taken from the
Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02656</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02656</id><created>2016-02-08</created><authors><author><keyname>Coto-Jim&#xe9;nez</keyname><forenames>Marvin</forenames></author><author><keyname>Goddard-Close</keyname><forenames>John</forenames></author></authors><title>LSTM Deep Neural Networks Postfiltering for Improving the Quality of
  Synthetic Voices</title><categories>cs.SD cs.NE</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in speech synthesis have produced systems capable of
outcome intelligible speech, but now researchers strive to create models that
more accurately mimic human voices. One such development is the incorporation
of multiple linguistic styles in various languages and accents.
  HMM-based Speech Synthesis is of great interest to many researchers, due to
its ability to produce sophisticated features with small footprint. Despite
such progress, its quality has not yet reached the level of the predominant
unit-selection approaches that choose and concatenate recordings of real
speech. Recent efforts have been made in the direction of improving these
systems.
  In this paper we present the application of Long-Short Term Memory Deep
Neural Networks as a Postfiltering step of HMM-based speech synthesis, in order
to obtain closer spectral characteristics to those of natural speech. The
results show how HMM-voices could be improved using this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02658</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02658</id><created>2016-02-08</created><updated>2016-02-17</updated><authors><author><keyname>Zahavy</keyname><forenames>Tom</forenames></author><author><keyname>Zrihem</keyname><forenames>Nir Ben</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Graying the black box: Understanding DQNs</title><categories>cs.LG cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there is a growing interest in using deep representations for
reinforcement learning. In this paper, we present a methodology and tools to
analyze Deep Q-networks (DQNs) in a non-blind matter. Using our tools we reveal
that the features learned by DQNs aggregate the state space in a hierarchical
fashion, explaining its success. Moreover we are able to understand and
describe the policies learned by DQNs for three different Atari2600 games and
suggest ways to interpret, debug and optimize of deep neural networks in
Reinforcement Learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02660</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02660</id><created>2016-02-08</created><authors><author><keyname>Dieleman</keyname><forenames>Sander</forenames></author><author><keyname>De Fauw</keyname><forenames>Jeffrey</forenames></author><author><keyname>Kavukcuoglu</keyname><forenames>Koray</forenames></author></authors><title>Exploiting Cyclic Symmetry in Convolutional Neural Networks</title><categories>cs.LG cs.CV cs.NE</categories><comments>10 pages, 6 figures, submitted to ICML 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classes of images exhibit rotational symmetry. Convolutional neural
networks are sometimes trained using data augmentation to exploit this, but
they are still required to learn the rotation equivariance properties from the
data. Encoding these properties into the network architecture, as we are
already used to doing for translation equivariance by using convolutional
layers, could result in a more efficient use of the parameter budget by
relieving the model from learning them. We introduce four operations which can
be inserted into neural network models as layers, and which can be combined to
make these models partially equivariant to rotations. They also enable
parameter sharing across different orientations. We evaluate the effect of
these architectural modifications on three datasets which exhibit rotational
symmetry and demonstrate improved performance with smaller models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02665</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02665</id><created>2016-02-08</created><authors><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>van de Leemput</keyname><forenames>Ingrid</forenames></author><author><keyname>Ruan</keyname><forenames>Guangchen</forenames></author></authors><title>The happiness paradox: your friends are happier than you</title><categories>cs.SI cs.CL cs.HC physics.soc-ph</categories><comments>15 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most individuals in social networks experience a so-called Friendship
Paradox: they are less popular than their friends on average. This effect may
explain recent findings that widespread social network media use leads to
reduced happiness. However the relation between popularity and happiness is
poorly understood. A Friendship paradox does not necessarily imply a Happiness
paradox where most individuals are less happy than their friends. Here we
report the first direct observation of a significant Happiness Paradox in a
large-scale online social network of $39,110$ Twitter users. Our results reveal
that popular individuals are indeed happier and that a majority of individuals
experience a significant Happiness paradox. The magnitude of the latter effect
is shaped by complex interactions between individual popularity, happiness, and
the fact that users cluster assortatively by level of happiness. Our results
indicate that the topology of online social networks and the distribution of
happiness in some populations can cause widespread psycho-social effects that
affect the well-being of billions of individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02666</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02666</id><created>2016-02-08</created><authors><author><keyname>Mandt</keyname><forenames>Stephan</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>A Variational Analysis of Stochastic Gradient Algorithms</title><categories>stat.ML cs.LG</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is an important algorithm in machine
learning. With constant learning rates, it is a stochastic process that, after
an initial phase of convergence, generates samples from a stationary
distribution. We show that SGD with constant rates can be effectively used as
an approximate posterior inference algorithm for probabilistic modeling.
Specifically, we show how to adjust the tuning parameters of SGD such as to
match the resulting stationary distribution to the posterior. This analysis
rests on interpreting SGD as a continuous-time stochastic process and then
minimizing the Kullback-Leibler divergence between its stationary distribution
and the target posterior. (This is in the spirit of variational inference.) In
more detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then
use properties of this process to derive the optimal parameters. This
theoretical framework also connects SGD to modern scalable inference
algorithms; we analyze the recently proposed stochastic gradient Fisher scoring
under this perspective. We demonstrate that SGD with properly chosen constant
rates gives a new way to optimize hyperparameters in probabilistic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02670</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02670</id><created>2016-02-08</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Dvo&#x159;&#xe1;k</keyname><forenames>Wolfgang</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Loitzenbauer</keyname><forenames>Veronika</forenames></author></authors><title>Model and Objective Separation with Conditional Lower Bounds:
  Disjunction is Harder than Conjunction</title><categories>cs.DS cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a model of a system and an objective, the model-checking question asks
whether the model satisfies the objective. We study polynomial-time problems in
two classical models, graphs and Markov Decision Processes (MDPs), with respect
to several fundamental $\omega$-regular objectives, e.g., Rabin and Streett
objectives. For many of these problems the best-known upper bounds are
quadratic or cubic, yet no super-linear lower bounds are known. In this work
our contributions are two-fold: First, we present several improved algorithms,
and second, we present the first conditional super-linear lower bounds based on
widely believed assumptions about the complexity of CNF-SAT and combinatorial
Boolean matrix multiplication. A separation result for two models with respect
to an objective means a conditional lower bound for one model that is strictly
higher than the existing upper bound for the other model, and similarly for two
objectives with respect to a model. Our results establish the following
separation results: (1) A separation of models (graphs and MDPs) for
disjunctive queries of reachability and B\&quot;uchi objectives. (2) Two kinds of
separations of objectives, both for graphs and MDPs, namely, (2a) the
separation of dual objectives such as reachability/safety (for disjunctive
questions) and Streett/Rabin objectives, and (2b) the separation of conjunction
and disjunction of multiple objectives of the same type such as safety,
B\&quot;uchi, and coB\&quot;uchi. In summary, our results establish the first model and
objective separation results for graphs and MDPs for various classical
$\omega$-regular objectives. Quite strikingly, we establish conditional lower
bounds for the disjunction of objectives that are strictly higher than the
existing upper bounds for the conjunction of the same objectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02672</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02672</id><created>2016-02-08</created><authors><author><keyname>Foerster</keyname><forenames>Jakob N.</forenames></author><author><keyname>Assael</keyname><forenames>Yannis M.</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Whiteson</keyname><forenames>Shimon</forenames></author></authors><title>Learning to Communicate to Solve Riddles with Deep Distributed Recurrent
  Q-Networks</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose deep distributed recurrent Q-networks (DDRQN), which enable teams
of agents to learn to solve communication-based coordination tasks. In these
tasks, the agents are not given any pre-designed communication protocol.
Therefore, in order to successfully communicate, they must first automatically
develop and agree upon their own communication protocol. We present empirical
results on two multi-agent learning problems based on well-known riddles,
demonstrating that DDRQN can successfully solve such tasks and discover elegant
communication protocols to do so. To our knowledge, this is the first time deep
reinforcement learning has succeeded in learning communication protocols. In
addition, we present ablation experiments that confirm that each of the main
components of the DDRQN architecture are critical to its success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02673</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02673</id><created>2016-02-08</created><authors><author><keyname>Loeliger</keyname><forenames>Hans-Andrea</forenames></author><author><keyname>Bruderer</keyname><forenames>Lukas</forenames></author><author><keyname>Malmberg</keyname><forenames>Hampus</forenames></author><author><keyname>Wadehn</keyname><forenames>Federico</forenames></author><author><keyname>Zalmai</keyname><forenames>Nour</forenames></author></authors><title>On Sparsity by NUV-EM, Gaussian Message Passing, and Kalman Smoothing</title><categories>cs.IT math.IT</categories><journal-ref>2016 Information Theory &amp; Applications Workshop (ITA), La Jolla,
  CA, Jan. 31 - Feb. 5, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normal priors with unknown variance (NUV) have long been known to promote
sparsity and to blend well with parameter learning by expectation maximization
(EM). In this paper, we advocate this approach for linear state space models
for applications such as the estimation of impulsive signals, the detection of
localized events, smoothing with occasional jumps in the state space, and the
detection and removal of outliers. The actual computations boil down to
multivariate-Gaussian message passing algorithms that are closely related to
Kalman smoothing. We give improved tables of Gaussian-message computations from
which such algorithms are easily synthesized, and we point out two preferred
such algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02675</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02675</id><created>2016-02-08</created><authors><author><keyname>Komeili</keyname><forenames>M.</forenames></author><author><keyname>Mirzaei</keyname><forenames>M.</forenames></author><author><keyname>Shabouei</keyname><forenames>M.</forenames></author></authors><title>Performance of 1-D and 2-D Lattice Boltzmann (LB) in Solution of the
  Shock Tube Problem</title><categories>cs.CE</categories><comments>in International Conference on Fascinating Advancement in Mechanical
  Engineering, FAME08-T30, India, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we presented a lattice Boltzmann with square grid for
compressible flow problems. Triple level velocity is considered for each cell.
Migration step use discrete velocity but continuous parameters are utilized to
calculate density, velocity, and energy. So, we called this semi-discrete
method. To evaluate the performance of the method the well-known shock tube
problem is solved, using 1-D and 2-D version of the lattice Boltzmann method.
The results of these versions are compared with each other and with the results
of the analytical solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02680</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02680</id><created>2016-02-08</created><authors><author><keyname>Shabouei</keyname><forenames>M.</forenames></author><author><keyname>Ebrahimi</keyname><forenames>R.</forenames></author><author><keyname>Body</keyname><forenames>K. Mazaheri</forenames></author></authors><title>Numerical Solution of Cylindrically Converging Shock Waves</title><categories>cs.CE</categories><comments>International Conference on Fascinating Advancement in Mechanical
  Engineering, FAME08-T38, India, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cylindrically converging shock wave was numerically simulated by solving
the Euler equations in cylindrical coordinates with TVD scheme and MUSCL
approach, using Roe's approximate Riemann solver and super-bee nonlinear
limiter. The present study used the in house code developed for this purpose.
The behavior of the solution in the vicinity of axis is investigated and the
results of the numerical solution are compared with the computed data given by
Payne, Lapidus, Abarbanel, and Goldberg, Sod, and Leutioff et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02685</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02685</id><created>2016-02-08</created><authors><author><keyname>Esteban</keyname><forenames>Crist&#xf3;bal</forenames></author><author><keyname>Staeck</keyname><forenames>Oliver</forenames></author><author><keyname>Yang</keyname><forenames>Yinchong</forenames></author><author><keyname>Tresp</keyname><forenames>Volker</forenames></author></authors><title>Predicting Clinical Events by Combining Static and Dynamic Information
  Using Recurrent Neural Networks</title><categories>cs.LG cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In clinical data sets we often find static information (e.g. gender of the
patients, blood type, etc.) combined with sequences of data that are recorded
during multiple hospital visits (e.g. medications prescribed, tests performed,
etc.). Recurrent Neural Networks (RNNs) have proven to be very successful for
modelling sequences of data in many areas of Machine Learning. In this work we
present an approach based on RNNs that is specifically designed for the
clinical domain and that combines static and dynamic information in order to
predict future events. We work with a database collected in the Charit\'{e}
Hospital in Berlin that contains all the information concerning patients that
underwent a kidney transplantation. After the transplantation three main
endpoints can occur: rejection of the kidney, loss of the kidney and death of
the patient. Our goal is to predict, given the Electronic Health Record of each
patient, whether any of those endpoints will occur within the next six or
twelve months after each visit to the clinic. We compared different types of
RNNs that we developed for this work, a model based on a Feedforward Neural
Network and a Logistic Regression model. We found that the RNN that we
developed based on Gated Recurrent Units provides the best performance for this
task. We also performed an additional experiment using these models to predict
next actions and found that for such use case the model based on a Feedforward
Neural Network outperformed the other models. Our hypothesis is that long-term
dependencies are not as relevant in this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02690</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02690</id><created>2016-02-08</created><authors><author><keyname>Tsai</keyname><forenames>Yueh-Ting</forenames></author><author><keyname>Su</keyname><forenames>Borching</forenames></author><author><keyname>Tsao</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Syu-Siang</forenames></author></authors><title>Robust Beamforming Against DoA Mismatch Using Subspace-Constrained
  Diagonal Loading</title><categories>math.OC cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to IEEE SP Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a new subspace-constrained diagonal loading (SSC-DL) method is
presented for robust beamforming against the issue of a mismatched direction of
arrival (DoA), based on an extension to the well known diagonal loading (DL)
technique. One important difference of the proposed SSC-DL from conventional DL
is that it imposes an additional constraint to restrict the optimal weight
vector within a subspace whose basis vectors are determined by a number of
angles neighboring to the assumed DoA. Unlike many existing methods which
resort to a beamwidth expansion, the weight vector produced by SSC-DL has a
relatively small beamwidth around the DoA of the target signal. Yet, the SSC-DL
beamformer has a great interference suppression level, thereby achieving an
improved overall SINR performance. Simulation results suggest the proposed
method has a near-to-optimal SINR performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02692</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02692</id><created>2016-02-08</created><authors><author><keyname>Allgaier</keyname><forenames>Joachim</forenames></author></authors><title>Science on YouTube: What user find when they search for climate science
  and climate manipulation</title><categories>cs.CY cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online video-sharing sites such as YouTube are very popular and also used by
a lot of people to obtain knowledge and information, also on science, health
and technology. Technically they could be valuable tools for the public
communication of science and technology, but the users of YouTube are also
confronted with conspiracy theories and erroneous and misleading information
that deviates from scientific consensus views. This contribution details the
results of a study that investigates what kind of information users find when
they are searching for climate science and climate manipulation topics on
YouTube and whether this information corresponds with or challenges scientific
consensus views. An innovative methodological approach using the anonymization
network Tor is introduced for drawing randomized samples of YouTube videos.
This approach was used to select and examine a sample of 140 YouTube videos on
climate topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02695</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02695</id><created>2016-02-08</created><authors><author><keyname>Most&#xe9;faoui</keyname><forenames>Achour</forenames><affiliation>LINA</affiliation></author><author><keyname>Raynal</keyname><forenames>Michel</forenames><affiliation>ASAP</affiliation></author></authors><title>Two-Bit Messages are Sufficient to Implement Atomic Read/Write Registers
  in Crash-prone Systems</title><categories>cs.DC cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Atomic registers are certainly the most basic objects of computing science.
Their implementation on top of an n-process asynchronous message-passing system
has received a lot of attention. It has been shown that t \textless{} n/2
(where t is the maximal number of processes that may crash) is a necessary and
sufficient requirement to build an atomic register on top of a crash-prone
asynchronous message-passing system. Considering such a context, this paper
presents an algorithm which implements a single-writer multi-reader atomic
register with four message types only, and where no message needs to carry
control information in addition to its type. Hence, two bits are sufficient to
capture all the control information carried by all the implementation messages.
Moreover, the messages of two types need to carry a data value while the
messages of the two other types carry no value at all. As far as we know, this
algorithm is the first with such an optimality property on the size of control
information carried by messages. It is also particularly efficient from a time
complexity point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02697</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02697</id><created>2016-02-08</created><updated>2016-02-18</updated><authors><author><keyname>Papernot</keyname><forenames>Nicolas</forenames></author><author><keyname>McDaniel</keyname><forenames>Patrick</forenames></author><author><keyname>Goodfellow</keyname><forenames>Ian</forenames></author><author><keyname>Jha</keyname><forenames>Somesh</forenames></author><author><keyname>Celik</keyname><forenames>Z. Berkay</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author></authors><title>Practical Black-Box Attacks against Deep Learning Systems using
  Adversarial Examples</title><categories>cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in deep learning have led to the broad adoption of Deep Neural
Networks (DNNs) to a range of important machine learning problems, e.g.,
guiding autonomous vehicles, speech recognition, malware detection. Yet,
machine learning models, including DNNs, were shown to be vulnerable to
adversarial samples-subtly (and often humanly indistinguishably) modified
malicious inputs crafted to compromise the integrity of their outputs.
Adversarial examples thus enable adversaries to manipulate system behaviors.
Potential attacks include attempts to control the behavior of vehicles, have
spam content identified as legitimate content, or have malware identified as
legitimate software. Adversarial examples are known to transfer from one model
to another, even if the second model has a different architecture or was
trained on a different set. We introduce the first practical demonstration that
this cross-model transfer phenomenon enables attackers to control a remotely
hosted DNN with no access to the model, its parameters, or its training data.
In our demonstration, we only assume that the adversary can observe outputs
from the target DNN given inputs chosen by the adversary. We introduce the
attack strategy of fitting a substitute model to the input-output pairs in this
manner, then crafting adversarial examples based on this auxiliary model. We
evaluate the approach on existing DNN datasets and real-world settings. In one
experiment, we force a DNN supported by MetaMind (one of the online APIs for
DNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude with
experiments exploring why adversarial samples transfer between DNNs, and a
discussion on the applicability of our attack when targeting machine learning
algorithms distinct from DNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02698</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02698</id><created>2016-02-08</created><authors><author><keyname>Elkhatib</keyname><forenames>Yehia</forenames></author></authors><title>Defining Cross-Cloud Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen an increasing number of cross-cloud architectures,
i.e. systems that span across cloud provisioning boundaries. However, the cloud
computing world still lacks any standards in terms of programming interfaces,
which has a knock-on effect on the costs associated with interoperability and
severely limits the flexibility and portability of applications and virtual
infrastructures. This paper outlines the different types of cross-cloud
systems, and the associated design decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02701</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02701</id><created>2016-02-08</created><authors><author><keyname>Mensch</keyname><forenames>Arthur</forenames><affiliation>PARIETAL</affiliation></author><author><keyname>Varoquaux</keyname><forenames>Ga&#xeb;l</forenames><affiliation>PARIETAL</affiliation></author><author><keyname>Thirion</keyname><forenames>Bertrand</forenames><affiliation>PARIETAL</affiliation></author></authors><title>Compressed Online Dictionary Learning for Fast fMRI Decomposition</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for fast resting-state fMRI spatial decomposi-tions of
very large datasets, based on the reduction of the temporal dimension before
applying dictionary learning on concatenated individual records from groups of
subjects. Introducing a measure of correspondence between spatial
decompositions of rest fMRI, we demonstrates that time-reduced dictionary
learning produces result as reliable as non-reduced decompositions. We also
show that this reduction significantly improves computational scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02704</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02704</id><created>2016-02-08</created><authors><author><keyname>Blaum</keyname><forenames>Mario</forenames></author><author><keyname>Hetzler</keyname><forenames>Steven R.</forenames></author></authors><title>Integrated Interleaved Codes as Locally Recoverable Codes: Properties
  and Performance</title><categories>cs.IT math.IT</categories><comments>24 pages, 5 figures and 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considerable interest has been paid in recent literature to codes combining
local and global properties for erasure correction. Applications are in cloud
type of implementations, in which fast recovery of a failed storage device is
important, but additional protection is required in order to avoid data loss,
and in RAID type of architectures, in which total device failures coexist with
silent failures at the page or sector level in each device. Existing solutions
to these problems require in general relatively large finite fields. The
techniques of Integrated Interleaved Codes (which are closely related to
Generalized Concatenated Codes) are proposed to reduce significantly the size
of the finite field, and it is shown that when the parameters of these codes
are judiciously chosen, their performance may be competitive with the one of
codes optimizing the minimum distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02706</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02706</id><created>2016-02-08</created><authors><author><keyname>Audiffren</keyname><forenames>Julien</forenames><affiliation>CMLA</affiliation></author><author><keyname>Liva</keyname><forenames>Ralaivola</forenames><affiliation>LIF</affiliation></author></authors><title>Indistinguishable Bandits Dueling with Decoys on a Poset</title><categories>cs.LG cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adress the problem of dueling bandits defined on partially ordered sets,
or posets. In this setting, arms may not be comparable, and there may be
several (incomparable) optimal arms. We propose an algorithm, UnchainedBandits,
that efficiently finds the set of optimal arms of any poset even when pairs of
comparable arms cannot be distinguished from pairs of incomparable arms, with a
set of minimal assumptions. This algorithm relies on the concept of decoys,
which stems from social psychology. For the easier case where the
incomparability information may be accessible, we propose a second algorithm,
SlicingBandits, which takes advantage of this information and achieves a very
significant gain of performance compared to UnchainedBandits. We provide
theoretical guarantees and experimental evaluation for both algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02710</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02710</id><created>2016-02-05</created><authors><author><keyname>Grandi</keyname><forenames>Umberto</forenames></author><author><keyname>Lorini</keyname><forenames>Emiliano</forenames></author><author><keyname>Perrussel</keyname><forenames>Laurent</forenames></author></authors><title>Strategic disclosure of opinions on a social network</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the strategic aspects of social influence in a society of agents
linked by a trust network, introducing a new class of games called games of
influence. A game of influence is an infinite repeated game with incomplete
information in which, at each stage of interaction, an agent can make her
opinions visible (public) or invisible (private) in order to influence other
agents' opinions. The influence process is mediated by a trust network, as we
assume that the opinion of a given agent is only affected by the opinions of
those agents that she considers trustworthy (i.e., the agents in the trust
network that are directly linked to her). Each agent is endowed with a goal,
expressed in a suitable temporal language inspired from linear temporal logic
(LTL). We show that games of influence provide a simple abstraction to explore
the effects of the trust network structure on the agents' behaviour, by
considering solution concepts from game-theory such as Nash equilibrium, weak
dominance and winning strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02715</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02715</id><created>2016-02-03</created><updated>2016-03-07</updated><authors><author><keyname>Ambroszkiewicz</keyname><forenames>Stanislaw</forenames></author></authors><title>On the notion of &quot;von Neumann vicious circle&quot; coined by John Backus</title><categories>cs.PL</categories><comments>In this version (March 7, 2016) only small error in Fig. 3 was
  removed. Keywords: von Neumann bottleneck, von Neumann vicious circle,
  non-von Neumann computer architectures, lazy evaluation, higher order
  functionals, functional programming languages, higher order HDL (Hardware
  Description Language)</comments><msc-class>03D</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;The von Neumann vicious circle&quot; means that non-von Neumann computer
architectures cannot be developed because of the lack of widely available and
effective non-von Neumann languages. New languages cannot be created because of
lack of conceptual foundations for non-von Neumann architectures. The reason is
that programming languages are high-level abstract isomorphic copies of von
Neumann computer architectures. This constitutes the current paradigm in
Computer Science. The paradigm is equivalent to the predominant view that
computations on higher order objects (functionals) can be done only
symbolically, i.e. by term rewriting. The paper is a short introduction to the
papers arXiv:1501.03043 and arXiv:1510.02787 trying to break the paradigm by
introducing a framework that may be seen as a higher order functional HDL
(Hardware Description Language).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02718</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02718</id><created>2016-02-08</created><updated>2016-03-08</updated><authors><author><keyname>Psomas</keyname><forenames>Constantinos</forenames></author><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author></authors><title>Impact of Directionality on Interference Mitigation in Full-Duplex
  Cellular Network</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider two fundamental full-duplex (FD) architectures,
two-node and three-node, in the context of cellular networks where the
terminals employ directional antennas. The simultaneous transmission and
reception of data in non-orthogonal channels makes FD radio a potential
solution for the currently limited spectrum. However, its implementation
generates high levels of interference either in the form of loopback
interference (LI) from the output to the input antenna of a transceiver or in
the form of co-channel interference in large-scale multicell networks due to
the large number of active links. Using a stochastic geometry model, we
investigate how directional antennas can control and mitigate the co-channel
interference. Furthermore, we provide a model which characterizes the way
directional antennas manage the LI in order to passively suppress it. Our
results show that both architectures can benefit significantly by the
employment of directional antennas. Finally, we consider the case where both
architectures are employed in the network and derive the optimal values for the
density fraction of each architecture which maximize the success probability
and the network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02720</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02720</id><created>2016-02-08</created><authors><author><keyname>Uss</keyname><forenames>M. L.</forenames></author><author><keyname>Vozel</keyname><forenames>B.</forenames></author><author><keyname>Lukin</keyname><forenames>V. V.</forenames></author><author><keyname>Chehdi</keyname><forenames>K.</forenames></author></authors><title>Multimodal Remote Sensing Image Registration with Accuracy Estimation at
  Local and Global Scales</title><categories>cs.CV</categories><comments>44 pages, 7 figures, 4 tables, 47 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates and takes advantage of estimation of registration
accuracy for mono- and multi-modal pairs of remote sensing images, following an
integrated framework from local to global scales. At the local scale, the
Cramer-Rao lower bound on parameter estimation error is estimated for
characterizing registration accuracy of local fragment correspondence between a
coarsely registered pair of images. Each local correspondence is assigned its
estimated registration accuracy dependent on local image texture and noise
properties. Opposite to the standard approach, where registration accuracy is
found a posteriori at the output of the registration process, such valuable
information is used by us as additional a priori information in the
registration process at global scale. It greatly helps detecting and discarding
outliers and refining the estimation of geometrical transformation model
parameters. Based on these ideas, a new area-based registration method called
RAE (Registration with Accuracy Estimation) is proposed. The RAE method is able
to provide registration accuracy at the global scale as error estimation
covariance matrix of geometrical transformation model parameters that can be
used to estimate point-wise registration Standard Deviation (SD). This accuracy
does not rely on any ground truth and characterizes each pair of registered
images individually. The RAE method is proved successful with reaching subpixel
accuracy while registering the three complex multimodal and multitemporal image
pairs: optical to radar, optical to Digital Elevation Model (DEM) images and
DEM to radar images. Other methods employed in comparisons fail to provide
accurate results on the same test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02722</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02722</id><created>2016-02-08</created><updated>2016-03-01</updated><authors><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Langford</keyname><forenames>John</forenames></author></authors><title>Contextual-MDPs for PAC-Reinforcement Learning with Rich Observations</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study a new tractable model for reinforcement learning with
high-dimensional observation called Contextual-MDPs, generalizing contextual
bandits to a sequential decision making setting. These models require an agent
to take actions based on high-dimensional observations (features) with the goal
of achieving long-term performance competitive with a large set of policies.
Since the size of the observation space is a primary obstacle to
sample-efficient learning, Contextual-MDPs are assumed to be summarizable by a
small number of hidden states. In this setting, we design a new reinforcement
learning algorithm that engages in global exploration while using a function
class to approximate future performance. We also establish a sample complexity
guarantee for this algorithm, proving that it learns near optimal behavior
after a number of episodes that is polynomial in all relevant parameters,
logarithmic in the number of policies, and independent of the size of the
observation space. This represents an exponential improvement on the sample
complexity of all existing alternative approaches and provides theoretical
justification for reinforcement learning with function approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02726</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02726</id><created>2016-02-08</created><authors><author><keyname>Johnstone</keyname><forenames>Patrick R.</forenames></author><author><keyname>Moulin</keyname><forenames>Pierre</forenames></author></authors><title>Local and Global Convergence of a General Inertial Proximal Splitting
  Scheme</title><categories>math.OC cs.LG math.NA</categories><comments>33 pages 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with convex composite minimization problems in a
Hilbert space. In these problems, the objective is the sum of two closed,
proper, and convex functions where one is smooth and the other admits a
computationally inexpensive proximal operator. We analyze a general family of
inertial proximal splitting algorithms (GIPSA) for solving such problems. We
establish finiteness of the sum of squared increments of the iterates and
optimality of the accumulation points. Weak convergence of the entire sequence
then follows if the minimum is attained. Our analysis unifies and extends
several previous results.
  We then focus on $\ell_1$-regularized optimization, which is the ubiquitous
special case where the nonsmooth term is the $\ell_1$-norm. For certain
parameter choices, GIPSA is amenable to a local analysis for this problem. For
these choices we show that GIPSA achieves finite &quot;active manifold
identification&quot;, i.e. convergence in a finite number of iterations to the
optimal support and sign, after which GIPSA reduces to minimizing a local
smooth function. Local linear convergence then holds under certain conditions.
We determine the rate in terms of the inertia, stepsize, and local curvature.
Our local analysis is applicable to certain recent variants of the Fast
Iterative Shrinkage-Thresholding Algorithm (FISTA), for which we establish
active manifold identification and local linear convergence. Our analysis
motivates the use of a momentum restart scheme in these FISTA variants to
obtain the optimal local linear convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02737</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02737</id><created>2016-02-08</created><authors><author><keyname>Li</keyname><forenames>Yuanxin</forenames></author><author><keyname>Sun</keyname><forenames>Yue</forenames></author><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Low-Rank Positive Semidefinite Matrix Recovery from Quadratic
  Measurements with Outliers</title><categories>cs.IT math.IT</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of estimating a low-rank positive semidefinite (PSD)
matrix $\boldsymbol{X}\in\mathbb{R}^{n\times n}$ from a set of measurements
$z_i= \boldsymbol{a}_i^T\boldsymbol{X}\boldsymbol{a}_i$, $i=1,\ldots, m$, which
are quadratic in the sensing vectors $\boldsymbol{a}_i$'s composed of i.i.d.
standard Gaussian entries. This problem arises from applications such as
covariance sketching, quantum space tomography, and power spectrum estimation.
Specifically, we consider a convex optimization problem that seeks the PSD
matrix with minimum $\ell_1$ norm of the observation residual. The advantage of
our algorithm is that it is free of parameters, therefore eliminating the need
for tuning parameters and allowing easy implementations. We establish that with
high probability, a rank-r PSD matrix $\boldsymbol{X}$ of size-n can be exactly
recovered from $\mathcal{O}(nr^{2})$ measurements, even when a fraction of the
measurements is corrupted by outliers with arbitrary magnitudes. Moreover, the
recovery is also stable against bounded noise. With the additional information
of the rank of the PSD matrix, a non-convex algorithm based on subgradient
descent is proposed that demonstrates superior empirical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02739</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02739</id><created>2016-02-08</created><authors><author><keyname>Anaya</keyname><forenames>Maria</forenames></author><author><keyname>Anipchenko-Ulaj</keyname><forenames>Olga</forenames></author><author><keyname>Ashfaq</keyname><forenames>Aisha</forenames></author><author><keyname>Chiu</keyname><forenames>Joyce</forenames></author><author><keyname>Kaiser</keyname><forenames>Mahedi</forenames></author><author><keyname>Ohsawa</keyname><forenames>Max Shoji</forenames></author><author><keyname>Owen</keyname><forenames>Megan</forenames></author><author><keyname>Pavlechko</keyname><forenames>Ella</forenames></author><author><keyname>John</keyname><forenames>Katherine St.</forenames></author><author><keyname>Suleria</keyname><forenames>Shivam</forenames></author><author><keyname>Thompson</keyname><forenames>Keith</forenames></author><author><keyname>Yap</keyname><forenames>Corrine</forenames></author></authors><title>On Determining if Tree-based Networks Contain Fixed Trees</title><categories>q-bio.PE cs.DS</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address an open question of Francis and Steel about phylogenetic networks
and trees. They give a polynomial time algorithm to decide if a phylogenetic
network, N, is tree-based and pose the problem: given a fixed tree T and
network N, is N based on T? We show that it is NP-hard to decide, by reduction
from 3-Dimensional Matching (3DM), and further, that the problem is fixed
parameter tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02740</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02740</id><created>2016-02-08</created><authors><author><keyname>Kronenburg</keyname><forenames>M. J.</forenames></author></authors><title>Toom-Cook Multiplication: Some Theoretical and Practical Aspects</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Toom-Cook multiprecision multiplication is a well-known multiprecision
multiplication method, which can make use of multiprocessor systems. In this
paper the Toom-Cook complexity is derived, some explicit proofs of the
Toom-Cook interpolation method are given, the even-odd method for interpolation
is explained, and certain aspects of a 32-bit C++ and assembler implementation,
which is in development, are discussed. A performance graph of this
implementation is provided. The Toom-Cook method can also be used to
multithread other types of multiplication, which is demonstrated for 32-bit GMP
FFT multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02743</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02743</id><created>2016-02-06</created><authors><author><keyname>Brand</keyname><forenames>Michael</forenames></author><author><keyname>Dowe</keyname><forenames>David L.</forenames></author></authors><title>The IMP game: Learnability, approximability and adversarial learning
  beyond $\Sigma^0_1$</title><categories>cs.LO cs.AI cs.CC cs.FL</categories><comments>23 pages</comments><msc-class>68Q32 (Primary) 68Q05, 68T42, 68T05</msc-class><acm-class>F.1.1; F.4.1; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a problem set-up we call the Iterated Matching Pennies (IMP)
game and show that it is a powerful framework for the study of three problems:
adversarial learnability, conventional (i.e., non-adversarial) learnability and
approximability. Using it, we are able to derive the following theorems. (1) It
is possible to learn by example all of $\Sigma^0_1 \cup \Pi^0_1$ as well as
some supersets; (2) in adversarial learning (which we describe as a
pursuit-evasion game), the pursuer has a winning strategy (in other words,
$\Sigma^0_1$ can be learned adversarially, but $\Pi^0_1$ not); (3) some
languages in $\Pi^0_1$ cannot be approximated by any language in $\Sigma^0_1$.
  We show corresponding results also for $\Sigma^0_i$ and $\Pi^0_i$ for
arbitrary $i$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02744</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02744</id><created>2016-02-07</created><updated>2016-02-10</updated><authors><author><keyname>Gluskin</keyname><forenames>Emanuel</forenames></author></authors><title>The physical and circuit-theoretic significance of the Memristor : Full
  version</title><categories>cs.ET</categories><comments>23 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is observed that the inductive and capacitive features of the memristor
reflect (and are a quintessence of) such features of any resistor. The very
presence of the voltage and current state variables, associated by their
electrodynamics sense with electrical and magnetic fields, in the resistive
characteristic v = f(i), forces any resister to accumulate some magnetic and
electrostatic fields and energies around itself, i.e. L and C elements are
always present. From the circuit-theoretic point of view, the role of the
memristor is seen, first of all, in the elimination of the use of a unique
v(i). This makes circuits with hysteresis characteristics relevant, and also
suggests that the concept of memristor should influence the basic problem of
definition of nonlinearity. Since the memristor mainly originates from the
resistor, it was found necessary to overview some unusual cases of resistive
circuits. The present opinion is that the framework of basic circuit theory and
its connection with applications should be logically expanded in order to
naturally include the new element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02747</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02747</id><created>2016-02-07</created><authors><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author></authors><title>Independent sets and cuts in large-girth regular graphs</title><categories>math.CO cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a local algorithm producing an independent set of expected size
$0.44533n$ on large-girth 3-regular graphs and $0.40407n$ on large-girth
4-regular graphs. We also construct a cut (or bisection or bipartite subgraph)
with $1.34105n$ edges on large-girth 3-regular graphs. These decrease the gaps
between the best known upper and lower bounds from $0.0178$ to $0.01$, from
$0.0242$ to $0.0123$ and from $0.0724$ to $0.0616$, respectively. We are using
local algorithms, therefore, the method also provides upper bounds for the
fractional coloring numbers of $1 / 0.44533 \approx 2.24554$ and $1 / 0.40407
\approx 2.4748$ and fractional edge coloring number $1.5 / 1.34105 \approx
1.1185$. Our algorithms are applications of the technique introduced by Hoppen
and Wormald.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02788</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02788</id><created>2016-02-08</created><authors><author><keyname>Aggarwal</keyname><forenames>Divesh</forenames></author><author><keyname>Bri&#xeb;t</keyname><forenames>Jop</forenames></author></authors><title>Revisiting the Sanders-Freiman-Ruzsa Theorem in $\mathbb{F}_p^n$ and its
  Application to Non-malleable Codes</title><categories>cs.DM cs.CC cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-malleable codes (NMCs) protect sensitive data against degrees of
corruption that prohibit error detection, ensuring instead that a corrupted
codeword decodes correctly or to something that bears little relation to the
original message. The split-state model, in which codewords consist of two
blocks, considers adversaries who tamper with either block arbitrarily but
independently of the other. The simplest construction in this model, due to
Aggarwal, Dodis, and Lovett (STOC'14), was shown to give NMCs sending k-bit
messages to $O(k^7)$-bit codewords. It is conjectured, however, that the
construction allows linear-length codewords. Towards resolving this conjecture,
we show that the construction allows for code-length $O(k^5)$. This is achieved
by analysing a special case of Sanders's Bogolyubov-Ruzsa theorem for general
Abelian groups. Closely following the excellent exposition of this result for
the group $\mathbb{F}_2^n$ by Lovett, we expose its dependence on $p$ for the
group $\mathbb{F}_p^n$, where $p$ is a prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02794</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02794</id><created>2016-02-01</created><authors><author><keyname>Kim</keyname><forenames>Kyeong Soo</forenames></author></authors><title>Comments on &quot;On Clock Synchronization Algorithms for Wireless Sensor
  Networks Under Unknown Delay&quot;</title><categories>cs.DC cs.NI cs.SY</categories><comments>2 pages, 1 figure, submitted to IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalization of the maximum-likelihood-like estimator for clock skew by
Leng and Wu in the above paper is erroneous because the correlation of the
noise components in the model is not taken into account in the derivation of
the maximum likelihood estimator, its performance bound, and the optimal
selection of the gap between two subtracting time stamps. This comment
investigates the issue of noise correlation in the model and provides the range
of the gap for which the maximum likelihood estimator and its performance bound
are valid and corrects the optimal selection of the gap based on the provided
range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02800</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02800</id><created>2016-02-08</created><authors><author><keyname>Kasis</keyname><forenames>Andreas</forenames></author><author><keyname>Devane</keyname><forenames>Eoin</forenames></author><author><keyname>Lestas</keyname><forenames>Ioannis</forenames></author></authors><title>Primary frequency regulation with load-side participation: stability and
  optimality</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to design distributed generation and demand control
schemes for primary frequency regulation in power networks that guarantee
asymptotic stability and ensure fairness of allocation. We impose a passivity
condition on net power supply variables and provide explicit steady state
conditions on a general class of generation and demand control dynamics that
ensure convergence of solutions to equilibria that solve an appropriately
constructed network optimization problem. We also show that the inclusion of
controllable demand results in a drop in steady state frequency deviations. We
discuss how various classes of dynamics used in recent studies fit within our
framework and show that this allows for less conservative stability and
optimality conditions. We illustrate our results with simulations on the IEEE
68 bus system and observe that both static and dynamic demand response schemes
that fit within our framework offer improved transient and steady state
behavior compared with control of generation alone. The dynamic scheme is also
seen to enhance the robustness of the system to time-delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02822</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02822</id><created>2016-02-08</created><authors><author><keyname>Dai</keyname><forenames>Xiyang</forenames></author><author><keyname>Khamis</keyname><forenames>Sameh</forenames></author><author><keyname>Zhang</keyname><forenames>Yangmuzi</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes
  On Second Order Statistics</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse representations have been successfully applied to signal processing,
computer vision and machine learning. Currently there is a trend to learn
sparse models directly on structure data, such as region covariance. However,
such methods when combined with region covariance often require complex
computation. We present an approach to transform a structured sparse model
learning problem to a traditional vectorized sparse modeling problem by
constructing a Euclidean space representation for region covariance matrices.
Our new representation has multiple advantages. Experiments on several vision
tasks demonstrate competitive performance with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02823</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02823</id><created>2016-02-08</created><authors><author><keyname>Tygert</keyname><forenames>Mark</forenames></author></authors><title>Poor starting points in machine learning</title><categories>cs.LG cs.NE math.OC stat.ML</categories><comments>11 pages, 3 figures, 1 table; this initial version is literally
  identical to that circulated among a restricted audience over a month ago</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poor (even random) starting points for learning/training/optimization are
common in machine learning. In many settings, the method of Robbins and Monro
(online stochastic gradient descent) is known to be optimal for good starting
points, but may not be optimal for poor starting points -- indeed, for poor
starting points Nesterov acceleration can help during the initial iterations,
even though Nesterov methods not designed for stochastic approximation could
hurt during later iterations. The common practice of training with nontrivial
minibatches enhances the advantage of Nesterov acceleration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02830</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02830</id><created>2016-02-08</created><updated>2016-02-29</updated><authors><author><keyname>Courbariaux</keyname><forenames>Matthieu</forenames></author><author><keyname>Hubara</keyname><forenames>Itay</forenames></author><author><keyname>Soudry</keyname><forenames>Daniel</forenames></author><author><keyname>El-Yaniv</keyname><forenames>Ran</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Binarized Neural Networks: Training Deep Neural Networks with Weights
  and Activations Constrained to +1 or -1</title><categories>cs.LG</categories><comments>11 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method to train Binarized Neural Networks (BNNs) - neural
networks with binary weights and activations at run-time and when computing the
parameters' gradient at train-time. We conduct two sets of experiments, each
based on a different framework, namely Torch7 and Theano, where we train BNNs
on MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results.
During the forward pass, BNNs drastically reduce memory size and accesses, and
replace most arithmetic operations with bit-wise operations, which might lead
to a great increase in power-efficiency. Last but not least, we wrote a binary
matrix multiplication GPU kernel with which it is possible to run our MNIST BNN
7 times faster than with an unoptimized GPU kernel, without suffering any loss
in classification accuracy. The code for training and running our BNNs is
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02831</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02831</id><created>2016-02-08</created><authors><author><keyname>Butler</keyname><forenames>Brian K.</forenames></author></authors><title>Minimum Distances of the QC-LDPC Codes in IEEE 802 Communication
  Standards</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE ISIT 2016. 5 pages, 2 figures, and 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work applies earlier results on Quasi-Cyclic (QC) LDPC codes to the
codes specified in six separate IEEE 802 standards, specifying wireless
communications from 54 MHz to 60 GHz. First, we examine the weight matrices
specified to upper bound the codes' minimum distance independent of block
length. Next, we search for the minimum distance achieved for the parity check
matrices selected at each block length. Finally, solutions to the computational
challenges encountered are addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02834</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02834</id><created>2016-02-08</created><authors><author><keyname>Salim</keyname><forenames>Omar H.</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Nasi</keyname><forenames>Ali A.</forenames></author><author><keyname>Wang</keyname><forenames>Gengkun</forenames></author><author><keyname>Mehrpouyan</keyname><forenames>Hani</forenames></author></authors><title>Joint Data Detection and Phase Noise Mitigation for Light Field Video
  Transmission in MIMO-OFDM Systems</title><categories>cs.MM cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies in the literature for video transmission over wireless
communication systems focused on combating the effects of additive channel
noise and fading channels without taking the impairments in the physical layer
such as phase noise (PHN) into account. Oscillator phase noise impairs the
performance of multi-input multi-output- orthogonal frequency division
multiplexing (MIMO-OFDM) systems in providing high data rates for video
applications and may lead to decoding failure. In this paper, we propose a
light field (LF) video transmission system in wireless channels, and analyze
joint data detection and phase mitigation in MIMO-OFDM systems for LF video
transmission. The signal model and rate-distortion (RD) model for LF video
transmission in the presence of multiple PHNs are discussed. Moreover, we
propose an iterative algorithm based on the extended Kalman filter for joint
data detection and PHN tracking. Numerical results show that the proposed
detector can significantly improve the average bit-error rate (BER) and
peak-to-noise ratio (PSNR) performance for LF video transmission compared to
existing algorithms. Moreover, the BER and PSNR performance of the proposed
system is closer to that of the ideal case of perfect PHN estimation. Finally,
it is demonstrated that the proposed system model and algorithm are well suited
for LF video transmission in wireless channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02841</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02841</id><created>2016-02-08</created><authors><author><keyname>Alexeev</keyname><forenames>Nikita</forenames></author><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author></authors><title>Combinatorial Scoring of Phylogenetic Networks</title><categories>q-bio.PE cs.DS math.CO</categories><comments>14 pages; 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Construction of phylogenetic trees and networks for extant species from their
characters represents one of the key problems in phylogenomics. While solution
to this problem is not always uniquely defined and there exist multiple methods
for tree/network construction, it becomes important to measure how well
constructed networks capture the given character relationship across the
species.
  In the current study, we propose a novel method for measuring the specificity
of a given phylogenetic network in terms of the total number of distributions
of character states at the leaves that the network may impose. While for binary
phylogenetic trees, this number has an exact formula and depends only on the
number of leaves and character states but not on the tree topology, the
situation is much more complicated for non-binary trees or networks.
Nevertheless, we develop an algorithm for combinatorial enumeration of such
distributions, which is applicable for arbitrary trees and networks under some
reasonable assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02842</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02842</id><created>2016-02-08</created><authors><author><keyname>Tran</keyname><forenames>Truyen</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Collaborative filtering via sparse Markov random fields</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems play a central role in providing individualized access to
information and services. This paper focuses on collaborative filtering, an
approach that exploits the shared structure among mind-liked users and similar
items. In particular, we focus on a formal probabilistic framework known as
Markov random fields (MRF). We address the open problem of structure learning
and introduce a sparsity-inducing algorithm to automatically estimate the
interaction structures between users and between items. Item-item and user-user
correlation networks are obtained as a by-product. Large-scale experiments on
movie recommendation and date matching datasets demonstrate the power of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02845</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02845</id><created>2016-02-08</created><updated>2016-02-10</updated><authors><author><keyname>Riquelme</keyname><forenames>Carlos</forenames></author><author><keyname>Johari</keyname><forenames>Ramesh</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author></authors><title>Online Active Linear Regression via Thresholding</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online active learning to collect data for
regression modeling. Specifically, we consider a decision maker that faces a
limited experimentation budget but must efficiently learn an underlying linear
population model. Our goal is to develop algorithms that provide substantial
gains over passive random sampling of observations. To that end, our main
contribution is a novel threshold-based algorithm for selection of
observations; we characterize its performance and related lower bounds. We also
apply our approach successfully to regularized regression. Simulations suggest
the algorithm is remarkably robust: it provides significant benefits over
passive random sampling even in several real-world datasets that exhibit high
nonlinearity and high dimensionality --- significantly reducing the mean and
variance of the squared error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02847</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02847</id><created>2016-02-08</created><authors><author><keyname>Azami</keyname><forenames>Hamed</forenames></author><author><keyname>Fernandez</keyname><forenames>Alberto</forenames></author><author><keyname>Escudero</keyname><forenames>Javier</forenames></author></authors><title>Refined Multiscale Fuzzy Entropy based on Standard Deviation for
  Biomedical Signal Analysis</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiscale entropy (MSE) has been a prevalent algorithm to quantify the
complexity of fluctuations in the local mean value of biomedical time series.
Recent developments in the field have tried to improve the MSE by reducing its
variability in large scale factors. On the other hand, there has been recent
interest in using other statistical moments than the mean, i.e. variance, in
the coarse-graining step of the MSE. Building on these trends, here we
introduce the so-called refined composite multiscale fuzzy entropy based on the
standard deviation (RCMFE{\sigma}) to quantify the dynamical properties of
spread over multiple time scales. We demonstrate the dependency of the
RCMFE{\sigma}, in comparison with other multiscale approaches, on several
straightforward signal processing concepts using a set of synthetic signals. We
also investigate the complementarity of using the standard deviation instead of
the mean in the coarse-graining process using magnetoencephalograms in
Alzheimer disease and publicly available electroencephalograms recorded from
focal and non-focal areas in epilepsy. Our results indicate that RCMFE{\sigma}
offers complementary information to that revealed by classical coarse-graining
approaches and that it has superior performance to distinguish different types
of physiological activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02850</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02850</id><created>2016-02-08</created><authors><author><keyname>Tang</keyname><forenames>Bo</forenames></author><author><keyname>Kay</keyname><forenames>Steven</forenames></author><author><keyname>He</keyname><forenames>Haibo</forenames></author></authors><title>Toward Optimal Feature Selection in Naive Bayes for Text Categorization</title><categories>stat.ML cs.CL cs.IR cs.LG</categories><comments>This paper has been submitted to the IEEE Trans. Knowledge and Data
  Engineering. 14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated feature selection is important for text categorization to reduce
the feature size and to speed up the learning process of classifiers. In this
paper, we present a novel and efficient feature selection framework based on
the Information Theory, which aims to rank the features with their
discriminative capacity for classification. We first revisit two information
measures: Kullback-Leibler divergence and Jeffreys divergence for binary
hypothesis testing, and analyze their asymptotic properties relating to type I
and type II errors of a Bayesian classifier. We then introduce a new divergence
measure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure
multi-distribution divergence for multi-class classification. Based on the
JMH-divergence, we develop two efficient feature selection methods, termed
maximum discrimination ($MD$) and $MD-\chi^2$ methods, for text categorization.
The promising results of extensive experiments demonstrate the effectiveness of
the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02852</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02852</id><created>2016-02-08</created><authors><author><keyname>Della Penna</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Compliance-Aware Bandits</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by clinical trials, we study bandits with observable
non-compliance. At each step, the learner chooses an arm, after, instead of
observing only the reward, it also observes the action that took place. We show
that such noncompliance can be helpful or hurtful to the learner in general.
Unfortunately, naively incorporating compliance information into bandit
algorithms loses guarantees on sublinear regret. We present hybrid algorithms
that maintain regret bounds up to a multiplicative factor and can incorporate
compliance information. Simulations based on real data from the International
Stoke Trial show the practical potential of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02860</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02860</id><created>2016-02-08</created><authors><author><keyname>Tan</keyname><forenames>Rui</forenames></author><author><keyname>Krishna</keyname><forenames>Varun Badrinath</forenames></author><author><keyname>Yau</keyname><forenames>David K. Y.</forenames></author><author><keyname>Kalbarczyk</keyname><forenames>Zbigniew</forenames></author></authors><title>Impact of integrity attacks on real-time pricing in smart grids</title><categories>cs.CR</categories><comments>Proceedings of the 2013 ACM SIGSAC conference on Computer &amp;
  communications security</comments><doi>10.1145/2508859.2516705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern information and communication technologies used by smart grids are
subject to cybersecurity threats. This paper studies the impact of integrity
attacks on real-time pricing (RTP), a key feature of smart grids that uses such
technologies to improve system efficiency. Recent studies have shown that RTP
creates a closed loop formed by the mutually dependent real-time price signals
and price-taking demand. Such a closed loop can be exploited by an adversary
whose objective is to destabilize the pricing system. Specifically, small
malicious modifications to the price signals can be iteratively amplified by
the closed loop, causing inefficiency and even severe failures such as
blackouts. This paper adopts a control-theoretic approach to deriving the
fundamental conditions of RTP stability under two broad classes of integrity
attacks, namely, the scaling and delay attacks. We show that the RTP system is
at risk of being destabilized only if the adversary can compromise the price
signals advertised to smart meters by reducing their values in the scaling
attack, or by providing old prices to over half of all consumers in the delay
attack. The results provide useful guidelines for system operators to analyze
the impact of various attack parameters on system stability, so that they may
take adequate measures to secure RTP systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02862</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02862</id><created>2016-02-09</created><authors><author><keyname>Poursoltan</keyname><forenames>Shayan</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>A Feature-Based Prediction Model of Algorithm Selection for Constrained
  Continuous Optimisation</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With this paper, we contribute to the growing research area of feature-based
analysis of bio-inspired computing. In this research area, problem instances
are classified according to different features of the underlying problem in
terms of their difficulty of being solved by a particular algorithm. We
investigate the impact of different sets of evolved instances for building
prediction models in the area of algorithm selection. Building on the work of
Poursoltan and Neumann [11,10], we consider how evolved instances can be used
to predict the best performing algorithm for constrained continuous
optimisation from a set of bio-inspired computing methods, namely high
performing variants of differential evolution, particle swarm optimization, and
evolution strategies. Our experimental results show that instances evolved with
a multi-objective approach in combination with random instances of the
underlying problem allow to build a model that accurately predicts the best
performing algorithm for a wide range of problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02863</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02863</id><created>2016-02-09</created><authors><author><keyname>Mirzaei</keyname><forenames>Saber</forenames></author><author><keyname>Kfoury</keyname><forenames>Assaf</forenames></author></authors><title>Efficient Reassembling of Graphs, Part 2: The Balanced Case</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reassembling of a simple connected graph G = (V,E) is an abstraction of a
problem arising in earlier studies of network analysis. The reassembling
process has a simple formulation (there are several equivalent formulations)
relative to a binary tree B (reassembling tree), with root node at the top and
$n$ leaf nodes at the bottom, where every cross-section corresponds to a
partition of V such that:
  - the bottom (or first) cross-section (all the leaves) is the finest
partition of V with n one-vertex blocks,
  - the top (or last) cross-section (the root) is the coarsest partition with a
single block, the entire set V,
  - a node (or block) in an intermediate cross-section (or partition) is the
result of merging its two children nodes (or blocks) in the cross-section (or
partition) below it.
  The maximum edge-boundary degree encountered during the reassembling process
is what we call the alpha-measure of the reassembling, and the sum of all
edge-boundary degrees is its beta-measure. The alpha-optimization (resp.
beta-optimization) of the reassembling of G is to determine a reassembling tree
B that minimizes its alpha-measure (resp. beta-measure).
  There are different forms of reassembling. In an earlier report, we studied
linear reassembling, which is the case when the height of B is (n-1). In this
report, we study balanced reassembling, when B has height [log n].
  The two main results in this report are the NP-hardness of alpha-optimization
and beta-optimization of balanced reassembling. The first result is obtained by
a sequence of polynomial-time reductions from minimum bisection of graphs
(known to be NP-hard), and the second by a sequence of polynomial-time
reductions from clique cover of graphs (known to be NP-hard).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02864</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02864</id><created>2016-02-09</created><updated>2016-02-09</updated><authors><author><keyname>Zheng</keyname><forenames>Da</forenames></author><author><keyname>Mhembere</keyname><forenames>Disa</forenames></author><author><keyname>Lyzinski</keyname><forenames>Vince</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua</forenames></author><author><keyname>Priebe</keyname><forenames>Carey E.</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author></authors><title>Semi-External Memory Sparse Matrix Multiplication on Billion-node Graphs
  in a Multicore Architecture</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1602.01421</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to random memory access patterns, sparse matrix multiplication is
traditionally performed in memory and scales to large matrices using the
distributed memory of multiple nodes. In contrast, we scale sparse matrix
multiplication by utilizing commodity SSDs. We implement sparse matrix dense
matrix multiplication (SpMM) in a semi-external memory (SEM) fashion, i.e., we
keep the sparse matrix on SSDs and dense matrices in memory. Our SEM SpMM can
incorporate many in-memory optimizations for large power- law graphs with
near-random vertex connection. Coupled with many I/O optimizations, our SEM
SpMM achieves performance comparable to our in-memory implementation on a large
parallel machine and outperforms the implementations in Trilinos and Intel MKL.
Our experiments show that the SEM SpMM achieves almost 100% performance of the
in-memory implementation on graphs when the dense matrix has more than four
columns; it achieves at least 65% performance of the in-memory implementation
for all of our graphs when the dense matrix has only one column. We apply our
SpMM to three important data analysis applications and show that our SSD-based
implementations can significantly outperform state of the art of these
applications and scale to billion-node graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02865</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02865</id><created>2016-02-09</created><authors><author><keyname>Saleh</keyname><forenames>Babak</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Feldman</keyname><forenames>Jacob</forenames></author></authors><title>The Role of Typicality in Object Classification: Improving The
  Generalization Capacity of Convolutional Neural Networks</title><categories>cs.CV cs.LG cs.NE</categories><comments>In Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep artificial neural networks have made remarkable progress in different
tasks in the field of computer vision. However, the empirical analysis of these
models and investigation of their failure cases has received attention
recently. In this work, we show that deep learning models cannot generalize to
atypical images that are substantially different from training images. This is
in contrast to the superior generalization ability of the visual system in the
human brain. We focus on Convolutional Neural Networks (CNN) as the
state-of-the-art models in object recognition and classification; investigate
this problem in more detail, and hypothesize that training CNN models suffer
from unstructured loss minimization. We propose computational models to improve
the generalization capacity of CNNs by considering how typical a training image
looks like. By conducting an extensive set of experiments we show that
involving a typicality measure can improve the classification results on a new
set of images by a large margin. More importantly, this significant improvement
is achieved without fine-tuning the CNN model on the target image set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02867</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02867</id><created>2016-02-09</created><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Value Iteration Networks</title><categories>cs.AI cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the value iteration network: a fully differentiable neural
network with a `planning module' embedded within. Value iteration networks are
suitable for making predictions about outcomes that involve planning-based
reasoning, such as predicting a desired trajectory from an observation of a
map. Key to our approach is a novel differentiable approximation of the
value-iteration algorithm, which can be represented as a convolutional neural
network, and trained end-to-end using standard backpropagation. We evaluate our
value iteration networks on the task of predicting optimal obstacle-avoiding
trajectories from an image of a landscape, both on synthetic data, and on
challenging raw images of the Mars terrain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02868</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02868</id><created>2016-02-09</created><authors><author><keyname>Jung</keyname><forenames>Deokwoo</forenames></author><author><keyname>Krishna</keyname><forenames>Varun Badrinath</forenames></author><author><keyname>Temple</keyname><forenames>William</forenames></author><author><keyname>Yau</keyname><forenames>David K. Y.</forenames></author></authors><title>Data-Driven Evaluation of Building Demand Response Capacity</title><categories>cs.SY</categories><comments>In proceedings of the 2014 IEEE International Conference on Smart
  Grid Communications (IEEE SmartGridComm 2014)</comments><doi>10.1109/SmartGridComm.2014.7007703</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Before a building can participate in a demand response program, its facility
managers must characterize the site's ability to reduce load. Today, this is
often done through manual audit processes and prototypical control strategies.
In this paper, we propose a new approach to estimate a building's demand
response capacity using detailed data from various sensors installed in a
building. We derive a formula for a probabilistic measure that characterizes
various tradeoffs between the available demand response capacity and the
confidence level associated with that curtailment under the constraints of
building occupant comfort level (or utility). Then, we develop a data-driven
framework to associate observed or projected building energy consumption with a
particular set of rules learned from a large sensor dataset. We apply this
methodology using testbeds in two buildings in Singapore: a unique net-zero
energy building and a modern commercial office building. Our experimental
results identify key control parameters and provide insight into the available
demand response strategies at each site.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02881</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02881</id><created>2016-02-09</created><authors><author><keyname>Lu</keyname><forenames>Jing</forenames></author><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Wimmer</keyname><forenames>Andreas</forenames></author><author><keyname>Gro&#xdf;kopf</keyname><forenames>Stefan</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author></authors><title>Detection and Visualization of Endoleaks in CT Data for Monitoring of
  Thoracic and Abdominal Aortic Aneurysm Stents</title><categories>cs.CV cs.CG cs.GR</categories><comments>7 pages, 7 figures, 1 table, 12 references, Proc. SPIE 6918, Medical
  Imaging 2008: Visualization, Image-Guided Procedures, and Modeling, 69181F
  (17 March 2008)</comments><doi>10.1117/12.769414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an efficient algorithm for the segmentation of the
inner and outer boundary of thoratic and abdominal aortic aneurysms (TAA &amp; AAA)
in computed tomography angiography (CTA) acquisitions. The aneurysm
segmentation includes two steps: first, the inner boundary is segmented based
on a grey level model with two thresholds; then, an adapted active contour
model approach is applied to the more complicated outer boundary segmentation,
with its initialization based on the available inner boundary segmentation. An
opacity image, which aims at enhancing important features while reducing
spurious structures, is calculated from the CTA images and employed to guide
the deformation of the model. In addition, the active contour model is extended
by a constraint force that prevents intersections of the inner and outer
boundary and keeps the outer boundary at a distance, given by the thrombus
thickness, to the inner boundary. Based upon the segmentation results, we can
measure the aneurysm size at each centerline point on the centerline orthogonal
multiplanar reformatting (MPR) plane. Furthermore, a 3D TAA or AAA model is
reconstructed from the set of segmented contours, and the presence of endoleaks
is detected and highlighted. The implemented method has been evaluated on nine
clinical CTA data sets with variations in anatomy and location of the pathology
and has shown promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02885</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02885</id><created>2016-02-09</created><authors><author><keyname>Lee</keyname><forenames>Y. J.</forenames></author><author><keyname>Hirakawa</keyname><forenames>K.</forenames></author><author><keyname>Nguyen</keyname><forenames>T. Q.</forenames></author></authors><title>Joint Defogging and Demosaicking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image defogging is a technique used extensively for enhancing visual quality
of images in bad weather condition. Even though defogging algorithms have been
well studied, defogging performance is degraded by demosaicking artifacts and
sensor noise amplification in distant scenes. In order to improve visual
quality of restored images, we propose a novel approach to perform defogging
and demosaicking simultaneously. We conclude that better defogging performance
with fewer artifacts can be achieved when a defogging algorithm is combined
with a demosaicking algorithm simultaneously. We also demonstrate that the
proposed joint algorithm has the benefit of suppressing noise amplification in
distant scene. In addition, we validate our theoretical analysis and
observations for both synthesized datasets with ground truth fog-free images
and natural scene datasets captured in a raw format.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02886</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02886</id><created>2016-02-09</created><authors><author><keyname>Rozar</keyname><forenames>F</forenames><affiliation>IRFM, MDLS</affiliation></author><author><keyname>Steiner</keyname><forenames>C</forenames><affiliation>IRMA, TONUS</affiliation></author><author><keyname>Latu</keyname><forenames>G</forenames><affiliation>IRFM</affiliation></author><author><keyname>Mehrenberger</keyname><forenames>M</forenames><affiliation>IRMA, TONUS</affiliation></author><author><keyname>Grandgirard</keyname><forenames>V</forenames><affiliation>IRFM</affiliation></author><author><keyname>Bigot</keyname><forenames>Julien</forenames><affiliation>MDLS</affiliation></author><author><keyname>Cartier-Michaud</keyname><forenames>T</forenames><affiliation>IRFM</affiliation></author><author><keyname>Roman</keyname><forenames>Jean</forenames><affiliation>HiePACS</affiliation></author></authors><title>Optilization of the gyroaverage operator based on hermite interpolation</title><categories>physics.comp-ph cs.DC cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gyrokinetic modeling is appropriate for describing Tokamak plasma turbulence,
and the gyroaverage operator is a cornerstone of this approach. In a
gyrokinetic code, the gyroaveraging scheme needs to be accurate enough to avoid
spoiling the data but also requires a low computation cost because it is
applied often on the main unknown, the 5D guiding-center distribution function,
and on the 3D electric potentials. In the present paper, we improve a
gyroaverage scheme based on Hermite interpolation used in the Gysela code. This
initial implementation represents a too large fraction of the total execution
time. The gyroaverage operator has been reformulated and is now expressed as a
matrix-vector product and a cache-friendly algorithm has been setup. Different
techniques have been investigated to quicken the computations by more than a
factor two. Description of the algorithms is given, together with an analysis
of the achieved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02887</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02887</id><created>2016-02-09</created><authors><author><keyname>&#xc7;atak</keyname><forenames>Ferhat &#xd6;zg&#xfc;r</forenames></author></authors><title>Classification with Boosting of Extreme Learning Machine Over
  Arbitrarily Partitioned Data</title><categories>cs.LG</categories><comments>Springer Soft Computing</comments><doi>10.1007/s00500-015-1938-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning based computational intelligence methods are widely used to
analyze large scale data sets in this age of big data. Extracting useful
predictive modeling from these types of data sets is a challenging problem due
to their high complexity. Analyzing large amount of streaming data that can be
leveraged to derive business value is another complex problem to solve. With
high levels of data availability (\textit{i.e. Big Data}) automatic
classification of them has become an important and complex task. Hence, we
explore the power of applying MapReduce based Distributed AdaBoosting of
Extreme Learning Machine (ELM) to build a predictive bag of classification
models. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm is
used to build weak learners (classifier functions); and (iii) builds a strong
learner from a set of weak learners. We applied this training model to the
benchmark knowledge discovery and data mining data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02888</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02888</id><created>2016-02-09</created><authors><author><keyname>&#xc7;atak</keyname><forenames>Ferhat &#xd6;zg&#xfc;r</forenames></author></authors><title>Robust Ensemble Classifier Combination Based on Noise Removal with
  One-Class SVM</title><categories>cs.LG</categories><comments>22nd International Conference, ICONIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning area, as the number of labeled input samples becomes very
large, it is very difficult to build a classification model because of input
data set is not fit in a memory in training phase of the algorithm, therefore,
it is necessary to utilize data partitioning to handle overall data set.
Bagging and boosting based data partitioning methods have been broadly used in
data mining and pattern recognition area. Both of these methods have shown a
great possibility for improving classification model performance. This study is
concerned with the analysis of data set partitioning with noise removal and its
impact on the performance of multiple classifier models. In this study, we
propose noise filtering preprocessing at each data set partition to increment
classifier model performance. We applied Gini impurity approach to find the
best split percentage of noise filter ratio. The filtered sub data set is then
used to train individual ensemble models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02899</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02899</id><created>2016-02-09</created><authors><author><keyname>&#xc7;atak</keyname><forenames>Ferhat &#xd6;zg&#xfc;r</forenames></author></authors><title>Secure Multi-Party Computation Based Privacy Preserving Extreme Learning
  Machine Algorithm Over Vertically Distributed Data</title><categories>cs.CR cs.LG</categories><comments>22nd International Conference, ICONIP 2015</comments><doi>10.1007/978-3-319-26535-3_39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Especially in the Big Data era, the usage of different classification methods
is increasing day by day. The success of these classification methods depends
on the effectiveness of learning methods. Extreme learning machine (ELM)
classification algorithm is a relatively new learning method built on
feed-forward neural-network. ELM classification algorithm is a simple and fast
method that can create a model from high-dimensional data sets. Traditional ELM
learning algorithm implicitly assumes complete access to whole data set. This
is a major privacy concern in most of cases. Sharing of private data (i.e.
medical records) is prevented because of security concerns. In this research,
we propose an efficient and secure privacy-preserving learning algorithm for
ELM classification over data that is vertically partitioned among several
parties. The new learning method preserves the privacy on numerical attributes,
builds a classification model without sharing private data without disclosing
the data of each party to others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02911</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02911</id><created>2016-02-09</created><authors><author><keyname>McMurry</keyname><forenames>Andrew J.</forenames></author></authors><title>Searching PubMed for articles relevant to clinical interpretation of
  rare human genetic variants</title><categories>cs.IR q-bio.QM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Numerous challenges persist that delay clinical interpretation of human
genetic variants, to name a few: (1) un- structured PubMed articles are the
most abundant source of evidence, yet their variant annotations are difficult
to query uniformly, (2) variants can be reported many different ways, for
example as DNA sequence change or protein modification, (3) historical drift in
annotations over time between various genome reference assemblies and
transcript alignments, (4) no single laboratory has sufficient numbers of human
samples, necessitating precompetitive efforts to share evidence for clinical
interpretation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02923</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02923</id><created>2016-02-09</created><authors><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Achieving Global Optimality for Energy Efficiency Maximization in
  Wireless Networks</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characterization of the global maximum of energy efficiency (EE) problems
in wireless networks is a challenging problem due to its nonconvex nature in
interference channels. The aim of this work is to develop a new and general
framework to achieve globally optimal power control solutions. First, the
hidden monotonic structure of the most common EE maximization problems is
exploited jointly with fractional programming theory to obtain globally optimal
solutions with exponential complexity in the number of network links. To
overcome this issue, we also propose a framework to compute suboptimal power
control strategies characterized by affordable complexity. This is achieved by
merging fractional programming and sequential optimization. The proposed
monotonic framework is used to shed light on the ultimate performance of
wireless networks in terms of EE and also to benchmark the performance of the
lower-complexity framework based on sequential programming. Numerical evidence
is provided to show that the sequential fractional programming achieves global
optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02924</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02924</id><created>2016-02-09</created><authors><author><keyname>Hu</keyname><forenames>Yulin</forenames></author><author><keyname>Schmeink</keyname><forenames>Anke</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author></authors><title>Blocklength-Limited Performance of Relaying under Quasi-Static Rayleigh
  Channels</title><categories>cs.IT math.IT</categories><comments>12 figures, submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the blocklength-limited performance of a relaying system is
studied, where channels are assumed to experience quasi-static Rayleigh fading
while at the same time only the average channel state information (CSI) is
available at the source. Both the physical-layer performance
(blocklength-limited throughput) and the link-layer performance (effective
capacity) of the relaying system are investigated. We propose a simple system
operation by introducing a factor based on which we weight the average CSI and
let the source determine the coding rate accordingly. In particular, we prove
that both the blocklength-limited throughput and the effective capacity are
quasi-concave in the weight factor. Through numerical investigations, we show
the appropriateness of our theoretical model. In addition, we observe that
relaying is more efficient than direct transmission. Moreover, this performance
advantage of relaying under the average CSI scenario is more significant than
under the perfect CSI scenario. Finally, the speed of convergence (between the
blocklength-limited performance and the performance in the Shannon capacity
regime) in relaying system is faster in comparison to the direct transmission
under both the average CSI scenario and the perfect CSI scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02934</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02934</id><created>2016-02-09</created><authors><author><keyname>Newling</keyname><forenames>James</forenames></author><author><keyname>Fleuret</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Turbocharging Mini-Batch K-Means</title><categories>stat.ML cs.LG</categories><comments>8 pages + Supplementary Material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an accelerated Mini-Batch k-means algorithm which combines three
key improvements. The first is a modified center update which results in
convergence to a local minimum in fewer iterations. The second is an adaptive
increase of batchsize to meet an increasing requirement for centroid accuracy.
The third is the inclusion of distance bounds based on the triangle inequality,
which are used to eliminate distance calculations along the same lines as
Elkan's algorithm. The combination of the two latter constitutes a very
powerful scheme to reuse computation already done over samples until
statistical accuracy requires the use of additional data points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02938</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02938</id><created>2016-02-09</created><authors><author><keyname>Schott</keyname><forenames>Benjamin</forenames></author><author><keyname>Stegmaier</keyname><forenames>Johannes</forenames></author><author><keyname>Takamiya</keyname><forenames>Masanari</forenames></author><author><keyname>Mikut</keyname><forenames>Ralf</forenames></author></authors><title>Challenges of Integrating A Priori Information Efficiently in the
  Discovery of Spatio-Temporal Objects in Large Databases</title><categories>cs.CV</categories><comments>Proc., 25. Workshop Computational Intelligence, Dortmund, 2015</comments><msc-class>92-08, 92C37, 92C55, 68U10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Using the knowledge discovery framework, it is possible to explore object
databases and extract groups of objects with highly heterogeneous movement
behavior by efficiently integrating a priori knowledge through interacting with
the framework. The whole process is modular expandable and is therefore
adaptive to any problem formulation. Further, the flexible use of different
information allocation processes reveal a great potential to efficiently
incorporate the a priori knowledge of different users in different ways.
Therefore, the stepwise knowledge discovery process embedded in the knowledge
discovery framework is described in detail to point out the flexibility of such
a system incorporating object databases from different applications. The
described framework can be used to gain knowledge out of object databases in
many different fields. This knowledge can be used to gain further insights and
improve the understanding of underlying phenomena. The functionality of the
proposed framework is exemplarily demonstrated using a benchmark database based
on real biological object data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02943</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02943</id><created>2016-02-09</created><authors><author><keyname>Bayram</keyname><forenames>Islam Safak</forenames></author><author><keyname>Zamani</keyname><forenames>Vahraz</forenames></author><author><keyname>Hanna</keyname><forenames>Ryan</forenames></author><author><keyname>Kleissl</keyname><forenames>Jan</forenames></author></authors><title>On the Evaluation of Plug-in Electric Vehicle Data of a Campus Charging
  Network</title><categories>math.OC cs.SY</categories><comments>Accepted by IEEE Energycon 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mass adoption of plug-in electric vehicles (PEVs) requires the deployment
of public charging stations. Such facilities are expected to employ distributed
generation and storage units to reduce the stress on the grid and boost
sustainable transportation. While prior work has made considerable progress in
deriving insights for understanding the adverse impacts of PEV chargings and
how to alleviate them, a critical issue that affects the accuracy is the lack
of real world PEV data. As the dynamics and pertinent design of such charging
stations heavily depend on actual customer demand profile, in this paper we
present and evaluate the data obtained from a $17$ node charging network
equipped with Level $2$ chargers at a major North American University campus.
The data is recorded for $166$ weeks starting from late $2011$. The result
indicates that the majority of the customers use charging lots to extend their
driving ranges. Also, the demand profile shows that there is a tremendous
opportunity to employ solar generation to fuel the vehicles as there is a
correlation between the peak customer demand and solar irradiation. Also, we
provided a more detailed data analysis and show how to use this information in
designing future sustainable charging facilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02944</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02944</id><created>2016-02-09</created><authors><author><keyname>Rajaei</keyname><forenames>Boshra</forenames></author><author><keyname>Gigan</keyname><forenames>Sylvain</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author></authors><title>Fast phase retrieval for high dimensions: A block-based approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses fundamental scaling issues that hinder phase retrieval
(PR) in high dimensions. We show that, if the measurement matrix can be put
into a generalized block-diagonal form, a large PR problem can be solved on
separate blocks, at the cost of a few extra global measurements to merge the
partial results. We illustrate this principle using two distinct PR methods,
and discuss different design trade-offs. Experimental results indicate that
this block-based PR framework can reduce computational cost and memory
requirements by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02949</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02949</id><created>2016-02-09</created><authors><author><keyname>Luko&#x165;ka</keyname><forenames>Robert</forenames></author><author><keyname>Maz&#xe1;k</keyname><forenames>J&#xe1;n</forenames></author></authors><title>Weak oddness as an approximation of oddness and resistance in cubic
  graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce weak oddness $\omega_{\textrm w}$, a new measure of
uncolourability of cubic graphs, defined as the least number of odd components
in an even factor. For every bridgeless cubic graph $G$,
$\rho(G)\le\omega_{\textrm w}(G)\le\omega(G)$, where $\rho(G)$ denotes the
resistance of $G$ and $\omega(G)$ denotes the oddness of $G$, so this new
measure is an approximation of both oddness and resistance. We demonstrate that
there are graphs $G$ satisfying $\rho(G) &lt; \omega_{\textrm w}(G) &lt; \omega(G)$,
and that the difference between any two of those three measures can be
arbitrarily large. The construction implies that if we replace a vertex of a
cubic graph with a triangle, then its oddness can decrease by an arbitrarily
large amount.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02950</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02950</id><created>2016-02-09</created><authors><author><keyname>Tian</keyname><forenames>Xiaohai</forenames></author><author><keyname>Wu</keyname><forenames>Zhizheng</forenames></author><author><keyname>Xiao</keyname><forenames>Xiong</forenames></author><author><keyname>Chng</keyname><forenames>Eng Siong</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author></authors><title>Spoofing detection under noisy conditions: a preliminary investigation
  and an initial database</title><categories>cs.LG cs.SD</categories><comments>Submitted to Odyssey: The Speaker and Language Recognition Workshop
  2016</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spoofing detection for automatic speaker verification (ASV), which is to
discriminate between live speech and attacks, has received increasing
attentions recently. However, all the previous studies have been done on the
clean data without significant additive noise. To simulate the real-life
scenarios, we perform a preliminary investigation of spoofing detection under
additive noisy conditions, and also describe an initial database for this task.
The noisy database is based on the ASVspoof challenge 2015 database and
generated by artificially adding background noises at different signal-to-noise
ratios (SNRs). Five different additive noises are included. Our preliminary
results show that using the model trained from clean data, the system
performance degrades significantly in noisy conditions. Phase-based feature is
more noise robust than magnitude-based features. And the systems perform
significantly differ under different noise scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02982</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02982</id><created>2016-02-09</created><authors><author><keyname>Gustavsen</keyname><forenames>Bjorn</forenames></author><author><keyname>Mo</keyname><forenames>Olve</forenames></author></authors><title>Variable Transmission Voltage for Loss Minimization in Long Offshore
  Wind Farm AC Export Cables</title><categories>cs.CE</categories><comments>To be submitted to IEEE Transactions on Power Delivery</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Connection of offshore wind farms to shore requires the use of submarine
cables. In the case of long HVAC connections, the capacitive charging currents
limit the transfer capability and lead to high losses. This paper shows that
the losses can be substantially reduced by continuously adjusting the cable
operating voltage according to the instantaneous wind farm power
production.Calculations for a 320 MW windfarm connected to shore via a 200 km
cable at 220 kV nominal voltage shows that an annual loss reduction of 9
percent is achievable by simply using a 15 percent tap changer voltage
regulation on the two transformers. Allowing a larger voltage regulation range
leads to further loss reduction (13 percent for 0.4-1.0 p.u. voltage range). If
the windfarm has a low utilization factor, the loss reduction potential is
demonstrated to be as high as 21 percent . The methodology can be applied
without introducing new technology that needs to be developed or qualified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02990</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02990</id><created>2016-02-09</created><authors><author><keyname>Der</keyname><forenames>Ralf</forenames></author><author><keyname>Martius</keyname><forenames>Georg</forenames></author></authors><title>The world as its own best controller: a case study with anthropomimetic
  robots</title><categories>cs.RO cs.LG cs.SY</categories><comments>13 pages, 2 figures</comments><msc-class>37N35, 68T05, 68T40, 93C40</msc-class><acm-class>I.2.9; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the accelerated development of robot technologies, optimal control
becomes one of the central themes of research. In traditional approaches, the
controller, by its internal functionality, finds appropriate actions on the
basis of the history of sensor values, guided by the goals, intentions,
objectives, learning schemes, and so on planted into it. The idea is that the
controller controls the world---the body plus its environment---as reliably as
possible. This paper advocates for a new paradigm of control, obtained by
making the world control its controller in the first place. The paper presents
a solution with a controller that is devoid of any functionalities of its own,
given by a fixed, explicit and context-free function of the recent history of
the sensor values. When applying this controller to a muscle-tendon driven
arm-shoulder system from the Myorobotics toolkit, we observe a vast variety of
self-organized behavior patterns: when left alone, the arm realizes
pseudo-random sequences of different poses but one can also manipulate the
system into definite motion patterns. But most interestingly, after attaching
an object, the controller gets in a functional resonance with the object's
internal dynamics: when given a half-filled bottle, the system spontaneously
starts shaking the bottle so that maximum response from the dynamics of the
water is being generated. After attaching a pendulum to the arm, the controller
drives the pendulum into a circular mode. In this way, the robot discovers
affordances of objects its body is interacting with. We also discuss
perspectives for using this controller paradigm for intention driven behavior
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02991</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02991</id><created>2016-02-09</created><updated>2016-02-12</updated><authors><author><keyname>Amiri</keyname><forenames>Saeed Akhoondian</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author><author><keyname>Siebertz</keyname><forenames>Sebastian</forenames></author></authors><title>A local constant factor approximation for the minimum dominating set
  problem on bounded genus graphs</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Dominating Set (MDS) problem is not only one of the most
fundamental problems in distributed computing, it is also one of the most
challenging ones. While it is well-known that minimum dominating sets cannot be
approximated locally on general graphs, over the last years, several
breakthroughs have been made on computing local approximations on sparse
graphs.
  This paper presents a deterministic and local constant factor approximation
for minimum dominating sets on bounded genus graphs, a very large family of
sparse graphs. Our main technical contribution is a new analysis of a slightly
modified, first-order definable variant of an existing algorithm by Lenzen et
al. Interestingly, unlike existing proofs for planar graphs, our analysis does
not rely on any topological arguments. We believe that our techniques can be
useful for the study of local problems on sparse graphs beyond the scope of
this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02992</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02992</id><created>2016-02-09</created><authors><author><keyname>Burattin</keyname><forenames>Andrea</forenames></author><author><keyname>Bernstein</keyname><forenames>Vered</forenames></author><author><keyname>Neurauter</keyname><forenames>Manuel</forenames></author><author><keyname>Soffer</keyname><forenames>Pnina</forenames></author><author><keyname>Weber</keyname><forenames>Barbara</forenames></author></authors><title>Detection and Quantification of Flow Consistency in Business Process
  Models</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business process models abstract complex business processes by representing
them as graphical models. Their layout, solely determined by the modeler,
affects their understandability. To support the construction of understandable
models it would be beneficial to systematically study this effect. However,
this requires a basic set of measurable key visual features, depicting the
layout properties that are meaningful to the human user. The aim of this
research is thus twofold. First, to empirically identify key visual features of
business process models which are perceived as meaningful to the user. Second,
to show how such features can be quantified into computational metrics, which
are applicable to business process models. We focus on one particular feature,
consistency of flow direction, and show the challenges that arise when
transforming it into a precise metric. We propose three different metrics
addressing these challenges, each following a different view of flow
consistency. We then report the results of an empirical evaluation, which
indicates which metric is more effective in predicting the human perception of
this feature. Moreover, two other automatic evaluations describing the
performance and the computational capabilities of our metrics are reported as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02995</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02995</id><created>2016-02-09</created><authors><author><keyname>Lea</keyname><forenames>Colin</forenames></author><author><keyname>Reiter</keyname><forenames>Austin</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author><author><keyname>Hager</keyname><forenames>Gregory D.</forenames></author></authors><title>Efficient Segmental Inference for Spatiotemporal Modeling of
  Fine-grained Actions</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint segmentation and classification of fine-grained actions is important
for applications in human-robot interaction, video surveillance, and human
skill evaluation. In the first part of this paper we develop a spatiotemporal
model that takes inspiration from early work in robot task modeling by learning
how the state of the world changes over time. The spatial component models
objects, locations, and object relationships and the temporal component models
how actions transition throughout a sequence. In the second part of the paper
we introduce an efficient algorithm for segmental inference that jointly
segments and predicts all actions within a video. Our algorithm is tens to
thousands of times faster than the current method on two fine-grained action
recognition datasets. We highlight the effectiveness of our approach on the 50
Salads and JIGSAWS datasets and observe our model produces many fewer
false-positives than competing methods and achieves state of the art
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.02999</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.02999</id><created>2016-02-09</created><authors><author><keyname>Mandal</keyname><forenames>Bappaditya</forenames></author></authors><title>Face Recognition: Perspectives from the Real-World</title><categories>cs.CV</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze some of our real-world deployment of face
recognition (FR) systems for various applications and discuss the gaps between
expectations of the user and what the system can deliver. We evaluate some of
our proposed algorithms with ad-hoc modifications for applications such as FR
on wearable devices (like Google Glass), monitoring of elderly people in senior
citizens centers, FR of children in child care centers and face matching
between a scanned IC/passport face image and a few live webcam images for
automatic hotel/resort checkouts. We describe each of these applications, the
challenges involved and proposed solutions. Since FR is intuitive in nature and
we human beings use it for interactions with the outside world, people have
high expectations of its performance in real-world scenarios. However, we
analyze and discuss here that it is not the case, machine recognition of faces
for each of these applications poses unique challenges and demands specific
research components so as to adapt in the actual sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03001</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03001</id><created>2016-02-09</created><authors><author><keyname>Allamanis</keyname><forenames>Miltiadis</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>A Convolutional Attention Network for Extreme Summarization of Source
  Code</title><categories>cs.LG cs.CL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attention mechanisms in neural networks have proved useful for problems in
which the input and output do not have fixed dimension. Often there exist
features that are locally translation invariant and would be valuable for
directing the model's attention, but previous attentional architectures are not
constructed to learn such features specifically. We introduce an attentional
neural network that employs convolution on the input tokens to detect local
time-invariant and long-range topical attention features in a context-dependent
way. We apply this architecture to the problem of extreme summarization of
source code snippets into short, descriptive function name-like summaries.
Using those features, the model sequentially generates a summary by
marginalizing over two attention mechanisms: one that predicts the next summary
token based on the attention weights of the input tokens and another that is
able to copy a code token as-is directly into the summary. We demonstrate our
convolutional attention neural network's performance on 10 popular Java
projects showing that it achieves better performance compared to previous
attentional mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03012</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03012</id><created>2016-02-09</created><authors><author><keyname>Twinanda</keyname><forenames>Andru P.</forenames></author><author><keyname>Shehata</keyname><forenames>Sherif</forenames></author><author><keyname>Mutter</keyname><forenames>Didier</forenames></author><author><keyname>Marescaux</keyname><forenames>Jacques</forenames></author><author><keyname>de Mathelin</keyname><forenames>Michel</forenames></author><author><keyname>Padoy</keyname><forenames>Nicolas</forenames></author></authors><title>EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic
  Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surgical workflow recognition has numerous potential medical applications,
such as the automatic indexing of surgical video databases and the optimization
of real-time OR scheduling, among others. As a result, phase recognition has
been studied in the context of several kinds of surgeries, such as cataract,
neurological, and laparoscopic surgeries. In the literature, two types of
features are typically used to perform this task: visual features and tool
usage signals. However, the visual features used are mostly handcrafted.
Furthermore, since additional equipment is needed to obtain the tool usage
signals automatically, they are usually collected via a tedious manual
annotation process. In this paper, we propose a novel method for phase
recognition that uses a convolutional neural network (CNN) to automatically
learn features from cholecystectomy videos and that relies uniquely on visual
information. In previous studies, it has been shown that the tool signals can
provide valuable information in performing the phase recognition task. Thus, we
present a novel CNN architecture, called EndoNet, that is designed to carry out
the phase recognition and tool presence detection tasks in a multi-task manner.
To the best of our knowledge, this is the first work proposing to use a CNN for
multiple recognition tasks on laparoscopic videos. Extensive experimental
comparisons to other methods show that EndoNet yields state-of-the-art results
for both tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03014</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03014</id><created>2016-02-09</created><updated>2016-03-01</updated><authors><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Herding as a Learning System with Edge-of-Chaos Dynamics</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herding defines a deterministic dynamical system at the edge of chaos. It
generates a sequence of model states and parameters by alternating parameter
perturbations with state maximizations, where the sequence of states can be
interpreted as &quot;samples&quot; from an associated MRF model. Herding differs from
maximum likelihood estimation in that the sequence of parameters does not
converge to a fixed point and differs from an MCMC posterior sampling approach
in that the sequence of states is generated deterministically. Herding may be
interpreted as a&quot;perturb and map&quot; method where the parameter perturbations are
generated using a deterministic nonlinear dynamical system rather than randomly
from a Gumbel distribution. This chapter studies the distinct statistical
characteristics of the herding algorithm and shows that the fast convergence
rate of the controlled moments may be attributed to edge of chaos dynamics. The
herding algorithm can also be generalized to models with latent variables and
to a discriminative learning setting. The perceptron cycling theorem ensures
that the fast moment matching property is preserved in the more general
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03016</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03016</id><created>2016-02-09</created><authors><author><keyname>Ortega-Zamorano</keyname><forenames>Francisco</forenames></author><author><keyname>Montemurro</keyname><forenames>Marcelo A.</forenames></author><author><keyname>Cannas</keyname><forenames>Sergio A.</forenames></author><author><keyname>Jerez</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Franco</keyname><forenames>Leonardo</forenames></author></authors><title>FPGA Hardware Acceleration of Monte Carlo Simulations for the Ising
  Model</title><categories>cs.AR physics.comp-ph</categories><comments>19 pages, 10 figures</comments><doi>10.1109/TPDS.2015.2505725</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-dimensional Ising model with nearest-neighbors ferromagnetic
interactions is implemented in a Field Programmable Gate Array (FPGA)
board.Extensive Monte Carlo simulations were carried out using an efficient
hardware representation of individual spins and a combined global-local LFSR
random number generator. Consistent results regarding the descriptive
properties of magnetic systems, like energy, magnetization and susceptibility
are obtained while a speed-up factor of approximately 6 times is achieved in
comparison to previous FPGA-based published works and almost $10^4$ times in
comparison to a standard CPU simulation. A detailed description of the logic
design used is given together with a careful analysis of the quality of the
random number generator used. The obtained results confirm the potential of
FPGAs for analyzing the statistical mechanics of magnetic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03027</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03027</id><created>2016-02-09</created><authors><author><keyname>Tolstikhin</keyname><forenames>Ilya</forenames></author><author><keyname>Lopez-Paz</keyname><forenames>David</forenames></author></authors><title>Minimax Lower Bounds for Realizable Transductive Classification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transductive learning considers a training set of $m$ labeled samples and a
test set of $u$ unlabeled samples, with the goal of best labeling that
particular test set. Conversely, inductive learning considers a training set of
$m$ labeled samples drawn iid from $P(X,Y)$, with the goal of best labeling any
future samples drawn iid from $P(X)$. This comparison suggests that
transduction is a much easier type of inference than induction, but is this
really the case? This paper provides a negative answer to this question, by
proving the first known minimax lower bounds for transductive, realizable,
binary classification. Our lower bounds show that $m$ should be at least
$\Omega(d/\epsilon + \log(1/\delta)/\epsilon)$ when $\epsilon$-learning a
concept class $\mathcal{H}$ of finite VC-dimension $d&lt;\infty$ with confidence
$1-\delta$, for all $m \leq u$. This result draws three important conclusions.
First, general transduction is as hard as general induction, since both
problems have $\Omega(d/m)$ minimax values. Second, the use of unlabeled data
does not help general transduction, since supervised learning algorithms such
as ERM and (Hanneke, 2015) match our transductive lower bounds while ignoring
the unlabeled test set. Third, our transductive lower bounds imply lower bounds
for semi-supervised learning, which add to the important discussion about the
role of unlabeled data in machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03031</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03031</id><created>2016-02-09</created><updated>2016-02-19</updated><authors><author><keyname>Ileri</keyname><forenames>Atalay M.</forenames></author><author><keyname>Ozercan</keyname><forenames>Halil I.</forenames></author><author><keyname>Gundogdu</keyname><forenames>Alper</forenames></author><author><keyname>Senol</keyname><forenames>Ahmet K.</forenames></author><author><keyname>Ozkaya</keyname><forenames>M. Yusuf</forenames></author><author><keyname>Alkan</keyname><forenames>Can</forenames></author></authors><title>Coinami: A Cryptocurrency with DNA Sequence Alignment as Proof-of-work</title><categories>cs.CE cs.CR q-bio.GN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rate of growth of the amount of data generated using the high throughput
sequencing (HTS) platforms now exceeds the growth stipulated by Moore's Law.
The HTS data is expected to surpass those of other &quot;big data&quot; domains such as
astronomy, before the year 2025. In addition to sequencing genomes for research
purposes, genome and exome sequencing in clinical settings will be a routine
part of health care. The analysis of such large amounts of data, however, is
not without computational challenges. This burden is even more increased due to
the periodic updates to reference genomes, which typically require re-analysis
of existing data. Here we propose Coin-Application Mediator Interface (Coinami)
to distribute the workload for mapping reads to reference genomes using a
volunteer grid computer approach similar to Berkeley Open Infrastructure for
Network Computing (BOINC). However, since HTS read mapping requires substantial
computational resources and fast analysis turnout is desired, Coinami uses the
HTS read mapping as proof-of-work to generate valid blocks to main its own
cryptocurrency system, which may help motivate volunteers to dedicate more
resources. The Coinami protocol includes mechanisms to ensure that jobs
performed by volunteers are correct, and provides genomic data privacy. The
prototype implementation of Coinami is available at http://coinami.github.io/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03032</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03032</id><created>2016-02-09</created><authors><author><keyname>Danihelka</keyname><forenames>Ivo</forenames></author><author><keyname>Wayne</keyname><forenames>Greg</forenames></author><author><keyname>Uria</keyname><forenames>Benigno</forenames></author><author><keyname>Kalchbrenner</keyname><forenames>Nal</forenames></author><author><keyname>Graves</keyname><forenames>Alex</forenames></author></authors><title>Associative Long Short-Term Memory</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a new method to augment recurrent neural networks with extra
memory without increasing the number of network parameters. The system has an
associative memory based on complex-valued vectors and is closely related to
Holographic Reduced Representations and Long Short-Term Memory networks.
Holographic Reduced Representations have limited capacity: as they store more
information, each retrieval becomes noisier due to interference. Our system in
contrast creates redundant copies of stored information, which enables
retrieval with reduced noise. Experiments demonstrate faster learning on
multiple memorization tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03033</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03033</id><created>2016-02-09</created><authors><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author></authors><title>Strengthening the Entropy Power Inequality</title><categories>cs.IT math.IT</categories><comments>23 pages. Full version of submission to 2016 International Symposium
  on Information Theory. Presented in part at Institut Henri Poincar\'{e} Feb
  10, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tighten the Entropy Power Inequality (EPI) when one of the random summands
is Gaussian. Our strengthening is closely connected to the concept of strong
data processing for Gaussian channels and generalizes the (vector extension of)
Costa's EPI. This leads to a new reverse entropy power inequality and, as a
corollary, sharpens Stam's inequality relating entropy power and Fisher
information. Applications to network information theory are given, including a
short self-contained proof of the rate region for the two-encoder quadratic
Gaussian source coding problem.
  Our argument is based on weak convergence and a technique employed by Geng
and Nair for establishing Gaussian optimality via rotational-invariance, which
traces its roots to a `doubling trick' that has been successfully used in the
study of functional inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03040</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03040</id><created>2016-02-09</created><authors><author><keyname>Dror</keyname><forenames>Rotem</forenames></author><author><keyname>Reichart</keyname><forenames>Roi</forenames></author></authors><title>The Structured Weighted Violations Perceptron Algorithm</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a
new perceptron algorithm for structured prediction, that generalizes the
Collins Structured Perceptron (CSP, (Collins, 2002)). Unlike CSP, the update
rule of SWVP explicitly exploits the internal structure of the predicted
labels. We prove that for linearly separable training sets, SWVP converges to a
weight vector that separates the data, under certain conditions on the
parameters of the algorithm. We further prove bounds for SWVP on: (a) the
number of updates in the separable case; (b) mistakes in the non-separable
case; and (c) the probability to misclassify an unseen example
(generalization), and show that for most SWVP variants these bounds are tighter
than those of the CSP special case. In synthetic data experiments where data is
drawn from a generative hidden variable model, SWVP provides substantial
improvements over CSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03050</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03050</id><created>2016-02-09</created><authors><author><keyname>L&#xfc;ck</keyname><forenames>Martin</forenames></author></authors><title>Complete Problems of Propositional Logic for the Exponential Hierarchy</title><categories>cs.CC</categories><msc-class>68Q15</msc-class><acm-class>F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large complexity classes, like the exponential time hierarchy, received
little attention in terms of finding complete problems. In this work a
generalization of propositional logic is investigated which fills this gap with
the introduction of Boolean higher-order quantifiers or equivalently Boolean
Skolem functions. This builds on the important results of Wrathall and
Stockmeyer regarding complete problems, namely QBF and QBF$_k$, for the
polynomial hierarchy. Furthermore it generalizes the Dependency QBF problem
introduced by Peterson, Reif and Azhar which is complete for NEXP, the first
level of the exponential hierarchy. Also it turns out that the hardness results
are much more robust against conjunctive and disjunctive normal forms than
plain QBF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03061</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03061</id><created>2016-02-09</created><updated>2016-02-23</updated><authors><author><keyname>Reyes</keyname><forenames>Matthew G.</forenames></author><author><keyname>Neuhoff</keyname><forenames>David L.</forenames></author></authors><title>Minimum Conditional Description Length Estimation for Markov Random
  Fields</title><categories>cs.IT cs.LG math.IT math.ST stat.TH</categories><comments>Information Theory and Applications (ITA) workshop, February 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss a method, which we call Minimum Conditional
Description Length (MCDL), for estimating the parameters of a subset of sites
within a Markov random field. We assume that the edges are known for the entire
graph $G=(V,E)$. Then, for a subset $U\subset V$, we estimate the parameters
for nodes and edges in $U$ as well as for edges incident to a node in $U$, by
finding the exponential parameter for that subset that yields the best
compression conditioned on the values on the boundary $\partial U$. Our
estimate is derived from a temporally stationary sequence of observations on
the set $U$. We discuss how this method can also be applied to estimate a
spatially invariant parameter from a single configuration, and in so doing,
derive the Maximum Pseudo-Likelihood (MPL) estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03072</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03072</id><created>2016-02-09</created><authors><author><keyname>Aridhi</keyname><forenames>Sabeur</forenames></author><author><keyname>Nguifo</keyname><forenames>Engelbert Mephu</forenames></author></authors><title>Big Graph Mining: Frameworks and Techniques</title><categories>cs.DC cs.DB</categories><comments>Submitted to Big Data Research, Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big graph mining is an important research area and it has attracted
considerable attention. It allows to process, analyze, and extract meaningful
information from large amounts of graph data. Big graph mining has been highly
motivated not only by the tremendously increasing size of graphs but also by
its huge number of applications. Such applications include bioinformatics,
chemoinformatics and social networks. One of the most challenging tasks in big
graph mining is pattern mining in big graphs. This task consists on using data
mining algorithms to discover interesting, unexpected and useful patterns in
large amounts of graph data. It aims also to provide deeper understanding of
graph data. In this context, several graph processing frameworks and scaling
data mining/pattern mining techniques have been proposed to deal with very big
graphs. This paper gives an overview of existing data mining and graph
processing frameworks that deal with very big graphs. Then it presents a survey
of current researches in the field of data mining / pattern mining in big
graphs and discusses the main research issues related to this field. It also
gives a categorization of both distributed data mining and machine learning
techniques, graph processing frameworks and large scale pattern mining
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03084</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03084</id><created>2016-02-09</created><authors><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Xie</keyname><forenames>Hongmei</forenames></author></authors><title>Local Codes with Cooperative Repair in Distributed Storage System</title><categories>cs.IT cs.DC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the research on local repair codes is mainly confined to repair the
failed nodes within each repair group. But if the extreme cases occur that the
entire repair group has failed, the local code stored in the failed group need
to be recovered as a whole. In this paper, local codes with cooperative repair,
in which the local codes are constructed based on minimum storage regeneration
(MSR) codes, is proposed to achieve repairing the failed groups. Specifically,
the proposed local codes with cooperative repair construct a kind of mutual
interleaving structure among the parity symbols, that the parity symbols of
each local code, named as distributed local parity, can be generated by the
parity symbols of the MSR codes in its two adjacent local codes. Taking
advantage of the structure given, the failed local groups can be repaired
cooperatively by their adjacent local groups with lower repair locality, and
meanwhile the minimum distance of local codes with cooperative repair is
derived. Theoretical analysis and simulation experiments show that, compared
with codes with local regeneration (such as MSR-local codes and MBR-local
codes), the proposed local codes with cooperative repair have benefits in
bandwidth overhead and repair locality for the case of local groups failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03086</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03086</id><created>2016-02-09</created><authors><author><keyname>Dlugosz</keyname><forenames>Maciej</forenames></author><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author></authors><title>RECKONER: Read Error Corrector Based on KMC</title><categories>q-bio.GN cs.DS</categories><comments>7 pages + 24 pages of supplementary material</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Next-generation sequencing tools have enabled producing of huge
amount of genomic information at low cost. Unfortunately, presence of
sequencing errors in such data affects quality of downstream analyzes. Accuracy
of them can be improved by performing error correction. Because of huge amount
of such data correction algorithms have to: be fast, memory-frugal, and provide
high accuracy of error detection and elimination for variously-sized organisms.
  Results: We introduce a new algorithm for genomic data correction, capable of
processing eucaryotic 300 Mbp-genome-size, high error-rated data using less
than 4 GB of RAM in less than 40 minutes on 16-core CPU. The algorithm allows
to correct sequencing data at better or comparable level than competitors. This
was achieved by using very robust KMC~2 $k$-mer counter, new method of
erroneous regions correction based on both $k$-mer counts and FASTQ quality
indicators as well as careful optimization. Availability: Program is freely
available at http://sun.aei.posl.pl/REFRESH/reckoner. Contact:
sebastian.deorowicz@polsl.pl
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03090</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03090</id><created>2016-02-07</created><authors><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Lim</keyname><forenames>Hyun woo</forenames></author><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Vadhan</keyname><forenames>Salil</forenames></author></authors><title>Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit
  and Independence Testing</title><categories>math.ST cs.CR stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hypothesis testing is a useful statistical tool in determining whether a
given model should be rejected based on a sample from the population. Sample
data may contain sensitive information about individuals, such as medical
information. Thus it is important to design statistical tests that guarantee
the privacy of subjects in the data. In this work, we study hypothesis testing
subject to differential privacy, specifically chi-squared tests for goodness of
fit for multinomial data and independence between two categorical variables.
  We propose new tests for goodness of fit and independence testing that like
the classical versions can be used to determine whether a given model should be
rejected or not, and that additionally can ensure differential privacy. We give
both Monte Carlo based hypothesis tests as well as hypothesis tests that more
closely follow the classical chi-squared goodness of fit test and the Pearson
chi-squared test for independence. Crucially, our tests account for the
distribution of the noise that is injected to ensure privacy in determining
significance.
  We show that these tests can be used to achieve desired significance levels,
in sharp contrast to direct applications of classical tests to differentially
private contingency tables which can result in wildly varying significance
levels. Moreover, we study the statistical power of these tests. We empirically
show that to achieve the same level of power as the classical non-private tests
our new tests need only a relatively modest increase in sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03091</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03091</id><created>2016-02-09</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Enhancing the Estimation of mm-Wave Large Array Channels by Exploiting
  Spatio-Temporal Correlation and Sparse Scattering</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures. Accepted for presentation at WSA 2016, Munich,
  Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to cope with the large path-loss exponent of mm-Wave channels, a
high beamforming gain is needed. This can be achieved with small hardware
complexity and high hardware power efficiency by Hybrid Digital-Analog (HDA)
beamforming, where a very large number $M\gg 1$ of antenna array elements
requires only a relatively small $m\ll M$ number of A/D converters and
modulators/demodulators. As such, the estimation of mm-Wave MIMO channels must
deal with two specific problems: 1) high Doppler, due to the large carrier
frequency; 2) impossibility of observing directly the M-dimensional channel
vector at the antenna array elements, due to the mentioned HDA implementation.
In this paper, we consider a novel scheme inspired by recent results on
gridless multiple measurement vectors problem in compressed sensing, that is
able to exploit the inherent mm-Wave channel sparsity in the angular domain in
order to cope with both the above problems simultaneously. Our scheme uses past
pilot-symbol observations in a window of length $T$ in order to estimate a
low-dimensional subspace that approximately contains the channel vector at the
current time. This subspace information can be used directly, in order to
separate users in the spatial domain, or indirectly, in order to improve the
estimate of the user channel vector from the current pilot-symbol observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03095</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03095</id><created>2016-02-09</created><authors><author><keyname>Cong</keyname><forenames>Kai</forenames></author><author><keyname>Lei</keyname><forenames>Li</forenames></author><author><keyname>Yang</keyname><forenames>Zhenkun</forenames></author><author><keyname>Xie</keyname><forenames>Fei</forenames></author></authors><title>OpenRISC System-on-Chip Design Emulation</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently the hardware emulation technique has emerged as a promising approach
to accelerating hardware verification/debugging process. To fully evaluate the
powerfulness of the emulation approach and demonstrate its potential impact, we
propose to emulate a system-on-chip (SoC) design using Mentor Graphics Veloce
emulation platform. This article presents our project setup and the results we
have achieved. The results are encouraging. ORPSoC emulation with Veloce has
more than ten times faster than hardware simulation. Our experimental results
demonstrate that Mentor Graphics Veloce has major advantages in emulation,
verification, and debugging of complicated real hardware designs, especially in
the context of SoC complexity. Through our three major tasks, we will
demonstrate that (1) Veloce can successfully emulate large-scale SoC designs;
(2) it has much better performance comparing to the state-of-the-art simulation
tools; (3) it can significantly accelerate the process of hardware verification
and debugging while maintaining full signal visibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03097</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03097</id><created>2016-02-08</created><updated>2016-02-11</updated><authors><author><keyname>Wetzels</keyname><forenames>Jos</forenames></author></authors><title>Open Sesame: The Password Hashing Competition and Argon2</title><categories>cs.CR</categories><comments>17 pages</comments><acm-class>E.3</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this document we present an overview of the background to and goals of the
Password Hashing Competition (PHC) as well as the design of its winner, Argon2,
and its security requirements and properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03100</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03100</id><created>2016-02-09</created><authors><author><keyname>Megler</keyname><forenames>V. M.</forenames></author><author><keyname>Tufte</keyname><forenames>Kristin</forenames></author><author><keyname>Maier</keyname><forenames>David</forenames></author></authors><title>Improving Data Quality in Intelligent Transportation Systems</title><categories>cs.OH</categories><report-no>PID3924199</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent Transportation Systems (ITS) use data and information technology
to improve the operation of our transportation network. ITS contributes to
sustainable development by using technology to make the transportation system
more efficient; improving our environment by reducing emissions, reducing the
need for new construction and improving our daily lives through reduced
congestion. A key component of ITS is traveler information. The Oregon
Department of Transportation (ODOT) recently implemented a new traveler
information system on selected freeways to provide drivers with travel time
estimates that allow them to make more informed decisions about routing to
their destinations. The ODOT project aims to improve traffic flow and promote
efficient traffic movement, which can reduce emissions rates and improve air
quality. The new ODOT system is based on travel data collected from a
recently-increased set of sensors installed on its freeways. Our current
project investigates novel data cleaning methodologies and the integration of
those methodologies into the prediction of travel times. We use machine
learning techniques on our archive to identify suspect data, and calculate
revised travel times excluding this suspect data. We compare the resulting
travel time predictions to ground-truth data, and to predictions based on
simple, rule-based data cleaning. We report on the results of our study using
qualitative and quantitative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03101</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03101</id><created>2016-02-09</created><authors><author><keyname>Martins</keyname><forenames>Fl&#xe1;vio</forenames></author><author><keyname>Magalh&#xe3;es</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Callan</keyname><forenames>Jamie</forenames></author></authors><title>Barbara Made the News: Mining the Behavior of Crowds for Time-Aware
  Learning to Rank</title><categories>cs.IR</categories><comments>To appear in WSDM 2016</comments><acm-class>H.3.3</acm-class><doi>10.1145/2835776.2835825</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Twitter, and other microblogging services, the generation of new content
by the crowd is often biased towards immediacy: what is happening now. Prompted
by the propagation of commentary and information through multiple mediums,
users on the Web interact with and produce new posts about newsworthy topics
and give rise to trending topics. This paper proposes to leverage on the
behavioral dynamics of users to estimate the most relevant time periods for a
topic. Our hypothesis stems from the fact that when a real-world event occurs
it usually has peak times on the Web: a higher volume of tweets, new visits and
edits to related Wikipedia articles, and news published about the event. In
this paper, we propose a novel time-aware ranking model that leverages on
multiple sources of crowd signals. Our approach builds on two major novelties.
First, a unifying approach that given query q, mines and represents temporal
evidence from multiple sources of crowd signals. This allows us to predict the
temporal relevance of documents for query q. Second, a principled retrieval
model that integrates temporal signals in a learning to rank framework, to rank
results according to the predicted temporal relevance. Evaluation on the TREC
2013 and 2014 Microblog track datasets demonstrates that the proposed model
achieves a relative improvement of 13.2% over lexical retrieval models and 6.2%
over a learning to rank baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03103</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03103</id><created>2016-02-09</created><authors><author><keyname>Clough</keyname><forenames>James R.</forenames></author><author><keyname>Evans</keyname><forenames>Tim S.</forenames></author></authors><title>Embedding Graphs in Lorentzian Spacetime</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>15 pages, 6 figure, 2 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geometric approaches to network analysis combine simply defined models with
great descriptive power. In this work we provide a method for embedding
directed acyclic graphs into Minkowski spacetime using Multidimensional scaling
(MDS). First we generalise the classical MDS algorithm, defined only for
metrics with a Euclidean signature, to manifolds of any metric signature. We
then use this general method to develop an algorithm to be used on networks
which have causal structure allowing them to be embedded in Lorentzian
manifolds. The method is demonstrated by calculating embeddings for both causal
sets and citation networks in Minkowski spacetime. We finally suggest a number
of applications in citation analysis such as paper recommendation, identifying
missing citations and fitting citation models to data using this geometric
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03104</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03104</id><created>2016-02-09</created><authors><author><keyname>Dutta</keyname><forenames>Ayan</forenames></author><author><keyname>Dasgupta</keyname><forenames>Prithviraj</forenames></author><author><keyname>Nelson</keyname><forenames>Carl</forenames></author></authors><title>A Graph Isomorphism-based Decentralized Algorithm for Modular Robot
  Configuration Formation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of configuration formation in modular robot systems
where a set of modules that are initially in different configurations and
located at different locations are required to assume appropriate positions so
that they can get into a new, user-specified, target configuration. We propose
a novel algorithm based on graph isomorphism, where the modules select
locations or spots in the target configuration using a utility-based framework,
while retaining their original configuration to the greatest extent possible,
to reduce the time and energy required by the modules to assume the target
configuration. We have shown analytically that our proposed algorithm is
complete and guarantees a Pareto-optimal allocation. Experimental simulations
of our algorithm with different number of modules in different initial
configurations and located initially at different locations, show that the
planning time of our algorithm is nominal (order of msec. for 100 modules). We
have also compared our algorithm against a market-based allocation algorithm
and shown that our proposed algorithm performs better in terms of time and
number of messages exchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03105</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03105</id><created>2016-02-09</created><authors><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Bui</keyname><forenames>Hung</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Theocharous</keyname><forenames>Georgios</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Sun</keyname><forenames>Siqi</forenames></author></authors><title>Graphical Model Sketch</title><categories>cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured high-cardinality data arises in many domains and poses a major
challenge for both modeling and inference, which is beyond current graphical
model frameworks. We view these data as a stream $(x^{(t)})_{t = 1}^n$ of $n$
observations from an unknown distribution $P$, where $x^{(t)} \in [M]^K$ is a
$K$-dimensional vector and $M$ is the cardinality of its entries, which is very
large. Suppose that the graphical model $\mathcal{G}$ of $P$ is known, and let
$\bar{P}$ be the maximum-likelihood estimate (MLE) of $P$ from $(x^{(t)})_{t =
1}^n$ conditioned on $\mathcal{G}$. In this work, we design and analyze
algorithms that approximate $\bar{P}$ with $\hat{P}$, such that $\hat{P}(x)
\approx \bar{P}(x)$ for any $x \in [M]^K$ with a high probability, and
crucially in the space independent of $M$. The key idea of our approximations
is to use the structure of $\mathcal{G}$ and approximately estimate its factors
by &quot;sketches&quot;. The sketches hash high-cardinality variables using random
projections. Our approximations are computationally and space efficient, being
independent of $M$. Our error bounds are multiplicative and provably improve
upon those of the count-min (CM) sketch, a state-of-the-art approach to
estimating the frequency of values in a stream, in a class of naive Bayes
models. We evaluate our algorithms on synthetic and real-world problems, and
report an order of magnitude improvements over the CM sketch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03109</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03109</id><created>2016-02-09</created><authors><author><keyname>Aracena</keyname><forenames>Julio</forenames></author><author><keyname>Richard</keyname><forenames>Adrien</forenames></author><author><keyname>Salinas</keyname><forenames>Lilian</forenames></author></authors><title>Number of fixed points and disjoint cycles in monotone Boolean networks</title><categories>math.CO cs.DM cs.IT math.IT q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a digraph $G$, a lot of attention has been deserved on the maximum
number $\phi(G)$ of fixed points in a Boolean network $f:\{0,1\}^n\to\{0,1\}^n$
with $G$ as interaction graph. In particular, a central problem in network
coding consists in studying the optimality of the classical upper bound
$\phi(G)\leq 2^{\tau}$, where $\tau$ is the minimum size of a feedback vertex
set of $G$. In this paper, we study the maximum number $\phi_m(G)$ of fixed
points in a {\em monotone} Boolean network with interaction graph $G$. We
establish new upper and lower bounds on $\phi_m(G)$ that depends on the cycle
structure of $G$. In addition to $\tau$, the involved parameters are the
maximum number $\nu$ of vertex-disjoint cycles, and the maximum number
$\nu^{*}$ of vertex-disjoint cycles verifying some additional technical
conditions. We improve the classical upper bound $2^\tau$ by proving that
$\phi_m(G)$ is at most the largest sub-lattice of $\{0,1\}^\tau$ without chain
of size $\nu+1$, and without another forbidden-pattern of size $2\nu^{*}$.
Then, we prove two optimal lower bounds: $\phi_m(G)\geq \nu+1$ and
$\phi_m(G)\geq 2^{\nu^{*}}$. As a consequence, we get the following
characterization: $\phi_m(G)=2^\tau$ if and only if $\nu^{*}=\tau$. As another
consequence, we get that if $c$ is the maximum length of a chordless cycle of
$G$ then $2^{\nu/3^c}\leq\phi_m(G)\leq 2^{c\nu}$. Finally, with the technics
introduced, we establish an upper bound on the number of fixed points of any
Boolean network according to its signed interaction graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03110</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03110</id><created>2016-02-09</created><authors><author><keyname>Galhotra</keyname><forenames>Sainyam</forenames></author><author><keyname>Arora</keyname><forenames>Akhil</forenames></author><author><keyname>Roy</keyname><forenames>Shourya</forenames></author></authors><title>Holistic Influence Maximization: Combining Scalability and Efficiency
  with Opinion-Aware Models</title><categories>cs.SI cs.DB</categories><comments>ACM SIGMOD Conference 2016, 18 pages, 29 figures</comments><acm-class>H.2.8</acm-class><doi>10.1145/2882903.2882929</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The steady growth of graph data from social networks has resulted in
wide-spread research in finding solutions to the influence maximization
problem. In this paper, we propose a holistic solution to the influence
maximization (IM) problem. (1) We introduce an opinion-cum-interaction (OI)
model that closely mirrors the real-world scenarios. Under the OI model, we
introduce a novel problem of Maximizing the Effective Opinion (MEO) of
influenced users. We prove that the MEO problem is NP-hard and cannot be
approximated within a constant ratio unless P=NP. (2) We propose a heuristic
algorithm OSIM to efficiently solve the MEO problem. To better explain the OSIM
heuristic, we first introduce EaSyIM - the opinion-oblivious version of OSIM, a
scalable algorithm capable of running within practical compute times on
commodity hardware. In addition to serving as a fundamental building block for
OSIM, EaSyIM is capable of addressing the scalability aspect - memory
consumption and running time, of the IM problem as well.
  Empirically, our algorithms are capable of maintaining the deviation in the
spread always within 5% of the best known methods in the literature. In
addition, our experiments show that both OSIM and EaSyIM are effective,
efficient, scalable and significantly enhance the ability to analyze real
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03111</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03111</id><created>2016-02-09</created><authors><author><keyname>Lin</keyname><forenames>Yishi</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Boosting Information Spread: An Algorithmic Approach</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization has been actively studied in recent years. The
majority of prior studies focus on how to target highly influential users as
initial adopters so that they can trigger substantial information cascades in
social networks. In this paper, we consider a novel problem of how to increase
the influence spread when initial adopters are given. We assume that we can
&quot;boost&quot; a small set of users by offering them incentives (e.g., discounts) so
that they are more likely to be influenced by their friends. We study the
$k$-boosting problem which asks for $k$ users so that the influence spread upon
&quot;boosting&quot; them is maximized. Both the NP-hardness of the problem and the
non-submodularity of the objective function pose challenges to the $k$-boosting
problem. To tackle the problem, we devise efficient algorithms that have a
data-dependent approximation ratio. For the $k$-boosting problem on bidirected
trees, we present an efficient greedy algorithm and a rounded dynamic
programming that is a fully polynomial-time approximation scheme. We conduct
extensive experiments using real social networks and synthetic bidirected
trees. Experimental results verify the efficiency and effectiveness of the
proposed algorithms. In our experiments, boosting solutions returned by our
algorithms achieve boosts of influence that are $3\%$ to $2555\%$ higher than
those achieved by boosting solutions returned by intuitive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03115</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03115</id><created>2016-02-09</created><authors><author><keyname>Xiao</keyname><forenames>Li</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Huo</keyname><forenames>Haiye</forenames></author></authors><title>Towards Robustness in Residue Number Systems</title><categories>cs.IT math.IT math.NT</categories><comments>32 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of robustly reconstructing a large number from its erroneous
remainders with respect to several moduli, namely the robust remaindering
problem, may occur in many applications including phase unwrapping, frequency
detection from several undersampled waveforms, wireless sensor networks, etc.
Assuming that the dynamic range of the large number is the maximal possible
one, i.e., the least common multiple (lcm) of all the moduli, a method called
robust Chinese remainder theorem (CRT) for solving the robust remaindering
problem has been recently proposed. In this paper, by relaxing the assumption
that the dynamic range is fixed to be the lcm of all the moduli, a trade-off
between the dynamic range and the robustness bound for two-modular systems is
studied. It basically says that a decrease in the dynamic range may lead to an
increase of the robustness bound. We first obtain a general condition on the
remainder errors and derive the exact dynamic range with a closed-form formula
for the robustness to hold. We then propose simple closed-form reconstruction
algorithms. Furthermore, the newly obtained two-modular results are applied to
the robust reconstruction for multi-modular systems and generalized to real
numbers. Finally, some simulations are carried out to verify our proposed
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03117</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03117</id><created>2016-02-09</created><authors><author><keyname>Cyran</keyname><forenames>Michael</forenames></author><author><keyname>Schotsch</keyname><forenames>Birgit</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author><author><keyname>Fischer</keyname><forenames>Robert F. H.</forenames></author><author><keyname>Forutan</keyname><forenames>Vahid</forenames></author></authors><title>Layering of Communication Networks and a Forward-Backward Duality</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In layered communication networks there are only connections between
intermediate nodes in adjacent layers. Applying network coding to such networks
provides a number of benefits in theory as well as in practice. We propose a
&quot;layering procedure&quot; to transform an arbitrary network into a layered
structure. Furthermore, we derive a &quot;forward-backward duality&quot; for linear
network codes, which can be seen as an analogon to the &quot;uplink-downlink
duality&quot; in MIMO communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03124</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03124</id><created>2016-02-09</created><updated>2016-03-02</updated><authors><author><keyname>Kazda</keyname><forenames>Alexandr</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Rol&#xed;nek</keyname><forenames>Michal</forenames></author></authors><title>Even Delta-Matroids and the Complexity of Planar Boolean CSPs</title><categories>cs.CC</categories><comments>28 pages, 9 figures</comments><msc-class>68Q25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main result of our paper is a generalization of the classical blossom
algorithm for finding perfect matchings that can efficiently solve Boolean CSPs
where each variable appears in exactly two constraints and all constraints are
even $\Delta$-matroid relations (represented by lists of tuples). As a
consequence of this, we settle the complexity classification of planar Boolean
CSPs started by Dvo\v{r}\'ak and Kupec.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03139</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03139</id><created>2016-02-09</created><authors><author><keyname>Guiochet</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>LAAS-TSF</affiliation></author></authors><title>Hazard analysis of human--robot interactions with HAZOP--UML</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>Safety Science, Elsevier, 2016, 84</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New safety critical systems are about to appear in our everyday life:
advanced robots able to interact with humans and perform tasks at home, in
hospitals , or at work. A hazardous behavior of those systems, induced by
failures or extreme environment conditions, may lead to catastrophic
consequences. Well-known risk analysis methods used in other critical domains
(e.g., avion-ics, nuclear, medical, transportation), have to be extended or
adapted due to the non-deterministic behavior of those systems, evolving in
unstructured environments. One major challenge is thus to develop methods that
can be applied at the very beginning of the development process, to identify
hazards induced by robot tasks and their interactions with humans. In this
paper we present a method which is based on an adaptation of a hazard
identification technique, HAZOP (Hazard Operability), coupled with a system
description notation, UML (Unified Modeling Language). This systematic approach
has been applied successfully in research projects, and is now applied by robot
manufacturers. Some results of those studies are presented and discussed to
explain the benefits and limits of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03145</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03145</id><created>2016-02-09</created><authors><author><keyname>Noyel</keyname><forenames>Guillaume</forenames><affiliation>CMM</affiliation></author><author><keyname>Angulo</keyname><forenames>Jesus</forenames><affiliation>CMM</affiliation></author><author><keyname>Jeulin</keyname><forenames>Dominique</forenames><affiliation>CMM</affiliation></author></authors><title>A New Spatio-Spectral Morphological Segmentation For Multi-Spectral
  Remote-Sensing Images</title><categories>cs.CV</categories><proxy>ccsd</proxy><journal-ref>International Journal of Remote Sensing, Taylor \&amp; Francis, 2010,
  31 (22), pp.5895-5920</journal-ref><doi>10.1080/01431161.2010.512314</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general framework of spatio-spectral segmentation for multi-spectral images
is introduced in this paper. The method is based on classification-driven
stochastic watershed (WS) by Monte Carlo simulations, and it gives more regular
and reliable contours than standard WS. The present approach is decomposed into
several sequential steps. First, a dimensionality-reduction stage is performed
using the factor-correspondence analysis method. In this context, a new way to
select the factor axes (eigenvectors) according to their spatial information is
introduced. Then, a spectral classification produces a spectral
pre-segmentation of the image. Subsequently, a probability density function
(pdf) of contours containing spatial and spectral information is estimated by
simulation using a stochastic WS approach driven by the spectral
classification. The pdf of the contours is finally segmented by a WS controlled
by markers from a regularization of the initial classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03146</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03146</id><created>2016-02-09</created><authors><author><keyname>Katariya</keyname><forenames>Sumeet</forenames></author><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author></authors><title>DCM Bandits: Learning to Rank with Multiple Clicks</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines recommend a list of web pages. The user examines this list,
from the first page to the last, and may click on multiple attractive pages.
This type of user behavior can be modeled by the \emph{dependent click model
(DCM)}. In this work, we propose \emph{DCM bandits}, an online learning variant
of the DCM model where the objective is to maximize the probability of
recommending a satisfactory item. The main challenge of our problem is that the
learning agent does not observe the reward. It only observes the clicks. This
imbalance between the feedback and rewards makes our setting challenging. We
propose a computationally-efficient learning algorithm for our problem, which
we call dcmKL-UCB; derive gap-dependent upper bounds on its regret under
reasonable assumptions; and prove a matching lower bound up to logarithmic
factors. We experiment with dcmKL-UCB on both synthetic and real-world
problems. Our algorithm outperforms a range of baselines and performs well even
when our modeling assumptions are violated. To the best of our knowledge, this
is the first regret-optimal online learning algorithm for learning to rank with
multiple clicks in a cascade-like model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03153</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03153</id><created>2016-02-09</created><authors><author><keyname>Zhou</keyname><forenames>You</forenames></author><author><keyname>Zhou</keyname><forenames>Yian</forenames></author><author><keyname>Chen</keyname><forenames>Shigang</forenames></author><author><keyname>Kreidl</keyname><forenames>O. Patrick</forenames></author></authors><title>Limiting Self-Propagating Malware Based on Connection Failure Behavior
  through Hyper-Compact Estimators</title><categories>cs.NI cs.CR</categories><comments>International Journal of Network Security &amp; Its Applications (IJNSA)
  Vol.8, No.1, January 2016</comments><doi>10.5121/ijnsa.2016.8101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-propagating malware (e.g., an Internet worm) exploits security loopholes
in software to infect servers and then use them to scan the Internet for more
vulnerable servers. While the mechanisms of worm infection and their
propagation models are well understood, defense against worms remains an open
problem. One branch of defense research investigates the behavioral difference
between worm-infected hosts and normal hosts to set them apart. One particular
observation is that a worm-infected host, which scans the Internet with
randomly selected addresses, has a much higher connection-failure rate than a
normal host. Rate-limit algorithms have been proposed to control the spread of
worms by traffic shaping based on connection failure rate. However, these
rate-limit algorithms can work properly only if it is possible to measure
failure rates of individual hosts efficiently and accurately. This paper points
out a serious problem in the prior method. To address this problem, we first
propose a solution based on a highly efficient double-bitmap data structure,
which places only a small memory footprint on the routers, while providing good
measurement of connection failure rates whose accuracy can be tuned by system
parameters. Furthermore, we propose another solution based on shared register
array data structure, achieving better memory efficiency and much larger
estimation range than our double-bitmap solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03199</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03199</id><created>2016-02-09</created><authors><author><keyname>Hoang</keyname><forenames>Thang</forenames></author><author><keyname>Choi</keyname><forenames>Deokjai</forenames></author><author><keyname>Nguyen</keyname><forenames>Thuc</forenames></author></authors><title>On the Instability of Sensor Orientation in Gait Verification on Mobile
  Phone</title><categories>cs.CR</categories><comments>12 pages, 7 figures</comments><journal-ref>SECRYPT 2015, pp. 148-159</journal-ref><doi>10.5220/0005572001480159</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authentication schemes using tokens or biometric modalities have been
proposed to ameliorate the security strength on mobile devices. However, the
existing approaches are obtrusive since the user is required to perform
explicit gestures in order to be authenticated. While the gait signal captured
by inertial sensors is understood to be a reliable profile for effective
implicit authentication, recent studies have been conducted in ideal conditions
and might therefore be inapplicable in the real mobile context. Particularly,
the acquiring sensor is always fixed to a specific position and orientation.
This paper mainly focuses on addressing the instability of sensor's orientation
which mostly happens in the reality. A flexible solution taking advantages of
available sensors on mobile devices which can help to handle this problem is
presented. Moreover, a novel gait recognition method utilizes statistical
analysis and supervised learning to adapt itself to the instability of the
biometric gait under various circumstances is also proposed. By adopting
PCA+SVM to construct the gait model, the proposed method outperformed other
state-of-the-art studies, with an equal error rate of 2.45\% and accuracy rate
of 99.14\% in terms of the verification and identification aspects being
achieved, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03202</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03202</id><created>2016-02-06</created><authors><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Alsheikh</keyname><forenames>Mohammad Abu</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Market Model and Optimal Pricing Scheme of Big Data and Internet of
  Things (IoT)</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data has been emerging as a new approach in utilizing large datasets to
optimize complex system operations. Big data is fueled with Internet-of-Things
(IoT) services that generate immense sensory data from numerous sensors and
devices. While most current research focus of big data is on machine learning
and resource management design, the economic modeling and analysis have been
largely overlooked. This paper thus investigates the big data market model and
optimal pricing scheme. We first study the utility of data from the data
science perspective, i.e., using the machine learning methods. We then
introduce the market model and develop an optimal pricing scheme afterward. The
case study shows clearly the suitability of the proposed data utility
functions. The numerical examples demonstrate that big data and IoT service
provider can achieve the maximum profit through the proposed market model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03203</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03203</id><created>2016-02-09</created><authors><author><keyname>Sidor</keyname><forenames>Szymon</forenames></author><author><keyname>Yu</keyname><forenames>Peng</forenames></author><author><keyname>Fang</keyname><forenames>Cheng</forenames></author><author><keyname>Williams</keyname><forenames>Brian</forenames></author></authors><title>Time Resource Networks</title><categories>cs.AI</categories><comments>7 pages, submitted for review to IJCAI16</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of scheduling under resource constraints is widely applicable.
One prominent example is power management, in which we have a limited
continuous supply of power but must schedule a number of power-consuming tasks.
Such problems feature tightly coupled continuous resource constraints and
continuous temporal constraints.
  We address such problems by introducing the Time Resource Network (TRN), an
encoding for resource-constrained scheduling problems. The definition allows
temporal specifications using a general family of representations derived from
the Simple Temporal network, including the Simple Temporal Network with
Uncertainty, and the probabilistic Simple Temporal Network (Fang et al.
(2014)).
  We propose two algorithms for determining the consistency of a TRN: one based
on Mixed Integer Programing and the other one based on Constraint Programming,
which we evaluate on scheduling problems with Simple Temporal Constraints and
Probabilistic Temporal Constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03205</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03205</id><created>2016-02-09</created><authors><author><keyname>Abdmouleh</keyname><forenames>Med Karim</forenames></author><author><keyname>Khalfallah</keyname><forenames>Ali</forenames></author><author><keyname>Bouhlel</keyname><forenames>Med Salim</forenames></author></authors><title>Image encryption with dynamic chaotic Look-Up Table</title><categories>cs.CR cs.CV</categories><comments>7 pages, 12 figures, 6th International Conference on Sciences of
  Electronics, Technologies of Information and Telecommunications (SETIT), 2012</comments><doi>10.1109/SETIT.2012.6481937</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel image encryption scheme. The proposed method
is based on the chaos theory. Our cryptosystem uses the chaos theory to define
a dynamic chaotic Look-Up Table (LUT) to compute the new value of the current
pixel to cipher. Applying this process on each pixel of the plain image, we
generate the encrypted image. The results of different experimental tests, such
as Key space analysis, Information Entropy and Histogram analysis, show that
the proposed encryption image scheme seems to be protected against various
attacks. A comparison between the plain and encrypted image, in terms of
correlation coefficient, proves that the plain image is very different from the
encrypted one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03206</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03206</id><created>2016-02-06</created><authors><author><keyname>Sala</keyname><forenames>Filip A.</forenames></author></authors><title>Design of false color palettes for grayscale reproduction</title><categories>cs.GR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design of false color palette is quite easy but some effort has to be done to
achieve good dynamic range, contrast and overall appearance of the palette.
Such palettes, for instance, are commonly used in scientific papers for
presenting the data. However, to lower the cost of the paper most scientists
decide to let the data to be printed in grayscale. The same applies to e-book
readers based on e-ink where most of them are still grayscale. For majority of
false color palettes reproducing them in grayscale results in ambiguous mapping
of the colors and may be misleading for the reader. In this article design of
false color palettes suitable for grayscale reproduction is described. Due to
the monotonic change of luminance of these palettes grayscale representation is
very similar to the data directly presented with a grayscale palette. Some
suggestions and examples how to design such palettes are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03208</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03208</id><created>2016-02-05</created><authors><author><keyname>Barmpalias</keyname><forenames>George</forenames></author><author><keyname>Lewis-Pye</keyname><forenames>Andrew</forenames></author><author><keyname>Fang</keyname><forenames>Nan</forenames></author></authors><title>Optimal asymptotic bounds on the oracle use in computations from
  Chaitin's Omega</title><categories>math.LO cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaitin's number Omega is the halting probability of a universal prefix-free
machine, and although it depends on the underlying enumeration of prefix-free
machines, it is always Turing-complete. It can be observed, in fact, that for
every computably enumerable (c.e.) real, there exists a Turing functional via
which Omega computes it, and such that the number of bits of omega that are
needed for the computation of the first n bits of the given number (i.e. the
use on argument n) is bounded above by a computable function h(n) = n+o(n). We
characterise the asymptotic upper bounds on the use of Chaitin's omega in
oracle computations of halting probabilities (i.e. c.e. reals). We show that
the following two conditions are equivalent for any computable function h such
that h(n)-n is non-decreasing: (1) h(n)-n is an information content measure,
(2) for every c.e. real there exists a Turing functional via which omega
computes the real with use bounded by h. We also give a similar
characterisation with respect to computations of c.e. sets from Omega, by
showing that the following are equivalent for any computable non-decreasing
function g: (1) g is an information-content measure, (2) for every c.e. set A,
Omega computes A with use bounded by g. Further results and some connections
with Solovay functions are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03218</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03218</id><created>2016-02-09</created><updated>2016-02-23</updated><authors><author><keyname>Andrychowicz</keyname><forenames>Marcin</forenames></author><author><keyname>Kurach</keyname><forenames>Karol</forenames></author></authors><title>Learning Efficient Algorithms with Hierarchical Attentive Memory</title><categories>cs.LG</categories><comments>Added soft attention appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and investigate a novel memory architecture for
neural networks called Hierarchical Attentive Memory (HAM). It is based on a
binary tree with leaves corresponding to memory cells. This allows HAM to
perform memory access in O(log n) complexity, which is a significant
improvement over the standard attention mechanism that requires O(n)
operations, where n is the size of the memory.
  We show that an LSTM network augmented with HAM can learn algorithms for
problems like merging, sorting or binary searching from pure input-output
examples. In particular, it learns to sort n numbers in time O(n log n) and
generalizes well to input sequences much longer than the ones seen during the
training. We also show that HAM can be trained to act like classic data
structures: a stack, a FIFO queue and a priority queue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03220</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03220</id><created>2016-02-09</created><updated>2016-02-15</updated><authors><author><keyname>Lamb</keyname><forenames>Alex</forenames></author><author><keyname>Dumoulin</keyname><forenames>Vincent</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author></authors><title>Discriminative Regularization for Generative Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the question of whether the representations learned by classifiers
can be used to enhance the quality of generative models. Our conjecture is that
labels correspond to characteristics of natural data which are most salient to
humans: identity in faces, objects in images, and utterances in speech. We
propose to take advantage of this by using the representations from
discriminative classifiers to augment the objective function corresponding to a
generative model. In particular we enhance the objective function of the
variational autoencoder, a popular generative model, with a discriminative
regularization term. We show that enhancing the objective function in this way
leads to samples that are clearer and have higher visual quality than the
samples from the standard variational autoencoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03228</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03228</id><created>2016-02-09</created><authors><author><keyname>Harland</keyname><forenames>James</forenames></author></authors><title>Busy Beaver Machines and the Observant Otter Heuristic (or How to Tame
  Dreadful Dragons)</title><categories>cs.FL</categories><comments>This has been submitted to the Theoretical Computer Science journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The busy beaver is a well-known specific example of a non-computable
function. Whilst many aspect of this problem have been investigated, it is not
always easy to find thorough and convincing evidence for the claims made about
the maximality of particular machines, and the phenomenal size of some of the
numbers involved means that it is not obvious that the problem can be feasibly
addressed at all. In this paper we address both of these issues. We discuss a
framework in which the busy beaver problem and similar problems may be
addressed, and the appropriate processes for providing evidence of claims made.
We also show how a simple heuristic, which we call the observant otter, can be
used to evaluate machines with an extremely large number of execution steps
required to terminate. We also show empirical results for an implementation of
this heuristic which show how this heuristic is effective for all known
`monster' machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03231</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03231</id><created>2016-02-09</created><authors><author><keyname>D'Aniello</keyname><forenames>Alma</forenames></author><author><keyname>de Luca</keyname><forenames>Aldo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author></authors><title>On Christoffel and standard words and their derivatives</title><categories>cs.DM cs.FL math.CO</categories><comments>28 pages. Final version, to appear in TCS</comments><msc-class>68R15</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and study natural derivatives for Christoffel and finite
standard words, as well as for characteristic Sturmian words. These
derivatives, which are realized as inverse images under suitable morphisms,
preserve the aforementioned classes of words. In the case of Christoffel words,
the morphisms involved map $a$ to $a^{k+1}b$ (resp.,~$ab^{k}$) and $b$ to
$a^{k}b$ (resp.,~$ab^{k+1}$) for a suitable $k&gt;0$. As long as derivatives are
longer than one letter, higher-order derivatives are naturally obtained. We
define the depth of a Christoffel or standard word as the smallest order for
which the derivative is a single letter. We give several combinatorial and
arithmetic descriptions of the depth, and (tight) lower and upper bounds for
it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03247</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03247</id><created>2016-02-09</created><authors><author><keyname>Pi&#xf1;a-Garc&#xed;a</keyname><forenames>C. A.</forenames></author><author><keyname>Gu</keyname><forenames>Dongbing</forenames></author><author><keyname>Siqueiros-Garc&#xed;a</keyname><forenames>J. Mario</forenames></author><author><keyname>Carre&#xf3;n</keyname><forenames>Gustavo</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Exploring Dynamic Environments Using Stochastic Search Strategies</title><categories>cs.RO cs.CG cs.DS</categories><comments>8 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we conduct a literature review of laws of motion based on
stochastic search strategies which are mainly focused on exploring highly
dynamic environments. In this regard, stochastic search strategies represent an
interesting alternative to cope with uncertainty and reduced perceptual
capabilities. This study aims to present an introductory overview of research
in terms of directional rules and searching methods mainly based on
bio-inspired approaches. This study critically examines the role of animal
searching behavior applied to random walk models using stochastic rules and
kinesis or taxis. The aim of this study is to examine existing techniques and
to select relevant work on random walks and analyze their actual contributions.
In this regard, we cover a wide range of displacement events with an
orientation mechanism given by a reactive behavior or a source-seeking
behavior. Finally, we conclude with a discussion concerning the usefulness of
using optimal foraging strategies as a reliable methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03254</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03254</id><created>2016-02-09</created><authors><author><keyname>Gay</keyname><forenames>Simon</forenames><affiliation>University of Glasgow, UK</affiliation></author><author><keyname>Alglave</keyname><forenames>Jade</forenames><affiliation>University College London, UK</affiliation></author></authors><title>Proceedings Eighth International Workshop on Programming Language
  Approaches to Concurrency- and Communication-cEntric Software</title><categories>cs.PL cs.DC cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016</journal-ref><doi>10.4204/EPTCS.203</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PLACES 2015 (full title: Programming Language Approaches to Concurrency- and
Communication-Centric Software) is the eighth edition of the PLACES workshop
series. After the first PLACES, which was affiliated to DisCoTec in 2008, the
workshop has been part of ETAPS every year since 2009 and is now an established
part of the ETAPS satellite events. PLACES 2015 was held on 18th April in
London, UK.
  The workshop series was started in order to promote the application of novel
programming language ideas to the increasingly important problem of developing
software for systems in which concurrency and communication are intrinsic
aspects. This includes software for both multi-core systems and large-scale
distributed and/or service-oriented systems. The scope of PLACES includes new
programming language features, whole new programming language designs, new type
systems, new semantic approaches, new program analysis techniques, and new
implementation mechanisms.
  This volume consists of revised versions of the papers that were presented at
the workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03256</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03256</id><created>2016-02-09</created><authors><author><keyname>Mandal</keyname><forenames>Bappaditya</forenames></author></authors><title>Improved Eigenfeature Regularization for Face Identification</title><categories>cs.CV</categories><comments>6 pages, 4 figures, ICIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose to divide each class (a person) into subclasses
using spatial partition trees which helps in better capturing the
intra-personal variances arising from the appearances of the same individual.
We perform a comprehensive analysis on within-class and within-subclass
eigenspectrums of face images and propose a novel method of eigenspectrum
modeling which extracts discriminative features of faces from both
within-subclass and total or between-subclass scatter matrices. Effective
low-dimensional face discriminative features are extracted for face recognition
(FR) after performing discriminant evaluation in the entire eigenspace.
Experimental results on popular face databases (AR, FERET) and the challenging
unconstrained YouTube Face database show the superiority of our proposed
approach on all three databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03258</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03258</id><created>2016-02-09</created><updated>2016-02-12</updated><authors><author><keyname>Vikram</keyname><forenames>Sharad</forenames></author><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Interactive Bayesian Hierarchical Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is a powerful tool in data analysis, but it is often difficult to
find a grouping that aligns with a user's needs. To address this, several
methods incorporate constraints obtained from users into clustering algorithms,
but unfortunately do not apply to hierarchical clustering. We design an
interactive Bayesian algorithm that incorporates user interaction into
hierarchical clustering while still utilizing the geometry of the data by
sampling a constrained posterior distribution over hierarchies. We also suggest
several ways to intelligently query a user. The algorithm, along with the
querying schemes, shows promising results on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03264</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03264</id><created>2016-02-09</created><authors><author><keyname>Xie</keyname><forenames>Jianwen</forenames></author><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author><author><keyname>Wu</keyname><forenames>Ying Nian</forenames></author></authors><title>A Theory of Generative ConvNet</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convolutional neural network (ConvNet or CNN) is a powerful
discriminative learning machine. In this paper, we show that a generative
random field model that we call generative ConvNet can be derived from the
discriminative ConvNet. The probability distribution of the generative ConvNet
model is in the form of exponential tilting of a reference distribution.
Assuming re-lu non-linearity and Gaussian white noise reference distribution,
we show that the generative ConvNet model contains a representational structure
with multiple layers of binary activation variables. The model is non-Gaussian,
or more precisely, piecewise Gaussian, where each piece is determined by an
instantiation of the binary activation variables that reconstruct the mean of
the Gaussian piece. The Langevin dynamics for synthesis is driven by the
reconstruction error, and the corresponding gradient descent dynamics converges
to a local energy minimum that is auto-encoding. As for learning, we show that
the contrastive divergence learning tends to reconstruct the observed images.
Finally, we show that the maximum likelihood learning algorithm can generate
realistic natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03265</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03265</id><created>2016-02-09</created><updated>2016-02-10</updated><authors><author><keyname>Nematzadeh</keyname><forenames>Aida</forenames></author><author><keyname>Miscevic</keyname><forenames>Filip</forenames></author><author><keyname>Stevenson</keyname><forenames>Suzanne</forenames></author></authors><title>Simple Search Algorithms on Semantic Networks Learned from Language Use</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent empirical and modeling research has focused on the semantic fluency
task because it is informative about semantic memory. An interesting interplay
arises between the richness of representations in semantic memory and the
complexity of algorithms required to process it. It has remained an open
question whether representations of words and their relations learned from
language use can enable a simple search algorithm to mimic the observed
behavior in the fluency task. Here we show that it is plausible to learn rich
representations from naturalistic data for which a very simple search algorithm
(a random walk) can replicate the human patterns. We suggest that explicitly
structuring knowledge about words into a semantic network plays a crucial role
in modeling human behavior in memory search and retrieval; moreover, this is
the case across a range of semantic information sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03266</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03266</id><created>2016-02-09</created><updated>2016-02-12</updated><authors><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Prabhu</keyname><forenames>Vinayak S.</forenames></author></authors><title>Computing Distances between Reach Flowpipes</title><categories>cs.SY cs.CG</categories><comments>Full version of paper accepted at HSCC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate quantifying the difference between two hybrid dynamical
systems under noise and initial-state uncertainty. While the set of traces for
these systems is infinite, it is possible to symbolically approximate trace
sets using \emph{reachpipes} that compute upper and lower bounds on the
evolution of the reachable sets with time. We estimate distances between
corresponding sets of trajectories of two systems in terms of distances between
the reachpipes.
  In case of two individual traces, the Skorokhod distance has been proposed as
a robust and efficient notion of distance which captures both value and timing
distortions. In this paper, we extend the computation of the Skorokhod distance
to reachpipes, and provide algorithms to compute upper and lower bounds on the
distance between two sets of traces. Our algorithms use new geometric insights
that are used to compute the worst-case and best-case distances between two
polyhedral sets evolving with time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03273</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03273</id><created>2016-02-10</created><updated>2016-03-06</updated><authors><author><keyname>Kanuparthy</keyname><forenames>Partha</forenames></author><author><keyname>Dai</keyname><forenames>Yuchen</forenames></author><author><keyname>Fatourehchi</keyname><forenames>Vahid</forenames></author><author><keyname>Pathak</keyname><forenames>Sudhir</forenames></author><author><keyname>Samal</keyname><forenames>Sambit</forenames></author><author><keyname>Narayan</keyname><forenames>P. P. S.</forenames></author></authors><title>YTrace: End-to-end Performance Diagnosis in Large Content Providers</title><categories>cs.DC</categories><acm-class>B.8.2; C.2.4; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content providers build serving stacks to deliver content to users. An
important goal of a content provider is to ensure good user experience, since
user experience has an impact on revenue. In this paper, we describe a system
at Yahoo called YTrace that diagnoses bad user experience in near real time. We
present the different components of YTrace for end-to-end diagnosis. We show
two case studies of performance problems with user sessions. We describe a
number of open directions for performance diagnosis in content providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03275</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03275</id><created>2016-02-10</created><authors><author><keyname>Arapostathis</keyname><forenames>Ari</forenames></author><author><keyname>Pang</keyname><forenames>Guodong</forenames></author></authors><title>Infinite Horizon Average Optimality of the N-network Queueing Model in
  the Halfin-Whitt Regime</title><categories>math.OC cs.SY math.PR</categories><comments>33 pages</comments><msc-class>60K25, 68M20, 90B22, 90B36</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the infinite horizon optimal control problem for N-network queueing
systems, which consist of two customer classes and two server pools, under
average (ergodic) criteria in the Halfin-Whitt regime. We consider three
control objectives: 1) minimizing the queueing (and idleness) cost, 2)
minimizing the queueing cost while imposing a constraint on idleness at each
server pool, and 3) minimizing the queueing cost while requiring fairness on
idleness. The running costs can be any nonnegative convex functions having at
most polynomial growth.
  For all three problems we establish asymptotic optimality, namely, the
convergence of the value functions of the diffusion-scaled state process to the
corresponding values of the controlled diffusion limit. We also present a
simple state-dependent priority scheduling policy under which the
diffusion-scaled state process is geometrically ergodic in the Halfin-Whitt
regime, and some results on convergence of mean empirical measures which
facilitate the proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03277</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03277</id><created>2016-02-10</created><authors><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Miao</keyname><forenames>Chunyan</forenames></author></authors><title>A Survey of Incentives and Mechanism Design for Human Computation
  Systems</title><categories>cs.HC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human computation systems (HCSs) have been widely adopted in various domains.
Their goal is to harness human intelligence to solve computational problems
that are beyond the capability of modern computers. One of the most challenging
problems in HCSs is how to incentivize a broad range of users to participate in
the system and make high efforts. This article surveys the field of HCSs from
the perspective of incentives and mechanism design. We first review
state-of-the-art HCSs, focusing on how incentives are provided to users. We
then use mechanism design to theoretically analyze different incentives. We
survey the mechanisms derived from state-of-the-art HCSs as well as classic
mechanisms that have been used in HCSs. Finally, we discuss eight promising
research directions for designing incentives in HCSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03283</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03283</id><created>2016-02-10</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Samrat</forenames></author><author><keyname>Das</keyname><forenames>Bijit Kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mrityunjoy</forenames></author></authors><title>Performance Analysis of $l_0$ Norm Constrained Recursive Least Squares
  Algorithm</title><categories>cs.IT math.IT nlin.AO stat.ME</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance analysis of $l_0$ norm constrained Recursive least Squares (RLS)
algorithm is attempted in this paper. Though the performance pretty attractive
compared to its various alternatives, no thorough study of theoretical analysis
has been performed. Like the popular $l_0$ Least Mean Squares (LMS) algorithm,
in $l_0$ RLS, a $l_0$ norm penalty is added to provide zero tap attractions on
the instantaneous filter taps. A thorough theoretical performance analysis has
been conducted in this paper with white Gaussian input data under assumptions
suitable for many practical scenarios. An expression for steady state MSD is
derived and analyzed for variations of different sets of predefined variables.
Also a Taylor series expansion based approximate linear evolution of the
instantaneous MSD has been performed. Finally numerical simulations are carried
out to corroborate the theoretical analysis and are shown to match well for a
wide range of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03291</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03291</id><created>2016-02-10</created><authors><author><keyname>Rahman</keyname><forenames>Habibur</forenames></author><author><keyname>Joppa</keyname><forenames>Lucas</forenames></author><author><keyname>Roy</keyname><forenames>Senjuti Basu</forenames></author></authors><title>Feature Based Task Recommendation in Crowdsourcing with Implicit
  Observations</title><categories>cs.AI cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing research in crowdsourcing has investigated how to recommend tasks to
workers based on which task the workers have already completed, referred to as
{\em implicit feedback}. We, on the other hand, investigate the task
recommendation problem, where we leverage both implicit feedback and explicit
features of the task. We assume that we are given a set of workers, a set of
tasks, interactions (such as the number of times a worker has completed a
particular task), and the presence of explicit features of each task (such as,
task location). We intend to recommend tasks to the workers by exploiting the
implicit interactions, and the presence or absence of explicit features in the
tasks. We formalize the problem as an optimization problem, propose two
alternative problem formulations and respective solutions that exploit implicit
feedback, explicit features, as well as similarity between the tasks. We
compare the efficacy of our proposed solutions against multiple
state-of-the-art techniques using two large scale real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03297</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03297</id><created>2016-02-10</created><authors><author><keyname>Cheng</keyname><forenames>Hao-Chung</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>On the Concavity of Auxiliary Function in Classical-Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The auxiliary function of a classical channel appears in two fundamental
quantities that upper and lower bound the error probability, respectively. A
crucial property of the auxiliary function is its concavity, which leads to
several important results in finite block length analysis. In this paper, we
prove that the auxiliary function of a classical-quantum channel also enjoys
the same concave property, extending an earlier partial result to its full
generality. The key component in our proof is a beautiful result of geometric
means of operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03303</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03303</id><created>2016-02-10</created><authors><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>Fischer</keyname><forenames>Matthias</forenames></author><author><keyname>Jung</keyname><forenames>Daniel</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author></authors><title>Asymptotically Optimal Gathering on a Grid</title><categories>cs.DC cs.MA</categories><comments>arXiv admin note: substantial text overlap with arXiv:1510.05454</comments><acm-class>F.1.2; F.2.2; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we solve the local gathering problem of a swarm of $n$
indistinguishable, point-shaped robots on a two dimensional grid in
asymptotically optimal time $\mathcal{O}(n)$ in the fully synchronous
$\mathcal{FSYNC}$ time model. Given an arbitrarily distributed (yet connected)
swarm of robots, the gathering problem on the grid is to locate all robots
within a $2\times 2$-sized area that is not known beforehand. Two robots are
connected if they are vertical or horizontal neighbors on the grid. The
locality constraint means that no global control, no compass, no global
communication and only local vision is available; hence, a robot can only see
its grid neighbors up to a constant $L_1$-distance, which also limits its
movements. A robot can move to one of its eight neighboring grid cells and if
two or more robots move to the same location they are \emph{merged} to be only
one robot. The locality constraint is the significant challenging issue here,
since robot movements must not harm the (only globally checkable) swarm
connectivity. For solving the gathering problem, we provide a synchronous
algorithm -- executed by every robot -- which ensures that robots merge without
breaking the swarm connectivity. In our model, robots can obtain a special
state, which marks such a robot to be performing specific connectivity
preserving movements in order to allow later merge operations of the swarm.
Compared to the grid, for gathering in the Euclidean plane for the same robot
and time model the best known upper bound is $\mathcal{O}(n^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03305</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03305</id><created>2016-02-10</created><updated>2016-02-29</updated><authors><author><keyname>Nguyen</keyname><forenames>Van Minh</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Coverage and capacity scaling laws in downlink ultra-dense cellular
  networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for IEEE ICC 2016 - revised version. 7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by new types of wireless devices and the proliferation of
bandwidth-intensive applications, data traffic and the corresponding network
load are increasing dramatically. Network densification has been recognized as
a promising and efficient way to provide higher network capacity and enhanced
coverage. Most prior work on performance analysis of ultra-dense networks
(UDNs) has focused on random spatial deployment with idealized singular path
loss models and Rayleigh fading. In this paper, we consider a more precise and
general model, which incorporates multi-slope path loss and general fading
distributions. We derive the tail behavior and scaling laws for the coverage
probability and the capacity considering strongest base station association in
a Poisson field network. Our analytical results identify the regimes in which
the signal-to-interference-plus-noise ratio (SINR) either asymptotically grows,
saturates, or decreases with increasing network density. We establish general
results on when UDNs lead to worse or even zero SINR coverage and capacity, and
we provide crisp insights on the fundamental limits of wireless network
densification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03308</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03308</id><created>2016-02-10</created><authors><author><keyname>Barina</keyname><forenames>David</forenames></author></authors><title>Gabor Wavelets in Image Processing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work shows the use of a two-dimensional Gabor wavelets in image
processing. Convolution with such a two-dimensional wavelet can be separated
into two series of one-dimensional ones. The key idea of this work is to
utilize a Gabor wavelet as a multiscale partial differential operator of a
given order. Gabor wavelets are used here to detect edges, corners and blobs. A
performance of such an interest point detector is compared to detectors
utilizing a Haar wavelet and a derivative of a Gaussian function. The proposed
approach may be useful when a fast implementation of the Gabor transform is
available or when the transform is already precomputed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03313</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03313</id><created>2016-02-10</created><authors><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>A Remark on Channels with Transceiver Distortion</title><categories>cs.IT math.IT</categories><comments>2016 Information Theory and Applications (ITA) Workshop, La Jolla,
  CA, USA, Jan.-Feb. 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information transmission over channels with transceiver distortion is
investigated via generalized mutual information (GMI) under Gaussian input
distribution and nearest-neighbor decoding. A canonical transceiver structure
in which the channel output is processed by a minimum mean-squared error
estimator before decoding is established to maximize the GMI, and the
well-known Bussgang's decomposition is shown to be a heuristic that is
consistent with the GMI under linear output processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03316</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03316</id><created>2016-02-10</created><authors><author><keyname>Gunn</keyname><forenames>Lachlan J.</forenames></author><author><keyname>Allison</keyname><forenames>Andrew</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Verifying Public Keys without Trust: How Anonymity Can Guarantee Data
  Integrity</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the inroads that public-key cryptography has made on the web, the
difficulty of key management has for the most part kept it out of the hands of
the general public. With key-distribution software for PGP beginning to
natively support the anonymizing service Tor, we describe a protocol to take
advantage of this, allowing users and identity holders to detect malicious
keyservers with an known and arbitrarily small probability of failure, allowing
users to safely exchange public keys across the internet without the need for
certificate authorities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03320</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03320</id><created>2016-02-10</created><updated>2016-02-25</updated><authors><author><keyname>Silva</keyname><forenames>Arlei</forenames></author><author><keyname>Dang</keyname><forenames>Xuan-Hong</forenames></author><author><keyname>Basu</keyname><forenames>Prithwish</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K</forenames></author><author><keyname>Swami</keyname><forenames>Ananthram</forenames></author></authors><title>Graph Wavelets via Sparse Cuts</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling information that resides on vertices of large graphs is a key
problem in several real-life applications, ranging from social networks to the
Internet-of-things. Signal Processing on Graphs and, in particular, graph
wavelets can exploit the intrinsic smoothness of these datasets in order to
represent them in a both compact and accurate manner. However, how to discover
wavelet bases that capture the geometry of the data with respect to the signal
as well as the graph structure remains an open question. In this paper, we
study the problem of computing graph wavelet bases via sparse cuts in order to
produce low-dimensional encodings of data-driven bases. This problem is
connected to known hard problems in graph theory (e.g. multiway cuts) and thus
requires an efficient heuristic. We formulate the basis discovery task as a
relaxation of a vector optimization problem, which leads to an elegant solution
as a regularized eigenvalue computation. Moreover, we propose several
strategies in order to scale our algorithm to large graphs. Experimental
results show that the proposed algorithm can effectively encode both the graph
structure and signal, producing compressed and accurate representations for
vertex values in a wide range of datasets (e.g. sensor and gene networks) and
outperforming the best baseline by up to 8 times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03328</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03328</id><created>2016-02-10</created><authors><author><keyname>Johnny</keyname><forenames>Milad</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Degrees of Freedom Rate Region of the $K$-user Interference Channel with
  Blind CSIT Using Staggered Antenna Switching</title><categories>cs.IT math.IT</categories><comments>15 pages, 4 figures, This paper submitted to IEEE Transactions on
  Wireless Communications and partially IWCIT 2016. arXiv admin note: text
  overlap with arXiv:1408.6427 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we consider the problem of the interference alignment for the
$K$-user SISO interference channel with blind channel state information at
transmitters (CSIT). Our achievement in contrast to popular $K-$user
interference alignment (IA) scheme has more practical notions. In this case
every receiver is equipped with one reconfigurable antenna which tries to place
its desired signal in a subspace which is linearly independent from
interference signals. We show that if the channel values are known to the
receivers only, the sum degrees-of-freedom (DOF) rate region of the linear BIA
with staggered antenna switching is $\frac{Kr}{r^2-r+K}$, where $r = \left
\lceil{\frac{\sqrt{1+4K}-1}{2}} \right \rceil$. The result indicates that the
optimum DoF rate region of the $K-$user interference channel is to achieve the
DoF of $\frac{\sqrt{K}}{2}$ for an asymptotically large network. Thus, the DoF
of the $K$-user interference channel using staggered antenna switching grows
sub-linearly with the number of the users, whereas it grows linearly in the
case where transmitters access the CSI. In addition we propose both
achievability and converse proof so as to show that this is the DoF rate region
of blind interference alignment (BIA) with staggered antenna switching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03332</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03332</id><created>2016-02-10</created><authors><author><keyname>Moser</keyname><forenames>Philippe</forenames></author></authors><title>Polynomial Depth, Highness and Lowness for E</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relations between the notions of highness, lowness and logical
depth in the setting of complexity theory. We introduce a new notion of
polynomial depth based on time bounded Kolmogorov complexity. We show our
polynomial depth notion satisfies all basic logical depth properties, namely
neither sets in P nor sets random for EXP are polynomial deep, and only
polynomial deep sets can polynomially Turing compute a polynomial deep set. We
prove all EXP- complete sets are poly-deep, and under the assumption that NP
does not have p-measure zero, then NP contains a polynomial deep set. We show
that every high set for E contains a polynomial deep set in its polynomial
Turing degree, and that there exists low for E polynomial deep sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03333</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03333</id><created>2016-02-10</created><authors><author><keyname>Beller</keyname><forenames>Timo</forenames></author><author><keyname>Ohlebusch</keyname><forenames>Enno</forenames></author></authors><title>A representation of a compressed de Bruijn graph for pan-genome analysis
  that enables search</title><categories>cs.DS</categories><comments>Submitted to Algorithmica special issue of CPM2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Marcus et al. (Bioinformatics 2014) proposed to use a compressed de
Bruijn graph to describe the relationship between the genomes of many
individuals/strains of the same or closely related species. They devised an
$O(n \log g)$ time algorithm called splitMEM that constructs this graph
directly (i.e., without using the uncompressed de Bruijn graph) based on a
suffix tree, where $n$ is the total length of the genomes and $g$ is the length
of the longest genome. In this paper, we present a construction algorithm that
outperforms their algorithm in theory and in practice. Moreover, we propose a
new space-efficient representation of the compressed de Bruijn graph that adds
the possibility to search for a pattern (e.g. an allele - a variant form of a
gene) within the pan-genome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03337</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03337</id><created>2016-02-10</created><authors><author><keyname>Kyambille</keyname><forenames>Godphrey G</forenames></author><author><keyname>Kalegele</keyname><forenames>Khamisi</forenames></author></authors><title>Enhancing Patient Appointments Scheduling that Uses Mobile Technology</title><categories>cs.CY</categories><comments>7 pages</comments><journal-ref>International Journal of Computer Science and Information
  Security, 13(11), 21 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Appointment scheduling systems are utilized mainly by specialty care clinics
to manage access to service providers as well as by hospitals to schedule
patient appointments. When attending hospitals in Tanzania, patients experience
challenges to see an appropriate specialist doctor because of service interval
inconsistency. Timely availability of doctors is critical whenever a patient
needs to see a specialist doctor for treatment and a serious bottleneck lies in
the application of appropriate technology techniques to enhance appointment
scheduling. In this paper, we present a mobile based application scheduling
system for managing patient appointments. Furthermore, forthcoming
opportunities for the innovative use of the mobile based application scheduling
system are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03342</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03342</id><created>2016-02-10</created><authors><author><keyname>Mizrahi</keyname><forenames>Tal</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>The Case for Data Plane Timestamping in SDN</title><categories>cs.NI</categories><comments>This technical report is an extended version of &quot;The Case for Data
  Plane Timestamping in SDN&quot;, which was accepted to the IEEE INFOCOM Workshop
  on Software-Driven Flexible and Agile Networking (SWFAN), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the case for Data Plane Timestamping (DPT). We argue that
in the unique environment of Software-Defined Networks (SDN), attaching a
timestamp to the header of all packets is a powerful feature that can be
leveraged by various diverse SDN applications. We analyze three key use cases
that demonstrate the advantages of using DPT, and show that SDN applications
can benefit even from using as little as one bit for the timestamp field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03346</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03346</id><created>2016-02-10</created><authors><author><keyname>Liu</keyname><forenames>Li</forenames></author><author><keyname>Zhou</keyname><forenames>Yi</forenames></author><author><keyname>Shao</keyname><forenames>Ling</forenames></author></authors><title>DAP3D-Net: Where, What and How Actions Occur in Videos?</title><categories>cs.CV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Action parsing in videos with complex scenes is an interesting but
challenging task in computer vision. In this paper, we propose a generic 3D
convolutional neural network in a multi-task learning manner for effective Deep
Action Parsing (DAP3D-Net) in videos. Particularly, in the training phase,
action localization, classification and attributes learning can be jointly
optimized on our appearancemotion data via DAP3D-Net. For an upcoming test
video, we can describe each individual action in the video simultaneously as:
Where the action occurs, What the action is and How the action is performed. To
well demonstrate the effectiveness of the proposed DAP3D-Net, we also
contribute a new Numerous-category Aligned Synthetic Action dataset, i.e.,
NASA, which consists of 200; 000 action clips of more than 300 categories and
with 33 pre-defined action attributes in two hierarchical levels (i.e.,
low-level attributes of basic body part movements and high-level attributes
related to action motion). We learn DAP3D-Net using the NASA dataset and then
evaluate it on our collected Human Action Understanding (HAU) dataset.
Experimental results show that our approach can accurately localize, categorize
and describe multiple actions in realistic videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03348</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03348</id><created>2016-02-10</created><authors><author><keyname>Mankowitz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Mann</keyname><forenames>Timothy A.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)</title><categories>cs.LG cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1506.03624</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement Learning (RL) aims to learn an optimal policy for a Markov
Decision Process (MDP). For complex, high-dimensional MDPs, it may only be
feasible to represent the policy with function approximation. If the policy
representation used cannot represent good policies, the problem is misspecified
and the learned policy may be far from optimal. We introduce IHOMP as an
approach for solving misspecified problems. IHOMP iteratively refines a set of
specialized policies based on a limited representation. We refer to these
policies as policy threads. At the same time, IHOMP stitches these policy
threads together in a hierarchical fashion to solve a problem that was
otherwise misspecified. We prove that IHOMP enjoys theoretical convergence
guarantees and extend IHOMP to exploit Option Interruption (OI) enabling it to
learn where policy threads can be reused. Our experiments demonstrate that
IHOMP can find near-optimal solutions to otherwise misspecified problems and
that OI can further improve the solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03350</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03350</id><created>2016-02-10</created><authors><author><keyname>Renna</keyname><forenames>Francesco</forenames></author><author><keyname>Doyle</keyname><forenames>Joseph</forenames></author><author><keyname>Giotsas</keyname><forenames>Vasileios</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Query Processing For The Internet-of-Things: Coupling Of Device Energy
  Consumption And Cloud Infrastructure Billing</title><categories>cs.NI</categories><comments>To be presented at the 1st IEEE International Conference on
  Internet-of-Things Design and Implementation (IoTDI 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio/visual recognition and retrieval applications have recently garnered
significant attention within Internet-of-Things (IoT) oriented services, given
that video cameras and audio processing chipsets are now ubiquitous even in
low-end embedded systems. In the most typical scenario for such services, each
device extracts audio/visual features and compacts them into feature
descriptors, which comprise media queries. These queries are uploaded to a
remote cloud computing service that performs content matching for
classification or retrieval applications. Two of the most crucial aspects for
such services are: (i) controlling the device energy consumption when using the
service; (ii) reducing the billing cost incurred from the cloud infrastructure
provider. In this paper we derive analytic conditions for the optimal coupling
between the device energy consumption and the incurred cloud infrastructure
billing. Our framework encapsulates: the energy consumption to produce and
transmit audio/visual queries, the billing rates of the cloud infrastructure,
the number of devices concurrently connected to the same cloud server, and the
statistics of the query data production volume per device. Our analytic results
are validated via a deployment with: (i) the device side comprising compact
image descriptors (queries) computed on Beaglebone Linux embedded platforms and
transmitted to Amazon Web Services (AWS) Simple Storage Service; (ii) the cloud
side carrying out image similarity detection via AWS Elastic Compute Cloud
(EC2) spot instances, with the AWS Auto Scaling being used to control the
number of instances according to the demand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03351</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03351</id><created>2016-02-10</created><authors><author><keyname>Mankowitz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Mann</keyname><forenames>Timothy A.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Adaptive Skills, Adaptive Partitions (ASAP)</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Adaptive Skills, Adaptive Partitions (ASAP) algorithm that
(1) learns skills (i.e., temporally extended actions or options) as well as (2)
where to apply them to solve a Markov decision process. ASAP is initially
provided with a misspecified hierarchical model and is able to correct this
model and learn a near-optimal set of skills to solve a given task. We believe
that (1) and (2) are the core components necessary for a truly general skill
learning framework, which is a key building block needed to scale up to
lifelong learning agents. ASAP is also able to solve related new tasks simply
by adapting where it applies its existing learned skills. We prove that ASAP
converges to a local optimum under natural conditions. Finally, our extensive
experimental results, which include a RoboCup domain, demonstrate the ability
of ASAP to learn where to reuse skills as well as solve multiple tasks with
considerably less experience than solving each task from scratch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03364</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03364</id><created>2016-02-10</created><authors><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Relations on words</title><categories>cs.FL math.CO</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this survey, we present classical notions arising in
combinatorics on words: growth function of a language, complexity function of
an infinite word, pattern avoidance, periodicity and uniform recurrence. Our
presentation tries to set up a unified framework with respect to a given binary
relation.
  In the second part, we mainly focus on abelian equivalence, $k$-abelian
equivalence, combinatorial coefficients and associated relations, Parikh
matrices and $M$-equivalence. In particular, some new refinements of abelian
equivalence are introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03368</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03368</id><created>2016-02-10</created><authors><author><keyname>Demircioglu</keyname><forenames>Aydin</forenames></author><author><keyname>Horn</keyname><forenames>Daniel</forenames></author><author><keyname>Glasmachers</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Weihs</keyname><forenames>Claus</forenames></author></authors><title>Fast model selection by limiting SVM training times</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernelized Support Vector Machines (SVMs) are among the best performing
supervised learning methods. But for optimal predictive performance,
time-consuming parameter tuning is crucial, which impedes application. To
tackle this problem, the classic model selection procedure based on grid-search
and cross-validation was refined, e.g. by data subsampling and direct search
heuristics. Here we focus on a different aspect, the stopping criterion for SVM
training. We show that by limiting the training time given to the SVM solver
during parameter tuning we can reduce model selection times by an order of
magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03379</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03379</id><created>2016-02-10</created><authors><author><keyname>Mitra</keyname><forenames>Anupam</forenames></author><author><keyname>Pathak</keyname><forenames>Anagh</forenames></author><author><keyname>Majumdar</keyname><forenames>Kaushik</forenames></author></authors><title>Comparison of feature extraction and dimensionality reduction methods
  for single channel extracellular spike sorting</title><categories>q-bio.QM cs.CV q-bio.NC</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spikes in the membrane electrical potentials of neurons play a major role in
the functioning of nervous systems of animals. Obtaining the spikes from
different neurons has been a challenging problem for decades. Several schemes
have been proposed for spike sorting to isolate the spikes of individual
neurons from electrical recordings in extracellular media. However, there is
much scope for improvement in the accuracies obtained using the prevailing
methods of spike sorting. To determine more effective spike sorting strategies
using well known methods, we compared different types of signal features and
techniques for dimensionality reduction in feature space. We tried to determine
an optimum or near optimum feature extraction and dimensionality reduction
methods and an optimum or near optimum number of features for spike sorting. We
assessed relative performance of well known methods on simulated recordings
specially designed for development and benchmarking of spike sorting schemes,
with varying number of spike classes and the well established method of
$k$-means clustering of selected features. We found that almost all well known
methods performed quite well. Nevertheless, from spike waveforms of 64 samples,
sampled at 24 kHz, using principal component analysis (PCA) to select around 46
to 55 features led to the better spike sorting performance than most other
methods (Wilcoxon signed rank sum test, $p &lt; 0.001$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03400</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03400</id><created>2016-02-10</created><authors><author><keyname>Determe</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Louveaux</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Horlin</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>On the Noise Robustness of Simultaneous Orthogonal Matching Pursuit</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the joint support recovery of several sparse signals whose
supports present similarities is examined. Each sparse signal is acquired using
the same noisy linear measurement process, which returns fewer observations
than the dimension of the sparse signals. The measurement noise is assumed
additive, Gaussian, and admits different variances for each sparse signal that
is measured. Using the theory of compressed sensing, the performance of
simultaneous orthogonal matching pursuit (SOMP) is analysed for the envisioned
signal model. The cornerstone of this paper is a novel analysis method upper
bounding the probability that SOMP recovers at least one incorrect entry of the
joint support during a prescribed number of iterations. Furthermore, the
probability of SOMP failing is investigated whenever the number of sparse
signals being recovered simultaneously increases and tends to infinity. In
particular, convincing observations and theoretical results suggest that SOMP
committing no mistake in the noiseless case does not guarantee the absence of
error in the noisy case whenever the number of acquired sparse signals scales
to infinity. While this property of SOMP had been suggested in an earlier work,
no satisfactory justifications had been proposed. Finally, simulation results
confirm the validity of the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03404</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03404</id><created>2016-02-10</created><authors><author><keyname>Castells-Rufas</keyname><forenames>David</forenames></author><author><keyname>Bastoul</keyname><forenames>C&#xe9;dric</forenames></author></authors><title>Proceedings of the Workshop on High Performance Energy Efficient
  Embedded Systems (HIP3ES) 2016</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proceedings of the Workshop on High Performance Energy Efficient Embedded
Systems (HIP3ES) 2016. Prague, January 18th. Collocated with HIPEAC 2016
Conference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03407</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03407</id><created>2016-01-21</created><authors><author><keyname>Tong</keyname><forenames>Fei</forenames></author><author><keyname>Pan</keyname><forenames>Jianping</forenames></author></authors><title>Random Distances Associated with Arbitrary Polygons: An Algorithmic
  Approach between Two Random Points</title><categories>cs.CG</categories><comments>16 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a new, algorithmic approach to the distributions of the
distance between two points distributed uniformly at random in various
polygons, based on the extended Kinematic Measure (KM) from integral geometry.
We first obtain such random Point Distance Distributions (PDDs) associated with
arbitrary triangles (i.e., triangle-PDDs), including the PDD within a triangle,
and that between two triangles sharing either a common side or a common vertex.
For each case, we provide an algorithmic procedure showing the mathematical
derivation process, based on which either the closed-form expressions or the
algorithmic results can be obtained. The obtained triangle-PDDs can be utilized
for modeling and analyzing the wireless communication networks associated with
triangle geometries, such as sensor networks with triangle-shaped clusters and
triangle-shaped cellular systems with highly directional antennas. Furthermore,
based on the obtained triangle-PDDs, we then show how to obtain the PDDs
associated with arbitrary polygons through the decomposition and recursion
approach, since any polygons can be triangulated, and any geometry shapes can
be approximated by polygons with a needed precision. Finally, we give the PDDs
associated with ring geometries. The results shown in this report can enrich
and expand the theory and application of the probabilistic distance models for
the analysis of wireless communication networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03409</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03409</id><created>2016-02-10</created><authors><author><keyname>Shin</keyname><forenames>Hoo-Chang</forenames></author><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Gao</keyname><forenames>Mingchen</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Xu</keyname><forenames>Ziyue</forenames></author><author><keyname>Nogues</keyname><forenames>Isabella</forenames></author><author><keyname>Yao</keyname><forenames>Jianhua</forenames></author><author><keyname>Mollura</keyname><forenames>Daniel</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>Deep Convolutional Neural Networks for Computer-Aided Detection: CNN
  Architectures, Dataset Characteristics and Transfer Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remarkable progress has been made in image recognition, primarily due to the
availability of large-scale annotated datasets and the revival of deep CNN.
CNNs enable learning data-driven, highly representative, layered hierarchical
image features from sufficient training data. However, obtaining datasets as
comprehensively annotated as ImageNet in the medical imaging domain remains a
challenge. There are currently three major techniques that successfully employ
CNNs to medical image classification: training the CNN from scratch, using
off-the-shelf pre-trained CNN features, and conducting unsupervised CNN
pre-training with supervised fine-tuning. Another effective method is transfer
learning, i.e., fine-tuning CNN models pre-trained from natural image dataset
to medical image tasks. In this paper, we exploit three important, but
previously understudied factors of employing deep convolutional neural networks
to computer-aided detection problems. We first explore and evaluate different
CNN architectures. The studied models contain 5 thousand to 160 million
parameters, and vary in numbers of layers. We then evaluate the influence of
dataset scale and spatial image context on performance. Finally, we examine
when and why transfer learning from pre-trained ImageNet (via fine-tuning) can
be useful. We study two specific computer-aided detection (CADe) problems,
namely thoraco-abdominal lymph node (LN) detection and interstitial lung
disease (ILD) classification. We achieve the state-of-the-art performance on
the mediastinal LN detection, with 85% sensitivity at 3 false positive per
patient, and report the first five-fold cross-validation classification results
on predicting axial CT slices with ILD categories. Our extensive empirical
evaluation, CNN model analysis and valuable insights can be extended to the
design of high performance CAD systems for other medical imaging tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03418</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03418</id><created>2016-02-10</created><authors><author><keyname>Sankaranarayanan</keyname><forenames>Swami</forenames></author><author><keyname>Alavi</keyname><forenames>Azadeh</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Triplet Similarity Embedding for Face Verification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present an unconstrained face verification algorithm and
evaluate it on the recently released IJB-A dataset that aims to push the
boundaries of face verification methods. The proposed algorithm couples a deep
CNN-based approach with a low-dimensional discriminative embedding learnt using
triplet similarity constraints in a large margin fashion. Aside from yielding
performance improvement, this embedding provides significant advantages in
terms of memory and post-processing operations like hashing and visualization.
Experiments on the IJB-A dataset show that the proposed algorithm outperforms
state of the art methods in verification and identification metrics, while
requiring less training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03419</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03419</id><created>2016-02-10</created><authors><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames></author><author><keyname>Chistikov</keyname><forenames>Dmitry</forenames></author><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Kumar</keyname><forenames>K Narayan</forenames></author><author><keyname>Saivasan</keyname><forenames>Prakash</forenames></author><author><keyname>Zetzsche</keyname><forenames>Georg</forenames></author></authors><title>Complexity of regular abstractions of one-counter languages</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational and descriptional complexity of the following
transformation: Given a one-counter automaton (OCA) A, construct a
nondeterministic finite automaton (NFA) B that recognizes an abstraction of the
language L(A): its (1) downward closure, (2) upward closure, or (3) Parikh
image.
  For the Parikh image over a fixed alphabet and for the upward and downward
closures, we find polynomial-time algorithms that compute such an NFA. For the
Parikh image with the alphabet as part of the input, we find a quasi-polynomial
time algorithm and prove a completeness result: we construct a sequence of OCA
that admits a polynomial-time algorithm iff there is one for all OCA.
  For all three abstractions, it was previously unknown if appropriate NFA of
sub-exponential size exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03426</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03426</id><created>2016-02-10</created><authors><author><keyname>Joshi</keyname><forenames>Aditya</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author><author><keyname>Carman</keyname><forenames>Mark James</forenames></author></authors><title>Automatic Sarcasm Detection: A Survey</title><categories>cs.CL</categories><comments>11 pages. This paper is likely to submitted to a NLP conference in
  the near future, with the same author list. All necessary regulations of
  arxiv and the conference will be followed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic detection of sarcasm has witnessed interest from the sentiment
analysis research community. With diverse approaches, datasets and analyses
that have been reported, there is an essential need to have a collective
understanding of the research in this area. In this survey of automatic sarcasm
detection, we describe datasets, approaches (both supervised and rule-based),
and trends in sarcasm detection research. We also present a research matrix
that summarizes past work, and list pointers to future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03434</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03434</id><created>2016-02-10</created><authors><author><keyname>Harling</keyname><forenames>Guy</forenames></author><author><keyname>Onnela</keyname><forenames>Jukka-Pekka</forenames></author></authors><title>Impact of degree truncation on the spread of a contagious process on
  networks</title><categories>physics.soc-ph cs.SI stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how person-to-person contagious processes spread through a
population requires accurate information on connections between population
members. However, such connectivity data, when collected via interview, is
often incomplete due to partial recall, respondent fatigue or study design,
e.g., fixed choice designs (FCD) truncate out-degree by limiting the number of
contacts each respondent can report. Past research has shown how FCD truncation
affects network properties, but its implications for predicted speed and size
of spreading processes remain largely unexplored. To study the impact of degree
truncation on spreading processes, we generated collections of synthetic
networks containing specific properties (degree distribution,
degree-assortativity, clustering), and also used empirical social network data
from 75 villages in Karnataka, India. We simulated FCD using various truncation
thresholds and ran a susceptible-infectious-recovered (SIR) process on each
network. We found that spreading processes propagated on truncated networks
resulted in slower and smaller epidemics, with a sudden decrease in prediction
accuracy at a level of truncation that varied by network type. Our results have
implications beyond FCD to truncation due to any limited sampling from a larger
network. We conclude that knowledge of network structure is important for
understanding the accuracy of predictions of process spread on degree truncated
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03458</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03458</id><created>2016-02-10</created><authors><author><keyname>K&#xf6;hler</keyname><forenames>Thomas</forenames></author><author><keyname>Heinrich</keyname><forenames>Axel</forenames></author><author><keyname>Maier</keyname><forenames>Andreas</forenames></author><author><keyname>Hornegger</keyname><forenames>Joachim</forenames></author><author><keyname>Tornow</keyname><forenames>Ralf P.</forenames></author></authors><title>Super-Resolved Retinal Image Mosaicing</title><categories>cs.CV</categories><comments>accepted for 2016 IEEE 13th International Symposium on Biomedical
  Imaging (ISBI 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The acquisition of high-resolution retinal fundus images with a large field
of view (FOV) is challenging due to technological, physiological and economic
reasons. This paper proposes a fully automatic framework to reconstruct retinal
images of high spatial resolution and increased FOV from multiple
low-resolution images captured with non-mydriatic, mobile and video-capable but
low-cost cameras. Within the scope of one examination, we scan different
regions on the retina by exploiting eye motion conducted by a patient guidance.
Appropriate views for our mosaicing method are selected based on optic disk
tracking to trace eye movements. For each view, one super-resolved image is
reconstructed by fusion of multiple video frames. Finally, all super-resolved
views are registered to a common reference using a novel polynomial
registration scheme and combined by means of image mosaicing. We evaluated our
framework for a mobile and low-cost video fundus camera. In our experiments, we
reconstructed retinal images of up to 30{\deg} FOV from 10 complementary views
of 15{\deg} FOV. An evaluation of the mosaics by human experts as well as a
quantitative comparison to conventional color fundus images encourage the
clinical usability of our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03468</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03468</id><created>2016-02-10</created><updated>2016-02-22</updated><authors><author><keyname>Kadkhodamohammadi</keyname><forenames>Abdolrahim</forenames></author><author><keyname>Gangi</keyname><forenames>Afshin</forenames></author><author><keyname>de Mathelin</keyname><forenames>Michel</forenames></author><author><keyname>Padoy</keyname><forenames>Nicolas</forenames></author></authors><title>3D Pictorial Structures on RGB-D Data for Articulated Human Detection in
  Operating Rooms</title><categories>cs.CV</categories><comments>The supplementary video is available at https://youtu.be/iabbGSqRSgE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable human pose estimation (HPE) is essential to many clinical
applications, such as surgical workflow analysis, radiation safety monitoring
and human-robot cooperation. Proposed methods for the operating room (OR) rely
either on foreground estimation using a multi-camera system, which is a
challenge in real ORs due to color similarities and frequent illumination
changes, or on wearable sensors or markers, which are invasive and therefore
difficult to introduce in the room. Instead, we propose a novel approach based
on Pictorial Structures (PS) and on RGB-D data, which can be easily deployed in
real ORs. We extend the PS framework in two ways. First, we build robust and
discriminative part detectors using both color and depth images. We also
present a novel descriptor for depth images, called histogram of depth
differences (HDD). Second, we extend PS to 3D by proposing 3D pairwise
constraints and a new method for exact and tractable inference. Our approach is
evaluated for pose estimation and clinician detection on a challenging RGB-D
dataset recorded in a busy operating room during live surgeries. We conduct
series of experiments to study the different part detectors in conjunction with
the various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3D
PS with RGB-D part detectors significantly improves the results in a visually
challenging operating environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03471</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03471</id><created>2016-02-10</created><authors><author><keyname>Aldridge</keyname><forenames>Matthew</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author></authors><title>Improved group testing rates with constant column weight designs</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider nonadaptive group testing with a form of constant column weight
design, where each item is placed in a fixed number of tests, chosen uniformly
at random with replacement. We show that performance is improved compared to
Bernoulli designs, where each item is placed in each test independently with a
fixed probability. In particular, we show that the rate of the practical COMP
detection algorithm is increased by 31% in all sparsity regimes. In dense
cases, this beats the best possible algorithm with Bernoulli tests, and in
sparse cases is the best proven performance of any practical algorithm. We also
give a universal upper bound for the constant column weight case; for dense
cases this is again a 31% increase over the equivalent Bernoulli result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03476</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03476</id><created>2016-02-10</created><updated>2016-02-12</updated><authors><author><keyname>Gao</keyname><forenames>Weihao</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Causal Strength via Shannon Capacity: Axioms, Estimators and
  Applications</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>41 pages, 3 figures, typos fixed and references added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct an axiomatic study of the problem of estimating the strength of a
known causal relationship between a pair of variables. We propose that an
estimate of causal strength should be based on the conditional distribution of
the effect given the cause (and not on the driving distribution of the cause),
and study dependence measures on conditional distributions. Shannon capacity,
appropriately regularized, emerges as a natural measure under these axioms. We
examine the problem of calculating Shannon capacity from the observed samples
and propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate its
consistency. Finally, we demonstrate an application to single-cell
flow-cytometry, where the proposed estimators significantly reduce sample
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03481</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03481</id><created>2016-02-10</created><authors><author><keyname>Khetan</keyname><forenames>Ashish</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author></authors><title>Reliable Crowdsourcing under the Generalized Dawid-Skene Model</title><categories>cs.LG cs.HC cs.SI stat.ML</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing systems provide scalable and cost-effective human-powered
solutions at marginal cost, for classification tasks where humans are
significantly better than the machines. Although traditional approaches in
aggregating crowdsourced labels have relied on the Dawid-Skene model, this
fails to capture how some tasks are inherently more difficult than the others.
Several generalizations have been proposed, but inference becomes intractable
and typical solutions resort to heuristics. To bridge this gap, we study a
recently proposed generalize Dawid-Skene model, and propose a linear-time
algorithm based on spectral methods. We show near-optimality of the proposed
approach, by providing an upper bound on the error and comparing it to a
fundamental limit. We provide numerical experiments on synthetic data matching
our analyses, and also on real datasets demonstrating that the spectral method
significantly improves over simple majority voting and is comparable to other
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03483</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03483</id><created>2016-02-10</created><authors><author><keyname>Hill</keyname><forenames>Felix</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Korhonen</keyname><forenames>Anna</forenames></author></authors><title>Learning Distributed Representations of Sentences from Unlabelled Data</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised methods for learning distributed representations of words are
ubiquitous in today's NLP research, but far less is known about the best ways
to learn distributed phrase or sentence representations from unlabelled data.
This paper is a systematic comparison of models that learn such
representations. We find that the optimal approach depends critically on the
intended application. Deeper, more complex models are preferable for
representations to be used in supervised systems, but shallow log-linear models
work best for building representation spaces that can be decoded with simple
spatial distance metrics. We also propose two new unsupervised
representation-learning objectives designed to optimise the trade-off between
training time, domain portability and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03490</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03490</id><created>2016-02-10</created><authors><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author><author><keyname>Jasper</keyname><forenames>John</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Peterson</keyname><forenames>Jesse</forenames></author></authors><title>Tremain equiangular tight frames</title><categories>math.FA cs.IT math.CO math.IT</categories><comments>11 pages</comments><msc-class>Primary: 42C15, Secondary: 51E10, 05B20, 05C12</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Equiangular tight frames provide optimal packings of lines through the
origin. We combine Steiner triple systems with Hadamard matrices to produce a
new infinite family of equiangular tight frames. This in turn leads to new
constructions of strongly regular graphs and distance-regular antipodal covers
of the complete graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03494</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03494</id><created>2016-02-06</created><authors><author><keyname>Dongale</keyname><forenames>T. D.</forenames></author><author><keyname>Gaikwad</keyname><forenames>P. K.</forenames></author><author><keyname>Kamat</keyname><forenames>R. K.</forenames></author></authors><title>Time and Frequency Domain Investigation of Selected Memristor Based
  Analog Circuits</title><categories>cs.ET</categories><comments>14 Pages, 9 Figures</comments><msc-class>94C05, 00A72</msc-class><acm-class>J.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper we investigate few memristor based analog circuits namely the
phase shift oscillator, integrator and differentiator which have been explored
numerously using the traditional lumped components. We use LTspice-IV platform
for simulation of the above said circuits. The investigation resorts to the
nonlinear dopent drift model of memristor and the window function portrayed in
the literature for nonlinearity realization. The results of our investigations
depict good agreement with the conventional lumped component based phase shift
oscillator, integrator and differentiator circuits. The results are evident to
showcase the potential of memristor as a promising candidate for the next
generation analog circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03498</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03498</id><created>2016-02-10</created><authors><author><keyname>Haikin</keyname><forenames>Marina</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Analog Coding of a Source with Erasures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analog coding decouples the tasks of protecting against erasures and noise.
For erasure correction, it creates an &quot;analog redundancy&quot; by means of
band-limited discrete Fourier transform (DFT) interpolation, or more generally,
by an over-complete expansion based on a frame. We examine the analog coding
paradigm for the dual setup of a source with &quot;erasure&quot; side-information (SI) at
the encoder. The excess rate of analog coding above the rate-distortion
function (RDF) is associated with the energy of the inverse of submatrices of
the frame, where each submatrix corresponds to a possible erasure pattern. We
give a theoretical as well as numerical evidence that in each dimension where
an equiangular tight frames (ETF) exists, it minimizes the excess rate over all
possible frames. However, it does not achieve the RDF even in the limit as the
dimension goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03501</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03501</id><created>2016-02-10</created><authors><author><keyname>Schultz</keyname><forenames>Patrick</forenames></author><author><keyname>Spivak</keyname><forenames>David I.</forenames></author><author><keyname>Vasilakopoulou</keyname><forenames>Christina</forenames></author><author><keyname>Wisnesky</keyname><forenames>Ryan</forenames></author></authors><title>Algebraic Databases</title><categories>math.CT cs.DB</categories><comments>80 pages</comments><msc-class>18C10, 18D05, 68P15</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Databases have been studied category-theoretically for decades. The database
schema---whose purpose is to arrange high-level conceptual entities---is
generally modeled as a category or sketch. The data itself, often called an
instance, is generally modeled as a set-valued functor, assigning to each
conceptual entity a set of examples. While mathematically elegant, these
categorical models have typically struggled with representing concrete data
such as integers or strings.
  In the present work, we propose an extension of the set-valued functor model,
making use of multisorted algebraic theories (a.k.a. Lawvere theories) to
incorporate concrete data in a principled way. This also allows constraints and
queries to make use of operations on data, such as multiplication or comparison
of numbers, helping to bridge the gap between traditional databases and
programming languages.
  We also show how all of the components of our model---including schemas,
instances, change-of-schema functors, and queries \dash fit into a single
double categorical structure called a proarrow equipment (a.k.a. framed
bicategory).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03506</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03506</id><created>2016-02-10</created><authors><author><keyname>Russell</keyname><forenames>Stuart</forenames><affiliation>Berkeley</affiliation></author><author><keyname>Dewey</keyname><forenames>Daniel</forenames><affiliation>FHI</affiliation></author><author><keyname>Tegmark</keyname><forenames>Max</forenames><affiliation>MIT</affiliation></author></authors><title>Research Priorities for Robust and Beneficial Artificial Intelligence</title><categories>cs.AI stat.ML</categories><comments>This article gives examples of the type of research advocated by the
  open letter for robust &amp; beneficial AI at
  http://futureoflife.org/ai-open-letter</comments><journal-ref>AI Magazine 36:4 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Success in the quest for artificial intelligence has the potential to bring
unprecedented benefits to humanity, and it is therefore worthwhile to
investigate how to maximize these benefits while avoiding potential pitfalls.
This article gives numerous examples (which should by no means be construed as
an exhaustive list) of such worthwhile research aimed at ensuring that AI
remains robust and beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03508</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03508</id><created>2016-02-10</created><authors><author><keyname>Kuang</keyname><forenames>Quan</forenames></author><author><keyname>Utschick</keyname><forenames>Wolfgang</forenames></author></authors><title>Energy Management in Heterogeneous Networks with Cell Activation, User
  Association and Interference Coordination</title><categories>cs.IT math.IT</categories><comments>This is an extended version of a paper to appear in IEEE Transactions
  on Wireless Communications with the same title, containing all detailed
  proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The densification and expansion of wireless network pose new challenges on
interference management and reducing energy consumption. This paper studies
energy-efficient resource management in heterogeneous networks by jointly
optimizing cell activation, user association and multicell multiuser channel
assignment, according to the long-term average traffic and channel conditions.
The proposed framework is built on characterizing the interference coupling by
pre-defined interference patterns, and performing resource allocation among
these patterns. In this way, the interference fluctuation caused by
(de)activating cells is explicitly taken into account when calculating the user
achievable rates. A tailored algorithm is developed to solve the formulated
problem in the dual domain by exploiting the problem structure, which gives a
significant complexity saving. Numerical results show a huge improvement in
energy saving achieved by the proposed scheme. The user association derived
from the proposed joint resource optimization is mapped to standard-compliant
cell selection biasing. This mapping reveals that the cell-specific biasing for
energy saving is quite different from that for load balancing investigated in
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03534</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03534</id><created>2016-02-10</created><updated>2016-02-12</updated><authors><author><keyname>Sener</keyname><forenames>Ozan</forenames></author><author><keyname>Song</keyname><forenames>Hyun Oh</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>Unsupervised Transductive Domain Adaptation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised learning with large scale labeled datasets and deep layered models
has made a paradigm shift in diverse areas in learning and recognition.
However, this approach still suffers generalization issues under the presence
of a domain shift between the training and the test data distribution. In this
regard, unsupervised domain adaptation algorithms have been proposed to
directly address the domain shift problem. In this paper, we approach the
problem from a transductive perspective. We incorporate the domain shift and
the transductive target inference into our framework by jointly solving for an
asymmetric similarity metric and the optimal transductive target label
assignment. We also show that our model can easily be extended for deep feature
learning in order to learn features which are discriminative in the target
domain. Our experiments show that the proposed method significantly outperforms
state-of-the-art algorithms in both object recognition and digit classification
experiments by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03536</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03536</id><created>2016-02-10</created><authors><author><keyname>Kim</keyname><forenames>Yongjune</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Duality between erasures and defects</title><categories>cs.IT math.IT</categories><comments>Presented at Information Theory and Applications (ITA) Workshop 2016.
  arXiv admin note: text overlap with arXiv:1602.01202</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the duality of the binary erasure channel (BEC) and the binary
defect channel (BDC). This duality holds for channel capacities, capacity
achieving schemes, minimum distances, and upper bounds on the probability of
failure to retrieve the original message. In addition, the relations between
BEC, BDC, binary erasure quantization (BEQ), and write-once memory (WOM) are
described. From these relations we claim that the capacity of the BDC can be
achieved by Reed-Muller (RM) codes under maximum a posterior (MAP) decoding.
Also, polar codes with a successive cancellation encoder achieve the capacity
of the BDC.
  Inspired by the duality between the BEC and the BDC, we introduce locally
rewritable codes (LWC) for resistive memories, which are the counterparts of
locally repairable codes (LRC) for distributed storage systems. The proposed
LWC can improve endurance limit and power efficiency of resistive memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03540</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03540</id><created>2016-02-10</created><authors><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Cohen</keyname><forenames>Johanne</forenames></author><author><keyname>Rabie</keyname><forenames>Mika&#xeb;l</forenames></author></authors><title>Homonym Population Protocols</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1412.2497</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The population protocol model was introduced by Angluin \emph{et al.} as a
model of passively mobile anonymous finite-state agents. This model computes a
predicate on the multiset of their inputs via interactions by pairs. The
original population protocol model has been proved to compute only semi-linear
predicates and has been recently extended in various ways. In the community
protocol model by Guerraoui and Ruppert, agents have unique identifiers but may
only store a finite number of the identifiers they already heard about. The
community protocol model provides the power of a Turing machine with a $O(n\log
n)$ space. We consider variations on the two above models and we obtain a whole
landscape that covers and extends already known results. Namely, by considering
the case of homonyms, that is to say the case when several agents may share the
same identifier, we provide a hierarchy that goes from the case of no
identifier (population protocol model) to the case of unique identifiers
(community protocol model). We obtain in particular that any Turing Machine on
space $O(\log^{O(1)} n)$ can be simulated with at least $O(\log^{O(1)} n)$
identifiers, a result filling a gap left open in all previous studies. Our
results also extend and revisit in particular the hierarchy provided by
Chatzigiannakis \emph{et al.} on population protocols carrying Turing Machines
on limited space, solving the problem of the gap left by this work between
per-agent space $o(\log \log n)$ (proved to be equivalent to population
protocols) and $O(\log n)$ (proved to be equivalent to Turing machines).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03551</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03551</id><created>2016-02-10</created><authors><author><keyname>Hyland</keyname><forenames>Stephanie L.</forenames></author><author><keyname>Karaletsos</keyname><forenames>Theofanis</forenames></author><author><keyname>R&#xe4;tsch</keyname><forenames>Gunnar</forenames></author></authors><title>Knowledge Transfer with Medical Language Embeddings</title><categories>cs.CL stat.AP</categories><comments>6 pages, 2 figures, to appear at SDM-DMMH 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying relationships between concepts is a key aspect of scientific
knowledge synthesis. Finding these links often requires a researcher to
laboriously search through scien- tific papers and databases, as the size of
these resources grows ever larger. In this paper we describe how distributional
semantics can be used to unify structured knowledge graphs with unstructured
text to predict new relationships between medical concepts, using a
probabilistic generative model. Our approach is also designed to ameliorate
data sparsity and scarcity issues in the medical domain, which make language
modelling more challenging. Specifically, we integrate the medical relational
database (SemMedDB) with text from electronic health records (EHRs) to perform
knowledge graph completion. We further demonstrate the ability of our model to
predict relationships between tokens not appearing in the relational database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03552</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03552</id><created>2016-02-10</created><authors><author><keyname>Hamm</keyname><forenames>Jihun</forenames></author><author><keyname>Cao</keyname><forenames>Paul</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author></authors><title>Learning Privately from Multiparty Data</title><categories>cs.LG cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning a classifier from private data collected by multiple parties is an
important problem that has many potential applications. How can we build an
accurate and differentially private global classifier by combining
locally-trained classifiers from different parties, without access to any
party's private data? We propose to transfer the `knowledge' of the local
classifier ensemble by first creating labeled data from auxiliary unlabeled
data, and then train a global $\epsilon$-differentially private classifier. We
show that majority voting is too sensitive and therefore propose a new risk
weighted by class probabilities estimated from the ensemble. Relative to a
non-private solution, our private solution has a generalization error bounded
by $O(\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows
strong privacy without performance loss when $M$ is large, such as in
crowdsensing applications. We demonstrate the performance of our method with
realistic tasks of activity recognition, network intrusion detection, and
malicious URL detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03557</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03557</id><created>2016-02-10</created><authors><author><keyname>Aberger</keyname><forenames>Christopher R.</forenames></author><author><keyname>Tu</keyname><forenames>Susan</forenames></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Old Techniques for New Join Algorithms: A Case Study in RDF Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been significant interest around designing specialized RDF
engines, as traditional query processing mechanisms incur orders of magnitude
performance gaps on many RDF workloads. At the same time researchers have
released new worst-case optimal join algorithms which can be asymptotically
better than the join algorithms in traditional engines. In this paper we apply
worst-case optimal join algorithms to a standard RDF workload, the LUBM
benchmark, for the first time. We do so using two worst-case optimal engines:
(1) LogicBlox, a commercial database engine, and (2) EmptyHeaded, our prototype
research engine with enhanced worst-case optimal join algorithms. We show that
without any added optimizations both LogicBlox and EmptyHeaded outperform two
state-of-the-art specialized RDF engines, RDF-3X and TripleBit, by up to 6x on
cyclic join queries-the queries where traditional optimizers are suboptimal. On
the remaining, less complex queries in the LUBM benchmark, we show that three
classic query optimization techniques enable EmptyHeaded to compete with RDF
engines, even when there is no asymptotic advantage to the worst-case optimal
approach. We validate that our design has merit as EmptyHeaded outperforms
MonetDB by three orders of magnitude and LogicBlox by two orders of magnitude,
while remaining within an order of magnitude of RDF-3X and TripleBit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03563</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03563</id><created>2016-02-10</created><authors><author><keyname>Farid</keyname><forenames>Farnaz</forenames></author><author><keyname>Shahrestani</keyname><forenames>Seyed</forenames></author><author><keyname>Ruan</keyname><forenames>Chun</forenames></author></authors><title>QoS Evaluation of Heterogeneous Networks: Application-based Approach</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.8, No.1, January 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an application-based QoS evaluation approach for heterogeneous
networks is proposed.It is possible to expand the network capacity and coverage
in a dynamic fashion by applying heterogeneous wireless network architecture.
However, the Quality of Service (QoS) evaluation of this type of network
architecture is very challenging due to the presence of different communication
technologies. Different communication technologies have different
characteristics and the applications that utilize them have unique QoS
requirements. Although, the communication technologies have different
performance measurement parameters, the applications using these radio access
networks have the same QoS requirements. As a result, it would be easier to
evaluate the QoS of the access networks and the overall network configuration
based on the performance of applications running on them. Using such
application-based QoS evaluation approach, the heterogeneous nature of the
underlying networks and the diversity of their traffic can be adequately taken
into account. Through simulation studies, we show that the application
performance based assessment approach facilitates better QoS management and
monitoring of heterogeneous network configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03570</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03570</id><created>2016-02-10</created><updated>2016-02-18</updated><authors><author><keyname>Alavi</keyname><forenames>Azadeh</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Distance Preserving Projection Space of Symmetric Positive Definite
  Manifolds</title><categories>cs.CV</categories><comments>9 pages, 6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in computer vision suggest that encoding images through
Symmetric Positive Definite (SPD) matrices can lead to increased classification
performance. Taking into account manifold geometry is typically done via
embedding the manifolds in tangent spaces, or Reproducing Kernel Hilbert Spaces
(RKHS). Recently it was shown that projecting such manifolds into a
kernel-based random projection space (RPS) leads to higher classification
performance. In this paper, we propose to learn an optimized projection, based
on building local and global sparse similarity graphs that encode the
association of data points to the underlying subspace of each point. To this
end, we project SPD matrices into an optimized distance preserving projection
space (DPS), which can be followed by any Euclidean-based classification
algorithm. Further, we adopt the concept of dictionary learning and sparse
coding, and discriminative analysis, for the learned DPS on SPD manifolds.
Experiments on face recognition, texture classification, person
re-identification, and virus classification demonstrate that the proposed
methods outperform state-of-the-art methods on such manifolds
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03571</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03571</id><created>2016-02-10</created><authors><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author><author><keyname>Maji</keyname><forenames>Subhransu</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>High Dimensional Inference with Random Maximum A-Posteriori
  Perturbations</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a new approach for high-dimensional statistical
inference that is based on optimization and random perturbations. This
framework injects randomness to maximum a-posteriori (MAP) predictors by
randomly perturbing its potential function. When the perturbations are of low
dimension, sampling the perturb-max prediction is as efficient as MAP
optimization. A classic result from extreme value statistics asserts that
perturb-max operations generate unbiased samples from the Gibbs distribution
using high-dimensional perturbations. Unfortunately, the computational cost of
generating so many high-dimensional random variables can be prohibitive. In
this work we show that the expected value of perturb-max inference with low
dimensional perturbations can be used sequentially to generate unbiased samples
from the Gibbs distribution. We also show that the expected value of the
maximal perturbations is a natural bound on the entropy of such perturb-max
models. Finally we describe the measure concentration properties of perturb-max
values while showing that the deviation of their sampled average from its
expectation decays exponentially in the number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03573</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03573</id><created>2016-02-10</created><authors><author><keyname>Roque</keyname><forenames>Pedro</forenames></author><author><keyname>Ventura</keyname><forenames>Rodrigo</forenames></author></authors><title>Space CoBot: a collaborative aerial robot for indoor microgravity
  environments</title><categories>cs.RO</categories><comments>21 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a first contribution to the design of a small aerial
robot for inhabited microgravity environments, such as orbiting space stations.
In particular, we target a fleet of robots for collaborative tasks with humans,
such as telepresence and cooperative mobile manipulation. We explore a
propeller based propulsion system, arranged in such a way that the
translational and the rotational components can be decoupled, resulting in an
holonomic hexarotor. Since propellers have limited thrust, we employ an
optimization approach to select the geometric configuration given a criteria of
uniform maximum thrust across all directions in the body reference frame. We
also tackle the problem of motion control: due to the decoupling of
translational and rotational modes we use separate converging controllers for
each one of these modes. In addition, we present preliminary simulation results
in a realistic simulator, in closed loop with the proposed controller, thus
providing a first validation of the followed methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03585</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03585</id><created>2016-02-10</created><authors><author><keyname>Zhang</keyname><forenames>Yangmuzi</forenames></author><author><keyname>Jiang</keyname><forenames>Zhuolin</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Generating Discriminative Object Proposals via Submodular Ranking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-scale greedy-based object proposal generation approach is presented.
Based on the multi-scale nature of objects in images, our approach is built on
top of a hierarchical segmentation. We first identify the representative and
diverse exemplar clusters within each scale by using a diversity ranking
algorithm. Object proposals are obtained by selecting a subset from the
multi-scale segment pool via maximizing a submodular objective function, which
consists of a weighted coverage term, a single-scale diversity term and a
multi-scale reward term. The weighted coverage term forces the selected set of
object proposals to be representative and compact; the single-scale diversity
term encourages choosing segments from different exemplar clusters so that they
will cover as many object patterns as possible; the multi-scale reward term
encourages the selected proposals to be discriminative and selected from
multiple layers generated by the hierarchical image segmentation. The
experimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012
segmentation dataset demonstrate the accuracy and efficiency of our object
proposal model. Additionally, we validate our object proposals in simultaneous
segmentation and detection and outperform the state-of-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03586</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03586</id><created>2016-02-10</created><authors><author><keyname>Atkins</keyname><forenames>Ross</forenames></author><author><keyname>Rombach</keyname><forenames>Puck</forenames></author><author><keyname>Skerman</keyname><forenames>Fiona</forenames></author></authors><title>Guessing Numbers of Odd Cycles</title><categories>cs.IT math.CO math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given number of colours, $s$, the guessing number of a graph is the
base $s$ logarithm of the size of the largest family of colourings of the
vertex set of the graph such that the colour of each vertex can be determined
from the colours of the vertices in its neighbourhood. An upper bound for the
guessing number of the $n$-vertex cycle graph $C_n$ is $n/2$. It is known that
the guessing number equals $n/2$ whenever $n$ is even or $s$ is a perfect
square \cite{Christofides2011guessing}. We show that, for any given integer
$s\geq 2$, if $a$ is the largest factor of $s$ less than or equal to
$\sqrt{s}$, for sufficiently large odd $n$, the guessing number of $C_n$ with
$s$ colours is $(n-1)/2 + \log_s(a)$. This answers a question posed by
Christofides and Markstr\&quot;{o}m in 2011 \cite{Christofides2011guessing}. We also
present an explicit protocol which achieves this bound for every $n$. Linking
this to index coding with side information, we deduce that the information
defect of $C_n$ with $s$ colours is $(n+1)/2 - \log_s(a)$ for sufficiently
large odd $n$. Our results are a generalisation of the $s=2$ case which was
proven in \cite{bar2011index}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03591</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03591</id><created>2016-02-10</created><authors><author><keyname>Orchard</keyname><forenames>Dominic</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Using session types as an effect system</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><acm-class>F.3.3; D.3.2; F.3.2</acm-class><journal-ref>EPTCS 203, 2016, pp. 1-13</journal-ref><doi>10.4204/EPTCS.203.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Side effects are a core part of practical programming. However, they are
often hard to reason about, particularly in a concurrent setting. We propose a
foundation for reasoning about concurrent side effects using sessions.
Primarily, we show that session types are expressive enough to encode an effect
system for stateful processes. This is formalised via an effect-preserving
encoding of a simple imperative language with an effect system into the
pi-calculus with session primitives and session types (into which we encode
effect specifications). This result goes towards showing a connection between
the expressivity of session types and effect systems. We briefly discuss how
the encoding could be extended and applied to reason about and control
concurrent side effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03592</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03592</id><created>2016-02-10</created><authors><author><keyname>H&#xfc;ttel</keyname><forenames>Hans</forenames><affiliation>Department of Computer Science, Aalborg University</affiliation></author><author><keyname>Pratas</keyname><forenames>Nuno</forenames><affiliation>Department of Electronic Systems, Aalborg University</affiliation></author></authors><title>Broadcast and aggregation in BBC</title><categories>cs.LO</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 15-28</journal-ref><doi>10.4204/EPTCS.203.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed systems, where multi-party communication is essential, two
communication paradigms are ever present: (1) one-to-many, commonly denoted as
broadcast; and (2) many-to-one denoted as aggregation or collection.
  In this paper we present the BBC process calculus, which inherently models
the broadcast and aggregation communication modes. We then apply this process
calculus to reason on hierarchical network structure and provide examples on
its expressive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03593</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03593</id><created>2016-02-10</created><authors><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Universit&#xe0; di Torino, Italy</affiliation></author><author><keyname>Ghilezan</keyname><forenames>Silvia</forenames><affiliation>Univerzitet u Novom Sadu, Serbia</affiliation></author><author><keyname>Jak&#x161;i&#x107;</keyname><forenames>Svetlana</forenames><affiliation>Univerzitet u Novom Sadu, Serbia</affiliation></author><author><keyname>Pantovi&#x107;</keyname><forenames>Jovanka</forenames><affiliation>Univerzitet u Novom Sadu, Serbia</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London, UK</affiliation></author></authors><title>Precise subtyping for synchronous multiparty sessions</title><categories>cs.LO cs.PL</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><acm-class>D.3.3; F.3.2</acm-class><journal-ref>EPTCS 203, 2016, pp. 29-43</journal-ref><doi>10.4204/EPTCS.203.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of subtyping has gained an important role both in theoretical and
applicative domains: in lambda and concurrent calculi as well as in programming
languages. The soundness and the completeness, together referred to as the
preciseness of subtyping, can be considered from two different points of view:
operational and denotational. The former preciseness has been recently
developed with respect to type safety, i.e. the safe replacement of a term of a
smaller type when a term of a bigger type is expected. The latter preciseness
is based on the denotation of a type which is a mathematical object that
describes the meaning of the type in accordance with the denotations of other
expressions from the language. The result of this paper is the operational and
denotational preciseness of the subtyping for a synchronous multiparty session
calculus. The novelty of this paper is the introduction of characteristic
global types to prove the operational completeness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03594</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03594</id><created>2016-02-10</created><authors><author><keyname>Brown</keyname><forenames>Geoffrey</forenames><affiliation>Indiana University School of Informatics and Computing</affiliation></author><author><keyname>Sabry</keyname><forenames>Amr</forenames><affiliation>Indiana University School of Informatics and Computing</affiliation></author></authors><title>Reversible Communicating Processes</title><categories>cs.PL cs.DC</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 45-59</journal-ref><doi>10.4204/EPTCS.203.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible distributed programs have the ability to abort unproductive
computation paths and backtrack, while unwinding communication that occurred in
the aborted paths. While it is natural to assume that reversibility implies
full state recovery (as with traditional roll-back recovery protocols), an
interesting alternative is to separate backtracking from local state recovery.
For example, such a model could be used to create complex transactions out of
nested compensable transactions where a programmer-supplied compensation
defines the work required to &quot;unwind&quot; a transaction.
  Reversible distributed computing has received considerable theoretical
attention, but little reduction to practice; the few published implementations
of languages supporting reversibility depend upon a high degree of central
control. The objective of this paper is to demonstrate that a practical
reversible distributed language can be efficiently implemented in a fully
distributed manner.
  We discuss such a language, supporting CSP-style synchronous communication,
embedded in Scala. While this language provided the motivation for the work
described in this paper, our focus is upon the distributed implementation. In
particular, we demonstrate that a &quot;high-level&quot; semantic model can be
implemented using a simple point-to-point protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03595</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03595</id><created>2016-02-10</created><authors><author><keyname>Barbanera</keyname><forenames>Franco</forenames><affiliation>Dipartimento di Matematica e Informatica, University of Catania</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Dipartimento di Informatica, University of Torino</affiliation></author><author><keyname>Lanese</keyname><forenames>Ivan</forenames><affiliation>Dipartimento di Informatica - Scienza e Ingegneria, University of Bologna/INRIA</affiliation></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames><affiliation>Dipartimento di Informatica, University of Torino</affiliation></author></authors><title>Retractable Contracts</title><categories>cs.LO cs.PL</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 61-72</journal-ref><doi>10.4204/EPTCS.203.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In calculi for modelling communication protocols, internal and external
choices play dual roles. Two external choices can be viewed naturally as dual
too, as they represent an agreement between the communicating parties. If the
interaction fails, the past agreements are good candidates as points where to
roll back, in order to take a different agreement. We propose a variant of
contracts with synchronous rollbacks to agreement points in case of deadlock.
The new calculus is equipped with a compliance relation which is shown to be
decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03596</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03596</id><created>2016-02-10</created><authors><author><keyname>Ghilezan</keyname><forenames>Silvia</forenames><affiliation>University of Novi Sad, Serbia</affiliation></author><author><keyname>Jak&#x161;i&#x107;</keyname><forenames>Svetlana</forenames><affiliation>University of Novi Sad, Serbia</affiliation></author><author><keyname>Pantovi&#x107;</keyname><forenames>Jovanka</forenames><affiliation>University of Novi Sad, Serbia</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames><affiliation>University of Groningen, The Netherlands</affiliation></author><author><keyname>Vieira</keyname><forenames>Hugo Torres</forenames><affiliation>IMT Institute for Advanced Studies Lucca, Italy</affiliation></author></authors><title>A Typed Model for Dynamic Authorizations</title><categories>cs.LO cs.PL</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 73-84</journal-ref><doi>10.4204/EPTCS.203.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security requirements in distributed software systems are inherently dynamic.
In the case of authorization policies, resources are meant to be accessed only
by authorized parties, but the authorization to access a resource may be
dynamically granted/yielded. We describe ongoing work on a model for specifying
communication and dynamic authorization handling. We build upon the pi-calculus
so as to enrich communication-based systems with authorization specification
and delegation; here authorizations regard channel usage and delegation refers
to the act of yielding an authorization to another party. Our model includes:
(i) a novel scoping construct for authorization, which allows to specify
authorization boundaries, and (ii) communication primitives for authorizations,
which allow to pass around authorizations to act on a given channel. An
authorization error may consist in, e.g., performing an action along a name
which is not under an appropriate authorization scope. We introduce a typing
discipline that ensures that processes never reduce to authorization errors,
even when authorizations are dynamically delegated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03597</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03597</id><created>2016-02-10</created><authors><author><keyname>Vissani</keyname><forenames>Ignacio</forenames><affiliation>Department of computing, School of Science, Universidad de Buenos Aires</affiliation></author><author><keyname>Pombo</keyname><forenames>Carlos Gustavo Lopez</forenames><affiliation>Department of computing, School of Science, Universidad de Buenos Aires and Consejo Nacional de Investigaciones Cient&#xed;ficas y Tecnol&#xf3;gicas</affiliation></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames><affiliation>Department of Computer Science, University of Leicester</affiliation></author></authors><title>Communicating machines as a dynamic binding mechanism of services</title><categories>cs.LO</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 85-98</journal-ref><doi>10.4204/EPTCS.203.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed software is becoming more and more dynamic to support
applications able to respond and adapt to the changes of their execution
environment. For instance, service-oriented computing (SOC) envisages
applications as services running over globally available computational
resources where discovery and binding between them is transparently performed
by a middleware. Asynchronous Relational Networks (ARNs) is a well-known formal
orchestration model, based on hypergraphs, for the description of
service-oriented software artefacts. Choreography and orchestration are the two
main design principles for the development of distributed software. In this
work, we propose Communicating Relational Networks (CRNs), which is a variant
of ARNs, but relies on choreographies for the characterisation of the
communicational aspects of a software artefact, and for making their automated
analysis more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03598</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03598</id><created>2016-02-10</created><authors><author><keyname>Haller</keyname><forenames>Philipp</forenames><affiliation>KTH Royal Institute of Technology</affiliation></author><author><keyname>Miller</keyname><forenames>Heather</forenames><affiliation>EPFL</affiliation></author></authors><title>Distributed Programming via Safe Closure Passing</title><categories>cs.PL cs.DC</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 99-107</journal-ref><doi>10.4204/EPTCS.203.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming systems incorporating aspects of functional programming, e.g.,
higher-order functions, are becoming increasingly popular for large-scale
distributed programming. New frameworks such as Apache Spark leverage
functional techniques to provide high-level, declarative APIs for in-memory
data analytics, often outperforming traditional &quot;big data&quot; frameworks like
Hadoop MapReduce. However, widely-used programming models remain rather ad-hoc;
aspects such as implementation trade-offs, static typing, and semantics are not
yet well-understood. We present a new asynchronous programming model that has
at its core several principles facilitating functional processing of
distributed data. The emphasis of our model is on simplicity, performance, and
expressiveness. The primary means of communication is by passing functions
(closures) to distributed, immutable data. To ensure safe and efficient
distribution of closures, our model leverages both syntactic and type-based
restrictions. We report on a prototype implementation in Scala. Finally, we
present preliminary experimental results evaluating the performance impact of a
static, type-based optimization of serialization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03599</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03599</id><created>2016-02-10</created><authors><author><keyname>Franco</keyname><forenames>Juliana</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Drossopoulou</keyname><forenames>Sophia</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Behavioural types for non-uniform memory accesses</title><categories>cs.PL</categories><comments>In Proceedings PLACES 2015, arXiv:1602.03254</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 203, 2016, pp. 109-120</journal-ref><doi>10.4204/EPTCS.203.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent programs executing on NUMA architectures consist of concurrent
entities (e.g. threads, actors) and data placed on different nodes. Execution
of these concurrent entities often reads or updates states from remote nodes.
The performance of such systems depends on the extent to which the concurrent
entities can be executing in parallel, and on the amount of the remote reads
and writes.
  We consider an actor-based object oriented language, and propose a type
system which expresses the topology of the program (the placement of the actors
and data on the nodes), and an effect system which characterises remote reads
and writes (in terms of which node reads/writes from which other nodes). We use
a variant of ownership types for the topology, and a combination of behavioural
and ownership types for the effect system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03600</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03600</id><created>2016-02-10</created><authors><author><keyname>Atan</keyname><forenames>Onur</forenames></author><author><keyname>Whoiles</keyname><forenames>William</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Data-Driven Online Decision Making with Costly Information Acquisition</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing work on online learning for decision making takes the information
available as a given and focuses solely on choosing best actions given this
information. Instead, in this paper, the decision maker needs to simultaneously
learn both what decisions to make and what source(s) of contextual information
to gather data from in order to inform its decisions such that its reward is
maximized. We propose algorithms that obtain costly source(s) of contextual
information over time, while simultaneously learning what actions to take based
on the contextual information revealed by the selected source(s). We prove that
our algorithms achieve regret that is logarithmic in time. We demonstrate the
performance of our algorithms using a medical dataset. The proposed algorithm
can be applied in many applications including clinical decision assist systems
for medical diagnosis, recommender systems, actionable intelligence etc., where
observing the complete information in every instance or consulting all the
available sources to gather intelligence before making decisions is costly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03602</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03602</id><created>2016-02-10</created><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Wireless Communications with Unmanned Aerial Vehicles: Opportunities and
  Challenges</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communication systems that include unmanned aerial vehicles (UAVs)
promise to provide cost-effective wireless connectivity for devices without
infrastructure coverage. Compared to terrestrial communications or those based
on high-altitude platforms (HAPs), on-demand wireless systems with low-altitude
UAVs are in general faster to deploy, more flexibly re-configured, and are
likely to have better communication channels due to the presence of short-range
line-of-sight (LoS) links. However, the utilization of highly mobile and
energy-constrained UAVs for wireless communications also introduces many new
challenges. In this article, we provide an overview of UAV-aided wireless
communications, by introducing the basic networking architecture and main
channel characteristics, highlighting the key design considerations as well as
the new opportunities to be exploited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03606</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03606</id><created>2016-02-10</created><authors><author><keyname>Barrios</keyname><forenames>Federico</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Federico</forenames></author><author><keyname>Argerich</keyname><forenames>Luis</forenames></author><author><keyname>Wachenchauzer</keyname><forenames>Rosa</forenames></author></authors><title>Variations of the Similarity Function of TextRank for Automated
  Summarization</title><categories>cs.CL cs.IR</categories><comments>8 pages, 2 figures. Presented at the Argentine Symposium on
  Artificial Intelligence (ASAI) 2015 - 44 JAIIO (September 2015)</comments><acm-class>I.2.7</acm-class><journal-ref>44 JAIIO - ASAI 2015 - ISSN: 2451-7585, pages 65-72</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents new alternatives to the similarity function for the
TextRank algorithm for automatic summarization of texts. We describe the
generalities of the algorithm and the different functions we propose. Some of
these variants achieve a significative improvement using the same metrics and
dataset as the original publication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03609</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03609</id><created>2016-02-10</created><authors><author><keyname>Santos</keyname><forenames>Cicero dos</forenames></author><author><keyname>Tan</keyname><forenames>Ming</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Attentive Pooling Networks</title><categories>cs.CL cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose Attentive Pooling (AP), a two-way attention
mechanism for discriminative model training. In the context of pair-wise
ranking or classification with neural networks, AP enables the pooling layer to
be aware of the current input pair, in a way that information from the two
input items can directly influence the computation of each other's
representations. Along with such representations of the paired inputs, AP
jointly learns a similarity measure over projected segments (e.g. trigrams) of
the pair, and subsequently, derives the corresponding attention vector for each
input to guide the pooling. Our two-way attention mechanism is a general
framework independent of the underlying representation learning, and it has
been applied to both convolutional neural networks (CNNs) and recurrent neural
networks (RNNs) in our studies. The empirical results, from three very
different benchmark tasks of question answering/answer selection, demonstrate
that our proposed models outperform a variety of strong baselines and achieve
state-of-the-art performance in all the benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03616</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03616</id><created>2016-02-11</created><authors><author><keyname>Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author></authors><title>Multifaceted Feature Visualization: Uncovering the Different Types of
  Features Learned By Each Neuron in Deep Neural Networks</title><categories>cs.NE cs.CV</categories><comments>23 pages (including SI), 24 figures. Submitted to ICML 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We can better understand deep neural networks by identifying which features
each of their neurons have learned to detect. To do so, researchers have
created Deep Visualization techniques including activation maximization, which
synthetically generates inputs (e.g. images) that maximally activate each
neuron. A limitation of current techniques is that they assume each neuron
detects only one type of feature, but we know that neurons can be multifaceted,
in that they fire in response to many different types of features: for example,
a grocery store class neuron must activate either for rows of produce or for a
storefront. Previous activation maximization techniques constructed images
without regard for the multiple different facets of a neuron, creating
inappropriate mixes of colors, parts of objects, scales, orientations, etc.
Here, we introduce an algorithm that explicitly uncovers the multiple facets of
each neuron by producing a synthetic visualization of each of the types of
images that activate a neuron. We also introduce regularization methods that
produce state-of-the-art results in terms of the interpretability of images
obtained by activation maximization. By separately synthesizing each type of
image a neuron fires in response to, the visualizations have more appropriate
colors and coherent global structure. Multifaceted feature visualization thus
provides a clearer and more comprehensive description of the role of each
neuron.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03617</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03617</id><created>2016-02-11</created><authors><author><keyname>Bengua</keyname><forenames>Johann A.</forenames></author><author><keyname>Tuan</keyname><forenames>Hoang D.</forenames></author><author><keyname>Phien</keyname><forenames>Ho N.</forenames></author><author><keyname>Kha</keyname><forenames>Ha H.</forenames></author></authors><title>Two-hop Power-Relaying for Linear Wireless Sensor Networks</title><categories>cs.NI cs.SY</categories><comments>Submitted to IEEE ICCE 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents two-hop relay gain-scheduling control in a Wireless
Sensor Network to estimate a static target prior characterized by Gaussian
probability distribution. The target is observed by a network of linear
sensors, whose observations are transmitted to a fusion center for carrying out
final estimation via a amplify-and-forward relay node. We are concerned with
the joint transmission power allocation for sensors and relay to optimize the
minimum mean square error (MMSE) estimator, which is deployed at the fusion
center. Particularly, such highly nonlinear optimization problems are solved by
an iterative procedure of very low computational complexity. Simulations are
provided to support the efficiency of our proposed power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03618</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03618</id><created>2016-02-11</created><updated>2016-02-11</updated><authors><author><keyname>Thakor</keyname><forenames>Satyajit</forenames></author><author><keyname>Chan</keyname><forenames>Terence</forenames></author><author><keyname>Grant</keyname><forenames>Alex</forenames></author></authors><title>Characterising Probability Distributions via Entropies</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterising the capacity region for a network can be extremely difficult,
especially when the sources are dependent. Most existing computable outer
bounds are relaxations of the Linear Programming bound. One main challenge to
extend linear program bounds to the case of correlated sources is the
difficulty (or impossibility) of characterising arbitrary dependencies via
entropy functions. This paper tackles the problem by addressing how to use
entropy functions to characterise correlation among sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03619</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03619</id><created>2016-02-11</created><authors><author><keyname>Ok</keyname><forenames>Jungseul</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author><author><keyname>Yi</keyname><forenames>Yung</forenames></author></authors><title>Optimality of Belief Propagation for Crowdsourced Classification</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing systems are popular for solving large-scale labelling tasks
with low-paid (or even non-paid) workers. We study the problem of recovering
the true labels from the possibly erroneous crowdsourced labels under the
popular Dawid-Skene model. To address this inference problem, several
algorithms have recently been proposed, but the best known guarantee is still
significantly larger than the fundamental limit. We close this gap under a
simple but canonical scenario where each worker is assigned at most two tasks.
In particular, we introduce a tighter lower bound on the fundamental limit and
prove that Belief Propagation (BP) exactly matches this lower bound. The
guaranteed optimality of BP is the strongest in the sense that it is
information-theoretically impossible for any other algorithm to correctly label
a larger fraction of the tasks. In the general setting, when more than two
tasks are assigned to each worker, we establish the dominance result on BP that
it outperforms all existing algorithms with provable guarantees. Experimental
results suggest that BP is close to optimal for all regimes considered, while
all other algorithms show suboptimal performances in certain regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03623</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03623</id><created>2016-02-11</created><authors><author><keyname>He</keyname><forenames>Liang</forenames></author><author><keyname>Pan</keyname><forenames>Jia</forenames></author><author><keyname>Narang</keyname><forenames>Sahil</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Dynamic Group Behaviors for Interactive Crowd Simulation</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm to simulate dynamic group behaviors for
interactive multi-agent crowd simulation. Our approach is general and makes no
assumption about the environment, shape, or size of the groups. We use the
least effort principle to perform coherent group navigation and present
efficient inter-group and intra-group maintenance techniques. We extend the
reciprocal collision avoidance scheme to perform agent-group and group-group
collision avoidance that can generate collision-free as well as coherent and
trajectories. The additional overhead of dynamic group simulation is relatively
small. We highlight its interactive performance on complex scenarios with
hundreds of agents and compare the trajectory behaviors with real-world videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03635</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03635</id><created>2016-02-11</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames></author><author><keyname>Bai</keyname><forenames>Xinwei</forenames></author><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author></authors><title>Optimization of Caching Devices with Geometric Constraints</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been recently advocated that in large communication systems it is
beneficial both for the users and for the network as a whole to store content
closer to users. One particular implementation of such an approach is to
co-locate caches with wireless base stations. In this paper we study
geographically distributed caching of a fixed collection of files. We model
cache placement with the help of stochastic geometry and optimize the
allocation of storage capacity among files in order to minimize the cache miss
probability. We consider both per cache capacity constraints as well as an
average capacity constraint over all caches. The case of per cache capacity
constraints can be efficiently solved using dynamic programming, whereas the
case of the average constraint leads to a convex optimization problem. We
demonstrate that the average constraint leads to significantly smaller cache
miss probability. Finally, we suggest a simple LRU-based policy for
geographically distributed caching and show that its performance is close to
the optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03636</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03636</id><created>2016-02-11</created><authors><author><keyname>Fortuna</keyname><forenames>Rok</forenames></author><author><keyname>Marovt</keyname><forenames>Urban</forenames></author></authors><title>Link prediction in Foursquare network</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Foursquare is an online social network and can be represented with a
bipartite network of users and venues. A user-venue pair is connected if a user
has checked-in at that venue. In the case of Foursquare, network analysis
techniques can be used to enhance the user experience. One such technique is
link prediction, which can be used to build a personalized recommendation
system of venues. Recommendation systems in bipartite networks are very often
designed using the global ranking method and collaborative filtering. A less
known method- network based inference is also a feasible choice for link
prediction in bipartite networks and sometimes performs better than the
previous two. In this paper we test these techniques on the Foursquare network.
The best technique proves to be the network based inference. We also show that
taking into account the available metadata can be beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03638</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03638</id><created>2016-02-11</created><authors><author><keyname>Mortensen</keyname><forenames>Mikael</forenames></author><author><keyname>Langtangen</keyname><forenames>Hans Petter</forenames></author></authors><title>High performance Python for direct numerical simulations of turbulent
  flows</title><categories>cs.MS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct Numerical Simulations (DNS) of the Navier Stokes equations is an
invaluable research tool in fluid dynamics. Still, there are few publicly
available research codes and, due to the heavy number crunching implied,
available codes are usually written in low-level languages such as C/C++ or
Fortran. In this paper we describe a pure scientific Python pseudo-spectral DNS
code that nearly matches the performance of C++ for thousands of processors and
billions of unknowns. We also describe a version optimized through Cython, that
is found to match the speed of C++. The solvers are written from scratch in
Python, both the mesh, the MPI domain decomposition, and the temporal
integrators. The solvers have been verified and benchmarked on the Shaheen
supercomputer at the KAUST supercomputing laboratory, and we are able to show
very good scaling up to several thousand cores.
  A very important part of the implementation is the mesh decomposition (we
implement both slab and pencil decompositions) and 3D parallel Fast Fourier
Transforms (FFT). The mesh decomposition and FFT routines have been implemented
in Python using serial FFT routines (either NumPy, pyFFTW or any other serial
FFT module), NumPy array manipulations and with MPI communications handled by
MPI for Python (mpi4py). We show how we are able to execute a 3D parallel FFT
in Python for a slab mesh decomposition using 4 lines of compact Python code,
for which the parallel performance on Shaheen is found to be slightly better
than similar routines provided through the FFTW library. For a pencil mesh
decomposition 7 lines of code is required to execute a transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03641</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03641</id><created>2016-02-11</created><authors><author><keyname>Xing</keyname><forenames>Feng</forenames><affiliation>JAD, COFFEE, BRGM</affiliation></author><author><keyname>Masson</keyname><forenames>Roland</forenames><affiliation>JAD, COFFEE</affiliation></author><author><keyname>Lopez</keyname><forenames>Simon</forenames><affiliation>BRGM</affiliation></author></authors><title>Parallel Vertex Approximate Gradient discretization of hybrid
  dimensional Darcy flow and transport in discrete fracture networks</title><categories>math.NA cs.CE cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a parallel numerical algorithm to simulate the flow and
the transport in a discrete fracture network taking into account the mass
exchanges with the surrounding matrix. The discretization of the Darcy fluxes
is based on the Vertex Approximate Gradient finite volume scheme adapted to
polyhedral meshes and to heterogeneous anisotropic media, and the transport
equation is discretized by a first order upwind scheme combined with an Euler
explicit integration in time. The parallelization is based on the SPMD (Single
Program, Multiple Data) paradigm and relies on a distribution of the mesh on
the processes with one layer of ghost cells in order to allow for a local
assembly of the discrete systems. The linear system for the Darcy flow is
solved using different linear solvers and preconditioners implemented in the
PETSc and Trilinos libraries. The convergence of the scheme is validated on two
original analytical solutions with one and four intersecting fractures. Then,
the parallel efficiency of the algorithm is assessed on up to 512 processes
with different types of meshes, different matrix fracture permeability ratios,
and different levels of complexity of the fracture network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03642</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03642</id><created>2016-02-11</created><authors><author><keyname>Damg&#xe5;rd</keyname><forenames>Ivan</forenames></author><author><keyname>Haagh</keyname><forenames>Helene</forenames></author><author><keyname>Orlandi</keyname><forenames>Claudio</forenames></author></authors><title>Access Control Encryption: Enforcing Information Flow with Cryptography</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of Access Control Encryption (ACE), a novel
cryptographic primitive that allows fine-grained access control, by giving
different rights to different users not only in terms of which messages they
are allowed to receive, but also which messages they are allowed to send.
  Classical examples of security policies for information flow are the well
known Bell-Lapadula [BL73] or Biba [Bib75] model: in a nutshell, the
Bell-Lapadula model assigns roles to every user in the system (e.g., public,
secret and top-secret). A users' role specifies which messages the user is
allowed to receive (i.e., the no read-up rule, meaning that users with public
clearance should not be able to read messages marked as secret or top-secret)
but also which messages the user is allowed to send (i.e., the no write-down
rule, meaning that a user with top-secret clearance should not be able to write
messages marked as secret or public).
  To the best of our knowledge, no existing cryptographic primitive allows for
even this simple form of access control, since no existing cryptographic
primitive enforces any restriction on what kind of messages one should be able
to encrypt.
  Our contributions are: - Introducing and formally defining access control
encryption (ACE); - A construction of ACE with complexity linear in the number
of the roles based on classic number theoretic assumptions (DDH, Paillier); - A
construction of ACE with complexity polylogarithmic in the number of roles
based on recent results on cryptographic obfuscation;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03643</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03643</id><created>2016-02-11</created><authors><author><keyname>Mortensen</keyname><forenames>Mikael</forenames></author><author><keyname>Valen-Sendstad</keyname><forenames>Kristian</forenames></author></authors><title>Oasis: a high-level/high-performance open source Navier-Stokes solver</title><categories>cs.MS</categories><journal-ref>Computer Physics Communications, Volume 188, p 177-188, 2015</journal-ref><doi>10.1016/j.cpc.2014.10.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oasis is a high-level/high-performance finite element Navier-Stokes solver
written from scratch in Python using building blocks from the FEniCS project
(fenicsproject.org). The solver is unstructured and targets large-scale
applications in complex geometries on massively parallel clusters. Oasis
utilizes MPI and interfaces, through FEniCS, to the linear algebra backend
PETSc. Oasis advocates a high-level, programmable user interface through the
creation of highly flexible Python modules for new problems. Through the
high-level Python interface the user is placed in complete control of every
aspect of the solver. A version of the solver, that is using piecewise linear
elements for both velocity and pressure, is shown reproduce very well the
classical, spectral, turbulent channel simulations of Moser, Kim and Mansour at
$Re_{\tau}=180$ [Phys. Fluids, vol 11(4), p. 964]. The computational speed is
strongly dominated by the iterative solvers provided by the linear algebra
backend, which is arguably the best performance any similar implicit solver
using PETSc may hope for. Higher order accuracy is also demonstrated and new
solvers may be easily added within the same framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03644</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03644</id><created>2016-02-11</created><authors><author><keyname>Arnau</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Atzeni</keyname><forenames>Italo</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author></authors><title>Impact of LOS/NLOS Propagation and Path Loss in Ultra-Dense Cellular
  Networks</title><categories>cs.IT math.IT</categories><comments>Paper accepted at IEEE ICC 2016 - Wireless Communications Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most prior work on performance analysis of ultra-dense cellular networks
(UDNs) has considered standard power-law path loss models and non-line-of-sight
(NLOS) propagation modeled by Rayleigh fading. The effect of line-of-sight
(LOS) on coverage and throughput and its implication on network densification
are still not fully understood. In this paper, we investigate the performance
of UDNs when the signal propagation includes both LOS and NLOS components.
Using a stochastic geometry based cellular network model, we derive expressions
for the coverage probability, as well as tight approximations and upper bounds
for both closest and strongest base station (BS) association. Our results show
that under standard singular path loss model, LOS propagation increases the
coverage, especially with nearest BS association. On the contrary, using dual
slope path loss, LOS propagation is beneficial with closest BS association and
detrimental for strongest BS association.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03647</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03647</id><created>2016-02-11</created><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>On the Difficulty of Selecting Ising Models with Approximate Recovery</title><categories>cs.IT cs.LG cs.SI math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of estimating the underlying graphical
model of an Ising distribution given a number of independent and identically
distributed samples. We adopt an \emph{approximate recovery} criterion that
allows for a number of missed edges or incorrectly-included edges, thus
departing from the extensive literature considering the exact recovery problem.
Our main results provide information-theoretic lower bounds on the required
number of samples (i.e., the sample complexity) for graph classes imposing
constraints on the number of edges, maximal degree, and sparse separation
properties. We identify a broad range of scenarios where, either up to constant
factors or logarithmic factors, our lower bounds match the best known lower
bounds for the exact recovery criterion, several of which are known to be tight
or near-tight. Hence, in these cases, we prove that the approximate recovery
problem is not much easier than the exact recovery problem.
  Our bounds are obtained via a modification of Fano's inequality for handling
the approximate recovery criterion, along with suitably-designed ensembles of
graphs that can broadly be classed into two categories: (i) Those containing
graphs that contain several isolated edges or cliques and are thus difficult to
distinguish from the empty graph; (ii) Those containing graphs for which
certain groups of nodes are highly correlated, thus making it difficult to
determine precisely which edges connect them. We support our theoretical
results on these ensembles with numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03648</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03648</id><created>2016-02-11</created><authors><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Joint Beamforming and Broadcasting in Massive MIMO</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Wireless Communications, 2016</journal-ref><doi>10.1109/TWC.2016.2515598</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The downlink of a massive MIMO system is considered for the case in which the
base station must concurrently serve two categories of terminals: one group to
which imperfect instantaneous channel state information (CSI) is available, and
one group to which no CSI is available. Motivating applications include
broadcasting of public channels and control information in wireless networks.
  A new technique is developed and analyzed: joint beamforming and broadcasting
(JBB), by which the base station beamforms to the group of terminals to which
CSI is available, and broadcasts to the other group of terminals, to which no
CSI is available. The broadcast information does not interfere with the
beamforming as it is placed in the nullspace of the channel matrix collectively
seen by the terminals targeted by the beamforming. JBB is compared to
orthogonal access (OA), by which the base station partitions the time-frequency
resources into two disjunct parts, one for each group of terminals.
  It is shown that JBB can substantially outperform OA in terms of required
total radiated power for given rate targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03650</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03650</id><created>2016-02-11</created><authors><author><keyname>Arcagni</keyname><forenames>Alberto</forenames></author><author><keyname>Grassi</keyname><forenames>Rosanna</forenames></author><author><keyname>Stefani</keyname><forenames>Silvana</forenames></author><author><keyname>Torriero</keyname><forenames>Anna</forenames></author></authors><title>Higher order assortativity in complex networks</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assortativity was first introduced by Newman and has been extensively studied
and applied to many real world networked systems since then. Assortativity is a
graph metrics and describes the tendency of high degree nodes to be directly
connected to high degree nodes and low degree nodes to low degree nodes. It can
be interpreted as a first order measure of the connection between nodes, i.e.
the first autocorrelation of the degree-degree vector. Even though
assortativity has been used so extensively, to the author's knowledge, no
attempt has been made to extend it theoretically. This is the scope of our
paper. We will introduce higher order assortativity by extending the Newman
index based on a suitable choice of the matrix driving the connections. Higher
order assortativity will be defined for paths, shortest paths, random walks of
a given time length, connecting any couple of nodes. The Newman assortativity
is achieved for each of these measures when the matrix is the adjacency matrix,
or, in other words, the correlation is of order 1. Our higher order
assortativity indexes can be used for describing a variety of real networks,
help discriminating networks having the same Newman index and may reveal new
topological network features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03654</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03654</id><created>2016-02-11</created><authors><author><keyname>Xiao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Xia</keyname><forenames>Pengfei</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>Enabling UAV Cellular with Millimeter-Wave Communication: Potentials and
  Approaches</title><categories>cs.IT math.IT</categories><comments>This paper explores the potentials and approaches to exploit mmWave
  communication to establish a UAV cellular. It is to appear in IEEE
  Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support high data rate urgent or ad hoc communications, we consider mmWave
UAV cellular networks and the associated challenges and solutions. To enable
fast beamforming training and tracking, we first investigate a hierarchical
structure of beamforming codebooks and design of hierarchical codebooks with
different beam widths via the sub-array techniques. We next examine the Doppler
effect as a result of UAV movement and find that the Doppler effect may not be
catastrophic when high gain directional transmission is used. We further
explore the use of millimeter wave spatial division multiple access and
demonstrate its clear advantage in improving the cellular network capacity. We
also explore different ways of dealing with signal blockage and point out that
possible adaptive UAV cruising algorithms would be necessary to counteract
signal blockage. Finally, we identify a close relationship between UAV
positioning and directional millimeter wave user discovery, where update of the
former may directly impact the latter and vice versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03655</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03655</id><created>2016-02-11</created><authors><author><keyname>Ben-Porat</keyname><forenames>Omer</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>The (l,k) Facility Location Duel</title><categories>cs.GT</categories><comments>Pre-Print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in clustering and information retrieval, we extend
the classical Hotelling setting to deal with multi-facility location duels. In
the classical Hotelling setting, customers' locations are taken from the
uniform distribution on the $[0,1]$ segment, and there are two facility owners,
each needs to decide on the location of her (single) facility, aiming to
maximize the proportion of customers closer to it. We extend this duel to
competition among the owner of $k$ facilities to the owner of $l$ facilities,
for arbitrary $k,l$, where $l\leq k$. Our main message is quite striking: in no
equilibrium a facility will materialize in a location which is not part of the
social welfare maximizing locations of the player who has $k$ facilities, if
she were to locate her facilities under no competition. This is obtained
despite the lack of pure strategy equilibrium in many of these settings. We
also study two sets of other natural families of distributions; for one of the
families we show the above findings extend beyond the uniform distribution; for
the other we show that selecting only among the stronger-player social welfare
maximizing locations will not materialize in equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03661</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03661</id><created>2016-02-11</created><authors><author><keyname>Loreto</keyname><forenames>Vittorio</forenames></author><author><keyname>Gravino</keyname><forenames>Pietro</forenames></author><author><keyname>Servedio</keyname><forenames>Vito D. P.</forenames></author><author><keyname>Tria</keyname><forenames>Francesca</forenames></author></authors><title>On the emergence of syntactic structures: quantifying and modelling
  duality of patterning</title><categories>physics.soc-ph cs.CL</categories><comments>11 pages, to appear in the Proceedings of the Workshop on Origins of
  Communication Systems: Modeling and Ethologically-based Theory, Konrad Lorenz
  Institute for Evolution and Cognition Research, Altenberg, Austria (2013).
  Special Issue of topiCS on New frontiers in language evolution and
  development</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complex organization of syntax in hierarchical structures is one of the
core design features of human language. Duality of patterning refers for
instance to the organization of the meaningful elements in a language at two
distinct levels: a combinatorial level where meaningless forms are combined
into meaningful forms and a compositional level where meaningful forms are
composed into larger lexical units. The question remains wide open regarding
how such a structure could have emerged. Furthermore a clear mathematical
framework to quantify this phenomenon is still lacking. The aim of this paper
is that of addressing these two aspects in a self-consistent way. First, we
introduce suitable measures to quantify the level of combinatoriality and
compositionality in a language, and present a framework to estimate these
observables in human natural languages. Second, we show that the theoretical
predictions of a multi-agents modeling scheme, namely the Blending Game, are in
surprisingly good agreement with empirical data. In the Blending Game a
population of individuals plays language games aiming at success in
communication. It is remarkable that the two sides of duality of patterning
emerge simultaneously as a consequence of a pure cultural dynamics in a
simulated environment that contains meaningful relations, provided a simple
constraint on message transmission fidelity is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03664</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03664</id><created>2016-02-11</created><authors><author><keyname>Tribu&#x161;on</keyname><forenames>Matic</forenames></author><author><keyname>Leni&#x10d;</keyname><forenames>Matev&#x17e;</forenames></author></authors><title>Identifying top football players and springboard clubs from a football
  player collaboration and club transfer networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider all players and clubs in top twenty world football leagues in the
last fifteen seasons. The purpose of this paper is to reveal top football
players and identify springboard clubs. To do that, we construct two separate
weighted networks. Player collaboration network consists of players, that are
connected to each other if they ever played together at the same club. In
directed club transfer network, clubs are connected if players were ever
transferred from one club to another. To get meaningful results, we perform
different network analysis methods on our networks. Our approach based on
PageRank reveals Christiano Ronaldo as the top player. Using a variation of
betweenness centrality, we identify Standard Liege as the best springboard
club.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03674</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03674</id><created>2016-02-11</created><authors><author><keyname>&#x10c;rnigoj</keyname><forenames>Dean</forenames></author></authors><title>Analysis of friendship network from MMORPG based data</title><categories>cs.SI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyzes friendship network from a Massively Multiplayer Online
Role-Playing Game (MMORPG). The network is based on data from a private server
that was active from 2007 until 2011. The work conducts a standard analysis of
the network and then divides players according to different groups based on
their activity. Work checks how friendship network can be correlated to the
clan (a self-organized group of players who often form a league and play on the
same side in a match) network. Main part of the work is the recommendation
method for players that are not part of any clan and it is based on communities
of friendship network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03676</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03676</id><created>2016-02-11</created><updated>2016-02-16</updated><authors><author><keyname>AlAmmouri</keyname><forenames>Ahmad</forenames></author><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Sultan-Salem</keyname><forenames>Ahmed</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Modeling Cellular Networks in Fading Environments with Dominant Specular
  Components</title><categories>cs.IT math.IT</categories><comments>IEEE ICC16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic geometry (SG) has been widely accepted as a fundamental tool for
modeling and analyzing cellular networks. However, the fading models used with
SG analysis are mainly confined to the simplistic Rayleigh fading, which is
extended to the Nakagami-m fading in some special cases. However, neither the
Rayleigh nor the Nakagami-m accounts for dominant specular components (DSCs)
which may appear in realistic fading channels. In this paper, we present a
tractable model for cellular networks with generalized two-ray (GTR) fading
channel. The GTR fading explicitly accounts for two DSCs in addition to the
diffuse components and offers high flexibility to capture diverse fading
channels that appear in realistic outdoor/indoor wireless communication
scenarios. It also encompasses the famous Rayleigh and Rician fading as special
cases. To this end, the prominent effect of DSCs is highlighted in terms of
average spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03681</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03681</id><created>2016-02-11</created><authors><author><keyname>Slijep&#x10d;evi&#x107;</keyname><forenames>Tomislav</forenames></author></authors><title>Package equivalence in complex software network</title><categories>cs.SI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The public package registry npm is one of the biggest software registry. With
its 216 911 software packages, it forms a big network of software dependencies.
In this paper we evaluate various methods for finding similar packages in the
npm network, using only the structure of the graph. Namely, we want to find a
way of categorizing similar packages, which would be useful for recommendation
systems. This size enables us to compute meaningful results, as it softened the
particularities of the graph. Npm is also quite famous as it is the default
package repository of Node.js. We believe that it will make our results
interesting for more people than a less used package repository. This makes it
a good subject of analysis of software networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03686</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03686</id><created>2016-02-11</created><authors><author><keyname>Choi</keyname><forenames>Edward</forenames></author><author><keyname>Schuetz</keyname><forenames>Andy</forenames></author><author><keyname>Stewart</keyname><forenames>Walter F.</forenames></author><author><keyname>Sun</keyname><forenames>Jimeng</forenames></author></authors><title>Medical Concept Representation Learning from Electronic Health Records
  and its Application on Heart Failure Prediction</title><categories>cs.LG cs.NE</categories><comments>45 pages, under revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: To transform heterogeneous clinical data from electronic health
records into clinically meaningful constructed features using data driven
method that rely, in part, on temporal relations among data. Materials and
Methods: The clinically meaningful representations of medical concepts and
patients are the key for health analytic applications. Most of existing
approaches directly construct features mapped to raw data (e.g., ICD or CPT
codes), or utilize some ontology mapping such as SNOMED codes. However, none of
the existing approaches leverage EHR data directly for learning such concept
representation. We propose a new way to represent heterogeneous medical
concepts (e.g., diagnoses, medications and procedures) based on co-occurrence
patterns in longitudinal electronic health records. The intuition behind the
method is to map medical concepts that are co-occuring closely in time to
similar concept vectors so that their distance will be small. We also derive a
simple method to construct patient vectors from the related medical concept
vectors. Results: We evaluate similar medical concepts across diagnosis,
medication and procedure. The results show xx% relevancy between similar pairs
of medical concepts. Our proposed representation significantly improves the
predictive modeling performance for onset of heart failure (HF), where
classification methods (e.g. logistic regression, neural network, support
vector machine and K-nearest neighbors) achieve up to 23% improvement in area
under the ROC curve (AUC) using this proposed representation. Conclusion: We
proposed an effective method for patient and medical concept representation
learning. The resulting representation can map relevant concepts together and
also improves predictive modeling performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03689</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03689</id><created>2016-02-11</created><authors><author><keyname>Porotsky</keyname><forenames>Sergey</forenames></author></authors><title>Analytic Methods to Calculate Fault Trees with Loops - Restrictions of
  Application and Solution Uniqueness</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important tasks of the Reliability Estimation is Analysis of the
Fault Tree. A problem of Fault Trees analysis is considered one of the most
complex ones, since structure of such trees is characterized by a considerable
number of interconnections. Usually analytical methods are used and most
applicable method is Minimal Cut Sets building and calculation. Classical Fault
Tree Analysis methods are applicable only for Fault Trees without loops. Loops
can appear in Fault Tree, when a TOP or some intermediate gates appear as input
to another gate at a lower level of the model. An occurrence of a Loop has been
a problematic issue in a Fault Tree calculation. The article relates to the
uniqueness of the solution for the Fault Trees with arbitrary Loops. There are
assumed, that failures of the Basic Events are non-repairable and Fault Tree
gates may be expressed by two main logic gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03699</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03699</id><created>2016-02-11</created><authors><author><keyname>Al-Maqri</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author><author><keyname>Ali</keyname><forenames>Borhanuddin Mohd</forenames></author><author><keyname>Hanapi</keyname><forenames>Zurina Mohd</forenames></author></authors><title>Adaptive TXOP Assignment for QoS Support of Video Traffic in IEEE
  802.11e Networks</title><categories>cs.NI</categories><doi>10.1109/RFM.2013.6757236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality of Service (QoS) is provided in IEEE 802.11e protocol by means of HCF
Controlled Channel Access (HCCA) scheduler which is efficient for supporting
Constant Bit Rate (CBR) applications. Numerous researches have been carried out
to enhance the HCCA scheduler attempting to accommodate the needs of Variable
Bit Rate (VBR) video traffics which probably demonstrates a non-deterministic
profile during the time. This paper presents an adaptive TXOP assignment
mechanism for supporting the transmission of the prerecorded video traffics
over IEEE 802.11e wireless networks. The proposed mechanism uses a feedback
about the size of the subsequent video frames of the uplink traffic to assist
the Hybrid Coordinator (HC) accurately assign TXOP according to the fast
changes in the VBR profile. The simulation results show that our mechanism
reduces the delay experienced by VBR traffic streams comparable to HCCA
scheduler due to the accurate assignment of the TXOP which preserve the channel
time for data transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03706</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03706</id><created>2016-02-11</created><authors><author><keyname>Mirkhanzadeh</keyname><forenames>Behzad</forenames></author><author><keyname>Taheri</keyname><forenames>Naeim</forenames></author><author><keyname>Khorsandi</keyname><forenames>Siyavash</forenames></author></authors><title>SDxVPN: A Software-Defined Solution for VPN Service Providers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BGP/MPLS IP VPN and VPLS services are considered to be widely used in IP/MPLS
networks for connecting customers' remote sites. However, service providers
struggle with many challenges to provide these services. Management complexity,
equipment costs, and last but not least, scalability issues emerging as the
customers increase in number, are just some of these problems. Software-defined
networking (SDN) is an emerging paradigm that can solve aforementioned issues
using a logically centralized controller for network devices. In this paper, we
propose a SDN-based solution called SDxVPN which considerably lowers the
complexity of VPN service definition and management. Our method eliminates
complex and costly device interactions that used to be done through several
control plane protocols and enables customers to determine their service
specifications, define restriction policies and even interconnect with other
customers automatically without operator's intervention. We describe our
prototype implementation of SDxVPN and its scalability evaluations under
several representative scenarios. The results indicate the effectiveness of the
proposed solution for deployment to provide large scale VPN services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03713</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03713</id><created>2016-02-11</created><updated>2016-02-12</updated><authors><author><keyname>Bar-Yehuda</keyname><forenames>Reuven</forenames></author><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Schwartzman</keyname><forenames>Gregory</forenames></author></authors><title>A Distributed $(2+\epsilon)$-Approximation for Vertex Cover in
  $O(\log{\Delta}/\epsilon\log\log{\Delta})$ Rounds</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple deterministic distributed $(2+\epsilon)$-approximation
algorithm for minimum weight vertex cover, which completes in
$O(\log{\Delta}/\epsilon\log\log{\Delta})$ rounds, where $\Delta$ is the
maximum degree in the graph, for any $\epsilon&gt;0$ which is at most $O(1)$. For
a constant $\epsilon$, this implies a constant approximation in
$O(\log{\Delta}/\log\log{\Delta})$ rounds, which contradicts the lower bound of
[KMW10].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03716</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03716</id><created>2016-02-11</created><authors><author><keyname>Al-Maqri</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author><author><keyname>Ali</keyname><forenames>Borhanuddin Mohd</forenames></author><author><keyname>Hanapi</keyname><forenames>Zurina Mohd</forenames></author></authors><title>Feasible HCCA Polling Mechanism for Video Transmission in IEEE 802.11e
  WLANs</title><categories>cs.NI cs.MM</categories><doi>10.1007/s11277-015-2816-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEEE 802.11e standard defines two Medium Access Control (MAC) functions to
support Quality of Service (QoS) for wireless local area networks: Enhanced
Distributed Channel Access (EDCA) and HCF Controlled Channel Access (HCCA).
EDCA provides fair prioritized QoS support while HCCA guarantees parameterized
QoS for the traffics with rigid QoS requirements. The latter shows higher QoS
provisioning with Constant Bit Rate (CBR) traffics. However, it does not
efficiently cope with the fluctuation of the Variable Bit Rate (VBR) video
streams since its reference scheduler generates a schedule based on the mean
characteristics of the traffic. Scheduling based on theses characteristics is
not always accurate as these tra_cs show high irregularity over the time. In
this paper, we propose an enhancement on the HCCA polling mechanism to address
the problem of scheduling pre-recorded VBR video streams. Our approach enhances
the polling mechanism by feed-backing the arrival time of the subsequent video
frame of the uplink traffic obtained through cross-layering approach.
Simulation experiments have been conducted on several publicly available video
traces in order to show the efficiency of our mechanism. The simulation results
reveal the efficiency of the proposed mechanism in providing less delay and
high throughput with conserving medium channel through minimizing the number of
Null-Frames caused by wasted polls
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03718</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03718</id><created>2016-02-11</created><updated>2016-02-15</updated><authors><author><keyname>Censor-Hillel</keyname><forenames>Keren</forenames></author><author><keyname>Fischer</keyname><forenames>Eldar</forenames></author><author><keyname>Schwartzman</keyname><forenames>Gregory</forenames></author><author><keyname>Vasudev</keyname><forenames>Yadu</forenames></author></authors><title>Fast Distributed Algorithms for Testing Graph Properties</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a thorough study of \emph{distributed property testing} --
producing algorithms for the approximation problems of property testing in the
CONGEST model. In particular, for the so-called \emph{dense} testing model we
emulate sequential tests for nearly all graph properties having $1$-sided
tests, while in the \emph{sparse} and \emph{general} models we obtain faster
tests for triangle-freeness and bipartiteness respectively.
  In most cases, aided by parallelism, the distributed algorithms have a much
shorter running time as compared to their counterparts from the sequential
querying model of traditional property testing. The simplest property testing
algorithms allow a relatively smooth transitioning to the distributed model.
For the more complex tasks we develop new machinery that is of independent
interest. This includes a method for distributed maintenance of multiple random
walks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03719</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03719</id><created>2016-02-11</created><authors><author><keyname>Klju&#x10d;ev&#x161;ek</keyname><forenames>Aleksander</forenames></author><author><keyname>Krapi&#x107;</keyname><forenames>Luka</forenames></author></authors><title>Discovering novel ingredient pairings in molecular gastronomy using
  network analysis</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular gastronomy is a distinct sub-discipline of food science that takes
an active role in examining chemical and physical properties of ingredients and
as such lends itself to more scientific approaches to finding novel ingredient
pairings. With thousands of ingredients and molecules, which participate in the
creation of each ingredient's flavour, it can be difficult to find compatible
flavours in an efficient manner. Existing literature is focused mainly on
analysis of already established cuisine based on the flavour profile of its
ingredients, but fails to consider the potential in finding flavour
compatibility for use in creation of completely new recipes. Expressing
relationships between ingredients and their molecular structure as a bipartite
network opens up this problem to effective analysis with methods from network
science. We describe a series of experiments on a database of food using
network analysis, which produce a set of compatible ingredients that can be
used in creation of new recipes. We expect this approach and its results to
dramatically simplify the creation of new recipes with previously unseen and
fresh combinations of ingredients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03722</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03722</id><created>2016-02-11</created><authors><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author></authors><title>Sequences with small correlation</title><categories>cs.IT math.CO math.IT</categories><comments>Survey paper, 32 pages</comments><journal-ref>Des. Codes Cryptogr. 78(1), 237-267, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extent to which a sequence of finite length differs from a shifted
version of itself is measured by its aperiodic autocorrelations. Of particular
interest are sequences whose entries are 1 or -1, called binary sequences, and
sequences whose entries are complex numbers of unit magnitude, called
unimodular sequences. Since the 1950s, there is sustained interest in sequences
with small aperiodic autocorrelations relative to the sequence length. One of
the main motivations is that a sequence with small aperiodic autocorrelations
is intrinsically suited for the separation of signals from noise, and therefore
has natural applications in digital communications. This survey reviews the
state of knowledge concerning the two central problems in this area: How small
can the aperiodic autocorrelations of a binary or a unimodular sequence
collectively be and how can we efficiently find the best such sequences? Since
the analysis and construction of sequences with small aperiodic
autocorrelations is closely tied to the (often much easier) analysis of
periodic autocorrelation properties, several fundamental results on
corresponding problems in the periodic setting are also reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03725</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03725</id><created>2016-02-11</created><authors><author><keyname>Rhodin</keyname><forenames>Helge</forenames></author><author><keyname>Robertini</keyname><forenames>Nadia</forenames></author><author><keyname>Richardt</keyname><forenames>Christian</forenames></author><author><keyname>Seidel</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>A Versatile Scene Model with Differentiable Visibility Applied to
  Generative Pose Estimation</title><categories>cs.CV</categories><comments>9 pages, In proceedings of ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative reconstruction methods compute the 3D configuration (such as pose
and/or geometry) of a shape by optimizing the overlap of the projected 3D shape
model with images. Proper handling of occlusions is a big challenge, since the
visibility function that indicates if a surface point is seen from a camera can
often not be formulated in closed form, and is in general discrete and
non-differentiable at occlusion boundaries. We present a new scene
representation that enables an analytically differentiable closed-form
formulation of surface visibility. In contrast to previous methods, this yields
smooth, analytically differentiable, and efficient to optimize pose similarity
energies with rigorous occlusion handling, fewer local minima, and
experimentally verified improved convergence of numerical optimization. The
underlying idea is a new image formation model that represents opaque objects
by a translucent medium with a smooth Gaussian density distribution which turns
visibility into a smooth phenomenon. We demonstrate the advantages of our
versatile scene model in several generative pose estimation problems, namely
marker-less multi-object pose estimation, marker-less human motion capture with
few cameras, and image-based 3D geometry estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03729</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03729</id><created>2016-02-11</created><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Choreographies, Divided and Conquered</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreographic Programming is a paradigm for developing concurrent software
that is correct by construction, by syntactically disallowing mismatched I/O
operations in programs, called choreographies. Due to their benefits,
choreographies have been largely adopted for the writing of business processes
and communication protocols. However, current choreography language models
cannot capture many kinds of communication structures, limiting their
applicability.
  In this paper, we present Procedural Choreographies (PC), a new language
model that includes the novel feature of reusable choreographic procedures,
parameterised on the processes they use. PC also combines, for the first time
in choreographies, general recursion with the ability to create new processes
at runtime. The combination of these features yields a powerful framework where
we can write divide-and-conquer concurrent algorithms based on message passing.
This enhanced expressivity makes it possible to write new behaviours that
cannot be faithfully implemented (unrealisability); to tackle this issue, we
endow PC with a new typing discipline that supports both decidable type
checking and type inference. PC is equipped with an EndPoint Projection (EPP)
that, from a well-typed choreography, synthesises a correct-by-construction
distributed implementation in a process calculus. Extending a previous line of
work on choreographies, our model supports two important properties wrt the
programming of concurrent algorithms: implicit parallelism and transparent
projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03730</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03730</id><created>2016-02-11</created><updated>2016-02-16</updated><authors><author><keyname>Rahman</keyname><forenames>Md Farhadur</forenames></author><author><keyname>Liu</keyname><forenames>Weimo</forenames></author><author><keyname>Suhaim</keyname><forenames>Saad Bin</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>HDBSCAN: Density based Clustering over Location Based Services</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location Based Services (LBS) have become extremely popular and used by
millions of users. Popular LBS run the entire gamut from mapping services (such
as Google Maps) to restaurants (such as Yelp) and real-estate (such as Redfin).
The public query interfaces of LBS can be abstractly modeled as a kNN interface
over a database of two dimensional points: given an arbitrary query point, the
system returns the k points in the database that are nearest to the query
point. Often, k is set to a small value such as 20 or 50. In this paper, we
consider the novel problem of enabling density based clustering over an LBS
with only a limited, kNN query interface. Due to the query rate limits imposed
by LBS, even retrieving every tuple once is infeasible. Hence, we seek to
construct a cluster assignment function f(.) by issuing a small number of kNN
queries, such that for any given tuple t in the database which may or may not
have been accessed, f(.) outputs the cluster assignment of t with high
accuracy. We conduct a comprehensive set of experiments over benchmark datasets
and popular real-world LBS such as Yahoo! Flickr, Zillow, Redfin and Google
Maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03739</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03739</id><created>2016-02-11</created><authors><author><keyname>Momeni</keyname><forenames>Naghmeh</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Qualities and Inequalities in Online Social Networks through the Lens of
  the Generalized Friendship Paradox</title><categories>cs.SI physics.soc-ph</categories><journal-ref>PLoS ONE 11(2): e0143633 (2016)</journal-ref><doi>10.1371/journal.pone.0143633</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The friendship paradox is the phenomenon that in social networks, people on
average have fewer friends than their friends do. The generalized friendship
paradox is an extension to attributes other than the number of friends. The
friendship paradox and its generalized version have gathered recent attention
due to the information they provide about network structure and local
inequalities. In this paper, we propose several measures of nodal qualities
which capture different aspects of their activities and influence in online
social networks. Using these measures we analyse the prevalence of the
generalized friendship paradox over Twitter and we report high levels of
prevalence (up to over 90\% of nodes). We contend that this prevalence of the
friendship paradox and its generalized version arise because of the
hierarchical nature of the connections in the network. This hierarchy is nested
as opposed to being star-like. We conclude that these paradoxes are collective
phenomena not created merely by a minority of well-connected or high-attribute
nodes. Moreover, our results show that a large fraction of individuals can
experience the generalized friendship paradox even in the absence of a
significant correlation between degrees and attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03742</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03742</id><created>2016-02-11</created><authors><author><keyname>Palma</keyname><forenames>Carlos</forenames></author><author><keyname>Salazar</keyname><forenames>Augusto</forenames></author><author><keyname>Vargas</keyname><forenames>Francisco</forenames></author></authors><title>HMM and DTW for evaluation of therapeutical gestures using kinect</title><categories>cs.HC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic recognition of the quality of movement in human beings is a
challenging task, given the difficulty both in defining the constraints that
make a movement correct, and the difficulty in using noisy data to determine if
these constraints were satisfied. This paper presents a method for the
detection of deviations from the correct form in movements from physical
therapy routines based on Hidden Markov Models, which is compared to Dynamic
Time Warping. The activities studied include upper an lower limbs movements,
the data used comes from a Kinect sensor. Correct repetitions of the activities
of interest were recorded, as well as deviations from these correct forms. The
ability of the proposed approach to detect these deviations was studied.
Results show that a system based on HMM is much more likely to determine if a
certain movement has deviated from the specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03746</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03746</id><created>2016-02-11</created><updated>2016-03-07</updated><authors><author><keyname>Afsarmanesh</keyname><forenames>Nazanin</forenames></author><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author></authors><title>Finding overlapping communities in multiplex networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an approach to identify overlapping communities in multiplex
networks, extending the popular clique percolation method for simple graphs.
The extension requires to rethink the basic concepts on which the clique
percolation algorithm is based, including cliques and clique adjacency, to
allow the presence of multiple types of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03755</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03755</id><created>2016-02-11</created><authors><author><keyname>Chistikov</keyname><forenames>Dmitry</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Niksic</keyname><forenames>Filip</forenames></author></authors><title>Hitting Families of Schedules for Asynchronous Programs</title><categories>cs.DM cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous programming is a ubiquitous idiom for concurrent programming,
where sequential units of code, called events, are scheduled and run atomically
by a scheduler. While running, an event can post additional events for future
execution by the scheduler. Asynchronous programs can have subtle bugs due to
the non-deterministic scheduling of events, and a lot of recent research has
focused on systematic testing of these programs.
  Empirically, many bugs in asynchronous programs have small bug depth: that
is, the number of events d that must be scheduled in a specific order for a bug
to be exposed is small.
  A natural question then is to find a d-hitting family of schedules: a set of
schedules is a d-hitting family if for each set of d events, and for each
allowed ordering of these events, there is some schedule in the family that
executes these events in this ordering. A d-hitting family is guaranteed to
expose all bugs with d events. By analyzing the structure of the tree of events
in an asynchronous execution, we provide explicit constructions for small
d-hitting families of schedules. When the tree is balanced, our construction is
polylogarithmic in the number of events.
  We have implemented our algorithm for computing d-hitting families on top of
a race condition checker for web pages. We empirically confirm previous
findings that many bugs occur with small bug depth. We demonstrate that even
with d=2 we are able to detect bugs in real web applications and that we get a
small 3-hitting family for d=3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03757</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03757</id><created>2016-02-10</created><authors><author><keyname>Nazin</keyname><forenames>R&#xe9;mi</forenames><affiliation>MOSEL</affiliation></author><author><keyname>Fass</keyname><forenames>Didier</forenames><affiliation>MOSEL</affiliation></author></authors><title>Human Machine Epistemology Survey</title><categories>cs.HC</categories><proxy>ccsd</proxy><journal-ref>Vincent G. Duffy. HCI Interantional 2016, Aug 2015, Los Angeles,
  United States. Springer, LNCS 9184, pp.12, 2015, Digital Human Modeling.
  Applications in Health, Safety, Ergonomics and Risk Management: Human
  Modeling</journal-ref><doi>10.1007/978-3-319-21073-5_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pluridisciplinar convergence is a major problem that had emerged with
Human-Artefact Systems and so-called &quot; Augmented Humanity &quot; as academical
fields and even more as technical fields. Problems come mainly from the
juxtaposition of two very different types of system, a biological one and an
artificial one. Thus, conceiving and designing the multiple couplings between
them has become a major difficulty. Some came with reductionnist solutions to
answer these problems but since we know that a biological system and a
technical system are different, this approach is limited from its beginning.
Using a specifically designed questionnaire and statistical analysis we
determined how specialists (medical practitioners, ergonomists and engineers)
in the domain conceive themselves what is a Human-Artifact System and how they
relate to existent traditions and showed that some of them relate to the
integrativist views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03768</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03768</id><created>2016-02-11</created><authors><author><keyname>Hao</keyname><forenames>Chenxi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>MISO Networks with Imperfect CSIT: A Topological Rate-Splitting Approach</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE transactions on communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the Degrees-of-Freedom (DoF) region of multiple-input-single-output
(MISO) networks with imperfect channel state information at the transmitter
(CSIT) has attracted significant attentions. An achievable scheme is known as
rate-splitting (RS) that integrates common-message-multicasting and
private-message-unicasting. In this paper, focusing on the general $K$-cell
MISO IC where the CSIT of each interference link has an arbitrary quality of
imperfectness, we firstly identify the DoF region achieved by RS. Secondly, we
introduce a novel scheme, so called Topological RS (TRS), whose novelties
compared to RS lie in a multi-layer structure and transmitting multiple common
messages to be decoded by groups of users rather than all users. The design of
TRS is motivated by a novel interpretation of the $K$-cell IC with imperfect
CSIT as a weighted-sum of a series of partially connected networks. We show
that the DoF region achieved by TRS covers that achieved by RS. Also, we find
the maximal sum DoF achieved by TRS via hypergraph fractional packing, which
yields the best sum DoF so far. Lastly, for a realistic scenario where each
user is connected to three dominant transmitters, we identify the sufficient
condition where TRS strictly outperforms conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03770</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03770</id><created>2016-02-11</created><authors><author><keyname>Madsen</keyname><forenames>Kasper Grud Skat</forenames></author><author><keyname>Zhou</keyname><forenames>Yongluan</forenames></author><author><keyname>Cao</keyname><forenames>Jianneng</forenames></author></authors><title>Integrative Dynamic Reconfiguration in a Parallel Stream Processing
  Engine</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Load balancing, operator instance collocations and horizontal scaling are
critical issues in Parallel Stream Processing Engines to achieve low data
processing latency, optimized cluster utilization and minimized communication
cost respectively. In previous work, these issues are typically tackled
separately and independently. We argue that these problems are tightly coupled
in the sense that they all need to determine the allocations of workloads and
migrate computational states at runtime. Optimizing them independently would
result in suboptimal solutions. Therefore, in this paper, we investigate how
these three issues can be modeled as one integrated optimization problem. In
particular, we first consider jobs where workload allocations have little
effect on the communication cost, and model the problem of load balance as a
Mixed-Integer Linear Program. Afterwards, we present an extended solution
called ALBIC, which support general jobs. We implement the proposed techniques
on top of Apache Storm, an open-source Parallel Stream Processing Engine. The
extensive experimental results over both synthetic and real datasets show that
our techniques clearly outperform existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03779</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03779</id><created>2016-02-11</created><updated>2016-03-01</updated><authors><author><keyname>F&#xe9;raud</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>Network of Bandits</title><categories>cs.AI cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The distribution of the best arm identification task on the user's devices
offers several advantages for application purposes: scalability, reduction of
deployment costs and privacy. We propose a distributed version of the algorithm
Successive Elimination using a simple architecture based on a single server
which synchronizes each task executed on the user's devices. We show that this
algorithm is near optimal both in terms of transmitted number of bits and in
terms of number of pulls per player. Finally, we propose an extension of this
approach to distribute the contextual bandit algorithm Bandit Forest, which is
able to finely exploit the user's data while guaranteeing the privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03780</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03780</id><created>2016-02-11</created><updated>2016-02-15</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Teng</keyname><forenames>Shang-Hua</forenames></author></authors><title>Interplay between Social Influence and Network Centrality: Shapley
  Values and Scalable Algorithms</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic concept in network analysis is centrality, which measures the
importance of nodes in a network. In this research, we address the following
fundamental question: &quot;Given a social network, what is the impact of the social
influence models on network centrality?&quot;
  Social influence is commonly formulated as a stochastic process, which
defines how each group of nodes can collectively influence other nodes in an
underlying graph. This process defines a natural cooperative game, in which
each group's utility is its influence spread. Thus, fundamental
game-theoretical concepts of this social-influence game can be instrumental in
understanding network influence.
  We present a comprehensive analysis of the effectiveness of the
game-theoretical approach to capture the impact of influence models on
centrality. In this paper, we focus on the Shapley value of the above
social-influence game. Algorithmically, we give a scalable algorithm for
approximating the Shapley values of a large family of social-influence
instances. Mathematically, we present an axiomatic characterization which
captures the essence of using the Shapley value as the centrality measure to
incorporate the impact of social-influence processes. We establish the
soundness and completeness of our representation theorem by proving that the
Shapley value of this social-influence game is the unique solution to a set of
natural axioms for desirable centrality measures to characterize this
interplay. The dual axiomatic-and-algorithmic characterization provides a
comparative framework for evaluating different centrality formulations of
influence models. Empirically, through a number of real-world social networks
--- both small and large --- we demonstrate the important features of the
Shapley centrality as well as the efficiency of our scalable algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03796</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03796</id><created>2016-02-11</created><authors><author><keyname>Calafiore</keyname><forenames>Giuseppe C.</forenames></author></authors><title>On Repetitive Scenario Design</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repetitive Scenario Design (RSD) is a randomized approach to robust design
based on iterating two phases: a standard scenario design phase that uses $N$
scenarios (design samples), followed by randomized feasibility phase that uses
$N_o$ test samples on the scenario solution. We give a full and exact
probabilistic characterization of the number of iterations required by the RSD
approach for returning a solution, as a function of $N$, $N_o$, and of the
desired levels of probabilistic robustness in the solution. This novel approach
broadens the applicability of the scenario technology, since the user is now
presented with a clear tradeoff between the number $N$ of design samples and
the ensuing expected number of repetitions required by the RSD algorithm. The
plain (one-shot) scenario design becomes just one of the possibilities, sitting
at one extreme of the tradeoff curve, in which one insists in finding a
solution in a single repetition: this comes at the cost of possibly high $N$.
Other possibilities along the tradeoff curve use lower $N$ values, but possibly
require more than one repetition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03805</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03805</id><created>2016-02-11</created><authors><author><keyname>Kim</keyname><forenames>Kwang In</forenames></author><author><keyname>Tompkin</keyname><forenames>James</forenames></author><author><keyname>Pfister</keyname><forenames>Hanspeter</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Local High-order Regularization on Data Manifolds</title><categories>cs.CV</categories><comments>Accepted version of paper published at CVPR 2015,
  http://dx.doi.org/10.1109/CVPR.2015.7299186</comments><doi>10.1109/CVPR.2015.7299186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The common graph Laplacian regularizer is well-established in semi-supervised
learning and spectral dimensionality reduction. However, as a first-order
regularizer, it can lead to degenerate functions in high-dimensional manifolds.
The iterated graph Laplacian enables high-order regularization, but it has a
high computational complexity and so cannot be applied to large problems. We
introduce a new regularizer which is globally high order and so does not suffer
from the degeneracy of the graph Laplacian regularizer, but is also sparse for
efficient computation in semi-supervised learning applications. We reduce
computational complexity by building a local first-order approximation of the
manifold as a surrogate geometry, and construct our high-order regularizer
based on local derivative evaluations therein. Experiments on human body shape
and pose analysis demonstrate the effectiveness and efficiency of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03808</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03808</id><created>2016-02-11</created><authors><author><keyname>Kim</keyname><forenames>Kwang In</forenames></author><author><keyname>Tompkin</keyname><forenames>James</forenames></author><author><keyname>Pfister</keyname><forenames>Hanspeter</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Semi-supervised Learning with Explicit Relationship Regularization</title><categories>cs.CV cs.LG</categories><comments>Accepted version of paper published at CVPR 2015,
  http://dx.doi.org/10.1109/CVPR.2015.7298831</comments><doi>10.1109/CVPR.2015.7298831</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many learning tasks, the structure of the target space of a function holds
rich information about the relationships between evaluations of functions on
different data points. Existing approaches attempt to exploit this relationship
information implicitly by enforcing smoothness on function evaluations only.
However, what happens if we explicitly regularize the relationships between
function evaluations? Inspired by homophily, we regularize based on a smooth
relationship function, either defined from the data or with labels. In
experiments, we demonstrate that this significantly improves the performance of
state-of-the-art algorithms in semi-supervised classification and in spectral
data embedding for constrained clustering and dimensionality reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03814</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03814</id><created>2016-02-11</created><authors><author><keyname>Sarathy</keyname><forenames>Vasanth</forenames></author><author><keyname>Wilson</keyname><forenames>Jason R.</forenames></author><author><keyname>Arnold</keyname><forenames>Thomas</forenames></author><author><keyname>Scheutz</keyname><forenames>Matthias</forenames></author></authors><title>Enabling Basic Normative HRI in a Cognitive Robotic Architecture</title><categories>cs.RO cs.AI cs.HC</categories><comments>Presented at &quot;2nd Workshop on Cognitive Architectures for Social
  Human-Robot Interaction 2016 (arXiv:1602.01868)&quot;</comments><report-no>CogArch4sHRI/2016/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative human activities are grounded in social and moral norms, which
humans consciously and subconsciously use to guide and constrain their
decision-making and behavior, thereby strengthening their interactions and
preventing emotional and physical harm. This type of norm-based processing is
also critical for robots in many human-robot interaction scenarios (e.g., when
helping elderly and disabled persons in assisted living facilities, or
assisting humans in assembly tasks in factories or even the space station). In
this position paper, we will briefly describe how several components in an
integrated cognitive architecture can be used to implement processes that are
required for normative human-robot interactions, especially in collaborative
tasks where actions and situations could potentially be perceived as
threatening and thus need a change in course of action to mitigate the
perceived threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03819</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03819</id><created>2016-02-11</created><updated>2016-03-02</updated><authors><author><keyname>Deutch</keyname><forenames>Daniel</forenames></author><author><keyname>Gilad</keyname><forenames>Amir</forenames></author></authors><title>Learning Queries from Examples and Their Explanations</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To assist non-specialists in formulating database queries, multiple
frameworks that automatically infer queries from a set of examples have been
proposed. While highly useful, a shortcoming of the approach is that if users
can only provide a small set of examples, many inherently different queries may
qualify, and only some of these actually match the user intentions. Our main
observation is that if users further explain their examples, the set of
qualifying queries may be significantly more focused. We develop a novel
framework where users explain example tuples by choosing input tuples that are
intuitively the &quot;cause&quot; for their examples. Their explanations are
automatically &quot;compiled&quot; into a formal model for explanations, based on
previously developed models of data provenance. Then, our novel algorithms
infer conjunctive queries from the examples and their explanations. We prove
the computational efficiency of the algorithms and favorable properties of
inferred queries. We have further implemented our solution in a system
prototype with an interface that assists users in formulating explanations in
an intuitive way. Our experimental results, including a user study as well as
experiments using the TPC-H benchmark, indicate the effectiveness of our
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03822</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03822</id><created>2016-02-11</created><updated>2016-02-24</updated><authors><author><keyname>Murphy</keyname><forenames>Robert A.</forenames></author></authors><title>Neural Network Support Vector Detection via a Soft-Label, Hybrid K-Means
  Classifier</title><categories>cs.LG</categories><comments>This paper is a combined replacement for the papers &quot;A Neural Network
  Anomaly Detector using the Random Cluster Model&quot; (arXiv:1501.07227), &quot;On the
  Sharp Threshold Interval Length of Partially Connected Random Geometric
  Graphs During K-Means Classification&quot; (arXiv:1412.4178) and &quot;Estimating the
  Mean Number of K-Means Clusters to Form&quot; (arXiv:1503.03488), which have all
  been withdrawn</comments><msc-class>60D05, 62C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use random geometric graphs to describe clusters of higher dimensional
data points which are bijectively mapped to a (possibly) lower dimensional
space where an equivalent random cluster model is used to calculate the
expected number of modes to be found when separating the data of a multi-modal
data set into distinct clusters. Furthermore, as a function of the expected
number of modes and the number of data points in the sample, an upper bound on
a given distance measure is found such that data points have the greatest
correlation if their mutual distances from a common center is less than or
equal to the calculated bound. Anomalies are exposed, which lie outside of the
union of all regularized clusters of data points.
  Similar to finding a hyperplane which can be shifted along its normal to
expose the maximal distance between binary classes, it is shown that the union
of regularized clusters can be used to define a hyperplane which can be shifted
by a certain amount to separate the data into binary classes and that the
shifted hyperplane defines the activation function for a two-class
discriminating neural network. Lastly, this neural network is used to detect
the set of support vectors which determines the maximally-separating region
between the binary classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03828</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03828</id><created>2016-02-11</created><updated>2016-02-15</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Kamath</keyname><forenames>Govinda</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Community Recovery in Graphs with Locality</title><categories>cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH</categories><comments>Preliminary versions; the proofs of all theorems are deferred to the
  supplemental materials
  (http://www.stanford.edu/~yxchen/publications/locality_supp.pdf)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in domains such as social networks and
computational biology, we study the problem of community recovery in graphs
with locality. In this problem, pairwise noisy measurements of whether two
nodes are in the same community or different communities come mainly or
exclusively from nearby nodes rather than uniformly sampled between all nodes
pairs, as in most existing models. We present an algorithm that runs nearly
linearly in the number of measurements and which achieves the information
theoretic limit for exact recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03834</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03834</id><created>2016-02-11</created><authors><author><keyname>Haunschild</keyname><forenames>Robin</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Marx</keyname><forenames>Werner</forenames></author></authors><title>Climate Change Research in View of Bibliometrics</title><categories>cs.DL</categories><comments>40 pages, 6 figures, and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This bibliometric study of a large publication set dealing with research on
climate change aims at mapping the relevant literature from a bibliometric
perspective and presents a multitude of quantitative data: (1) The growth of
the overall publication output as well as (2) of some major subfields, (3) the
contributing journals and countries as well as their citation impact, and (4) a
title word analysis aiming to illustrate the time evolution and relative
importance of specific research topics. The study is based on 222,060 papers
published between 1980 and 2014. The total number of papers shows a strong
increase with a doubling every 5-6 years. Continental biomass related research
is the major subfield, closely followed by climate modeling. Research dealing
with adaptation, mitigation, risks, and vulnerability of global warming is
comparatively small, but their share of papers increased exponentially since
2005. Research on vulnerability and on adaptation published the largest
proportion of very important papers. Research on climate change is
quantitatively dominated by the USA, followed by the UK, Germany, and Canada.
The citation-based indicators exhibit consistently that the UK has produced the
largest proportion of high impact papers compared to the other countries
(having published more than 10,000 papers). The title word analysis shows that
the term climate change comes forward with time. Furthermore, the term impact
arises and points to research dealing with the various effects of climate
change. Finally, the term model and related terms prominently appear
independent of time, indicating the high relevance of climate modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03854</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03854</id><created>2016-01-25</created><authors><author><keyname>Dindarloo</keyname><forenames>Saeid R.</forenames></author><author><keyname>Siami-Irdemoosa</keyname><forenames>Elnaz</forenames></author></authors><title>Estimating the unconfined compressive strength of carbonate rocks using
  gene expression programming</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conventionally, many researchers have used both regression and black box
techniques to estimate the unconfined compressive strength (UCS) of different
rocks. The advantage of the regression approach is that it can be used to
render a functional relationship between the predictive rock indices and its
UCS. The advantage of the black box techniques is in rendering more accurate
predictions. Gene expression programming (GEP) is proposed, in this study, as a
robust mathematical alternative for predicting the UCS of carbonate rocks. The
two parameters of total porosity and P-wave speed were selected as predictive
indices. The proposed GEP model had the advantage of the both traditionally
used approaches by proposing a mathematical model, similar to a regression,
while keeping the prediction errors as low as the black box methods. The GEP
outperformed both artificial neural networks and support vector machines in
terms of yielding more accurate estimates of UCS. Both the porosity and the
P-wave velocity were sufficient predictive indices for estimating the UCS of
the carbonate rocks in this study. Nearly, 95% of the observed variation in the
UCS values was explained by these two parameters (i.e., R2 =95%).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03860</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03860</id><created>2016-02-11</created><authors><author><keyname>Sridhar</keyname><forenames>Srinath</forenames></author><author><keyname>Rhodin</keyname><forenames>Helge</forenames></author><author><keyname>Seidel</keyname><forenames>Hans-Peter</forenames></author><author><keyname>Oulasvirta</keyname><forenames>Antti</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model</title><categories>cs.CV</categories><comments>8 pages, Accepted version of paper published at 3DV 2014</comments><journal-ref>2nd International Conference on , vol.1, no., pp.319-326, 8-11
  Dec. 2014</journal-ref><doi>10.1109/3DV.2014.37</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time marker-less hand tracking is of increasing importance in
human-computer interaction. Robust and accurate tracking of arbitrary hand
motion is a challenging problem due to the many degrees of freedom, frequent
self-occlusions, fast motions, and uniform skin color. In this paper, we
propose a new approach that tracks the full skeleton motion of the hand from
multiple RGB cameras in real-time. The main contributions include a new
generative tracking method which employs an implicit hand shape representation
based on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that is
smooth and analytically differentiable making fast gradient based pose
optimization possible. This shape representation, together with a full
perspective projection model, enables more accurate hand modeling than a
related baseline method from literature. Our method achieves better accuracy
than previous methods and runs at 25 fps. We show these improvements both
qualitatively and quantitatively on publicly available datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03865</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03865</id><created>2016-02-11</created><authors><author><keyname>Danciger</keyname><forenames>Jeffrey</forenames></author><author><keyname>Maloni</keyname><forenames>Sara</forenames></author><author><keyname>Schlenker</keyname><forenames>Jean-Marc</forenames></author></authors><title>Higher signature Delaunay decompositions</title><categories>cs.CG cs.DM math.DG</categories><comments>25 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Delaunay decomposition is a cell decomposition in R^d for which each cell
is inscribed in a Euclidean ball which is empty of all other vertices. This
article introduces a generalization of the Delaunay decomposition in which the
Euclidean balls in the empty ball condition are replaced by other families of
regions bounded by certain quadratic hypersurfaces. This generalized notion is
adaptable to geometric contexts in which the natural space from which the point
set is sampled is not Euclidean, but rather some other flat semi-Riemannian
geometry, possibly with degenerate directions. We prove the existence and
uniqueness of the decomposition and discuss some of its basic properties. In
the case of dimension d = 2, we study the extent to which some of the
well-known optimality properties of the Euclidean Delaunay triangulation
generalize to the higher signature setting. In particular, we describe a higher
signature generalization of a well-known description of Delaunay decompositions
in terms of the intersection angles between the circumscribed circles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03881</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03881</id><created>2016-02-11</created><authors><author><keyname>Blelloch</keyname><forenames>Guy E.</forenames></author><author><keyname>Gu</keyname><forenames>Yan</forenames></author><author><keyname>Sun</keyname><forenames>Yihan</forenames></author><author><keyname>Tangwongsan</keyname><forenames>Kanat</forenames></author></authors><title>Parallel Shortest-Paths Using Radius Stepping</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The single-source shortest path problem (SSSP) with nonnegative edge weights
is a notoriously difficult problem to solve efficiently in parallel---it is one
of the graph problems said to suffer from the transitive-closure bottleneck. In
practice, the $\Delta$-stepping algorithm of Meyer and Sanders (J. Algorithms,
2003) often works efficiently but has no known theoretical bounds on general
graphs. The algorithm takes a sequence of steps, each increasing the radius by
a user-specified value $\Delta$. Each step settles the vertices in its annulus
but can take $\Theta(n)$ substeps, each requiring $\Theta(m)$ work ($n$
vertices and $m$ edges).
  In this paper, we describe Radius-Stepping, an algorithm with the best-known
tradeoff between work and depth bounds for SSSP with nearly-linear
($\otilde(m)$) work. The algorithm is a $\Delta$-stepping-like algorithm but
uses a variable instead of fixed-size increase in radii, allowing us to prove a
bound on the number of steps. In particular, by using what we define as a
vertex $k$-radius, each step takes at most $k+2$ substeps. Furthermore, we
define a $(k, \rho)$-graph property and show that if an undirected graph has
this property, then the number of steps can be bounded by $O(\frac{n}{\rho}
\log \rho L)$, for a total of $O(\frac{kn}{\rho} \log \rho L)$ substeps, each
parallel. We describe how to preprocess a graph to have this property.
Altogether, Radius-Stepping takes $O((m+n\log n)\log \frac{n}{\rho})$ work and
$O(\frac{n}{\rho}\log n \log (\rho{}L))$ depth per source after preprocessing.
The preprocessing step can be done in $O(m\log n + n\rho^2)$ work and
$O(\rho^2)$ depth or in $O(m\log n + n\rho^2\log n)$ work and $O(\rho\log
\rho)$ depth, and adds no more than $O(n\rho)$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03885</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03885</id><created>2016-02-11</created><updated>2016-02-12</updated><authors><author><keyname>Vila&#xe7;a</keyname><forenames>Xavier</forenames></author><author><keyname>Rodrigues</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>Accountability in Dynamic Networks</title><categories>cs.GT</categories><comments>28 pages, 3 figures, 6 main theorems, 1 algorithm, to be submitted to
  a conference; rearranged one-shot-deviation principle; polished correctness
  of algorithms; removed undefined references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a game theoretical approach to determine necessary and sufficient
conditions under which we can persuade rational agents to exchange messages in
pairwise exchanges over links of a dynamic network, by holding them accountable
for deviations with punishments. We make three contributions: (1) we provide a
new game theoretical model of repeated interactions in dynamic networks, where
agents have incomplete information of the topology, (2) we define a new
solution concept for this model, and (3) we identify necessary and sufficient
conditions for enforcing accountability, i.e., for persuading agents to
exchange messages in the aforementioned model.
  Our results are of technical interest but also of practical relevance. We
show that we cannot enforce accountability if the dynamic network does not
allow for \emph{timely punishments}. In practice, this means for instance that
we cannot enforce accountability in some networks formed in file-sharing
applications such as Bittorrent\,\cite{Cohen:03}. We also show that for
applications such as secret exchange, where the benefits of the exchanges
significantly surpass the communication costs, timely punishments are enough to
enforce accountability. However, we cannot in general enforce accountability if
agents do not possess enough information about the network topology.
Nevertheless, we can enforce accountability in a wide variety of networks that
satisfy 1-connectivity\,\cite{Kuhn:10} with minimal knowledge about the network
topology, including overlays for gossip dissemination such as
\cite{Li:06,Li:08}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03886</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03886</id><created>2016-02-11</created><authors><author><keyname>Al-Maqri</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author><author><keyname>Ali</keyname><forenames>Borhanuddin Mohd</forenames></author><author><keyname>Hanapi</keyname><forenames>Zurina Mohd</forenames></author></authors><title>Providing Dynamic TXOP for QoS Support of Video Transmission in IEEE
  802.11e WLANs</title><categories>cs.NI cs.MM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1602.03699</comments><doi>10.4304/jnw.10.9.501-511</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 802.11e standard introduced by IEEE 802.11 Task Group E (TGe)
enhances the Quality of Service (QoS) by means of HCF Controlled Channel Access
(HCCA). The scheduler of HCCA allocates Transmission Opportunities (TXOPs) to
QoS-enabled Station (QSTA) based on their TS Specifications (TSPECs) negotiated
at the traffic setup time so that it is only efficient for Constant Bit Rate
(CBR) applications. However, Variable Bit Rate (VBR) traffics are not
efficiently supported as they exhibit nondeterministic profile during the time.
In this paper, we present a dynamic TXOP assignment Scheduling Algorithm for
supporting the video traffics transmission over IEEE 802.11e wireless networks.
This algorithm uses a piggybacked information about the size of the subsequent
video frames of the uplink traffic to assist the Hybrid Coordinator accurately
assign the TXOP according to the fast changes in the VBR profile. The proposed
scheduling algorithm has been evaluated using simulation with different
variability level video streams. The simulation results show that the proposed
algorithm reduces the delay experienced by VBR traffic streams comparable to
HCCA scheduler due to the accurate assignment of the TXOP which preserve the
channel time for transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03903</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03903</id><created>2016-02-11</created><authors><author><keyname>Feng</keyname><forenames>Siwei</forenames></author><author><keyname>Itoh</keyname><forenames>Yuki</forenames></author><author><keyname>Parente</keyname><forenames>Mario</forenames></author><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author></authors><title>Wavelet-Based Semantic Features for Hyperspectral Signature
  Discrimination</title><categories>cs.CV cs.LG</categories><comments>23 pages, 8 figures, preprint, first published online on April 28
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral signature classification is a quantitative analysis approach
for hyperspectral imagery which performs detection and classification of the
constituent materials at the pixel level in the scene. The classification
procedure can be operated directly on hyperspectral data or performed by using
some features extracted from the corresponding hyperspectral signatures
containing information like the signature's energy or shape. In this paper, we
describe a technique that applies non-homogeneous hidden Markov chain (NHMC)
models to hyperspectral signature classification. The basic idea is to use
statistical models (such as NHMC) to characterize wavelet coefficients which
capture the spectrum semantics (i.e., structural information) at multiple
levels. Experimental results show that the approach based on NHMC models can
outperform existing approaches relevant in classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03906</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03906</id><created>2016-02-11</created><authors><author><keyname>Giovanidis</keyname><forenames>Anastasios</forenames></author></authors><title>How to group wireless nodes together?</title><categories>cs.NI cs.IT math.IT</categories><comments>35 pages, 14 figures, 3 tables, Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a survey on how to group together in a static way planar
nodes, that may belong to a wireless network (ad hoc or cellular). The aim is
to identify appropriate methods that could also be applied for Point Processes.
Specifically matching pairs and algorithms are initially discussed. Next,
specifically for Point Processes, the Nearest Neighbour and Lilypond models are
presented. Properties and results for the two models are stated. Original
bounds are given for the value of the so-called generation number, which is
related to the size of the nearest neighbour cluster. Finally, a variation of
the nearest neighbour grouping is proposed and an original metric is
introduced, named here the ancestor number. This is used to facilitate the
analysis of the distribution of cluster size. Based on this certain related
bounds are derived. The report and the analysis included show clearly the
difficulty of working in point processes with static clusters of size greater
than two, when these are defined by proximity criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03924</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03924</id><created>2016-02-11</created><authors><author><keyname>Krafft</keyname><forenames>Peter M.</forenames></author><author><keyname>Baker</keyname><forenames>Chris L.</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Modeling Human Ad Hoc Coordination</title><categories>cs.AI cs.GT cs.MA</categories><comments>AAAI 2016</comments><acm-class>I.2.0; I.2.11; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whether in groups of humans or groups of computer agents, collaboration is
most effective between individuals who have the ability to coordinate on a
joint strategy for collective action. However, in general a rational actor will
only intend to coordinate if that actor believes the other group members have
the same intention. This circular dependence makes rational coordination
difficult in uncertain environments if communication between actors is
unreliable and no prior agreements have been made. An important normative
question with regard to coordination in these ad hoc settings is therefore how
one can come to believe that other actors will coordinate, and with regard to
systems involving humans, an important empirical question is how humans arrive
at these expectations. We introduce an exact algorithm for computing the
infinitely recursive hierarchy of graded beliefs required for rational
coordination in uncertain environments, and we introduce a novel mechanism for
multiagent coordination that uses it. Our algorithm is valid in any environment
with a finite state space, and extensions to certain countably infinite state
spaces are likely possible. We test our mechanism for multiagent coordination
as a model for human decisions in a simple coordination game using existing
experimental data. We then explore via simulations whether modeling humans in
this way may improve human-agent collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03926</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03926</id><created>2016-02-11</created><authors><author><keyname>Barahona</keyname><forenames>Igor</forenames></author><author><keyname>Cavazos</keyname><forenames>Judith</forenames></author><author><keyname>Yang</keyname><forenames>Jian-Bo</forenames></author></authors><title>Modelling the level of adoption of analytical tools; An implementation
  of multi-criteria evidential reasoning</title><categories>stat.AP cs.CY</categories><comments>Keywords: MCDA methods; evidential reasoning; analytical tools;
  multiple source data</comments><journal-ref>International Journal of Supply and Operations Management. (2014)
  Vol.1, Issue 2, pp 129-151</journal-ref><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In the future, competitive advantages will be given to organisations that can
extract valuable information from massive data and make better decisions. In
most cases, this data comes from multiple sources. Therefore, the challenge is
to aggregate them into a common framework in order to make them meaningful and
useful. This paper will first review the most important multi-criteria decision
analysis methods (MCDA) existing in current literature. We will offer a novel,
practical and consistent methodology based on a type of MCDA, to aggregate data
from two different sources into a common framework. Two datasets that are
different in nature but related to the same topic are aggregated to a common
scale by implementing a set of transformation rules. This allows us to generate
appropriate evidence for assessing and finally prioritising the level of
adoption of analytical tools in four types of companies. A numerical example is
provided to clarify the form for implementing this methodology. A six-step
process is offered as a guideline to assist engineers, researchers or
practitioners interested in replicating this methodology in any situation where
there is a need to aggregate and transform multiple source data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03929</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03929</id><created>2016-02-11</created><authors><author><keyname>Arachchilage</keyname><forenames>Nalin Asanka Gamagedara</forenames></author><author><keyname>Cole</keyname><forenames>Melissa</forenames></author></authors><title>Designing a Mobile Game for Home Computer Users to Protect Against
  Phishing Attacks</title><categories>cs.CY cs.CR</categories><comments>8 in International Journal for e-Learning Security (IJeLS), Volume 1,
  Issue 1/2, March/June 2011. arXiv admin note: substantial text overlap with
  arXiv:1511.07093</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research aims to design an educational mobile game for home computer
users to prevent from phishing attacks. Phishing is an online identity theft
which aims to steal sensitive information such as username, password and online
banking details from victims. To prevent this, phishing education needs to be
considered. Mobile games could facilitate to embed learning in a natural
environment. The paper introduces a mobile game design based on a story which
is simplifying and exaggerating real life. We use a theoretical model derived
from Technology Threat Avoidance Theory (TTAT) to address the game design
issues and game design principles were used as a set of guidelines for
structuring and presenting information. The overall mobile game design was
aimed to enhance avoidance behaviour through motivation of home computer users
to protect against phishing threats. The prototype game design is presented on
Google App Inventor Emulator. We believe by training home computer users to
protect against phishing attacks, would be an aid to enable the cyberspace as a
secure environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03930</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03930</id><created>2016-02-11</created><authors><author><keyname>Nekrasov</keyname><forenames>Vladimir</forenames></author><author><keyname>Ju</keyname><forenames>Janghoon</forenames></author><author><keyname>Choi</keyname><forenames>Jaesik</forenames></author></authors><title>Global Deconvolutional Networks for Semantic Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic image segmentation is an important low-level computer vision problem
aimed to correctly classify each individual pixel of the image. Recent
empirical improvements achieved in this area have primarily been motivated by
successful exploitation of Convolutional Neural Networks (CNNs) pre-trained for
image classification and object recognition tasks. However, the pixel-wise
labeling with CNNs has its own unique challenges: (1) an accurate
deconvolution, or upsampling, of low-resolution output into a higher-resolution
segmentation mask and (2) an inclusion of global information, or context,
within locally extracted features. To address these issues, we propose a novel
architecture to conduct the deconvolution operation and acquire dense
predictions, and an additional refinement, which allows to incorporate global
information into the network. We demonstrate that these alterations lead to
improved performance of state-of-the-art semantic segmentation models on the
PASCAL VOC 2012 benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03934</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03934</id><created>2016-02-11</created><authors><author><keyname>Barbay</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author></authors><title>Bouncing Towers move faster than {\Hanoi} Towers, but still require
  exponential time</title><categories>cs.GL</categories><comments>17 pages and many figures, one appendix with the disk pile problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of the Hanoi Tower is a classic exercise in recursive
programming: the solution has a simple recursive definition, and its complexity
and the matching lower bound are the solution of a simple recursive function
(the solution is so easy that most students memorize it and regurgitate it at
exams without truly understanding it). We describe how some very minor changes
in the rules of the Hanoi Tower yield various increases of complexity in the
solution, so that they require a deeper analysis than the classical Hanoi Tower
problem while still yielding exponential solutions. In particular, we analyze
the problem fo the Bouncing Tower, where just changing the insertion and
extraction position from the top to the middle of the tower results in a
surprising increase of complexity in the solution: such a tower of $n$ disks
can be optimally moved in $\sqrt{3}^n$ moves for $n$ even (i.e. less than a
Hanoi Tower of same height), via $5$ recursive functions (or, equivalently, one
recursion function with $5$ states).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03935</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03935</id><created>2016-02-11</created><authors><author><keyname>Zhong</keyname><forenames>Yang</forenames></author><author><keyname>Sullivan</keyname><forenames>Josephine</forenames></author><author><keyname>Li</keyname><forenames>Haibo</forenames></author></authors><title>Face Attribute Prediction Using Off-The-Shelf Deep Learning Networks</title><categories>cs.CV</categories><comments>in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute prediction from face images in the wild is a challenging problem.
To automatically describe face attributes from face containing images,
traditionally one needs to cascade three technical blocks --- face
localization, facial feature extraction, and classification --- in a pipeline.
As a typical classification problem, face attribute prediction has been
addressed by using deep learning networks. Current state-of-the-art performance
was achieved by using two cascaded CNNs, which were specifically trained to
learn face localization and facial attribute prediction. In this paper we
experiment in an alternative way of exploring the power of deep representations
from the networks: we employ off-the-shelf CNNs trained for face recognition
tasks to do facial feature extraction, combined with conventional face
localization techniques. Recognizing that the describable face attributes are
diverse, we select representations from different levels of the CNNs and
investigate their utilities for attribute classification. Experiments on two
large datasets, LFWA and CeleA, show that the performance is totally comparable
to state-of-the-art approach. Our findings suggest two potentially important
questions in using CNNs for face attribute prediction: 1) How to maximally
leverage the power of CNN representations. 2) How to best combine traditional
computer vision techniques with deep learning networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03936</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03936</id><created>2016-02-11</created><authors><author><keyname>Gu</keyname><forenames>J.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Interference Cancellation and Relay Selection Algorithms Using
  Greedy Techniques for Cooperative DS-CDMA Systems</title><categories>cs.IT math.IT</categories><comments>6 figures in Eurasip Journal on Wireless Communications and
  Networking, 2016. arXiv admin note: text overlap with arXiv:1410.0444,
  arXiv:1406.0234</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study interference cancellation techniques and a multi-relay
selection algorithm based on greedy methods for the uplink of cooperative
direct-sequence code-division multiple access (DS-CDMA) systems. We first
devise low-cost list-based successive interference cancellation (GL-SIC) and
parallel interference cancellation (GL-PIC) algorithms with RAKE receivers as
the front-end that can approach the maximum likelihood detector performance and
be used at both the relays and the destination of cooperative systems. Unlike
prior art, the proposed GL-SIC and GL-PIC algorithms exploit the Euclidean
distance between users of interest and the potential nearest constellation
point with a chosen threshold in order to build an effective list of detection
candidates. A low-complexity multi-relay selection algorithm based on greedy
techniques that can approach the performance of an exhaustive search is also
proposed. A cross-layer design strategy that brings together the proposed
multiuser detection algorithms and the greedy relay selection is then developed
along with an analysis of the proposed techniques. Simulations show an
excellent bit error rate performance of the proposed detection and relay
selection algorithms as compared to existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03942</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03942</id><created>2016-02-11</created><authors><author><keyname>Yamamoto</keyname><forenames>Koji</forenames></author><author><keyname>Matsutsuka</keyname><forenames>Taka</forenames></author></authors><title>Efficient Call Path Detection for Android-OS Size of Huge Source Code</title><categories>cs.DS</categories><comments>in Sixth International Conference on Computer Science, Engineering
  and Applications (CCSEA 2016), Dubai, UAE, January 23~24, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today most developers utilize source code written by other parties. Because
the code is modified frequently, the developers need to grasp the impact of the
modification repeatedly. A call graph and especially its special type, a call
path, help the developers comprehend the modification. Source code written by
other parties, however, becomes too huge to be held in memory in the form of
parsed data for a call graph or path. This paper offers a bidirectional search
algorithm for a call graph of too huge amount of source code to store all parse
results of the code in memory. It refers to a method definition in source code
corresponding to the visited node in the call graph. The significant feature of
the algorithm is the referenced information is used not in order to select a
prioritized node to visit next but in order to select a node to postpone
visiting. It reduces path extraction time by 8% for a case in which ordinary
path search algorithms do not reduce the time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03943</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03943</id><created>2016-02-11</created><updated>2016-02-26</updated><authors><author><keyname>Agarwal</keyname><forenames>Naman</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Second Order Stochastic Optimization in Linear Time</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic optimization and, in particular, first-order stochastic methods
are a cornerstone of modern machine learning due to their extremely efficient
per-iteration computational cost. Second-order methods, while able to provide
faster per-iteration convergence, have been much less explored due to the high
cost of computing the second-order information. In this paper we develop a
second-order stochastic method for optimization problems arising in machine
learning based on novel matrix randomization techniques that match the
per-iteration cost of gradient descent, yet enjoy the linear-convergence
properties of second-order optimization. We also consider the special case of
self-concordant functions where we show that a first order method can achieve
linear convergence with guarantees independent of the condition number. We
demonstrate significant speedups for training linear classifiers over several
convex benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03945</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03945</id><created>2016-02-11</created><authors><author><keyname>Ristic</keyname><forenames>Branko</forenames></author><author><keyname>Beard</keyname><forenames>Michael</forenames></author><author><keyname>Fantacci</keyname><forenames>Claudio</forenames></author></authors><title>An Overview of Particle Methods for Random Finite Set Models</title><categories>cs.SY</categories><comments>50 pages including 6 figures in Information Fusion, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This overview paper describes the particle methods developed for the
implementation of the a class of Bayes filters formulated using the random
finite set formalism. It is primarily intended for the readership already
familiar with the particle methods in the context of the standard Bayes filter.
The focus in on the Bernoulli particle filter, the probability hypothesis
density (PHD) particle filter and the generalised labelled multi-Bernoulli
(GLMB) particle filter. The performance of the described filters is
demonstrated in the context of bearings-only target tracking application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03950</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03950</id><created>2016-02-11</created><authors><author><keyname>Zhao</keyname><forenames>Hong</forenames></author></authors><title>General Vector Machine</title><categories>stat.ML cs.LG</categories><comments>57pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The support vector machine (SVM) is an important class of learning machines
for function approach, pattern recognition, and time-serious prediction, etc.
It maps samples into the feature space by so-called support vectors of selected
samples, and then feature vectors are separated by maximum margin hyperplane.
The present paper presents the general vector machine (GVM) to replace the SVM.
The support vectors are replaced by general project vectors selected from the
usual vector space, and a Monte Carlo (MC) algorithm is developed to find the
general vectors. The general project vectors improves the feature-extraction
ability, and the MC algorithm can control the width of the separation margin of
the hyperplane. By controlling the separation margin, we show that the maximum
margin hyperplane can usually induce the overlearning, and the best learning
machine is achieved with a proper separation margin. Applications in function
approach, pattern recognition, and classification indicate that the developed
method is very successful, particularly for small-set training problems.
Additionally, our algorithm may induce some particular applications, such as
for the transductive inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03954</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03954</id><created>2016-02-11</created><authors><author><keyname>Yang</keyname><forenames>Heecheol</forenames></author><author><keyname>Shin</keyname><forenames>Wonjae</forenames></author><author><keyname>Lee</keyname><forenames>Jungwoo</forenames></author></authors><title>Linear Degrees of Freedom for $K $-user MISO Interference Channels with
  Blind Interference Alignment</title><categories>cs.IT math.IT</categories><comments>25 pages, 6 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the degrees of freedom (DoF) for $K $-user $M
\times 1 $ multiple-input single-output interference channels with
reconfigurable antennas which have multiple preset modes at the receivers,
assuming linear coding strategies in the absence of channel state information
at the transmitters, i.e., blind interference alignment. Our linear DoF
converse builds on the lemma that if a set of transmit symbols is aligned at
their common unintended receivers, those symbols must have independent signal
subspace at their corresponding receivers. This lemma arises from the inherent
feature that channel state's changing patterns of the links towards the same
receiver are always identical, assuming that the coherence time of the channel
is long enough. We derive an upper bound for the linear sum DoF, and propose an
achievable scheme that exactly achieves the linear sum DoF upper-bound when
both of the $\frac{n^{*}}{M}=R_{1} $ and $\frac{MK}{n^{*}}=R_{2} $ are
integers. For the other cases, where either $R_1 $ or $R_2 $ is not an integer,
we only give some guidelines how the interfering signals are aligned at the
receivers to achieve the upper-bound. As an extension, we also show the linear
sum DoF upper-bound for downlink/uplink cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03956</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03956</id><created>2016-02-11</created><authors><author><keyname>Farinha</keyname><forenames>Daniel Filipe G.</forenames></author></authors><title>Grokya: a Privacy-Friendly Framework for Ubiquitous Computing</title><categories>cs.CY cs.HC</categories><comments>Master thesis</comments><doi>10.13140/RG.2.1.2452.3281</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a world where for-profit enterprises are increasingly looking to maximize
profits by engaging in privacy invading consumer-profiling techniques, the rise
of ubiquitous computing and the Internet of Things (IoT) poses a major problem.
If not acted upon quickly, the combination of Big Data with IoT will explode
into a dystopian world that even George Orwell could not have predicted. The
proposed project aims to fill a gap that no other solution is addressing, which
is to reach a win-win scenario that works for both the enterprises and the
consumers. It aims to do this by creating the building blocks for a consumer
owned infrastructure that can provide both privacy for the user, and still
enable the enterprises to achieve their high-level goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03960</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03960</id><created>2016-02-11</created><authors><author><keyname>Jauhar</keyname><forenames>Sujay Kumar</forenames></author><author><keyname>Turney</keyname><forenames>Peter</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author></authors><title>TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice
  Questions</title><categories>cs.CL</categories><comments>Keywords: Data, General Knowledge, Tables, Question Answering, MCQ,
  Crowd-sourcing, Mechanical Turk</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two new related resources that facilitate modelling of general
knowledge reasoning in 4th grade science exams. The first is a collection of
curated facts in the form of tables, and the second is a large set of
crowd-sourced multiple-choice questions covering the facts in the tables.
Through the setup of the crowd-sourced annotation task we obtain implicit
alignment information between questions and tables. We envisage that the
resources will be useful not only to researchers working on question answering,
but also to people investigating a diverse range of other applications such as
information extraction, question parsing, answer type identification, and
lexical semantic modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03963</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03963</id><created>2016-02-12</created><authors><author><keyname>Xu</keyname><forenames>Easton Li</forenames></author><author><keyname>Qian</keyname><forenames>Xiaoning</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Detection of Cooperative Interactions in Logistic Regression Models</title><categories>cs.AI</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem in the field of bioinformatics is to identify
interactive effects among profiled variables for outcome prediction. In this
paper, a simple logistic regression model with pairwise interactions among a
set of binary covariates is considered. Modeling the structure of the
interactions by a graph, our goal is to recover the interaction graph from
independently identically distributed (i.i.d.) samples of the covariates and
the outcome.
  When viewed as a feature selection problem, a simple quantity called
influence is proposed as a measure of the marginal effects of the interaction
terms on the outcome. For the case when the underlying interaction graph is
known to be acyclic, it is shown that a simple algorithm that is based on a
maximum-weight spanning tree with respect to the plug-in estimates of the
influences not only has strong theoretical performance guarantees, but can also
significantly outperform generic feature selection algorithms for recovering
the interaction graph from i.i.d. samples of the covariates and the outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03966</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03966</id><created>2016-02-12</created><authors><author><keyname>Zhao</keyname><forenames>Pengpeng</forenames></author><author><keyname>Li</keyname><forenames>Yongkun</forenames></author><author><keyname>Xie</keyname><forenames>Hong</forenames></author><author><keyname>Wu</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xu</keyname><forenames>Yinlong</forenames></author><author><keyname>Ma</keyname><forenames>Richard T. B.</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Impact of Online Activities on Influence Maximization: A Random Walk
  Approach</title><categories>cs.SI</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the popularity of OSNs, finding a set of most influential users so as to
trigger the largest influence cascade is of significance. This task is usually
modeled as an influence maximization problem, and it has been widely studied in
the past decade. However, considering that users in OSNs may participate in
various kinds of online activities, e.g., giving ratings to products, joining
discussion groups, etc., influence diffusion through online activities becomes
even more significant. Thus, it necessitates to revisit the influence
maximization problem by taking online activities into consideration.
  In this paper, we study the impact of online activities by formulating the
influence maximization problem for social-activity networks (SANs) containing
both users and online activities. To address the computation challenge, we
define an influence centrality via random walks on hypergraph to measure
influence, then use the Monte Carlo framework to efficiently estimate the
centrality in SANs, and use the Hoeffding inequality to bound the approximation
error. Furthermore, we develop a greedy-based algorithm with two novel
optimization techniques to find the most influential users. By conducting
extensive experiments with real-world datasets, we show that compared to the
state-of-the-art algorithm IMM [20], our approach improves the accuracy of
selecting the top k most influential users in SANs by up to an order of
magnitude, and our approach has a comparable computation time even though it
needs to handle a large amount of online activities. Furthermore, our approach
is also general enough to work under different diffusion models, e.g., IC and
LT models, as well as heterogeneous systems, e.g., multiple types of users and
online activities in SANs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03969</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03969</id><created>2016-02-12</created><authors><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Opportunistic Detection Rules: Finite and Asymptotic Analysis</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic detection rules (ODRs) are variants of fixed-sample-size
detection rules in which the statistician is allowed to make an early decision
on the alternative hypothesis opportunistically based on the sequentially
observed samples. From a sequential decision perspective, ODRs are also
mixtures of one-sided and truncated sequential detection rules. Several results
regarding ODRs are established in this paper. In the finite regime, the maximum
sample size is modeled either as a fixed finite number, or a geometric random
variable with a fixed finite mean. For both cases, the corresponding Bayesian
formulations are investigated. The former case is a slight variation of the
well-known finite-length sequential hypothesis testing procedure in the
literature, whereas the latter case is new, for which the Bayesian optimal ODR
is shown to be a sequence of likelihood ratio threshold tests with two
different thresholds: a running threshold, which is determined by solving a
stationary state equation, is used when future samples are still available, and
a terminal threshold (simply the ratio between the priors scaled by costs) is
used when the statistician reaches the final sample and thus has to make a
decision immediately. In the asymptotic regime, the tradeoff among the
exponents of the (false alarm and miss) error probabilities and the normalized
expected stopping time under the alternative hypothesis is completely
characterized and proved to be tight, via an information-theoretic argument.
Within the tradeoff region, one noteworthy fact is that the performance of the
Stein-Chernoff Lemma is attainable by ODRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03979</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03979</id><created>2016-02-12</created><authors><author><keyname>Deng</keyname><forenames>Shi-Wen</forenames></author><author><keyname>Han</keyname><forenames>Ji-Qing</forenames></author></authors><title>Signal periodic decomposition with conjugate subspaces</title><categories>cs.CE</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on hidden period identification and the periodic
decomposition of signals. Based on recent results on the Ramanujan subspace, we
reveal the conjugate symmetry of the Ramanujan subspace with a set of complex
exponential basis functions and represent the subspace as the union of a series
of conjugate subspaces. With these conjugate subspaces, the signal periodic
model is introduced to characterize the periodic structure of a signal. To
achieve the decomposition of the proposed model, the conjugate subspace
matching pursuit (CSMP) algorithm is proposed based on two different greedy
strategies. The CSMP is performed iteratively in two stages. In the first
stage, the dominant hidden period is chosen with the periodicity strategy.
Then, the dominant conjugate subspace is chosen with the energy strategy in the
second stage. Compared with the current state-of-the-art methods for hidden
period identification, the main advantages provided by the CSMP are the
following: (i) the capability of identifying all the hidden periods in the
range from $1$ to the maximum hidden period $Q$ of a signal of any length,
without truncating the signal; (ii) the ability to identify the time-varying
hidden period with its shifted version; and (iii) the low computational cost,
without generating and using a large over-complete dictionary. Moreover, we
provide examples and applications to demonstrate the abilities of the proposed
two-stage CSMP algorithm, which include hidden period identification, signal
approximation, time-varying period detection, and pitch detection of speech.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03985</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03985</id><created>2016-02-12</created><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author></authors><title>A complexity trichotomy for approximately counting list H-colourings</title><categories>cs.CC</categories><msc-class>68Q17, 05C15, 05C75, 68Q25</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the computational complexity of approximately counting the list
H-colourings of a graph. We discover a natural graph-theoretic trichotomy based
on the structure of the graph H. If H is an irreflexive bipartite graph or a
reflexive complete graph then counting list H-colourings is trivially in
polynomial time. Otherwise, if H is an irreflexive bipartite permutation graph
or a reflexive proper interval graph then approximately counting list
H-colourings is equivalent to #BIS, the problem of approximately counting
independent sets in a bipartite graph. This is a well-studied problem which is
believed to be of intermediate complexity -- it is believed that it does not
have an FPRAS, but that it is not as difficult as approximating the most
difficult counting problems in #P. For every other graph H, approximately
counting list H-colourings is complete for #P with respect to
approximation-preserving reductions (so there is no FPRAS unless NP=RP). Two
pleasing features of list H-colourings, from the perspective of approximate
counting complexity are (i) the trichotomy has a natural formulation in terms
of hereditary graph classes, and (ii) the proof is largely self-contained and
does not require any universal algebra (unlike similar dichotomies in the
weighted case).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03992</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03992</id><created>2016-02-12</created><authors><author><keyname>Benidis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Babu</keyname><forenames>Prabhu</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Orthogonal Sparse PCA and Covariance Estimation via Procrustes
  Reformulation</title><categories>stat.ML cs.LG math.OC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating sparse eigenvectors of a symmetric matrix attracts
a lot of attention in many applications, especially those with high dimensional
data set. While classical eigenvectors can be obtained as the solution of a
maximization problem, existing approaches formulate this problem by adding a
penalty term into the objective function that encourages a sparse solution.
However, the resulting methods achieve sparsity at the expense of sacrificing
the orthogonality property. In this paper, we develop a new method to estimate
dominant sparse eigenvectors without trading off their orthogonality. The
problem is highly non-convex and hard to handle. We apply the MM framework
where we iteratively maximize a tight lower bound (surrogate function) of the
objective function over the Stiefel manifold. The inner maximization problem
turns out to be a rectangular Procrustes problem, which has a closed form
solution. In addition, we propose a method to improve the covariance estimation
problem when its underlying eigenvectors are known to be sparse. We use the
eigenvalue decomposition of the covariance matrix to formulate an optimization
problem where we impose sparsity on the corresponding eigenvectors. Numerical
experiments show that the proposed eigenvector extraction algorithm matches or
outperforms existing algorithms in terms of support recovery and explained
variance, while the covariance estimation algorithms improve significantly the
sample covariance estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.03995</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.03995</id><created>2016-02-12</created><authors><author><keyname>de Siqueira</keyname><forenames>Alexandre Fioravante</forenames></author><author><keyname>Nakasuga</keyname><forenames>Wagner Massayuki</forenames></author><author><keyname>Pagamisse</keyname><forenames>Aylton</forenames></author><author><keyname>Saenz</keyname><forenames>Carlos Alberto Tello</forenames></author><author><keyname>Job</keyname><forenames>Aldo Eloizo</forenames></author></authors><title>An automatic method for segmentation of fission tracks in epidote
  crystal photomicrographs</title><categories>cs.CV</categories><comments>16 pages, 5 figures</comments><msc-class>65T60</msc-class><acm-class>G.1.2; I.4.0; I.4.6; I.5.4</acm-class><journal-ref>Computers &amp; Geosciences, v. 69, pp. 55-61, aug 2014</journal-ref><doi>10.1016/j.cageo.2014.04.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manual identification of fission tracks has practical problems, such as
variation due to observer-observation efficiency. An automatic processing
method that could identify fission tracks in a photomicrograph could solve this
problem and improve the speed of track counting. However, separation of
non-trivial images is one of the most difficult tasks in image processing.
Several commercial and free softwares are available, but these softwares are
meant to be used in specific images. In this paper, an automatic method based
on starlet wavelets is presented in order to separate fission tracks in mineral
photomicrographs. Automatization is obtained by Matthews correlation
coefficient, and results are evaluated by precision, recall and accuracy. This
technique is an improvement of a method aimed at segmentation of scanning
electron microscopy images. This method is applied in photomicrographs of
epidote phenocrystals, in which accuracy higher than 89% was obtained in
fission track segmentation, even for difficult images. Algorithms corresponding
to the proposed method are available for download. Using the method presented
here, an user could easily determine fission tracks in photomicrographs of
mineral samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04000</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04000</id><created>2016-02-12</created><authors><author><keyname>Cheng</keyname><forenames>Chu-Han</forenames></author><author><keyname>Chen</keyname><forenames>Po-An</forenames></author><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author></authors><title>Budget-Constrained Multi-Battle Contests: A New Perspective and Analysis</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-battle contest, each time a player competes by investing some of
her budgets or resources in a component battle to collect a value if winning
the battle. There are multiple battles to fight, and the budgets get consumed
over time. The final winner in the overall contest is the one who first reaches
some amount of total value. Examples include R &amp; D races, sports competition,
elections, and many more. A player needs to make adequate sequential actions to
win the contest against dynamic competition over time from the others. We are
interested in how much budgets the players would need and what actions they
should take in order to perform well.
  We model and study such budget-constrained multi-battle contests where each
component battle is a first-price or all-pay auction. We focus on analyzing the
2-player budget ratio that guarantees a player's winning (or falling behind in
just a bounded amount of collected value) against the other omnipotent player.
In the settings considered, we give efficient dynamic programs to find the
optimal budget ratios and the corresponding bidding strategies. Our definition
of game, budget constraints, and emphasis on budget analyses provide a new
perspective and analysis in the related context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04007</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04007</id><created>2016-02-12</created><authors><author><keyname>Naumchev</keyname><forenames>Alexandr</forenames></author><author><keyname>Meyer</keyname><forenames>Bertrand</forenames></author></authors><title>Complete contracts through specification drivers</title><categories>cs.SE</categories><comments>8 pages; 11 figures; submitted to TASE 2016; pending for acceptance
  decision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing techniques of Design by Contract do not allow software developers to
specify complete contracts in many cases. Incomplete contracts leave room for
malicious implementations. This article complements Design by Contract with a
simple yet powerful technique that removes the problem without adding
syntactical mechanisms. The proposed technique makes it possible not only to
derive complete contracts, but also to rigorously check and improve
completeness of existing contracts without instrumenting them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04019</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04019</id><created>2016-02-12</created><authors><author><keyname>Sandberg</keyname><forenames>Anders</forenames></author></authors><title>Energetics of the brain and AI</title><categories>cs.AI q-bio.NC</categories><report-no>STR 2016-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Does the energy requirements for the human brain give energy constraints that
give reason to doubt the feasibility of artificial intelligence? This report
will review some relevant estimates of brain bioenergetics and analyze some of
the methods of estimating brain emulation energy requirements. Turning to AI,
there are reasons to believe the energy requirements for de novo AI to have
little correlation with brain (emulation) energy requirements since cost could
depend merely of the cost of processing higher-level representations rather
than billions of neural firings. Unless one thinks the human way of thinking is
the most optimal or most easily implementable way of achieving software
intelligence, we should expect de novo AI to make use of different, potentially
very compressed and fast, processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04026</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04026</id><created>2016-02-12</created><authors><author><keyname>Blake</keyname><forenames>Christopher G.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Energy, Latency, and Reliability Tradeoffs in Coding Circuits</title><categories>cs.IT math.IT</categories><comments>13 pages, 2 figures, submitted for journal publication, submitted in
  part for presentation at 2016 International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that fully-parallel encoding and decoding schemes with asymptotic
block error probability that scales as $O\left(f\left(n\right)\right)$ have
Thompson energy that scales as $\Omega\left(\sqrt{\ln
f\left(n\right)}n\right)$. As well, it is shown that the number of clock cycles
(denoted $T\left(n\right)$) required for any encoding or decoding scheme that
reaches this bound must scale as $T\left(n\right)\ge\sqrt{\ln
f\left(n\right)}$. Similar scaling results are extended to serialized
computation.
  The Grover information-friction energy model is generalized to three
dimensions and the optimal energy of encoding or decoding schemes with
probability of block error $P_\mathrm{e}$ is shown to be at least
$\Omega\left(n\left(\ln
P_{\mathrm{e}}\left(n\right)\right)^{\frac{1}{3}}\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04031</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04031</id><created>2016-02-12</created><authors><author><keyname>Aum&#xfc;ller</keyname><forenames>Martin</forenames></author><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author><author><keyname>Heuberger</keyname><forenames>Clemens</forenames></author><author><keyname>Krenn</keyname><forenames>Daniel</forenames></author><author><keyname>Prodinger</keyname><forenames>Helmut</forenames></author></authors><title>Counting Zeros in Random Walks on the Integers and Analysis of Optimal
  Dual-Pivot Quicksort</title><categories>math.CO cs.DS</categories><comments>extended abstract</comments><msc-class>05A16, 68R05, 68P10, 68Q25, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an average case analysis of two variants of dual-pivot quicksort,
one with a non-algorithmic comparison-optimal partitioning strategy, the other
with a closely related algorithmic strategy. For both we calculate the expected
number of comparisons exactly as well as asymptotically, in particular, we
provide exact expressions for the linear, logarithmic, and constant terms. An
essential step is the analysis of zeros of lattice paths in a certain
probability model. Along the way a combinatorial identity is proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04032</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04032</id><created>2016-02-12</created><authors><author><keyname>Bhat</keyname><forenames>Satyanath</forenames></author><author><keyname>Padmanabhan</keyname><forenames>Divya</forenames></author><author><keyname>Jain</keyname><forenames>Shweta</forenames></author><author><keyname>Narahari</keyname><forenames>Y</forenames></author></authors><title>A Truthful Mechanism with Biparameter Learning for Online Crowdsourcing</title><categories>cs.AI cs.GT cs.HC</categories><comments>To appear as Extended Abstract in AAMAS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a problem of allocating divisible jobs, arriving online, to workers
in a crowdsourcing setting which involves learning two parameters of
strategically behaving workers. Each job is split into a certain number of
tasks that are then allocated to workers. Each arriving job has to be completed
within a deadline and each task has to be completed satisfying an upper bound
on probability of failure. The job population is homogeneous while the workers
are heterogeneous in terms of costs, completion times, and times to failure.
The job completion time and time to failure of each worker are stochastic with
fixed but unknown means. The requester is faced with the challenge of learning
two separate parameters of each (strategically behaving) worker simultaneously,
namely, the mean job completion time and the mean time to failure. The time to
failure of a worker depends on the duration of the task handled by the worker.
Assuming non-strategic workers to start with, we solve this biparameter
learning problem by applying the Robust UCB algorithm. Then, we non-trivially
extend this algorithm to the setting where the workers are strategic about
their costs. Our proposed mechanism is dominant strategy incentive compatible
and ex-post individually rational with asymptotically optimal regret
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04034</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04034</id><created>2016-02-12</created><authors><author><keyname>Blake</keyname><forenames>Christopher G.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>On Scaling Rules for Energy of VLSI Polar Encoders and Decoders</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that all polar encoding schemes of rate $R&gt;\frac{1}{2}$ of block
length $N$ implemented according to the Thompson VLSI model must take energy
$E\ge\Omega\left(N^{3/2}\right)$. This lower bound is achievable up to
polylogarithmic factors using a mesh network topology defined by Thompson and
the encoding algorithm defined by Arikan. A general class of circuits that
compute successive cancellation decoding adapted from Arikan's butterfly
network algorithm is defined. It is shown that such decoders implemented on a
rectangle grid for codes of rate $R&gt;2/3$ must take energy
$E\ge\Omega(N^{3/2})$, and this can also be reached up to polylogarithmic
factors using a mesh network. Capacity approaching sequences of energy optimal
polar encoders and decoders, as a function of reciprocal gap to capacity $\chi
= (1-R/C)^{-1}$, have energy that scales as $\Omega\left(\chi^{5.325}\right)\le
E \le O\left(\chi^{7.05}\log^{4}\left(\chi\right)\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04049</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04049</id><created>2016-02-12</created><updated>2016-02-24</updated><authors><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Cabezas-Clavijo</keyname><forenames>Alvaro</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author></authors><title>Tracking the performance of an R&amp;D programme in the Biomedical Sciences</title><categories>cs.DL</categories><comments>Pre-refereed version of paper accepted for publication in Research
  Evaluation (2016). 15 pages, 5 figures, 6 tables. V2 some typos corrected</comments><doi>10.1093/reseval/rvw003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at offering an evaluation framework of an Research and
Development programme in the Biomedical Sciences. It showcases the Spanish
Biomedical Research Networking Centres initiative (CIBER) as an example of the
effect of research policy management on performance. For this it focuses on
three specific aspects: its role on the national research output in the
biomedical sciences, its effect on promoting translational research through
internal collaboration between research groups, and the perception of
researchers on the programme as defined by their inclusion of their CIBER
centres in the address field. Research output derived from this programme
represents around 25 per cent of the country's publications in the biomedical
fields. After analysing a seven year period, the programme has enhanced
collaborations between its members, but they do not seem to be sufficiently
strong. With regard to the credit given to the initiative, 54.5 per cent of the
publications mentioned this programme in their address, however an increase on
the share of papers mention it is observed two years after it was launched. We
suggest that by finding the point in which the share of mentions stabilises may
be a good strategy to identify the complete fulfilment of these types of
Research and Development policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04052</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04052</id><created>2016-02-12</created><authors><author><keyname>Teodoro</keyname><forenames>Afonso M.</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>Image Restoration and Reconstruction using Variable Splitting and
  Class-adapted Image Priors</title><categories>cs.CV</categories><comments>Submitted</comments><msc-class>94A08, 68U10, 47N10</msc-class><acm-class>I.4.5; I.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes using a Gaussian mixture model as a prior, for solving
two image inverse problems, namely image deblurring and compressive imaging. We
capitalize on the fact that variable splitting algorithms, like ADMM, are able
to decouple the handling of the observation operator from that of the
regularizer, and plug a state-of-the-art algorithm into the pure denoising
step. Furthermore, we show that, when applied to a specific type of image, a
Gaussian mixture model trained from an database of images of the same type is
able to outperform current state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04056</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04056</id><created>2016-02-12</created><authors><author><keyname>Burow</keyname><forenames>Nathan</forenames></author><author><keyname>Carr</keyname><forenames>Scott A.</forenames></author><author><keyname>Brunthaler</keyname><forenames>Stefan</forenames></author><author><keyname>Payer</keyname><forenames>Mathias</forenames></author><author><keyname>Nash</keyname><forenames>Joseph</forenames></author><author><keyname>Larsen</keyname><forenames>Per</forenames></author><author><keyname>Franz</keyname><forenames>Michael</forenames></author></authors><title>Control-Flow Integrity: Precision, Security, and Performance</title><categories>cs.CR cs.PL</categories><comments>Version submitted to IEEE SP'16 as SoK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory corruption errors in C/C++ programs are still the most common source
of security vulnerabilities in today's systems. Modern &quot;control-flow hijacking&quot;
attacks exploit memory corruption vulnerabilities to divert program execution
away from the intended control-flow. There is intense research interest in
defenses based on Control-Flow Integrity (CFI), and this technique is now being
integrated into several production compilers. However, so far no study has
systematically compared the various proposed CFI mechanisms, nor is there even
any protocol on how to compare such mechanisms.
  We compare a range of CFI mechanisms using a unified nomenclature based on
(i) a qualitative discussion of the conceptual security guarantees, (ii) a
quantitative security evaluation, and (iii) an empirical evaluation of their
performance on the same test environment. For each mechanism, we evaluate (i)
protected types of control-flow transfers, (ii) the precision of the protection
for forward and backward edges, and, for open-sourced compiler-based
implementations additionally (iii) quantitatively the generated equivalence
classes and target sets, and (iv) runtime performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04061</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04061</id><created>2016-02-12</created><authors><author><keyname>Aubrun</keyname><forenames>Nathalie</forenames></author><author><keyname>Sablik</keyname><forenames>Mathieu</forenames></author></authors><title>Row-constrained effective sets of colourings in the $2$-fold horocyclic
  tessellations of $\mathbb{H}^2$ are sofic</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we prove that, restricted to the row-constrained case,
effective sets of colourings in the $2$-fold horocyclic tessellations of the
hyperbolic plane $\mathbb{H}^2$ are sofic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04062</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04062</id><created>2016-02-12</created><authors><author><keyname>Hansen</keyname><forenames>Samantha</forenames></author></authors><title>Using Deep Q-Learning to Control Optimization Hyperparameters</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel definition of the reinforcement learning state, actions
and reward function that allows a deep Q-network (DQN) to learn to control an
optimization hyperparameter. Using Q-learning with experience replay, we train
two DQNs to accept a state representation of an objective function as input and
output the expected discounted return of rewards, or q-values, connected to the
actions of either adjusting the learning rate or leaving it unchanged. The two
DQNs learn a policy similar to a line search, but differ in the number of
allowed actions. The trained DQNs in combination with a gradient-based update
routine form the basis of the Q-gradient descent algorithms. To demonstrate the
viability of this framework, we show that the DQN's q-values associated with
optimal action converge and that the Q-gradient descent algorithms outperform
gradient descent with an Armijo or nonomonotone line search. Unlike traditional
optimization methods, Q-gradient descent can incorporate any objective
statistic and by varying the actions we gain insight into the type of learning
rate adjustment strategies that are successful for neural network optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04095</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04095</id><created>2016-02-12</created><authors><author><keyname>Montealegre</keyname><forenames>Pedro</forenames></author><author><keyname>Todinca</keyname><forenames>Ioan</forenames></author></authors><title>Deterministic graph connectivity in the broadcast congested clique</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic constant-round protocol for the graph connectivity
problem in the model where each of the $n$ nodes of a graph receives a row of
the adjacency matrix, and broadcasts a single sublinear size message to all
other nodes. Communication rounds are synchronous. This model is sometimes
called the broadcast congested clique. Specifically, we exhibit a deterministic
protocol that computes the connected components of the input graph in three
rounds, each player communicating $\mathcal{O}(\sqrt{n} \cdot \log n)$ bits per
round. Our result is based on a $d$-pruning protocol, which consists in
successively removing nodes of degree at most $d$ until obtaining a graph with
minimum degree larger than $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04101</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04101</id><created>2016-02-12</created><authors><author><keyname>Wang</keyname><forenames>Tai</forenames></author><author><keyname>Hu</keyname><forenames>Xiangen</forenames></author><author><keyname>Shubeck</keyname><forenames>Keith</forenames></author><author><keyname>Cai</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Tang</keyname><forenames>Jie</forenames></author></authors><title>An Empirical Study on Academic Commentary and Its Implications on
  Reading and Writing</title><categories>cs.CY cs.CL</categories><comments>22 pages, 5 figures, 6 tables. An extension of an oral presentation
  &quot;a simple model for social media&quot; in 45th Annual Mathematical Psychology
  Meeting,2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between reading and writing (RRW) is one of the major themes
in learning science. One of its obstacles is that it is difficult to define or
measure the latent background knowledge of the individual. However, in an
academic research setting, scholars are required to explicitly list their
background knowledge in the citation sections of their manuscripts. This unique
opportunity was taken advantage of to observe RRW, especially in the published
academic commentary scenario. RRW was visualized under a proposed topic process
model by using a state of the art version of latent Dirichlet allocation (LDA).
The empirical study showed that the academic commentary is modulated both by
its target paper and the author's background knowledge. Although this
conclusion was obtained in a unique environment, we suggest its implications
can also shed light on other similar interesting areas, such as dialog and
conversation, group discussion, and social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04104</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04104</id><created>2016-02-12</created><authors><author><keyname>Zahr</keyname><forenames>Sawsan Al</forenames></author><author><keyname>Gagnaire</keyname><forenames>Maurice</forenames></author></authors><title>An Analytical Model of the IEEE 802.3ah MAC Protocol for EPON-based
  Access Systems</title><categories>cs.NI</categories><comments>5 pages, 7 figures</comments><report-no>ISSN 0751-1337 ENST C (Technical report 2006)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ethernet Passive Optical Networks (EPON) access systems are considered as an
alternative to ADSL for high speed access to the Internet. Supported by the
Ethernet First Mile Alliance and the IEEE, the MPCP protocol has been adopted
as the standardized MAC protocol for EPONs. Moreover, the IPACT dynamic
bandwidth allocation mechanism has been proposed for hierarchical multiplexing.
Today, IPACT is considered as interesting complement to MPCP for dynamic
bandwidth allocation over EPONs. In this paper, we propose an original
analytical model of the MPCP+IPACT protocol. Numerical results obtained from
this model are commented and compared to computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04105</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04105</id><created>2016-02-12</created><authors><author><keyname>O'Shea</keyname><forenames>Timothy J</forenames></author><author><keyname>Corgan</keyname><forenames>Johnathan</forenames></author></authors><title>Convolutional Radio Modulation Recognition Networks</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the adaptation of convolutional neural networks to the complex
temporal radio signal domain. We compare the efficacy of radio modulation
classification using naively learned features against using expert features,
which are currently used widely and well regarded in the field and we show
significant performance improvements. We show that blind temporal learning on
large and densely encoded time series using deep convolutional neural networks
is viable and a strong candidate approach for this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04115</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04115</id><created>2016-02-12</created><authors><author><keyname>Mehrnezhad</keyname><forenames>Maryam</forenames></author><author><keyname>Toreini</keyname><forenames>Ehsan</forenames></author><author><keyname>Shahandashti</keyname><forenames>Siamak F.</forenames></author><author><keyname>Hao</keyname><forenames>Feng</forenames></author></authors><title>TouchSignatures: Identification of User Touch Actions and PINs Based on
  Mobile Sensor Data via JavaScript</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conforming to W3C specifications, mobile web browsers allow JavaScript code
in a web page to access motion and orientation sensor data without the user's
permission. The associated risks to user security and privacy are however not
considered in W3C specifications. In this work, for the first time, we show how
user security can be compromised using these sensor data via browser, despite
that the data rate is 3 to 5 times slower than what is available in app. We
examine multiple popular browsers on Android and iOS platforms and study their
policies in granting permissions to JavaScript code with respect to access to
motion and orientation sensor data. Based on our observations, we identify
multiple vulnerabilities, and propose TouchSignatures which implements an
attack where malicious JavaScript code on an attack tab listens to such sensor
data measurements. Based on these streams, TouchSignatures is able to
distinguish the user's touch actions (i.e., tap, scroll, hold, and zoom) and
her PINs, allowing a remote website to learn the client-side user activities.
We demonstrate the practicality of this attack by collecting data from real
users and reporting high success rates using our proof-of-concept
implementations. We also present a set of potential solutions to address the
vulnerabilities. The W3C community and major mobile browser vendors including
Mozilla, Google, Apple and Opera have acknowledge our work and are implementing
some of our proposed countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04123</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04123</id><created>2016-02-12</created><authors><author><keyname>Yamada</keyname><forenames>Norihiro</forenames></author></authors><title>Game-theoretic Interpretation of Type Theory Part II: Uniqueness of
  Identity Proofs and Univalence</title><categories>cs.LO math.CO math.CT math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, based on the previous work (Part I), we present a game
semantics for the intensional variant of intuitionistic type theory with a
hierarchy of universes that refutes the principle of uniqueness of identity
proofs and validates the univalence axiom, though we have the hierarchy of
Id-types only up to 1-type. Specifically, we first add a groupoid structure on
predicative games introduced in Part I, which gives rise to the notion of
gamoids. Roughly, gamoids are &quot;games with equalities specified&quot;, a refinement
of the predicative games, which interprets the subtleties in Id-types. We then
formulate a category with families of gamoids, equipped with dependent product,
dependent sum and Id-types as well as universes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04124</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04124</id><created>2016-02-12</created><authors><author><keyname>Sridhar</keyname><forenames>Srinath</forenames></author><author><keyname>Mueller</keyname><forenames>Franziska</forenames></author><author><keyname>Oulasvirta</keyname><forenames>Antti</forenames></author><author><keyname>Theobalt</keyname><forenames>Christian</forenames></author></authors><title>Fast and Robust Hand Tracking Using Detection-Guided Optimization</title><categories>cs.CV</categories><comments>9 pages, Accepted version of paper published at CVPR 2015</comments><journal-ref>Computer Vision and Pattern Recognition (CVPR), 2015 IEEE
  Conference on , vol., no., pp.3213-3221, 7-12 June 2015</journal-ref><doi>10.1109/CVPR.2015.7298941</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markerless tracking of hands and fingers is a promising enabler for
human-computer interaction. However, adoption has been limited because of
tracking inaccuracies, incomplete coverage of motions, low framerate, complex
camera setups, and high computational requirements. In this paper, we present a
fast method for accurately tracking rapid and complex articulations of the hand
using a single depth camera. Our algorithm uses a novel detection-guided
optimization strategy that increases the robustness and speed of pose
estimation. In the detection step, a randomized decision forest classifies
pixels into parts of the hand. In the optimization step, a novel objective
function combines the detected part labels and a Gaussian mixture
representation of the depth to estimate a pose that best fits the depth. Our
approach needs comparably less computational resources which makes it extremely
fast (50 fps without GPU support). The approach also supports varying static,
or moving, camera-to-scene arrangements. We show the benefits of our method by
evaluating on public datasets and comparing against previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04128</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04128</id><created>2016-02-12</created><authors><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author><author><keyname>P&#xe1;l</keyname><forenames>D&#xe1;vid</forenames></author></authors><title>From Coin Betting to Parameter-Free Online Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent years a number of parameter-free algorithms for online linear
optimization over Hilbert spaces and for learning with expert advice have been
developed. While these two families of algorithms might seem different to a
distract eye, the proof methods are indeed very similar, making the reader
wonder if such a connection is only accidental.
  In this paper, we unify these two families, showing that both can be
instantiated from online coin betting algorithms. We present two new reductions
from online coin betting to online linear optimization over Hilbert spaces and
to learning with expert advice. We instantiate our framework using a betting
algorithm based on the Krichevsky-Trofimov estimator. We obtain a simple
algorithm for online linear optimization over any Hilbert space with
$O(\norm{u}\sqrt{T \log(1+T \norm{u}}))$ regret with respect to any competitor
$u$. For learning with expert advice we obtain an algorithm that has $O(\sqrt{T
(1 + \KL{u}{\pi})})$ regret against any competitor $u$ and where $\KL{u}{\pi}$
is the Kullback-Leibler divergence between algorithm's prior distribution $\pi$
and the competitor. In both cases, no parameters need to be tuned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04133</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04133</id><created>2016-02-12</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Hern&#xe1;ndez-Lobato</keyname><forenames>Daniel</forenames></author><author><keyname>Li</keyname><forenames>Yingzhen</forenames></author><author><keyname>Hern&#xe1;ndez-Lobato</keyname><forenames>Jos&#xe9; Miguel</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Deep Gaussian Processes for Regression using Approximate Expectation
  Propagation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations
of Gaussian processes (GPs) and are formally equivalent to neural networks with
multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic
models and as such are arguably more flexible, have a greater capacity to
generalise, and provide better calibrated uncertainty estimates than
alternative deep models. This paper develops a new approximate Bayesian
learning scheme that enables DGPs to be applied to a range of medium to large
scale regression problems for the first time. The new method uses an
approximate Expectation Propagation procedure and a novel and efficient
extension of the probabilistic backpropagation algorithm for learning. We
evaluate the new method for non-linear regression on eleven real-world
datasets, showing that it always outperforms GP regression and is almost always
better than state-of-the-art deterministic and sampling-based approximate
inference methods for Bayesian neural networks. As a by-product, this work
provides a comprehensive analysis of six approximate Bayesian methods for
training neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04138</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04138</id><created>2016-02-12</created><authors><author><keyname>Grining</keyname><forenames>Krzysztof</forenames></author><author><keyname>Klonowski</keyname><forenames>Marek</forenames></author><author><keyname>Syga</keyname><forenames>Piotr</forenames></author></authors><title>Practical Fault-Tolerant Data Aggregation</title><categories>cs.CR</categories><comments>Submitted to ACNS 2016;30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During Financial Cryptography 2012 Chan et al. presented a novel
privacy-protection fault-tolerant data aggregation protocol. Comparing to
previous work, their scheme guaranteed provable privacy of individuals and
could work even if some number of users refused to participate. In our paper we
demonstrate that despite its merits, their method provides unacceptably low
accuracy of aggregated data for a wide range of assumed parameters and cannot
be used in majority of real-life systems. To show this we use both precise
analytic and experimental methods. Additionally, we present a precise data
aggregation protocol that provides provable level of security even facing
massive failures of nodes. Moreover, the protocol requires significantly less
computation (limited exploiting of heavy cryptography) than most of currently
known fault tolerant aggregation protocols and offers better security
guarantees that make it suitable for systems of limited resources (including
sensor networks). To obtain our result we relax however the model and allow
some limited communication between the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04145</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04145</id><created>2016-02-12</created><authors><author><keyname>Bouland</keyname><forenames>Adam</forenames></author><author><keyname>Man&#x10d;inska</keyname><forenames>Laura</forenames></author><author><keyname>Zhang</keyname><forenames>Xue</forenames></author></authors><title>Complexity classification of two-qubit commuting hamiltonians</title><categories>quant-ph cs.CC</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify two-qubit commuting Hamiltonians in terms of their computational
complexity. Suppose one has a two-qubit commuting Hamiltonian H which one can
apply to any pair of qubits, starting in a computational basis state. We prove
a dichotomy theorem: either this model is efficiently classically simulable or
it allows one to sample from probability distributions which cannot be sampled
from classically unless the polynomial hierarchy collapses. Furthermore, the
only simulable Hamiltonians are those which fail to generate entanglement. This
shows that generic two-qubit commuting Hamiltonians can be used to perform
computational tasks which are intractable for classical computers under
plausible assumptions. Our proof makes use of new postselection gadgets and Lie
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04146</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04146</id><created>2016-02-12</created><authors><author><keyname>Sabau</keyname><forenames>Serban</forenames></author><author><keyname>Morarescu</keyname><forenames>Irinel-Constantin</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Decoupled Dynamics Distributed Control for Strings of Nonlinear
  Autonomous Agents</title><categories>cs.SY</categories><comments>12 pges, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel distributed control architecture for the trajectory
tracking problem associated with a homogeneous string of nonlinear dynamical
agents. The scheme achieves velocity matching and collision avoidance, while
featuring a remarkable decoupling property of the closed-loop dynamics at each
autonomous agent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04152</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04152</id><created>2016-02-12</created><authors><author><keyname>Bhowmick</keyname><forenames>Santanu</forenames></author><author><keyname>Inamdar</keyname><forenames>Tanmay</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author></authors><title>Improved Approximation for Metric Multi-Cover</title><categories>cs.CG</categories><comments>16 Pages, 2 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the metric multi-cover problem (MMC). The input consists of two
point sets $Y$(servers) and $X$(clients) in an arbitrary metric space $(X \cup
Y, d)$, a positive integer $k$ that represents the coverage demand of each
client, and a constant $\alpha \geq 1$. Each server can have a single ball of
arbitrary radius centered on it. Each client $x \in X$ needs to be covered by
at least $k$ such balls centered on servers. The objective function that we
wish to minimize is the sum of the $\alpha$-th powers of the radii of the
balls. Bar-Yehuda[2013] gave a $3^{\alpha} \cdot k$-approximation for the MMC
problem. Bhowmick et al [SoCG 13, JoCG 15] showed that an $O(1)$ approximation
(independent of the coverage demand $k$) exists for the special case in which
$X, Y$ are points in a fixed-dimensional space $\mathbb{R}^d$. In this paper,
we present an $O(1)$-approximation for the MMC problem in any arbitrary metric
space, improving the result of Bar-Yehuda[2013] and matching the guarantee for
fixed-dimensional Euclidean spaces in Bhowmick et al [SoCG 13, JoCG 15].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04181</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04181</id><created>2016-02-12</created><authors><author><keyname>Feizi</keyname><forenames>Soheil</forenames></author><author><keyname>Quon</keyname><forenames>Gerald</forenames></author><author><keyname>Recamonde-Mendoza</keyname><forenames>Mariana</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Kellis</keyname><forenames>Manolis</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Spectral Alignment of Networks</title><categories>cs.DS cs.DM math.CO</categories><comments>68 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network alignment refers to the problem of finding a bijective mapping across
vertices of two graphs to maximize the number of overlapping edges and/or to
minimize the number of mismatched interactions across networks. This problem is
often cast as an expensive quadratic assignment problem (QAP). Although
spectral methods have received significant attention in different network
science problems such as network clustering, the use of spectral techniques in
the network alignment problem has been limited partially owing to the lack of
principled connections between spectral methods and relaxations of the network
alignment optimization. In this paper, we propose a network alignment framework
that uses an orthogonal relaxation of the underlying QAP in a maximum weight
bipartite matching optimization. Our method takes into account the ellipsoidal
level sets of the quadratic objective function by exploiting eigenvalues and
eigenvectors of (transformations of) adjacency graphs. Our framework not only
can be employed to provide a theoretical justification for existing heuristic
spectral network alignment methods, but it also leads to a new scalable network
alignment algorithm which outperforms existing ones over various synthetic and
real networks. Moreover, we generalize the objective function of the network
alignment problem to consider both matched and mismatched interactions in a
standard QAP formulation. This can be critical in applications where networks
have low similarity and therefore we expect more mismatches than matches. We
assess the effectiveness of our proposed method theoretically for certain
classes of networks, through simulations over various synthetic network models,
and in two real-data applications; in comparative analysis of gene regulatory
networks across human, fly and worm, and in user de-anonymization over twitter
follower subgraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04183</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04183</id><created>2016-02-12</created><authors><author><keyname>Pedram</keyname><forenames>Ardavan</forenames></author><author><keyname>Richardson</keyname><forenames>Stephen</forenames></author><author><keyname>Galal</keyname><forenames>Sameh</forenames></author><author><keyname>Kvatinsky</keyname><forenames>Shahar</forenames></author><author><keyname>Horowitz</keyname><forenames>Mark</forenames></author></authors><title>Dark Memory and Accelerator-Rich System Optimization in the Dark Silicon
  Era</title><categories>cs.AR cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key challenge to improving performance in the age of Dark Silicon is how
to leverage transistors when they cannot all be used at the same time. In
modern SOCs, these transistors are often used to create specialized
accelerators which improve energy efficiency for some applications by 10-1000X.
While this might seem like the magic bullet we need, for most CPU applications
more energy is dissipated in the memory system than in the processor: these
large gains in efficiency are only possible if the DRAM and memory hierarchy
are mostly idle. We refer to this desirable state as Dark Memory, and it only
occurs for applications with an extreme form of locality.
  To show our findings, we introduce Pareto curves in the energy/op and
mm$^2$/(ops/s) metric space for compute units, accelerators, and on-chip
memory/interconnect. These Pareto curves allow us to solve the power,
performance, area constrained optimization problem to determine which
accelerators should be used, and how to set their design parameters to optimize
the system. This analysis shows that memory accesses create a floor to the
achievable energy-per-op. Thus high performance requires Dark Memory, which in
turn requires co-design of the algorithm for parallelism and locality, with the
hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04184</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04184</id><created>2016-02-12</created><updated>2016-02-19</updated><authors><author><keyname>Critch</keyname><forenames>Andrew</forenames></author></authors><title>Parametric Bounded L\&quot;ob's Theorem and Robust Cooperation of Bounded
  Agents</title><categories>cs.GT cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  L\&quot;ob's theorem and G\&quot;odel's theorem make predictions about the behavior of
self-reflective systems with unbounded computational resources with which to
write and evaluate proofs. However, in the real world, self-reflective systems
will have limited memory and processing speed, so in this paper we introduce an
effective version of L\&quot;ob's theorem theorem which is applicable given such
bounded resources. These results have powerful implications for the game theory
of bounded agents who are able to write proofs about themselves and one
another, including the capacity to out-perform classical Nash equilibria and
correlated equilibria, attaining mutually cooperative program equilibrium in
the Prisoner's Dilemma. Previous cooperative program equilibria studied by
Tennenholtz (2004) and Fortnow (2009) have depended on tests for program
equality, a fragile condition, whereas &quot;L\&quot;obian&quot; cooperation is much more
robust and agnostic of the opponent's implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04186</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04186</id><created>2016-02-12</created><authors><author><keyname>Javarone</keyname><forenames>Marco Alberto</forenames></author></authors><title>An Evolutionary Strategy based on Partial Imitation for Solving
  Optimization Problems</title><categories>cond-mat.dis-nn cs.NE math.OC</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce an evolutionary strategy to solve optimization
tasks. In particular, we focus on the Travel Salesman Problem (TSP), i.e., a
NP-hard problem with a discrete search space. The solutions of the TSP can be
codified by arrays of cities, and can be evaluated by a fitness computed
according to a cost function (e.g., the length of a path). Our method is based
on the evolution of an agent population by means of a `partial imitation'
mechanism. In particular, agents receive a random solution and then,
interacting among themselves, imitate the solutions of agents with a higher
fitness. Moreover, as stated above, the imitation is only partial, i.e., agents
copy only one, randomly chosen, entry of better (array) solutions. In doing so,
the population converges towards a shared solution, behaving like a spin system
undergoing a cooling process, i.e., driven towards an ordered phase. We
highlight that the adopted `partial imitation' mechanism allows the population
to generate new solutions over time, before reaching the final equilibrium.
Remarkably, results of numerical simulations show that our method is able to
find the optimal solution in all considered search spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04207</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04207</id><created>2016-02-12</created><authors><author><keyname>Naderializadeh</keyname><forenames>Navid</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Fundamental Limits of Cache-Aided Interference Management</title><categories>cs.IT math.IT</categories><comments>This work was partially submitted to the 2016 IEEE International
  Symposium on Information Theory (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a system comprising a library of $N$ files (e.g., movies) and a
wireless network with $K_T$ transmitters, each equipped with an isolated cache
of size of $M_T$ files, and $K_R$ receivers, each equipped with an isolated
cache of size of $M_R$ files. Each receiver will ask for one of the $N$ files
in the library which needs to be delivered. The goal is to design the cache
placement (without prior knowledge of receivers' future requests) and the
communication scheme to maximize the throughput of the delivery. In this
setting, we show that the sum degrees-of-freedom (sum-DoF) of
$\min\left\{\frac{K_T M_T+K_R M_R}{N},K_R\right\}$ is achievable, and this is
within a factor of 2 of the optimum, under one-shot linear schemes. This result
shows that (i) the one-shot sum-DoF scales linearly with the aggregate cache
size in the network (i.e., the cumulative memory available at all nodes), (ii)
the transmitter and receiver caches contribute equally in the one-shot sum-DoF,
(iii) caching can offer a throughput gain that scales linearly with the size of
the network.
  To prove the result, we propose an achievable scheme that exploits the
redundancy of the content at transmitter caches to cooperatively zero-force
some outgoing interference and availability of the unintended content at the
receiver caches to cancel (subtract) some of the incoming interference. We
develop a particular pattern for cache placement that maximizes the overall
gains of cache-aided transmit and receive interference cancellations. For the
converse, we present an integer-linear optimization problem which minimizes the
number of communication blocks needed to deliver any set of requested files to
the receivers. We then provide a lower bound on the value of this optimization
problem, hence leading to an upper bound on the linear one-shot sum-DoF of the
network which is within a factor of 2 of the achievable sum-DoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04208</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04208</id><created>2016-02-12</created><authors><author><keyname>Khanna</keyname><forenames>Rajiv</forenames></author><author><keyname>Tschannen</keyname><forenames>Michael</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author></authors><title>Pursuits in Structured Non-Convex Matrix Factorizations</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficiently representing real world data in a succinct and parsimonious
manner is of central importance in many fields. We present a generalized greedy
pursuit framework, allowing us to efficiently solve structured matrix
factorization problems, where the factors are allowed to be from arbitrary sets
of structured vectors. Such structure may include sparsity, non-negativeness,
order, or a combination thereof. The algorithm approximates a given matrix by a
linear combination of few rank-1 matrices, each factorized into an outer
product of two vector atoms of the desired structure. For the non-convex
subproblems of obtaining good rank-1 structured matrix atoms, we employ and
analyze a general atomic power method. In addition to the above applications,
we prove linear convergence for generalized pursuit variants in Hilbert spaces
- for the task of approximation over the linear span of arbitrary dictionaries
- which generalizes OMP and is useful beyond matrix problems. Our experiments
on real datasets confirm both the efficiency and also the broad applicability
of our framework in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04210</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04210</id><created>2016-02-12</created><authors><author><keyname>Al-Maqri</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Othman</keyname><forenames>Mohamed</forenames></author><author><keyname>Ali</keyname><forenames>Borhanuddin Mohd</forenames></author><author><keyname>Hanapi</keyname><forenames>Zurina Mohd</forenames></author></authors><title>Adaptive Multi-polling Scheduler for QoS Support of Video Transmission
  in IEEE 802.11e WLANs</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1602.03699,
  arXiv:1602.03886</comments><doi>10.1007/s11235-015-0020-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 802.11E Task Group has been established to enhance Quality of Service
(QoS) provision for time-bounded services in the current IEEE 802.11 Medium
Access Control (MAC) protocol. The QoS is introduced throughout Hybrid
Coordination Function Controlled Channel Access (HCCA) for the rigorous QoS
provision. In HCCA, the station is allocated a fixed Transmission Opportunity
(TXOP) based on its TSPEC parameters so that it is efficient for Constant Bit
Rate streams. However, as the profile of Variable Bit Rate (VBR) traffics is
inconstant, they are liable to experience a higher delay especially in bursty
traffic case. In this paper, we present a dynamic TXOP assignment algorithm
called Adaptive Multi-polling TXOP scheduling algorithm (AMTXOP) for supporting
the video traffics transmission over IEEE 802.11e wireless networks. This
scheme invests a piggybacked information about the size of the subsequent video
frames of the uplink streams to assist the Hybrid Coordinator accurately assign
the TXOP according to actual change in the traffic profile. The proposed
scheduler is powered by integrating multi-polling scheme to further reduce the
delay and polling overhead. Extensive simulation experiments have been carried
out to show the efficiency of the AMTXOP over the existing schemes in terms of
the packet delay and the channel utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04211</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04211</id><created>2016-02-12</created><authors><author><keyname>Mantas</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Ramos</keyname><forenames>Fernando M. V.</forenames></author></authors><title>Consistent and fault-tolerant SDN with unmodified switches</title><categories>cs.NI</categories><comments>1 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a reliable SDN environment, different controllers coordinate different
switches and backup controllers can be set in place to tolerate faults. This
approach increases the challenge to maintain a consistent network view. If this
global view is not consistent with the actual network state, applications will
operate on a stale state and potentially lead to incorrect behavior.
  Faced with this problem, we propose a fault-tolerant SDN controller that is
able to maintain a consistent network view by using transactional semantics on
both control and data plane state. Different from previous proposals, our
solution does not require changes to OpenFlow or to switches, increasing the
chances of quicker adoption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04229</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04229</id><created>2016-02-12</created><authors><author><keyname>Abr&#xe3;o</keyname><forenames>Taufik</forenames></author><author><keyname>Sampaio</keyname><forenames>Lucas D. H.</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Cheung</keyname><forenames>Kent Tsz Kan</forenames></author><author><keyname>Jeszensky</keyname><forenames>Paul Jean E.</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Energy efficient OFDMA networks maintaining statistical QoS guarantees
  for delay-sensitive traffic</title><categories>cs.IT math.IT</categories><comments>16 pages, 10 figures, 3 tables, accepted to appear on IEEE Access,
  Jan. 2016</comments><doi>10.1109/ACCESS.2016.2530688</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An energy-efficient design is proposed under specific statistical
quality-of-service (QoS) guarantees for delay-sensitive traffic in the downlink
orthogonal frequency-division multiple-access (OFDMA) networks. This design is
based on Wu's $\textit{effective capacity}$ (EC) concept [1], which
characterizes the maximum throughput of a system subject to statistical
delay-QoS requirements at the data-link layer. In the particular context
considered, our main contributions consist of quantifying the
$\textit{effective energy-efficiency}$ (EEE)-versus-EC tradeoff and
characterizing the delay-sensitive traffic as a function of the QoS-exponent
$\theta$, which expresses the exponential decay rate of the delay-QoS violation
probabilities. Upon exploiting the properties of fractional programming, the
originally quasi-concave EEE optimization problem having a fractional form is
transformed into a subtractive optimization problem by applying Dinkelbach's
method. As a result, an iterative inner-outer loop based resource allocation
algorithm is conceived for efficiently solving the transformed EEE optimization
problem. Our simulation results demonstrate that the proposed scheme converges
within a few Dinkelbach algorithm's iterations to the desired solution
accuracy. Furthermore, the impact of the circuitry power, of the QoS-exponent
and of the power amplifier inefficiency is characterized numerically. These
results reveal that the optimally allocated power maximizing the EEE decays
exponentially with respect to both the circuitry power and the QoS-exponent,
whilst decaying linearly with respect to the power amplifier inefficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04234</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04234</id><created>2016-02-12</created><authors><author><keyname>Baros</keyname><forenames>Stefanos</forenames></author></authors><title>Consensus-Based Torque Control of Deloaded Wind DFIGs for Distributed
  and Fair Dynamic Dispatching</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we aim to address the problem of dynamically dispatching a
group of state-of-the-art deloaded wind generators (WGs) in a fair-sharing
manner. We use the term dynamically since the WGs aim to dispatch themselves
according to a varying committed WF power output. We first propose a
leader-follower protocol whose execution guarantees asymptotically, two control
objectives. These are 1) reaching asymptotic consensus on the utilization level
of all WGs and 2) the total power output of the WGs asymptotically converges to
the reference value. Thereafter, we combine singular perturbation and Lyapunov
theory to prove that, under certain conditions, the proposed protocol will
asymptotically converge to its equilibrium. Finally, we derive a cooperative
Control Lyapunov Function-based (CLF) controller for the rotor side converter
(RSC) of each WG that realizes the protocol in practice. We demonstrate the
effectiveness of our proposed protocol and the corresponding RSC controller
design via simulations on the modified IEEE 24-bus RT system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04256</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04256</id><created>2016-02-12</created><authors><author><keyname>Gao</keyname><forenames>Yihan</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Squish: Near-Optimal Compression for Archival of Relational Datasets</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In every sphere of existence, relational datasets are being generated at an
alarmingly rapid rate. Compressing these datasets could significantly reduce
storage and archival costs. Traditional compression algorithms, e.g., gzip, are
suboptimal for compressing relational datasets since they ignore the table
structure and relationships between attributes.
  In this paper, we study compression algorithms that leverage the relational
structure to compress datasets to a much greater extent. We develop Squish, a
system that uses a combination of Bayesian Networks and Arithmetic Coding to
capture multiple kinds of dependencies among attributes and achieve
near-entropy compression rate. Squish also supports user-defined attributes:
users can instantiate new data types by simply implementing five functions for
a new class interface. We prove the asymptotic optimality of our compression
algorithm and conduct experiments to show the effective of our system: Squish
achieves a reduction of over 50\% in compression relative to systems developed
in prior work on a variety of real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04257</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04257</id><created>2016-02-12</created><authors><author><keyname>Bhuvan</keyname><forenames>Malladihalli S</forenames></author><author><keyname>Kumar</keyname><forenames>Ankit</forenames></author><author><keyname>Zafar</keyname><forenames>Adil</forenames></author><author><keyname>Kishore</keyname><forenames>Vinith</forenames></author></authors><title>Identifying Diabetic Patients with High Risk of Readmission</title><categories>cs.AI cs.CY</categories><comments>10 pages, 5 figures, 7 tables</comments><acm-class>J.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hospital readmissions are expensive and reflect the inadequacies in
healthcare system. In the United States alone, treatment of readmitted diabetic
patients exceeds 250 million dollars per year. Early identification of patients
facing a high risk of readmission can enable healthcare providers to to conduct
additional investigations and possibly prevent future readmissions. This not
only improves the quality of care but also reduces the medical expenses on
readmission. Machine learning methods have been leveraged on public health data
to build a system for identifying diabetic patients facing a high risk of
future readmission. Number of inpatient visits, discharge disposition and
admission type were identified as strong predictors of readmission. Further, it
was found that the number of laboratory tests and discharge disposition
together predict whether the patient will be readmitted shortly after being
discharged from the hospital (i.e. &lt;30 days) or after a longer period of time
(i.e. &gt;30 days). These insights can help healthcare providers to improve
inpatient diabetic care. Finally, the cost analysis suggests that \$252.76
million can be saved across 98,053 diabetic patient encounters by incorporating
the proposed cost sensitive analysis model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04259</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04259</id><created>2016-02-12</created><authors><author><keyname>Krakovna</keyname><forenames>Viktoriya</forenames></author><author><keyname>Looks</keyname><forenames>Moshe</forenames></author></authors><title>A Minimalistic Approach to Sum-Product Network Learning for Real
  Applications</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sum-Product Networks (SPNs) are a class of expressive yet tractable
hierarchical graphical models. LearnSPN is a structure learning algorithm for
SPNs that uses hierarchical co-clustering to simultaneously identifying similar
entities and similar features. The original LearnSPN algorithm assumes that all
the variables are discrete and there is no missing data. We introduce a
practical, simplified version of LearnSPN, MiniSPN, that runs faster and can
handle missing data and heterogeneous features common in real applications. We
demonstrate the performance of MiniSPN on standard benchmark datasets and on
two datasets from Google's Knowledge Graph exhibiting high missingness rates
and a mix of discrete and continuous features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04260</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04260</id><created>2016-02-12</created><authors><author><keyname>Dutta</keyname><forenames>Sanghamitra</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author></authors><title>Adaptivity provably helps: information-theoretic limits on $l_0$ cost of
  non-adaptive sensing</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures, Submitted to ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advantages of adaptivity and feedback are of immense interest in signal
processing and communication with many positive and negative results. Although
it is established that adaptivity does not offer substantial reductions in
minimax mean square error for a fixed number of measurements, existing results
have shown several advantages of adaptivity in complexity of reconstruction,
accuracy of support detection, and gain in signal-to-noise ratio, under
constraints on sensing energy. Sensing energy has often been measured in terms
of the Frobenius Norm of the sensing matrix. This paper uses a different metric
that we call the $l_0$ cost of a sensing matrix-- to quantify the complexity of
sensing. Thus sparse sensing matrices have a lower cost. We derive
information-theoretic lower bounds on the $l_0$ cost that hold for any
non-adaptive sensing strategy. We establish that any non-adaptive sensing
strategy must incur an $l_0$ cost of $\Theta\left( N \log_2(N)\right) $ to
reconstruct an $N$-dimensional, one--sparse signal when the number of
measurements are limited to $\Theta\left(\log_2 (N)\right)$. In comparison,
bisection-type adaptive strategies only require an $l_0$ cost of at most
$\mathcal{O}(N)$ for an equal number of measurements. The problem has an
interesting interpretation as a sphere packing problem in a multidimensional
space, such that all the sphere centres have minimum non-zero co-ordinates. We
also discuss the variation in $l_0$ cost as the number of measurements increase
from $\Theta\left(\log_2 (N)\right)$ to $\Theta\left(N\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04261</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04261</id><created>2016-02-12</created><authors><author><keyname>Baros</keyname><forenames>Stefanos</forenames></author></authors><title>Short-term Predictable Power of a Wind Farm via Distributed Control of
  Wind Generators with Integrated Storage</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a Leader-follower consensus protocol and study its
stability properties with and without communication delays. On the practical
side, we explore its application on coordinating a group of wind Double-Fed
Induction Generators (DFIGs) with integrated storage. To begin with, we
establish asymptotic stability of the consensus protocol by employing singular
perturbation theory. Subsequently, we establish asymptotic stability of the
protocol under communication delays using a Lyapunov-Krasovskii functional.
Lastly, we use the proposed protocol to design a methodology that can be
adopted by a fleet of state-of-the-art wind generators (WGs). The objective is
that the WGs selforganize and control their storage devices such that WF total
power output is tracking a reference while equal contribution from each storage
device is attained i.e equal power output from each storage. We demonstrate the
effectiveness of our results and the corresponding approach via simulations on
the IEEE 24-bus RT system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04265</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04265</id><created>2016-02-12</created><authors><author><keyname>Wong</keyname><forenames>Kam Chung</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Li</keyname><forenames>Zifan</forenames></author></authors><title>Regularized Estimation in High Dimensional Time Series under Mixing
  Conditions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lasso is one of the most popular methods in high dimensional statistical
learning. Most existing theoretical results for the Lasso, however, require the
samples to be iid. Recent work has provided guarantees for the Lasso assuming
that the time series is generated by a sparse Vector Auto-Regressive (VAR)
model with Gaussian innovations. Proofs of these results rely critically on the
fact that the true data generating mechanism (DGM) is a finite-order Gaussian
VAR. This assumption is quite brittle: linear transformations, including
selecting a subset of variables, can lead to the violation of this assumption.
In order to break free from such assumptions, we derive nonasymptotic
inequalities for estimation error and prediction error of the Lasso estimate of
the best linear predictor without assuming any special parametric form of the
DGM. Instead, we rely only on (strict) stationarity and mixing conditions to
establish consistency of the Lasso in the following two scenarios: (a)
alpha-mixing Gaussian processes, and (b) beta-mixing sub-Gaussian random
vectors. Our work provides an alternative proof of the consistency of the Lasso
for sparse Gaussian VAR models. But the applicability of our results extends to
non-Gaussian and non-linear times series models as the examples we provide
demonstrate. In order to prove our results, we derive a novel Hanson-Wright
type concentration inequality for beta-mixing sub-Gaussian random vectors that
may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04268</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04268</id><created>2016-02-12</created><authors><author><keyname>Kent</keyname><forenames>Robert E.</forenames></author></authors><title>The ERA of FOLE: Superstructure</title><categories>cs.LO cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses the representation of ontologies in the first-order
logical environment FOLE (Kent 2013). An ontology defines the primitives with
which to model the knowledge resources for a community of discourse (Gruber
2009). These primitives, consisting of classes, relationships and properties,
are represented by the ERA (entity-relationship-attribute) data model (Chen
1976). An ontology uses formal axioms to constrain the interpretation of these
primitives. In short, an ontology specifies a logical theory. This paper is the
second in a series of three papers that provide a rigorous mathematical
representation for the ERA data model in particular, and ontologies in general,
within the first-order logical environment FOLE. The first two papers show how
FOLE represents the formalism and semantics of (many-sorted) first-order logic
in a classification form corresponding to ideas discussed in the Information
Flow Framework (IFF). In particular, the first paper (Kent 2015) provided a
&quot;foundation&quot; that connected elements of the ERA data model with components of
the first-order logical environment FOLE, and this second paper provides a
&quot;superstructure&quot; that extends FOLE to the formalisms of first-order logic. The
third paper will define an &quot;interpretation&quot; of FOLE in terms of the
transformational passage, first described in (Kent 2013), from the
classification form of first-order logic to an equivalent interpretation form,
thereby defining the formalism and semantics of first-order logical/relational
database systems (Kent 2011). The FOLE representation follows a conceptual
structures approach, that is completely compatible with Formal Concept Analysis
(Ganter and Wille 1999) and Information Flow (Barwise and Seligman 1997).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04270</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04270</id><created>2016-02-12</created><authors><author><keyname>Kirkpatrick</keyname><forenames>Bonnie</forenames></author></authors><title>Haplotype Inference for Pedigrees with Few Recombinations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedigrees, or family trees, are graphs of family relationships that are used
to study inheritance. A fundamental problem in computational biology is to
find, for a pedigree with $n$ individuals genotyped at every site, a set of
Mendelian-consistent haplotypes that have the minimum number of recombinations.
This is an NP-hard problem and some pedigrees can have thousands of individuals
and hundreds of thousands of sites.
  This paper formulates this problem as a optimization on a graph and
introduces a tailored algorithm with a running time of O(n^{(k+2)}m^{6k}) for n
individuals, m sites, and k recombinations. Since there are generally only 1-2
recombinations per chromosome in each meiosis, k is small enough to make this
algorithm practically relevant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04274</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04274</id><created>2016-02-12</created><authors><author><keyname>Zaribafiyan</keyname><forenames>Arman</forenames></author><author><keyname>Marchand</keyname><forenames>Dominic J. J.</forenames></author><author><keyname>Rezaei</keyname><forenames>Seyed Saeed Changiz</forenames></author></authors><title>Systematic and Deterministic Graph-Minor Embedding for Cartesian
  Products of Graphs</title><categories>cs.DM cs.DS quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The limited connectivity of current and next-generation quantum annealers
motivates the need for efficient graph-minor embedding methods. These methods
allow non-native problems to be adapted to the target annealer's architecture.
The overhead of the widely used heuristic techniques is quickly proving to be a
significant bottleneck for solving real-world applications. To alleviate this
difficulty, we propose a systematic and deterministic embedding method,
exploiting the structures of both the input graph of the specific problem and
the quantum annealer. We focus on the specific case of the Cartesian product of
two complete graphs, a regular structure that occurs in many problems. We
divide the embedding problem by first embedding one of the factors of the
Cartesian product in a repeatable pattern. The resulting simplified problem
consists of the placement and connecting together of these copies to reach a
valid solution. Aside from the obvious advantage of a systematic and
deterministic approach with respect to speed and efficiency, the embeddings
produced are easily scaled for larger processors and show desirable properties
for the number of qubits used and the chain length distribution. To conclude,
we briefly address the problem of circumventing inoperable qubits by presenting
possible extensions of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04275</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04275</id><created>2016-02-12</created><authors><author><keyname>Guan</keyname><forenames>Yonghui</forenames></author></authors><title>Equations for secant varieties of Chow varieties</title><categories>math.AG cs.CC</categories><comments>14 pages</comments><msc-class>14Q20, 68Q15, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Chow variety of polynomials that decompose as a product of linear forms
has been studied for more than 100 years. Finding equations in the ideal of
secant varieties of Chow varieties would enable one to measure the complexity
the permanent polynomial. In this article, I use the method of prolongation to
obtain equations for secant varieties of Chow varieties as $GL(V)$-modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04277</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04277</id><created>2016-02-12</created><authors><author><keyname>Cao</keyname><forenames>Renzhi</forenames></author><author><keyname>Jo</keyname><forenames>Taeho</forenames></author><author><keyname>Cheng</keyname><forenames>Jianlin</forenames></author></authors><title>Evaluation of Protein Structural Models Using Random Forests</title><categories>cs.LG q-bio.BM q-bio.QM stat.ML</categories><comments>13 pages, 3 figures, 6 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Protein structure prediction has been a grand challenge problem in the
structure biology over the last few decades. Protein quality assessment plays a
very important role in protein structure prediction. In the paper, we propose a
new protein quality assessment method which can predict both local and global
quality of the protein 3D structural models. Our method uses both multi and
single model quality assessment method for global quality assessment, and uses
chemical, physical, geo-metrical features, and global quality score for local
quality assessment. CASP9 targets are used to generate the features for local
quality assessment. We evaluate the performance of our local quality assessment
method on CASP10, which is comparable with two stage-of-art QA methods based on
the average absolute distance between the real and predicted distance. In
addition, we blindly tested our method on CASP11, and the good performance
shows that combining single and multiple model quality assessment method could
be a good way to improve the accuracy of model quality assessment, and the
random forest technique could be used to train a good local quality assessment
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04278</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04278</id><created>2016-02-12</created><authors><author><keyname>Kim</keyname><forenames>Taehwan</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author></authors><title>Signer-independent Fingerspelling Recognition with Deep Neural Network
  Adaptation</title><categories>cs.CL cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of recognition of fingerspelled letter sequences in
American Sign Language in a signer-independent setting. Fingerspelled sequences
are both challenging and important to recognize, as they are used for many
content words such as proper nouns and technical terms. Previous work has shown
that it is possible to achieve almost 90% accuracies on fingerspelling
recognition in a signer-dependent setting. However, the more realistic
signer-independent setting presents challenges due to significant variations
among signers, coupled with the dearth of available training data. We
investigate this problem with approaches inspired by automatic speech
recognition. We start with the best-performing approaches from prior work,
based on tandem models and segmental conditional random fields (SCRFs), with
features based on deep neural network (DNN) classifiers of letters and
phonological features. Using DNN adaptation, we find that it is possible to
bridge a large part of the gap between signer-dependent and signer-independent
performance. Using only about 115 transcribed words for adaptation from the
target signer, we obtain letter accuracies of up to 82.7% with frame-level
adaptation labels and 69.7% with only word labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04281</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04281</id><created>2016-02-12</created><authors><author><keyname>Bolten</keyname><forenames>Nicholas</forenames></author><author><keyname>Amini</keyname><forenames>Amirhossein</forenames></author><author><keyname>Hao</keyname><forenames>Yun</forenames></author><author><keyname>Ravichandran</keyname><forenames>Vaishnavi</forenames></author><author><keyname>Stephens</keyname><forenames>Andre</forenames></author><author><keyname>Caspi</keyname><forenames>Anat</forenames></author></authors><title>Urban sidewalks: visualization and routing for individuals with limited
  mobility</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People with limited mobility in the U.S. (defined as having difficulty or
inability to walk a quarter of a mile without help and without the use of
special equipment) face a growing informational gap: while pedestrian routing
algorithms are getting faster and more informative, planning a route with a
wheeled device in urban centers is very difficult due to lack of integrated
pertinent information regarding accessibility along the route. Moreover,
reducing access to street-spaces translates to reduced access to other public
information and services that are increasingly made available to the public
along urban streets. To adequately plan a commute, a traveler with limited or
wheeled mobility must know whether her path may be blocked by construction,
whether the sidewalk would be too steep or rendered unusable due to poor
conditions, whether the street can be crossed or a highway is blocking the way,
or whether there is a sidewalk at all. These details populate different
datasets in many modern municipalities, but they are not immediately available
in a convenient, integrated format to be useful to people with limited
mobility. Our project, AccessMap, in its first phase (v.1) overlayed the
information that is most relevant to people with limited mobility on a map,
enabling self-planning of routes. Here, we describe the next phase of the
project: synthesizing commonly available open data (including streets,
sidewalks, curb ramps, elevation data, and construction permit information) to
generate a graph of paths to enable variable cost-function accessible routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04282</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04282</id><created>2016-02-12</created><authors><author><keyname>Wu</keyname><forenames>Yifan</forenames></author><author><keyname>Shariff</keyname><forenames>Roshan</forenames></author><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Szepesv&#xe1;ri</keyname><forenames>Csaba</forenames></author></authors><title>Conservative Bandits</title><categories>stat.ML cs.LG</categories><comments>9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a novel multi-armed bandit problem that models the challenge faced
by a company wishing to explore new strategies to maximize revenue whilst
simultaneously maintaining their revenue above a fixed baseline, uniformly over
time. While previous work addressed the problem under the weaker requirement of
maintaining the revenue constraint only at a given fixed time in the future,
the algorithms previously proposed are unsuitable due to their design under the
more stringent constraints. We consider both the stochastic and the adversarial
settings, where we propose, natural, yet novel strategies and analyze the price
for maintaining the constraints. Amongst other things, we prove both high
probability and expectation bounds on the regret, while we also consider both
the problem of maintaining the constraints with high probability or
expectation. For the adversarial setting the price of maintaining the
constraint appears to be higher, at least for the algorithm considered. A lower
bound is given showing that the algorithm for the stochastic setting is almost
optimal. Empirical results obtained in synthetic environments complement our
theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04283</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04283</id><created>2016-02-12</created><authors><author><keyname>Lacey</keyname><forenames>Griffin</forenames></author><author><keyname>Taylor</keyname><forenames>Graham W.</forenames></author><author><keyname>Areibi</keyname><forenames>Shawki</forenames></author></authors><title>Deep Learning on FPGAs: Past, Present, and Future</title><categories>cs.DC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of data size and accessibility in recent years has
instigated a shift of philosophy in algorithm design for artificial
intelligence. Instead of engineering algorithms by hand, the ability to learn
composable systems automatically from massive amounts of data has led to
ground-breaking performance in important domains such as computer vision,
speech recognition, and natural language processing. The most popular class of
techniques used in these domains is called deep learning, and is seeing
significant attention from industry. However, these models require incredible
amounts of data and compute power to train, and are limited by the need for
better hardware acceleration to accommodate scaling beyond current data and
model sizes. While the current solution has been to use clusters of graphics
processing units (GPU) as general purpose processors (GPGPU), the use of field
programmable gate arrays (FPGA) provide an interesting alternative. Current
trends in design tools for FPGAs have made them more compatible with the
high-level software practices typically practiced in the deep learning
community, making FPGAs more accessible to those who build and deploy models.
Since FPGA architectures are flexible, this could also allow researchers the
ability to explore model-level optimizations beyond what is possible on fixed
architectures such as GPUs. As well, FPGAs tend to provide high performance per
watt of power consumption, which is of particular importance for application
scientists interested in large scale server-based deployment or
resource-limited embedded applications. This review takes a look at deep
learning and FPGAs from a hardware acceleration perspective, identifying trends
and innovations that make these technologies a natural fit, and motivates a
discussion on how FPGAs may best serve the needs of the deep learning community
moving forward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04286</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04286</id><created>2016-02-12</created><authors><author><keyname>Kulumani</keyname><forenames>Shankar</forenames></author><author><keyname>Poole</keyname><forenames>Christopher</forenames></author><author><keyname>Lee</keyname><forenames>Taeyoung</forenames></author></authors><title>Geometric Adaptive Control of Attitude Dynamics on SO(3) with State
  Inequality Constraints</title><categories>math.OC cs.SY</categories><comments>2016 American Control Conference (ACC)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a new geometric adaptive control system with state
inequality constraints for the attitude dynamics of a rigid body. The control
system is designed such that the desired attitude is asymptotically stabilized,
while the controlled attitude trajectory avoids undesired regions defined by an
inequality constraint. In addition, we develop an adaptive update law that
enables attitude stabilization in the presence of unknown disturbances. The
attitude dynamics and the proposed control systems are developed on the special
orthogonal group such that singularities and ambiguities of other attitude
parameterizations, such as Euler angles and quaternions are completely avoided.
The effectiveness of the proposed control system is demonstrated through
numerical simulations and experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04287</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04287</id><created>2016-02-12</created><authors><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Jing</forenames></author><author><keyname>Fienberg</keyname><forenames>Stephen E.</forenames></author></authors><title>A Minimax Theory for Adaptive Data Analysis</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In adaptive data analysis, the user makes a sequence of queries on the data,
where at each step the choice of query may depend on the results in previous
steps. The releases are often randomized in order to reduce overfitting for
such adaptively chosen queries. In this paper, we propose a minimax framework
for adaptive data analysis. Assuming Gaussianity of queries, we establish the
first sharp minimax lower bound on the squared error in the order of
$O(\frac{\sqrt{k}\sigma^2}{n})$, where $k$ is the number of queries asked, and
$\sigma^2/n$ is the ordinary signal-to-noise ratio for a single query. Our
lower bound is based on the construction of an approximately least favorable
adversary who picks a sequence of queries that are most likely to be affected
by overfitting. This approximately least favorable adversary uses only one
level of adaptivity, suggesting that the minimax risk for 1-step adaptivity
with k-1 initial releases and that for $k$-step adaptivity are on the same
order. The key technical component of the lower bound proof is a reduction to
finding the convoluting distribution that optimally obfuscates the sign of a
Gaussian signal. Our lower bound construction also reveals a transparent and
elementary proof of the matching upper bound as an alternative approach to
Russo and Zou (2015), who used information-theoretic tools to provide the same
upper bound. We believe that the proposed framework opens up opportunities to
obtain theoretical insights for many other settings of adaptive data analysis,
which would extend the idea to more practical realms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04290</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04290</id><created>2016-02-13</created><authors><author><keyname>Knuth</keyname><forenames>Kevin H.</forenames></author><author><keyname>Erner</keyname><forenames>Philip M.</forenames></author><author><keyname>Frasso</keyname><forenames>Scott</forenames></author></authors><title>Designing Intelligent Instruments</title><categories>cs.AI cs.RO</categories><comments>9 pages, 2 figures. Published in the MaxEnt 2007 Proceedings</comments><journal-ref>AIP Conference Proceedings 954, American Institute of Physics,
  Melville NY, 203-211, 2007</journal-ref><doi>10.1063/1.2821263</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote science operations require automated systems that can both act and
react with minimal human intervention. One such vision is that of an
intelligent instrument that collects data in an automated fashion, and based on
what it learns, decides which new measurements to take. This innovation
implements experimental design and unites it with data analysis in such a way
that it completes the cycle of learning. This cycle is the basis of the
Scientific Method.
  The three basic steps of this cycle are hypothesis generation, inquiry, and
inference. Hypothesis generation is implemented by artificially supplying the
instrument with a parameterized set of possible hypotheses that might be used
to describe the physical system. The act of inquiry is handled by an inquiry
engine that relies on Bayesian adaptive exploration where the optimal
experiment is chosen as the one which maximizes the expected information gain.
The inference engine is implemented using the nested sampling algorithm, which
provides the inquiry engine with a set of posterior samples from which the
expected information gain can be estimated. With these computational structures
in place, the instrument will refine its hypotheses, and repeat the learning
cycle by taking measurements until the system under study is described within a
pre-specified tolerance. We will demonstrate our first attempts toward
achieving this goal with an intelligent instrument constructed using the LEGO
MINDSTORMS NXT robotics platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04294</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04294</id><created>2016-02-13</created><authors><author><keyname>Vasile</keyname><forenames>Cristian-Ioan</forenames></author><author><keyname>Aksaray</keyname><forenames>Derya</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Time Window Temporal Logic</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces time window temporal logic (TWTL), a rich expressivity
language for describing various time bounded specifications. In particular, the
syntax and semantics of TWTL enable the compact representation of serial tasks,
which are typically seen in robotics and control applications. This paper also
discusses the relaxation of TWTL formulae with respect to deadlines of tasks.
Efficient automata-based frameworks to solve synthesis, verification and
learning problems are also presented. The key ingredient to the presented
solution is an algorithm to translate a TWTL formula to an annotated finite
state automaton that encodes all possible temporal relaxations of the
specification. Case studies illustrating the expressivity of the logic and the
proposed algorithms are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04300</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04300</id><created>2016-02-13</created><authors><author><keyname>Francis</keyname><forenames>Maria</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>A Gr\&quot;obner Basis Algorithm for Computing the Krull Dimension of
  $A$-Algebras</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an ideal, $\mathfrak{a}$ in $A[x_1, \ldots, x_n]$, where $A$ is a
Noetherian, commutative ring, we give a Gr\&quot;obner basis algorithm to determine
the Krull dimension of $A[x_1,\ldots,x_n]/\mathfrak{a}$. When $A$ is a field,
Krull dimension is equal to combinatorial dimension, but this may not be true
in the case of arbitrary Noetherian rings. We first extend the characterization
given in (Francis \&amp; Dukkipati, 2014) for a finitely generated $A$-module,
$A[x_1,\ldots,x_n]/\mathfrak{a}$, to have a free $A$-module representation
w.r.t. a monomial order to the infinite case. We also introduce the notion of
combinatorial dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$ and give a
Gr\&quot;obner basis algorithm to compute it for $A$-algebras that have a free
$A$-module representation w.r.t. a lexicographic monomial ordering. For such
$A$-algebras, we derive a relation between Krull dimension and combinatorial
dimension. An immediate application of this relation is that it gives a uniform
method to compute the dimension of $A[x_1, \ldots, x_n]/\mathfrak{a}$ without
having to consider individual properties of the ideal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04301</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04301</id><created>2016-02-13</created><updated>2016-02-16</updated><authors><author><keyname>Deng</keyname><forenames>Dingxiong</forenames></author><author><keyname>Shahabi</keyname><forenames>Cyrus</forenames></author><author><keyname>Demiryurek</keyname><forenames>Ugur</forenames></author><author><keyname>Zhu</keyname><forenames>Linhong</forenames></author><author><keyname>Yu</keyname><forenames>Rose</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>Latent Space Model for Road Networks to Predict Time-Varying Traffic</title><categories>cs.SI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor
datasets is an important problem for intelligent transportation systems and
sustainability. However, it is challenging due to the complex topological
dependencies and high dynamics associated with changing road conditions. In
this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to
address these challenges. In particular, given a series of road network
snapshots, we learn the attributes of vertices in latent spaces which capture
both topological and temporal properties. As these latent attributes are
time-dependent, they can estimate how traffic patterns form and evolve. In
addition, we present an incremental online algorithm which sequentially and
adaptively learn the latent attributes from the temporal graph changes. Our
framework enables real-time traffic prediction by 1) exploiting real-time
sensor readings to adjust/update the existing latent spaces, and 2) training as
data arrives and making predictions on-the-fly with given data. By conducting
extensive experiments with a large volume of real-world traffic sensor data, we
demonstrate the utility superiority of our framework for real-time traffic
prediction on large road networks over competitors as well as a baseline
graph-based LSM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04302</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04302</id><created>2016-02-13</created><updated>2016-02-17</updated><authors><author><keyname>Yuan</keyname><forenames>Ganzhao</forenames></author><author><keyname>Yang</keyname><forenames>Yin</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenjie</forenames></author><author><keyname>Hao</keyname><forenames>Zhifeng</forenames></author></authors><title>Optimal Linear Aggregate Query Processing under Approximate Differential
  Privacy</title><categories>cs.DB cs.LG stat.ML</categories><comments>16 pages, 48 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy enables organizations to collect accurate aggregates
over sensitive data with strong, rigorous guarantees on individuals' privacy.
Previous work has found that under differential privacy, computing multiple
correlated aggregates as a batch, using an appropriate \emph{strategy}, may
yield higher accuracy than computing each of them independently. However,
finding the optimal strategy that maximizes result accuracy is non-trivial, as
it involves solving a complex constrained optimization program that appears to
be non-linear and non-convex. Hence, in the past much effort has been devoted
in solving this non-convex optimization program. Existing approaches include
various sophisticated heuristics and expensive numerical solutions. None of
them, however, guarantees to find the optimal strategy.
  This paper points out that under ($\epsilon$, $\delta$)-differential privacy,
the optimal strategy for answering an arbitrary batch of linear aggregate
queries can be found, rather surprisingly, by solving a simple and elegant
convex optimization program. Then, we propose an efficient algorithm based on
Newton's method, which we prove to always converge to the optimal solution with
linear global convergence rate and quadratic local convergence rate. Empirical
evaluations demonstrate the accuracy and efficiency of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04328</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04328</id><created>2016-02-13</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Molinero</keyname><forenames>Xavier</forenames></author><author><keyname>Olsen</keyname><forenames>Martin</forenames></author><author><keyname>Serna</keyname><forenames>Maria</forenames></author></authors><title>Dimension and codimension of simple games</title><categories>cs.GT</categories><comments>5 pages</comments><msc-class>91B12, 91A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the complexity of computing a representation of a simple
game as the intersection (union) of weighted majority games, as well as, the
dimension or the codimension. We also present some examples with linear
dimension and exponential codimension with respect to the number of players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04329</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04329</id><created>2016-02-13</created><updated>2016-02-19</updated><authors><author><keyname>Lu</keyname><forenames>Lu</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author></authors><title>Diffusion leaky LMS algorithm: analysis and implementation</title><categories>cs.SY</categories><comments>fast communication, Begin in 7. Feb., 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diffusion least-mean square (dLMS) algorithms have attracted much
attention owing to its robustness for distributed estimation problems. However,
the performance of such filters may change when they are implemented for
suppressing noises from speech signals. To overcome this problem, a diffusion
leaky dLMS algorithm is proposed in this work, which is characterized by its
numerical stability and small misadjustment for noisy speech signals when the
unknown system is a lowpass filter. Then, we present some convergence analyses
of proposed algorithm for Gaussian input. Finally, two implementations of the
leaky dLMS are introduced. It is demonstrated that the leaky dLMS can be
effectively introduced into a noise reduction network for speech signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04330</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04330</id><created>2016-02-13</created><authors><author><keyname>Kelma</keyname><forenames>Florian</forenames></author><author><keyname>Kent</keyname><forenames>John T.</forenames></author><author><keyname>Hotz</keyname><forenames>Thomas</forenames></author></authors><title>On the Topology of Projective Shape Spaces</title><categories>math.ST cs.CV math.GT stat.TH</categories><msc-class>51H25 (Primary), 68U05 (Secondary)</msc-class><acm-class>I.4.1; I.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The projective shape of a configuration consists of the information that is
invariant under projective transformations. It encodes the information about an
object reconstructable from uncalibrated camera views. The space of projective
shapes of k points in d-dimensional real projective space is by definition the
quotient space of k copies of that projective space modulo the action of the
projective linear group. A detailed examination of the topology of projective
shape space is given, and it is shown how to derive subsets that are maximal
Hausdorff manifolds. A special case are Tyler regular shapes for which one can
construct a Riemannian metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04331</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04331</id><created>2016-02-13</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Generalized roll-call model for the Shapley-Shubik index</title><categories>math.CO cs.GT</categories><comments>18 pages</comments><msc-class>91A12, 91A40, 91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1996 Dan Felsenthal and Mosh\'e Machover considered the following model.
An assembly consisting of $n$ voters exercises roll-call. All $n!$ possible
orders in which the voters may be called are assumed to be equiprobable. The
votes of each voter are independent with expectation $0&lt;p&lt;1$ for an individual
vote {\lq\lq}yea{\rq\rq}. For a given decision rule $v$ the \emph{pivotal}
voter in a roll-call is the one whose vote finally decides the aggregated
outcome. It turned out that the probability to be pivotal is equivalent to the
Shapley-Shubik index. Here we give an easy combinatorial proof of this
coincidence, further weaken the assumptions of the underlying model, and study
generalizations to the case of more than two alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04335</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04335</id><created>2016-02-13</created><authors><author><keyname>Salehinejad</keyname><forenames>Hojjat</forenames></author></authors><title>Learning Over Long Time Lags</title><categories>cs.NE</categories><comments>This is a draft article, in preparation to submit for peer-review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advantage of recurrent neural networks (RNNs) in learning dependencies
between time-series data has distinguished RNNs from other deep learning
models. Recently, many advances are proposed in this emerging field. However,
there is a lack of comprehensive review on memory models in RNNs in the
literature. This paper provides a fundamental review on RNNs and long short
term memory (LSTM) model. Then, provides a surveys of recent advances in
different memory enhancements and learning techniques for capturing long term
dependencies in RNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04339</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04339</id><created>2016-02-13</created><authors><author><keyname>Maletzky</keyname><forenames>Alexander</forenames></author></authors><title>Mathematical Theory Exploration in Theorema: Reduction Rings</title><categories>cs.SC cs.LO math.AC</categories><comments>submitted to CICM 2015 (Conference on Intelligent Computer
  Mathematics)</comments><msc-class>13P10</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the first-ever computer formalization of the theory
of Gr\&quot;obner bases in reduction rings, which is an important theory in
computational commutative algebra, in Theorema. Not only the formalization, but
also the formal verification of all results has already been fully completed by
now; this, in particular, includes the generic implementation and correctness
proof of Buchberger's algorithm in reduction rings. Thanks to the seamless
integration of proving and computing in Theorema, this implementation can now
be used to compute Gr\&quot;obner bases in various different domains directly within
the system. Moreover, a substantial part of our formalization is made up solely
by &quot;elementary theories&quot; such as sets, numbers and tuples that are themselves
independent of reduction rings and may therefore be used as the foundations of
future theory explorations in Theorema.
  In addition, we also report on two general-purpose Theorema tools we
developed for an efficient and convenient exploration of mathematical theories:
an interactive proving strategy and a &quot;theory analyzer&quot; that already proved
extremely useful when creating large structured knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04341</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04341</id><created>2016-02-13</created><authors><author><keyname>Yin</keyname><forenames>Wenpeng</forenames></author><author><keyname>Ebert</keyname><forenames>Sebastian</forenames></author><author><keyname>Sch&#xfc;tze</keyname><forenames>Hinrich</forenames></author></authors><title>Attention-Based Convolutional Neural Network for Machine Comprehension</title><categories>cs.CL</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding open-domain text is one of the primary challenges in natural
language processing (NLP). Machine comprehension benchmarks evaluate the
system's ability to understand text based on the text content only. In this
work, we investigate machine comprehension on MCTest, a question answering (QA)
benchmark. Prior work is mainly based on feature engineering approaches. We
come up with a neural network framework, named hierarchical attention-based
convolutional neural network (HABCNN), to address this task without any
manually designed features. Specifically, we explore HABCNN for this task by
two routes, one is through traditional joint modeling of passage, question and
answer, one is through textual entailment. HABCNN employs an attention
mechanism to detect key phrases, key sentences and key snippets that are
relevant to answering the question. Experiments show that HABCNN outperforms
prior deep learning approaches by a big margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04345</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04345</id><created>2016-02-13</created><authors><author><keyname>Joudeh</keyname><forenames>Hamdi</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Robust Transmission in Downlink Multiuser MISO Systems: A Rate-Splitting
  Approach</title><categories>cs.IT math.IT</categories><comments>submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a downlink multiuser MISO system with bounded errors in the
Channel State Information at the Transmitter (CSIT). We first look at the
robust design problem of achieving max-min fairness amongst users (in the
worst-case sense). Contrary to the conventional approach adopted in literature,
we propose a rather unorthodox design based on a Rate-Splitting (RS) strategy.
Each user's message is split into two parts, a common part and a private part.
All common parts are packed into one super common message encoded using a
public codebook, while private parts are independently encoded. The resulting
symbol streams are linearly precoded and simultaneously transmitted, and each
receiver retrieves its intended message by decoding both the common stream and
its corresponding private stream. For CSIT uncertainty regions that scale with
SNR (e.g. by scaling the number of feedback bits), we prove that a RS-based
design achieves higher max-min Degrees of Freedom (DoF) compared to
conventional designs (NoRS). For the special case of non-scaling CSIT (e.g.
fixed number of feedback bits), and contrary to NoRS, RS can achieve a
non-saturating max-min rate. We propose a robust algorithm based on the
cutting-set method coupled with the Rate-Weighted Mean Square Error (WMSE)
approach, and we demonstrate its performance gains over state-of-the art
designs. Finally, we extend the RS strategy to address the Quality of Service
(QoS) constrained power minimization problem, and we demonstrate significant
gains over NoRS-based designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04348</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04348</id><created>2016-02-13</created><authors><author><keyname>Zhang</keyname><forenames>Shuye</forenames></author><author><keyname>Lin</keyname><forenames>Mude</forenames></author><author><keyname>Chen</keyname><forenames>Tianshui</forenames></author><author><keyname>Jin</keyname><forenames>Lianwen</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>Character Proposal Network for Robust Text Extraction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximally stable extremal regions (MSER), which is a popular method to
generate character proposals/candidates, has shown superior performance in
scene text detection. However, the pixel-level operation limits its capability
for handling some challenging cases (e.g., multiple connected characters,
separated parts of one character and non-uniform illumination). To better
tackle these cases, we design a character proposal network (CPN) by taking
advantage of the high capacity and fast computing of fully convolutional
network (FCN). Specifically, the network simultaneously predicts characterness
scores and refines the corresponding locations. The characterness scores can be
used for proposal ranking to reject non-character proposals and the refining
process aims to obtain the more accurate locations. Furthermore, considering
the situation that different characters have different aspect ratios, we
propose a multi-template strategy, designing a refiner for each aspect ratio.
The extensive experiments indicate our method achieves recall rates of 93.88%,
93.60% and 96.46% on ICDAR 2013, SVT and Chinese2k datasets respectively using
less than 1000 proposals, demonstrating promising performance of our character
proposal network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04353</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04353</id><created>2016-02-13</created><authors><author><keyname>Barto</keyname><forenames>Libor</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author></authors><title>The algebraic dichotomy conjecture for infinite domain Constraint
  Satisfaction Problems</title><categories>cs.LO cs.CC math.LO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that an $\omega$-categorical core structure primitively positively
interprets all finite structures with parameters if and only if some stabilizer
of its polymorphism clone has a homomorphism to the clone of projections, and
that this happens if and only if its polymorphism clone does not contain
operations $\alpha$, $\beta$, $s$ satisfying the identity $\alpha
s(x,y,x,z,y,z) \approx \beta s(y,x,z,x,z,y)$.
  This establishes an algebraic criterion equivalent to the conjectured
borderline between P and NP-complete CSPs over reducts of finitely bounded
homogenous structures, and accomplishes one of the steps of a proposed strategy
for reducing the infinite domain CSP dichotomy conjecture to the finite case.
  Our theorem is also of independent mathematical interest, characterizing a
topological property of any $\omega$-categorical core structure (the existence
of a continuous homomorphism of a stabilizer of its polymorphism clone to the
projections) in purely algebraic terms (the failure of an identity as above).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04358</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04358</id><created>2016-02-13</created><authors><author><keyname>Gugel</keyname><forenames>Leonid</forenames></author><author><keyname>Shkolnisky</keyname><forenames>Yoel</forenames></author><author><keyname>Dekel</keyname><forenames>Shai</forenames></author></authors><title>Machine olfaction using time scattering of sensor multiresolution graphs</title><categories>cs.AI cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct a learning architecture for high dimensional time
series sampled by sensor arrangements. Using a redundant wavelet decomposition
on a graph constructed over the sensor locations, our algorithm is able to
construct discriminative features that exploit the mutual information between
the sensors. The algorithm then applies scattering networks to the time series
graphs to create the feature space. We demonstrate our method on a machine
olfaction problem, where one needs to classify the gas type and the location
where it originates from data sampled by an array of sensors. Our experimental
results clearly demonstrate that our method outperforms classical machine
learning techniques used in previous studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04364</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04364</id><created>2016-02-13</created><authors><author><keyname>Ren</keyname><forenames>Jimmy</forenames></author><author><keyname>Hu</keyname><forenames>Yongtao</forenames></author><author><keyname>Tai</keyname><forenames>Yu-Wing</forenames></author><author><keyname>Wang</keyname><forenames>Chuan</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Sun</keyname><forenames>Wenxiu</forenames></author><author><keyname>Yan</keyname><forenames>Qiong</forenames></author></authors><title>Look, Listen and Learn - A Multimodal LSTM for Speaker Identification</title><categories>cs.LG</categories><comments>The 30th AAAI Conference on Artificial Intelligence (AAAI-16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speaker identification refers to the task of localizing the face of a person
who has the same identity as the ongoing voice in a video. This task not only
requires collective perception over both visual and auditory signals, the
robustness to handle severe quality degradations and unconstrained content
variations are also indispensable. In this paper, we describe a novel
multimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies
both visual and auditory modalities from the beginning of each sequence input.
The key idea is to extend the conventional LSTM by not only sharing weights
across time steps, but also sharing weights across modalities. We show that
modeling the temporal dependency across face and voice can significantly
improve the robustness to content quality degradations and variations. We also
found that our multimodal LSTM is robustness to distractors, namely the
non-speaking identities. We applied our multimodal LSTM to The Big Bang Theory
dataset and showed that our system outperforms the state-of-the-art systems in
speaker identification with lower false alarm rate and higher recognition
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04365</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04365</id><created>2016-02-13</created><authors><author><keyname>D&#xfc;rr</keyname><forenames>Christoph</forenames></author><author><keyname>Hanz&#xe1;lek</keyname><forenames>Zden&#x11b;k</forenames></author><author><keyname>Konrad</keyname><forenames>Christian</forenames></author><author><keyname>Sitters</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>V&#xe1;squez</keyname><forenames>&#xd3;scar C.</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard</forenames></author></authors><title>The triangle scheduling problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel scheduling problem, where jobs occupy a
triangular shape on the time line. This problem is motivated by scheduling jobs
with different criticality levels. A measure is introduced, namely the binary
tree ratio. It is shown that the greedy algorithm solves the problem to
optimality when the binary tree ratio is at most 2. We also show that the
problem is unary NP-hard for instances with binary tree ratio strictly larger
than 2, and provide a quasi polynomial time approximation scheme (QPTAS). The
approximation ratio of Greedy on general instances is shown to be between 1.5
and 25/24.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04368</identifier>
 <datestamp>2016-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04368</id><created>2016-02-13</created><authors><author><keyname>Kirkpatrick</keyname><forenames>Bonnie</forenames></author></authors><title>Fast Computation of the Kinship Coefficients</title><categories>cs.DS q-bio.PE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For families, kinship coefficients are quantifications of the amount of
genetic sharing between a pair of individuals. These coefficients are critical
for understanding the breeding habits and genetic diversity of diploid
populations. Historically, computations of the inbreeding coefficient were used
to prohibit inbred marriages and prohibit breeding of some pairs of pedigree
animals. Such prohibitions foster genetic diversity and help prevent recessive
Mendelian disease at a population level.
  This paper gives the fastest known algorithms for computing the kinship
coefficient of a set of individuals with a known pedigree. The algorithms given
here consider the possibility that the founders of the known pedigree may
themselves be inbred, and they compute the appropriate inbreeding-adjusted
kinship coefficients. The exact kinship algorithm has running-time $O(n^2)$ for
an $n$-individual pedigree. The recursive-cut exact kinship algorithm has
running time $O(s^2m)$ where $s$ is the number of individuals in the largest
segment of the pedigree and $m$ is the number of cuts. The approximate
algorithm has running-time $O(n)$ for an $n$-individual pedigree on which to
estimate the kinship coefficients of $\sqrt{n}$ individuals from $\sqrt{n}$
founder kinship coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04375</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04375</id><created>2016-02-13</created><authors><author><keyname>Sachan</keyname><forenames>Mrinmaya</forenames></author><author><keyname>Dubey</keyname><forenames>Avinava</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Science Question Answering using Instructional Materials</title><categories>cs.CL cs.AI cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a solution for elementary science test using instructional
materials. We posit that there is a hidden structure that explains the
correctness of an answer given the question and instructional materials and
present a unified max-margin framework that learns to find these hidden
structures (given a corpus of question-answer pairs and instructional
materials), and uses what it learns to answer novel elementary science
questions. Our evaluation shows that our framework outperforms several strong
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04376</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04376</id><created>2016-02-13</created><authors><author><keyname>Fahad</keyname><forenames>Muhammad</forenames></author></authors><title>BPCMont: Business Process Change Management Ontology</title><categories>cs.AI</categories><comments>5 pages, 7 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Change management for evolving collaborative business process development is
crucial when the business logic, transections and workflow change due to
changes in business strategies or organizational and technical environment.
During the change implementation, business processes are analyzed and improved
ensuring that they capture the proposed change and they do not contain any
undesired functionalities or change side-effects. This paper presents Business
Process Change Management approach for the efficient and effective
implementation of change in the business process. The key technology behind our
approach is our proposed Business Process Change Management Ontology (BPCMont)
which is the main contribution of this paper. BPCMont, as a formalized change
specification, helps to revert BP into a consistent state in case of system
crash, intermediate conflicting stage or unauthorized change done, aid in
change traceability in the new and old versions of business processes, change
effects can be seen and estimated effectively, ease for Stakeholders to
validate and verify change implementation, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1602.04381</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1602.04381</id><created>2016-02-13</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Ghosh</keyname><forenames>Anirban</forenames></author></authors><title>Lattice spanners of low degree</title><categories>math.MG cs.CG</categories><comments>15 pages, 11 figures; A preliminary version in: Proceedings of the
  2nd International Conference on Algorithms and Discrete Applied Mathematics,
  Thiruvananthapuram, India, Febr. 2016, vol $9602$ of LNCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\delta_0(P,k)$ denote the degree $k$ dilation of a point set~$P$ in the
domain of plane geometric spanners. If $\Lambda$ is the infinite square
lattice, it is shown that $1+\sqrt{2} \leq \delta_0(\Lambda,3) \leq (3+2\sqrt2)
\, 5^{-1/2} = 2.6065\ldots$ and $\delta_0(\Lambda,4) = \sqrt{2}$. If $\Lambda$
is the infinite hexagonal lattice, it is shown that $\delta_0(\Lambda,3) =
1+\sqrt{3}$ and $\delta_0(\Lambda,4) = 2$. All our constructions are planar
lattice tilings constrained to degree $3$ or $4$.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="91000" completeListSize="102538">1122234|92001</resumptionToken>
</ListRecords>
</OAI-PMH>
