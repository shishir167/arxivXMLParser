<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:57:35Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|85001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01648</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01648</id><created>2015-10-06</created><authors><author><keyname>Chen</keyname><forenames>George</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author><author><keyname>Golland</keyname><forenames>Polina</forenames></author></authors><title>A Latent Source Model for Patch-Based Image Segmentation</title><categories>cs.CV</categories><comments>International Conference on Medical Image Computing and Computer
  Assisted Interventions 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the popularity and empirical success of patch-based nearest-neighbor
and weighted majority voting approaches to medical image segmentation, there
has been no theoretical development on when, why, and how well these
nonparametric methods work. We bridge this gap by providing a theoretical
performance guarantee for nearest-neighbor and weighted majority voting
segmentation under a new probabilistic model for patch-based image
segmentation. Our analysis relies on a new local property for how similar
nearby patches are, and fuses existing lines of work on modeling natural
imagery patches and theory for nonparametric classification. We use the model
to derive a new patch-based segmentation algorithm that iterates between
inferring local label patches and merging these local segmentations to produce
a globally consistent image segmentation. Many existing patch-based algorithms
arise as special cases of the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01659</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01659</id><created>2015-10-06</created><authors><author><keyname>Fahad</keyname><forenames>Muhammad</forenames></author></authors><title>DKP-AOM: results for OAEI 2015</title><categories>cs.AI</categories><comments>8 pages, 3 figures, 3 tables, initial results of OM workshop,
  Ontology Matching Workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the results obtained by our DKP-AOM system within
the OAEI 2015 campaign. DKP-AOM is an ontology merging tool designed to merge
heterogeneous ontologies. In OAEI, we have participated with its ontology
mapping component which serves as a basic module capable of matching large
scale ontologies before their merging. This is our first successful
participation in the Conference, OA4QA and Anatomy track of OAEI. DKP-AOM is
participating with two versions (DKP-AOM and DKP-AOM_lite), DKP-AOM performs
coherence analysis. In OA4QA track, DKPAOM out-performed in the evaluation and
generated accurate alignments allowed to answer all the queries of the
evaluation. We can also see its competitive results for the conference track in
the evaluation initiative among other reputed systems. In the anatomy track, it
has produced alignments within an allocated time and appeared in the list of
systems which produce coherent results. Finally, we discuss some future work
towards the development of DKP-AOM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01663</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01663</id><created>2015-10-06</created><authors><author><keyname>Vupparaboina</keyname><forenames>Kiran Kumar</forenames></author><author><keyname>Raghavan</keyname><forenames>Kamala</forenames></author><author><keyname>Jana</keyname><forenames>Soumya</forenames></author></authors><title>Euclidean Auto Calibration of Camera Networks: Baseline Constraint
  Removes Scale Ambiguity</title><categories>cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metric auto calibration of a camera network from multiple views has been
reported by several authors. Resulting 3D reconstruction recovers shape
faithfully, but not scale. However, preservation of scale becomes critical in
applications, such as multi-party telepresence, where multiple 3D scenes need
to be fused into a single coordinate system. In this context, we propose a
camera network configuration that includes a stereo pair with known baseline
separation, and analytically demonstrate Euclidean auto calibration of such
network under mild conditions. Further, we experimentally validate our theory
using a four-camera network. Importantly, our method not only recovers scale,
but also compares favorably with the well known Zhang and Pollefeys methods in
terms of shape recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01665</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01665</id><created>2015-10-06</created><updated>2015-10-19</updated><authors><author><keyname>Osmani</keyname><forenames>Venet</forenames></author><author><keyname>Gruenerbl</keyname><forenames>Agnes</forenames></author><author><keyname>Bahle</keyname><forenames>Gernot</forenames></author><author><keyname>Haring</keyname><forenames>Christian</forenames></author><author><keyname>Lukowicz</keyname><forenames>Paul</forenames></author><author><keyname>Mayora</keyname><forenames>Oscar</forenames></author></authors><title>Smartphones in Mental Health: Detecting Depressive and Manic Episodes</title><categories>cs.HC</categories><journal-ref>IEEE Pervasive Computing, vol.14, no. 3, pp. 10-13, July-Sept.
  2015</journal-ref><doi>10.1109/MPRV.2015.54</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An observational study with patients diagnosed with bipolar disorder
investigates whether data from smartphone sensors can be used to recognize
bipolar disorder episodes and detect behavior changes that can signal an onset
of an episode using objective data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01670</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01670</id><created>2015-10-06</created><updated>2015-10-08</updated><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Sketching for Simultaneously Sparse and Low-Rank Covariance Matrices</title><categories>cs.IT math.IT math.NA math.ST stat.TH</categories><comments>Accepted in 2015 IEEE International Workshop on Computational
  Advances in Multi-Sensor Adaptive Processing (CAMSAP 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a technique for estimating a structured covariance matrix from
observations of a random vector which have been sketched. Each observed random
vector $\boldsymbol{x}_t$ is reduced to a single number by taking its inner
product against one of a number of pre-selected vector $\boldsymbol{a}_\ell$.
These observations are used to form estimates of linear observations of the
covariance matrix $\boldsymbol{\varSigma}$, which is assumed to be
simultaneously sparse and low-rank. We show that if the sketching vectors
$\boldsymbol{a}_\ell$ have a special structure, then we can use straightforward
two-stage algorithm that exploits this structure. We show that the estimate is
accurate when the number of sketches is proportional to the maximum of the rank
times the number of significant rows/columns of $\boldsymbol{\varSigma}$.
Moreover, our algorithm takes direct advantage of the low-rank structure of
$\boldsymbol{\varSigma}$ by only manipulating matrices that are far smaller
than the original covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01671</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01671</id><created>2015-10-06</created><updated>2016-01-09</updated><authors><author><keyname>Riedel</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Cross-boundary Behavioural Reprogrammability Reveals Evidence of
  Pervasive Turing Universality</title><categories>cs.FL cs.CC nlin.CG</categories><comments>20 main pages, 30 including Sup. Mat., 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a reasonable framework to ask the question
of estimating the density of Turing universality and the reprogramming
capabilities of random computer programs. By means of a natural enumeration and
based upon the concept of 'intrinsic universality', we provide strong evidence
of ubiquitous Turing-universality under a compiler space based on rescaling
computation and coarse-graining emulation. We show a series of boundary
crossing results, including instances of emulation (in all cases for all
initial conditions) of Wolfram Class 2 Elementary Cellular Automata (ECA) by
Class 1 ECA, Classes 2 and 3 ECA emulating classes 1, 2 and 3, and Class 3 ECA
emulating Classes 1, 2 and 3, along with results of a similar type for general
CA (neighborhood r=3/2), including Class 1 CA emulating Classes 2 and 3,
Classes 3 and 4 emulating all other classes (1, 2, 3 and 4). We also found that
there is no possible hacking strategy with which to compress the search space
based on compiler complexity or compiler similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01705</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01705</id><created>2015-10-06</created><authors><author><keyname>Tanovic</keyname><forenames>Omer</forenames></author><author><keyname>Megretski</keyname><forenames>Alexandre</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Stojanovic</keyname><forenames>Vladimir M.</forenames></author><author><keyname>Osqui</keyname><forenames>Mitra</forenames></author></authors><title>Baseband Equivalent Models Resulting From Dynamic Continuous-Time
  Perturbations In Phase-Amplitude Modulation-Demodulation Schemes (Expanded
  version)</title><categories>cs.SY</categories><comments>26 pages, 10 figures. This is an expanded version, with more details,
  of the paper which was published in the proceedings of the European Control
  Conference 2015. Old/short version available at arXiv:1411.1328</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider discrete-time (DT) systems S in which a DT input is first
transformed to a continuous-time (CT) format by phase-amplitude modulation,
then modified by a non-linear CT dynamical transformation F, and finally
converted back to DT output using an ideal de-modulation scheme. Assuming that
F belongs to a special class of CT Volterra series models with fixed degree and
memory depth, we provide a complete characterization of S as a series
connection of a DT Volterra series model of fixed degree and memory depth, and
an LTI system with special properties. The result suggests a new, non-obvious,
analytically motivated structure of digital compensation of analog nonlinear
distortions (for example, those caused by power amplifiers) in digital
communication systems. We also argue that this baseband model, and its
corresponding digital compensation structure, can be readily extended to OFDM
modulation. Results from a MATLAB simulation are used to demonstrate
effectiveness of the new compensation scheme, as compared to the standard
Volterra series approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01713</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01713</id><created>2015-09-28</created><authors><author><keyname>Samiei</keyname><forenames>Ehsan</forenames></author><author><keyname>Butcher</keyname><forenames>Eric A.</forenames></author></authors><title>Chatter Avoidance in Delayed Feedback Attitude Control with MRP Shadow
  Set Switching</title><categories>cs.SY math.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The chattering response at the MRP shadow set switching point for the
controlled attitude dynamics of a rigid tumbling spacecraft using delayed state
feedback control with MRPs is investigated, where the time delay is assumed to
be in the measurement of the state. In addition, a strategy to reduce or
completely avoid the chattering phenomena using a hysteretic boundary layer
switching rule is employed. Simulations are performed to demonstrate the
chattering phenomenon and the advantages of the modified MRP shadow set
switching rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01714</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01714</id><created>2015-10-06</created><updated>2016-01-27</updated><authors><author><keyname>Creusefond</keyname><forenames>Jean</forenames></author><author><keyname>Largillier</keyname><forenames>Thomas</forenames></author><author><keyname>Peyronnet</keyname><forenames>Sylvain</forenames></author></authors><title>On the evaluation potential of quality functions in community detection
  for different contexts</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 11 tables, Advances in Network Science: 12th International
  Conference and School, NetSci-X 2016, Wroclaw, Poland, January 11-13, 2016</comments><doi>10.1007/978-3-319-28361-6_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to nowadays networks' sizes, the evaluation of a community detection
algorithm can only be done using quality functions. These functions measure
different networks/graphs structural properties, each of them corresponding to
a different definition of a community. Since there exists many definitions for
a community, choosing a quality function may be a difficult task, even if the
networks' statistics/origins can give some clues about which one to choose.
  In this paper, we apply a general methodology to identify different contexts,
i.e. groups of graphs where the quality functions behave similarly. In these
contexts we identify the best quality functions, i.e. quality functions whose
results are consistent with expectations from real life applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01717</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01717</id><created>2015-10-06</created><authors><author><keyname>Alfter</keyname><forenames>David</forenames></author></authors><title>Language Segmentation</title><categories>cs.CL</categories><comments>Master Thesis</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Language segmentation consists in finding the boundaries where one language
ends and another language begins in a text written in more than one language.
This is important for all natural language processing tasks. The problem can be
solved by training language models on language data. However, in the case of
low- or no-resource languages, this is problematic. I therefore investigate
whether unsupervised methods perform better than supervised methods when it is
difficult or impossible to train supervised approaches. A special focus is
given to difficult texts, i.e. texts that are rather short (one sentence),
containing abbreviations, low-resource languages and non-standard language. I
compare three approaches: supervised n-gram language models, unsupervised
clustering and weakly supervised n-gram language model induction. I devised the
weakly supervised approach in order to deal with difficult text specifically.
In order to test the approach, I compiled a small corpus of different text
types, ranging from one-sentence texts to texts of about 300 words. The weakly
supervised language model induction approach works well on short and difficult
texts, outperforming the clustering algorithm and reaching scores in the
vicinity of the supervised approach. The results look promising, but there is
room for improvement and a more thorough investigation should be undertaken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01722</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01722</id><created>2015-10-06</created><authors><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author><author><keyname>Sainath</keyname><forenames>Tara N.</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author></authors><title>Structured Transforms for Small-Footprint Deep Learning</title><categories>stat.ML cs.CV cs.LG</categories><comments>To appear in NIPS 2015; 9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of building compact deep learning pipelines suitable for
deployment on storage and power constrained mobile devices. We propose a
unified framework to learn a broad family of structured parameter matrices that
are characterized by the notion of low displacement rank. Our structured
transforms admit fast function and gradient evaluation, and span a rich range
of parameter sharing configurations whose statistical modeling capacity can be
explicitly tuned along a continuum from structured to unstructured.
Experimental results show that these transforms can significantly accelerate
inference and forward/backward passes during training, and offer superior
accuracy-compactness-speed tradeoffs in comparison to a number of existing
techniques. In keyword spotting applications in mobile speech recognition, our
methods are much more effective than standard linear low-rank bottleneck layers
and nearly retain the performance of state of the art models, while providing
more than 3.5-fold compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01728</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01728</id><created>2015-10-06</created><authors><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author><author><keyname>Kramer</keyname><forenames>Boris</forenames></author><author><keyname>Boufounos</keyname><forenames>Petros</forenames></author><author><keyname>Grover</keyname><forenames>Piyush</forenames></author></authors><title>Learning-based Reduced Order Model Stabilization for Partial
  Differential Equations: Application to the Coupled Burgers Equation</title><categories>cs.SY cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present results on stabilization for reduced order models (ROM) of partial
differential equations using learning. Stabilization is achieved via closure
models for ROMs, where we use a model-free extremum seeking (ES) dither-based
algorithm to learn the best closure models' parameters, for optimal ROM
stabilization. We first propose to auto-tune linear closure models using ES,
and then extend the results to a closure model combining linear and nonlinear
terms, for better stabilization performance. The coupled Burgers' equation is
employed as a test-bed for the proposed tuning method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01752</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01752</id><created>2015-10-06</created><updated>2015-12-20</updated><authors><author><keyname>Padovani</keyname><forenames>Luca</forenames><affiliation>Universit&#xe1; di Torino</affiliation></author></authors><title>Type Reconstruction for the Linear \pi-Calculus with Composite Regular
  Types</title><categories>cs.PL</categories><comments>45 pages</comments><proxy>LMCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the linear {\pi}-calculus with composite regular types in such a
way that data containing linear values can be shared among several processes,
if there is no overlapping access to such values. We describe a type
reconstruction algorithm for the extended type system and discuss some
practical aspects of its implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01753</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01753</id><created>2015-10-06</created><authors><author><keyname>Ochem</keyname><forenames>Pascal</forenames></author></authors><title>Doubled patterns are $3$-avoidable</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In combinatorics on words, a word $w$ over an alphabet $\Sigma$ is said to
avoid a pattern $p$ over an alphabet $\Delta$ if there is no factor $f$ of $w$
such that $f=h(p)$ where $h:\Delta^*\to\Sigma^*$ is a non-erasing morphism. A
pattern $p$ is said to be $k$-avoidable if there exists an infinite word over a
$k$-letter alphabet that avoids $p$. A pattern is said to be doubled if no
variable occurs only once. Doubled patterns with at most 3 variables and
patterns with at least 6 variables are $3$-avoidable. We show that doubled
patterns with 4 and 5 variables are also $3$-avoidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01764</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01764</id><created>2015-10-06</created><authors><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>On the Throughput of Multi-Source Multi-Destination Relay Networks with
  Queueing Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the throughput of relay networks with multiple
source-destination pairs under queueing constraints has been investigated for
both variable-rate and fixed-rate schemes. When channel side information (CSI)
is available at the transmitter side, transmitters can adapt their transmission
rates according to the channel conditions, and achieve the instantaneous
channel capacities. In this case, the departure rates at each node have been
characterized for different system parameters, which control the power
allocation, time allocation and decoding order. In the other case of no CSI at
the transmitters, a simple automatic repeat request (ARQ) protocol with fixed
rate transmission is used to provide reliable communication. Under this ARQ
assumption, the instantaneous departure rates at each node can be modeled as an
ON-OFF process, and the probabilities of ON and OFF states are identified. With
the characterization of the arrival and departure rates at each buffer,
stability conditions are identified and effective capacity analysis is
conducted for both cases to determine the system throughput under statistical
queueing constraints. In addition, for the variable-rate scheme, the concavity
of the sum rate is shown for certain parameters, helping to improve the
efficiency of parameter optimization. Finally, via numerical results, the
influence of system parameters and the behavior of the system throughput are
identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01776</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01776</id><created>2015-10-06</created><updated>2016-01-27</updated><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Hui</keyname><forenames>Dennis</forenames></author><author><keyname>Mari&#x107;</keyname><forenames>Ivana</forenames></author></authors><title>Capacity-Achieving Rate-Compatible Polar Codes</title><categories>cs.IT math.IT</categories><comments>Presented at the 8th North American School of Information Theory
  (NASIT 2015), UCSD, La Jolla, CA, Aug. 10-14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method of constructing rate-compatible polar codes that are
capacity-achieving with low-complexity sequential decoders. The proposed code
construction allows for incremental retransmissions at different rates in order
to adapt to channel conditions. The main idea of the construction exploits the
common characteristics of polar codes optimized for a sequence of successively
degraded channels. The proposed approach allows for an optimized polar code to
be used at every transmission thereby achieving capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01780</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01780</id><created>2015-10-06</created><authors><author><keyname>Cloud</keyname><forenames>Alex</forenames></author><author><keyname>Huber</keyname><forenames>Mark</forenames></author></authors><title>Fast Perfect Simulation of Vervaat Perpetutities</title><categories>math.PR cs.DM</categories><comments>14 pages, 1 figure</comments><msc-class>60J10, 65C05, 68U20, 60E05, 60E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a faster method of simulating exactly from a distribution
known as a Vervaat perpetuity. A parameter of the Vervaat perpetuity is $\beta
\in (0,\infty)$. An earlier method for simulating from this distributon ran in
time $O((2.23\beta)^{\beta}).$ This earlier method utilized dominated coupling
from the past that bounded a stochastic process for perpetuities from above. By
extending to non-Markovian update functions, it is possible to create a new
method that bounds the perpetuities from both above and below. This new
approach is shown to run in $O(\beta \ln(\beta))$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01783</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01783</id><created>2015-10-06</created><authors><author><keyname>Asoodeh</keyname><forenames>Shahab</forenames></author><author><keyname>Alajaji</keyname><forenames>Fady</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Lossless Secure Source Coding: Yamamoto's Setting</title><categories>cs.IT math.IT</categories><comments>Presented at Allerton 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a private source of information, $X^n$ and a public correlated source,
$Y^n$, we study the problem of encoding the two-dimensional source $(X^n, Y^n)$
into an index $J$ such that a remote party, knowing $J$ and some external side
information $Z^n$, can losslessly recover $Y^n$ while any eavesdropper knowing
$J$ and possibly a correlated side information $E^n$ can retrieve very little
information about $X^n$. We give general converse results for the amount of
information about $X^n$ that might be leaked in such systems and and also
achievability results that are optimal in some special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01784</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01784</id><created>2015-10-06</created><authors><author><keyname>He</keyname><forenames>Ruining</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author></authors><title>VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title><categories>cs.IR cs.AI</categories><comments>AAAI'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern recommender systems model people and items by discovering or `teasing
apart' the underlying dimensions that encode the properties of items and users'
preferences toward them. Critically, such dimensions are uncovered based on
user feedback, often in implicit form (such as purchase histories, browsing
logs, etc.); in addition, some recommender systems make use of side
information, such as product attributes, temporal information, or review text.
However one important feature that is typically ignored by existing
personalized recommendation and ranking methods is the visual appearance of the
items being considered. In this paper we propose a scalable factorization model
to incorporate visual signals into predictors of people's opinions, which we
apply to a selection of large, real-world datasets. We make use of visual
features extracted from product images using (pre-trained) deep networks, on
top of which we learn an additional layer that uncovers the visual dimensions
that best explain the variation in people's feedback. This not only leads to
significantly more accurate personalized ranking methods, but also helps to
alleviate cold start issues, and qualitatively to analyze the visual dimensions
that influence people's opinions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01799</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01799</id><created>2015-10-06</created><updated>2015-10-09</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian</forenames></author></authors><title>Efficient Per-Example Gradient Computations</title><categories>stat.ML cs.LG</categories><comments>This revision fixed some typos. Many thanks to Hugo Larochelle for
  reporting them!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report describes an efficient technique for computing the norm
of the gradient of the loss function for a neural network with respect to its
parameters. This gradient norm can be computed efficiently for every example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01800</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01800</id><created>2015-10-06</created><updated>2016-02-21</updated><authors><author><keyname>Flajolet</keyname><forenames>Arthur</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Logarithmic regret bounds for Bandits with Knapsacks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal regret bounds for Multi-Armed Bandit problems are now well
documented. They can be classified into two categories based on the growth rate
with respect to the time horizon $T$: (i) small, distribution-dependent, bounds
of order of magnitude $\ln(T)$ and (ii) robust, distribution-free, bounds of
order of magnitude $\sqrt{T}$. The Bandits with Knapsacks model, an extension
to the framework allowing to model resource consumption, lacks this clear-cut
distinction. While several algorithms have been shown to achieve asymptotically
optimal distribution-free bounds on regret, there has been little progress
toward the development of small distribution-dependent regret bounds. We
partially bridge the gap by designing a general-purpose algorithm with
distribution-dependent regret bounds that are optimal in several important
cases that cover many practical applications, including dynamic pricing with
limited supply, online bid optimization for sponsored search auctions, and
dynamic procurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01801</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01801</id><created>2015-10-06</created><updated>2015-10-15</updated><authors><author><keyname>Park</keyname><forenames>Kunwoo</forenames></author><author><keyname>Kim</keyname><forenames>Jaewoo</forenames></author><author><keyname>Park</keyname><forenames>Jaram</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author><author><keyname>Nam</keyname><forenames>Jiin</forenames></author><author><keyname>Yoon</keyname><forenames>Seunghyun</forenames></author><author><keyname>Rhim</keyname><forenames>Eunhee</forenames></author></authors><title>Mining the Minds of Customers from Online Chat Logs</title><categories>cs.CY cs.SI</categories><comments>4 pages, ACM CIKM'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates factors that may determine satisfaction in customer
service operations. We utilized more than 170,000 online chat sessions between
customers and agents to identify characteristics of chat sessions that incurred
dissatisfying experience. Quantitative data analysis suggests that sentiments
or moods conveyed in online conversation are the most predictive factor of
perceived satisfaction. Conversely, other session related meta data (such as
that length, time of day, and response time) has a weaker correlation with user
satisfaction. Knowing in advance what can predict satisfaction allows customer
service staffs to identify potential weaknesses and improve the quality of
service for better customer experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01806</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01806</id><created>2015-10-06</created><authors><author><keyname>Febres</keyname><forenames>Gerardo</forenames></author><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Music viewed by its Entropy content: A novel window for comparative
  analysis</title><categories>cs.SD</categories><comments>28 pages, 15 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Texts of polyphonic music MIDI files were analyzed using the set of symbols
that produced the Fundamental Scale (a set of symbols leading to the Minimal
Entropy Description). We created a space to represent music pieces by
developing: (a) a method to adjust a description from its original scale of
observation to a general scale, (b) the concept of higher order entropy as the
entropy associated to the deviations of a frequency ranked symbol profile from
a perfect Zipf profile. We called this diversity index the &quot;2nd Order Entropy&quot;.
Applying these methods to a variety of musical pieces showed how the space
&quot;symbolic specific diversity-entropy - 2nd order entropy&quot; captures some of the
essence of music types, styles, composers and genres. Some clustering around
each musical category is shown. We also observed the historic trajectory of
music across this space, from medieval to contemporary academic music. We show
that description of musical structures using entropy allows to characterize
traditional and popular expressions of music. These classification techniques
promise to be useful in other disciplines for pattern recognition, machine
learning, and automated experimental design for example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01812</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01812</id><created>2015-10-06</created><updated>2015-10-07</updated><authors><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George</forenames></author></authors><title>Wireless Powered Communications: Performance Analysis and Optimization</title><categories>cs.IT math.IT</categories><comments>accepted to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the average throughput of a wireless powered
communications system, where an energy constrained source, powered by a
dedicated power beacon (PB), communicates with a destination. It is assumed
that the PB is capable of performing channel estimation, digital beamforming,
and spectrum sensing as a communication device. Considering a time splitting
approach, the source first harvests energy from the PB equipped with multiple
antennas, and then transmits information to the destination. Assuming
Nakagami-m fading channels, analytical expressions for the average throughput
are derived for two different transmission modes, namely, delay tolerant and
delay intolerant. In addition, closed-form solutions for the optimal time
split, which maximize the average throughput are obtained in some special
cases, i.e., high transmit power regime and large number of antennas. Finally,
the impact of co-channel interference is studied. Numerical and simulation
results have shown that increasing the number of transmit antennas at the PB is
an effective tool to improve the average throughput and the interference can be
potentially exploited to enhance the average throughput, since it can be
utilized as an extra source of energy. Also, the impact of fading severity
level of the energy transfer link on the average throughput is not significant,
especially if the number of PB antennas is large. Finally, it is observed that
the source position has a great impact on the average throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01814</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01814</id><created>2015-10-06</created><authors><author><keyname>Zhu</keyname><forenames>Kai</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>Source Localization in Networks: Trees and Beyond</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information diffusion in networks can be used to model many real-world
phenomena, including rumor spreading on online social networks, epidemics in
human beings, and malware on the Internet. Informally speaking, the source
localization problem is to identify a node in the network that provides the
best explanation of the observed diffusion. Despite significant efforts and
successes over last few years, theoretical guarantees of source localization
algorithms were established only for tree networks due to the complexity of the
problem. This paper presents a new source localization algorithm, called the
Short-Fat Tree (SFT) algorithm. Loosely speaking, the algorithm selects the
node such that the breadth-first search (BFS) tree from the node has the
minimum depth but the maximum number of leaf nodes. Performance guarantees of
SFT under the independent cascade (IC) model are established for both tree
networks and the Erdos-Renyi (ER) random graph. On tree networks, SFT is the
maximum a posterior (MAP) estimator. On the ER random graph, the following
fundamental limits have been obtained: $(i)$ when the infection duration
$&lt;\frac{2}{3}t_u,$ SFT identifies the source with probability one
asymptotically, where $t_u=\left\lceil\frac{\log n}{\log \mu}\right\rceil+2$
and $\mu$ is the average node degree, $(ii)$ when the infection duration
$&gt;t_u,$ the probability of identifying the source approaches zero
asymptotically under any algorithm; and $(iii)$ when infection duration $&lt;t_u,$
the BFS tree starting from the source is a fat tree. Numerical experiments on
tree networks, the ER random graphs and real world networks with different
evaluation metrics show that the SFT algorithm outperforms existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01816</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01816</id><created>2015-10-07</created><authors><author><keyname>Lee</keyname><forenames>Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Kyoung-Jae</forenames></author><author><keyname>Kim</keyname><forenames>Hanjin</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Lee</keyname><forenames>Inkyu</forenames></author></authors><title>Resource Allocation Techniques for Wireless Powered Communication
  Networks with Energy Storage Constraint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies multi-user wireless powered communication networks, where
energy constrained users charge their energy storages by scavenging energy of
the radio frequency signals radiated from a hybrid access point (H-AP). The
energy is then utilized for the users' uplink information transmission to the
H-AP in time division multiple access mode. In this system, we aim to maximize
the uplink sum rate performance by jointly optimizing energy and time resource
allocation for multiple users in both infinite capacity and finite capacity
energy storage cases. First, when the users are equipped with the infinite
capacity energy storages, we derive the optimal downlink energy transmission
policy at the H-AP. Based on this result, analytical resource allocation
solutions are obtained. Next, we propose the optimal energy and time allocation
algorithm for the case where each user has finite capacity energy storage.
Simulation results confirm that the proposed algorithms offer 30% average sum
rate performance gain over conventional schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01819</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01819</id><created>2015-10-07</created><updated>2015-10-29</updated><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Atienza</keyname><forenames>Nieves</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Perez-Lantero</keyname><forenames>Pablo</forenames></author><author><keyname>D&#x131;az-B&#xe1;&#xf1;ez</keyname><forenames>Jose M.</forenames></author><author><keyname>Flores-Pe&#xf1;aloza</keyname><forenames>David</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author></authors><title>Balanced Islands in Two Colored Point Sets in the Plane</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in general position in the plane, $r$ of which
are red and $b$ of which are blue. In this paper we prove that there exist: for
every $\alpha \in \left [ 0,\frac{1}{2} \right ]$, a convex set containing
exactly $\lceil \alpha r\rceil$ red points and exactly $\lceil \alpha b \rceil$
blue points of $P$; a convex set containing exactly $\left \lceil
\frac{r+1}{2}\right \rceil$ red points and exactly $\left \lceil
\frac{b+1}{2}\right \rceil$ blue points of $P$. Furthermore, we present
polynomial time algorithms to find these convex sets. In the first case we
provide an $O(n^4)$ time algorithm and an $O(n^2\log n)$ time algorithm in the
second case. Finally, if $\lceil \alpha r\rceil+\lceil \alpha b\rceil$ is
small, that is, not much larger than $\frac{1}{3}n$, we improve the running
time to $O(n \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01823</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01823</id><created>2015-10-07</created><authors><author><keyname>Tsai</keyname><forenames>Pei-Chuan</forenames></author><author><keyname>Chen</keyname><forenames>Chih-Ming</forenames></author><author><keyname>Chen</keyname><forenames>Ying-ping</forenames></author></authors><title>Multiple Configurations LT Codes</title><categories>cs.IT math.IT</categories><comments>11 pages, 8 figures, 3 tables</comments><acm-class>E.4; H.1.1; C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new scheme of LT codes, named multiple
configurations. In multiple configurations LT codes (MC-LT codes), multiple
sets of output symbols are simultaneously provided to receivers for recovering
the source data. Each receiver, without the need to send information back to
the sender, is capable of receiving the output symbols generated by some
configuration chosen according to its own decoding phase. Aiming at the
broadcasting scenarios without feedback channels, the proposed MC-LT codes are
shown to outperform the optimal pure LT codes at the cost of encoding and
transmitting units. In this paper, the inspiration of MC-LT codes is presented,
how MC-LT codes work is described by giving examples, in which the optimal pure
LT codes are outperformed, and a practical design of MC-LT codes, which is
analytically proved to have at least the same performance bound as the pure LT
codes, is proposed. The results of numerical simulation experiments demonstrate
that the proposed practical design of MC-LT codes can deliver better
performance than the LT codes in comparison. In summary, this paper creates new
potential research directions for LT codes, and MC-LT codes are a promising
variant of LT codes, especially for broadcasting scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01844</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01844</id><created>2015-10-07</created><updated>2015-10-07</updated><authors><author><keyname>Makur</keyname><forenames>Anuran</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Bounds between Contraction Coefficients</title><categories>cs.IT math.IT</categories><comments>Part of this work has been published in the 53rd Annual Allerton
  Conference on Communication, Control, and Computing, 2015. Minor changes made
  from first version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we delineate how the contraction coefficient of the strong
data processing inequality for KL divergence can be used to learn likelihood
models. We then present an alternative formulation to learn likelihood models
that forces the input KL divergence of the data processing inequality to
vanish, and achieves a contraction coefficient equivalent to the squared
maximal correlation. This formulation turns out to admit a linear algebraic
solution. To analyze the performance loss in using this simple but suboptimal
procedure, we bound these contraction coefficients in the discrete and finite
regime, and prove their equivalence in the Gaussian regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01852</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01852</id><created>2015-10-07</created><authors><author><keyname>Ghali</keyname><forenames>Cesar</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author><author><keyname>Wood</keyname><forenames>Christopher A.</forenames></author><author><keyname>Yeh</keyname><forenames>Edmund</forenames></author></authors><title>Practical Accounting in Content-Centric Networking (extended version)</title><categories>cs.NI</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-Centric Networking (CCN) is a new class of network architectures
designed to address some key limitations of the current IP-based Internet. One
of its main features is in-network content caching, which allows requests for
content to be served by routers. Despite improved bandwidth utilization and
lower latency for popular content retrieval, in-network content caching offers
producers no means of collecting information about content that is requested
and later served from network caches. Such information is often needed for
accounting purposes. In this paper, we design some secure accounting schemes
that vary in the degree of consumer, router, and producer involvement. Next, we
identify and analyze performance and security tradeoffs, and show that specific
per-consumer accounting is impossible in the presence of router caches and
without application-specific support. We then recommend accounting strategies
that entail a few simple requirements for CCN architectures. Finally, our
experimental results show that forms of native and secure CCN accounting are
both more viable and practical than application-specific approaches with little
modification to the existing architecture and protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01861</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01861</id><created>2015-10-07</created><authors><author><keyname>Biswas</keyname><forenames>Abhinav</forenames></author><author><keyname>Karunakaran</keyname><forenames>Sukanya</forenames></author></authors><title>Cybernetic modeling of Industrial Control Systems: Towards threat
  analysis of critical infrastructure</title><categories>cs.CR cs.SY</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial Control Systems (ICS) encompassing resources for process
automation are subjected to a wide variety of security threats. The threat
landscape is arising due to increased adoption of Commercial-of-the-shelf
(COTS) products as well as the convergence of Internet and legacy systems.
Prevalent security approaches for protection of critical infrastructure are
scattered among various subsystems and modules of ICS networks. This demands a
new state-of-the-art cybernetic model of ICS networks, which can help in threat
analysis by providing a comprehensive view of the relationships and
interactions between the subsystems. Towards this direction, the principles of
the Viable System Model (VSM) are applied to introduce a conceptual recursive
model of secure ICS networks that can drive cyber security decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01866</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01866</id><created>2015-10-07</created><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>Markarian</keyname><forenames>Christine</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author><author><keyname>Schubert</keyname><forenames>Michael</forenames></author></authors><title>Approximation and Heuristic Algorithms for Computing Backbones in
  Asymmetric Ad-Hoc Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of dominating set-based virtual backbone used for
routing in asymmetric wireless ad-hoc networks. These networks have non-uniform
transmission ranges and are modeled using the well-established disk graphs. The
corresponding graph theoretic problem seeks a strongly connected
dominating-absorbent set of minimum cardinality in a digraph. A subset of nodes
in a digraph is a strongly connected dominating-absorbent set if the subgraph
induced by these nodes is strongly connected and each node in the graph is
either in the set or has both an in-neighbor and an out-neighbor in it.
Distributed algorithms for this problem are of practical significance due to
the dynamic nature of ad-hoc networks. We present a first distributed
approximation algorithm, with a constant approximation factor and O(Diam)
running time, where Diam is the diameter of the graph. Moreover we present a
simple heuristic algorithm and conduct an extensive simulation study showing
that our heuristic outperforms previously known approaches for the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01871</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01871</id><created>2015-10-07</created><authors><author><keyname>Besselaar</keyname><forenames>Peter van den</forenames></author><author><keyname>Sandstrom</keyname><forenames>Ulf</forenames></author></authors><title>Does Quantity Make a Difference? The importance of publishing many
  papers</title><categories>cs.DL</categories><comments>Presented at ISSI 2015, Istanbul</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do highly productive researchers have significantly higher probability to
produce top cited papers? Or does the increased productivity in science only
result in a sea of irrelevant papers as a perverse effect of competition and
the increased use of indicators for research evaluation and accountability
focus? We use a Swedish author disambiguated data set consisting of 48,000
researchers and their WoS-publications during the period of 2008 2011 with
citations until 2014 to investigate the relation between productivity and
production of highly cited papers. As the analysis shows, quantity does make a
difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01886</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01886</id><created>2015-10-07</created><authors><author><keyname>Moussallem</keyname><forenames>Diego</forenames></author><author><keyname>Choren</keyname><forenames>Ricardo</forenames></author></authors><title>Using Ontology-Based Context in the Portuguese-English Translation of
  Homographs in Textual Dialogues</title><categories>cs.CL</categories><comments>17 pages, 7 figures, 2 tables in International journal of Artificial
  Intelligence &amp; Applications 2015</comments><doi>10.5121/ijaia.2015.6502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel approach to tackle the existing gap on message
translations in dialogue systems. Currently, submitted messages to the dialogue
systems are considered as isolated sentences. Thus, missing context information
impede the disambiguation of homographs words in ambiguous sentences. Our
approach solves this disambiguation problem by using concepts over existing
ontologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01891</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01891</id><created>2015-10-07</created><authors><author><keyname>Kurpisz</keyname><forenames>Adam</forenames></author><author><keyname>Lepp&#xe4;nen</keyname><forenames>Samuli</forenames></author><author><keyname>Mastrolilli</keyname><forenames>Monaldo</forenames></author></authors><title>On the Hardest Problem Formulations for the 0/1 Lasserre Hierarchy</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lasserre/Sum-of-Squares (SoS) hierarchy is a systematic procedure for
constructing a sequence of increasingly tight semidefinite relaxations. It is
known that the hierarchy converges to the 0/1 polytope in n levels and captures
the convex relaxations used in the best available approximation algorithms for
a wide variety of optimization problems.
  In this paper we characterize the set of 0/1 integer linear problems and
unconstrained 0/1 polynomial optimization problems that can still have an
integrality gap at level n-1. These problems are the hardest for the Lasserre
hierarchy in this sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01913</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01913</id><created>2015-10-07</created><authors><author><keyname>Brattka</keyname><forenames>Vasco</forenames></author><author><keyname>Hendtlass</keyname><forenames>Matthew</forenames></author><author><keyname>Kreuzer</keyname><forenames>Alexander P.</forenames></author></authors><title>On the Uniform Computational Content of the Baire Category Theorem</title><categories>math.LO cs.LO</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the uniform computational content of different versions of the Baire
Category Theorem in the Weihrauch lattice. The Baire Category Theorem can be
seen as a pigeonhole principle that states that a complete (i.e., &quot;large&quot;)
metric space cannot be decomposed into countably many nowhere dense (i.e.,
&quot;small&quot;) pieces. The Baire Category Theorem is an illuminating example of a
theorem that can be used to demonstrate that one classical theorem can have
several different computational interpretations. For one, we distinguish two
different logical versions of the theorem, where one can be seen as the
contrapositive form of the other one. The first version aims to find an
uncovered point in the space, given a sequence of nowhere dense closed sets.
The second version aims to find the index of a closed set that is somewhere
dense, given a sequence of closed sets that cover the space. Even though the
two statements behind these versions are equivalent to each other in classical
logic, they are not equivalent in intuitionistic logic and likewise they
exhibit different computational behavior in the Weihrauch lattice. Besides this
logical distinction, we also consider different ways how the sequence of closed
sets is &quot;given&quot;. Essentially, we can distinguish between positive and negative
information on closed sets. We discuss all the four resulting versions of the
Baire Category Theorem. Somewhat surprisingly it turns out that the difference
in providing the input information can also be expressed with the jump
operation. Finally, we also relate the Baire Category Theorem to notions of
genericity and computably comeager sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01920</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01920</id><created>2015-10-07</created><authors><author><keyname>Graells-Garrido</keyname><forenames>Eduardo</forenames></author><author><keyname>Lalmas</keyname><forenames>Mounia</forenames></author><author><keyname>Baeza-Yates</keyname><forenames>Ricardo</forenames></author></authors><title>Encouraging Diversity- and Representation-Awareness in Geographically
  Centralized Content</title><categories>cs.SI cs.CY cs.HC</categories><comments>12 pages. Under review. Please contact authors before citing /
  distributing</comments><acm-class>H.3.3; H.5.2</acm-class><doi>10.1145/2856767.2856775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In centralized countries, not only population, media and economic power are
concentrated, but people give more attention to central locations. While this
is not inherently bad, this behavior extends to micro-blogging platforms:
central locations get more attention in terms of information flow. In this
paper we study the effects of an information filtering algorithm that
decentralizes content in such platforms. Particularly, we find that users from
non-central locations were not able to identify the geographical diversity on
timelines generated by the algorithm, which were diverse by construction. To
make users see the inherent diversity, we define a design rationale to approach
this problem, focused on an already known visualization technique: treemaps.
Using interaction data from an &quot;in the wild&quot; deployment of our proposed system,
we find that, even though there are effects of centralization in exploratory
user behavior, the treemap was able to make users see the inherent geographical
diversity of timelines, and engage with user generated content. With these
results in mind, we propose practical actions for micro-blogging platforms to
account for the differences and biased behavior induced by centralization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01932</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01932</id><created>2015-10-07</created><authors><author><keyname>Tsvetkovaa</keyname><forenames>Milena</forenames></author><author><keyname>Nilsson</keyname><forenames>Olof</forenames></author><author><keyname>&#xd6;hman</keyname><forenames>Camilla</forenames></author><author><keyname>Sumpter</keyname><forenames>Lovisa</forenames></author><author><keyname>Sumpter</keyname><forenames>David</forenames></author></authors><title>An Experimental Study of Segregation Mechanisms</title><categories>cs.SI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segregation is widespread in all realms of human society. Several influential
studies have argued that intolerance is not a prerequisite for a segregated
society, and that segregation can arise even when people generally prefer
diversity. We investigated this paradox experimentally, by letting groups of
high-school students play four different real-time interactive games.
Incentives for neighbor similarity produced segregation, but incentives for
neighbor dissimilarity and neighborhood diversity prevented it. The
participants continued to move while their game scores were below optimal, but
their individual moves did not consistently take them to the best alternative
position. These small differences between human and simulated agents produced
different segregation patterns than previously predicted, thus challenging
conclusions about segregation arising from these models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01942</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01942</id><created>2015-10-07</created><authors><author><keyname>Rayner</keyname><forenames>Manny</forenames></author><author><keyname>Armando</keyname><forenames>Alejandro</forenames></author><author><keyname>Bouillon</keyname><forenames>Pierrette</forenames></author><author><keyname>Ebling</keyname><forenames>Sarah</forenames></author><author><keyname>Gerlach</keyname><forenames>Johanna</forenames></author><author><keyname>Halimi</keyname><forenames>Sonia</forenames></author><author><keyname>Strasly</keyname><forenames>Irene</forenames></author><author><keyname>Tsourakis</keyname><forenames>Nikos</forenames></author></authors><title>Helping Domain Experts Build Speech Translation Systems</title><categories>cs.HC cs.CL</categories><comments>12 pages, 1 figure, to appear in Proc. Future and Emerging Trends in
  Language Technology 2015, Seville, Spain</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new platform, &quot;Regulus Lite&quot;, which supports rapid development
and web deployment of several types of phrasal speech translation systems using
a minimal formalism. A distinguishing feature is that most development work can
be performed directly by domain experts. We motivate the need for platforms of
this type and discuss three specific cases: medical speech translation,
speech-to-sign-language translation and voice questionnaires. We briefly
describe initial experiences in developing practical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01949</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01949</id><created>2015-10-07</created><authors><author><keyname>Suni</keyname><forenames>Antti</forenames></author><author><keyname>Aalto</keyname><forenames>Daniel</forenames></author><author><keyname>Vainio</keyname><forenames>Martti</forenames></author></authors><title>Hierarchical Representation of Prosody for Statistical Speech Synthesis</title><categories>cs.CL cs.SD</categories><comments>22 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prominences and boundaries are the essential constituents of prosodic
structure in speech. They provide for means to chunk the speech stream into
linguistically relevant units by providing them with relative saliences and
demarcating them within coherent utterance structures. Prominences and
boundaries have both been widely used in both basic research on prosody as well
as in text-to-speech synthesis. However, there are no representation schemes
that would provide for both estimating and modelling them in a unified fashion.
Here we present an unsupervised unified account for estimating and representing
prosodic prominences and boundaries using a scale-space analysis based on
continuous wavelet transform. The methods are evaluated and compared to earlier
work using the Boston University Radio News corpus. The results show that the
proposed method is comparable with the best published supervised annotation
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01963</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01963</id><created>2015-10-07</created><updated>2015-10-08</updated><authors><author><keyname>Lacour</keyname><forenames>Renaud</forenames></author><author><keyname>Klamroth</keyname><forenames>Kathrin</forenames></author><author><keyname>Fonseca</keyname><forenames>Carlos M.</forenames></author></authors><title>A Box Decomposition Algorithm to Compute the Hypervolume Indicator</title><categories>cs.DM cs.DS</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach to the computation of the hypervolume indicator,
based on partitioning the dominated region into a set of axis-parallel
hyperrectangles or boxes. We present a nonincremental algorithm and an
incremental algorithm, which allows insertions of points, whose time
complexities are $O(n^{\lfloor \frac{p-1}{2} \rfloor+1})$ and $O(n^{\lfloor
\frac{p}{2} \rfloor+1})$, respectively. While the theoretical complexity of
such a method is lower bounded by the complexity of the partition, which is, in
the worst-case, larger than the best upper bound on the complexity of the
hypervolume computation, we show that it is practically efficient. In
particular, the nonincremental algorithm competes with the currently most
practically efficient algorithms. Finally, we prove an enhanced upper bound of
$O(n^{p-1})$ and a lower bound of $\Omega (n^{\lfloor \frac{p}{2}\rfloor} \log
n )$ for $p \geq 4$ on the worst-case complexity of the WFG algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01970</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01970</id><created>2015-10-07</created><authors><author><keyname>Tijani</keyname><forenames>Khadija</forenames><affiliation>CSTB, G-SCOP\_GCSP, LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Ngo</keyname><forenames>Dung</forenames><affiliation>G-SCOP\_GCSP</affiliation></author><author><keyname>Ploix</keyname><forenames>Stephane</forenames><affiliation>G-SCOP\_GCSP</affiliation></author><author><keyname>Haas</keyname><forenames>Benjamin</forenames><affiliation>CSTB</affiliation></author><author><keyname>Dugdale</keyname><forenames>Julie</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Towards a general framework for an observation and knowledge based model
  of occupant behaviour in office buildings</title><categories>cs.AI cs.CY math.PR</categories><comments>IBPC 2015 Turin , Jun 2015, Turin, Italy. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new general approach based on Bayesian networks to
model the human behaviour. This approach represents human behaviour
withprobabilistic cause-effect relations based not only on previous works, but
also with conditional probabilities coming either from expert knowledge or
deduced from observations. The approach has been used in the co-simulation of
building physics and human behaviour in order to assess the CO 2 concentration
in an office.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01972</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01972</id><created>2015-10-07</created><authors><author><keyname>Gallego</keyname><forenames>Guillermo</forenames></author><author><keyname>Forster</keyname><forenames>Christian</forenames></author><author><keyname>Mueggler</keyname><forenames>Elias</forenames></author><author><keyname>Scaramuzza</keyname><forenames>Davide</forenames></author></authors><title>Event-based Camera Pose Tracking using a Generative Event Model</title><categories>cs.CV cs.RO</categories><comments>7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-based vision sensors mimic the operation of biological retina and they
represent a major paradigm shift from traditional cameras. Instead of providing
frames of intensity measurements synchronously, at artificially chosen rates,
event-based cameras provide information on brightness changes asynchronously,
when they occur. Such non-redundant pieces of information are called &quot;events&quot;.
These sensors overcome some of the limitations of traditional cameras (response
time, bandwidth and dynamic range) but require new methods to deal with the
data they output. We tackle the problem of event-based camera localization in a
known environment, without additional sensing, using a probabilistic generative
event model in a Bayesian filtering framework. Our main contribution is the
design of the likelihood function used in the filter to process the observed
events. Based on the physical characteristics of the sensor and on empirical
evidence of the Gaussian-like distribution of spiked events with respect to the
brightness change, we propose to use the contrast residual as a measure of how
well the estimated pose of the event-based camera and the environment explain
the observed events. The filter allows for localization in the general case of
six degrees-of-freedom motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01989</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01989</id><created>2015-10-07</created><authors><author><keyname>Atkinson</keyname><forenames>Malcolm</forenames></author><author><keyname>Carpen&#xe9;</keyname><forenames>Michele</forenames></author><author><keyname>Casarotti</keyname><forenames>Emanuele</forenames></author><author><keyname>Claus</keyname><forenames>Steffen</forenames></author><author><keyname>Filgueira</keyname><forenames>Rosa</forenames></author><author><keyname>Frank</keyname><forenames>Anton</forenames></author><author><keyname>Galea</keyname><forenames>Michelle</forenames></author><author><keyname>Garth</keyname><forenames>Tom</forenames></author><author><keyname>Gem&#xfc;nd</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Igel</keyname><forenames>Heiner</forenames></author><author><keyname>Klampanos</keyname><forenames>Iraklis</forenames></author><author><keyname>Krause</keyname><forenames>Amrey</forenames></author><author><keyname>Krischer</keyname><forenames>Lion</forenames></author><author><keyname>Leong</keyname><forenames>Siew Hoon</forenames></author><author><keyname>Magnoni</keyname><forenames>Federica</forenames></author><author><keyname>Matser</keyname><forenames>Jonas</forenames></author><author><keyname>Michelini</keyname><forenames>Alberto</forenames></author><author><keyname>Rietbrock</keyname><forenames>Andreas</forenames></author><author><keyname>Schwichtenberg</keyname><forenames>Horst</forenames></author><author><keyname>Spinuso</keyname><forenames>Alessandro</forenames></author><author><keyname>Vilotte</keyname><forenames>Jean-Pierre</forenames></author></authors><title>VERCE delivers a productive e-Science environment for seismology
  research</title><categories>cs.DC</categories><comments>14 pages, 3 figures. Pre-publication version of paper accepted and
  published at the IEEE eScience 2015 conference in Munich with substantial
  additions, particularly in the analysis of issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The VERCE project has pioneered an e-Infrastructure to support researchers
using established simulation codes on high-performance computers in conjunction
with multiple sources of observational data. This is accessed and organised via
the VERCE science gateway that makes it convenient for seismologists to use
these resources from any location via the Internet. Their data handling is made
flexible and scalable by two Python libraries, ObsPy and dispel4py and by data
services delivered by ORFEUS and EUDAT. Provenance driven tools enable rapid
exploration of results and of the relationships between data, which accelerates
understanding and method improvement. These powerful facilities are integrated
and draw on many other e-Infrastructures. This paper presents the motivation
for building such systems, it reviews how solid-Earth scientists can make
significant research progress using them and explains the architecture and
mechanisms that make their construction and operation achievable. We conclude
with a summary of the achievements to date and identify the crucial steps
needed to extend the capabilities for seismologists, for solid-Earth scientists
and for similar disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01991</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01991</id><created>2015-10-07</created><authors><author><keyname>Wan</keyname><forenames>Ji</forenames></author><author><keyname>Tang</keyname><forenames>Sheng</forenames></author><author><keyname>Zhang</keyname><forenames>Yongdong</forenames></author><author><keyname>Li</keyname><forenames>Jintao</forenames></author><author><keyname>Wu</keyname><forenames>Pengcheng</forenames></author><author><keyname>Hoi</keyname><forenames>Steven C. H.</forenames></author></authors><title>HDIdx: High-Dimensional Indexing for Efficient Approximate Nearest
  Neighbor Search</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale
data processing and analytics, particularly for analyzing multimedia contents
which are often of high dimensionality. Instead of using exact NN search,
extensive research efforts have been focusing on approximate NN search
algorithms. In this work, we present &quot;HDIdx&quot;, an efficient high-dimensional
indexing library for fast approximate NN search, which is open-source and
written in Python. It offers a family of state-of-the-art algorithms that
convert input high-dimensional vectors into compact binary codes, making them
very efficient and scalable for NN search with very low space complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01993</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01993</id><created>2015-10-07</created><authors><author><keyname>Cao</keyname><forenames>Nianxia</forenames></author><author><keyname>Choi</keyname><forenames>Sora</forenames></author><author><keyname>Masazade</keyname><forenames>Engin</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Sensor Selection for Target Tracking in Wireless Sensor Networks with
  Uncertainty</title><categories>cs.SY stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multiobjective optimization framework for the
sensor selection problem in uncertain Wireless Sensor Networks (WSNs). The
uncertainties of the WSNs result in a set of sensor observations with
insufficient information about the target. We propose a novel mutual
information upper bound (MIUB) based sensor selection scheme, which has low
computational complexity, same as the Fisher information (FI) based sensor
selection scheme, and gives estimation performance similar to the mutual
information (MI) based sensor selection scheme. Without knowing the number of
sensors to be selected a priori, the multiobjective optimization problem (MOP)
gives a set of sensor selection strategies that reveal different trade-offs
between two conflicting objectives: minimization of the number of selected
sensors and minimization of the gap between the performance metric (MIUB and
FI) when all the sensors transmit measurements and when only the selected
sensors transmit their measurements based on the sensor selection strategy.
Illustrative numerical results that provide valuable insights are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.01997</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.01997</id><created>2015-10-07</created><authors><author><keyname>P&#xe9;rez-Ros&#xe9;s</keyname><forenames>Hebert</forenames></author><author><keyname>Seb&#xe9;</keyname><forenames>Francesc</forenames></author><author><keyname>Rib&#xf3;</keyname><forenames>Josep Maria</forenames></author></authors><title>Endorsement Deduction and Ranking in Social Networks</title><categories>cs.SI</categories><comments>32 pages</comments><acm-class>H.3.3; H.3.4</acm-class><doi>10.1016/j.comcom.2015.08.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some social networks, such as LinkedIn and ResearchGate, allow user
endorsements for specific skills. In this way, for each skill we get a directed
graph where the nodes correspond to users' profiles and the arcs represent
endorsement relations. From the number and quality of the endorsements
received, an authority score can be assigned to each profile. In this paper we
propose an authority score computation method that takes into account the
relations existing among different skills. Our method is based on enriching the
information contained in the digraph of endorsements corresponding to a
specific skill, and then applying a ranking method admitting weighted digraphs,
such as PageRank. We describe the method, and test it on a synthetic network of
1493 nodes, fitted with endorsements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02004</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02004</id><created>2015-10-02</created><authors><author><keyname>Alvarez</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Becher</keyname><forenames>Ver&#xf3;nica</forenames></author></authors><title>M. Levin's construction of absolutely normal numbers with very low
  discrepancy</title><categories>math.NT cs.DS</categories><comments>20 pages with references; submitted to AMS Mathematics of Computation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the currently known constructions of absolutely normal numbers, the one
given by Mordechay Levin in 1979 achieves the lowest discrepancy bound. In this
work we analyze this construction in terms of computability and computational
complexity. We show that, under basic assumptions, it yields a computable real
number. The construction does not give the digits of the fractional expansion
explicitly, but it gives a sequence of increasing approximations whose limit is
the announced absolutely normal number. The $n$-th approximation has an error
less than $2^{2^{-n}}$. To obtain the $n$-th approximation the construction
requires, in the worst case, a number of mathematical operations that is double
exponential in $n$. We consider variants on the construction that reduce the
computational complexity at the expense of an increment in discrepancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02032</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02032</id><created>2015-10-07</created><authors><author><keyname>Elyasi</keyname><forenames>Mehran</forenames></author><author><keyname>Mohajer</keyname><forenames>Soheil</forenames></author></authors><title>A Probabilistic Approach Towards Exact-Repair Regeneration Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regeneration codes with exact-repair property for distributed storage systems
is studied in this paper. For exact- repair problem, the achievable points of
({\alpha},{\beta}) tradeoff match with the outer bound only for minimum storage
regenerating (MSR), minimum bandwidth regenerating (MBR), and some specific
values of n, k, and d. Such tradeoff is characterized in this work for general
(n, k, k), (i.e., k = d) for some range of per-node storage ({\alpha}) and
repair-bandwidth ({\beta}). Rather than explicit code construction,
achievability of these tradeoff points is shown by proving existence of
exact-repair regeneration codes for any (n,k,k). More precisely, it is shown
that an (n, k, k) system can be extended by adding a new node, which is
randomly picked from some ensemble, and it is proved that, with high
probability, the existing nodes together with the newly added one maintain
properties of exact-repair regeneration codes. The new achievable region
improves upon the existing code constructions. In particular, this result
provides a complete tradeoff characterization for an (n,3,3) distributed
storage system for any value of n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02036</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02036</id><created>2015-10-07</created><authors><author><keyname>Engelfriet</keyname><forenames>Joost</forenames></author></authors><title>Tree Automata and Tree Grammars</title><categories>cs.FL</categories><comments>76 pages, slightly revised version of lecture notes from 1975</comments><report-no>DAIMI FN-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lecture notes on tree language theory, in particular recognizable tree
languages and finite state tree transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02037</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02037</id><created>2015-10-07</created><updated>2015-11-11</updated><authors><author><keyname>Eyal</keyname><forenames>Ittay</forenames></author><author><keyname>Gencer</keyname><forenames>Adem Efe</forenames></author><author><keyname>Sirer</keyname><forenames>Emin Gun</forenames></author><author><keyname>van Renesse</keyname><forenames>Robbert</forenames></author></authors><title>Bitcoin-NG: A Scalable Blockchain Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptocurrencies, based on and led by Bitcoin, have shown promise as
infrastructure for pseudonymous online payments, cheap remittance, trustless
digital asset exchange, and smart contracts. However, Bitcoin-derived
blockchain protocols have inherent scalability limits that trade-off between
throughput and latency and withhold the realization of this potential.
  This paper presents Bitcoin-NG, a new blockchain protocol designed to scale.
Based on Bitcoin's blockchain protocol, Bitcoin-NG is Byzantine fault tolerant,
is robust to extreme churn, and shares the same trust model obviating
qualitative changes to the ecosystem.
  In addition to Bitcoin-NG, we introduce several novel metrics of interest in
quantifying the security and efficiency of Bitcoin-like blockchain protocols.
We implement Bitcoin-NG and perform large-scale experiments at 15% the size of
the operational Bitcoin system, using unchanged clients of both protocols.
These experiments demonstrate that Bitcoin-NG scales optimally, with bandwidth
limited only by the capacity of the individual nodes and latency limited only
by the propagation time of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02045</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02045</id><created>2015-10-07</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil</forenames></author><author><keyname>Dud&#xed;k</keyname><forenames>Miroslav</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Pennock</keyname><forenames>David M.</forenames></author></authors><title>Budget Constraints in Prediction Markets</title><categories>cs.GT cs.AI</categories><journal-ref>In Proceedings of the 31st Conference on Uncertainty in Artificial
  Intelligence, pages 238-247, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a detailed characterization of optimal trades under budget
constraints in a prediction market with a cost-function-based automated market
maker. We study how the budget constraints of individual traders affect their
ability to impact the market price. As a concrete application of our
characterization, we give sufficient conditions for a property we call budget
additivity: two traders with budgets B and B' and the same beliefs would have a
combined impact equal to a single trader with budget B+B'. That way, even if a
single trader cannot move the market much, a crowd of like-minded traders can
have the same desired effect. When the set of payoff vectors associated with
outcomes, with coordinates corresponding to securities, is affinely
independent, we obtain that a generalization of the heavily-used logarithmic
market scoring rule is budget additive, but the quadratic market scoring rule
is not. Our results may be used both descriptively, to understand if a
particular market maker is affected by budget constraints or not, and
prescriptively, as a recipe to construct markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02049</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02049</id><created>2015-10-07</created><authors><author><keyname>Gella</keyname><forenames>Spandana</forenames></author><author><keyname>Dymetman</keyname><forenames>Marc</forenames></author><author><keyname>Renders</keyname><forenames>Jean Michel</forenames></author><author><keyname>Venkatapathy</keyname><forenames>Sriram</forenames></author></authors><title>Assisting Composition of Email Responses: a Topic Prediction Approach</title><categories>cs.CL</categories><comments>8 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose an approach for helping agents compose email replies to customer
requests. To enable that, we use LDA to extract latent topics from a collection
of email exchanges. We then use these latent topics to label our data,
obtaining a so-called &quot;silver standard&quot; topic labelling. We exploit this
labelled set to train a classifier to: (i) predict the topic distribution of
the entire agent's email response, based on features of the customer's email;
and (ii) predict the topic distribution of the next sentence in the agent's
reply, based on the customer's email features and on features of the agent's
current sentence. The experimental results on a large email collection from a
contact center in the tele- com domain show that the proposed ap- proach is
effective in predicting the best topic of the agent's next sentence. In 80% of
the cases, the correct topic is present among the top five recommended topics
(out of fifty possible ones). This shows the potential of this method to be
applied in an interactive setting, where the agent is presented a small list of
likely topics to choose from for the next sentence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02054</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02054</id><created>2015-10-07</created><authors><author><keyname>Wang</keyname><forenames>Weiran</forenames></author><author><keyname>Arora</keyname><forenames>Raman</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations</title><categories>cs.LG</categories><comments>in 2015 Annual Allerton Conference on Communication, Control and
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep CCA is a recently proposed deep neural network extension to the
traditional canonical correlation analysis (CCA), and has been successful for
multi-view representation learning in several domains. However, stochastic
optimization of the deep CCA objective is not straightforward, because it does
not decouple over training examples. Previous optimizers for deep CCA are
either batch-based algorithms or stochastic optimization using large
minibatches, which can have high memory consumption. In this paper, we tackle
the problem of stochastic optimization for deep CCA with small minibatches,
based on an iterative solution to the CCA objective, and show that we can
achieve as good performance as previous optimizers and thus alleviate the
memory requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02055</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02055</id><created>2015-10-07</created><authors><author><keyname>Eichel</keyname><forenames>Justin A.</forenames></author><author><keyname>Mishra</keyname><forenames>Akshaya</forenames></author><author><keyname>Miller</keyname><forenames>Nicholas</forenames></author><author><keyname>Jankovic</keyname><forenames>Nicholas</forenames></author><author><keyname>Thomas</keyname><forenames>Mohan A.</forenames></author><author><keyname>Abbott</keyname><forenames>Tyler</forenames></author><author><keyname>Swanson</keyname><forenames>Douglas</forenames></author><author><keyname>Keller</keyname><forenames>Joel</forenames></author></authors><title>Diverse Large-Scale ITS Dataset Created from Continuous Learning for
  Real-Time Vehicle Detection</title><categories>cs.CV</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In traffic engineering, vehicle detectors are trained on limited datasets
resulting in poor accuracy when deployed in real world applications. Annotating
large-scale high quality datasets is challenging. Typically, these datasets
have limited diversity; they do not reflect the real-world operating
environment. There is a need for a large-scale, cloud based positive and
negative mining (PNM) process and a large-scale learning and evaluation system
for the application of traffic event detection. The proposed positive and
negative mining process addresses the quality of crowd sourced ground truth
data through machine learning review and human feedback mechanisms. The
proposed learning and evaluation system uses a distributed cloud computing
framework to handle data-scaling issues associated with large numbers of
samples and a high-dimensional feature space. The system is trained using
AdaBoost on $1,000,000$ Haar-like features extracted from $70,000$ annotated
video frames. The trained real-time vehicle detector achieves an accuracy of at
least $95\%$ for $1/2$ and about $78\%$ for $19/20$ of the time when tested on
approximately $7,500,000$ video frames. At the end of 2015, the dataset is
expect to have over one billion annotated video frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02065</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02065</id><created>2015-10-07</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Alexandre Domingues</forenames></author><author><keyname>Pessoa</keyname><forenames>Artur Alves</forenames></author><author><keyname>Drummond</keyname><forenames>L&#xfa;cia Maria de Assump&#xe7;&#xe3;o</forenames></author><author><keyname>Bentes</keyname><forenames>Cristiana</forenames></author><author><keyname>Farias</keyname><forenames>Ricardo</forenames></author></authors><title>Solving the Quadratic Assignment Problem on heterogeneous environment
  (CPUs and GPUs) with the application of Level 2 Reformulation and
  Linearization Technique</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Quadratic Assignment Problem, QAP, is a classic combinatorial
optimization problem, classified as NP-hard and widely studied. This problem
consists in assigning N facilities to N locations obeying the relation of 1 to
1, aiming to minimize costs of the displacement between the facilities. The
application of Reformulation and Linearization Technique, RLT, to the QAP leads
to a tight linear relaxation but large and difficult to solve. Previous works
based on level 3 RLT needed about 700GB of working memory to process one large
instances (N = 30 facilities). We present a modified version of the algorithm
proposed by Adams et al. which executes on heterogeneous systems (CPUs and
GPUs), based on level 2 RLT. For some instances, our algorithm is up to 140
times faster and occupy 97% less memory than the level 3 RLT version. The
proposed algorithm was able to solve by first time two instances: tai35b and
tai40b.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02067</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02067</id><created>2015-10-07</created><updated>2015-10-16</updated><authors><author><keyname>Lianeas</keyname><forenames>Thanasis</forenames></author><author><keyname>Nikolova</keyname><forenames>Evdokia</forenames></author><author><keyname>Stier-Moses</keyname><forenames>Nicolas E.</forenames></author></authors><title>Asymptotically tight bounds for inefficiency in risk-averse selfish
  routing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a nonatomic selfish routing model with independent stochastic
travel times, represented by mean and variance latency functions for each edge
that depend on their flows. In an effort to decouple the effect of risk-averse
player preferences from selfish behavior on the degradation of system
performance, Nikolova and Stier- Moses [16] defined the concept of the price of
risk aversion as the worst-case ratio of the cost of an equilibrium with
risk-averse players and that of an equilibrium with risk-neutral users. For
risk-averse users who seek to minimize the mean plus variance of travel time on
a path, they proved an upper bound on the price of risk aversion, which is
independent of the latency functions, and grows linearly with the size of the
graph and players' risk-aversion. In this follow-up paper, we provide a
matching lower bound for graphs with number of vertices equal to powers of two,
via the construction of a graph family inductively generated from the Braess
graph. We also provide conceptually different bounds, which we call functional,
that depend on the class of mean latency functions and provide
characterizations that are independent of the network topology (first derived,
in a more complicated way, by Meir and Parkes [10] in a different context with
different techniques). We also supplement the upper bound with a new
asymptotically-tight lower bound. Our third contribution is a tight bound on
the price of risk aversion for a family of graphs that generalize
series-parallel graphs which applies to users minimizing the mean plus standard
deviation of a path, a much more complex model of risk-aversion due to the cost
of a path being non-additive over edge costs. This is a refinement of previous
results in [16] that characterized the price of risk-aversion for
series-parallel graphs and for the Braess graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02070</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02070</id><created>2015-10-07</created><authors><author><keyname>Chatterjee</keyname><forenames>Kingshuk</forenames></author><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author></authors><title>Non-regular unary language and parallel communicating Watson-Crick
  automata systems</title><categories>cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1507.05284</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2006, Czeizler et.al. introduced parallel communicating Watson-Crick
automata system. They showed that parallel communicating Watson-Crick automata
system can accept the non-regular unary language L={a^(n^2 ),where n&gt;1} using
non-injective complementarity relation and three components. In this paper, we
improve on Czeizler et.al. work by showing that parallel communicating
Watson-Crick automata system can accept the same language L using just two
components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02071</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02071</id><created>2015-10-07</created><authors><author><keyname>Bettadapura</keyname><forenames>Vinay</forenames></author><author><keyname>Schindler</keyname><forenames>Grant</forenames></author><author><keyname>Plotz</keyname><forenames>Thomaz</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Augmenting Bag-of-Words: Data-Driven Discovery of Temporal and
  Structural Information for Activity Recognition</title><categories>cs.CV</categories><comments>8 pages</comments><journal-ref>Proceedings of the 2013 IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR 2013) -- Pages 2619 - 2626</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present data-driven techniques to augment Bag of Words (BoW) models, which
allow for more robust modeling and recognition of complex long-term activities,
especially when the structure and topology of the activities are not known a
priori. Our approach specifically addresses the limitations of standard BoW
approaches, which fail to represent the underlying temporal and causal
information that is inherent in activity streams. In addition, we also propose
the use of randomly sampled regular expressions to discover and encode patterns
in activities. We demonstrate the effectiveness of our approach in experimental
evaluations where we successfully recognize activities and detect anomalies in
four complex datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02073</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02073</id><created>2015-10-07</created><authors><author><keyname>Bettadapura</keyname><forenames>Vinay</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author><author><keyname>Pantofaru</keyname><forenames>Caroline</forenames></author></authors><title>Egocentric Field-of-View Localization Using First-Person Point-of-View
  Devices</title><categories>cs.CV</categories><comments>8 pages in Proceedings of the 2015 IEEE Winter Conference on
  Applications of Computer Vision (WACV 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique that uses images, videos and sensor data taken from
first-person point-of-view devices to perform egocentric field-of-view (FOV)
localization. We define egocentric FOV localization as capturing the visual
information from a person's field-of-view in a given environment and
transferring this information onto a reference corpus of images and videos of
the same space, hence determining what a person is attending to. Our method
matches images and video taken from the first-person perspective with the
reference corpus and refines the results using the first-person's head
orientation information obtained using the device sensors. We demonstrate
single and multi-user egocentric FOV localization in different indoor and
outdoor environments with applications in augmented reality, event
understanding and studying social interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02078</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02078</id><created>2015-10-07</created><authors><author><keyname>Bettadapura</keyname><forenames>Vinay</forenames></author><author><keyname>Thomaz</keyname><forenames>Edison</forenames></author><author><keyname>Parnami</keyname><forenames>Aman</forenames></author><author><keyname>Abowd</keyname><forenames>Gregory</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Leveraging Context to Support Automated Food Recognition in Restaurants</title><categories>cs.CV</categories><comments>8 pages in Proceedings of the 2015 IEEE Winter Conference on
  Applications of Computer Vision (WACV 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pervasiveness of mobile cameras has resulted in a dramatic increase in
food photos, which are pictures reflecting what people eat. In this paper, we
study how taking pictures of what we eat in restaurants can be used for the
purpose of automating food journaling. We propose to leverage the context of
where the picture was taken, with additional information about the restaurant,
available online, coupled with state-of-the-art computer vision techniques to
recognize the food being consumed. To this end, we demonstrate image-based
recognition of foods eaten in restaurants by training a classifier with images
from restaurant's online menu databases. We evaluate the performance of our
system in unconstrained, real-world settings with food images taken in 10
restaurants across 5 different types of food (American, Indian, Italian,
Mexican and Thai).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02086</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02086</id><created>2015-10-07</created><authors><author><keyname>Okech</keyname><forenames>Peter</forenames></author><author><keyname>Guire</keyname><forenames>Nicholas Mc</forenames></author><author><keyname>Okelo-Odongo</keyname><forenames>William</forenames></author></authors><title>Inherent Diversity in Replicated Architectures</title><categories>cs.DC cs.IT math.IT</categories><comments>in Yann Busnel. 11th European Dependable Computing Conference (EDCC
  2015), Sep 2015, Paris, France. 2015, Proceedings of Student Forum - EDCC
  2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we report our ongoing investigations of the inherent
non-determinism in contemporary execution environments that can potentially
lead to divergence in state of a multi-channel hardware/software system. Our
approach involved setting up of experiments to study execution path variability
of a simple program by tracing its execution at the kernel level. In the first
of the two experiments, we analyzed the execution path by repeated execution of
the program. In the second, we executed in parallel two instances of the same
program, each pinned to a separate processor core. Our results show that for a
program executing in a contemporary hardware/software platform , there is
sufcient path non-determinism in kernel space that can potentially lead to
diversity in replicated architectures. We believe the execution non-determinism
can impact the activation of residual systematic faults in software. If this is
true, then the inherent diversity can be used together with architectural means
to protect safety related systems against residual systematic faults in the
operating systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02104</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02104</id><created>2015-10-07</created><authors><author><keyname>Hughes</keyname><forenames>Jeffrey</forenames></author><author><keyname>Sparks</keyname><forenames>Cassandra</forenames></author><author><keyname>Stoughton</keyname><forenames>Alley</forenames></author><author><keyname>Parikh</keyname><forenames>Rinku</forenames></author><author><keyname>Reuther</keyname><forenames>Albert</forenames></author><author><keyname>Jagannathan</keyname><forenames>Suresh</forenames></author></authors><title>Building Resource Adaptive Software Systems (BRASS): Objectives and
  System Evaluation</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As modern software systems continue inexorably to increase in complexity and
capability, users have become accustomed to periodic cycles of updating and
upgrading to avoid obsolescence -- if at some cost in terms of frustration. In
the case of the U.S. military, having access to well-functioning software
systems and underlying content is critical to national security, but updates
are no less problematic than among civilian users and often demand considerable
time and expense. To address these challenges, DARPA has announced a new
four-year research project to investigate the fundamental computational and
algorithmic requirements necessary for software systems and data to remain
robust and functional in excess of 100 years. The Building Resource Adaptive
Software Systems, or BRASS, program seeks to realize foundational advances in
the design and implementation of long-lived software systems that can
dynamically adapt to changes in the resources they depend upon and environments
in which they operate. MIT Lincoln Laboratory will provide the test framework
and evaluation of proposed software tools in support of this revolutionary
vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02125</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02125</id><created>2015-10-07</created><authors><author><keyname>Schlangen</keyname><forenames>David</forenames></author><author><keyname>Zarriess</keyname><forenames>Sina</forenames></author><author><keyname>Kennington</keyname><forenames>Casey</forenames></author></authors><title>Resolving References to Objects in Photographs using the
  Words-As-Classifiers Model</title><categories>cs.CL</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced &quot;words as classifiers&quot; model of grounded semantics
(Kennington &amp; Schlangen 2015) views words as classifiers on (perceptual)
contexts, and composes the meaning of a phrase through 'soft' intersection of
the denotations of its component words. The model is trained from instances of
referential language use, and was first evaluated with references in a
game-playing scenario with a small number of different types of objects. Here,
we apply this model to a large set of real-world photographs (SAIAPR TC-12,
(Escalante et al. 2010)) that contain objects with a much larger variety of
types, and we show that it achieves good performance in a reference resolution
task on this data set. We also extend the model to deal with quantification and
negation, and evaluate these extensions, with good results. To investigate what
the classifiers learn, we introduce 'intensional' and 'denotational' word
vectors, and show that they capture meaning similarity in a way that is
different from and complementary to word2vec word embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02131</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02131</id><created>2015-10-07</created><authors><author><keyname>Iandola</keyname><forenames>Forrest N.</forenames></author><author><keyname>Shen</keyname><forenames>Anting</forenames></author><author><keyname>Gao</keyname><forenames>Peter</forenames></author><author><keyname>Keutzer</keyname><forenames>Kurt</forenames></author></authors><title>DeepLogo: Hitting Logo Recognition with the Deep Neural Network Hammer</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a flurry of industrial activity around logo
recognition, such as Ditto's service for marketers to track their brands in
user-generated images, and LogoGrab's mobile app platform for logo recognition.
However, relatively little academic or open-source logo recognition progress
has been made in the last four years. Meanwhile, deep convolutional neural
networks (DCNNs) have revolutionized a broad range of object recognition
applications. In this work, we apply DCNNs to logo recognition. We propose
several DCNN architectures, with which we surpass published state-of-art
accuracy on a popular logo recognition dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02135</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02135</id><created>2015-10-05</created><authors><author><keyname>Soumagne</keyname><forenames>Jerome</forenames></author><author><keyname>Carns</keyname><forenames>Philip H.</forenames></author><author><keyname>Kimpe</keyname><forenames>Dries</forenames></author><author><keyname>Koziol</keyname><forenames>Quincey</forenames></author><author><keyname>Ross</keyname><forenames>Robert B.</forenames></author></authors><title>A Remote Procedure Call Approach for Extreme-scale Services</title><categories>cs.DC</categories><comments>CSESSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When working at exascale, the various constraints imposed by the extreme
scale of the system bring new challenges for application users and
software/middleware developers. In that context, and to provide best
performance, resiliency and energy efficiency, software may be provided as a
service oriented approach, adjusting resource utilization to best meet facility
and user requirements. Remote procedure call (RPC) is a technique that
originally followed a client/server model and allowed local calls to be
transparently executed on remote resources. RPC consists of serializing the
local function parameters into a memory buffer and sending that buffer to a
remote target that in turn deserializes the parameters and executes the
corresponding function call, returning the result back to the caller. Building
reusable services requires the definition of a communication model to remotely
access these services and for this purpose, RPC can serve as a foundation for
accessing them. We introduce the necessary building blocks to enable this
ecosystem to software and middleware developers with an RPC framework called
Mercury.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02138</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02138</id><created>2015-10-07</created><authors><author><keyname>Alghazawy</keyname><forenames>Bahaa Aldeen</forenames></author><author><keyname>Fujita</keyname><forenames>Satoshi</forenames></author></authors><title>A Scheme for Maximal Resource Utilization in Peer-to-Peer Live Streaming</title><categories>cs.NI</categories><comments>16 pages in International Journal of Computer Networks &amp;
  Communications (IJCNC) Vol.7, No.5, September 2015</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.7, No.5, September 2015</journal-ref><doi>10.5121/ijcnc.2015.7502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer streaming technology has become one of the major Internet
applications as it offers the opportunity of broadcasting high quality video
content to a large number of peers with low costs. It is widely accepted that
with the efficient utilization of peers and server's upload capacities, peers
can enjoy watching a high bit rate video with minimal end-to-end delay. In this
paper, we present a practical scheduling algorithm that works in the
challenging condition where no spare capacity is available, i.e., it maximally
utilizes the resources and broadcasts the maximum streaming rate. Each peer
contacts with only a small number of neighbours in the overlay network and
autonomously subscribes to sub-streams according to a budget-model in such a
way that the number of peers forwarding exactly one sub-stream will be
maximized. The hop-count delay is also taken into account to construct a short
depth trees. Finally, we show through simulation that peers dynamically
converge to an efficient overlay structure with a short hop-count delay.
Moreover, the proposed scheme gives nice features in the homogeneous case and
overcomes SplitStream in all simulated scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02163</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02163</id><created>2015-10-07</created><authors><author><keyname>Noormofidi</keyname><forenames>Vahid</forenames></author><author><keyname>Atlas</keyname><forenames>Susan R.</forenames></author><author><keyname>Duan</keyname><forenames>Huaiyu</forenames></author></authors><title>Performance Analysis of an Astrophysical Simulation Code on the Intel
  Xeon Phi Architecture</title><categories>cs.DC cs.CE</categories><report-no>CARC-2015-174</report-no><acm-class>D.1.3; I.6.8; D.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed the astrophysical simulation code XFLAT to study neutrino
oscillations in supernovae. XFLAT is designed to utilize multiple levels of
parallelism through MPI, OpenMP, and SIMD instructions (vectorization). It can
run on both CPU and Xeon Phi co-processors based on the Intel Many Integrated
Core Architecture (MIC). We analyze the performance of XFLAT on configurations
with CPU only, Xeon Phi only and both CPU and Xeon Phi. We also investigate the
impact of I/O and the multi-node performance of XFLAT on the Xeon Phi-equipped
Stampede supercomputer at the Texas Advanced Computing Center (TACC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02173</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02173</id><created>2015-10-07</created><updated>2015-10-09</updated><authors><author><keyname>Assael</keyname><forenames>John-Alexander M.</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Niklas</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author></authors><title>Data-Efficient Learning of Feedback Policies from Image Pixels using
  Deep Dynamical Models</title><categories>cs.AI cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-efficient reinforcement learning (RL) in continuous state-action spaces
using very high-dimensional observations remains a key challenge in developing
fully autonomous systems. We consider a particularly important instance of this
challenge, the pixels-to-torques problem, where an RL agent learns a
closed-loop control policy (&quot;torques&quot;) from pixel information only. We
introduce a data-efficient, model-based reinforcement learning algorithm that
learns such a closed-loop policy directly from pixel information. The key
ingredient is a deep dynamical model for learning a low-dimensional feature
embedding of images jointly with a predictive model in this low-dimensional
feature space. Joint learning is crucial for long-term predictions, which lie
at the core of the adaptive nonlinear model predictive control strategy that we
use for closed-loop control. Compared to state-of-the-art RL methods for
continuous states and actions, our approach learns quickly, scales to
high-dimensional state spaces, is lightweight and an important step toward
fully autonomous end-to-end learning from pixels to torques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02177</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02177</id><created>2015-10-07</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Mehmood</keyname><forenames>Irfan</forenames></author><author><keyname>Lee</keyname><forenames>Mi Young</forenames></author><author><keyname>Ji</keyname><forenames>Su Mi</forenames></author><author><keyname>Baik</keyname><forenames>Sung Wook</forenames></author></authors><title>Ontology-based Secure Retrieval of Semantically Significant Visual
  Contents</title><categories>cs.MM cs.IR</categories><comments>A short paper of 11 pages for secure visual contents retrieval.The
  original version can be accessed at this link:
  http://www.kingpc.or.kr/inc_html/index.html</comments><journal-ref>Khan Muhammad, Irfan Mehmood, Mi Young Lee, Su Mi Ji, Sung Wook
  Baik, &quot;Ontology-based Secure Retrieval of Semantically Significant Visual
  Contents,&quot; JOURNAL OF KOREAN INSTITUTE OF NEXT GENERATION COMPUTING, vol. 11,
  pp. 87-96, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image classification is an enthusiastic research field where large amount of
image data is classified into various classes based on their visual contents.
Researchers have presented various low-level features-based techniques for
classifying images into different categories. However, efficient and effective
classification and retrieval is still a challenging problem due to complex
nature of visual contents. In addition, the traditional information retrieval
techniques are vulnerable to security risks, making it easy for attackers to
retrieve personal visual contents such as patients records and law enforcement
agencies databases. Therefore, we propose a novel ontology-based framework
using image steganography for secure image classification and information
retrieval. The proposed framework uses domain-specific ontology for mapping the
low-level image features to high-level concepts of ontologies which
consequently results in efficient classification. Furthermore, the proposed
method utilizes image steganography for hiding the image semantics as a secret
message inside them, making the information retrieval process secure from third
parties. The proposed framework minimizes the computational complexity of
traditional techniques, increasing its suitability for secure and real-time
visual contents retrieval from personalized image databases. Experimental
results confirm the efficiency, effectiveness, and security of the proposed
framework as compared with other state-of-the-art systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02181</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02181</id><created>2015-10-07</created><updated>2016-02-16</updated><authors><author><keyname>Erickson</keyname><forenames>Alejandro</forenames></author><author><keyname>Stewart</keyname><forenames>and Iain A.</forenames></author><author><keyname>Navaridas</keyname><forenames>Javier</forenames></author><author><keyname>Kiasari</keyname><forenames>Abbas E.</forenames></author></authors><title>The Stellar Transformation: From Interconnection Networks to Datacenter
  Networks</title><categories>cs.DC</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first dual-port server-centric datacenter network, FiConn, was introduced
in 2009, and there are several others now in existence; however, the pool of
topologies to choose from remains small. We propose a new generic construction
that dramatically increases the size of this pool by facilitating the
transformation of well-studied topologies from interconnection networks, along
with their networking properties and routing algorithms, into viable
server-centric datacenter network topologies. Under our transformation,
numerous interconnection networks yield datacenter network topologies with
good, and easily computed, baseline properties. We instantiate our construction
so as to apply it to generalized hypercubes and obtain the datacenter networks
GQ*. Our construction automatically yields routing algorithms for GQ* and we
empirically compare GQ* and its routing algorithms with the established
datacenter networks FiConn and DPillar; this comparison is with respect to
network throughput, latency, load balancing, scalability, fault tolerance, and
cost. We find that GQ* outperforms both FiConn and DPillar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02184</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02184</id><created>2015-10-07</created><updated>2016-01-14</updated><authors><author><keyname>Suhov</keyname><forenames>Yuri</forenames></author><author><keyname>Stuhl</keyname><forenames>Izabella</forenames></author><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author><author><keyname>Kelbert</keyname><forenames>Mark</forenames></author></authors><title>Basic inequalities for weighted entropies</title><categories>cs.IT math.IT math.PR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1409.4102</comments><journal-ref>Aequationes Mathematicae (2016)</journal-ref><doi>10.1007/s00010-015-0396-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of weighted entropy takes into account values of different
outcomes, i.e., makes entropy context-dependent, through the weight function.
In this paper, we establish a number of simple inequalities for the weighted
entropies (general as well as specific), mirroring similar bounds on standard
(Shannon) entropies and related quantities. The required assumptions are
written in terms of various expectations of the weight functions. Examples are
weighted Ky Fan and weighted Hadamard inequalities involving determinants of
positive-definite matrices, and weighted Cram\'{e}r-Rao inequalities involving
the weighted Fisher information matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02186</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02186</id><created>2015-10-07</created><authors><author><keyname>Wei</keyname><forenames>Ning</forenames></author><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongpei</forenames></author></authors><title>Optimal Relay Probing in Millimeter Wave Cellular Systems with
  Device-to-Device Relaying</title><categories>cs.IT cs.ET math.IT</categories><comments>13 pages, 3 figures, submitted to IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter-wave (mmWave) cellular systems are power-limited and susceptible
to blockages. As a result, mmWave connectivity will be likely to be
intermittent. One promising approach to increasing mmWave connectivity and
range is to use relays. Device-to-device (D2D) communications open the door to
the vast opportunities of D2D and device-to-network relaying for mmWave
cellular systems. In this correspondence, we study how to select a good relay
for a given source-destination pair in a two-hop mmWave cellular system, where
the mmWave links are subject to random Bernoulli blockages. In such a system,
probing more relays could potentially lead to the discovery of a better relay
but at the cost of more overhead. We find that the throughput-optimal relay
probing strategy is a pure threshold policy: the system can stop relay probing
once the achievable spectral efficiency of the currently probed two-hop link
exceeds some threshold. In general, the spectral efficiency threshold can be
obtained by solving a fixed point equation. For the special case with on/off
mmWave links, we derive a closed-form solution for the threshold. Numerical
results demonstrate that the threshold-based relay probing strategy can yield
remarkable throughput gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02188</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02188</id><created>2015-10-07</created><authors><author><keyname>Deng</keyname><forenames>Zhi-Hong</forenames></author><author><keyname>Ma</keyname><forenames>Shulei</forenames></author><author><keyname>Liu</keyname><forenames>He</forenames></author></authors><title>An Efficient Data Structure for Fast Mining High Utility Itemsets</title><categories>cs.DB cs.DS</categories><comments>25 pages,9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel data structure called PUN-list, which
maintains both the utility information about an itemset and utility upper bound
for facilitating the processing of mining high utility itemsets. Based on
PUN-lists, we present a method, called MIP (Mining high utility Itemset using
PUN-Lists), for fast mining high utility itemsets. The efficiency of MIP is
achieved with three techniques. First, itemsets are represented by a highly
condensed data structure, PUN-list, which avoids costly, repeatedly utility
computation. Second, the utility of an itemset can be efficiently calculated by
scanning the PUN-list of the itemset and the PUN-lists of long itemsets can be
fast constructed by the PUN-lists of short itemsets. Third, by employing the
utility upper bound lying in the PUN-lists as the pruning strategy, MIP
directly discovers high utility itemsets from the search space, called
set-enumeration tree, without generating numerous candidates. Extensive
experiments on various synthetic and real datasets show that PUN-list is very
effective since MIP is at least an order of magnitude faster than recently
reported algorithms on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02189</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02189</id><created>2015-10-07</created><updated>2016-03-02</updated><authors><author><keyname>Nakanishi-Ohno</keyname><forenames>Yoshinori</forenames></author><author><keyname>Obuchi</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Okada</keyname><forenames>Masato</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Sparse approximation based on a random overcomplete basis</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>35 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a strategy of sparse approximation that is based on the use of an
overcomplete basis, and evaluate its performance when a random matrix is used
as this basis. A small combination of basis vectors is chosen from a given
overcomplete basis, according to a given compression rate, such that they
compactly represent the target data with as small a distortion as possible. As
a selection method, we study the $\ell_0$- and $\ell_1$-based methods, which
employ the exhaustive search and $\ell_1$-norm regularization techniques,
respectively. The performance is assessed in terms of the trade-off relation
between the representation distortion and the compression rate. First, we
evaluate the performance analytically in the case that the methods are carried
out ideally, using methods of statistical mechanics. Our result clarifies the
fact that the $\ell_0$-based method greatly outperforms the $\ell_1$-based one.
Second, we examine the practical performances of two well-known algorithms,
orthogonal matching pursuit and approximate message passing, when they are used
to execute the $\ell_0$- and $\ell_1$-based methods, respectively. Our
examination shows that orthogonal matching pursuit achieves a much better
performance than the exact execution of the $\ell_1$-based method, as well as
approximate message passing. However, regarding the $\ell_0$-based method,
there is still room to design more effective greedy algorithms than orthogonal
matching pursuit. Finally, we evaluate the performances of the algorithms when
they are applied to image data compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02190</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02190</id><created>2015-10-07</created><updated>2016-01-03</updated><authors><author><keyname>Kostina</keyname><forenames>Victoria</forenames></author></authors><title>Data compression with low distortion and finite blocklength</title><categories>cs.IT math.IT</categories><comments>Presented in part in Proceedings 53rd Annual Allerton Conference on
  Communication, Control and Computing, Monticello, IL, Oct. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers lossy source coding of $n$-dimensional continuous
memoryless sources with low distortion and shows a simple, explicit
approximation to the minimum source coding rate. More precisely, a
nonasymptotic version of Shannon's lower bound is presented. Lattice quantizers
are shown to approach that lower bound, provided that the source density is
smooth enough and the distortion is low, which implies that fine
multidimensional lattice coverings are nearly optimal in the rate-distortion
sense even at finite $n$. The achievability proof technique avoids both the
usual random coding argument and the simplifying assumption of the presence of
a dither signal.
  The paper also presents a necessary and sufficient condition for Shannon's
lower bound to be attained exactly. Although most continuous sources violate
that condition, all finite alphabet sources satisfy it at low distortion
levels. The rate-dispersion function is then given simply by the varentropy of
the source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02192</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02192</id><created>2015-10-07</created><authors><author><keyname>Tzeng</keyname><forenames>Eric</forenames></author><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Simultaneous Deep Transfer Across Domains and Tasks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent reports suggest that a generic supervised deep CNN model trained on a
large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning
deep models in a new domain can require a significant amount of labeled data,
which for many applications is simply not available. We propose a new CNN
architecture to exploit unlabeled and sparsely labeled target domain data. Our
approach simultaneously optimizes for domain invariance to facilitate domain
transfer and uses a soft label distribution matching loss to transfer
information between tasks. Our proposed adaptation method offers empirical
performance which exceeds previously published results on two standard
benchmark visual domain adaptation tasks, evaluated across supervised and
semi-supervised adaptation settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02195</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02195</id><created>2015-10-08</created><authors><author><keyname>Sun</keyname><forenames>Xiaorui</forenames></author><author><keyname>Wilmes</keyname><forenames>John</forenames></author></authors><title>Structure and automorphisms of primitive coherent configurations</title><categories>math.CO cs.CC math.GR</categories><comments>An extended abstract of this paper appeared in the Proceedings of the
  47th ACM Symposium on Theory of Computing (STOC'15) under the title &quot;Faster
  canonical forms for primitive coherent configurations&quot;</comments><msc-class>05E18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coherent configurations (CCs) are highly regular colorings of the set of
ordered pairs of a &quot;vertex set&quot;; each color represents a &quot;constituent digraph.&quot;
A CC is primitive (PCC) if all its constituent digraphs are connected.
  We address the problem of classifying PCCs with large automorphism groups.
This project was started in Babai's 1981 paper in which he showed that only the
trivial PCC admits more than $\exp(\tilde{O}(n^{1/2}))$ automorphisms. (Here,
$n$ is the number of vertices and the $\tilde{O}$ hides polylogarithmic
factors.)
  In the present paper we classify all PCCs with more than
$\exp(\tilde{O}(n^{1/3}))$ automorphisms, making the first progress on Babai's
conjectured classification of all PCCs with more than $\exp(n^{\epsilon})$
automorphisms.
  A corollary to Babai's 1981 result solved a then 100-year-old problem on
primitive but not doubly transitive permutation groups, giving an
$\exp(\tilde{O}(n^{1/2}))$ bound on their order. In a similar vein, our result
implies an $\exp(\tilde{O}(n^{1/3}))$ upper bound on the order of such groups,
with known exceptions. This improvement of Babai's result was previously known
only through the Classification of Finite Simple Groups (Cameron, 1981), while
our proof, like Babai's, is elementary and almost purely combinatorial.
  Our result also has implications to the complexity of the graph isomorphism
problem. PCCs arise naturally as obstacles to combinatorial partitioning
approaches to the problem. Our results give an algorithm for deciding
isomorphism of PCCs in time $\exp(\tilde{O}(n^{1/3}))$, the first improvement
over Babai's $\exp(\tilde{O}(n^{1/2}))$ bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02196</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02196</id><created>2015-10-08</created><authors><author><keyname>Hosseini</keyname><forenames>Vahid</forenames></author></authors><title>Algorithm and Related Application for Smart Wearable Devices to Reduce
  the Risk of Death and Brain Damage in Diabetic Coma</title><categories>cs.HC cs.CY</categories><comments>4 pages, 1 figure</comments><msc-class>68U99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diabetes is an epidemic disease of the 21st century and is growing globally.
Although, final diabetes treatments and cure are still on research phase,
related complications of diabetes endanger life of diabetic patients. Diabetic
coma which happens with extreme high or low blood glucose is one of the risk
factor for diabetic patients and if it remains unattended will lead to patient
death or permanent brain damage. To reduce the risk of such deaths or damages,
a novel algorithm for wearable devices application, especially for smart
watches are proposed. Such application can inform the patients relatives or
emergency centers, if the person falls in coma or irresponsive condition based
on readouts from smart watches sensors including mobility, heart rate and skin
moisture. However; such an application is not a final solution to detect all
types of coma, but it potentially could save lives of many patients, if widely
used among the diabetic patients around the world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02197</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02197</id><created>2015-10-08</created><authors><author><keyname>&#x106;usti&#x107;</keyname><forenames>Ante</forenames></author><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author></authors><title>A characterization of linearizable instances of the quadratic minimum
  spanning tree problem</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate special cases of the quadratic minimum spanning tree problem
(QMSTP) on a graph $G=(V,E)$ that can be solved as a linear minimum spanning
tree problem. Characterization of such problems on graphs with special
properties are given. This include complete graphs, complete bipartite graphs,
cactuses among others. Our characterization can be verified in $O(|E|^2)$ time.
In the case of complete graphs and when the cost matrix is given in factored
form, we show that our characterization can be verified in $O(|E|)$ time.
Related open problems are also indicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02211</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02211</id><created>2015-10-08</created><authors><author><keyname>Amram</keyname><forenames>Gal</forenames></author></authors><title>The F-snapshot Problem</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aguilera, Gafni and Lamport introduced the signaling problem in [5]. In this
problem, two processes numbered 0 and 1 can call two procedures: update and
Fscan. A parameter of the problem is a two- variable function $F(x_0,x_1)$.
Each process $p_i$ can assign values to variable $x_i$ by calling update(v)
with some data value v, and compute the value: $F(x_0,x_1)$ by executing an
Fscan procedure. The problem is interesting when the domain of $F$ is infinite
and the range of $F$ is finite. In this case, some &quot;access restrictions&quot; are
imposed that limit the size of the registers that the Fscan procedure can
access. Aguilera et al. provided a non-blocking solution and asked whether a
wait-free solution exists. A positive answer can be found in [7].
  The natural generalization of the two-process signaling problem to an
arbitrary number of processes turns out to yield an interesting generalization
of the fundamental snapshot problem, which we call the F-snapshot problem. In
this problem $n$ processes can write values to an $n$-segment array (each
process to its own segment), and can read and obtain the value of an n-variable
function $F$ on the array of segments. In case that the range of $F$ is finite,
it is required that only bounded registers are accessed when the processes
apply the function $F$ to the array, although the data values written to the
segments may be taken from an infinite set. We provide here an affirmative
answer to the question of Aguilera et al. for an arbitrary number of processes.
Our solution employs only single-writer atomic registers, and its time
complexity is $O(n \log n)$, which is also the time complexity of the fastest
snapshot algorithm that uses only single-writer registers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02215</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02215</id><created>2015-10-08</created><authors><author><keyname>Elenberg</keyname><forenames>Ethan R.</forenames></author><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Distributed Estimation of Graph 4-Profiles</title><categories>cs.SI cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel distributed algorithm for counting all four-node induced
subgraphs in a big graph. These counts, called the $4$-profile, describe a
graph's connectivity properties and have found several uses ranging from
bioinformatics to spam detection. We also study the more complicated problem of
estimating the local $4$-profiles centered at each vertex of the graph. The
local $4$-profile embeds every vertex in an $11$-dimensional space that
characterizes the local geometry of its neighborhood: vertices that connect
different clusters will have different local $4$-profiles compared to those
that are only part of one dense cluster.
  Our algorithm is a local, distributed message-passing scheme on the graph and
computes all the local $4$-profiles in parallel. We rely on two novel
theoretical contributions: we show that local $4$-profiles can be calculated
using compressed two-hop information and also establish novel concentration
results that show that graphs can be substantially sparsified and still retain
good approximation quality for the global $4$-profile.
  We empirically evaluate our algorithm using a distributed GraphLab
implementation that we scaled up to $640$ cores. We show that our algorithm can
compute global and local $4$-profiles of graphs with millions of edges in a few
minutes, significantly improving upon the previous state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02219</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02219</id><created>2015-10-08</created><authors><author><keyname>Tang</keyname><forenames>Nan</forenames></author><author><keyname>Chen</keyname><forenames>Qing</forenames></author><author><keyname>Mitra</keyname><forenames>Prasenjit</forenames></author></authors><title>On Summarizing Graph Streams</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph streams, which refer to the graph with edges being updated sequentially
in a form of a stream, have wide applications such as cyber security, social
networks and transportation networks. This paper studies the problem of
summarizing graph streams. Specifically, given a graph stream G, directed or
undirected, the objective is to summarize G as S with much smaller (sublinear)
space, linear construction time and constant maintenance cost for each edge
update, such that S allows many queries over G to be approximately conducted
efficiently. Due to the sheer volume and highly dynamic nature of graph
streams, summarizing them remains a notoriously hard, if not impossible,
problem. The widely used practice of summarizing data streams is to treat each
element independently by e.g., hash- or sampling-based method, without keeping
track of the connections between elements in a data stream, which gives these
summaries limited power in supporting complicated queries over graph streams.
This paper discusses a fundamentally different philosophy for summarizing graph
streams. We present gLava, a probabilistic graph model that, instead of
treating an edge (a stream element) as the operating unit, uses the finer
grained node in an element. This will naturally form a new graph sketch where
edges capture the connections inside elements, and nodes maintain relationships
across elements. We discuss a wide range of supported graph queries and
establish theoretical error bounds for basic queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02220</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02220</id><created>2015-10-08</created><updated>2015-10-16</updated><authors><author><keyname>John</keyname><forenames>Wolfgang</forenames></author><author><keyname>Meirosu</keyname><forenames>Catalin</forenames></author><author><keyname>Sk&#xf6;ldstr&#xf6;m</keyname><forenames>Pontus</forenames></author><author><keyname>Nemeth</keyname><forenames>Felician</forenames></author><author><keyname>Gulyas</keyname><forenames>Andras</forenames></author><author><keyname>Kind</keyname><forenames>Mario</forenames></author><author><keyname>Sharma</keyname><forenames>Sachin</forenames></author><author><keyname>Papafili</keyname><forenames>Ioanna</forenames></author><author><keyname>Agapiou</keyname><forenames>George</forenames></author><author><keyname>Marchetto</keyname><forenames>Guido</forenames></author><author><keyname>Sisto</keyname><forenames>Riccardo</forenames></author><author><keyname>Steinert</keyname><forenames>Rebecca</forenames></author><author><keyname>Kreuger</keyname><forenames>Per</forenames></author><author><keyname>Abrahamsson</keyname><forenames>Henrik</forenames></author><author><keyname>Manzalini</keyname><forenames>Antonio</forenames></author><author><keyname>Sarrar</keyname><forenames>Nadi</forenames></author></authors><title>Initial Service Provider DevOps concept, capabilities and proposed tools</title><categories>cs.NI</categories><comments>Deliverable D4.1 of the EU FP7 UNIFY project (ICT-619609)-&quot;Initial
  requirements for the SP-DevOps concept, Universal Node capabilities and
  proposed tools&quot;. Original Deliverable published at
  https://www.fp7-unify.eu/files/fp7-unify-eu-docs/Results/Deliverables/UNIFY_D4.1%20Initial%20requirements%20for%20the%20SP-DevOps%20concept,%20universal%20node%20capabilities%20and%20proposed%20tools.pdf</comments><acm-class>C.2.1; C.2.2; C.2.3; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents a first sketch of the Service Provider DevOps concept
including four major management processes to support the roles of both service
and VNF developers as well as the operator in a more agile manner. The sketch
is based on lessons learned from a study of management and operational
practices in the industry and recent related work with respect to management of
SDN and cloud. Finally, the report identifies requirements for realizing
SP-DevOps within an combined cloud and transport network environment as
outlined by the UNIFY NFV architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02225</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02225</id><created>2015-10-08</created><authors><author><keyname>Tijani</keyname><forenames>Khadija</forenames><affiliation>CSTB, G-SCOP\_GCSP, LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Kashif</keyname><forenames>Ayesha</forenames><affiliation>G-SCOP\_GCSP</affiliation></author><author><keyname>Ploix</keyname><forenames>St&#xe9;phane</forenames><affiliation>G-SCOP\_GCSP</affiliation></author><author><keyname>Haas</keyname><forenames>Benjamin</forenames><affiliation>CSTB</affiliation></author><author><keyname>Dugdale</keyname><forenames>Julie</forenames><affiliation>LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Comparison between purely statistical and multi-agent based ap-proaches
  for occupant behaviour modeling in buildings</title><categories>cs.MA</categories><comments>in IBPSA 2014 France, May 2014, Arras, France. 2014</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes two modeling approaches for occupant behaviour in
buildings. It compares a purely statistical approach with a multi-agent social
simulation based approach. The study concerns the door openings in an office.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02229</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02229</id><created>2015-10-08</created><authors><author><keyname>Bartoletti</keyname><forenames>Massimo</forenames></author><author><keyname>Castellani</keyname><forenames>Ilaria</forenames></author><author><keyname>Deni&#xe9;lou</keyname><forenames>Pierre-Malo</forenames></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames></author><author><keyname>Ghilezan</keyname><forenames>Silvia</forenames></author><author><keyname>Pantovic</keyname><forenames>Jovanka</forenames></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author><author><keyname>Toninho</keyname><forenames>Bernardo</forenames></author><author><keyname>Vieira</keyname><forenames>Hugo Torres</forenames></author></authors><title>Combining behavioural types with security analysis</title><categories>cs.PL cs.CR cs.DC</categories><proxy>ccsd</proxy><journal-ref>Journal of Logical and Algebraic Methods in Programming, Elsevier,
  2015, pp.18</journal-ref><doi>10.1016/j.jlamp.2015.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's software systems are highly distributed and interconnected, and they
increasingly rely on communication to achieve their goals; due to their
societal importance, security and trustworthiness are crucial aspects for the
correctness of these systems. Behavioural types, which extend data types by
describing also the structured behaviour of programs, are a widely studied
approach to the enforcement of correctness properties in communicating systems.
This paper offers a unified overview of proposals based on behavioural types
which are aimed at the analysis of security properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02237</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02237</id><created>2015-10-08</created><authors><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>Explicit Parallel-in-time Integration of a Linear Acoustic-Advection
  System</title><categories>cs.CE cs.DC math.NA</categories><journal-ref>Computers &amp; Fluids 59, pp. 72-83, 2012</journal-ref><doi>10.1016/j.compfluid.2012.02.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The applicability of the Parareal parallel-in-time integration scheme for the
solution of a linear, two-dimensional hyperbolic acoustic-advection system,
which is often used as a test case for integration schemes for numerical
weather prediction (NWP), is addressed. Parallel-in-time schemes are a possible
way to increase, on the algorithmic level, the amount of parallelism, a
requirement arising from the rapidly growing number of CPUs in high performance
computer systems. A recently introduced modification of the &quot;parallel implicit
time-integration algorithm&quot; could successfully solve hyperbolic problems
arising in structural dynamics. It has later been cast into the framework of
Parareal. The present paper adapts this modified Parareal and employs it for
the solution of a hyperbolic flow problem, where the initial value problem
solved in parallel arises from the spatial discretization of a partial
differential equation by a finite difference method. It is demonstrated that
the modified Parareal is stable and can produce reasonably accurate solutions
while allowing for a noticeable reduction of the time-to-solution. The
implementation relies on integration schemes already widely used in NWP (RK-3,
partially split forward Euler, forward-backward). It is demonstrated that using
an explicit partially split scheme for the coarse integrator allows to avoid
the use of an implicit scheme while still achieving speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02238</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02238</id><created>2015-10-08</created><authors><author><keyname>Marot</keyname><forenames>Michel</forenames></author><author><keyname>Sa&#xef;d</keyname><forenames>Adel Mounir</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author></authors><title>On the Maximal Shortest Path in a Connected Component in V2V</title><categories>cs.PF cs.NI</categories><comments>Accepted for publication in Performance Evaluation</comments><doi>10.1016/j.peva.2015.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a VANET (Vehicular Ad-hoc NETwork) is considered to operate on
a simple lane, without infrastructure. The arrivals of vehicles are assumed to
be general with any traffic and speed assumptions. The vehicles communicate
through the shortest path. In this paper, we study the probability distribution
of the number of hops on the maximal shortest path in a connected component of
vehicles. The general formulation is given for any assumption of road traffic.
Then, it is applied to calculate the z-transform of this distribution for
medium and dense networks in the Poisson case. Our model is validated with the
Madrid road traces of the Universitat Polit\`ecnica de Catalunya. These results
may be useful for example when evaluating diffusion protocols through the
shortest path in a VANET, where not only the mean but also the other moments
are needed to derive accurate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02255</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02255</id><created>2015-10-08</created><authors><author><keyname>Upadhya</keyname><forenames>Vidyadhar</forenames></author><author><keyname>Sastry</keyname><forenames>P. S.</forenames></author></authors><title>Empirical Analysis of Sampling Based Estimators for Evaluating RBMs</title><categories>cs.LG stat.ML</categories><comments>edited version of this manuscript will appear in proceedings of
  International Conference on Neural Information Processing (ICONIP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Restricted Boltzmann Machines (RBM) can be used either as classifiers or
as generative models. The quality of the generative RBM is measured through the
average log-likelihood on test data. Due to the high computational complexity
of evaluating the partition function, exact calculation of test log-likelihood
is very difficult. In recent years some estimation methods are suggested for
approximate computation of test log-likelihood. In this paper we present an
empirical comparison of the main estimation methods, namely, the AIS algorithm
for estimating the partition function, the CSL method for directly estimating
the log-likelihood, and the RAISE algorithm that combines these two ideas. We
use the MNIST data set to learn the RBM and then compare these methods for
estimating the test log-likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02259</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02259</id><created>2015-10-08</created><updated>2015-11-05</updated><authors><author><keyname>Ryu</keyname><forenames>Hyun-Gyu</forenames></author><author><keyname>Lee</keyname><forenames>Sang-Keum</forenames></author><author><keyname>Har</keyname><forenames>Dongsoo</forenames></author></authors><title>Data Transmission with Reduced Delay for Distributed Acoustic Sensors</title><categories>cs.DC cs.NI</categories><comments>Accepted to IJDSN, final preprinted version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a channel access control scheme fit to dense acoustic
sensor nodes in a sensor network. In the considered scenario, multiple acoustic
sensor nodes within communication range of a cluster head are grouped into
clusters. Acoustic sensor nodes in a cluster detect acoustic signals and
convert them into electric signals (packets). Detection by acoustic sensors can
be executed periodically or randomly and random detection by acoustic sensors
is event driven. As a result, each acoustic sensor generates their packets
(50bytes each) periodically or randomly over short time intervals
(400ms~4seconds) and transmits directly to a cluster head (coordinator node).
Our approach proposes to use a slotted carrier sense multiple access. All
acoustic sensor nodes in a cluster are allocated to time slots and the number
of allocated sensor nodes to each time slot is uniform. All sensor nodes
allocated to a time slot listen for packet transmission from the beginning of
the time slot for a duration proportional to their priority. The first node
that detect the channel to be free for its whole window is allowed to transmit.
The order of packet transmissions with the acoustic sensor nodes in the time
slot is autonomously adjusted according to the history of packet transmissions
in the time slot. In simulations, performances of the proposed scheme are
demonstrated by the comparisons with other low rate wireless channel access
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02305</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02305</id><created>2015-10-08</created><authors><author><keyname>Sun</keyname><forenames>Qifu Tyler</forenames></author><author><keyname>Li</keyname><forenames>Shuo-Yen Robert</forenames></author><author><keyname>Li</keyname><forenames>Zongpeng</forenames></author></authors><title>On Base Field of Linear Network Coding</title><categories>cs.IT math.IT</categories><comments>29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a (single-source) multicast network, the size of a base field is the most
known and studied algebraic identity that is involved in characterizing its
linear solvability over the base field. In this paper, we design a new class
$\mathcal{N}$ of multicast networks and obtain an explicit formula for the
linear solvability of these networks, which involves the associated coset
numbers of a multiplicative subgroup in a base field. The concise formula turns
out to be the first that matches the topological structure of a multicast
network and algebraic identities of a field other than size. It further
facilitates us to unveil \emph{infinitely many} new multicast networks linearly
solvable over GF($q$) but not over GF($q'$) with $q &lt; q'$, based on a subgroup
order criterion. In particular, i) for every $k\geq 2$, an instance in
$\mathcal{N}$ can be found linearly solvable over GF($2^{2k}$) but \emph{not}
over GF($2^{2k+1}$), and ii) for arbitrary distinct primes $p$ and $p'$, there
are infinitely many $k$ and $k'$ such that an instance in $\mathcal{N}$ can be
found linearly solvable over GF($p^k$) but \emph{not} over GF($p'^{k'}$) with
$p^k &lt; p'^{k'}$. On the other hand, the construction of $\mathcal{N}$ also
leads to a new class of multicast networks with $\Theta(q^2)$ nodes and
$\Theta(q^2)$ edges, where $q \geq 5$ is the minimum field size for linear
solvability of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02318</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02318</id><created>2015-10-08</created><authors><author><keyname>Asoodeh</keyname><forenames>Shahab</forenames></author><author><keyname>Alajaji</keyname><forenames>Fady</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Notes on Information-Theoretic Privacy</title><categories>cs.IT math.IT</categories><comments>The corrected version of a paper appeared in Allerton 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the tradeoff between privacy and utility in a situation where
both privacy and utility are measured in terms of mutual information. For the
binary case, we fully characterize this tradeoff in case of perfect privacy and
also give an upper-bound for the case where some privacy leakage is allowed. We
then introduce a new quantity which quantifies the amount of private
information contained in the observable data and then connect it to the optimal
tradeoff between privacy and utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02322</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02322</id><created>2015-10-08</created><updated>2015-10-12</updated><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author></authors><title>Small-Area Orthogonal Drawings of 3-Connected Graphs</title><categories>cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that every graph with maximum degree 4 has an orthogonal
drawing with area at most $\frac{49}{64} n^2+O(n) \approx 0.76n^2$. In this
paper, we show that if the graph is 3-connected, then the area can be reduced
even further to $\frac{9}{16}n^2+O(n) \approx 0.56n^2$. The drawing uses the
3-canonical order for (not necessarily planar) 3-connected graphs, which is a
special Mondshein sequence and can hence be computed in linear time. To our
knowledge, this is the first application of a Mondshein sequence in graph
drawing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02324</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02324</id><created>2015-10-08</created><authors><author><keyname>Ho&#xe0;ng</keyname><forenames>Ch&#xed;nh T.</forenames></author></authors><title>On the structure of (banner, odd hole)-free graphs</title><categories>math.CO cs.DM</categories><msc-class>05C15</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hole is a chordless cycle with at least four vertices. A hole is odd if it
has an odd number of vertices. A banner is a graph which consists of a hole on
four vertices and a single vertex with precisely one neighbor on the hole. We
prove that a (banner, odd hole)-free graph is either perfect, or does not
contain a stable set on three vertices, or contains a homogeneous set. Using
this structure result, we design a polynomial-time algorithm for recognizing
(banner, odd hole)-free graphs. We also design polynomial-time algorithms to
find, for such a graph, a minimum coloring and largest stable set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02330</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02330</id><created>2015-10-08</created><authors><author><keyname>Asoodeh</keyname><forenames>Shahab</forenames></author><author><keyname>Alajaji</keyname><forenames>Fady</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>On Maximal Correlation, Mutual Information and Data Privacy</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Appeared in Canadian Workshop on Information Theory 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rate-privacy function is defined in \cite{Asoodeh} as a tradeoff between
privacy and utility in a distributed private data system in which both privacy
and utility are measured using mutual information. Here, we use maximal
correlation in lieu of mutual information in the privacy constraint. We first
obtain some general properties and bounds for maximal correlation and then
modify the rate-privacy function to account for the privacy-constrained
estimation problem. We find a bound for the utility in this problem when the
maximal correlation privacy is set to some threshold $\epsilon&gt;0$ and construct
an explicit privacy scheme which achieves this bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02338</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02338</id><created>2015-10-08</created><authors><author><keyname>Proskurnikov</keyname><forenames>Anton V.</forenames></author><author><keyname>Cao</keyname><forenames>Ming</forenames></author></authors><title>Synchronization of Pulse-Coupled Oscillators and Clocks under Minimal
  Connectivity Assumptions</title><categories>cs.SY cs.MA math.OC nlin.AO</categories><comments>to be submitted to IEEE TAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Populations of flashing fireflies, claps of applauding audience, cells of
cardiac and circadian pacemakers reach synchrony via event-triggered
interactions, referred to as pulse couplings. Synchronization via pulse
coupling is widely used in wireless sensor networks, providing clock
synchronization with parsimonious packet exchanges. In spite of serious
attention paid to networks of pulse coupled oscillators, there is a lack of
mathematical results, addressing networks with general communication topologies
and phase-response curves. The most general results of this type establish
synchronization of oscillators with a delay-advance phase-response curve over
strongly connected networks. In this paper we extend this result by relaxing
the connectivity condition to the existence of a directed spanning tree
(rootedness), both necessary and sufficient for synchronization. Furthermore,
we prove synchronization under repeatedly rooted switching topologies, given
that the dwell time between consecutive switches exceeds the oscillators'
period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02342</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02342</id><created>2015-10-08</created><authors><author><keyname>Lee</keyname><forenames>Stella</forenames></author><author><keyname>Walda</keyname><forenames>Martin</forenames></author><author><keyname>Vasiliki</keyname><forenames>Delimpasi</forenames></author></authors><title>Born In Bradford Mobile Application</title><categories>cs.CY</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Born In Bradford mobile application is an Android mobile application and
a working prototype that enables interaction with a sample cohort of the Born
in Bradford study. It provides an interface and visualization for several
surveys participated in by mothers and their children. This data is stored in
the Born In Bradford database. A subset of this data is provided for mothers
and children. The mobile application provides a way to engage the mothers and
promote their consistency in participating in subsequent surveys. It has been
designed to allow selected mothers to participate in the visualization of their
babies data. Samsung mobile phones have been provided with the application
loaded on the phone to limit and control its use and access to data. Mothers
login to interact with the data. This includes the ability to compare children
data through infographics and graphs and comparing their children data with the
average child. This comparison is done at different stages of the children ages
as provided in the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02343</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02343</id><created>2015-10-08</created><authors><author><keyname>Isah</keyname><forenames>Haruna</forenames></author><author><keyname>Neagu</keyname><forenames>Daniel</forenames></author><author><keyname>Trundle</keyname><forenames>Paul</forenames></author></authors><title>Bipartite Network Model for Inferring Hidden Ties in Crime Data</title><categories>cs.SI physics.soc-ph</categories><comments>8 pages</comments><doi>10.1145/2808797.2808842</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Certain crimes are hardly committed by individuals but carefully organised by
group of associates and affiliates loosely connected to each other with a
single or small group of individuals coordinating the overall actions. A common
starting point in understanding the structural organisation of criminal groups
is to identify the criminals and their associates. Situations arise in many
criminal datasets where there is no direct connection among the criminals. In
this paper, we investigate ties and community structure in crime data in order
to understand the operations of both traditional and cyber criminals, as well
as to predict the existence of organised criminal networks. Our contributions
are twofold: we propose a bipartite network model for inferring hidden ties
between actors who initiated an illegal interaction and objects affected by the
interaction, we then validate the method in two case studies on pharmaceutical
crime and underground forum data using standard network algorithms for
structural and community analysis. The vertex level metrics and community
analysis results obtained indicate the significance of our work in
understanding the operations and structure of organised criminal networks which
were not immediately obvious in the data. Identifying these groups and mapping
their relationship to one another is essential in making more effective
disruption strategies in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02348</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02348</id><created>2015-10-08</created><authors><author><keyname>Chen</keyname><forenames>Ling-Jiao</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Liu</keyname><forenames>Jin-Hu</forenames></author><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>A novel similarity index for better personalized recommendation</title><categories>cs.IR physics.data-an</categories><comments>6 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems benefit us in tackling the problem of information
overload by predicting our potential choices among diverse niche objects. So
far, a variety of personalized recommendation algorithms have been proposed and
most of them are based on similarities, such as collaborative filtering and
mass diffusion. Here, we propose a novel similarity index named CosRA, which
combines advantages of both the cosine index and the resource-allocation (RA)
index. By applying the CosRA index to real recommender systems including
MovieLens, Netflix and RYM, we show that the CosRA-based method has better
performance in accuracy, diversity and novelty than the state-of-the-art
methods. Moreover, the CosRA index is free of parameters, which is a
significant advantage in real applications. Further experiments show that the
introduction of two turnable parameters cannot remarkably improve the overall
performance of CosRA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02358</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02358</id><created>2015-10-08</created><authors><author><keyname>Vera</keyname><forenames>Javier</forenames></author><author><keyname>Montealegre</keyname><forenames>Pedro</forenames></author><author><keyname>Goles</keyname><forenames>Eric</forenames></author></authors><title>Automata networks for multi-party communication in the Naming Game</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Naming Game has been studied to explore the role of self-organization in
the development and negotiation of linguistic conventions. In this paper, we
define an automata networks approach to the Naming Game. Two problems are
faced: (1) the definition of an automata networks for multi-party communicative
interactions; and (2) the proof of convergence for three different orders in
which the individuals are updated (updating schemes). Finally, computer
simulations are explored in two-dimensional lattices with the purpose to
recover the main features of the Naming Game and to describe the dynamics under
different updating schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02360</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02360</id><created>2015-10-08</created><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>CARTE</affiliation></author></authors><title>Aperiodic Subshifts on Polycyclic Groups</title><categories>cs.DM math.GR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every polycyclic group of nonlinear growth admits a strongly
aperiodic SFT and has an undecidable domino problem. This answers a question of
[4] and generalizes the result of [2].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02364</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02364</id><created>2015-10-08</created><authors><author><keyname>Versteegen</keyname><forenames>Ralph</forenames></author><author><keyname>Gimel'farb</keyname><forenames>Georgy</forenames></author><author><keyname>Riddle</keyname><forenames>Patricia</forenames></author></authors><title>Texture Modelling with Nested High-order Markov-Gibbs Random Fields</title><categories>cs.CV cs.LG stat.ML</categories><comments>Submitted to Computer Vision and Image Understanding</comments><doi>10.1016/j.cviu.2015.11.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, Markov-Gibbs random field (MGRF) image models which include
high-order interactions are almost always built by modelling responses of a
stack of local linear filters. Actual interaction structure is specified
implicitly by the filter coefficients. In contrast, we learn an explicit
high-order MGRF structure by considering the learning process in terms of
general exponential family distributions nested over base models, so that
potentials added later can build on previous ones. We relatively rapidly add
new features by skipping over the costly optimisation of parameters.
  We introduce the use of local binary patterns as features in MGRF texture
models, and generalise them by learning offsets to the surrounding pixels.
These prove effective as high-order features, and are fast to compute. Several
schemes for selecting high-order features by composition or search of a small
subclass are compared. Additionally we present a simple modification of the
maximum likelihood as a texture modelling-specific objective function which
aims to improve generalisation by local windowing of statistics.
  The proposed method was experimentally evaluated by learning high-order MGRF
models for a broad selection of complex textures and then performing texture
synthesis, and succeeded on much of the continuum from stochastic through
irregularly structured to near-regular textures. Learning interaction structure
is very beneficial for textures with large-scale structure, although those with
complex irregular structure still provide difficulties. The texture models were
also quantitatively evaluated on two tasks and found to be competitive with
other works: grading of synthesised textures by a panel of observers; and
comparison against several recent MGRF models by evaluation on a constrained
inpainting task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02371</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02371</id><created>2015-10-08</created><authors><author><keyname>Zhou</keyname><forenames>Qing</forenames></author><author><keyname>Li</keyname><forenames>Di</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Huie</keyname><forenames>Lauren</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Learning-Based Distributed Detection-Estimation in Sensor Networks with
  Unknown Sensor Defects</title><categories>cs.IT math.IT</categories><comments>15 pages, 2 figures, submitted to TSP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed estimation of an unknown deterministic
scalar parameter (the target signal) in a wireless sensor network (WSN), where
each sensor receives a single snapshot of the field. We assume that the
observation at each node randomly falls into one of two modes: a valid or an
invalid observation mode. Specifically, mode one corresponds to the desired
signal plus noise observation mode (\emph{valid}), and mode two corresponds to
the pure noise mode (\emph{invalid}) due to node defect or damage. With no
prior information on such local sensing modes, we introduce a learning-based
distributed procedure, called the mixed detection-estimation (MDE) algorithm,
based on iterative closed-loop interactions between mode learning (detection)
and target estimation. The online learning step re-assesses the validity of the
local observations at each iteration, thus refining the ongoing estimation
update process. The convergence of the MDE algorithm is established
analytically. Asymptotic analysis shows that, in the high signal-to-noise ratio
(SNR) regime, the MDE estimation error converges to that of an ideal
(centralized) estimator with perfect information about the node sensing modes.
This is in contrast to the estimation performance of a naive average consensus
based distributed estimator (without mode learning), whose estimation error
blows up with an increasing SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02374</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02374</id><created>2015-10-08</created><updated>2015-10-25</updated><authors><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author><author><keyname>Raimondi</keyname><forenames>Franco</forenames></author><author><keyname>Chen</keyname><forenames>Taolue</forenames></author><author><keyname>Martin</keyname><forenames>Jos</forenames></author></authors><title>The packing chromatic number of the infinite square lattice is less than
  or equal to 16</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a SAT-solver on top of a partial previously-known solution we improve
the upper bound of the packing chromatic number of the infinite square lattice
from 17 to 16. And then to 15.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02377</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02377</id><created>2015-10-08</created><authors><author><keyname>Tram&#xe8;r</keyname><forenames>Florian</forenames></author><author><keyname>Atlidakis</keyname><forenames>Vaggelis</forenames></author><author><keyname>Geambasu</keyname><forenames>Roxana</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Hubaux</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Humbert</keyname><forenames>Mathias</forenames></author><author><keyname>Juels</keyname><forenames>Ari</forenames></author><author><keyname>Lin</keyname><forenames>Huang</forenames></author></authors><title>Discovering Unwarranted Associations in Data-Driven Applications with
  the FairTest Testing Toolkit</title><categories>cs.CY</categories><comments>26 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's data-driven world, programmers routinely incorporate user data
into complex algorithms, heuristics, and application pipelines. While often
beneficial, this practice can have unintended and detrimental consequences,
such as the discriminatory effects identified in Staples's online pricing
algorithm and the racially offensive labels recently found in Google's image
tagger.
  We argue that such effects are bugs that should be tested for and debugged in
a manner similar to functionality, reliability, and performance bugs. We
describe FairTest, a testing toolkit that detects unwarranted associations
between an algorithm's outputs (e.g., prices or labels) and user
subpopulations, including sensitive groups (e.g., defined by race or gender).
FairTest reports statistically significant associations to programmers as
association bugs, ranked by their strength and likelihood of being
unintentional, rather than necessary effects.
  We designed FairTest for ease of use by programmers and integrated it into
the evaluation framework of SciPy, a popular library for data analytics. We
used FairTest experimentally to identify unfair disparate impact, offensive
labeling, and disparate rates of algorithmic error in six applications and
datasets. As examples, our results reveal subtle biases against older
populations in the distribution of error in a real predictive health
application, and offensive racial labeling in an image tagging system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02383</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02383</id><created>2015-10-08</created><authors><author><keyname>Ravagnani</keyname><forenames>Alberto</forenames></author></authors><title>Duality of codes supported on regular lattices, with an application to
  enumerative combinatorics</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a family of weight functions on finite abelian groups that yield
invertible MacWilliams identities for additive codes. The weights are obtained
composing a suitable support map with the rank function of a graded lattice
that satisfies certain regularity properties. We express the Krawtchouk
coefficients of the corresponding MacWilliams transformation in terms of the
combinatorial invariants of the underlying lattice, and show that the most
relevant weight functions studied in coding theory belong, up to equivalence,
to the class that we introduce. In particular, we compute some classical
Krawtchouk coefficients employing a simple combinatorial method. Our approach
also allows to systematically construct weight functions that endow the
underlying group with a metric space structure. We establish a Singleton-like
bound for additive codes, and call optimal the codes that attain the bound.
Then we prove that the dual of an optimal code is optimal, and that the weight
distribution of an optimal code is completely determined by three fundamental
parameters. Finally, we apply MacWilliams identities for the rank weight to
enumerative combinatorics problems, computing the number of matrices of given
rank over a finite field that satisfy certain linear conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02387</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02387</id><created>2015-10-08</created><authors><author><keyname>Madhyastha</keyname><forenames>Pranava Swaroop</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Gimpel</keyname><forenames>Kevin</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author></authors><title>Mapping Unseen Words to Task-Trained Embedding Spaces</title><categories>cs.CL cs.LG</categories><comments>10 + 3 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the setting in which we train a supervised model that learns
task-specific word representations. We assume that we have access to some
initial word representations (e.g., unsupervised embeddings), and that the
supervised learning procedure updates them to task-specific representations for
words contained in the training data. But what about words not contained in the
supervised training data? When such unseen words are encountered at test time,
they are typically represented by either their initial vectors or a single
unknown vector, which often leads to errors. In this paper, we address this
issue by learning to map from initial representations to task-specific ones. We
present a general technique that uses a neural network mapper with a weighted
multiple-loss criterion. This allows us to use the same learned model
parameters at test time but now with appropriate task-specific representations
for unseen words. We consider the task of dependency parsing and report
improvements in performance (and reductions in out-of-vocabulary rates) across
multiple domains such as news, Web, and speech. We also achieve downstream
improvements on the task of parsing-based sentiment analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02391</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02391</id><created>2015-10-07</created><authors><author><keyname>Khani</keyname><forenames>Shahedeh</forenames></author><author><keyname>Gacek</keyname><forenames>Cristina</forenames></author><author><keyname>Popov</keyname><forenames>Peter</forenames></author></authors><title>Security-aware selection of Web Services for Reliable Composition</title><categories>cs.DC cs.CR cs.IT cs.SE math.IT</categories><comments>Yann Busnel. 11th European Dependable Computing Conference (EDCC
  2015), Sep 2015, Paris, France. 2015, Proceedings of Student Forum - EDCC
  2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependability is an important characteristic that a trustworthy computer
system should have. It is a measure of Availability, Reliability,
Maintainability, Safety and Security. The focus of our research is on security
of web services. Web services enable the composition of independent services
with complementary functionalities to produce value-added services, which
allows organizations to implement their core business only and outsource other
service components over the Internet, either pre-selected or on-the-fly. The
selected third party web services may have security vulnerabilities. Vulnerable
web services are of limited practical use. We propose to use an
intrusion-tolerant composite web service for each functionality that should be
fulfilled by a third party web service. The third party services employed in
this approach should be selected based on their security vulnerabilities in
addition to their performance. The security vulnerabilities of the third party
services are assessed using a penetration testing tool. In this paper we
present our preliminary research work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02393</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02393</id><created>2015-10-08</created><authors><author><keyname>Cadilhac</keyname><forenames>Micha&#xeb;l</forenames></author><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Limaye</keyname><forenames>Nutan</forenames></author></authors><title>Value Automata with Filters</title><categories>cs.FL</categories><comments>8 pages, presented in the short track of NCMA'15, work in progress</comments><msc-class>68Q45, 68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to study value automata with filters, a natural generalization of
regular cost automata to nondeterminism. Models such as weighted automata and
Parikh automata appear naturally as specializations. Results on the
expressiveness of this model offer a general understanding of the behavior of
the models that arise as special cases. A landscape of such restrictions is
drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02395</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02395</id><created>2015-10-07</created><authors><author><keyname>Gollapalli</keyname><forenames>Mohammed</forenames></author></authors><title>Literature Review Of Attribute Level And Structure Level Data Linkage
  Techniques</title><categories>cs.DB</categories><comments>20 pages</comments><msc-class>62-07, 68Pxx, 97R50</msc-class><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) Vol.5, No.5, September 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Linkage is an important step that can provide valuable insights for
evidence-based decision making, especially for crucial events. Performing
sensible queries across heterogeneous databases containing millions of records
is a complex task that requires a complete understanding of each contributing
databases schema to define the structure of its information. The key aim is to
approximate the structure and content of the induced data into a concise
synopsis in order to extract and link meaningful data-driven facts. We identify
such problems as four major research issues in Data Linkage: associated costs
in pair-wise matching, record matching overheads, semantic flow of information
restrictions, and single order classification limitations. In this paper, we
give a literature review of research in Data Linkage. The purpose for this
review is to establish a basic understanding of Data Linkage, and to discuss
the background in the Data Linkage research domain. Particularly, we focus on
the literature related to the recent advancements in Approximate Matching
algorithms at Attribute Level and Structure Level. Their efficiency,
functionality and limitations are critically analysed and open-ended problems
have been exposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02405</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02405</id><created>2015-10-08</created><authors><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Ozcan</keyname><forenames>Gozde</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>Energy Efficiency of Hybrid-ARQ under Statistical Queuing Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, energy efficiency of hybrid automatic repeat request (HARQ)
schemes with statistical queuing constraints is studied for both constant-rate
and random Markov arrivals by characterizing the minimum energy per bit and
wideband slope. The energy efficiency is investigated when either an outage
constraint is imposed and (the transmission rate is selected accordingly) or
the transmission rate is optimized to maximize the throughput. In both cases,
it is also assumed that there is a limitation on the number of retransmissions
due to deadline constraints. Under these assumptions, closed-form expressions
are obtained for the minimum energy per bit and wideband slope for HARQ with
chase combining (CC). Through numerical results, the performances of HARQ-CC
and HARQ with incremental redundancy (IR) are compared. Moreover, the impact of
source variations/burstiness, deadline constraints, outage probability, queuing
constraints on the energy efficiency is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02413</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02413</id><created>2015-10-08</created><authors><author><keyname>Zhou</keyname><forenames>Tinghui</forenames></author><author><keyname>Kr&#xe4;henb&#xfc;hl</keyname><forenames>Philipp</forenames></author><author><keyname>Efros</keyname><forenames>Alexei A.</forenames></author></authors><title>Learning Data-driven Reflectance Priors for Intrinsic Image
  Decomposition</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a data-driven approach for intrinsic image decomposition, which is
the process of inferring the confounding factors of reflectance and shading in
an image. We pose this as a two-stage learning problem. First, we train a model
to predict relative reflectance ordering between image patches (`brighter',
`darker', `same') from large-scale human annotations, producing a data-driven
reflectance prior. Second, we show how to naturally integrate this learned
prior into existing energy minimization frameworks for intrinsic image
decomposition. We compare our method to the state-of-the-art approach of Bell
et al. on both decomposition and image relighting tasks, demonstrating the
benefits of the simple relative reflectance prior, especially for scenes under
challenging lighting conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02415</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02415</id><created>2015-10-08</created><updated>2016-02-02</updated><authors><author><keyname>Tarighati</keyname><forenames>Alla</forenames></author><author><keyname>Jalden</keyname><forenames>Joakim</forenames></author></authors><title>Optimality of Rate Balancing in Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed binary hypothesis testing in a
parallel network topology where sensors independently observe some phenomenon
and send a finite rate summary of their observations to a fusion center for the
final decision. We explicitly consider a scenario under which (integer) rate
messages are sent over an error free multiple access channel, modeled by a sum
rate constraint at the fusion center. This problem was previously studied by
Chamberland and Veeravalli, who provided sufficient conditions for the
optimality of one bit sensor messages. Their result is however crucially
dependent on the feasibility of having as many one bit sensors as the (integer)
sum rate constraint of the multiple access channel, an assumption that can
often not be satisfied in practice. This prompts us to consider the case of an
a-priori limited number of sensors and we provide sufficient condition under
which having no two sensors with rate difference more than one bit, so called
rate balancing, is an optimal strategy with respect to the Bhattacharyya
distance between the hypotheses at the input to the fusion center. We further
discuss explicit observation models under which these sufficient conditions are
satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02419</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02419</id><created>2015-10-08</created><authors><author><keyname>Benton</keyname><forenames>Nick</forenames></author><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author><author><keyname>Nigam</keyname><forenames>Vivek</forenames></author></authors><title>Effect-Dependent Transformations for Concurrent Programs</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a denotational semantics for an abstract effect system for a
higher-order, shared-variable concurrent programming language. We prove the
soundness of a number of general effect-based program equivalences, including a
parallelization equation that specifies sufficient conditions for replacing
sequential composition with parallel composition. Effect annotations are
relative to abstract locations specified by contracts rather than physical
footprints allowing us in particular to show the soundness of some
transformations involving fine-grained concurrent data structures, such as
Michael-Scott queues, that allow concurrent access to different parts of
mutable data structures.
  Our semantics is based on refining a trace-based semantics for first-order
programs due to Brookes. By moving from concrete to abstract locations, and
adding type refinements that capture the possible side-effects of both
expressions and their concurrent environments, we are able to validate many
equivalences that do not hold in an unrefined model. The meanings of types are
expressed using a game-based logical relation over sets of traces. Two programs
$e_1$ and $e_2$ are logically related if one is able to solve a two-player
game: for any trace with result value $v_1$ in the semantics of $e_1$
(challenge) that the player presents, the opponent can present an (response)
equivalent trace in the semantics of $e_2$ with a logically related result
value $v_2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02427</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02427</id><created>2015-10-08</created><authors><author><keyname>Mu&#xf1;oz-Gonz&#xe1;lez</keyname><forenames>Luis</forenames></author><author><keyname>Sgandurra</keyname><forenames>Daniele</forenames></author><author><keyname>Barr&#xe8;re</keyname><forenames>Mart&#xed;n</forenames></author><author><keyname>Lupu</keyname><forenames>Emil</forenames></author></authors><title>Exact Inference Techniques for the Dynamic Analysis of Attack Graphs</title><categories>cs.CR stat.AP stat.ML</categories><comments>14 pages, 13 figures</comments><msc-class>62F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attack graphs are a powerful tool for security risk assessment by analysing
network vulnerabilities and the paths attackers can use to compromise valuable
network resources. The uncertainty about the attacker's behaviour and
capabilities make Bayesian networks suitable to model attack graphs to perform
static and dynamic analysis. Previous approaches have focused on the
formalization of traditional attack graphs into a Bayesian model rather than
proposing mechanisms for their analysis. In this paper we propose to use
efficient algorithms to make exact inference in Bayesian attack graphs,
enabling the static and dynamic network risk assessments. To support the
validity of our proposed approach we have performed an extensive experimental
evaluation on synthetic Bayesian attack graphs with different topologies,
showing the computational advantages in terms of time and memory use of the
proposed techniques when compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02437</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02437</id><created>2015-10-08</created><authors><author><keyname>Papamakarios</keyname><forenames>George</forenames></author></authors><title>Distilling Model Knowledge</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Top-performing machine learning systems, such as deep neural networks, large
ensembles and complex probabilistic graphical models, can be expensive to
store, slow to evaluate and hard to integrate into larger systems. Ideally, we
would like to replace such cumbersome models with simpler models that perform
equally well.
  In this thesis, we study knowledge distillation, the idea of extracting the
knowledge contained in a complex model and injecting it into a more convenient
model. We present a general framework for knowledge distillation, whereby a
convenient model of our choosing learns how to mimic a complex model, by
observing the latter's behaviour and being penalized whenever it fails to
reproduce it.
  We develop our framework within the context of three distinct machine
learning applications: (a) model compression, where we compress large
discriminative models, such as ensembles of neural networks, into models of
much smaller size; (b) compact predictive distributions for Bayesian inference,
where we distil large bags of MCMC samples into compact predictive
distributions in closed form; (c) intractable generative models, where we
distil unnormalizable models such as RBMs into tractable models such as NADEs.
  We contribute to the state of the art with novel techniques and ideas. In
model compression, we describe and implement derivative matching, which allows
for better distillation when data is scarce. In compact predictive
distributions, we introduce online distillation, which allows for significant
savings in memory. Finally, in intractable generative models, we show how to
use distilled models to robustly estimate intractable quantities of the
original model, such as its intractable partition function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02438</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02438</id><created>2015-10-08</created><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Gurvich</keyname><forenames>Vladimir</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>1-Sperner hypergraphs</title><categories>math.CO cs.DM</categories><comments>20 pages</comments><msc-class>05C65, 05C69, 05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of hypergraphs, the class of $1$-Sperner
hypergraphs. A hypergraph ${\cal H}$ is said to be $1$-Sperner if every two
distinct hyperedges $e,f$ of ${\cal H}$ satisfy $\min\{|e\setminus
f|,|f\setminus e|\} = 1$. We prove a decomposition theorem for $1$-Sperner
hypergraphs and examine several of its consequences, including bounds on the
size of $1$-Sperner hypergraphs and a new, constructive proof of the fact that
every $1$-Sperner hypergraph is threshold. We also show that within the class
of normal Sperner hypergraphs, the (generally properly nested) classes of
$1$-Sperner hypergraphs, of threshold hypergraphs, and of $2$-asummable
hypergraphs coincide. This yields new characterizations of the class of
threshold graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02442</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02442</id><created>2015-10-08</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Uniform Learning in a Deep Neural Network via &quot;Oddball&quot; Stochastic
  Gradient Descent</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When training deep neural networks, it is typically assumed that the training
examples are uniformly difficult to learn. Or, to restate, it is assumed that
the training error will be uniformly distributed across the training examples.
Based on these assumptions, each training example is used an equal number of
times. However, this assumption may not be valid in many cases. &quot;Oddball SGD&quot;
(novelty-driven stochastic gradient descent) was recently introduced to drive
training probabilistically according to the error distribution - training
frequency is proportional to training error magnitude. In this article, using a
deep neural network to encode a video, we show that oddball SGD can be used to
enforce uniform error across the training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02448</identifier>
 <datestamp>2015-12-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02448</id><created>2015-10-08</created><updated>2015-12-23</updated><authors><author><keyname>Wu</keyname><forenames>Sissi Xiaoxiao</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author></authors><title>A Stochastic Beamformed Amplify-and-Forward Scheme in a Multigroup
  Multicast MIMO Relay Network with Per-Antenna Power Constraints</title><categories>cs.IT math.IT</categories><comments>This paper was first submitted for possible journal publication on
  24-May-2015 and this is the revised version on 15-Dec-2015. A preliminary
  version of this paper has been published in IEEE SPAWC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a two-hop one-way relay network for multigroup
multicast transmission between long-distance users, in which the relay is
equipped with multiple antennas, while the transmitters and receivers are all
with a single antenna. Assuming that perfect channel state information is
available, we study amplify-and-forward (AF) schemes that aim at optimizing the
max-min-fair (MMF) rate. We begin by considering the classic beamformed AF
(BF-AF) scheme, whose corresponding MMF design problem can be formulated as a
rank-constrained fractional semidefinite program (SDP). We show that the gap
between the BF-AF rate and the SDR rate associated with an optimal SDP solution
is sensitive to the number of users as well as the number of power constraints
in the relay system. This reveals that the BF-AF scheme may not be well suited
for large-scale systems. We therefore propose the stochastic beamformed AF
(SBF-AF) schemes, which differ from the BF-AF scheme in that time-varying AF
weights are used. We prove that the MMF rates of the proposed SBF-AF schemes
are at most $0.8317$ bits/s/Hz less than the SDR rate, irrespective of the
number of users or power constraints. Thus, SBF-AF can outperform BF-AF
especially in large-scale systems. Finally, we present numerical results to
demonstrate the viability of our proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02453</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02453</id><created>2015-10-08</created><updated>2015-11-06</updated><authors><author><keyname>Velez-Cuartas</keyname><forenames>Gabriel</forenames></author><author><keyname>Lucio-Arias</keyname><forenames>Diana</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Regional and Global Science: Latin American and Caribbean publications
  in the SciELO Citation Index and the Web of Science</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare the visibility of Latin American and Caribbean (LAC) publications
in the Core Collection indexes of the Web of Science (WoS)--Science Citation
Index Expanded, Social Sciences Citation Index, and Arts &amp; Humanities Citation
Index--and the SciELO Citation Index (SciELO CI) which was integrated into the
larger WoS platform in 2014. The purpose of this comparison is to contribute to
our understanding of the communication of scientific knowledge produced in
Latin America and the Caribbean, and to provide some reflections on the
potential benefits of the articulation of regional indexing exercises into WoS
for a better understanding of geographic and disciplinary contributions. How is
the regional level of SciELO CI related to the global range of WoS? In WoS, LAC
authors are integrated at the global level in international networks, while
SciELO has provided a platform for interactions among LAC researchers. The
articulation of SciELO into WoS may improve the international visibility of the
regional journals, but at the cost of independent journal inclusion criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02460</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02460</id><created>2015-10-08</created><authors><author><keyname>Son</keyname><forenames>Pham Ngoc</forenames></author><author><keyname>Har</keyname><forenames>Dongsoo</forenames></author><author><keyname>Kong</keyname><forenames>HyungYun</forenames></author></authors><title>Joint Power Allocation for Energy Harvesting and Power Superposition
  Coding in Cooperative Spectrum Sharing</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a decode-and-forward type cooperative spectrum sharing scheme
exploiting energy harvesting and power superposition coding is proposed. The
secondary transmitter (ST) acting as a relay harvests energy in the first phase
of every two phases from the radio frequency signal transmitted by the primary
transmitter (PT) of the primary network (PN) and uses it in the second phase
for transmitting power superposed codes of primary signal of the PN and secrecy
signal of the secondary network (SN). The ST splits the received primary signal
with adjustable power splitting ratio for decoding the primary signal and
charging the battery. The harvested energy in addition to internal energy from
the battery of the ST is used for power superposition coding with variable
power sharing coefficient. Our main concern here is to know the impact of the
two power parameters on outage performances (probabilities) of the PN and the
SN. Impact of the other system parameters on outage performances is also
considered to provide more comprehensive view of system operation. Analytical
or mathematical expressions of the outage probabilities of the PN and the SN
are derived in terms of the power parameters, location of the ST, channel gain,
and other system related parameters. Jointly optimal power splitting ratio and
power sharing coefficient achieving target outage probabilities of the PN and
the SN are found from the expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02462</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02462</id><created>2015-10-08</created><updated>2015-10-12</updated><authors><author><keyname>Mishra</keyname><forenames>Shaunak</forenames></author><author><keyname>Shoukry</keyname><forenames>Yasser</forenames></author><author><keyname>Karamchandani</keyname><forenames>Nikhil</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>Secure State Estimation against Sensor Attacks in the Presence of Noise</title><categories>math.OC cs.CR cs.IT cs.SY math.IT</categories><comments>Submitted to IEEE TCNS special issue on secure control of cyber
  physical systems. A preliminary version of this work appeared in the
  proceedings of ISIT 2015. arXiv admin note: text overlap with
  arXiv:1504.05566</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the state of a noisy linear dynamical
system when an unknown subset of sensors is arbitrarily corrupted by an
adversary. We propose a secure state estimation algorithm, and derive (optimal)
bounds on the achievable state estimation error given an upper bound on the
number of attacked sensors. The proposed state estimator involves Kalman
filters operating over subsets of sensors to search for a sensor subset which
is reliable for state estimation. To further improve the subset search time, we
propose Satisfiability Modulo Theory based techniques to exploit the
combinatorial nature of searching over sensor subsets. Finally, as a result of
independent interest, we give a coding theoretic view of attack detection and
state estimation against sensor attacks in a noiseless dynamical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02484</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02484</id><created>2015-10-08</created><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author></authors><title>On Contact Graphs with Cubes and Proportional Boxes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two variants of the problem of contact representation of planar
graphs with axis-aligned boxes. In a cube-contact representation we realize
each vertex with a cube, while in a proportional box-contact representation
each vertex is an axis-aligned box with a prespecified volume. We present
algorithms for constructing cube-contact representation and proportional
box-contact representation for several classes of planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02499</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02499</id><created>2015-10-08</created><authors><author><keyname>Westerb&#xe4;ck</keyname><forenames>Thomas</forenames></author><author><keyname>Freij-Hollanti</keyname><forenames>Ragnar</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>Applications of Polymatroid Theory to Distributed Storage Systems</title><categories>cs.IT math.IT</categories><comments>To appear in the proceedings of the 53rd Annual Allerton Conference
  on Communication, Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a link between polymatroid theory and locally repairable codes
(LRCs) is established. The codes considered here are completely general in that
they are subsets of $A^n$, where $A$ is an arbitrary finite set. Three classes
of LRCs are considered, both with and without availability, and for both
information-symbol and all-symbol locality. The parameters and classes of LRCs
are generalized to polymatroids, and a general- ized Singelton bound on the
parameters for these three classes of polymatroids and LRCs is given. This
result generalizes the earlier Singleton-type bounds given for LRCs. Codes
achieving these bounds are coined perfect, as opposed to the more common term
optimal used earlier, since they might not always exist. Finally, new
constructions of perfect linear LRCs are derived from gammoids, which are a
special class of matroids. Matroids, for their part, form a subclass of
polymatroids and have proven useful in analyzing and constructing linear LRCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02500</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02500</id><created>2015-10-08</created><authors><author><keyname>Dakka</keyname><forenames>Sam M</forenames></author></authors><title>Using Socrative to Enhance In-Class Student Engagement and Collaboration</title><categories>cs.CY</categories><acm-class>J.1.2</acm-class><doi>10.5121/ijite.2015.4302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning and teaching experiment was designed to incorporate SRS Student
Response System to measure and assess student engagement in higher education
for level 5 engineering students. The SRS system was based on getting an
immediate student feedback to short quizzes lasting 10 to 15 minutes using
Socrative software. The structure of the questions was a blend of true or
false, multiple choice and short answer questions. The experiment was conducted
through semester 2 of yearlong engineering module. The outcome of the
experiment was analyzed quantitatively based on student performance and
qualitatively through student questionnaire. The results indicate that using
student paced assessments method using Socrative enhanced students performance.
The results showed that 53% of the students improved their performance while
23% neither improved nor underperformed. Qualitative data showed students felt
improvement in their learning experience. Overall results indicate positive
impact using this technology in teaching and learning for engineering modules
in higher education
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02513</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02513</id><created>2015-10-08</created><authors><author><keyname>Noghabi</keyname><forenames>H. Sharifi</forenames></author><author><keyname>Mashhadi</keyname><forenames>H. Rajabi</forenames></author><author><keyname>Shojaei</keyname><forenames>K.</forenames></author></authors><title>Differential Evolution with Universal Mutation</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential Evolution (DE) is one of the most successful and powerful
evolutionary algorithms for global optimization problem. The most important
operator in this algorithm is mutation operator which parents are selected
randomly to participate in it. Recently, numerous papers are tried to make this
operator more intelligent by selection of parents for mutation intelligently.
The intelligent selection for mutation vectors is performed by applying design
space (also known as decision space) criterion or fitness space criterion,
however, in both cases, half of valuable information of the problem space is
disregarded. In this article, a Universal Differential Evolution (UDE) is
proposed which takes advantage of both design and fitness spaces criteria for
intelligent selection of mutation vectors. The experimental analysis on UDE are
performed on CEC2005 benchmarks and the results stated that UDE significantly
improved the performance of differential evolution in comparison with other
methods that only use one criterion for intelligent selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02516</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02516</id><created>2015-10-08</created><authors><author><keyname>Noghabi</keyname><forenames>H. Sharifi</forenames></author><author><keyname>Mashhadi</keyname><forenames>H. Rajabi</forenames></author><author><keyname>Shojaei</keyname><forenames>K.</forenames></author></authors><title>Differential Evolution with Generalized Mutation Operator</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential Evolution (DE) proved to be one of the most successful
evolutionary algorithms for global optimization purposes in continuous
problems. The core operator in DE is mutation which can provide the algorithm
with both exploration and exploitation. In this article, a new notation for DE
is proposed which has a formula that can be utilized for generating and
extracting novel mutations and by applying this new notation, four novel
mutations are proposed. More importantly, by combining these novel trial vector
generation strategies and four other well-known ones, we proposed Generalized
Mutation Differential Evolution (GMDE) that takes advantage of two mutation
pools that have both explorative and exploitative strategies inside them.
Results and experimental analysis are performed on CEC2005 benchmarks and the
results stated that GMDE is surprisingly competitive and significantly improved
the performance of this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02519</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02519</id><created>2015-10-08</created><authors><author><keyname>Sadiq</keyname><forenames>Bilal</forenames></author><author><keyname>Tavildar</keyname><forenames>Saurabh</forenames></author><author><keyname>Li</keyname><forenames>Junyi</forenames></author></authors><title>Inband device-to-device relays in cellular networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new design for two-hop opportunistic relaying in cellular networks is
proposed, with the objective of throughput improvement. We propose using idle
UEs in a cellular system with better channel to the base station to relay
traffic for active UEs. One of the key ideas proposed is the use of (only)
uplink spectrum for the Access links, and corresponding interference management
schemes for managing interference between Access and Backhaul links. The
proposed algorithms and architecture show a median throughput gain of 110% for
downlink and 40% for uplink in system simulations performed according to the
3GPP methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02526</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02526</id><created>2015-10-08</created><authors><author><keyname>Botler</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Jim&#xe9;nez</keyname><forenames>Andrea</forenames></author></authors><title>On path decompositions of 2k-regular graphs</title><categories>cs.DM math.CO</categories><msc-class>05B40, 05C70, 05C51, 05C38</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tibor Gallai conjectured that the edge set of every connected graph $G$ on
$n$ vertices can be partitioned into $\lceil n/2\rceil$ paths. Let
$\mathcal{G}_{k}$ be the class of all $2k$-regular graphs of girth at least
$2k-2$ that admit a pair of disjoint perfect matchings. In this work, we show
that Gallai's conjecture holds in $\mathcal{G}_{k}$, for every $k \geq 3$.
Further, we prove that for every graph $G$ in $\mathcal{G}_{k}$ on $n$
vertices, there exists a partition of its edge set into $n/2$ paths of lengths
in $\{2k-1,2k,2k+1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02533</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02533</id><created>2015-10-08</created><authors><author><keyname>Defazio</keyname><forenames>Aaron</forenames></author></authors><title>New Optimisation Methods for Machine Learning</title><categories>cs.LG stat.ML</categories><comments>PhD thesis, 205 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A thesis submitted for the degree of Doctor of Philosophy of The Australian
National University.
  In this work we introduce several new optimisation methods for problems in
machine learning. Our algorithms broadly fall into two categories: optimisation
of finite sums and of graph structured objectives. The finite sum problem is
simply the minimisation of objective functions that are naturally expressed as
a summation over a large number of terms, where each term has a similar or
identical weight. Such objectives most often appear in machine learning in the
empirical risk minimisation framework in the non-online learning setting. The
second category, that of graph structured objectives, consists of objectives
that result from applying maximum likelihood to Markov random field models.
Unlike the finite sum case, all the non-linearity is contained within a
partition function term, which does not readily decompose into a summation.
  For the finite sum problem, we introduce the Finito and SAGA algorithms, as
well as variants of each.
  For graph-structured problems, we take three complementary approaches. We
look at learning the parameters for a fixed structure, learning the structure
independently, and learning both simultaneously. Specifically, for the combined
approach, we introduce a new method for encouraging graph structures with the
&quot;scale-free&quot; property. For the structure learning problem, we establish
SHORTCUT, a O(n^{2.5}) expected time approximate structure learning method for
Gaussian graphical models. For problems where the structure is known but the
parameters unknown, we introduce an approximate maximum likelihood learning
algorithm that is capable of learning a useful subclass of Gaussian graphical
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02538</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02538</id><created>2015-10-08</created><authors><author><keyname>Bai</keyname><forenames>Tianyang</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Analyzing Uplink SIR and Rate in Massive MIMO Systems Using Stochastic
  Geometry</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a stochastic geometry framework to analyze the SIR and
rate performance in a large-scale uplink massive MIMO network. The framework
incorporates the impacts of fractional power control and spatial correlation in
fading. Expressions are derived for spatial average SIR distributions over user
and base station distributions with maximum ratio combining (MRC) and
zero-forcing (ZF) receivers. The analysis reveals that a super-linear scaling
law for MRC receivers between the number of base station antennas and scheduled
users per cell to preserve the uplink SIR distribution, while a linear scaling
applies to zero-forcing receivers. ZF receivers are shown to outperform MRC
receivers in the SIR coverage, and the performance gap is quantified in terms
of the difference in the number of antennas to achieve the same SIR
distribution. Numerical results verify the analysis. It is found that the
optimal compensation fraction in fractional power control to optimize rate is
generally different for MRC and ZF receivers. Besides, simulations show that
the scaling results derived from the proposed framework apply to the networks
where base stations are distributed according to a hexagonal lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02545</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02545</id><created>2015-10-08</created><authors><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Jordan</keyname><forenames>Charles</forenames></author></authors><title>Comparative computational results for some vertex and facet enumeration
  codes</title><categories>cs.MS cs.CG</categories><comments>4 pages, 4 tables</comments><msc-class>90-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report some preliminary computational results comparing parallel and
sequential codes for vertex/facet enumeration problems for convex polyhedra.
The problems chosen span the range from simple to highly degenerate polytopes.
For most problems tested,the results clearly show the advantage of using the
parallel implementation mplrs of the reverse search based code lrs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02551</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02551</id><created>2015-10-08</created><authors><author><keyname>He</keyname><forenames>Qian</forenames></author><author><keyname>Hu</keyname><forenames>Jianbin</forenames></author><author><keyname>Blum</keyname><forenames>Rick S.</forenames></author><author><keyname>Wu</keyname><forenames>Yonggang</forenames></author></authors><title>Generalized Cramer-Rao Bound for Joint Estimation of Target Position and
  Velocity for Active and Passive Radar Networks</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive the Cramer-Rao bound (CRB) for joint target position
and velocity estimation using an active or passive distributed radar network
under more general, and practically occurring, conditions than assumed in
previous work. In particular, the presented results allow nonorthogonal
signals, spatially dependent Gaussian reflection coefficients, and spatially
dependent Gaussian clutter-plus-noise. These bounds allow designers to compare
the performance of their developed approaches, which are deemed to be of
acceptable complexity, to the best achievable performance. If their developed
approaches lead to performance close to the bounds, these developed approaches
can be deemed &quot;good enough&quot;. A particular recent study where algorithms have
been developed for a practical radar application which must involve
nonorthogonal signals, for which the best performance is unknown, is a great
example. The presented results in our paper do not make any assumptions about
the approximate location of the target being known from previous target
detection signal processing. In addition, for situations in which we do not
know some parameters accurately, we also derive the mismatched CRB. Numerical
investigations of the mean squared error of the maximum likelihood estimation
are employed to support the validity of the CRBs. In order to demonstrate the
utility of the provided results to a topic of great current interest, the
numerical results focus on a passive radar system using the Global System for
Mobile communication (GSM) cellar system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02552</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02552</id><created>2015-10-08</created><authors><author><keyname>Haryono</keyname><forenames>Haryono</forenames></author></authors><title>Multitasking Programming of OBDH Satellite Based On PC-104</title><categories>cs.OS</categories><comments>8 pages</comments><journal-ref>International Journal of advanced studies in Computer Science and
  Engineering IJASCSE Volume 4, Issue 8, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On Board Data Handling (OBDH) has functions to monitor, control, acquire,
analyze, take a decision, and execute the command. OBDH should organize the
task between sub system. OBDH like a heart which has a vital function. Because
the function is seriously important therefore designing and implementing the
OBDH should be carefully, in order to have a good reliability. Many OBDHs have
been made to support the satellite mission using primitive programming. In
handling the data from various input, OBDH should always be available to all
sub systems, when the tasks are many, it is not easy to program using primitive
programming. Sometimes the data become corrupt because the data which come to
the OBDH is in the same time. Therefore it is required to have a way to handle
the data safely and also easy in programming perspective. In this research,
OBDH is programmed using multi tasking programming perspective has been
created. The Operating System (OS) has been implemented so that can run the
tasks simultaneously. The OS is prepared by configuring the Linux Kernel for
the specific processor, creating Root File System (RFS), installing the
BusyBox. In order to do the above method, preparing the environment in our
machine has been done, they are installing the Cross Tool Chain, U-Boot,
GNU-Linux Kernel Source etc. After that, programming using c code with
multitasking programming can be implemented. By using above method, it is found
that programming is easier and the corruption data because of reentrancy can be
minimized. Keywords- Operating System, PC-104, Kernel, C Programming
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02558</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02558</id><created>2015-10-08</created><authors><author><keyname>Wang</keyname><forenames>Chu</forenames></author><author><keyname>Wang</keyname><forenames>Yingfei</forenames></author><author><keyname>E</keyname><forenames>Weinan</forenames></author><author><keyname>Schapire</keyname><forenames>Robert</forenames></author></authors><title>Functional Frank-Wolfe Boosting for General Loss Functions</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting is a generic learning method for classification and regression. Yet,
as the number of base hypotheses becomes larger, boosting can lead to a
deterioration of test performance. Overfitting is an important and ubiquitous
phenomenon, especially in regression settings. To avoid overfitting, we
consider using $l_1$ regularization. We propose a novel Frank-Wolfe type
boosting algorithm (FWBoost) applied to general loss functions. By using
exponential loss, the FWBoost algorithm can be rewritten as a variant of
AdaBoost for binary classification. FWBoost algorithms have exactly the same
form as existing boosting methods, in terms of making calls to a base learning
algorithm with different weights update. This direct connection between
boosting and Frank-Wolfe yields a new algorithm that is as practical as
existing boosting methods but with new guarantees and rates of convergence.
Experimental results show that the test performance of FWBoost is not degraded
with larger rounds in boosting, which is consistent with the theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02574</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02574</id><created>2015-10-09</created><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>A High Throughput List Decoder Architecture for Polar Codes</title><categories>cs.AR</categories><comments>submitted to IEEE TVLSI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While long polar codes can achieve the capacity of arbitrary binary-input
discrete memoryless channels when decoded by a low complexity successive
cancelation (SC) algorithm, the error performance of the SC algorithm is
inferior for polar codes with finite block lengths. The cyclic redundancy check
(CRC) aided successive cancelation list (SCL) decoding algorithm has better
error performance than the SC algorithm. However, current CRC aided SCL
(CA-SCL) decoders still suffer from long decoding latency and limited
throughput. In this paper, a reduced latency list decoding (RLLD) algorithm for
polar codes is proposed. Our RLLD algorithm performs the list decoding on a
binary tree, whose leaves correspond to the bits of a polar code. In existing
SCL decoding algorithms, all the nodes in the tree are traversed and all
possibilities of the information bits are considered. Instead, our RLLD
algorithm visits much fewer nodes in the tree and considers fewer possibilities
of the information bits. When configured properly, our RLLD algorithm
significantly reduces the decoding latency and hence improves throughput, while
introducing little performance degradation. Based on our RLLD algorithm, we
also propose a high throughput list decoder architecture, which is suitable for
larger block lengths due to its scalable partial sum computation unit. Our
decoder architecture has been implemented for different block lengths and list
sizes using the TSMC 90nm CMOS technology. The implementation results
demonstrate that our decoders achieve significant latency reduction and area
efficiency improvement compared with other list polar decoders in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02583</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02583</id><created>2015-10-09</created><authors><author><keyname>Torghabeh</keyname><forenames>Ramezan Paravi</forenames></author><author><keyname>Santhanam</keyname><forenames>Narayana Prasad</forenames></author></authors><title>Community Detection Using Slow Mixing Markov Models</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of \emph{community detection} in a graph formalizes the intuitive
task of grouping together subsets of vertices such that vertices within
clusters are connected tighter than those in disparate clusters. This paper
approaches community detection in graphs by constructing Markov random walks on
the graphs. The mixing properties of the random walk are then used to identify
communities. We use coupling from the past as an algorithmic primitive to
translate the mixing properties of the walk into revealing the community
structure of the graph. We analyze the performance of our algorithms on
specific graph structures, including the stochastic block models (SBM) and LFR
random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02588</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02588</id><created>2015-10-09</created><authors><author><keyname>Beygi</keyname><forenames>Nima</forenames></author><author><keyname>Beigy</keyname><forenames>Maani</forenames></author><author><keyname>Siahi</keyname><forenames>Mehdi</forenames></author></authors><title>Design of Fuzzy self-tuning PID controller for pitch control system of
  aircraft autopilot</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of control systems have been proposed for aircraft autopilot
systems. Traditional approaches such as proportional controller and
conventional PID (CPID) controller are widely used. PID controller has a good
static performance especially for linear and time-invariant systems, but a weak
dynamic performance and discouraging function on nonlinear, time-varying, and
uncertain systems. Fuzzy control theory can improve dynamic response in various
conditions of system performance. This paper designs fuzzy self-tuning PID
(FSPID) controller to improve disadvantages of conventional PID in aircraft
autopilots. We apply proposed controller to pitch angle of aircraft then the
abilities of proposed controller will be compared to the conventional PID and
proportional controller. Inner feedback loop acts as oscillation damper in
traditional schemes, but here is removed to compare the capabilities of Fuzzy
self-tuning PID, conventional PID, and proportional controller. Based on the
simulations, both of Conventional and Fuzzy self-tuning PID controllers can
properly damp oscillations in lack of the inner feedback loop, but proportional
controller cannot do. Then short-period approximation is assumed to assess the
function of FSPID and CPID controllers in confront with abrupt and continuous
disturbances, in addition to inappropriate tuning of parameters. Simulation
results of short-period approximation show a better anti-disturbance function
for Fuzzy self-tuning PID compare to the conventional type. Fuzzy self-tuning
PID can tune the PID parameters for achieving the optimal response in view of
speed, overshoot, and steady-state error in conditions of inappropriate tuning
of PID parameters, based on the results of simulation in short-period
approximation, the proposed controller can adaptively improve the system
response by on-line setting of PID parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02614</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02614</id><created>2015-10-09</created><authors><author><keyname>Usman</keyname><forenames>Muhammad</forenames></author><author><keyname>Har</keyname><forenames>Dongsoo</forenames></author><author><keyname>Koo</keyname><forenames>Insoo</forenames></author></authors><title>Energy-Efficient Infrastructure Sensor Network for Ad Hoc Cognitive
  Radio Network</title><categories>cs.DC cs.IT math.IT</categories><comments>submitted to IEEE sensors journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an energy-efficient network architecture that consists of ad hoc
(mobile) cognitive radios (CRs) and infrastructure wireless sensor nodes. The
sensor nodes within communications range of each CR are grouped into a cluster
and the clusters of CRs are regularly updated according to the random mobility
of the CRs. We reduce the energy consumption and the end-to-end delay of the
sensor network by dividing each cluster into disjoint subsets with overlapped
sensing coverage of primary user (PU) activity. Respective subset of a CR
provides target detection and false alarm probabilities. Substantial energy
efficiency is achieved by activating only one subset of the cluster, while
putting the rest of the subsets in the cluster into sleep mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02637</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02637</id><created>2015-10-09</created><authors><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author></authors><title>Efficient Ranking of Lyndon Words and Decoding Lexicographically Minimal
  de Bruijn Sequence</title><categories>cs.DS cs.FL</categories><comments>Improved version of a paper presented at CPM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give efficient algorithms for ranking Lyndon words of length n over an
alphabet of size {\sigma}. The rank of a Lyndon word is its position in the
sequence of lexicographically ordered Lyndon words of the same length. The
outputs are integers of exponential size, and complexity of arithmetic
operations on such large integers cannot be ignored. Our model of computations
is the word-RAM, in which basic arithmetic operations on (large) numbers of
size at most {\sigma}^n take O(n) time. Our algorithm for ranking Lyndon words
makes O(n^2) arithmetic operations (this would imply directly cubic time on
word-RAM). However, using an algebraic approach we are able to reduce the total
time complexity on the word-RAM to O(n^2 log {\sigma}). We also present an
O(n^3 log^2 {\sigma})-time algorithm that generates the Lyndon word of a given
length and rank in lexicographic order. Finally we use the connections between
Lyndon words and lexicographically minimal de Bruijn sequences (theorem of
Fredricksen and Maiorana) to develop the first polynomial-time algorithm for
decoding minimal de Bruijn sequence of any rank n (it determines the position
of an arbitrary word of length n within the de Bruijn sequence).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02638</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02638</id><created>2015-10-09</created><updated>2016-01-18</updated><authors><author><keyname>Bennett</keyname><forenames>Daniel</forenames></author><author><keyname>Bleak</keyname><forenames>Collin</forenames></author></authors><title>A dynamical definition of f.g. virtually free groups</title><categories>math.GR cs.FL math.DS</categories><comments>13 pp, 6 figures</comments><msc-class>20F10, 20E06, 68Q45, 37B05</msc-class><doi>10.1142/S0218196716500053</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the class of finitely generated virtually free groups is
precisely the class of demonstrable subgroups for R. Thompson's group $V$. The
class of demonstrable groups for $V$ consists of all groups which can embed
into $V$ with a natural dynamical behaviour in their induced actions on the
Cantor space $\mathfrak{C}_2 := \left\{0,1\right\}^\omega$. There are also
connections with formal language theory, as the class of groups with
context-free word problem is also the class of finitely generated virtually
free groups, while R. Thompson's group $V$ is a candidate as a universal
$co\mathcal{CF}$ group by Lehnert's conjecture, corresponding to the class of
groups with context free co-word problem (as introduced by Holt, Rees, R\&quot;over,
and Thomas). Our main reults answers a question of Berns-Zieze, Fry, Gillings,
Hoganson, and Matthews, and separately of Bleak and Salazar-D\'iaz, and it fits
into the larger exploration of the class of $co\mathcal{CF}$ groups as it shows
that all four of the known closure properties of the class of $co\mathcal{CF}$
groups hold for the set of finitely generated subgroups of $V.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02639</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02639</id><created>2015-10-09</created><authors><author><keyname>Belmonte</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>The Price of Connectivity for Feedback Vertex Set</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let fvs$(G)$ and cfvs(G) denote the cardinalities of a minimum feedback
vertex set and a minimum connected feedback vertex set of a graph $G$,
respectively. The price of connectivity for feedback vertex set (poc-fvs) for a
class of graphs ${\cal G}$ is defined as the maximum ratio
$\mbox{cfvs}(G)/\mbox{fvs}(G)$ over all connected graphs $G\in {\cal G}$. We
study the poc-fvs for graph classes defined by a finite family ${\cal H}$ of
forbidden induced subgraphs. We characterize exactly those finite families
${\cal H}$ for which the poc-fvs for ${\cal H}$-free graphs is upper bounded by
a constant. Additionally, for the case where $|{\cal H}|=1$, we determine
exactly those graphs $H$ for which there exists a constant $c_H$ such that
$\mbox{cfvs}(G)\leq \mbox{fvs}(G) + c_H$ for every connected $H$-free graph
$G$, as well as exactly those graphs $H$ for which we can take $c_H=0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02642</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02642</id><created>2015-10-09</created><updated>2016-02-11</updated><authors><author><keyname>Reynolds</keyname><forenames>Andrew</forenames></author><author><keyname>King</keyname><forenames>Tim</forenames></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames></author></authors><title>An Instantiation-Based Approach for Solving Quantified Linear Arithmetic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework to derive instantiation-based decision
procedures for satisfiability of quantified formulas in first-order theories,
including its correctness, implementation, and evaluation. Using this framework
we derive decision procedures for linear real arithmetic (LRA) and linear
integer arithmetic (LIA) formulas with one quantifier alternation. Our
procedure can be integrated into the solving architecture used by typical SMT
solvers. Experimental results on standardized benchmarks from model checking,
static analysis, and synthesis show that our implementation of the procedure in
the SMT solver CVC4 outperforms existing tools for quantified linear
arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02644</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02644</id><created>2015-10-09</created><authors><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Song</keyname><forenames>Yi-Zhe</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author><author><keyname>Gong</keyname><forenames>Shaogang</forenames></author></authors><title>Free-hand Sketch Synthesis with Deformable Stroke Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generative model which can automatically summarize the stroke
composition of free-hand sketches of a given category. When our model is fit to
a collection of sketches with similar poses, it discovers and learns the
structure and appearance of a set of coherent parts, with each part represented
by a group of strokes. It represents both consistent (topology) as well as
diverse aspects (structure and appearance variations) of each sketch category.
Key to the success of our model are important insights learned from a
comprehensive study performed on human stroke data. By fitting this model to
images, we are able to synthesize visually similar and pleasant free-hand
sketches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02658</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02658</id><created>2015-10-09</created><updated>2016-02-22</updated><authors><author><keyname>Mejia</keyname><forenames>Carolina</forenames></author><author><keyname>Montoya</keyname><forenames>J. Andres</forenames></author></authors><title>The almost-entropic regions are not semialgebraic</title><categories>cs.IT math.IT</categories><comments>9 pages The paper has been withdrawn by the authors due to a flaw in
  the proof of the main result</comments><msc-class>94A17</msc-class><acm-class>H.1.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the almost-entropic region of order four is not semialgebraic,
we get as a corollary the well-known Theorem of Matus, which asserts that this
region is not polyhedral
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02659</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02659</id><created>2015-10-09</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Di Donato</keyname><forenames>Valentino</forenames></author><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Rote</keyname><forenames>G&#xfc;nter</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Windrose Planarity: Embedding Graphs with Direction-Constrained Edges</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a planar graph $G(V,E)$ and a partition of the neighbors of each vertex
$v \in V$ in four sets $UR(v)$, $UL(v)$, $DL(v)$, and $DR(v)$, the problem
Windrose Planarity asks to decide whether $G$ admits a windrose-planar drawing,
that is, a planar drawing in which (i) each neighbor $u \in UR(v)$ is above and
to the right of $v$, (ii) each neighbor $u \in UL(v)$ is above and to the left
of $v$, (iii) each neighbor $u \in DL(v)$ is below and to the left of $v$, (iv)
each neighbor $u \in DR(v)$ is below and to the right of $v$, and (v) edges are
represented by curves that are monotone with respect to each axis. By
exploiting both the horizontal and the vertical relationship among vertices,
windrose-planar drawings allow to simultaneously visualize two partial orders
defined by means of the edges of the graph.
  Although the problem is NP-hard in the general case, we give a
polynomial-time algorithm for testing whether there exists a windrose-planar
drawing that respects a combinatorial embedding that is given as part of the
input. This algorithm is based on a characterization of the plane
triangulations admitting a windrose-planar drawing. Furthermore, for any
embedded graph admitting a windrose-planar drawing we show how to construct one
with at most one bend per edge on an $O(n) \times O(n)$ grid. The latter result
contrasts with the fact that straight-line windrose-planar drawings may require
exponential area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02669</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02669</id><created>2015-10-09</created><authors><author><keyname>Barnat</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Bauch</keyname><forenames>Petr</forenames></author><author><keyname>Bene&#x161;</keyname><forenames>Nikola</forenames></author><author><keyname>Brim</keyname><forenames>Lubo&#x161;</forenames></author><author><keyname>Beran</keyname><forenames>Jan</forenames></author><author><keyname>Kratochv&#xed;la</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Analysing Sanity of Requirements for Avionics Systems (Preliminary
  Version)</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade it became a common practice to formalise software
requirements to improve the clarity of users' expectations. In this work we
build on the fact that functional requirements can be expressed in temporal
logic and we propose new sanity checking techniques that automatically detect
flaws and suggest improvements of given requirements. Specifically, we describe
and experimentally evaluate approaches to consistency and redundancy checking
that identify all inconsistencies and pinpoint their exact source (the smallest
inconsistent set). We further report on the experience obtained from employing
the consistency and redundancy checking in an industrial environment. To
complete the sanity checking we also describe a semi-automatic completeness
evaluation that can assess the coverage of user requirements and suggest
missing properties the user might have wanted to formulate. The usefulness of
our completeness evaluation is demonstrated in a case study of an aeroplane
control system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02670</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02670</id><created>2015-10-09</created><authors><author><keyname>Cocco</keyname><forenames>Giuseppe</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author><author><keyname>Ibars</keyname><forenames>Christian</forenames></author></authors><title>Throughput and Delay Analysis in Video Streaming over Block-Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>28 pages, 11 figures, to appear in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study video streaming over a slow fading wireless channel. In a streaming
application video packets are required to be decoded and displayed in the order
they are transmitted as the transmission goes on. This results in per-packet
delay constraints, and the resulting channel can be modeled as a physically
degraded fading broadcast channel with as many virtual users as the number of
packets. In this paper we study two important quality of user experience (QoE)
metrics, namely throughput and inter-decoding delay. We introduce several
transmission schemes, and compare their throughput and maximum inter-decoding
delay performances. We also introduce a genie-aided scheme, which provides
theoretical bounds on the achievable performance. We observe that adapting the
transmission rate at the packet level, i.e., periodically dropping a subset of
the packets, leads to a good tradeoff between the throughput and the maximum
inter-decoding delay. We also show that an approach based on initial buffering
leads to an asymptotically vanishing packet loss rate at the expense of a
relatively large initial delay. For this scheme we derive a condition on the
buffering time that leads to throughput maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02674</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02674</id><created>2015-10-09</created><authors><author><keyname>Ahmad</keyname><forenames>S. Raza</forenames></author></authors><title>Technical Report of Participation in Higgs Boson Machine Learning
  Challenge</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report entails the detailed description of the approach and
methodologies taken as part of competing in the Higgs Boson Machine Learning
Competition hosted by Kaggle Inc. and organized by CERN et al. It briefly
describes the theoretical background of the problem and the motivation for
taking part in the competition. Furthermore, the various machine learning
models and algorithms analyzed and implemented during the 4 month period of
participation are discussed and compared. Special attention is paid to the Deep
Learning techniques and architectures implemented from scratch using Python and
NumPy for this competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02675</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02675</id><created>2015-10-09</created><updated>2015-12-14</updated><authors><author><keyname>Wilson</keyname><forenames>Benjamin J.</forenames></author><author><keyname>Schakel</keyname><forenames>Adriaan M. J.</forenames></author></authors><title>Controlled Experiments for Word Embeddings</title><categories>cs.CL</categories><comments>Chagelog: Rerun experiment with subsampling turned off;
  re-interpreted results in light of Schnabel et al. (2015). 15 pages</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An experimental approach to studying the properties of word embeddings is
proposed. Controlled experiments, achieved through modifications of the
training corpus, permit the demonstration of direct relations between word
properties and word vector direction and length. The approach is demonstrated
using the word2vec CBOW model with experiments that independently vary word
frequency and word co-occurrence noise. The experiments reveal that word vector
length depends more or less linearly on both word frequency and the level of
noise in the co-occurrence distribution of the word. The coefficients of
linearity depend upon the word. The special point in feature space, defined by
the (artificial) word with pure noise in its co-occurrence distribution, is
found to be small but non-zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02676</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02676</id><created>2015-10-09</created><authors><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Le</keyname><forenames>Ya</forenames></author></authors><title>Some Theory For Practical Classifier Validation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare and contrast two approaches to validating a trained classifier
while using all in-sample data for training. One is simultaneous validation
over an organized set of hypotheses (SVOOSH), the well-known method that began
with VC theory. The other is withhold and gap (WAG). WAG withholds a validation
set, trains a holdout classifier on the remaining data, uses the validation
data to validate that classifier, then adds the rate of disagreement between
the holdout classifier and one trained using all in-sample data, which is an
upper bound on the difference in error rates. We show that complex hypothesis
classes and limited training data can make WAG a favorable alternative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02682</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02682</id><created>2015-10-06</created><updated>2016-02-18</updated><authors><author><keyname>Barr&#xf3;n-Romero</keyname><forenames>Carlos</forenames></author></authors><title>Classical and Quantum Algorithms for the Boolean Satisfiability Problem</title><categories>cs.CC</categories><msc-class>68Q10, 68Q12, 68Q19, 68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a complete algorithmic study of the decision Boolean
Satisfiability Problem under the classical computation and quantum computation
theories. The paper depicts deterministic and probabilistic algorithms,
propositions of their properties and the main result is that the problem has
not an efficient algorithm (NP is not P). Novel quantum algorithms and
propositions depict that the complexity by quantum computation approach for
solving the Boolean Satisfiability Problem or any NP problem is lineal time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02689</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02689</id><created>2015-10-09</created><authors><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Erickson</keyname><forenames>Alejandro</forenames></author><author><keyname>Fan</keyname><forenames>Jianxi</forenames></author><author><keyname>Jia</keyname><forenames>Xiaohua</forenames></author></authors><title>Hamiltonian Properties of DCell Networks</title><categories>cs.DC cs.DM</categories><comments>To appear</comments><acm-class>C.2.1</acm-class><journal-ref>Wang, Xi and Erickson, Alejandro and Fan, Jianxi and Jia, Xiaohua,
  Hamiltonian Properties of DCell Networks. The Computer Journal. 2015</journal-ref><doi>10.1093/comjnl/bxv019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DCell has been proposed for data centers as a server centric interconnection
network structure. DCell can support millions of servers with high network
capacity by only using commodity switches. With one exception, we prove that a
$k$ level DCell built with $n$ port switches is Hamiltonian-connected for $k
\geq 0$ and $n \geq 2$. Our proof extends to all generalized DCell connection
rules for $n\ge 3$. Then, we propose an $O(t_k)$ algorithm for finding a
Hamiltonian path in $DCell_{k}$, where $t_k$ is the number of servers in
$DCell_{k}$. What's more, we prove that $DCell_{k}$ is $(n+k-4)$-fault
Hamiltonian-connected and $(n+k-3)$-fault Hamiltonian. In addition, we show
that a partial DCell is Hamiltonian connected if it conforms to a few practical
restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02693</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02693</id><created>2015-10-09</created><authors><author><keyname>Zhang</keyname><forenames>ShiLiang</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author><author><keyname>Wei</keyname><forenames>Si</forenames></author><author><keyname>Dai</keyname><forenames>LiRong</forenames></author></authors><title>Feedforward Sequential Memory Neural Networks without Recurrent Feedback</title><categories>cs.NE cs.CL cs.LG</categories><comments>4 pages, 1figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new structure for memory neural networks, called feedforward
sequential memory networks (FSMN), which can learn long-term dependency without
using recurrent feedback. The proposed FSMN is a standard feedforward neural
networks equipped with learnable sequential memory blocks in the hidden layers.
In this work, we have applied FSMN to several language modeling (LM) tasks.
Experimental results have shown that the memory blocks in FSMN can learn
effective representations of long history. Experiments have shown that FSMN
based language models can significantly outperform not only feedforward neural
network (FNN) based LMs but also the popular recurrent neural network (RNN)
LMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02694</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02694</id><created>2015-10-09</created><updated>2015-10-16</updated><authors><author><keyname>Duan</keyname><forenames>Rann</forenames></author><author><keyname>Garg</keyname><forenames>Jugal</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author></authors><title>An Improved Combinatorial Polynomial Algorithm for the Linear
  Arrow-Debreu Market</title><categories>cs.DS cs.GT</categories><comments>to appear in SODA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an improved combinatorial algorithm for the computation of
equilibrium prices in the linear Arrow-Debreu model. For a market with $n$
agents and integral utilities bounded by $U$, the algorithm runs in $O(n^7
\log^3 (nU))$ time. This improves upon the previously best algorithm of Ye by a
factor of $\tOmega(n)$. The algorithm refines the algorithm described by Duan
and Mehlhorn and improves it by a factor of $\tOmega(n^3)$. The improvement
comes from a better understanding of the iterative price adjustment process,
the improved balanced flow computation for nondegenerate instances, and a novel
perturbation technique for achieving nondegeneracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02696</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02696</id><created>2015-10-09</created><updated>2016-01-07</updated><authors><author><keyname>Basescu</keyname><forenames>Cristina</forenames></author><author><keyname>Reischuk</keyname><forenames>Raphael M.</forenames></author><author><keyname>Szalachowski</keyname><forenames>Pawel</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author><author><keyname>Zhang</keyname><forenames>Yao</forenames></author><author><keyname>Hsiao</keyname><forenames>Hsu-Chun</forenames></author><author><keyname>Kubota</keyname><forenames>Ayumu</forenames></author><author><keyname>Urakawa</keyname><forenames>Jumpei</forenames></author></authors><title>SIBRA: Scalable Internet Bandwidth Reservation Architecture</title><categories>cs.NI</categories><comments>To appear in Proceedings of Symposium on Network and Distributed
  System Security (NDSS) 2016</comments><doi>10.14722/ndss.2016.23132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Scalable Internet Bandwidth Reservation Architecture
(SIBRA) as a new approach against DDoS attacks, which, until now, continue to
be a menace on today's Internet. SIBRA provides scalable inter-domain resource
allocations and botnet-size independence, an important property to realize why
previous defense approaches are insufficient. Botnet-size independence enables
two end hosts to set up communication regardless of the size of distributed
botnets in any Autonomous System in the Internet. SIBRA thus ends the arms race
between DDoS attackers and defenders. Furthermore, SIBRA is based on purely
stateless operations for reservation renewal, flow monitoring, and policing,
resulting in highly efficient router operation, which is demonstrated with a
full implementation. Finally, SIBRA supports Dynamic Interdomain Leased Lines
(DILLs), offering new business opportunities for ISPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02700</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02700</id><created>2015-10-09</created><updated>2016-01-25</updated><authors><author><keyname>Tepper</keyname><forenames>Mariano</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>A short-graph Fourier transform via personalized PageRank vectors</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The short-time Fourier transform (STFT) is widely used to analyze the spectra
of temporal signals that vary through time. Signals defined over graphs, due to
their intrinsic complexity, exhibit large variations in their patterns. In this
work we propose a new formulation for an STFT for signals defined over graphs.
This formulation draws on recent ideas from spectral graph theory, using
personalized PageRank vectors as its fundamental building block. Furthermore,
this work establishes and explores the connection between local spectral graph
theory and localized spectral analysis of graph signals. We accompany the
presentation with synthetic and real-world examples, showing the suitability of
the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02705</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02705</id><created>2015-10-08</created><authors><author><keyname>Houidi</keyname><forenames>Zied Ben</forenames></author></authors><title>From Rough Consensus to Automated Reasoning</title><categories>cs.NI</categories><report-no>ITD-14-55405Y</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unwritten languages today often have no official grammar, and are rather
governed by &quot;unspoken rules&quot;. Similarly, we argue that the young discipline of
networking is still a practice that lacks a deep understanding of the rules
that govern it. This situation results in a tremendous loss of time and
efforts. First, since the rules are unspoken, they are not systematically
reused. Second, since there is no grammar, it is impossible to assert if a
sentence is correct. Comparing two networking approaches or solutions is
sometimes a synonym of endless religious debates. Drawing the proper conclusion
from this claim, we advocate that networking research should spend more efforts
on better understanding its rules as a first step to automatically reuse them.
To illustrate our claim, we focus in this paper on one specific networking
problem, and show how different instances of this same problem were solved in
parallel, resulting in different solutions, and no explicit knowledge reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02706</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02706</id><created>2015-10-09</created><authors><author><keyname>Zimin</keyname><forenames>Alexander</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Conditional Risk Minimization for Stochastic Processes</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the task of learning from non-i.i.d. data. In particular, we aim at
learning predictors that minimize the conditional risk for a stochastic
process, i.e. the expected loss taking into account the set of training samples
observed so far. For non-i.i.d. data, the training set contains information
about the upcoming samples, so learning with respect to the conditional
distribution can be expected to yield better predictors than one obtains from
the classical setting of minimizing the marginal risk. Our main contribution is
a practical estimator for the conditional risk based on the theory of
non-parametric time-series prediction, and a finite sample concentration bound
that establishes exponential convergence of the estimator to the true
conditional risk under certain regularity assumptions on the process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02709</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02709</id><created>2015-10-09</created><authors><author><keyname>Sun</keyname><forenames>Kairan</forenames></author><author><keyname>Wei</keyname><forenames>Xu</forenames></author><author><keyname>Jia</keyname><forenames>Gengtao</forenames></author><author><keyname>Wang</keyname><forenames>Risheng</forenames></author><author><keyname>Li</keyname><forenames>Ruizhi</forenames></author></authors><title>Large-scale Artificial Neural Network: MapReduce-based Deep Learning</title><categories>cs.DC cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faced with continuously increasing scale of data, original back-propagation
neural network based machine learning algorithm presents two non-trivial
challenges: huge amount of data makes it difficult to maintain both efficiency
and accuracy; redundant data aggravates the system workload. This project is
mainly focused on the solution to the issues above, combining deep learning
algorithm with cloud computing platform to deal with large-scale data. A
MapReduce-based handwriting character recognizer will be designed in this
project to verify the efficiency improvement this mechanism will achieve on
training and practical large-scale data. Careful discussion and experiment will
be developed to illustrate how deep learning algorithm works to train
handwritten digits data, how MapReduce is implemented on deep learning neural
network, and why this combination accelerates computation. Besides performance,
the scalability and robustness will be mentioned in this report as well. Our
system comes with two demonstration software that visually illustrates our
handwritten digit recognition/encoding application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02710</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02710</id><created>2015-10-09</created><authors><author><keyname>Sato</keyname><forenames>Kosuke</forenames></author><author><keyname>Iwai</keyname><forenames>Daisuke</forenames></author><author><keyname>Ikeda</keyname><forenames>Sei</forenames></author><author><keyname>Takemura</keyname><forenames>Noriko</forenames></author></authors><title>Procams-Based Cybernetics</title><categories>cs.CV cs.GR cs.HC</categories><comments>2 pages, 2 figures, IEEE VR 2015 Lab/Project presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Procams-based cybernetics is a unique, emerging research field, which aims at
enhancing and supporting our activities by naturally connecting human and
computers/machines as a cooperative integrated system via projector-camera
systems (procams). It rests on various research domains such as
virtual/augmented reality, computer vision, computer graphics, projection
display, human computer interface, human robot interaction and so on. This
laboratory presentation provides a brief history including recent achievements
of our procams-based cybernetics project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02719</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02719</id><created>2015-10-09</created><authors><author><keyname>Bandara</keyname><forenames>Kosala</forenames></author><author><keyname>R&#xfc;berg</keyname><forenames>Thomas</forenames></author><author><keyname>Cirak</keyname><forenames>Fehmi</forenames></author></authors><title>Shape optimisation with multiresolution subdivision surfaces and
  immersed finite elements</title><categories>math.NA cs.CE</categories><doi>10.1016/j.cma.2015.11.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new optimisation technique that combines multiresolution
subdivision surfaces for boundary description with immersed finite elements for
the discretisation of the primal and adjoint problems of optimisation. Similar
to wavelets multiresolution surfaces represent the domain boundary using a
coarse control mesh and a sequence of detail vectors. Based on the
multiresolution decomposition efficient and fast algorithms are available for
reconstructing control meshes of varying fineness. During shape optimisation
the vertex coordinates of control meshes are updated using the computed shape
gradient information. By virtue of the multiresolution editing semantics,
updating the coarse control mesh vertex coordinates leads to large-scale
geometry changes and, conversely, updating the fine control mesh coordinates
leads to small-scale geometry changes. In our computations we start by
optimising the coarsest control mesh and refine it each time the cost function
reaches a minimum. This approach effectively prevents the appearance of
non-physical boundary geometry oscillations and control mesh pathologies, like
inverted elements. Independent of the fineness of the control mesh used for
optimisation, on the immersed finite element grid the domain boundary is always
represented with a relatively fine control mesh of fixed resolution. With the
immersed finite element method there is no need to maintain an analysis
suitable domain mesh. In some of the presented two- and three-dimensional
elasticity examples the topology derivative is used for creating new holes
inside the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02728</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02728</id><created>2015-10-09</created><authors><author><keyname>Sani</keyname><forenames>Alireza</forenames></author><author><keyname>Vosoughi</keyname><forenames>Azadeh</forenames></author></authors><title>On Distributed Vector Estimation for Power and Bandwidth Constrained
  Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 18 figures, submitted to Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed estimation of a Gaussian vector with a linear
observation model in an inhomogeneous wireless sensor network, where a fusion
center (FC) reconstructs the unknown vector, using a linear estimator. Sensors
employ uniform multi-bit quantizers and binary PSK modulation, and communicate
with the FC over orthogonal power- and bandwidth-constrained wireless channels.
We study transmit power and quantization rate (measured in bits per sensor)
allocation schemes that minimize mean-square error (MSE). In particular, we
derive two closed-form upper bounds on the MSE, in terms of the optimization
parameters and propose coupled and decoupled resource allocation schemes that
minimize these bounds. We show that the bounds are good approximations of the
simulated MSE and the performance of the proposed schemes approaches the
clairvoyant centralized estimation when total transmit power or bandwidth is
very large. We study how the power and rate allocation are dependent on sensors
observation qualities and channel gains, as well as total transmit power and
bandwidth constraints. Our simulations corroborate our analytical results and
illustrate the superior performance of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02735</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02735</id><created>2015-10-09</created><updated>2015-10-14</updated><authors><author><keyname>Couto</keyname><forenames>Rodrigo de Souza</forenames></author><author><keyname>Secci</keyname><forenames>Stefano</forenames></author><author><keyname>Campista</keyname><forenames>Miguel Elias Mitre</forenames></author><author><keyname>Costa</keyname><forenames>Lu&#xed;s Henrique Maciel Kosmalski</forenames></author></authors><title>Reliability and Survivability Analysis of Data Center Network Topologies</title><categories>cs.NI</categories><comments>Journal of Network and Systems Management, 2015</comments><doi>10.1007/s10922-015-9354-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The architecture of several data centers have been proposed as alternatives
to the conventional three-layer one.Most of them employ commodity equipment for
cost reduction. Thus, robustness to failures becomes even more important,
because commodity equipment is more failure-prone. Each architecture has a
different network topology design with a specific level of redundancy. In this
work, we aim at analyzing the benefits of different data center topologies
taking the reliability and survivability requirements into account. We consider
the topologies of three alternative data center architecture: Fat-tree, BCube,
and DCell. Also, we compare these topologies with a conventional three-layer
data center topology. Our analysis is independent of specific equipment,
traffic patterns, or network protocols, for the sake of generality. We derive
closed-form formulas for the Mean Time To Failure of each topology. The results
allow us to indicate the best topology for each failure scenario. In
particular, we conclude that BCube is more robust to link failures than the
other topologies, whereas DCell has the most robust topology when considering
switch failures. Additionally, we show that all considered alternative
topologies outperform a three-layer topology for both types of failures. We
also determine to which extent the robustness of BCube and DCell is influenced
by the number of network interfaces per server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02743</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02743</id><created>2015-10-09</created><authors><author><keyname>Alvarez</keyname><forenames>Pedro</forenames></author><author><keyname>Galiotto</keyname><forenames>Carlo</forenames></author><author><keyname>van de Belt</keyname><forenames>Jonathan</forenames></author><author><keyname>Finn</keyname><forenames>Danny</forenames></author><author><keyname>Ahmadi</keyname><forenames>Hamed</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz</forenames></author></authors><title>Simulating Dense Small Cell Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through massive deployment of additional small cell infrastructure, Dense
Small cell Networks (DSNs) are expected to help meet the foreseen increase in
traffic demand on cellular networks. Performance assessment of architectural
and protocol solutions tailored to DSNs will require system and network level
simulators that can appropriately model the complex interference environment
found in those networks. This paper identifies the main features of DSN
simulators, and guides the reader in the selection of an appropriate simulator
for their desired investigations. We extend our discussion with a comparison of
representative DSN simulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02749</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02749</id><created>2015-10-08</created><authors><author><keyname>Poder</keyname><forenames>Endel</forenames></author></authors><title>A framework for the measurement and prediction of an individual
  scientist's performance</title><categories>cs.DL physics.soc-ph</categories><comments>13 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative bibliometric indicators are widely used to evaluate the
performance of scientists. However, traditional indicators are not much based
on the analysis of the processes intended to measure and the practical goals of
the measurement. In this study, I propose a simple framework to measure and
predict an individual researcher's scientific performance that takes into
account the main regularities of publication and citation processes and the
requirements of practical tasks. Statistical properties of the new indicator -
a scientist's personal impact rate - are illustrated by its application to a
sample of Estonian researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02755</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02755</id><created>2015-10-04</created><updated>2015-12-12</updated><authors><author><keyname>Sarkar</keyname><forenames>Koushiki</forenames></author><author><keyname>Law</keyname><forenames>Ritwika</forenames></author></authors><title>A Novel Approach to Document Classification using WordNet</title><categories>cs.IR cs.CL</categories><comments>(Working Paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content based Document Classification is one of the biggest challenges in the
context of free text mining. Current algorithms on document classifications
mostly rely on cluster analysis based on bag-of-words approach. However that
method is still being applied to many modern scientific dilemmas. It has
established a strong presence in fields like economics and social science to
merit serious attention from the researchers. In this paper we would like to
propose and explore an alternative grounded more securely on the dictionary
classification and correlatedness of words and phrases. It is expected that
application of our existing knowledge about the underlying classification
structure may lead to improvement of the classifier's performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02767</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02767</id><created>2015-10-09</created><authors><author><keyname>Kueng</keyname><forenames>Richard</forenames></author><author><keyname>Gross</keyname><forenames>David</forenames></author></authors><title>Qubit stabilizer states are complex projective 3-designs</title><categories>quant-ph cs.IT math.IT math.PR</categories><comments>12 pages, 0 figures. See also closely related work by Zhu and by Webb</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complex projective $t$-design is a configuration of vectors which is
``evenly distributed'' on a sphere in the sense that sampling uniformly from it
reproduces the moments of Haar measure up to order $2t$. We show that the set
of all $n$-qubit stabilizer states forms a complex projective $3$-design in
dimension $2^n$. Stabilizer states had previously only been known to constitute
$2$-designs. The main technical ingredient is a general recursion formula for
the so-called frame potential of stabilizer states. To establish it, we need to
compute the number of stabilizer states with pre-described inner product with
respect to a reference state. This, in turn, reduces to a counting problem in
discrete symplectic vector spaces for which we find a simple formula. We sketch
applications in quantum information and signal analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02774</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02774</id><created>2015-10-09</created><authors><author><keyname>Borovikov</keyname><forenames>Eugene</forenames></author></authors><title>Human Head Pose Estimation by Facial Features Location</title><categories>cs.CV</categories><comments>This is a master's thesis completed at UMCP in 1998, being published
  here given enough of the demand on its contents from the Computer Vision R&amp;D
  community</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for estimating human head pose in a color image that
contains enough of information to locate the head silhouette and detect
non-trivial color edges of individual facial features. The method works by
spotting the human head on an arbitrary background, extracting the head
outline, and locating facial features necessary to describe the head
orientation in the 3D space. It is robust enough to work with both color and
gray-level images featuring quasi-frontal views of a human head under variable
lighting conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02775</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02775</id><created>2015-10-09</created><authors><author><keyname>Rashid</keyname><forenames>Mahmood A.</forenames></author><author><keyname>Khatib</keyname><forenames>Firas</forenames></author><author><keyname>Sattar</keyname><forenames>Abdul</forenames></author></authors><title>Protein preliminaries and structure prediction fundamentals for computer
  scientists</title><categories>cs.CE q-bio.BM</categories><comments>23 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein structure prediction is a challenging and unsolved problem in
computer science. Proteins are the sequence of amino acids connected together
by single peptide bond. The combinations of the twenty primary amino acids are
the constituents of all proteins. In-vitro laboratory methods used in this
problem are very time-consuming, cost-intensive, and failure-prone. Thus,
alternative computational methods come into play. The protein structure
prediction problem is to find the three-dimensional native structure of a
protein, from its amino acid sequence. The native structure of a protein has
the minimum free energy possible and arguably determines the function of the
protein. In this study, we present the preliminaries of proteins and their
structures, protein structure prediction problem, and protein models. We also
give a brief overview on experimental and computational methods used in protein
structure prediction. This study will provide a fundamental knowledge to the
computer scientists who are intending to pursue their future research on
protein structure prediction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02777</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02777</id><created>2015-10-09</created><updated>2016-02-07</updated><authors><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author></authors><title>Early Inference in Energy-Based Models Approximates Back-Propagation</title><categories>cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.05936</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that Langevin MCMC inference in an energy-based model with latent
variables has the property that the early steps of inference, starting from a
stationary point, correspond to propagating error gradients into internal
layers, similarly to back-propagation. The error that is back-propagated is
with respect to visible units that have received an outside driving force
pushing them away from the stationary point. Back-propagated error gradients
correspond to temporal derivatives of the activation of hidden units. This
observation could be an element of a theory for explaining how brains perform
credit assignment in deep hierarchies as efficiently as back-propagation does.
In this theory, the continuous-valued latent variables correspond to averaged
voltage potential (across time, spikes, and possibly neurons in the same
minicolumn), and neural computation corresponds to approximate inference and
error back-propagation at the same time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02781</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02781</id><created>2015-10-09</created><authors><author><keyname>Moreira</keyname><forenames>Thierry Pinheiro</forenames></author><author><keyname>Perez</keyname><forenames>Mauricio Lisboa</forenames></author><author><keyname>Werneck</keyname><forenames>Rafael de Oliveira</forenames></author><author><keyname>Valle</keyname><forenames>Eduardo</forenames></author></authors><title>Where is my puppy? Retrieving lost dogs by facial features</title><categories>cs.CV</categories><comments>11 pages, 7 figures, 1 table, Pattern Recognition Letters</comments><msc-class>68T45</msc-class><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pet that goes missing is among many people's worst fears. A moment of
distraction is enough for a dog or a cat wandering off from home. Animal
management services collect stray animals and try to find their owners, but not
always successfully. Some measures may improve the chances of matching lost
animals to their owners; but automated visual recognition is one that --
although convenient, highly available, and low-cost -- is surprisingly
overlooked. In this paper, we inaugurate that promising avenue by pursuing face
recognition for dogs. We contrast three ready-to-use human facial recognizers
(EigenFaces, FisherFaces and LBPH) to two original solutions based upon
existing convolutional neural networks: BARK (inspired in
architecture-optimized networks employed for human facial recognition) and WOOF
(based upon off-the-shelf OverFeat features). Human facial recognizers perform
poorly for dogs (up to 56.1% accuracy), showing that dog facial recognition is
not a trivial extension of human facial recognition. The convolutional network
solutions work much better, with BARK attaining up to 81.1% accuracy, and WOOF,
89.4%. The tests were conducted in two datasets: Flickr-dog, with 42 dogs of
two breeds (pugs and huskies); and Snoopybook, with 18 mongrel dogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02786</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02786</id><created>2015-10-09</created><authors><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author><author><keyname>Wu</keyname><forenames>Yihong</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Recovering a Hidden Community Beyond the Spectral Limit in $O(|E|
  \log^*|V|)$ Time</title><categories>stat.ML cs.CC cs.SI math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stochastic block model for one community with parameters $n, K, p,$ and
$q$ is considered: $K$ out of $n$ vertices are in the community; two vertices
are connected by an edge with probability $p$ if they are both in the community
and with probability $q$ otherwise, where $p &gt; q &gt; 0$ and $p/q$ is assumed to
be bounded. An estimator based on observation of the graph $G=(V,E)$ is said to
achieve weak recovery if the mean number of misclassified vertices is $o(K)$ as
$n \to \infty$. A critical role is played by the effective signal-to-noise
ratio $\lambda=K^2(p-q)^2/((n-K)q).$ In the regime $K=\Theta(n)$, a na\&quot;{i}ve
degree-thresholding algorithm achieves weak recovery in $O(|E|)$ time if
$\lambda \to \infty$, which coincides with the information theoretic
possibility of weak recovery.
  The main focus of the paper is on weak recovery in the sublinear regime
$K=o(n)$ and $np = n^{o(1)}.$ It is shown that weak recovery is provided by a
belief propagation algorithm running for $\log^\ast(n)+O(1) $ iterations, if
$\lambda &gt; 1/e,$ with the total time complexity $O(|E| \log^*n)$. Conversely,
no local algorithm with radius $t$ of interaction satisfying $t = o(\frac{\log
n}{\log(2+np)})$ can asymptotically outperform trivial random guessing if
$\lambda \leq 1/e.$ By analyzing a linear message-passing algorithm that
corresponds to applying power iteration to the non-backtracking matrix of the
graph, we provide evidence to suggest that spectral methods fail to provide
weak recovery if $\lambda \leq 1.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02787</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02787</id><created>2015-10-06</created><updated>2015-10-27</updated><authors><author><keyname>Ambroszkiewicz</keyname><forenames>Stanislaw</forenames></author></authors><title>Continuum as a primitive type (version 2)</title><categories>math.LO cs.LO</categories><comments>The introduced new primitive types along with constructors, primitive
  operations and primitive relations should be seen as a part of the general
  framework for a constructive type theory presented in the work Types and
  operations see arXiv:1501.03043</comments><msc-class>03D</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is the revision and extended version of the Section 6 of the paper
{\em Types and operations} arXiv:1501.03043. Here, primitive types
(corresponding to the intuitive concept of Continuum) are introduced along with
primitive operations, constructors, and relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02789</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02789</id><created>2015-10-08</created><authors><author><keyname>Chancelier</keyname><forenames>Jean-Philippe</forenames><affiliation>CERMICS</affiliation></author><author><keyname>Nikoukhah</keyname><forenames>Ramine</forenames><affiliation>METALAU</affiliation></author></authors><title>A novel code generation methodology for block diagram modeler and
  simulators Scicos and VSS</title><categories>cs.MS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Block operations during simulation in Scicos and VSS environments can
naturally be described as Nsp functions. But the direct use of Nsp functions
for simulation leads to poor performance since the Nsp language is interpreted,
not compiled. The methodology presented in this paper is used to develop a tool
for generating efficient compilable code, such as C and ADA, for Scicos and VSS
models from these block Nsp functions. Operator overloading and partial
evaluation are the key elements of this novel approach. This methodology may be
used in other simulation environments such as Matlab/Simulink.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02795</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02795</id><created>2015-10-09</created><authors><author><keyname>Hauberg</keyname><forenames>S&#xf8;ren</forenames></author><author><keyname>Freifeld</keyname><forenames>Oren</forenames></author><author><keyname>Larsen</keyname><forenames>Anders Boesen Lindbo</forenames></author><author><keyname>Fisher</keyname><forenames>John W.</forenames><suffix>III</suffix></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames></author></authors><title>Dreaming More Data: Class-dependent Distributions over Diffeomorphisms
  for Learned Data Augmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data augmentation is a key element in training high-dimensional models. In
this approach, one synthesizes new observations by applying pre-specified
transformations to the original training data; e.g. new images are formed by
rotating old ones. Current augmentation schemes, however, rely on manual
specification of the applied transformations, making data augmentation an
implicit form of feature engineering. Working towards true end-to-end learning,
we suggest to learn the applied transformations on a per-class basis.
Particularly, we align image pairs within each class under the assumption that
the spatial transformation between images belongs to a large class of
diffeomorphisms. For each class, we then build a probabilistic generative model
of the transformations in a Riemannian submanifold of the Lie group of
diffeomorphisms. We demonstrate significant performance improvements in
training deep neural nets over manually-specified augmentation schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02807</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02807</id><created>2015-10-09</created><authors><author><keyname>Pudwell</keyname><forenames>Lara</forenames></author><author><keyname>Rowland</keyname><forenames>Eric</forenames></author></authors><title>Avoiding fractional powers over the natural numbers</title><categories>math.CO cs.DM</categories><comments>42 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the lexicographically least infinite $a/b$-power-free word on the
alphabet of non-negative integers. Frequently this word is a fixed point of a
uniform morphism, or closely related to one. For example, the lexicographically
least $7/4$-power-free word is a fixed point of a $50847$-uniform morphism. We
identify the structure of the lexicographically least $a/b$-power-free word for
three infinite families of rationals $a/b$ as well many &quot;sporadic&quot; rationals
that do not seem to belong to general families. Along the way, we develop an
automated procedure for proving $a/b$-power-freeness for morphisms of a certain
form. Finally, we establish a connection to words on a finite alphabet. Namely,
the lexicographically least $27/23$-power-free word is in fact a word on the
finite alphabet $\{0, 1, 2\}$, and its sequence of letters is $353$-automatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02818</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02818</id><created>2015-10-09</created><updated>2015-10-15</updated><authors><author><keyname>Steinert</keyname><forenames>Rebecca</forenames></author><author><keyname>John</keyname><forenames>Wolfgang</forenames></author><author><keyname>Sk&#xf6;ldstr&#xf6;m</keyname><forenames>Pontus</forenames></author><author><keyname>Pechenot</keyname><forenames>Bertrand</forenames></author><author><keyname>Guly&#xe1;s</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Pelle</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>L&#xe9;vai</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>N&#xe9;meth</keyname><forenames>Felici&#xe1;n</forenames></author><author><keyname>Kim</keyname><forenames>Juhoon</forenames></author><author><keyname>Meirosu</keyname><forenames>Catalin</forenames></author><author><keyname>Cai</keyname><forenames>Xuejun</forenames></author><author><keyname>Fu</keyname><forenames>Chunyan</forenames></author><author><keyname>Pentikousis</keyname><forenames>Kostas</forenames></author><author><keyname>Sharma</keyname><forenames>Sachin</forenames></author><author><keyname>Papafili</keyname><forenames>Ioanna</forenames></author><author><keyname>Marchetto</keyname><forenames>Guido</forenames></author><author><keyname>Sisto</keyname><forenames>Riccardo</forenames></author><author><keyname>Risso</keyname><forenames>Fulvio</forenames></author><author><keyname>Kreuger</keyname><forenames>Per</forenames></author><author><keyname>Ekman</keyname><forenames>Jan</forenames></author><author><keyname>Liu</keyname><forenames>Shaoteng</forenames></author><author><keyname>Manzalini</keyname><forenames>Antonio</forenames></author><author><keyname>Shukla</keyname><forenames>Apoorv</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>Service Provider DevOps network capabilities and tools</title><categories>cs.NI</categories><comments>This is the public deliverable D4.2 of the EU FP7 UNIFY project
  (ICT-619609) - &quot;Proposal for SP-DevOps network capabilities and tools&quot;.
  Original Deliverable published at
  https://www.fp7-unify.eu/files/fp7-unify-eu-docs/UNIFY-WP4-D4.2%20Proposal%20for%20SP-DevOps%20network%20capabilities%20and%20tools.pdf</comments><acm-class>C.2.1; C.2.2; C.2.3; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report provides an understanding of how the UNIFY Service Provider
(SP)-DevOps concept can be applied and integrated with a combined cloud and
transport network NFV architecture. Specifically, the report contains technical
descriptions of a set of novel SP-DevOps tools and support functions that
facilitate observability, troubleshooting, verification, and VNF development
processes. The tools and support functions are described in detail together
with their architectural mapping, giving a wider understanding of the SP-DevOps
concept as a whole, and how SP-DevOps tools can be used for supporting
orchestration and programmability in the UNIFY NFV framework. The concept is
further exemplified in a case study for deployment and scaling of an Elastic
Firewall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02822</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02822</id><created>2015-10-09</created><authors><author><keyname>Venkateswaran</keyname><forenames>Vijay</forenames></author><author><keyname>Pivit</keyname><forenames>Florian</forenames></author><author><keyname>Guan</keyname><forenames>Lei</forenames></author></authors><title>Hybrid RF and Digital Beamformer for Cellular Networks: Algorithms,
  Microwave Architectures and Measurements</title><categories>cs.IT math.IT</categories><comments>Keywords - 5G, massive MIMO, beamforming, millimeter wave, microwave,
  Hybrid RF and digital beamforming, cellular networks, Butler matrix</comments><report-no>Bell Laboratories Technical report 2015-05-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern wireless communication networks, particularly cellular networks
utilize multiple antennas to improve the capacity and signal coverage. In these
systems, typically an active transceiver is connected to each antenna. However,
this one-to-one mapping between transceivers and antennas will dramatically
increase the cost and complexity of a large phased antenna array system.
  In this paper, firstly we propose a \emph{partially adaptive} beamformer
architecture where a reduced number of transceivers with a digital beamformer
(DBF) is connected to an increased number of antennas through an RF beamforming
network (RFBN). Then, based on the proposed architecture, we present a
methodology to derive the minimum number of transceivers that are required for
marco-cell and small-cell base stations, respectively. Subsequently, in order
to achieve optimal beampatterns with given cellular standard requirements and
RF operational constraints, we propose efficient algorithms to jointly design
DBF and RFBN. Starting from the proposed algorithms, we specify generic
microwave RFBNs for optimal marco-cell and small-cell networks. In order to
verify the proposed approaches, we compare the performance of RFBN using
simulations and anechoic chamber measurements. Experimental measurement results
confirm the robustness and performance of the proposed hybrid DBF-RFBN concept
eventually ensuring that theoretical multi-antenna capacity and coverage are
achieved at a little incremental cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02823</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02823</id><created>2015-10-09</created><authors><author><keyname>Gildea</keyname><forenames>Daniel</forenames></author><author><keyname>Jaeger</keyname><forenames>T. Florian</forenames></author></authors><title>Human languages order information efficiently</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most languages use the relative order between words to encode meaning
relations. Languages differ, however, in what orders they use and how these
orders are mapped onto different meanings. We test the hypothesis that, despite
these differences, human languages might constitute different `solutions' to
common pressures of language use. Using Monte Carlo simulations over data from
five languages, we find that their word orders are efficient for processing in
terms of both dependency length and local lexical probability. This suggests
that biases originating in how the brain understands language strongly
constrain how human languages change over generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02824</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02824</id><created>2015-10-09</created><updated>2015-10-20</updated><authors><author><keyname>Ahle</keyname><forenames>Thomas D.</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author><author><keyname>Silvestri</keyname><forenames>Francesco</forenames></author></authors><title>On the Complexity of Inner Product Similarity Join</title><categories>cs.DS cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of tasks in classification, information retrieval, recommendation
systems, and record linkage reduce to the core problem of inner product
similarity join (IPS join): identifying pairs of vectors in a collection that
have a sufficiently large inner product. IPS join is well understood when
vectors are normalized and some approximation of inner products is allowed.
However, the general case where vectors may have any length appears much more
challenging. Recently, new upper bounds based on asymmetric locality-sensitive
hashing (ALSH) and asymmetric embeddings have emerged, but little has been
known on the lower bound side. In this paper we initiate a systematic study of
inner product similarity join, showing new lower and upper bounds. Our main
results are:
  * Approximation hardness of IPS join in subquadratic time, assuming the
strong exponential time hypothesis.
  * New upper and lower bounds for (A)LSH-based algorithms. In particular, we
show that asymmetry can be avoided by relaxing the LSH definition to only
consider the collision probability of distinct elements.
  * A new indexing method for IPS based on linear sketches, implying that our
hardness results are not far from being tight.
  Our technical contributions include new asymmetric embeddings that may be of
independent interest. At the conceptual level we strive to provide greater
clarity, for example by distinguishing among signed and unsigned variants of
IPS join and shedding new light on the effect of asymmetry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02826</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02826</id><created>2015-10-09</created><updated>2015-12-30</updated><authors><author><keyname>Aijaz</keyname><forenames>Adnan</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Aghvami</keyname><forenames>A. Hamid</forenames></author><author><keyname>Friderikos</keyname><forenames>Vasilis</forenames></author><author><keyname>Frodigh</keyname><forenames>Magnus</forenames></author></authors><title>Realizing the Tactile Internet: Haptic Communications over Next
  Generation 5G Cellular Networks</title><categories>cs.NI</categories><comments>IEEE Wireless Communications - Accepted for Publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior Internet designs encompassed the fixed, mobile and lately the things
Internet. In a natural evolution to these, the notion of the Tactile Internet
is emerging which allows one to transmit touch and actuation in real-time. With
voice and data communications driving the designs of the current Internets, the
Tactile Internet will enable haptic communications, which in turn will be a
paradigm shift in how skills and labor are digitally delivered globally. Design
efforts for both the Tactile Internet and the underlying haptic communications
are in its infancy. The aim of this article is thus to review some of the most
stringent design challenges, as well as proposing first avenues for specific
solutions to enable the Tactile Internet revolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02828</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02828</id><created>2015-10-09</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author><author><keyname>Rueda</keyname><forenames>Camilo</forenames></author><author><keyname>Ag&#xf3;n</keyname><forenames>Carlos</forenames></author><author><keyname>Assayag</keyname><forenames>G&#xe9;rard</forenames></author></authors><title>Gelisp: A Library to Represent Musical CSPs and Search Strategies</title><categories>cs.AI</categories><comments>7 pages, 2 figures, not published</comments><acm-class>D.1.6; D.1.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present Gelisp, a new library to represent musical
Constraint Satisfaction Problems and search strategies intuitively. Gelisp has
two interfaces, a command-line one for Common Lisp and a graphical one for
OpenMusic. Using Gelisp, we solved a problem of automatic music generation
proposed by composer Michael Jarrell and we found solutions for the
All-interval series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02833</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02833</id><created>2015-10-09</created><authors><author><keyname>Gardner</keyname><forenames>Andrew</forenames></author><author><keyname>Duncan</keyname><forenames>Christian A.</forenames></author><author><keyname>Kanno</keyname><forenames>Jinko</forenames></author><author><keyname>Selmic</keyname><forenames>Rastko R.</forenames></author></authors><title>Earth Mover's Distance Yields Positive Definite Kernels For Certain
  Ground Distances</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positive definite kernels are an important tool in machine learning that
enable efficient solutions to otherwise difficult or intractable problems by
implicitly linearizing the problem geometry. In this paper we develop a
set-theoretic interpretation of the Earth Mover's Distance (EMD) that naturally
yields metric and kernel forms of EMD as generalizations of elementary set
operations. In particular, EMD is generalized to sets of unequal size. We also
offer the first proof of positive definite kernels based directly on EMD, and
provide propositions and conjectures concerning what properties are necessary
and sufficient for EMD to be conditionally negative definite. In particular, we
show that three distinct positive definite kernels -- intersection, minimum,
and Jaccard index -- can be derived from EMD with various ground distances. In
the process we show that the Jaccard index is simply the result of a positive
definite preserving transformation that can be applied to any kernel. Finally,
we evaluate the proposed kernels in various computer vision tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02834</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02834</id><created>2015-10-09</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author><author><keyname>Rueda</keyname><forenames>Camilo</forenames></author><author><keyname>Ag&#xf3;n</keyname><forenames>Carlos</forenames></author><author><keyname>Assayag</keyname><forenames>G&#xe9;rard</forenames></author></authors><title>NTCCRT: A concurrent constraint framework for real-time interaction
  (extended version)</title><categories>cs.LO cs.MM</categories><comments>12 pages, short version published in the International Computer Music
  Conference (ICMC), 2009</comments><acm-class>D.1.3; D.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Writing multimedia interaction systems is not easy. Their concurrent
processes usually access shared resources in a non-deterministic order, often
leading to unpredictable behavior. Using Pure Data (Pd) and Max/MSP is possible
to program concurrency, however, it is difficult to synchronize processes based
on multiple criteria. Process calculi such as the Non-deterministic Timed
Concurrent Constraint (ntcc) calculus, overcome that problem by representing
multiple criteria as constraints. We propose using our framework Ntccrt to
manage concurrency in Pd and Max. Ntccrt is a real-time capable inter- preter
for ntcc. Using Ntccrt externals (binary plugins) in Pd we ran models for
machine improvisation and signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02836</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02836</id><created>2015-10-09</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author><author><keyname>Desainte-Catherine</keyname><forenames>Myriam</forenames></author><author><keyname>Baltazar</keyname><forenames>Pascal</forenames></author></authors><title>A Model for Interactive Scores with Temporal Constraints and Conditional
  Branching</title><categories>cs.LO</categories><comments>14 pages, extended version of conference paper on Journ\'ees de
  INformatique Musicale 2010</comments><acm-class>D.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive Scores (IS) are a formalism for the design and performance of
interactive multimedia scenarios. IS provide temporal relations (TR), but they
cannot represent conditional branching and TRs simultaneously. We propose an
extension to Allombert et al.'s IS model by including a condition on the TRs.
We found out that in order to have a coherent model in all possible scenarios,
durations must be flexible; however, sometimes it is possible to have fixed
durations. To show the relevance of our model, we modeled an existing
multimedia installation called Mariona. In Mariona there is choice, random
durations and loops. Whether we can represent all the TRs available in
Allombert et al.'s model into ours, or we have to choose between a timed
conditional branching model and a pure temporal model before writing a
scenario, still remains as an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02838</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02838</id><created>2015-10-09</created><updated>2016-01-08</updated><authors><author><keyname>Petrovi&#x107;</keyname><forenames>Sonja</forenames></author></authors><title>A survey of discrete methods in (algebraic) statistics for networks</title><categories>cs.DM math.CO stat.ME</categories><comments>Revised for clarity, minor updates, added example, upon suggestions
  of people mentioned in the acknowledgements section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling algorithms, hypergraph degree sequences, and polytopes play a
crucial role in statistical analysis of network data. This article offers a
brief overview of open problems in this area of discrete mathematics from the
point of view of a particular family of statistical models for networks called
exponential random graph models. The problems and underlying constructions are
also related to well-known concepts in commutative algebra and graph-theoretic
concepts in computer science. We outline a few lines of recent work that
highlight the natural connection between these fields and unify them into some
open problems. While these problems are often relevant in discrete mathematics
in their own right, the emphasis here is on statistical relevance with the hope
that these lines of research do not remain disjoint. Suggested specific open
problems and general research questions should advance algebraic statistics
theory as well as applied statistical tools for rigorous statistical analysis
of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02840</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02840</id><created>2015-10-09</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author></authors><title>Concurrent Constraint Machine Improvisation: Models and Implementation</title><categories>cs.LO</categories><comments>8 pages</comments><acm-class>D.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine improvisation creates music either by explicit coding of rules or by
applying machine learning methods. We deal with the latter case. An
improvisation system capable of real-time must execute two process
concurrently: one to apply machine learning methods to musical sequences in
order to capture prominent musical features, and one to produce musical
sequences stylistically consistent with the learned material. As an example,
the Concurrent Constraint Factor Oracle Model for Music Improvisation (ccfomi),
based upon Non-deterministic Timed Concurrent Constraint (ntcc) calculus, uses
the Factor Oracle to store the learned sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02845</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02845</id><created>2015-10-09</created><updated>2016-03-02</updated><authors><author><keyname>Kulkarni</keyname><forenames>Mandar N.</forenames></author><author><keyname>Ghosh</keyname><forenames>Amitava</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>A Comparison of MIMO Techniques in Downlink Millimeter Wave Cellular
  Networks with Hybrid Beamforming</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Communications, under minor
  revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large antenna arrays will be needed in future millimeter wave (mmWave)
cellular networks, enabling a large number of different possible antenna
architectures and multiple-input multiple-output (MIMO) techniques. It is still
unclear which MIMO technique is most desirable as a function of different
network parameters. This paper, therefore, compares the coverage and rate
performance of hybrid beamforming enabled multi-user (MU) MIMO and single-user
spatial multiplexing (SM) with single-user analog beamforming (SU-BF). A
stochastic geometry model for coverage and rate analysis is proposed for
MU-MIMO mmWave cellular networks, taking into account important mmWave-specific
hardware constraints for hybrid analog/digital precoders and combiners, and a
blockage-dependent channel model which is sparse in angular domain. The
analytical results highlight the coverage, rate and power consumption tradeoffs
in multiuser mmWave networks. With perfect channel state information at the
transmitter and round robin scheduling, MU-MIMO is usually a better choice than
SM or SU-BF in mmWave cellular networks. This observation, however, neglects
any overhead due to channel acquisition or computational complexity.
Incorporating the impact of such overheads, our results can be re-interpreted
so as to quantify the minimum allowable efficiency of MU-MIMO to provide higher
rates than SM or SU-BF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02847</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02847</id><created>2015-10-09</created><updated>2015-10-15</updated><authors><author><keyname>Zhang</keyname><forenames>Chicheng</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author></authors><title>Active Learning from Weak and Strong Labelers</title><categories>cs.LG stat.ML</categories><comments>To appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An active learner is given a hypothesis class, a large set of unlabeled
examples and the ability to interactively query labels to an oracle of a subset
of these examples; the goal of the learner is to learn a hypothesis in the
class that fits the data well by making as few label queries as possible.
  This work addresses active learning with labels obtained from strong and weak
labelers, where in addition to the standard active learning setting, we have an
extra weak labeler which may occasionally provide incorrect labels. An example
is learning to classify medical images where either expensive labels may be
obtained from a physician (oracle or strong labeler), or cheaper but
occasionally incorrect labels may be obtained from a medical resident (weak
labeler). Our goal is to learn a classifier with low error on data labeled by
the oracle, while using the weak labeler to reduce the number of label queries
made to this labeler. We provide an active learning algorithm for this setting,
establish its statistical consistency, and analyze its label complexity to
characterize when it can provide label savings over using the strong labeler
alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02851</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02851</id><created>2015-10-09</created><authors><author><keyname>Ma</keyname><forenames>Yuanye</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Lin</keyname><forenames>Zihuai</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author><author><keyname>Li</keyname><forenames>Xu</forenames></author></authors><title>Spectrum Sharing in RF-Powered Cognitive Radio Networks using Game
  Theory</title><categories>cs.IT math.IT</categories><comments>Presented at PIMRC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the spectrum sharing problem of a radio frequency (RF)-powered
cognitive radio network, where a multi-antenna secondary user (SU) harvests
energy from RF signals radiated by a primary user (PU) to boost its available
energy before information transmission. In this paper, we consider that both
the PU and SU are rational and self-interested. Based on whether the SU helps
forward the PU's information, we develop two different operation modes for the
considered network, termed as non-cooperative and cooperative modes. In the
non-cooperative mode, the SU harvests energy from the PU and then use its
available energy to transmit its own information without generating any
interference to the primary link. In the cooperative mode, the PU employs the
SU to relay its information by providing monetary incentives and the SU splits
its energy for forwarding the PU's information as well as transmitting its own
information. Optimization problems are respectively formulated for both
operation modes, which constitute a Stackelberg game with the PU as a leader
and the SU as a follower. We analyze the Stackelberg game by deriving solutions
to the optimization problems and the Stackelberg Equilibrium (SE) is
subsequently obtained. Simulation results show that the performance of the
Stackelberg game can approach that of the centralized optimization scheme when
the distance between the SU and its receiver is large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02855</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02855</id><created>2015-10-09</created><authors><author><keyname>Wallach</keyname><forenames>Izhar</forenames></author><author><keyname>Dzamba</keyname><forenames>Michael</forenames></author><author><keyname>Heifets</keyname><forenames>Abraham</forenames></author></authors><title>AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction
  in Structure-based Drug Discovery</title><categories>cs.LG cs.NE q-bio.BM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks comprise a subclass of deep neural
networks (DNN) with a constrained architecture that leverages the spatial and
temporal structure of the domain they model. Convolutional networks achieve the
best predictive performance in areas such as speech and image recognition by
hierarchically composing simple local features into complex models. Although
DNNs have been used in drug discovery for QSAR and ligand-based bioactivity
predictions, none of these models have benefited from this powerful
convolutional architecture. This paper introduces AtomNet, the first
structure-based, deep convolutional neural network designed to predict the
bioactivity of small molecules for drug discovery applications. We demonstrate
how to apply the convolutional concepts of feature locality and hierarchical
composition to the modeling of bioactivity and chemical interactions. In
further contrast to existing DNN techniques, we show that AtomNet's application
of local convolutional filters to structural target information successfully
predicts new active molecules for targets with no previously known modulators.
Finally, we show that AtomNet outperforms previous docking approaches on a
diverse set of benchmarks by a large margin, achieving an AUC greater than 0.9
on 57.8% of the targets in the DUDE benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02856</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02856</id><created>2015-10-09</created><updated>2015-10-12</updated><authors><author><keyname>Wetzels</keyname><forenames>Jos</forenames></author><author><keyname>Bokslag</keyname><forenames>Wouter</forenames></author></authors><title>Sponges and Engines: An introduction to Keccak and Keyak</title><categories>cs.CR</categories><comments>30 pages Revision: corrected minor terminology error</comments><acm-class>E.3</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this document we present an introductory overview of the algorithms and
design components underlying the Keccac cryptographic primitive and the Keyak
encryption scheme for authenticated (session-supporting) encryption. This
document aims to familiarize readers with the basic principles of authenticated
encryption, the Sponge and Duplex constructions (full-state, keyed as well as
regular versions), the permutation functions underlying Keccak and Keyak as
well as Keyak v2's Motorist mode of operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02866</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02866</id><created>2015-10-09</created><authors><author><keyname>He</keyname><forenames>Liangtian</forenames></author><author><keyname>Wang</keyname><forenames>Yilun</forenames></author></authors><title>Wavelet Frame Based Image Restoration Using Sparsity, Nonlocal and
  Support Prior of Frame Coefficients</title><categories>cs.CV</categories><msc-class>90-08</msc-class><acm-class>I.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wavelet frame systems have been widely investigated and applied for image
restoration and many other image processing problems over the past decades,
attributing to their good capability of sparsely approximating piece-wise
smooth functions such as images. Most wavelet frame based models exploit the
$l_1$ norm of frame coefficients for a sparsity constraint in the past. The
authors in \cite{ZhangY2013, Dong2013} proposed an $l_0$ minimization model,
where the $l_0$ norm of wavelet frame coefficients is penalized instead, and
have demonstrated that significant improvements can be achieved compared to the
commonly used $l_1$ minimization model. Very recently, the authors in
\cite{Chen2015} proposed $l_0$-$l_2$ minimization model, where the nonlocal
prior of frame coefficients is incorporated. This model proved to outperform
the single $l_0$ minimization based model in terms of better recovered image
quality. In this paper, we propose a truncated $l_0$-$l_2$ minimization model
which combines sparsity, nonlocal and support prior of the frame coefficients.
The extensive experiments have shown that the recovery results from the
proposed regularization method performs better than existing state-of-the-art
wavelet frame based methods, in terms of edge enhancement and texture
preserving performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02867</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02867</id><created>2015-10-09</created><updated>2015-10-14</updated><authors><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author><author><keyname>Hurwitz</keyname><forenames>Evan</forenames></author></authors><title>Artificial Intelligence and Asymmetric Information Theory</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When human agents come together to make decisions, it is often the case that
one human agent has more information than the other. This phenomenon is called
information asymmetry and this distorts the market. Often if one human agent
intends to manipulate a decision in its favor the human agent can signal wrong
or right information. Alternatively, one human agent can screen for information
to reduce the impact of asymmetric information on decisions. With the advent of
artificial intelligence, signaling and screening have been made easier. This
paper studies the impact of artificial intelligence on the theory of asymmetric
information. It is surmised that artificial intelligent agents reduce the
degree of information asymmetry and thus the market where these agents are
deployed become more efficient. It is also postulated that the more artificial
intelligent agents there are deployed in the market the less is the volume of
trades in the market. This is because for many trades to happen the asymmetry
of information on goods and services to be traded should exist, creating a
sense of arbitrage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02873</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02873</id><created>2015-10-10</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>Almost disjunct matrices from codes and designs</title><categories>cs.IT cs.DM math.IT</categories><comments>16 pages</comments><msc-class>68Q25, 94C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In group testing, simple binary-output tests are designed to identify a small
number $t$ of defective items that are present in a large population of $N$
items. Each test takes as input a group of items and produces a binary output
indicating whether the group is free of the defective items or contains one or
more of them.
  In this paper we study a relaxation of the combinatorial group testing
problem. A matrix is called $(t,\epsilon)$-disjunct if it gives rise to a
nonadaptive group testing scheme with the property of identifying a uniformly
random $t$-set of defective subjects out of a population of size $N$ with false
positive probability of an item at most $\epsilon$. We establish a new
connection between $(t,\epsilon)$-disjunct matrices and error correcting codes
based on the dual distance of the codes and derive estimates of the parameters
of codes that give rise to such schemes. Our methods rely on the moments of the
distance distribution of codes and inequalities for moments of sums of
independent random variables. We also outline a new connection between group
testing schemes and combinatorial designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02874</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02874</id><created>2015-10-10</created><authors><author><keyname>Prasanna</keyname><forenames>P.</forenames></author><author><keyname>Chandar</keyname><forenames>Sarath</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author></authors><title>TSEB: More Efficient Thompson Sampling for Policy Learning</title><categories>cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In model-based solution approaches to the problem of learning in an unknown
environment, exploring to learn the model parameters takes a toll on the
regret. The optimal performance with respect to regret or PAC bounds is
achievable, if the algorithm exploits with respect to reward or explores with
respect to the model parameters, respectively. In this paper, we propose TSEB,
a Thompson Sampling based algorithm with adaptive exploration bonus that aims
to solve the problem with tighter PAC guarantees, while being cautious on the
regret as well. The proposed approach maintains distributions over the model
parameters which are successively refined with more experience. At any given
time, the agent solves a model sampled from this distribution, and the sampled
reward distribution is skewed by an exploration bonus in order to generate more
informative exploration. The policy by solving is then used for generating more
experience that helps in updating the posterior over the model parameters. We
provide a detailed analysis of the PAC guarantees, and convergence of the
proposed approach. We show that our adaptive exploration bonus encourages the
additional exploration required for better PAC bounds on the algorithm. We
provide empirical analysis on two different simulated domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02877</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02877</id><created>2015-10-10</created><authors><author><keyname>Mahyari</keyname><forenames>Arash Golibagh</forenames></author><author><keyname>Aviyente</keyname><forenames>Selin</forenames></author></authors><title>Simultaneous Sparse Approximation and Common Component Extraction using
  Fast Distributed Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous sparse approximation is a generalization of the standard sparse
approximation, for simultaneously representing a set of signals using a common
sparsity model. Generalizing the compressive sensing concept to the
simultaneous sparse approximation yields distributed compressive sensing (DCS).
DCS finds the sparse representation of multiple correlated signals using the
common + innovation signal model. However, DCS is not efficient for joint
recovery of a large number of signals since it requires large memory and
computational time. In this paper, we propose a new hierarchical algorithm to
implement the jointly sparse recovery framework of DCS more efficiently. The
proposed algorithm is applied to video background extraction problem, where the
background corresponds to the common sparse activity across frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02879</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02879</id><created>2015-10-10</created><updated>2015-12-21</updated><authors><author><keyname>Rajendran</keyname><forenames>Janarthanan</forenames></author><author><keyname>Prasanna</keyname><forenames>P</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author><author><keyname>Khapra</keyname><forenames>Mitesh M.</forenames></author></authors><title>ADAAPT: A Deep Architecture for Adaptive Policy Transfer from Multiple
  Sources</title><categories>cs.AI cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to transfer knowledge from learnt source tasks to a new target
task can be very useful in speeding up the learning process of a Reinforcement
Learning agent. This has been receiving a lot of attention, but the application
of transfer poses two serious challenges which have not been adequately
addressed in the past. First, the agent should be able to avoid negative
transfer, which happens when the transfer hampers or slows down the learning
instead of speeding it up. Secondly, the agent should be able to do selective
transfer which is the ability to select and transfer from different and
multiple source tasks for different parts of the state space of the target
task. We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer,
which addresses these challenges. We test ADAAPT using two different
instantiations: One as ADAAPTive REINFORCE algorithm for direct policy search
and another as ADAAPTive Actor-Critic where the actor uses ADAAPT. Empirical
evaluations on simulated domains show that ADAAPT can be effectively used for
policy transfer from multiple source MDPs sharing the same state and action
space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02882</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02882</id><created>2015-10-10</created><updated>2016-01-06</updated><authors><author><keyname>K&#xf6;ppl</keyname><forenames>Dominik</forenames></author><author><keyname>Sadakane</keyname><forenames>Kunihiko</forenames></author></authors><title>Lempel-Ziv Computation In Compressed Space (LZ-CICS)</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that both the Lempel Ziv 77- and the 78-factorization of a text of
length $n$ on an integer alphabet of size $\sigma$ can be computed in $O(n \lg
\lg \sigma)$ time (linear time if we allow randomization) using $O(n \lg
\sigma)$ bits of working space. Given that a compressed representation of the
suffix tree is loaded into RAM, we can compute both factorizations in linear
time using $O(n)$ space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02884</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02884</id><created>2015-10-10</created><authors><author><keyname>Xue</keyname><forenames>Wufeng</forenames></author><author><keyname>Mou</keyname><forenames>Xuanqin</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>Learn to Evaluate Image Perceptual Quality Blindly from Statistics of
  Self-similarity</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the various image quality assessment (IQA) tasks, blind IQA (BIQA) is
particularly challenging due to the absence of knowledge about the reference
image and distortion type. Features based on natural scene statistics (NSS)
have been successfully used in BIQA, while the quality relevance of the feature
plays an essential role to the quality prediction performance. Motivated by the
fact that the early processing stage in human visual system aims to remove the
signal redundancies for efficient visual coding, we propose a simple but very
effective BIQA method by computing the statistics of self-similarity (SOS) in
an image. Specifically, we calculate the inter-scale similarity and intra-scale
similarity of the distorted image, extract the SOS features from these
similarities, and learn a regression model to map the SOS features to the
subjective quality score. Extensive experiments demonstrate very competitive
quality prediction performance and generalization ability of the proposed SOS
based BIQA method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02886</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02886</id><created>2015-10-10</created><updated>2015-12-03</updated><authors><author><keyname>Dai</keyname><forenames>Jian</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Guo</keyname><forenames>Chenjuan</forenames></author><author><keyname>Jensen</keyname><forenames>Christian S.</forenames></author></authors><title>Efficient and Accurate Path Cost Estimation Using Trajectory Data</title><categories>cs.DB</categories><comments>16pages, 42 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Using the growing volumes of vehicle trajectory data, it becomes increasingly
possible to capture time-varying and uncertain travel costs in a road network,
including travel time and fuel consumption. The current paradigm represents a
road network as a graph, assigns weights to the graph's edges by fragmenting
trajectories into small pieces that fit the underlying edges, and then applies
a routing algorithm to the resulting graph. We propose a new paradigm that
targets more accurate and more efficient estimation of the costs of paths by
associating weights with sub-paths in the road network. The paper provides a
solution to a foundational problem in this paradigm, namely that of computing
the time-varying cost distribution of a path.
  The solution consists of several steps. We first learn a set of random
variables that capture the joint distributions of sub-paths that are covered by
sufficient trajectories. Then, given a departure time and a path, we select an
optimal subset of learned random variables such that the random variables'
corresponding paths together cover the path. This enables accurate joint
distribution estimation of the path, and by transferring the joint distribution
into a marginal distribution, the travel cost distribution of the path is
obtained. The use of multiple learned random variables contends with data
sparseness, the use of multi-dimensional histograms enables compact
representation of arbitrary joint distributions that fully capture the travel
cost dependencies among the edges in paths. Empirical studies with substantial
trajectory data from two different cities offer insight into the design
properties of the proposed solution and suggest that the solution is effective
in real-world settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02892</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02892</id><created>2015-10-10</created><authors><author><keyname>Abdallah</keyname><forenames>Tarek Amr</forenames></author><author><keyname>de La Iglesia</keyname><forenames>Beatriz</forenames></author></authors><title>Survey on Feature Selection</title><categories>cs.LG</categories><comments>Report, for Data Mining class during KDD Masters Degree at University
  of East Anglia</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Feature selection plays an important role in the data mining process. It is
needed to deal with the excessive number of features, which can become a
computational burden on the learning algorithms. It is also necessary, even
when computational resources are not scarce, since it improves the accuracy of
the machine learning tasks, as we will see in the upcoming sections. In this
review, we discuss the different feature selection approaches, and the relation
between them and the various machine learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02894</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02894</id><created>2015-10-10</created><authors><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Bhargava</keyname><forenames>Vijay K.</forenames></author></authors><title>Per-Antenna Constant Envelope Precoding for Secure Transmission in
  Large-Scale MISO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures. Accepted by 2015 IEEE/CIC International
  Conference on Communications in China (ICCC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure transmission in large-scale MISO systems employing artificial noise
(AN) is studied under the per-antenna constant envelope (CE) constraint.
Achievable secrecy rate of the per-antenna CE precoding scheme for large-scale
MISO is analyzed and compared with that of the matched filter linear precoding.
A two-stage per-antenna CE precoding scheme for joint signal-plus-AN
transmission is proposed. The first stage determines the per-antenna CE
precoding for the information-bearing signal. A properly generated AN using an
iteration algorithm is incorporated into the transmit signal in the second
stage such that the combined signal-plus-AN satisfies the per-antenna CE
constraint and the AN is orthogonal to the user channel. It is shown that
compared to conventional per-antenna CE transmission, this joint signal-plus-AN
secure transmission scheme does not require additional transmit power. An
alternative low-complexity AN generation scheme which uses a separate antenna
to cancel the AN leakage to the intended user introduced by randomly generated
AN is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02895</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02895</id><created>2015-10-10</created><authors><author><keyname>Gaafar</keyname><forenames>Mohamed</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Abediseid</keyname><forenames>Walid</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Spectrum Sharing Opportunities of Full-Duplex Systems using Improper
  Gaussian Signaling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing the licensed spectrum of full-duplex (FD) primary users (PU) brings
strict limitations on the underlay cognitive radio operation. Particularly, the
self interference may overwhelm the PU receiver and limit the opportunity of
secondary users (SU) to access the spectrum. Improper Gaussian signaling (IGS)
has demonstrated its superiority in improving the performance of interference
channel systems. Throughout this paper, we assume a FD PU pair that uses proper
Gaussian signaling (PGS), and a half-duplex SU pair that uses IGS. The
objective is to maximize the SU instantaneous achievable rate while meeting the
PU quality-of-service. To this end, we propose a simplified algorithm that
optimizes the SU signal parameters, i.e, the transmit power and the circularity
coefficient, which is a measure of the degree of impropriety of the SU signal,
to achieve the design objective. Numerical results show the merits of adopting
IGS compared with PGS for the SU especially with the existence of week PU
direct channels and/or strong SU interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02899</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02899</id><created>2015-10-10</created><authors><author><keyname>Mazloom</keyname><forenames>Masoud</forenames></author><author><keyname>Li</keyname><forenames>Xirong</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author></authors><title>TagBook: A Semantic Video Representation without Supervision for Event
  Detection</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of event detection in video for scenarios where only
few, or even zero examples are available for training. For this challenging
setting, the prevailing solutions in the literature rely on a semantic video
representation obtained from thousands of pre-trained concept detectors.
Different from existing work, we propose a new semantic video representation
that is based on freely available social tagged videos only, without the need
for training any intermediate concept detectors. We introduce a simple
algorithm that propagates tags from a video's nearest neighbors, similar in
spirit to the ones used for image retrieval, but redesign it for video event
detection by including video source set refinement and varying the video tag
assignment. We call our approach TagBook and study its construction,
descriptiveness and detection performance on the TRECVID 2013 and 2014
multimedia event detection datasets and the Columbia Consumer Video dataset.
Despite its simple nature, the proposed TagBook video representation is
remarkably effective for few-example and zero-example event detection, even
outperforming very recent state-of-the-art alternatives building on supervised
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02902</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02902</id><created>2015-10-10</created><authors><author><keyname>Gaafar</keyname><forenames>Mohamed</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Abediseid</keyname><forenames>Walid</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Sharing the Licensed Spectrum of Full-Duplex Systems using Improper
  Gaussian Signaling</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1510.02895</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing the spectrum with in-band full-duplex (FD) primary users (PU) is a
challenging and interesting problem in the underlay cognitive radio (CR)
systems. The self-interference introduced at the primary network may
dramatically impede the secondary user (SU) opportunity to access the spectrum.
In this work, we attempt to tackle this problem through the use of the
so-called improper Gaussian signaling. Such a signaling technique has
demonstrated its superiority in improving the overall performance in
interference limited networks. Particularly, we assume a system with a SU pair
working in half-duplex mode that uses improper Gaussian signaling while the FD
PU pair implements the regular proper Gaussian signaling techniques. First, we
derive a closed form expression for the SU outage probability and an upper
bound for the PU outage probability. Then, we optimize the SU signal parameters
to minimize its outage probability while maintaining the required PU
quality-of-service based on the average channel state information. Finally, we
provide some numerical results that validate the tightness of the PU outage
probability bound and demonstrate the advantage of employing the improper
Gaussian signaling to the SU in order to access the spectrum of the FD PU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02906</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02906</id><created>2015-10-10</created><authors><author><keyname>Yang</keyname><forenames>Min</forenames></author><author><keyname>Jia</keyname><forenames>Yunde</forenames></author></authors><title>Temporal Dynamic Appearance Modeling for Online Multi-Person Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust online multi-person tracking requires the correct associations of
online detection responses with existing trajectories. We address this problem
by developing a novel appearance modeling approach to provide accurate
appearance affinities to guide data association. In contrast to most existing
algorithms that only consider the spatial structure of human appearances, we
exploit the temporal dynamic characteristics within temporal appearance
sequences to discriminate different persons. The temporal dynamic makes a
sufficient complement to the spatial structure of varying appearances in the
feature space, which significantly improves the affinity measurement between
trajectories and detections. We propose a feature selection algorithm to
describe the appearance variations with mid-level semantic features, and
demonstrate its usefulness in terms of temporal dynamic appearance modeling.
Moreover, the appearance model is learned incrementally by alternatively
evaluating newly-observed appearances and adjusting the model parameters to be
suitable for online tracking. Reliable tracking of multiple persons in complex
scenes is achieved by incorporating the learned model into an online
tracking-by-detection framework. Our experiments on the challenging benchmark
MOTChallenge 2015 demonstrate that our method outperforms the state-of-the-art
multi-person tracking algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02923</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02923</id><created>2015-10-10</created><authors><author><keyname>Martin</keyname><forenames>Adrian</forenames></author><author><keyname>Schiavi</keyname><forenames>Emanuele</forenames></author><author><keyname>de Leon</keyname><forenames>Sergio Segura</forenames></author></authors><title>On 1-Laplacian Elliptic Equations Modeling Magnetic Resonance Image
  Rician Denoising</title><categories>math.AP cs.CV math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling magnitude Magnetic Resonance Images (MRI) rician denoising in a
Bayesian or generalized Tikhonov framework using Total Variation (TV) leads
naturally to the consideration of nonlinear elliptic equations. These involve
the so called $1$-Laplacian operator and special care is needed to properly
formulate the problem. The rician statistics of the data are introduced through
a singular equation with a reaction term defined in terms of modified first
order Bessel functions. An existence theory is provided here together with
other qualitative properties of the solutions. Remarkably, each positive global
minimum of the associated functional is one of such solutions. Moreover, we
directly solve this non--smooth non--convex minimization problem using a
convergent Proximal Point Algorithm. Numerical results based on synthetic and
real MRI demonstrate a better performance of the proposed method when compared
to previous TV based models for rician denoising which regularize or convexify
the problem. Finally, an application on real Diffusion Tensor Images, a
strongly affected by rician noise MRI modality, is presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02927</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02927</id><created>2015-10-10</created><authors><author><keyname>Kruthiventi</keyname><forenames>Srinivas S. S.</forenames></author><author><keyname>Ayush</keyname><forenames>Kumar</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>DeepFix: A Fully Convolutional Neural Network for predicting Human Eye
  Fixations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding and predicting the human visual attentional mechanism is an
active area of research in the fields of neuroscience and computer vision. In
this work, we propose DeepFix, a first-of-its-kind fully convolutional neural
network for accurate saliency prediction. Unlike classical works which
characterize the saliency map using various hand-crafted features, our model
automatically learns features in a hierarchical fashion and predicts saliency
map in an end-to-end manner. DeepFix is designed to capture semantics at
multiple scales while taking global context into account using network layers
with very large receptive fields. Generally, fully convolutional nets are
spatially invariant which prevents them from modeling location dependent
patterns (e.g. centre-bias). Our network overcomes this limitation by
incorporating a novel Location Biased Convolutional layer. We evaluate our
model on two challenging eye fixation datasets -- MIT300, CAT2000 and show that
it outperforms other recent approaches by a significant margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02930</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02930</id><created>2015-10-10</created><authors><author><keyname>Feng</keyname><forenames>Wensen</forenames></author><author><keyname>Chen</keyname><forenames>Yunjin</forenames></author></authors><title>Fast and Accurate Poisson Denoising with Optimized Nonlinear Diffusion</title><categories>cs.CV</categories><comments>11 pages, 12 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degradation of the acquired signal by Poisson noise is a common problem
for various imaging applications, such as medical imaging, night vision and
microscopy. Up to now, many state-of-the-art Poisson denoising techniques
mainly concentrate on achieving utmost performance, with little consideration
for the computation efficiency. Therefore, in this study we aim to propose an
efficient Poisson denoising model with both high computational efficiency and
recovery quality. To this end, we exploit the newly-developed trainable
nonlinear reaction diffusion model which has proven an extremely fast image
restoration approach with performance surpassing recent state-of-the-arts. We
retrain the model parameters, including the linear filters and influence
functions by taking into account the Poisson noise statistics, and end up with
an optimized nonlinear diffusion model specialized for Poisson denoising. The
trained model provides strongly competitive results against state-of-the-art
approaches, meanwhile bearing the properties of simple structure and high
efficiency. Furthermore, our proposed model comes along with an additional
advantage, that the diffusion process is well-suited for parallel computation
on GPUs. For images of size $512 \times 512$, our GPU implementation takes less
than 0.1 seconds to produce state-of-the-art Poisson denoising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02934</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02934</id><created>2015-10-10</created><updated>2015-11-19</updated><authors><author><keyname>Koay</keyname><forenames>Cheng Guan</forenames></author><author><keyname>Yeh</keyname><forenames>Ping-Hong</forenames></author><author><keyname>Ollinger</keyname><forenames>John M.</forenames></author><author><keyname>&#x130;rfano&#x11f;lu</keyname><forenames>M. Okan</forenames></author><author><keyname>Pierpaoli</keyname><forenames>Carlo</forenames></author><author><keyname>Basser</keyname><forenames>Peter J.</forenames></author><author><keyname>Oakes</keyname><forenames>Terrence R.</forenames></author><author><keyname>Riedy</keyname><forenames>Gerard</forenames></author></authors><title>Tract Orientation and Angular Dispersion Deviation Indicator (TOADDI): A
  framework for single-subject analysis in diffusion tensor imaging</title><categories>physics.med-ph cs.CV stat.AP stat.CO stat.ME</categories><comments>49 pages, 6 figures, 2 tables</comments><doi>10.1016/j.neuroimage.2015.11.046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this work is to develop a framework for single-subject
analysis of diffusion tensor imaging (DTI) data. This framework (termed TOADDI)
is capable of testing whether an individual tract as represented by the major
eigenvector of the diffusion tensor and its corresponding angular dispersion
are significantly different from a group of tracts on a voxel-by-voxel basis.
This work develops two complementary statistical tests based on the elliptical
cone of uncertainty (COU), which is a model of uncertainty or dispersion of the
major eigenvector of the diffusion tensor. The orientation deviation test
examines whether the major eigenvector from a single subject is within the
average elliptical COU formed by a collection of elliptical COUs. The shape
deviation test is based on the two-tailed Wilcoxon-Mann-Whitney two-sample test
between the normalized shape measures (area and circumference) of the
elliptical cones of uncertainty of the single subject against a group of
controls. The False Discovery Rate (FDR) and False Non-discovery Rate (FNR)
were incorporated in the orientation deviation test. The shape deviation test
uses FDR only. TOADDI was found to be numerically accurate and statistically
effective. Clinical data from two Traumatic Brain Injury (TBI) patients and one
non-TBI subject were tested against the data obtained from a group of 45
non-TBI controls to illustrate the application of the proposed framework in
single-subject analysis. The frontal portion of the superior longitudinal
fasciculus seemed to be implicated in both tests as significantly different
from that of the control group. The TBI patients and the single non-TBI subject
were well separated under the shape deviation test at the chosen FDR level of
0.0005. TOADDI is a simple but novel geometrically based statistical framework
for analyzing DTI data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02942</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02942</id><created>2015-10-10</created><authors><author><keyname>Gecer</keyname><forenames>Baris</forenames></author><author><keyname>Yalcinkaya</keyname><forenames>Ozge</forenames></author><author><keyname>Tasar</keyname><forenames>Onur</forenames></author><author><keyname>Aksoy</keyname><forenames>Selim</forenames></author></authors><title>Evaluation of Joint Multi-Instance Multi-Label Learning For Breast
  Cancer Diagnosis</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-instance multi-label (MIML) learning is a challenging problem in many
aspects. Such learning approaches might be useful for many medical diagnosis
applications including breast cancer detection and classification. In this
study subset of digiPATH dataset (whole slide digital breast cancer
histopathology images) are used for training and evaluation of six
state-of-the-art MIML methods.
  At the end, performance comparison of these approaches are given by means of
effective evaluation metrics. It is shown that MIML-kNN achieve the best
performance that is %65.3 average precision, where most of other methods attain
acceptable results as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02947</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02947</id><created>2015-10-10</created><authors><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Concentration of Measure Inequalities and Their Communication and
  Information-Theoretic Applications</title><categories>cs.IT math.IT math.PR</categories><comments>To appear in the IEEE IT Society Newsletter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last two decades, concentration of measure has been a subject of
various exciting developments in convex geometry, functional analysis,
statistical physics, high-dimensional statistics, probability theory,
information theory, communications and coding theory, computer science, and
learning theory. One common theme which emerges in these fields is
probabilistic stability: complicated, nonlinear functions of a large number of
independent or weakly dependent random variables often tend to concentrate
sharply around their expected values. Information theory plays a key role in
the derivation of concentration inequalities. Indeed, both the entropy method
and the approach based on transportation-cost inequalities are two major
information-theoretic paths toward proving concentration.
  This brief survey is based on a recent monograph of the authors in the
Foundations and Trends in Communications and Information Theory (online
available at http://arxiv.org/pdf/1212.4663v8.pdf), and a tutorial given by the
authors at ISIT 2015. It introduces information theorists to three main
techniques for deriving concentration inequalities: the martingale method, the
entropy method, and the transportation-cost inequalities. Some applications in
information theory, communications, and coding theory are used to illustrate
the main ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02949</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02949</id><created>2015-10-10</created><authors><author><keyname>Mrowca</keyname><forenames>Damian</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Hu</keyname><forenames>Ronghang</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Spatial Semantic Regularisation for Large Scale Object Detection</title><categories>cs.CV</categories><comments>accepted at ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale object detection with thousands of classes introduces the problem
of many contradicting false positive detections, which have to be suppressed.
Class-independent non-maximum suppression has traditionally been used for this
step, but it does not scale well as the number of classes grows. Traditional
non-maximum suppression does not consider label- and instance-level
relationships nor does it allow an exploitation of the spatial layout of
detection proposals. We propose a new multi-class spatial semantic
regularisation method based on affinity propagation clustering, which
simultaneously optimises across all categories and all proposed locations in
the image, to improve both the localisation and categorisation of selected
detection proposals. Constraints are shared across the labels through the
semantic WordNet hierarchy. Our approach proves to be especially useful in
large scale settings with thousands of classes, where spatial and semantic
interactions are very frequent and only weakly supervised detectors can be
built due to a lack of bounding box annotations. Detection experiments are
conducted on the ImageNet and COCO dataset, and in settings with thousands of
detected categories. Our method provides a significant precision improvement by
reducing false positives, while simultaneously improving the recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02951</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02951</id><created>2015-10-10</created><authors><author><keyname>Razgon</keyname><forenames>Igor</forenames></author></authors><title>On oblivious branching programs with bounded repetition that cannot
  efficiently compute CNFs of bounded treewidth</title><categories>cs.CC cs.AI</categories><comments>Follow-up work of arxiv:1308.3829</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study complexity of an extension of ordered binary decision
diagrams (OBDDs) called $c$-OBDDs on CNFs of bounded (primal graph) treewidth.
In particular, we show that for each $k$ there is a class of CNFs of treewidth
$k \geq 3$ for which the equivalent $c$-OBDDs are of size
$\Omega(n^{k/(8c-4)})$. Moreover, this lower bound holds if $c$-OBDD is
non-deterministic and semantic. Our second result uses the above lower bound to
separate the above model from sentential decision diagrams (SDDs). In order to
obtain the lower bound, we use a structural graph parameter called matching
width. Our third result shows that matching width and pathwidth are linearly
related.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02956</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02956</id><created>2015-10-10</created><authors><author><keyname>R.</keyname><forenames>Kavitha</forenames></author><author><keyname>Ambadi</keyname><forenames>Niranjana</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On The Number of Optimal Linear Index Codes For Unicast Index Coding
  Problems</title><categories>cs.IT math.IT</categories><comments>Part of the content appears in Proceedings of IEEE International
  Symposium on Information Theory, (ISIT 2015), Hong Kong, 14-19 June 2015,
  pp.1044-1048</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An index coding problem arises when there is a single source with a number of
messages and multiple receivers each wanting a subset of messages and knowing a
different set of messages a priori. The noiseless Index Coding Problem is to
identify the minimum number of transmissions (optimal length) to be made by the
source through noiseless channels so that all receivers can decode their wanted
messages using the transmitted symbols and their respective prior information.
Recently, it is shown that different optimal length codes perform differently
in a noisy channel. Towards identifying the best optimal length index code one
needs to know the number of optimal length index codes. In this paper we
present results on the number of optimal length index codes making use of the
representation of an index coding problem by an equivalent network code. Our
formulation results in matrices of smaller sizes compared to the approach of
Kotter and Medard. Our formulation leads to a lower bound on the minimum number
of optimal length codes possible for all unicast index coding problems which is
met with equality for several special cases of the unicast index coding
problem. A method to identify the optimal length codes which lead to
minimum-maximum probability of error is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02969</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02969</id><created>2015-10-10</created><authors><author><keyname>Khorrami</keyname><forenames>Pooya</forenames></author><author><keyname>Paine</keyname><forenames>Tom Le</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Do Deep Neural Networks Learn Facial Action Units When Doing Expression
  Recognition?</title><categories>cs.CV cs.LG cs.NE</categories><comments>Accepted at ICCV 2015 CV4AC Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite being the appearance-based classifier of choice in recent years,
relatively few works have examined how much convolutional neural networks
(CNNs) can improve performance on accepted expression recognition benchmarks
and, more importantly, examine what it is they actually learn. In this work,
not only do we show that CNNs can achieve strong performance, but we also
introduce an approach to decipher which portions of the face influence the
CNN's predictions. First, we train a zero-bias CNN on facial expression data
and achieve, to our knowledge, state-of-the-art performance on two expression
recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto
Face Dataset (TFD). We then qualitatively analyze the network by visualizing
the spatial patterns that maximally excite different neurons in the
convolutional layers and show how they resemble Facial Action Units (FAUs).
Finally, we use the FAU labels provided in the CK+ dataset to verify that the
FAUs observed in our filter visualizations indeed align with the subject's
facial movements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02975</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02975</id><created>2015-10-10</created><authors><author><keyname>Berj&#xf3;n</keyname><forenames>Daniel</forenames></author><author><keyname>Gallego</keyname><forenames>Guillermo</forenames></author><author><keyname>Cuevas</keyname><forenames>Carlos</forenames></author><author><keyname>Mor&#xe1;n</keyname><forenames>Francisco</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>Narciso</forenames></author></authors><title>Optimal Piecewise Linear Function Approximation for GPU-based
  Applications</title><categories>math.OC cs.CV cs.DC cs.NA cs.SY</categories><comments>12 pages, 12 figures, post-print, IEEE Transactions on Cybernetics,
  Oct. 2015</comments><doi>10.1109/TCYB.2015.2482365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision and human-computer interaction applications developed in
recent years need evaluating complex and continuous mathematical functions as
an essential step toward proper operation. However, rigorous evaluation of this
kind of functions often implies a very high computational cost, unacceptable in
real-time applications. To alleviate this problem, functions are commonly
approximated by simpler piecewise-polynomial representations. Following this
idea, we propose a novel, efficient, and practical technique to evaluate
complex and continuous functions using a nearly optimal design of two types of
piecewise linear approximations in the case of a large budget of evaluation
subintervals. To this end, we develop a thorough error analysis that yields
asymptotically tight bounds to accurately quantify the approximation
performance of both representations. It provides an improvement upon previous
error estimates and allows the user to control the trade-off between the
approximation error and the number of evaluation subintervals. To guarantee
real-time operation, the method is suitable for, but not limited to, an
efficient implementation in modern Graphics Processing Units (GPUs), where it
outperforms previous alternative approaches by exploiting the fixed-function
interpolation routines present in their texture units. The proposed technique
is a perfect match for any application requiring the evaluation of continuous
functions, we have measured in detail its quality and efficiency on several
functions, and, in particular, the Gaussian function because it is extensively
used in many areas of computer vision and cybernetics, and it is expensive to
evaluate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02983</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02983</id><created>2015-10-10</created><authors><author><keyname>Xie</keyname><forenames>Boyi</forenames></author><author><keyname>Passonneau</keyname><forenames>Rebecca J.</forenames></author></authors><title>OmniGraph: Rich Representation and Graph Kernel Learning</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OmniGraph, a novel representation to support a range of NLP classification
tasks, integrates lexical items, syntactic dependencies and frame semantic
parses into graphs. Feature engineering is folded into the learning through
convolution graph kernel learning to explore different extents of the graph. A
high-dimensional space of features includes individual nodes as well as complex
subgraphs. In experiments on a text-forecasting problem that predicts stock
price change from news for company mentions, OmniGraph beats several benchmarks
based on bag-of-words, syntactic dependencies, and semantic trees. The highly
expressive features OmniGraph discovers provide insights into the semantics
across distinct market sectors. To demonstrate the method's generality, we also
report its high performance results on a fine-grained sentiment corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02990</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02990</id><created>2015-10-10</created><authors><author><keyname>Nilsson</keyname><forenames>Petter</forenames></author><author><keyname>Ozay</keyname><forenames>Necmiye</forenames></author></authors><title>Synthesis of separable controlled invariant sets for modular local
  control design</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many correct-by-construction control synthesis methods suffer from the curse
of dimensionality. Motivated by this challenge, we seek to reduce a
correct-by-construction control synthesis problem to subproblems of more modest
dimension. As a step towards this goal, in this paper we consider the problem
of synthesizing decoupled robustly controlled invariant sets for dynamically
coupled linear subsystems with state and input constraints. Our approach, which
gives sufficient conditions for decoupled invariance, is based on optimization
over linear matrix inequalities which are obtained using slack variable
identities. We illustrate the applicability of our method on several examples,
including one where we solve local control synthesis problems in a
compositional manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02995</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02995</id><created>2015-10-10</created><authors><author><keyname>Dashdorj</keyname><forenames>Zolzaya</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author></authors><title>Characterization of behavioral patterns exploiting description of
  geographical areas</title><categories>cs.SI cs.CY</categories><comments>17 pages, 13 figures</comments><msc-class>68</msc-class><acm-class>H.1.2; H.2.8; H.3.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enormous amount of recently available mobile phone data is providing
unprecedented direct measurements of human behavior. Early recognition and
prediction of behavioral patterns are of great importance in many societal
applications like urban planning, transportation optimization, and health-care.
Understanding the relationships between human behaviors and location's context
is an emerging interest for understanding human-environmental dynamics. Growing
availability of Web 2.0, i.e. the increasing amount of websites with mainly
user created content and social platforms opens up an opportunity to study such
location's contexts. This paper investigates relationships existing between
human behavior and location context, by analyzing log mobile phone data
records. First an advanced approach to categorize areas in a city based on the
presence and distribution of categories of human activity (e.g., eating,
working, and shopping) found across the areas, is proposed. The proposed
classification is then evaluated through its comparison with the patterns of
temporal variation of mobile phone activity and applying machine learning
techniques to predict a timeline type of communication activity in a given
location based on the knowledge of the obtained category vs. land-use type of
the locations areas. The proposed classification turns out to be more
consistent with the temporal variation of human communication activity, being a
better predictor for those compared to the official land use classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.02996</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.02996</id><created>2015-10-10</created><authors><author><keyname>Guruacharya</keyname><forenames>Sudarshan</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Integral Approximations for Coverage Probability</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter gives approximations to an integral appearing in the formula for
downlink coverage probability of a typical user in Poisson point process (PPP)
based stochastic geometry frameworks of the form $\int_0^\infty \exp\{ - (Ax +
B x^{\alpha/2}) \} \ud x$. Four different approximations are studied. For
systems that are interference-limited or noise-limited, conditions are
identified when the approximations are valid. For intermediate cases, we
recommend the use of Laplace approximation. Numerical results validate the
accuracy of the approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03004</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03004</id><created>2015-10-10</created><authors><author><keyname>Santos-Neto</keyname><forenames>Elizeu</forenames></author><author><keyname>Figueiredo</keyname><forenames>Flavio</forenames></author><author><keyname>Oliveira</keyname><forenames>Nigini</forenames></author><author><keyname>Andrade</keyname><forenames>Nazareno</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>Assessing the Value of Peer-Produced Information for Exploratory Search</title><categories>cs.SI cs.IR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tagging is a popular feature that supports several collaborative tasks,
including search, as tags produced by one user can help others finding relevant
content. However, task performance depends on the existence of 'good' tags. A
first step towards creating incentives for users to produce 'good' tags is the
quantification of their value in the first place. This work fills this gap by
combining qualitative and quantitative research methods. In particular, using
contextual interviews, we first determine aspects that influence users'
perception of tags' value for exploratory search. Next, we formalize some of
the identified aspects and propose an information-theoretical method with
provable properties that quantifies the two most important aspects (according
to the qualitative analysis) that influence the perception of tag value: the
ability of a tag to reduce the search space while retrieving relevant items to
the user. The evaluation on real data shows that our method is accurate: tags
that users consider more important have higher value than tags users have not
expressed interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03009</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03009</id><created>2015-10-11</created><updated>2016-02-26</updated><authors><author><keyname>Lin</keyname><forenames>Zhouhan</forenames></author><author><keyname>Courbariaux</keyname><forenames>Matthieu</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Neural Networks with Few Multiplications</title><categories>cs.LG cs.NE</categories><comments>Published as a conference paper at ICLR 2016. 9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For most deep learning algorithms training is notoriously time consuming.
Since most of the computation in training neural networks is typically spent on
floating point multiplications, we investigate an approach to training that
eliminates the need for most of these. Our method consists of two parts: First
we stochastically binarize weights to convert multiplications involved in
computing hidden states to sign changes. Second, while back-propagating error
derivatives, in addition to binarizing the weights, we quantize the
representations at each layer to convert the remaining multiplications into
binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10,
SVHN) show that this approach not only does not hurt classification performance
but can result in even better performance than standard stochastic gradient
descent training, paving the way to fast, hardware-friendly training of neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03013</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03013</id><created>2015-10-11</created><authors><author><keyname>Mahata</keyname><forenames>Kaushik</forenames></author><author><keyname>Schoukens</keyname><forenames>Johan</forenames></author></authors><title>Gaussian information matrix for Wiener model identification</title><categories>cs.SY</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a closed form expression for the information matrix associated
with the Wiener model identification problem under the assumption that the
input signal is a stationary Gaussian process. This expression holds under
quite generic assumptions. We allow the linear sub-system to have a rational
transfer function of arbitrary order, and the static nonlinearity to be a
polynomial of arbitrary degree. We also present a simple expression for the
determinant of the information matrix. The expressions presented herein has
been used for optimal experiment design for Wiener model identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03019</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03019</id><created>2015-10-11</created><updated>2015-11-21</updated><authors><author><keyname>Yang</keyname><forenames>Tong</forenames></author><author><keyname>Liu</keyname><forenames>Alex X.</forenames></author><author><keyname>Shahzad</keyname><forenames>Muhammad</forenames></author><author><keyname>Zhong</keyname><forenames>Yuankun</forenames></author><author><keyname>Fu</keyname><forenames>Qiaobin</forenames></author><author><keyname>Li</keyname><forenames>Zi</forenames></author><author><keyname>Xie</keyname><forenames>Gaogang</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author></authors><title>A Shifting Bloom Filter Framework for Set Queries</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Set queries are fundamental operations in computer systems and
applications.This paper addresses the fundamental problem of designing a
probabilistic data structure that can quickly process set queries using a small
amount of memory. We propose a Shifting Bloom Filter (ShBF) framework for
representing and querying sets. We demonstrate the effectiveness of ShBF using
three types of popular set queries: membership, association, and multiplicity
queries. The key novelty of ShBF is on encoding the auxiliary information of a
set element in a location offset. In contrast, prior BF based set data
structures allocate additional memory to store auxiliary information. To
evaluate ShBF in comparison with prior art, we conducted experiments using
real-world network traces. Results show that ShBF significantly advances the
state-of-the-art on all three types of set queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03021</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03021</id><created>2015-10-11</created><authors><author><keyname>Liu</keyname><forenames>Chao-Lin</forenames></author><author><keyname>Jin</keyname><forenames>Guan-Tao</forenames></author><author><keyname>Wang</keyname><forenames>Hongsu</forenames></author><author><keyname>Liu</keyname><forenames>Qing-Feng</forenames></author><author><keyname>Cheng</keyname><forenames>Wen-Huei</forenames></author><author><keyname>Chiu</keyname><forenames>Wei-Yun</forenames></author><author><keyname>Tsai</keyname><forenames>Richard Tzong-Han</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Chun</forenames></author></authors><title>Textual Analysis for Studying Chinese Historical Documents and Literary
  Novels</title><categories>cs.CL cs.DL</categories><comments>11 pages, 7 figures, 2 tables, The Fourth ASE International
  Conference on Social Informatics</comments><doi>10.1145/2818869.2818912</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyzed historical and literary documents in Chinese to gain insights
into research issues, and overview our studies which utilized four different
sources of text materials in this paper. We investigated the history of
concepts and transliterated words in China with the Database for the Study of
Modern China Thought and Literature, which contains historical documents about
China between 1830 and 1930. We also attempted to disambiguate names that were
shared by multiple government officers who served between 618 and 1912 and were
recorded in Chinese local gazetteers. To showcase the potentials and challenges
of computer-assisted analysis of Chinese literatures, we explored some
interesting yet non-trivial questions about two of the Four Great Classical
Novels of China: (1) Which monsters attempted to consume the Buddhist monk
Xuanzang in the Journey to the West (JTTW), which was published in the 16th
century, (2) Which was the most powerful monster in JTTW, and (3) Which major
role smiled the most in the Dream of the Red Chamber, which was published in
the 18th century. Similar approaches can be applied to the analysis and study
of modern documents, such as the newspaper articles published about the 228
incident that occurred in 1947 in Taiwan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03023</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03023</id><created>2015-10-11</created><authors><author><keyname>Zhao</keyname><forenames>Haisen</forenames></author><author><keyname>Lu</keyname><forenames>Lin</forenames></author><author><keyname>Wei</keyname><forenames>Yuan</forenames></author><author><keyname>Lischinski</keyname><forenames>Dani</forenames></author><author><keyname>Sharf</keyname><forenames>Andrei</forenames></author><author><keyname>Cohen-Or</keyname><forenames>Daniel</forenames></author><author><keyname>Chen</keyname><forenames>Baoquan</forenames></author></authors><title>Printed Perforated Lampshades for Continuous Projective Images</title><categories>cs.GR</categories><comments>10 pages</comments><acm-class>I.3.3; I.3.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for designing 3D-printed perforated lampshades, which
project continuous grayscale images onto the surrounding walls. Given the
geometry of the lampshade and a target grayscale image, our method computes a
distribution of tiny holes over the shell, such that the combined footprints of
the light emanating through the holes form the target image on a nearby diffuse
surface. Our objective is to approximate the continuous tones and the spatial
detail of the target image, to the extent possible within the constraints of
the fabrication process.
  To ensure structural integrity, there are lower bounds on the thickness of
the shell, the radii of the holes, and the minimal distances between adjacent
holes. Thus, the holes are realized as thin tubes distributed over the
lampshade surface. The amount of light passing through a single tube may be
controlled by the tube's radius and by its direction (tilt angle). The core of
our technique thus consists of determining a suitable configuration of the
tubes: their distribution across the relevant portion of the lampshade, as well
as the parameters (radius, tilt angle) of each tube. This is achieved by
computing a capacity-constrained Voronoi tessellation over a suitably defined
density function, and embedding a tube inside the maximal inscribed circle of
each tessellation cell. The density function for a particular target image is
derived from a series of simulated images, each corresponding to a different
uniform density tube pattern on the lampshade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03025</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03025</id><created>2015-10-11</created><authors><author><keyname>Prakash</keyname><forenames>Abhay</forenames></author></authors><title>Mining Interesting Trivia for Entities from Wikipedia</title><categories>cs.IR</categories><comments>Part of the work presented in this dissertation has been published at
  IJCAI 2015 as following: &quot;Abhay Prakash, Manoj K. Chinnakotla, Dhaval Patel
  and Puneet Garg. Did you know?- Mining Interesting Trivia for Entities from
  Wikipedia, In 24th International Joint Conference on Artificial Intelligence
  (IJCAI, 2015).&quot; (Paper Link:
  http://ijcai.org/papers15/Papers/IJCAI15-446.pdf)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trivia is any fact about an entity, which is interesting due to any of the
following characteristics - unusualness, uniqueness, unexpectedness or
weirdness. Such interesting facts are provided in 'Did You Know?' section at
many places. Although trivia are facts of little importance to be known, but we
have presented their usage in user engagement purpose. Such fun facts generally
spark intrigue and draws user to engage more with the entity, thereby promoting
repeated engagement. The thesis has cited some case studies, which show the
significant impact of using trivia for increasing user engagement or for wide
publicity of the product/service.
  In this thesis, we propose a novel approach for mining entity trivia from
their Wikipedia pages. Given an entity, our system extracts relevant sentences
from its Wikipedia page and produces a list of sentences ranked based on their
interestingness as trivia. At the heart of our system lies an interestingness
ranker which learns the notion of interestingness, through a rich set of
domain-independent linguistic and entity based features. Our ranking model is
trained by leveraging existing user-generated trivia data available on the Web
instead of creating new labeled data for movie domain. For other domains like
sports, celebrities, countries etc. labeled data would have to be created as
described in thesis. We evaluated our system on movies domain and celebrity
domain, and observed that the system performs significantly better than the
defined baselines. A thorough qualitative analysis of the results revealed that
our engineered rich set of features indeed help in surfacing interesting trivia
in the top ranks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03027</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03027</id><created>2015-10-11</created><authors><author><keyname>Ferrante</keyname><forenames>Guido Carlo</forenames></author><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>Group-blind detection with very large antenna arrays in the presence of
  pilot contamination</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is, in general, severely affected by pilot contamination. As
opposed to traditional detectors, we propose a group-blind detector that takes
into account the presence of pilot contamination. While sticking to the
traditional structure of the training phase, where orthogonal pilot sequences
are reused, we use the excess antennas at each base station to partially remove
interference during the uplink data transmission phase. We analytically derive
the asymptotic SINR achievable with group-blind detection, and confirm our
findings by simulations. We show, in particular, that in an
interference-limited scenario with one dominant interfering cell, the SINR can
be doubled compared to non-group-blind detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03035</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03035</id><created>2015-10-11</created><authors><author><keyname>Tirronen</keyname><forenames>Maria</forenames></author></authors><title>Reliability Analysis of Processes with Moving Cracked Material</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reliability of processes with moving elastic and isotropic material
containing initial cracks is considered in terms of fracture. The material is
modelled as a moving plate which is simply supported from two of its sides and
subjected to homogeneous tension acting in the travelling direction. For
tension, two models are studied: i) tension is constant with respect to time,
and ii) tension varies temporally according to an Ornstein-Uhlenbeck process.
Cracks of random length are assumed to occur in the material according to a
stochastic counting process. For a general counting process, a representation
of the nonfracture probability of the system is obtained that exploits
conditional Monte Carlo simulation. Explicit formulae are derived for special
cases. To study the reliability of the system with temporally varying tension,
a known explicit result for the first passage time of an Ornstein-Uhlenbeck
process to a constant boundary is utilized. Numerical examples are provided for
printing presses and paper material.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03041</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03041</id><created>2015-10-11</created><authors><author><keyname>Chatzidimitriou</keyname><forenames>Dimitris</forenames></author><author><keyname>Raymond</keyname><forenames>Jean-Florent</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Minors in graphs of large ${\theta}_r$-girth</title><categories>math.CO cs.DS</categories><comments>Some of the results of this paper have been presented in WAOA 2015</comments><msc-class>05C35, 05C83, 05C85, 68R10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every $r \in \N$, let $\theta_r$ denote the graph with two vertices and
$r$ parallel edges. The \emph{$\theta_r$-girth} of a graph $G$ is the minimum
number of edges of a subgraph of $G$ that can be contracted to $\theta_r$. This
notion generalizes the usual concept of girth which corresponds to the
case~$r=2$. In [Minors in graphs of large girth, {\em Random Structures \&amp;
Algorithms}, 22(2):213--225, 2003], K\&quot;uhn and Osthus showed that graphs of
sufficiently large minimum degree contain clique-minors whose order is an
exponential function of their girth. We extend this result for the case of
$\theta_{r}$-girth and we show that the minimum degree can be replaced by some
connectivity measurement. As an application of our results, we prove that, for
every fixed $r$, graphs excluding as a minor the disjoint union of $k$
$\theta_{r}$'s have treewidth $O(k\cdot \log k)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03042</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03042</id><created>2015-10-11</created><authors><author><keyname>Le</keyname><forenames>Thuc Duy</forenames></author><author><keyname>Hoang</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Jiuyong</forenames></author><author><keyname>Liu</keyname><forenames>Lin</forenames></author><author><keyname>Hu</keyname><forenames>Shu</forenames></author></authors><title>ParallelPC: an R package for efficient constraint based causal
  exploration</title><categories>cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering causal relationships from data is the ultimate goal of many
research areas. Constraint based causal exploration algorithms, such as PC,
FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and
have many applications. A common problem with these methods is the high
computational complexity, which hinders their applications in real world high
dimensional datasets, e.g gene expression datasets. In this paper, we present
an R package, ParallelPC, that includes the parallelised versions of these
causal exploration algorithms. The parallelised algorithms help speed up the
procedure of experimenting big datasets and reduce the memory used when running
the algorithms. The package is not only suitable for super-computers or
clusters, but also convenient for researchers using personal computers with
multi core CPUs. Our experiment results on real world datasets show that using
the parallelised algorithms it is now practical to explore causal relationships
in high dimensional datasets with thousands of variables in a single multicore
computer. ParallelPC is available in CRAN repository at
https://cran.rproject.org/web/packages/ParallelPC/index.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03045</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03045</id><created>2015-10-11</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Kravchenko</keyname><forenames>Dmitry</forenames></author><author><keyname>Rai</keyname><forenames>Ashutosh</forenames></author></authors><title>Optimal Classical Random Access Codes Using Single d-level Systems</title><categories>quant-ph cs.CC cs.IT math.IT</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, in the letter [Phys. Rev. Lett. {\bf 114}, 170502 (2015)], Tavakoli
et al. derived interesting results by studying classical and quantum random
access codes (RACs) in which the parties communicate higher-dimensional
systems. They construct quantum RACs with a bigger advantage over classical
RACs compared to previously considered RACs with binary alphabet. However,
these results crucially hinge upon an unproven assertion that the classical
strategy &quot;majority-encoding-identity-decoding&quot; leads to the maximum average
success probability achievable for classical RACs; in this article we provide a
proof of this intuition. We characterize all optimal classical RACs and show
that indeed &quot;majority-encoding-identity-decoding&quot; is one among the several
optimal strategies. Along with strengthening the results in Tavakoli et al.,
our result provides a firm basis for future research on this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03050</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03050</id><created>2015-10-11</created><authors><author><keyname>Efthymiopoulos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Christakidis</keyname><forenames>Athanasios</forenames></author><author><keyname>Efthymiopoulou</keyname><forenames>Maria</forenames></author><author><keyname>Corazza</keyname><forenames>Loris</forenames></author><author><keyname>Denazis</keyname><forenames>Spyros</forenames></author></authors><title>Congestion Control for P2P Live Streaming</title><categories>cs.NI cs.MM</categories><comments>21 pages</comments><doi>10.5121/ijp2p.2015.6201</doi><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In recent years, research efforts tried to exploit peer-to-peer (P2P) systems
in order to provide Live Streaming (LS) and Video-on-Demand (VoD) services.
Most of these research efforts focus on the development of distributed P2P
block schedulers for content exchange among the participating peers and on the
characteristics of the overlay graph (P2P overlay) that interconnects the set
of these peers. Currently, researchers try to combine peer-to-peer systems with
cloud infrastructures. They developed monitoring and control architectures that
use resources from the cloud in order to enhance QoS and achieve an attractive
trade-off between stability and low cost operation. However, there is a lack of
research effort on the congestion control of these systems and the existing
congestion control architectures are not suitable for P2P live streaming
traffic (small sequential non persistent traffic towards multiple network
locations). This paper proposes a P2P live streaming traffic aware congestion
control protocol that: i) is capable to manage sequential traffic heading to
multiple network destinations , ii) efficiently exploits the available
bandwidth, iii) accurately measures the idle peer resources, iv) avoids network
congestion, and v) is friendly to traditional TCP generated traffic. The
proposed P2P congestion control has been implemented, tested and evaluated
through a series of real experiments powered across the BonFIRE infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03051</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03051</id><created>2015-10-11</created><authors><author><keyname>Priyadarshini</keyname><forenames>Gargi</forenames></author><author><keyname>Anand</keyname><forenames>Ashish</forenames></author></authors><title>Inferring disease correlation from healthcare data</title><categories>cs.IR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic Health Records maintained in health care settings are a potential
source of substantial clinical knowledge. The massive volume of data,
unstructured nature of records and obligatory requirement of domain
acquaintance together pose a challenge in knowledge extraction from it. The aim
of this study is to overcome this challenge with a methodical analysis,
abstraction and summarization of such data. This is an attempt to explain
clinical observations through bio-medical and genomic data. Discharge summaries
of obesity patients were processed to extract coherent patterns. This was
supported by Machine Learning and Natural Language Processing based
technologies and concept mapping tool along with biomedical, clinical and
genomic knowledge bases. Semantic relations between diseases were extracted and
filtered through Chi square test to remove spurious relations. The remaining
relations were validated against biomedical literature and gene interaction
networks. A collection of binary relations of diseases was derived from the
data. One set implied co-morbidity while the other set contained diseases which
are risk factors of others. Validation against bio-medical literature increased
the prospect of correlation between diseases. Gene interaction network revealed
that the diseases are related and their corresponding genes are in close
proximity. Conclusion: This study focuses on deducing meaningful relations
between diseases from discharge summaries. For analytical purpose, the scope
has been limited to a few common, well-researched diseases. It can be extended
to incorporate relatively unknown, complex diseases and discover new traits to
help in clinical assessments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03055</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03055</id><created>2015-10-11</created><updated>2016-01-07</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Galley</keyname><forenames>Michel</forenames></author><author><keyname>Brockett</keyname><forenames>Chris</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Dolan</keyname><forenames>Bill</forenames></author></authors><title>A Diversity-Promoting Objective Function for Neural Conversation Models</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence-to-sequence neural network models for generation of conversational
responses tend to generate safe, commonplace responses (e.g., \textit{I don't
know}) regardless of the input. We suggest that the traditional objective
function, i.e., the likelihood of output (responses) given input (messages) is
unsuited to response generation tasks. Instead we propose using Maximum Mutual
Information (MMI) as objective function in neural models. Experimental results
demonstrate that the proposed objective function produces more diverse,
interesting, and appropriate responses, yielding substantive gains in \bleu
scores on two conversational datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03057</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03057</id><created>2015-10-11</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author></authors><title>Towards non-threaded Concurrent Constraint Programming for implementing
  multimedia interaction systems</title><categories>cs.MM cs.LO</categories><comments>55 pages</comments><acm-class>D.1.6; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explain the implementation of event-driven real-time
interpreters for the Concurrent Constraint Programming (CCP) and
Non-deterministic Timed Concurrent Constraint (NTCC) for- malisms. The CCP
interpreter was tested with a program to find, concurrently, paths in a graph
and it will be used in the future to find musical sequences in the music
improvisation software Omax, developed by the French Acoustics/Music Research
Institute (IRCAM). In the other hand, the NTCC interpreter was tested with a
music improvisation system based on NTCC (CCFOMI), developed by the AVISPA
research group and IRCAM. Additionally, we present GECOL 2, a wrapper for the
Generic Constraints Development Environment (GECODE) to Common LISP, de-
veloped to port the interpreters to Common LISP in the future. We concluded
that using GECODE for the concurrency control avoids the need of having threads
and synchronizing them, leading to a simple and efficient implementation of CCP
and NTCC. We also noticed that the time units in NTCC interpreter do not
represent discrete time units, because when we simulate the NTCC specifications
in the interpreter, the time units have different durations. In the future, we
propose forcing the duration of each time unit to a fix time, that way we would
be able to reason about NTCC time units as we do with discrete time units.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03059</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03059</id><created>2015-10-11</created><updated>2015-11-09</updated><authors><author><keyname>Fontanari</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco A.</forenames></author></authors><title>Influence of network topology on cooperative problem-solving systems</title><categories>cs.SI nlin.AO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idea of a collective intelligence behind the complex natural structures
built by organisms suggests that the organization of social networks is
selected so as to optimize problem-solving competence at the group-level. Here
we study the influence of the social network topology on the performance of a
group of agents whose task is to locate the global maxima of NK fitness
landscapes. Agents cooperate by broadcasting messages informing on their
fitness and use this information to imitate the fittest agent in their
influence networks. In the case those messages convey accurate information on
the proximity of the solution (i.e., for smooth fitness landscapes) we find
that high connectivity as well as centralization boost the group performance.
For rugged landscapes, however, these characteristics are beneficial for small
groups only. For large groups, it is advantageous to slow down the information
transmission through the network to avoid local maximum traps. Long-range links
and modularity have marginal effects on the performance of the group, except
for a very narrow region of the model parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03060</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03060</id><created>2015-10-11</created><authors><author><keyname>Wang</keyname><forenames>Qiwen</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>End-to-End Error-Correcting Codes on Networks with Worst-Case Symbol
  Errors</title><categories>cs.IT math.IT</categories><comments>Submitted for publication. arXiv admin note: substantial text overlap
  with arXiv:1108.2393</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of coding for networks experiencing worst-case symbol errors is
considered. We argue that this is a reasonable model for highly dynamic
wireless network transmissions. We demonstrate that in this setup prior network
error-correcting schemes can be arbitrarily far from achieving the optimal
network throughput. A new transform metric for errors under the considered
model is proposed. Using this metric, we replicate many of the classical
results from coding theory. Specifically, we prove new Hamming-type,
Plotkin-type, and Elias-Bassalygo-type upper bounds on the network capacity. A
commensurate lower bound is shown based on Gilbert-Varshamov-type codes for
error-correction. The GV codes used to attain the lower bound can be
non-coherent, that is, they do not require prior knowledge of the network
topology. We also propose a computationally-efficient concatenation scheme. The
rate achieved by our concatenated codes is characterized by a Zyablov-type
lower bound. We provide a generalized minimum-distance decoding algorithm which
decodes up to half the minimum distance of the concatenated codes. The
end-to-end nature of our design enables our codes to be overlaid on the
classical distributed random linear network codes [1]. Furthermore, the
potentially intensive computation at internal nodes for the link-by-link
error-correction is un-necessary based on our design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03062</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03062</id><created>2015-10-11</created><authors><author><keyname>Yoon</keyname><forenames>Seung-Hyun</forenames></author><author><keyname>Jung</keyname><forenames>Ji-Woon</forenames></author><author><keyname>Kim</keyname><forenames>Su-Bong</forenames></author><author><keyname>Shin</keyname><forenames>Hyun-Chang</forenames></author><author><keyname>Lee</keyname><forenames>Jae-Hyang</forenames></author><author><keyname>Lee</keyname><forenames>Kyu-Yun</forenames></author><author><keyname>Shim</keyname><forenames>Hyo-Sun</forenames></author></authors><title>GPS Receiver with Enhanced User Positioning Time</title><categories>cs.SY</categories><comments>Proc. International Symposium on GPS/GNSS 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a Global Positioning System (GPS) Receiver that locates
user's position instantly. Recently, many mobile devices require location
information to add user position into their contents, and some applications
require quick positioning when the device is initially switched on. In order to
reduce the time to fix user's position, we propose the Instant-On GPS receiver
system which is implemented on an ARM based FPGA board, and operates under a
very low power mode. We've developed a repeated sleep mode by periods to
control the GPS receiver's main power in order to achieve reduced power
consumption. By using a high resolution Real Time Clock (RTC), we can estimate
frame sync timing without receiving the current frame sync preamble data from a
satellite when GPS turns back on. However, the navigation solution needs to be
calculated once in advance. The performance results of the proposed GPS
receiver in both real world and simulation environment are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03081</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03081</id><created>2015-10-11</created><updated>2015-11-21</updated><authors><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>Statistical Channel Model with Multi-Frequency and Arbitrary Antenna
  Beamwidth for Millimeter-Wave Outdoor Communications</title><categories>cs.IT math.IT</categories><comments>7 pages, 7 figures, accepted in 2015 IEEE Global Communications
  Conference, Exhibition &amp; Industry Forum (GLOBECOM) Workshop, Dec. 6-10, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a 3-dimensional millimeter-wave statistical channel
impulse response model from 28 GHz and 73 GHz ultrawideband propagation
measurements. An accurate 3GPP-like channel model that supports arbitrary
carrier frequency, RF bandwidth, and antenna beamwidth (for both
omnidirectional and arbitrary directional antennas), is provided. Time cluster
and spatial lobe model parameters are extracted from empirical distributions
from field measurements. A step-by-step modeling procedure for generating
channel coefficients is shown to agree with statistics from the field
measurements, thus confirming that the statistical channel model faithfully
recreates spatial and temporal channel impulse responses for use in
millimeter-wave 5G air interface designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03090</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03090</id><created>2015-10-11</created><authors><author><keyname>Toro</keyname><forenames>Mauricio</forenames></author><author><keyname>Desainte-Catherine</keyname><forenames>Myriam</forenames></author><author><keyname>Castet</keyname><forenames>Julien</forenames></author></authors><title>An Extension of Interactive Scores for Multimedia Scenarios with
  Temporal Relations for Micro and Macro Controls</title><categories>cs.MM cs.SD</categories><comments>extended version of article presented in the Sound and Music
  Computing Conference 2012</comments><acm-class>C.3; D.1.6; H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software to design multimedia scenarios is usually based either on a fixed
timeline or on cue lists, but both models are unrelated temporally. On the
contrary, the formalism of interactive scores can describe multimedia scenarios
with flexible and fixed temporal relations among the objects of the scenario,
but cannot express neither temporal relations for micro controls nor signal
processing. We extend interactive scores with such relations and with sound
processing. We show some applications and we describe how they can be
implemented in Pure Data. Our implementation has low average relative jitter
even under high cpu load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03102</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03102</id><created>2015-10-11</created><authors><author><keyname>Garc&#xed;a-Bertrand</keyname><forenames>R.</forenames></author><author><keyname>M&#xed;nguez</keyname><forenames>R.</forenames></author></authors><title>Multi-stage Robust Transmission Expansion Planning under Socioeconomic
  and Environmental Changes</title><categories>cs.CE</categories><comments>8 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1501.05480</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances on Transmission Network Expansion Planning (TNEP) demonstrate
that the use of robust optimization, in contrast with respect to stochastic
programming methods, make the expansion planning problem computationally
tractable for real systems. State-of-the-art robust methods treat the problem
as stationary during the study period, making all the required investments on
expansion of lines at the beginning of the project horizon. However, it is
well-known that socioeconomic and environmental constraints might change
considerably during the network lifetime, such as planned increments of
wind-power generation capacities, variations of environmental conditions due to
climate change or growing trends on consumption patterns, and these changes
might also affect the optimal capacity expansion planning. This paper drops the
classical stationarity assumption and extends an existing adaptive robust
transmission expansion planning formulation for non-stationary situations. The
solution of this problem provides information not only about what additional
lines must be installed but the construction timing during the study horizon as
well. Numerical results from an illustrative example and the IEEE 118-bus
system are presented and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03104</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03104</id><created>2015-10-11</created><updated>2016-02-25</updated><authors><author><keyname>D'Oliveira</keyname><forenames>Rafael G. L.</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author></authors><title>Channel Metrization</title><categories>cs.IT math.CO math.IT</categories><comments>17 pages, 3 figures, presented shorter version at WCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that, given a channel, determines if there is a
distance for it such that the maximum likelihood decoder coincides with the
minimum distance decoder.
  We also show that any metric, up to a decoding equivalence, can be
isometrically embedded into the hypercube with the Hamming metric, and thus, in
terms of decoding, the Hamming metric is universal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03125</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03125</id><created>2015-10-11</created><authors><author><keyname>Hu</keyname><forenames>Qichang</forenames></author><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Porikli</keyname><forenames>Fatih</forenames></author></authors><title>Fast detection of multiple objects in traffic scenes with a common
  detection framework</title><categories>cs.CV</categories><comments>Appearing in IEEE Transactions on Intelligent Transportation Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic scene perception (TSP) aims to real-time extract accurate on-road
environment information, which in- volves three phases: detection of objects of
interest, recognition of detected objects, and tracking of objects in motion.
Since recognition and tracking often rely on the results from detection, the
ability to detect objects of interest effectively plays a crucial role in TSP.
In this paper, we focus on three important classes of objects: traffic signs,
cars, and cyclists. We propose to detect all the three important objects in a
single learning based detection framework. The proposed framework consists of a
dense feature extractor and detectors of three important classes. Once the
dense features have been extracted, these features are shared with all
detectors. The advantage of using one common framework is that the detection
speed is much faster, since all dense features need only to be evaluated once
in the testing phase. In contrast, most previous works have designed specific
detectors using different features for each of these objects. To enhance the
feature robustness to noises and image deformations, we introduce spatially
pooled features as a part of aggregated channel features. In order to further
improve the generalization performance, we propose an object subcategorization
method as a means of capturing intra-class variation of objects. We
experimentally demonstrate the effectiveness and efficiency of the proposed
framework in three detection applications: traffic sign detection, car
detection, and cyclist detection. The proposed framework achieves the
competitive performance with state-of- the-art approaches on several benchmark
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03130</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03130</id><created>2015-10-11</created><authors><author><keyname>Daum&#xe9;</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Khuller</keyname><forenames>Samir</forenames></author><author><keyname>Purohit</keyname><forenames>Manish</forenames></author><author><keyname>Sanders</keyname><forenames>Gregory</forenames></author></authors><title>On Correcting Inputs: Inverse Optimization for Online Structured
  Prediction</title><categories>cs.LG</categories><comments>Conference version to appear in FSTTCS, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithm designers typically assume that the input data is correct, and then
proceed to find &quot;optimal&quot; or &quot;sub-optimal&quot; solutions using this input data.
However this assumption of correct data does not always hold in practice,
especially in the context of online learning systems where the objective is to
learn appropriate feature weights given some training samples. Such scenarios
necessitate the study of inverse optimization problems where one is given an
input instance as well as a desired output and the task is to adjust the input
data so that the given output is indeed optimal. Motivated by learning
structured prediction models, in this paper we consider inverse optimization
with a margin, i.e., we require the given output to be better than all other
feasible outputs by a desired margin. We consider such inverse optimization
problems for maximum weight matroid basis, matroid intersection, perfect
matchings, minimum cost maximum flows, and shortest paths and derive the first
known results for such problems with a non-zero margin. The effectiveness of
these algorithmic approaches to online learning for structured prediction is
also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03145</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03145</id><created>2015-10-12</created><authors><author><keyname>Dindokar</keyname><forenames>Ravikant</forenames></author><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author></authors><title>Elastic Resource Allocation for Distributed Graph Processing Platforms</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed graph platforms like Pregel have used vertex- centric programming
models to process the growing corpus of graph datasets using commodity
clusters. The irregular structure of graphs cause load imbalances across
machines operating on graph partitions, and this is exacerbated for
non-stationary graph algorithms such as traversals, where not all parts of the
graph are active at the same time. As a result, such graph platforms, even as
they scale, do not make efficient use of distributed resources. Clouds offer
elastic virtual machines that can be leveraged to improve the resource
utilization for such platforms and hence reduce the monetary cost for their
execution. In this paper, we propose strategies for elastic placement of graph
partitions on Cloud VMs for subgraphcentric programming model to reduce the
cost of execution compared to a static placement, even as we minimize the
increase in makespan. These strategies are innovative in modeling the graph
algorithms behavior. We validate our strategies for several graphs, using
runtime tra- ces for their distributed execution of a Breadth First Search
(BFS) algorithms on our subgraph-centric GoFFish graph platform. Our strategies
are able to reduce the cost of exe- cution by up to 42%, compared to a static
placement, while achieving a makespan that is within 29% of the optimal
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03146</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03146</id><created>2015-10-12</created><authors><author><keyname>Manral</keyname><forenames>Jai</forenames></author></authors><title>IoT enabled Insurance Ecosystem - Possibilities Challenges and Risks</title><categories>cs.CY</categories><comments>Internet of Things, Insurance, Smart Cars, M2M</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Thing (IoT) is looking over to overhaul the business processes of
many industries including insurance domain. The current line of business such
as Property and Casualty, Health, and Life Insurance can avail tremendous
benefits from the contextual and relevant data being generated from billions of
connected devices; Smartphone's, wearable gadget and other electronic smart
sensors. For P&amp;C insurer's the biggest challenges is not the rapidly changing
environment but tackling these challenges with strategies linked to the past.
  This paper presents the key opportunities, challenges, potential applications
and risks of IoT in Insurance domain. It crystallomancy on the efficient use of
data analytics from data generated in IoT ecosystem and highlights business
model changes that Insurance industries may face in near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03149</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03149</id><created>2015-10-12</created><updated>2015-10-20</updated><authors><author><keyname>Cheng</keyname><forenames>Peng</forenames></author><author><keyname>Lian</keyname><forenames>Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Han</keyname><forenames>Jinsong</forenames></author><author><keyname>Zhao</keyname><forenames>Jizhong</forenames></author></authors><title>Task Assignment on Multi-Skill Oriented Spatial Crowdsourcing</title><categories>cs.DB</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of mobile devices and crowdsourcing platforms, the
spatial crowdsourcing has attracted much attention from the database community.
Specifically, the spatial crowdsourcing refers to sending location-based
requests to workers, based on their current positions. In this paper, we
consider a spatial crowdsourcing scenario, in which each worker has a set of
qualified skills, whereas each spatial task (e.g., repairing a house,
decorating a room, and performing entertainment shows for a ceremony) is
time-constrained, under the budget constraint, and required a set of skills.
Under this scenario, we will study an important problem, namely multi-skill
spatial crowdsourcing (MS-SC), which finds an optimal worker-and-task
assignment strategy, such that skills between workers and tasks match with each
other, and workers' benefits are maximized under the budget constraint. We
prove that the MS-SC problem is NP-hard and intractable. Therefore, we propose
three effective heuristic approaches, including greedy, g-divide-and-conquer
and cost-model-based adaptive algorithms to get worker-and-task assignments.
Through extensive experiments, we demonstrate the efficiency and effectiveness
of our MS-SC processing approaches on both real and synthetic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03152</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03152</id><created>2015-10-12</created><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Treetopes and their Graphs</title><categories>cs.CG cs.DS math.CO</categories><comments>16 pages, 8 figures. To appear at 27th ACM-SIAM Symp. on Discrete
  Algorithms (SODA 2016)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define treetopes, a generalization of the three-dimensional roofless
polyhedra (Halin graphs) to arbitrary dimensions. Like roofless polyhedra,
treetopes have a designated base facet such that every face of dimension
greater than one intersects the base in more than one point. We prove an
equivalent characterization of the 4-treetopes using the concept of clustered
planarity from graph drawing, and we use this characterization to recognize the
graphs of 4-treetopes in polynomial time. This result provides one of the first
classes of 4-polytopes, other than pyramids and stacked polytopes, that can be
recognized efficiently from their graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03162</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03162</id><created>2015-10-12</created><authors><author><keyname>Guo</keyname><forenames>Jing</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Yanikomeroglu</keyname><forenames>Halim</forenames></author></authors><title>Device-to-Device Communication Underlaying a Finite Cellular Network
  Region</title><categories>cs.IT math.IT</categories><comments>submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Underlay device-to-device (D2D) communication can potentially improve the
spectrum efficiency of cellular networks. However, the coexistence of D2D and
cellular users causes severe inter-cell and intra-cell interference. In this
paper, we propose a scheme to enable D2D communication inside a finite cellular
network region which has the following features: (i) to remove the inter-cell
interference from nearby cells we adopt a fractional frequency reuse technique,
where the D2D users share the spectrum occupied by cell-edge users in an
underlay in-band fashion, and (ii) to ensure quality of service at the base
station (BS) we adopt a D2D mode selection scheme, where the potential D2D
users are controlled by the BS to operate in D2D mode based on the average
interference generated to the BS. Using stochastic geometry, we present a
tractable framework to study the outage probability experienced at the BS and a
typical D2D receiver, and the aggregate D2D link spectral efficiency. The
analysis shows that the outage probability at the D2D receiver varies for
different locations. In addition, the aggregate D2D link spectral efficiency
increases as the D2D node density increases, which is important since an
increasing level of D2D usage is expected in future networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03164</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03164</id><created>2015-10-12</created><updated>2015-11-24</updated><authors><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Karatzoglou</keyname><forenames>Alexandros</forenames></author></authors><title>Context-Aware Bandits</title><categories>cs.LG cs.AI stat.ML</categories><comments>9 pages, 5 figures. Submitted by SHUAI LI
  (https://sites.google.com/site/shuailidotsli). arXiv admin note: substantial
  text overlap with arXiv:1502.03473</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a simple and efficient Context-Aware Bandit (CAB)
algorithm. With CAB we attempt to craft a bandit algorithm that can capture
collaborative effects and that can be easily deployed in a real-world
recommendation system, where the multi-armed bandits have been shown to perform
well in particular with respect to the cold-start problem. CAB utilizes a
context-aware clustering technique augmenting exploration-exploitation
strategies. CAB dynamically clusters the users based on the content universe
under consideration. We provide a theoretical analysis in the standard
stochastic multi-armed bandits setting. We demonstrate the efficiency of our
approach on production and real-world datasets, showing the scalability and,
more importantly, the significantly increased prediction performance against
several existing state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03170</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03170</id><created>2015-10-12</created><authors><author><keyname>Segal-Halevi</keyname><forenames>Erel</forenames></author><author><keyname>Nitzan</keyname><forenames>Shmuel</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Aumann</keyname><forenames>Yonatan</forenames></author></authors><title>Fair and Square: Cake-Cutting in Two Dimensions</title><categories>cs.GT cs.CG</categories><comments>The first version was published at:
  https://ideas.repec.org/p/biu/wpaper/2014-01.html . The second version was
  publised at: http://arxiv.org/abs/1409.4511 . This is the third version</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We consider the problem of fairly dividing a two-dimensional heterogeneous
good, such as land or ad space in print and electronic media, among several
agents with different utilities. Classic cake-cutting procedures either
allocate each agent a collection of disconnected pieces, or assume that the
cake is a one-dimensional interval. In practice, however, the two-dimensional
shape of the allotted pieces may be of crucial importance. In particular, when
building a house or designing an advertisement, squares are more usable than
long and narrow rectangles. We thus introduce and study the problem of fair
two-dimensional division wherein the allotted pieces must be of some restricted
two-dimensional geometric shape(s). Adding this geometric constraint re-opens
most questions and challenges related to cake-cutting. Indeed, even the most
elementary fairness criterion - proportionality - can no longer be guaranteed.
In this paper we thus examine the level of proportionality that can be
guaranteed, providing both impossibility results (for proportionality that
cannot be guaranteed) and division procedures (for proportionality that can be
guaranteed). We consider cakes and pieces of various shapes, focusing primarily
on shapes with a balanced aspect ratio such as squares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03173</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03173</id><created>2015-10-12</created><authors><author><keyname>Abdulhamid</keyname><forenames>Shafii Muhammad</forenames></author><author><keyname>Latiff</keyname><forenames>Muhammad Shafie Abd</forenames></author><author><keyname>Idris</keyname><forenames>Ismaila</forenames></author></authors><title>Tasks Scheduling Technique Using League Championship Algorithm for
  Makespan Minimization in IaaS Cloud</title><categories>cs.DC</categories><comments>5 pages, 4 figures</comments><journal-ref>ARPN Journal of Engineering and Applied Sciences 9 (12), 2528 -
  2533 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Makespan minimization in tasks scheduling of infrastructure as a service
(IaaS) cloud is an NP-hard problem. A number of techniques had been used in the
past to optimize the makespan time of scheduled tasks in IaaS cloud, which is
propotional to the execution cost billed to customers. In this paper, we
proposed a League Championship Algorithm (LCA) based makespan time minimization
scheduling technique in IaaS cloud. The LCA is a sports-inspired population
based algorithmic framework for global optimization over a continuous search
space. Three other existing algorithms that is, First Come First Served (FCFS),
Last Job First (LJF) and Best Effort First (BEF) were used to evaluate the
performance of the proposed algorithm. All algorithms under consideration
assumed to be non-preemptive. The results obtained shows that, the LCA
scheduling technique perform moderately better than the other algorithms in
minimizing the makespan time of scheduled tasks in IaaS cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03174</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03174</id><created>2015-10-12</created><updated>2015-10-22</updated><authors><author><keyname>Chung</keyname><forenames>Ping Ngai</forenames><affiliation>LIX, GRACE</affiliation></author><author><keyname>Costello</keyname><forenames>Craig</forenames><affiliation>LIX, GRACE</affiliation></author><author><keyname>Smith</keyname><forenames>Benjamin</forenames><affiliation>LIX, GRACE</affiliation></author></authors><title>Fast, uniform, and compact scalar multiplication for elliptic curves and
  genus 2 Jacobians with applications to signature schemes</title><categories>math.NT cs.CR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a general framework for uniform, constant-time one-and
two-dimensional scalar multiplication algorithms for elliptic curves and
Jacobians of genus 2 curves that operate by projecting to the x-line or Kummer
surface, where we can exploit faster and more uniform pseudomultiplication,
before recovering the proper &quot;signed&quot; output back on the curve or Jacobian.
This extends the work of L{\'o}pez and Dahab, Okeya and Sakurai, and Brier and
Joye to genus 2, and also to two-dimensional scalar multiplication. Our results
show that many existing fast pseudomultiplication implementations (hitherto
limited to applications in Diffie--Hellman key exchange) can be wrapped with
simple and efficient pre-and post-computations to yield competitive full scalar
multiplication algorithms, ready for use in more general discrete
logarithm-based cryptosystems, including signature schemes. This is especially
interesting for genus 2, where Kummer surfaces can outperform comparable
elliptic curve systems. As an example, we construct an instance of the Schnorr
signature scheme driven by Kummer surface arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03179</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03179</id><created>2015-10-12</created><authors><author><keyname>Groza</keyname><forenames>Adrian</forenames></author></authors><title>Data structuring for the ontological modelling of wind energy systems</title><categories>cs.AI</categories><comments>th Int. Conf. on Modelling and Development of Intelligent Systems
  (MDIS2015), Sibiu, Romania, 28 Oct. - 1 Nov. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small wind projects encounter difficulties to be efficiently deployed, partly
because wrong way data and information are managed. Ontologies can overcome the
drawbacks of partially available, noisy, inconsistent, and heterogeneous data
sources, by providing a semantic middleware between low level data and more
general knowledge. In this paper, we engineer an ontology for the wind energy
domain using description logic as technical instrumentation. We aim to
integrate corpus of heterogeneous knowledge, both digital and human, in order
to help the interested user to speed-up the initialization of a small-scale
wind project. We exemplify one use case scenario of our ontology, that consists
of automatically checking whether a planned wind project is compliant or not
with the active regulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03189</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03189</id><created>2015-10-12</created><authors><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author></authors><title>Evaluating m-learning in Saudi Arabian higher education: a case study</title><categories>cs.CY</categories><comments>International Journal of Emerging Trends &amp; Technology in Computer
  Science (IJETTCS), Volume 4, Issue 5(1), September - October 2015 ISSN
  2278-6856</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, mobile devices have become increasingly a part of education for
those who study or teach at the university level and school levels. The support
of electronic learning (elearning) is essential to making mobile learning
(m-learning) successful. This paper presents a study that applies mlearning to
a course at Qassim University, where 100 students attended during the academic
year 2014, including summer courses. The study aims to demonstrate that
m-learning provides students with the ability to engage in reflective thinking,
to share information among peers and to facilitate the construction of social
knowledge. 90 student questionnaires were filled correctly, remaining 10 had
various anomalies and thus were not considered. The study also aims to
demonstrate that learning provides methods for education strategies to be
easily and rapidly applied. Such strategies include team work, time management,
etc. In order to judge the feasibility of applying m-learning widely, a
questionnaire was developed. The results indicate that m-learning helps make
the process of education more convenient than in the past. A major disadvantage
of applying m-learning is inadequate wireless network bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03191</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03191</id><created>2015-10-12</created><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>On the Capacity of the Two-Hop Half-Duplex Relay Channel</title><categories>cs.IT math.IT</categories><comments>Proc. of the IEEE Global Telecomm. Conf. (Globecom), San Diego, Dec.
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although extensively investigated, the capacity of the two-hop half-duplex
(HD) relay channel is not fully understood. In particular, a capacity
expression which can be evaluated straightforwardly is not available and an
explicit coding scheme which achieves the capacity is not known either. In this
paper, we derive a new expression for the capacity of the two-hop HD relay
channel based on a simplified converse. Compared to previous results, this
capacity expression can be easily evaluated. Moreover, we propose an explicit
coding scheme which achieves the capacity. To achieve the capacity, the relay
does not only send information to the destination by transmitting
information-carrying symbols but also with the zero symbols resulting from the
relay's silence during reception. As examples, we compute the capacities of the
two-hop HD relay channel for the cases when the source-relay and
relay-destination links are both binary-symmetric channels (BSCs) and additive
white Gaussian noise (AWGN) channels, respectively, and numerically compare the
capacities with the rates achieved by conventional relaying where the relay
receives and transmits in a codeword-by-codeword fashion and switches between
reception and transmission in a strictly alternating manner. Our numerical
results show that the capacities of the two-hop HD relay channel for BSC and
AWGN links are significantly larger than the rates achieved with conventional
relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03199</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03199</id><created>2015-10-12</created><authors><author><keyname>Mathieu</keyname><forenames>B&#xe9;reng&#xe8;re</forenames></author><author><keyname>Crouzil</keyname><forenames>Alain</forenames></author><author><keyname>Puel</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Interactive multiclass segmentation using superpixel classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper adresses the problem of interactive multiclass segmentation. We
propose a fast and efficient new interactive segmentation method called
Superpixel Classification-based Interactive Segmentation (SCIS). From a few
strokes drawn by a human user over an image, this method extracts relevant
semantic objects. To get a fast calculation and an accurate segmentation, SCIS
uses superpixel over-segmentation and support vector machine classification. In
this paper, we demonstrate that SCIS significantly outperfoms competing
algorithms by evaluating its performances on the reference benchmarks of
McGuinness and Santner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03203</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03203</id><created>2015-10-12</created><updated>2015-10-14</updated><authors><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author></authors><title>VB calibration to improve the interface between phone recognizer and
  i-vector extractor</title><categories>stat.ML cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EM training algorithm of the classical i-vector extractor is often
incorrectly described as a maximum-likelihood method. The i-vector model is
however intractable: the likelihood itself and the hidden-variable posteriors
needed for the EM algorithm cannot be computed in closed form. We show here
that the classical i-vector extractor recipe is actually a mean-field
variational Bayes (VB) recipe.
  This theoretical VB interpretation turns out to be of further use, because it
also offers an interpretation of the newer phonetic i-vector extractor recipe,
thereby unifying the two flavours of extractor.
  More importantly, the VB interpretation is also practically useful: it
suggests ways of modifying existing i-vector extractors to make them more
accurate. In particular, in existing methods, the approximate VB posterior for
the GMM states is fixed, while only the parameters of the generative model are
adapted. Here we explore the possibility of also mildly adjusting (calibrating)
those posteriors, so that they better fit the generative model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03206</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03206</id><created>2015-10-12</created><authors><author><keyname>Kuscu</keyname><forenames>Murat</forenames></author><author><keyname>Akan</keyname><forenames>Ozgur B.</forenames></author></authors><title>Modeling and Analysis of SiNW BioFET as Molecular Antenna for Bio-Cyber
  Interfaces towards the Internet of Bio-NanoThings</title><categories>cs.ET physics.ins-det</categories><comments>to appear in Proc. IEEE WF-IoT 2015, Milan, Italy, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seamless connection of molecular nanonetworks to macroscale cyber networks is
envisioned to enable the Internet of Bio-NanoThings, which promises for
cutting-edge applications, especially in the medical domain. The connection
requires the development of an interface between the biochemical domain of
molecular nanonetworks and the electrical domain of conventional
electromagnetic networks. To this aim, in this paper, we propose to exploit
field effect transistor based biosensors (bioFETs) to devise a molecular
antenna capable of transducing molecular messages into electrical signals. In
particular, focusing on the use of SiNW FET-based biosensors as molecular
antennas, we develop deterministic and noise models for the antenna operation
to provide a theoretical framework for the optimization of the device from
communication perspective. We numerically evaluate the performance of the
antenna in terms of the Signal-to-Noise Ratio (SNR) at the electrical output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03227</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03227</id><created>2015-10-12</created><authors><author><keyname>Potapov</keyname><forenames>Igor</forenames></author><author><keyname>Semukhin</keyname><forenames>Pavel</forenames></author></authors><title>Vector Reachability Problem in $\mathrm{SL}(2,\mathbb{Z})$</title><categories>cs.FL cs.CC</categories><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is showing the solution for two open problems about decidability
of the vector reachability problem in a finitely generated semigroup of
matrices from $\mathrm{SL}(2,\mathbb{Z})$ and the point to point reachability
(over rational numbers) for fractional linear transformations, where associated
matrices are form $\mathrm{SL}(2,\mathbb{Z})$. The approach of solving
reachability problems is based on analysis of reachability paths between points
following the translation of numerical reachability problems into computational
and combinatorial problems on words and formal languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03231</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03231</id><created>2015-10-12</created><authors><author><keyname>Potapov</keyname><forenames>Igor</forenames></author><author><keyname>Prianychnykova</keyname><forenames>Olena</forenames></author><author><keyname>Verlan</keyname><forenames>Sergey</forenames></author></authors><title>On insertion-deletion systems over relational words</title><categories>cs.FL</categories><comments>24 pages, 8 figures</comments><acm-class>F.4.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new notion of a relational word as a finite totally ordered
set of positions endowed with three binary relations that describe which
positions are labeled by equal data, by unequal data and those having an
undefined relation between their labels. We define the operations of insertion
and deletion on relational words generalizing corresponding operations on
strings. We prove that the transitive and reflexive closure of these operations
has a decidable membership problem for the case of short insertion-deletion
rules (of size two/three and three/two). At the same time, we show that in the
general case such systems can produce a coding of any recursively enumerable
language leading to undecidabilty of reachability questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03232</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03232</id><created>2015-10-12</created><authors><author><keyname>Caron</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Pham</keyname><forenames>Quang-Cuong</forenames></author><author><keyname>Nakamura</keyname><forenames>Yoshihiko</forenames></author></authors><title>ZMP support areas for multi-contact mobility under frictional
  constraints</title><categories>cs.RO</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for checking and enforcing multi-contact stability based
on the zero-moment point (ZMP). Our first contribution is to characterize and
provide a fast computation algorithm for ZMP support areas in arbitrary virtual
planes. Contrary to previous works, our support areas also account for
frictional constraints. We then select a ZMP plane above the center of mass of
the robot, which turns the usual (unstable) control law of a linear inverted
pendulum into that of a (marginally stable) linear pendulum (LP). We next
observe that pendular models involve a regulation of the dynamic momentum,
which in turn shrinks the support area of the ZMP. Our second contribution is a
new algorithm to compute ZMP support areas under both frictional and
dynamic-momentum constraints. We show that, under an LP control law, the
support areas thus constructed are a necessary and sufficient condition for
contact stability. Based on these developments, we implement a whole-body
controller and generate dynamically-stable motions where an HRP-4 humanoid
locomotes in challenging simulation environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03247</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03247</id><created>2015-10-12</created><updated>2015-10-13</updated><authors><author><keyname>Wu</keyname><forenames>Lucy Chenyun</forenames></author><author><keyname>Dou</keyname><forenames>Jason Xiaotian</forenames></author><author><keyname>Sleator</keyname><forenames>Danny</forenames></author><author><keyname>Frieze</keyname><forenames>Alan</forenames></author><author><keyname>Miller</keyname><forenames>David</forenames></author></authors><title>Impartial Redistricting: A Markov Chain Approach</title><categories>cs.CY</categories><comments>about authorship naming problem, will fix soon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The gerrymandering problem is a worldwide problem which sets great threat to
democracy and justice in district based elections. Thanks to partisan
redistricting commissions, district boundaries are often manipulated to benefit
incumbents. Since an independent commission is hard to come by, the possibility
of impartially generating districts with a computer is explored in this thesis.
We have developed an algorithm to randomly produce legal redistricting schemes
for Pennsylvania.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03250</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03250</id><created>2015-10-12</created><authors><author><keyname>Petrank</keyname><forenames>Yael</forenames></author><author><keyname>Smirin</keyname><forenames>Nahum</forenames></author><author><keyname>Tsadok</keyname><forenames>Yossi</forenames></author><author><keyname>Friedman</keyname><forenames>Zvi</forenames></author><author><keyname>Lysiansky</keyname><forenames>Peter</forenames></author><author><keyname>Adam</keyname><forenames>Dan</forenames></author></authors><title>Using Anatomical Markers for Left Ventricular Segmentation of Long Axis
  Ultrasound Images</title><categories>cs.CV</categories><comments>11 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Left ventricular segmentation is essential for measuring left ventricular
function indices. Segmentation of one or several images requires an initial
guess of the contour. It is hypothesized here that creating an initial guess by
first detecting anatomical markers, would lead to correct detection of the
endocardium. The first step of the algorithm presented here includes automatic
detection of the mitral valve. Next, the apex is detected in the same frame.
The valve is then tracked throughout the cardiac cycle. Contours passing from
the apex to each valve corner are then found using a dynamic programming
algorithm. The resulting contour is used as an input to an active contour
algorithm. The algorithm was tested on 21 long axis ultrasound clips and showed
good agreement with manually traced contours. Thus, this study demonstrates
that detection of anatomic markers leads to a reliable initial guess of the
left ventricle border.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03252</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03252</id><created>2015-10-12</created><authors><author><keyname>Assadi</keyname><forenames>Sepehr</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Tannen</keyname><forenames>Val</forenames></author></authors><title>Dynamic Sketching for Graph Optimization Problems with Applications to
  Cut-Preserving Sketches</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new model for sublinear algorithms called
\emph{dynamic sketching}. In this model, the underlying data is partitioned
into a large \emph{static} part and a small \emph{dynamic} part and the goal is
to compute a summary of the static part (i.e, a \emph{sketch}) such that given
any \emph{update} for the dynamic part, one can combine it with the sketch to
compute a given function. We say that a sketch is \emph{compact} if its size is
bounded by a polynomial function of the length of the dynamic data,
(essentially) independent of the size of the static part.
  A graph optimization problem $P$ in this model is defined as follows. The
input is a graph $G(V,E)$ and a set $T \subseteq V$ of $k$ terminals; the edges
between the terminals are the dynamic part and the other edges in $G$ are the
static part. The goal is to summarize the graph $G$ into a compact sketch (of
size poly$(k)$) such that given any set $Q$ of edges between the terminals, one
can answer the problem $P$ for the graph obtained by inserting all edges in $Q$
to $G$, using only the sketch.
  We study the fundamental problem of computing a maximum matching and prove
tight bounds on the sketch size. In particular, we show that there exists a
(compact) dynamic sketch of size $O(k^2)$ for the matching problem and any such
sketch has to be of size $\Omega(k^2)$. Our sketch for matchings can be further
used to derive compact dynamic sketches for other fundamental graph problems
involving cuts and connectivities. Interestingly, our sketch for matchings can
also be used to give an elementary construction of a \emph{cut-preserving
vertex sparsifier} with space $O(kC^2)$ for $k$-terminal graphs; here $C$ is
the total capacity of the edges incident on the terminals. Additionally, we
give an improved lower bound (in terms of $C$) of $\Omega(C/\log{C})$ on size
of cut-preserving vertex sparsifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03253</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03253</id><created>2015-10-12</created><authors><author><keyname>Rueckert</keyname><forenames>Elmar</forenames></author><author><keyname>Lioutikov</keyname><forenames>Rudolf</forenames></author><author><keyname>Calandra</keyname><forenames>Roberto</forenames></author><author><keyname>Schmidt</keyname><forenames>Marius</forenames></author><author><keyname>Beckerle</keyname><forenames>Philipp</forenames></author><author><keyname>Peters</keyname><forenames>Jan</forenames></author></authors><title>Low-cost Sensor Glove with Force Feedback for Learning from
  Demonstrations using Probabilistic Trajectory Representations</title><categories>cs.RO</categories><comments>3 pages, 3 figures. Workshop paper of the International Conference on
  Robotics and Automation (ICRA 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor gloves are popular input devices for a large variety of applications
including health monitoring, control of music instruments, learning sign
language, dexterous computer interfaces, and tele-operating robot hands. Many
commercial products as well as low-cost open source projects have been
developed. We discuss here how low-cost (approx. 250 EUROs) sensor gloves with
force feedback can be build, provide an open source software interface for
Matlab and present first results in learning object manipulation skills through
imitation learning on the humanoid robot iCub.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03271</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03271</id><created>2015-10-12</created><updated>2016-02-09</updated><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Choreographies, Computationally</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreographic Programming is a programming paradigm for building concurrent
software that is deadlock-free by construction, by disallowing mismatched I/O
operations in the language used to write programs (called choreographies).
Previous models for choreographic programming are either trivially Turing
complete, because they include arbitrary local computations at each process, or
trivially Turing incomplete, e.g., because termination is decidable.
  In this work, we explore the core expressivity of choreographies, by
introducing a minimal language (AC) with restricted local computation (zero,
successor, and equality). AC is Turing complete purely by virtue of the
communication structures that can be written in it. We show that a
Turing-complete fragment of AC can be correctly projected to an actor-like
process calculus (AP), thus identifying a process language that is both
deadlock-free and Turing-complete. By embedding AC into CC, a standard model
for choreographies based on sessions, we also characterise a Turing-complete
fragment of CC, showing that the local computation primitives found in previous
works do not add expressive power. As a corollary, we identify a fragment of
the session-based pi-calculus that is both deadlock-free and Turing complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03277</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03277</id><created>2015-10-08</created><authors><author><keyname>Li</keyname><forenames>Xiaoping</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author><author><keyname>Wang</keyname><forenames>Wenjie</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>A Robust Generalized Chinese Remainder Theorem for Two Integers</title><categories>cs.IT math.IT</categories><comments>36 pages</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  A generalized Chinese remainder theorem (CRT) for multiple integers from
residue sets has been studied recently, where the correspondence between the
remainders and the integers in each residue set modulo several moduli is not
known. A robust CRT has also been proposed lately for robustly reconstruct a
single integer from its erroneous remainders. In this paper, we consider the
reconstruction problem of two integers from their residue sets, where the
remainders are not only out of order but also may have errors. We prove that
two integers can be robustly reconstructed if their remainder errors are less
than $M/8$, where $M$ is the greatest common divisor (gcd) of all the moduli.
We also propose an efficient reconstruction algorithm. Finally, we present some
simulations to verify the efficiency of the proposed algorithm. The study is
motivated and has applications in the determination of multiple frequencies
from multiple undersampled waveforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03278</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03278</id><created>2015-10-12</created><authors><author><keyname>Clemente</keyname><forenames>Lorenzo</forenames></author><author><keyname>Parys</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Salvati</keyname><forenames>Sylvain</forenames></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames></author></authors><title>Ordered Tree-Pushdown Systems</title><categories>cs.FL</categories><comments>full technical report of FST-TCS'15 paper</comments><acm-class>D.1.1; D.2.4; F.1.1; F.3.1; F.4.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We define a new class of pushdown systems where the pushdown is a tree
instead of a word. We allow a limited form of lookahead on the pushdown
conforming to a certain ordering restriction, and we show that the resulting
class enjoys a decidable reachability problem. This follows from a preservation
of recognizability result for the backward reachability relation of such
systems. As an application, we show that our simple model can encode several
formalisms generalizing pushdown systems, such as ordered multi-pushdown
systems, annotated higher-order pushdown systems, the Krivine machine, and
ordered annotated multi-pushdown systems. In each case, our procedure yields
tight complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03283</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03283</id><created>2015-10-12</created><authors><author><keyname>He</keyname><forenames>Tong</forenames></author><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author><author><keyname>Yao</keyname><forenames>Jian</forenames></author></authors><title>Text-Attentional Convolutional Neural Networks for Scene Text Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent deep learning models have demonstrated strong capabilities for
classifying text and non-text components in natural images. They extract a
high-level feature computed globally from a whole image component (patch),
where the cluttered background information may dominate the true text features
in the deep representation, leading to less discriminative power and poorer
robustness. In this work, we present a new system for scene text detection by
proposing a novel Text-Attentional Convolutional Neural Networks (Text-CNN)
that particularly focus on extracting text-related regions and features from
the image components. We develop a new learning mechanism to train the Text-CNN
with multi-level and rich supervised information, including text region mask,
character label, and binary text/non-text information. The informative
supervision information enables the Text-CNN with a strong capability for
discriminating ambiguous texts, and also increases its robustness against
complicated background components. The training process is formulated as a
multi-task learning problem, where the low-level supervised information greatly
facilitates the main task of text/non-text classification. In addition, a
powerful low-level detector called the Contrast-Enhancement Maximally Stable
Extremal Regions (CE-MSERs) is developed, which extends the widely-used MSERs
by enhancing the intensity contrast between text patterns and background. This
allows it to detect highly challenging text patterns, resulting in a higher
recall. Our approach achieved promising results on the ICDAR 2011 and ICDAR
2013 datasets, with F-measure of 0.82 and 0.84, improving the state-of-the-art
results substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03299</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03299</id><created>2015-10-12</created><updated>2015-10-16</updated><authors><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Yu</keyname><forenames>Qian</forenames></author><author><keyname>Hou</keyname><forenames>Yuexian</forenames></author><author><keyname>Song</keyname><forenames>Dawei</forenames></author><author><keyname>Li</keyname><forenames>Jingfei</forenames></author><author><keyname>Hu</keyname><forenames>Bin</forenames></author></authors><title>Further Theoretical Study of Distribution Separation Method for
  Information Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a Distribution Separation Method (DSM) is proposed for relevant
feedback in information retrieval, which aims to approximate the true relevance
distribution by separating a seed irrelevance distribution from the mixture
one. While DSM achieved a promising empirical performance, theoretical analysis
of DSM is still need further study and comparison with other relative retrieval
model. In this article, we first generalize DSM's theoretical property, by
proving that its minimum correlation assumption is equivalent to the maximum
(original and symmetrized) KL-Divergence assumption. Second, we also
analytically show that the EM algorithm in a well-known Mixture Model is
essentially a distribution separation process and can be simplified using the
linear separation algorithm in DSM. Some empirical results are also presented
to support our theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03302</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03302</id><created>2015-10-12</created><authors><author><keyname>Damasio</keyname><forenames>Guilherme</forenames></author><author><keyname>Mierzejewski</keyname><forenames>Piotr</forenames></author><author><keyname>Szlichta</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Zuzarte</keyname><forenames>Calisto</forenames></author></authors><title>OptImatch: Semantic Web System with Knowledge Base for Query Performance
  Problem Determination</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database query performance problem determination is often performed by
analyzing query execution plans (QEPs) in addition to other performance data.
As the query workloads that organizations run, have become larger and more
complex, analyzing QEPs manually even by experts has become a very time
consuming. Most performance diagnostic tools help with identifying problematic
queries and most query tuning tools address a limited number of known problems
and recommendations. We present the OptImatch system that offers a way to (a)
look for varied user defined problem patterns in QEPs and (b) automatically get
recommendations from an expert provided and user customizable knowledge base.
Existing approaches do not provide the ability to perform workload analysis
with flexible user defined patterns, as they lack the ability to impose a
proper structure on QEPs. We introduce a novel semantic web system that allows
a relatively naive user to search for arbitrary patterns and to get
recommendations stored in a knowledge base either by experts or added by the
user tailored to the environment in which they operate. Our methodology
includes transforming a QEP into an RDF graph and transforming a GUI based
user-defined pattern into a SPARQL query through handlers. The SPARQL query is
matched against the abstracted RDF graph, and any matched portion of the
abstracted RDF graph is relayed back to the user. With the knowledge base, the
OptImatch system automatically scans and matches interesting stored patterns in
a statistical way as appropriate and returns the corresponding recommendations.
Although the knowledge base patterns and solution recommendations are not in
the context of the user supplied QEPs, the context is adapted automatically
through the handler tagging interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03311</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03311</id><created>2015-10-12</created><authors><author><keyname>Gurevich</keyname><forenames>Yuri</forenames></author><author><keyname>Hudis</keyname><forenames>Efim</forenames></author><author><keyname>Wing</keyname><forenames>Jeannette M.</forenames></author></authors><title>Inverse Privacy</title><categories>cs.CY</categories><comments>The article is submitted to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An item of your personal information is inversely private if some party has
access to it but you do not. We analyze the provenance of inversely private
information and its rise to dominance over other kinds of personal information.
In a nutshell, the inverse privacy problem is unjustified inaccessibility to
you of your inversely private information. We argue that the inverse privacy
problem has a market-based solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03317</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03317</id><created>2015-10-12</created><authors><author><keyname>Bessiere</keyname><forenames>Christian</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author><author><keyname>Guns</keyname><forenames>Tias</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Nanni</keyname><forenames>Mirco</forenames></author><author><keyname>Nijssen</keyname><forenames>Siegfried</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Barry</forenames></author><author><keyname>Paparrizou</keyname><forenames>Anastasia</forenames></author><author><keyname>Pedreschi</keyname><forenames>Dino</forenames></author><author><keyname>Simonis</keyname><forenames>Helmut</forenames></author></authors><title>The Inductive Constraint Programming Loop</title><categories>cs.AI cs.LG</categories><comments>17 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint programming is used for a variety of real-world optimisation
problems, such as planning, scheduling and resource allocation problems. At the
same time, one continuously gathers vast amounts of data about these problems.
Current constraint programming software does not exploit such data to update
schedules, resources and plans. We propose a new framework, that we call the
Inductive Constraint Programming loop. In this approach data is gathered and
analyzed systematically, in order to dynamically revise and adapt constraints
and optimization criteria. Inductive Constraint Programming aims at bridging
the gap between the areas of data mining and machine learning on the one hand,
and constraint programming on the other hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03319</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03319</id><created>2015-10-12</created><updated>2015-12-09</updated><authors><author><keyname>Montanari</keyname><forenames>Angelo</forenames><affiliation>University of Udine, Italy</affiliation></author><author><keyname>Puppis</keyname><forenames>Gabriele</forenames><affiliation>LaBRI - CNRS</affiliation></author><author><keyname>Sala</keyname><forenames>Pietro</forenames><affiliation>University of Verona, Italy</affiliation></author></authors><title>A decidable weakening of Compass Logic based on cone-shaped cardinal
  directions</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:7) 2015</journal-ref><doi>10.2168/LMCS-11(4:7)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a modal logic, called Cone Logic, whose formulas describe
properties of points in the plane and spatial relationships between them.
Points are labelled by proposition letters and spatial relations are induced by
the four cone-shaped cardinal directions. Cone Logic can be seen as a weakening
of Venema's Compass Logic. We prove that, unlike Compass Logic and other
projection-based spatial logics, its satisfiability problem is decidable
(precisely, PSPACE-complete). We also show that it is expressive enough to
capture meaningful interval temporal logics - in particular, the interval
temporal logic of Allen's relations &quot;Begins&quot;, &quot;During&quot;, and &quot;Later&quot;, and their
transposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03336</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03336</id><created>2015-10-12</created><updated>2015-11-17</updated><authors><author><keyname>Lavin</keyname><forenames>Alexander</forenames></author><author><keyname>Ahmad</keyname><forenames>Subutai</forenames></author></authors><title>Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly
  Benchmark</title><categories>cs.AI cs.LG</categories><comments>14th International Conference on Machine Learning and Applications
  (IEEE ICMLA), 2015. Fixed typo in equation and formatting</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Much of the world's data is streaming, time-series data, where anomalies give
significant information in critical situations; examples abound in domains such
as finance, IT, security, medical, and energy. Yet detecting anomalies in
streaming data is a difficult task, requiring detectors to process data in
real-time, not batches, and learn while simultaneously making predictions.
There are no benchmarks to adequately test and score the efficacy of real-time
anomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which
attempts to provide a controlled and repeatable environment of open-source
tools to test and measure anomaly detection algorithms on streaming data. The
perfect detector would detect all anomalies as soon as possible, trigger no
false alarms, work with real-world time-series data across a variety of
domains, and automatically adapt to changing statistics. Rewarding these
characteristics is formalized in NAB, using a scoring algorithm designed for
streaming data. NAB evaluates detectors on a benchmark dataset with labeled,
real-world time-series data. We present these components, and give results and
analyses for several open source, commercially-used algorithms. The goal for
NAB is to provide a standard, open source framework with which the research
community can compare and evaluate different algorithms for detecting anomalies
in streaming data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03339</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03339</id><created>2015-10-12</created><authors><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Saxena</keyname><forenames>Sanjeev</forenames></author></authors><title>A Still Simpler Way of Introducing the Interior-Point Method for Linear
  Programming</title><categories>cs.DS math.OC</categories><comments>Updates and replaces arXiv:1412.0652</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear programming is now included in algorithm undergraduate and
postgraduate courses for computer science majors. We show that it is possible
to teach interior-point methods directly to students with just minimal
knowledge of linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03346</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03346</id><created>2015-10-12</created><authors><author><keyname>Bonnefon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Shariff</keyname><forenames>Azim</forenames></author><author><keyname>Rahwan</keyname><forenames>Iyad</forenames></author></authors><title>Autonomous Vehicles Need Experimental Ethics: Are We Ready for
  Utilitarian Cars?</title><categories>cs.CY</categories><comments>Paper and supplementary appendix combined</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide adoption of self-driving, Autonomous Vehicles (AVs) promises to
dramatically reduce the number of traffic accidents. Some accidents, though,
will be inevitable, because some situations will require AVs to choose the
lesser of two evils. For example, running over a pedestrian on the road or a
passer-by on the side; or choosing whether to run over a group of pedestrians
or to sacrifice the passenger by driving into a wall. It is a formidable
challenge to define the algorithms that will guide AVs confronted with such
moral dilemmas. In particular, these moral algorithms will need to accomplish
three potentially incompatible objectives: being consistent, not causing public
outrage, and not discouraging buyers. We argue to achieve these objectives,
manufacturers and regulators will need psychologists to apply the methods of
experimental ethics to situations involving AVs and unavoidable harm. To
illustrate our claim, we report three surveys showing that laypersons are
relatively comfortable with utilitarian AVs, programmed to minimize the death
toll in case of unavoidable harm. We give special attention to whether an AV
should save lives by sacrificing its owner, and provide insights into (i) the
perceived morality of this self-sacrifice, (ii) the willingness to see this
self-sacrifice being legally enforced, (iii) the expectations that AVs will be
programmed to self-sacrifice, and (iv) the willingness to buy self-sacrificing
AVs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03349</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03349</id><created>2015-10-12</created><authors><author><keyname>Zheng</keyname><forenames>Wenjie</forenames></author></authors><title>Toward a Better Understanding of Leaderboard</title><categories>stat.ML cs.LG stat.AP</categories><comments>Research note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The leaderboard in machine learning competitions is a tool to show the
performance of various participants and to compare them. However, the
leaderboard quickly becomes no longer accurate, due to hack or overfitting.
This article gives two advices to avoid this. It also points out that the
Ladder leaderboard successfully prevents this with $\tilde{O}(\epsilon^{-3})$
samples in the validation set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03353</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03353</id><created>2015-10-12</created><authors><author><keyname>Kaushik</keyname><forenames>Ankit</forenames></author><author><keyname>Sharma</keyname><forenames>Shree Krishna</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Performance Analysis of Underlay Cognitive Radio Systems:
  Estimation-Throughput Tradeoff</title><categories>cs.IT math.IT</categories><comments>4 pages, 5 figures, submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we study the performance of cognitive Underlay Systems (USs)
that employ power control mechanism at the Secondary Transmitter (ST). Existing
baseline models considered for the performance analysis either assume the
knowledge of involved channels at the ST or retrieve this information by means
of a feedback channel, however, such situations hardly exist in practice.
Motivated by this fact, we propose a novel approach that incorporates the
estimation of the involved channels at the ST, in order to characterize the
performance of USs under realistic scenarios. Moreover, we apply an outage
constraint that captures the impact of imperfect channel knowledge,
particularly on the interference power received at the primary receiver.
Besides this, we employ a transmit power constraint at the ST to determine an
operating regime for the US. Finally, we analyze an interesting tradeoff
between the estimation time and the secondary throughput allowing an optimized
performance of the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03354</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03354</id><created>2015-10-12</created><authors><author><keyname>Ar&#xe1;oz</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Zoltan</keyname><forenames>Cristina</forenames></author></authors><title>Parallel Triangles Counting Using Pipelining</title><categories>cs.DC</categories><acm-class>D.1.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The generalized method to have a parallel solution to a computational
problem, is to find a way to use Divide &amp; Conquer paradigm in order to have
processors acting on its own data and therefore all can be scheduled in
parallel. MapReduce is an example of this approach: Input data is transformed
by the mappers, in order to feed the reducers that can run in parallel. In
general this schema gives efficient problem solutions, but it stops being true
when the replication factor grows. We present another program schema that is
useful for describing problem solutions, that can exploit dynamic pipeline
parallelism without having to deal with replica- tion factors. We present the
schema in an example: counting triangles in graphs, in particular when the
graph do not fit in memory. We describe the solution in NiMo, a graphical
programming language that implements the implicitly functional parallel data ow
model of computation. The solution obtained using NiMo, is architecture
agnostic and can be deployed in any parallel/distributed architecture adapting
dynamically the processor usage to input characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03362</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03362</id><created>2015-10-12</created><authors><author><keyname>Reineke</keyname><forenames>Jan</forenames></author><author><keyname>Salinger</keyname><forenames>Alejandro</forenames></author></authors><title>On the Smoothness of Paging Algorithms</title><categories>cs.DS</categories><comments>Full version of paper presented at WAOA 2015</comments><acm-class>C.3; F.1.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the smoothness of paging algorithms. How much can the number of page
faults increase due to a perturbation of the request sequence? We call a paging
algorithm smooth if the maximal increase in page faults is proportional to the
number of changes in the request sequence. We also introduce quantitative
smoothness notions that measure the smoothness of an algorithm. We derive lower
and upper bounds on the smoothness of deterministic and randomized
demand-paging and competitive algorithms. Among strongly-competitive
deterministic algorithms LRU matches the lower bound, while FIFO matches the
upper bound.
  Well-known randomized algorithms like Partition, Equitable, or Mark are shown
not to be smooth. We introduce two new randomized algorithms, called
Smoothed-LRU and LRU-Random. Smoothed- LRU allows to sacrifice competitiveness
for smoothness, where the trade-off is controlled by a parameter. LRU-Random is
at least as competitive as any deterministic algorithm while smoother.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03367</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03367</id><created>2015-10-12</created><authors><author><keyname>Huggins</keyname><forenames>Peter</forenames></author></authors><title>Layered Heaps Beating Standard and Fibonacci Heaps in Practice</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classic problem of designing heaps. Standard binary heaps run
faster in practice than Fibonacci heaps but have worse time guarantees. Here we
present a new type of heap, a layered heap, that runs faster in practice than
both standard binary and Fibonacci heaps, but has asymptotic insert times
better than that of binary heaps. Our heap is defined recursively and maximum
run time speed up occurs when a recursion depth of 1 is used, i.e. a heap of
heaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03370</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03370</id><created>2015-10-12</created><authors><author><keyname>Garrabrant</keyname><forenames>Scott</forenames></author><author><keyname>Bhaskar</keyname><forenames>Siddharth</forenames></author><author><keyname>Demski</keyname><forenames>Abram</forenames></author><author><keyname>Garrabrant</keyname><forenames>Joanna</forenames></author><author><keyname>Koleszarik</keyname><forenames>George</forenames></author><author><keyname>Lloyd</keyname><forenames>Evan</forenames></author></authors><title>Asymptotic Logical Uncertainty and The Benford Test</title><categories>cs.LG cs.AI</categories><report-no>2015--11</report-no><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm A which assigns probabilities to logical sentences. For
any simple infinite sequence of sentences whose truth-values appear
indistinguishable from a biased coin that outputs &quot;true&quot; with probability p, we
have that the sequence of probabilities that A assigns to these sentences
converges to p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03375</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03375</id><created>2015-10-12</created><authors><author><keyname>Ahmed</keyname><forenames>Irshad</forenames></author><author><keyname>Ahmed</keyname><forenames>Irfan</forenames></author><author><keyname>Shahzad</keyname><forenames>Waseem</forenames></author></authors><title>Scaling up for high dimensional and high speed data streams: HSDStream</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel high speed clustering scheme for high dimensional
data streams. Data stream clustering has gained importance in different
applications, for example, in network monitoring, intrusion detection, and
real-time sensing are few of those. High dimensional stream data is inherently
more complex when used for clustering because the evolving nature of the stream
data and high dimensionality make it non-trivial. In order to tackle this
problem, projected subspace within the high dimensions and limited window sized
data per unit of time are used for clustering purpose. We propose a High Speed
and Dimensions data stream clustering scheme (HSDStream) which employs
exponential moving averages to reduce the size of the memory and speed up the
processing of projected subspace data stream. The proposed algorithm has been
tested against HDDStream for cluster purity, memory usage, and the cluster
sensitivity. Experimental results have been obtained for corrected KDD
intrusion detection dataset. These results show that HSDStream outperforms the
HDDStream in all performance metrics, especially the memory usage and the
processing speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03380</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03380</id><created>2015-10-12</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Cyclic Communication and the Inseparability of MIMO Multi-way Relay
  Channels</title><categories>cs.IT math.IT</categories><comments>31 pages, 6 figures, to appear in IEEE Trans. on Info. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $K$-user MIMO multi-way relay channel (Y-channel) consisting of $K$ users
with $M$ antennas each and a common relay node with $N$ antennas is studied in
this paper. Each user wants to exchange messages with all the other users via
the relay. A transmission strategy is proposed for this channel. The proposed
strategy is based on two steps: channel diagonalization and cyclic
communication. The channel diagonalization is applied by using zero-forcing
beam-forming. After channel diagonalization, the channel is decomposed into
parallel sub-channels. Cyclic communication is then applied, where signal-space
alignment for network-coding is used over each sub-channel. The proposed
strategy achieves the optimal DoF region of the channel if $N\leq M$. To prove
this, a new degrees-of-freedom outer bound is derived. As a by-product, we
conclude that the MIMO Y-channel is not separable, i.e., independent coding on
separate sub-channels is not enough, and one has to code jointly over several
sub-channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03381</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03381</id><created>2015-10-12</created><authors><author><keyname>Brandt</keyname><forenames>Axel</forenames></author><author><keyname>Ferrara</keyname><forenames>Michael</forenames></author><author><keyname>Kumbhat</keyname><forenames>Mohit</forenames></author><author><keyname>Loeb</keyname><forenames>Sarah</forenames></author><author><keyname>Stolee</keyname><forenames>Derrick</forenames></author><author><keyname>Yancey</keyname><forenames>Matthew</forenames></author></authors><title>I,F-partitions of Sparse Graphs</title><categories>math.CO cs.DM</categories><comments>11 pages, 6 figures</comments><msc-class>05C10 (planar), 05C15 (coloring), 05C78 (labeling)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A star $k$-coloring is a proper $k$-coloring where the union of two color
classes induces a star forest. While every planar graph is 4-colorable, not
every planar graph is star 4-colorable. One method to produce a star 4-coloring
is to partition the vertex set into a 2-independent set and a forest; such a
partition is called an I,F-partition. We use a combination of potential
functions and discharging to prove that every graph with maximum average degree
less than $\frac{5}{2}$ has an I,F-partition, which is sharp and answers a
question of Cranston and West [A guide to the discharging method,
arXiv:1306.4434]. This result implies that planar graphs of girth at least 10
are star 4-colorable, improving upon previous results of Bu, Cranston,
Montassier, Raspaud, and Wang [Star coloring of sparse graphs, J. Graph Theory
62 (2009), 201-219].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03387</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03387</id><created>2015-10-12</created><updated>2015-10-22</updated><authors><author><keyname>Gabrielov</keyname><forenames>Andrei</forenames></author><author><keyname>Vorobjov</keyname><forenames>Nicolai</forenames></author></authors><title>Topological lower bounds for arithmetic networks</title><categories>cs.CC math.AG</categories><comments>18 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a complexity lower bound on deciding membership in a semialgebraic
set for arithmetic networks in terms of the sum of Betti numbers with respect
to &quot;ordinary&quot; (singular) homology. This result complements a similar lower
bound by Montana, Morais and Pardo for locally close semialgebraic sets in
terms of the sum of Borel-Moore Betti numbers. We also prove a lower bound in
terms of the sum of Betti numbers of the projection of a semialgebraic set to a
coordinate subspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03399</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03399</id><created>2015-10-12</created><updated>2015-11-22</updated><authors><author><keyname>Giannakopoulos</keyname><forenames>Yiannis</forenames></author><author><keyname>Koutsoupias</keyname><forenames>Elias</forenames></author></authors><title>Selling Two Goods Optimally</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide sufficient conditions for revenue maximization in a two-good
monopoly where the buyer's values for the items come from independent (but not
necessarily identical) distributions over bounded intervals. Under certain
distributional assumptions, we give exact, closed-form formulas for the prices
and allocation rule of the optimal selling mechanism. As a side result we give
the first example of an optimal mechanism in an i.i.d. setting over a support
of the form $[0,b]$ which is not deterministic. Since our framework is based on
duality techniques, we were also able to demonstrate how slightly relaxed
versions of it can still be used to design mechanisms that have very good
approximation ratios with respect to the optimal revenue, through a
&quot;convexification&quot; process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03409</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03409</id><created>2015-10-12</created><authors><author><keyname>Cur&#xe9;</keyname><forenames>Olivier</forenames></author><author><keyname>Naacke</keyname><forenames>Hubert</forenames></author><author><keyname>Randriamalala</keyname><forenames>Tendry</forenames></author><author><keyname>Amann</keyname><forenames>Bernd</forenames></author></authors><title>LiteMat: a scalable, cost-efficient inference encoding scheme for large
  RDF graphs</title><categories>cs.DB</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of linked data sources and the size of the linked open data graph
keep growing every day. As a consequence, semantic RDF services are more and
more confronted with various &quot;big data&quot; problems. Query processing in the
presence of inferences is one them. For instance, to complete the answer set of
SPARQL queries, RDF database systems evaluate semantic RDFS relationships
(subPropertyOf, subClassOf) through time-consuming query rewriting algorithms
or space-consuming data materialization solutions. To reduce the memory
footprint and ease the exchange of large datasets, these systems generally
apply a dictionary approach for compressing triple data sizes by replacing
resource identifiers (IRIs), blank nodes and literals with integer values. In
this article, we present a structured resource identification scheme using a
clever encoding of concepts and property hierarchies for efficiently evaluating
the main common RDFS entailment rules while minimizing triple materialization
and query rewriting. We will show how this encoding can be computed by a
scalable parallel algorithm and directly be implemented over the Apache Spark
framework. The efficiency of our encoding scheme is emphasized by an evaluation
conducted over both synthetic and real world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03419</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03419</id><created>2015-10-11</created><authors><author><keyname>Gogioso</keyname><forenames>Stefano</forenames></author></authors><title>Operational Mermin non-locality and All-vs-Nothing arguments</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextuality is a key resource in quantum information and the
device-independent security of quantum algorithms. In this work, we show that
the recently developed, operational Mermin non-locality arguments provide a
large, novel family of quantum realisable All-vs-Nothing models. In particular,
they result in a diverse wealth of quantum realisable models which are
maximally contextual (i.e. lie on the faces of the no-signalling polytope with
no local elements), and could be used as a resource for the security of a new
class of quantum secret sharing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03421</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03421</id><created>2015-10-12</created><updated>2016-03-01</updated><authors><author><keyname>Jungiewicz</keyname><forenames>Michal</forenames></author><author><keyname>&#x141;opuszy&#x144;ski</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Towards Meaningful Maps of Polish Case Law</title><categories>cs.CL</categories><doi>10.3233/978-1-61499-609-5-185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we analyze the utility of two dimensional document maps for
exploratory analysis of Polish case law. We start by comparing two methods of
generating such visualizations. First is based on linear principal component
analysis (PCA). Second makes use of the modern nonlinear t-Distributed
Stochastic Neighbor Embedding method (t-SNE). We apply both PCA and t-SNE to a
corpus of judgments from different courts in Poland. It emerges that t-SNE
provides better, more interpretable results than PCA. As a next test, we apply
t-SNE to randomly selected sample of common court judgments corresponding to
different keywords. We show that t-SNE, in this case, reveals hidden topical
structure of the documents related to keyword,,pension&quot;. In conclusion, we find
that the t-SNE method could be a promising tool to facilitate the exploitative
analysis of legal texts, e.g., by complementing search or browse functionality
in legal databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03466</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03466</id><created>2015-10-12</created><authors><author><keyname>Abbaszadeh</keyname><forenames>Masoud</forenames></author></authors><title>Adaptive Model Predictive Control of a Batch Solution Polymerization
  Process using Trajectory Linearization</title><categories>math.OC cs.SY math.DS</categories><comments>12 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1502.04266</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequential trajectory linearized adaptive model based predictive controller
is designed using the DMC algorithm to control the temperature of a batch MMA
polymerization process. Using the mechanistic model of the polymerization, a
parametric transfer function is derived to relate the reactor temperature to
the power of the heaters. Then, a multiple model predictive control approach is
taken in to track a desired temperature trajectory.The coefficients of the
multiple transfer functions are calculated along the selected temperature
trajectory by sequential linearization and the model is validated
experimentally. The controller performance is studied on a small scale batch
reactor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03482</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03482</id><created>2015-10-12</created><authors><author><keyname>Mathieson</keyname><forenames>Luke</forenames></author></authors><title>Graph Editing Problems with Extended Regularity Constraints</title><categories>cs.CC</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph editing problems offer an interesting perspective on sub- and
supergraph identification problems for a large variety of target properties.
They have also attracted significant attention in recent years, particularly in
the area of parameterized complexity as the problems have rich parameter
ecologies.
  In this paper we examine generalisations of the notion of editing a graph to
obtain a regular subgraph. In particular we extend the notion of regularity to
include two variants of edge-regularity along with the unifying constraint of
strong regularity. We present a number of results, with the central observation
that these problems retain the general complexity profile of their
regularity-based inspiration: when the number of edits $k$ and the maximum
degree $r$ are taken together as a combined parameter, the problems are
tractable (i.e. in \FPT{}), but are otherwise intractable.
  We also examine variants of the basic editing to obtain a regular subgraph
problem from the perspective of parameterizing by the treewidth of the input
graph. In this case the treewidth of the input graph essentially becomes a
limiting parameter on the natural $k+r$ parameterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03492</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03492</id><created>2015-10-12</created><authors><author><keyname>Clarke</keyname><forenames>P.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Interference Suppression in Multiuser Systems Based on Bidirectional
  Algorithms</title><categories>cs.IT math.IT</categories><comments>11 figures, Eurasip journal on wireless communications and
  networking, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents adaptive bidirectional minimum mean-square error
parameter estimation algorithms for fast-fading channels. The time correlation
between successive channel gains is exploited to improve the estimation and
tracking capabilities of adaptive algorithms and provide robustness against
time-varying channels. Bidirectional normalized least mean-square and conjugate
gradient algorithms are devised along with adaptive mixing parameters that
adjust to the time-varying channel correlation properties. An analysis of the
proposed algorithms is provided along with a discussion of their performance
advantages. Simulations for an application to interference suppression in
multiuser DS-CDMA systems show the advantages of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03495</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03495</id><created>2015-10-12</created><authors><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Langbort</keyname><forenames>Cedric</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Privacy Constrained Information Processing</title><categories>cs.IT cs.GT math.IT</categories><comments>will appear in CDC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies communication scenarios where the transmitter and the
receiver have different objectives due to privacy concerns, in the context of a
variation of the strategic information transfer (SIT) model of Sobel and
Crawford. We first formulate the problem as the minimization of a common
distortion by the transmitter and the receiver subject to a privacy constrained
transmitter. We show the equivalence of this formulation to a Stackelberg
equilibrium of the SIT problem. Assuming an entropy based privacy measure, a
quadratic distortion measure and jointly Gaussian variables, we characterize
the Stackelberg equilibrium. Next, we consider asymptotically optimal
compression at the transmitter which inherently provides some level of privacy,
and study equilibrium conditions. We finally analyze the impact of the presence
of an average power constrained Gaussian communication channel between the
transmitter and the receiver on the equilibrium conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03505</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03505</id><created>2015-10-12</created><authors><author><keyname>Qiao</keyname><forenames>Deli</forenames></author></authors><title>Effective Capacity of Buffer-Aided Full-Duplex Relay Systems with
  Selection Relaying</title><categories>cs.IT math.IT</categories><comments>submitted for publication. arXiv admin note: text overlap with
  arXiv:1411.4271</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the achievable rate of three-node relay systems with selection
relaying under statistical delay constraints, imposed on the limitations of the
maximum end-to-end delay violation probabilities, is investigated. It is
assumed that there are queues of infinite size at both the source and relay
node, and the source can select the relay or destination for data reception.
Given selection relaying policy, the effective bandwidth of the arrival
processes of the queue at the relay is derived. Then, the maximum constant
arrival rate can be identified as the maximum effective capacity as a function
of the statistical end-to-end queueing delay constraints, signal-to-noise
ratios (SNR) at the source and relay, the fading distributions of the links,
and the relay policy. Subsequently, a relay policy that incorporates the
statistical delay constraints is proposed. It is shown that the proposed relay
policy can achieve better performance than existing protocols. Moreover, it is
demonstrated that buffering relay model can still help improve the throughput
of relay systems in the presence of statistical delay constraints and
source-destination link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03507</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03507</id><created>2015-10-12</created><authors><author><keyname>Gliske</keyname><forenames>Stephen V.</forenames></author><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Stacey</keyname><forenames>William C.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>The intrinsic value of HFO features as a biomarker of epileptic activity</title><categories>q-bio.NC cs.LG stat.ML</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High frequency oscillations (HFOs) are a promising biomarker of epileptic
brain tissue and activity. HFOs additionally serve as a prototypical example of
challenges in the analysis of discrete events in high-temporal resolution,
intracranial EEG data. Two primary challenges are 1) dimensionality reduction,
and 2) assessing feasibility of classification. Dimensionality reduction
assumes that the data lie on a manifold with dimension less than that of the
feature space. However, previous HFO analyses have assumed a linear manifold,
global across time, space (i.e. recording electrode/channel), and individual
patients. Instead, we assess both a) whether linear methods are appropriate and
b) the consistency of the manifold across time, space, and patients. We also
estimate bounds on the Bayes classification error to quantify the distinction
between two classes of HFOs (those occurring during seizures and those
occurring due to other processes). This analysis provides the foundation for
future clinical use of HFO features and buides the analysis for other discrete
events, such as individual action potentials or multi-unit activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03510</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03510</id><created>2015-10-12</created><authors><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Chandrasetty</keyname><forenames>Vikram A.</forenames></author><author><keyname>Lance</keyname><forenames>Andrew M.</forenames></author></authors><title>Repeat-Accumulate Codes for Reconciliation in Continuous Variable
  Quantum Key Distribution</title><categories>cs.IT math.IT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the design of low-complexity error correction codes
for the verification step in continuous variable quantum key distribution
(CVQKD) systems. We design new coding schemes based on quasi-cyclic
repeat-accumulate codes which demonstrate good performances for CVQKD
reconciliation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03517</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03517</id><created>2015-10-12</created><authors><author><keyname>Feng</keyname><forenames>Qihong</forenames></author><author><keyname>Haynes</keyname><forenames>Ronald D.</forenames></author><author><keyname>Wang</keyname><forenames>Xiang</forenames></author></authors><title>A Multilevel Coordinate Search Algorithm for Well Placement, Control and
  Joint Optimization</title><categories>math.OC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining optimal well placements and controls are two important tasks in
oil field development. These problems are computationally expensive, nonconvex,
and contain multiple optima. The practical solution of these problems require
efficient and robust algorithms. In this paper, the multilevel coordinate
search (MCS) algorithm is applied for well placement and control optimization
problems. MCS is a derivative-free algorithm that combines global search and
local search. Both synthetic and real oil fields are considered, and the
performance of MCS is compared to the generalized pattern search (GPS), the
particle swarm optimization (PSO), and the covariance matrix adaptive evolution
strategy (CMA-ES) algorithms. Results show that the MCS algorithm is strongly
competitive, and outperforms for the joint optimization problem and with a
limited computational budget. The effect of parameter settings are compared for
the test examples. For the joint optimization problem we compare the
performance of the simultaneous and sequential procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03519</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03519</id><created>2015-10-12</created><updated>2016-02-06</updated><authors><author><keyname>Rajendran</keyname><forenames>Janarthanan</forenames></author><author><keyname>Khapra</keyname><forenames>Mitesh M.</forenames></author><author><keyname>Chandar</keyname><forenames>Sarath</forenames></author><author><keyname>Ravindran</keyname><forenames>Balaraman</forenames></author></authors><title>Bridge Correlational Neural Networks for Multilingual Multimodal
  Representation Learning</title><categories>cs.CL</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently there has been a lot of interest in learning common representations
for multiple views of data. These views could belong to different modalities or
languages. Typically, such common representations are learned using a parallel
corpus between the two views (say, 1M images and their English captions). In
this work, we address a real-world scenario where no direct parallel data is
available between two views of interest (say, V1 and V2) but parallel data is
available between each of these views and a pivot view (V3). We propose a model
for learning a common representation for V1, V2 and V3 using only the parallel
data available between V1V3 and V2V3. The proposed model is generic and even
works when there are n views of interest and only one pivot view which acts as
a bridge between them. There are two specific downstream applications that we
focus on (i) Transfer learning between languages L1,L2,...,Ln using a pivot
language L and (ii) cross modal access between images and a language L1 using a
pivot language L2. We evaluate our model using two datasets : (i) publicly
available multilingual TED corpus and (ii) a new multilingual multimodal
dataset created and released as a part of this work. On both these datasets,
our model outperforms state of the art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03528</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03528</id><created>2015-10-13</created><authors><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>$\ell_1$-regularized Neural Networks are Improperly Learnable in
  Polynomial Time</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the improper learning of multi-layer neural networks. Suppose that
the neural network to be learned has $k$ hidden layers and that the
$\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We
present a kernel-based method, such that with probability at least $1 -
\delta$, it learns a predictor whose generalization error is at most $\epsilon$
worse than that of the neural network. The sample complexity and the time
complexity of the presented method are polynomial in the input dimension and in
$(1/\epsilon,\log(1/\delta),F(k,L))$, where $F(k,L)$ is a function depending on
$(k,L)$ and on the activation function, independent of the number of neurons.
The algorithm applies to both sigmoid-like activation functions and ReLU-like
activation functions. It implies that any sufficiently sparse neural network is
learnable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03531</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03531</id><created>2015-10-13</created><updated>2015-12-27</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Jia</keyname><forenames>Limin</forenames><affiliation>Carnegie Mellon University</affiliation></author><author><keyname>Xu</keyname><forenames>Hao</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Luo</keyname><forenames>Cheng</forenames><affiliation>University of Pennsylvania</affiliation></author><author><keyname>Zhou</keyname><forenames>Wenchao</forenames><affiliation>Georgetown University</affiliation></author><author><keyname>Loo</keyname><forenames>Boon Thau</forenames><affiliation>University of Pennsylvania</affiliation></author></authors><title>A Program Logic for Verifying Secure Routing Protocols</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:19) 2015</journal-ref><doi>10.2168/LMCS-11(4:19)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet, as it stands today, is highly vulnerable to attacks. However,
little has been done to understand and verify the formal security guarantees of
proposed secure inter-domain routing protocols, such as Secure BGP (S-BGP). In
this paper, we develop a sound program logic for SANDLog-a declarative
specification language for secure routing protocols for verifying properties of
these protocols. We prove invariant properties of SANDLog programs that run in
an adversarial environment. As a step towards automated verification, we
implement a verification condition generator (VCGen) to automatically extract
proof obligations. VCGen is integrated into a compiler for SANDLog that can
generate executable protocol implementations; and thus, both verification and
empirical evaluation of secure routing protocols can be carried out in this
unified framework. To validate our framework, we encoded several proposed
secure routing mechanisms in SANDLog, verified variants of path authenticity
properties by manually discharging the generated verification conditions in
Coq, and generated executable code based on SANDLog specification and ran the
code in simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03533</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03533</id><created>2015-10-13</created><authors><author><keyname>Aly</keyname><forenames>Heba</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>semMatch: Road Semantics-based Accurate Map Matching for Challenging
  Positioning Data</title><categories>cs.CY</categories><comments>Accepted for publication as a full paper in the 23rd ACM SIGSPATIAL
  International Conference on Advances in Geographic Information Systems (ACM
  SIGSPATIAL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Map matching has been used to reduce the noisiness of the location estimates
by aligning them to the road network on a digital map. A growing number of
applications, e.g. energy-efficient localization and cellular provider side
localization, depend on the availability of only sparse and coarse-grained
positioning data; leading to a challenging map matching process.
  In this paper, we present semMatch: a system that can provide accurate
HMM-based map matching for challenging positioning traces. semMatch leverages
the smartphone's inertial sensors to detect different road semantics; such as
speed bumps, tunnels, and turns; and uses them in a mathe-matically-principled
way as hints to overcome the sparse, noisy, and coarse-grained input
positioning data, improving the HMM map matching accuracy and efficiency. To do
that, semMatch applies a series of preprocessing modules to handle the noisy
locations. The filtered location data is then processed by the core of semMatch
system using a novel incremental HMM algorithm that combines a
semantics-enriched digital map and the car's ambient road semantics in its
estimation process.
  We have evaluated semMatch using traces collected from different cities
covering more than 150km under different harsh scenarios including
coarse-grained cellular-based positioning data, sparse GPS traces with
extremely low sampling rate, and noisy traces with a large number of
back-and-force transitions. The results show that semMatch significantly
outperforms traditional map matching algorithms under all scenarios, with an
enhancement of at least 416% and 894% in precision and recall respectively in
the most difficult cases. This highlights its promise as a next generation map
matching algorithm for challenging environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03551</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03551</id><created>2015-10-13</created><authors><author><keyname>Mittal</keyname><forenames>Radhika</forenames></author><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Ratnasamy</keyname><forenames>Sylvia</forenames></author><author><keyname>Shenker</keyname><forenames>Scott</forenames></author></authors><title>Universal Packet Scheduling</title><categories>cs.NI</categories><comments>This is the same as our ACM HotNets 2015 paper, extended to include
  proofs for our theoretical results</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address a seemingly simple question: Is there a universal
packet scheduling algorithm? More precisely, we analyze (both theoretically and
empirically) whether there is a single packet scheduling algorithm that, at a
network-wide level, can match the results of any given scheduling algorithm. We
find that in general the answer is &quot;no&quot;. However, we show theoretically that
the classical Least Slack Time First (LSTF) scheduling algorithm comes closest
to being universal and demonstrate empirically that LSTF can closely, though
not perfectly, replay a wide range of scheduling algorithms in realistic
network settings. We then evaluate whether LSTF can be used {\em in practice}
to meet various network-wide objectives by looking at three popular performance
metrics (mean FCT, tail packet delays, and fairness); we find that LSTF
performs comparable to the state-of-the-art for each of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03560</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03560</id><created>2015-10-13</created><authors><author><keyname>Duchateau</keyname><forenames>Julien</forenames></author><author><keyname>Rousselle</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Maquignon</keyname><forenames>Nicolas</forenames></author><author><keyname>Roussel</keyname><forenames>Gilles</forenames></author><author><keyname>Renaud</keyname><forenames>Christophe</forenames></author></authors><title>A progressive mesh method for physical simulations using lattice
  Boltzmann method on single-node multi-gpu architectures</title><categories>cs.DC physics.comp-ph</categories><comments>15 pages, International Journal of Distributed and Parallel Systems
  (IJDPS) Vol.6, No.5, September 2015</comments><doi>10.5121/ijdps.2015.6501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new progressive mesh algorithm is introduced in order to
perform fast physical simulations by the use of a lattice Boltzmann method
(LBM) on a single-node multi-GPU architecture. This algorithm is able to mesh
automatically the simulation domain according to the propagation of fluids.
This method can also be useful in order to perform various types of simulations
on complex geometries. The use of this algorithm combined with the massive
parallelism of GPUs allows to obtain very good performance in comparison with
the static mesh method used in literature. Several simulations are shown in
order to evaluate the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03564</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03564</id><created>2015-10-13</created><authors><author><keyname>Barbero</keyname><forenames>Florian</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author><author><keyname>Sheng</keyname><forenames>Bin</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Linear-Vertex Kernel for the Problem of Packing $r$-Stars into a Graph
  without Long Induced Paths</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let integers $r\ge 2$ and $d\ge 3$ be fixed. Let ${\cal G}_d$ be the set of
graphs with no induced path on $d$ vertices. We study the problem of packing
$k$ vertex-disjoint copies of $K_{1,r}$ ($k\ge 2$) into a graph $G$ from
parameterized preprocessing, i.e., kernelization, point of view. We show that
every graph $G\in {\cal G}_d$ can be reduced, in polynomial time, to a graph
$G'\in {\cal G}_d$ with $O(k)$ vertices such that $G$ has at least $k$
vertex-disjoint copies of $K_{1,r}$ if and only if $G'$ has. Such a result is
known for arbitrary graphs $G$ when $r=2$ and we conjecture that it holds for
every $r\ge 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03565</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03565</id><created>2015-10-13</created><updated>2015-12-05</updated><authors><author><keyname>Fehenberger</keyname><forenames>Tobias</forenames></author><author><keyname>Lavery</keyname><forenames>Domani&#xe7;</forenames></author><author><keyname>Maher</keyname><forenames>Robert</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author><author><keyname>Hanik</keyname><forenames>Norbert</forenames></author></authors><title>Sensitivity Gains by Mismatched Probabilistic Shaping for Optical
  Communication Systems</title><categories>cs.IT math.IT</categories><comments>Title and introduction were updated and the discussion of Section
  IV-B was extended. Additionally, some minor modifications were made to the
  manuscript</comments><doi>10.1109/LPT.2015.2514078</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic shaping of quadrature amplitude modulation (QAM) is used to
enhance the sensitivity of an optical communication system. Sensitivity gains
of 0.43 dB and 0.8 dB are demonstrated in back-to-back experiments by shaping
of 16QAM and 64QAM, respectively. Further, numerical simulations are used to
prove the robustness of probabilistic shaping to a mismatch between the
constellation used and the signal-to-noise ratio (SNR) of the channel. It is
found that, accepting a 0.1 dB SNR penalty, only four shaping distributions are
required to support these gains for 64QAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03567</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03567</id><created>2015-10-13</created><updated>2015-12-01</updated><authors><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author></authors><title>On the line-symmetry of self-motions of linear pentapods</title><categories>cs.RO</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that all self-motions of pentapods with linear platform of Type 1 and
Type 2 can be generated by line-symmetric motions. Thus this paper closes a gap
between the more than 100 year old works of Duporcq and Borel and the extensive
study of line-symmetric motions done by Krames in the 1930's. As a consequence
we also get a new solution set for the Borel Bricard problem. Moreover we
discuss the reality of self-motions and give a sufficient condition for the
design of linear pentapods of Type 1 and Type 2, which have a self-motion free
workspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03575</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03575</id><created>2015-10-13</created><updated>2016-02-06</updated><authors><author><keyname>Haviv</keyname><forenames>Moshe</forenames></author><author><keyname>Ravner</keyname><forenames>Liron</forenames></author></authors><title>Strategic Bidding in an Accumulating Priority Queue: Equilibrium
  Analysis</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the strategic purchasing of priorities in a time-dependent
accumulating priority M/G/$1$ queue. We formulate a non-cooperative game in
which customers purchase priority coefficients with the goal of reducing
waiting costs in exchange. The priority of each customer in the queue is a
linear function of the individual waiting time, with the purchased coefficient
being the slope. The unique pure Nash equilibrium is solved explicitly for the
case with homogeneous customers. A general characterisation of the Nash
equilibrium is provided for the heterogeneous case. It is shown that both avoid
the crowd and follow the crowd behaviours are prevalent, within class types and
between them. We further present a pricing mechanism that ensures the order of
the accumulating priority rates in equilibrium follows a $C\mu$ type rule and
improves overall efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03588</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03588</id><created>2015-10-13</created><authors><author><keyname>Doumic</keyname><forenames>Marie</forenames><affiliation>MAMBA, LJLL</affiliation></author><author><keyname>Escobedo</keyname><forenames>Miguel</forenames></author></authors><title>Time Asymptotics for a Critical Case in Fragmentation and
  Growth-Fragmentation Equations</title><categories>math.AP cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fragmentation and growth-fragmentation equations is a family of problems with
varied and wide applications. This paper is devoted to description of the long
time time asymptotics of two critical cases of these equations, when the
division rate is constant and the growth rate is linear or zero. The study of
these cases may be reduced to the study of the following fragmentation
equation:$$\frac{\partial}{\partial t} u(t,x) + u(t,x)=\int\limits\_x^\infty
k\_0(\frac{x}{y}) u(t,y) dy.$$Using the Mellin transform of the equation, we
determine the long time behavior of the solutions. Our results show in
particular the strong dependence of this asymptotic behavior with respect to
the initial data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03591</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03591</id><created>2015-10-13</created><authors><author><keyname>Klenske</keyname><forenames>Edgar D.</forenames></author><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author></authors><title>Dual Control for Approximate Bayesian Reinforcement Learning</title><categories>stat.ML cs.SY math.OC</categories><comments>26 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Control of non-episodic, finite-horizon dynamical systems with uncertain
dynamics poses a tough and elementary case of the exploration-exploitation
trade-off. Bayesian reinforcement learning, reasoning about the effect of
actions and future observations, offers a principled solution, but is
intractable. We review, then extend an old approximate approach from control
theory---where the problem is known as dual control---in the context of modern
regression methods, specifically generalized linear regression. Experiments on
simulated systems show that this framework offers a useful approximation to the
intractable aspects of Bayesian RL, producing structured exploration strategies
that differ from standard RL approaches. We provide simple examples for the use
of this framework in (approximate) Gaussian process regression and feedforward
neural networks for the control of exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03592</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03592</id><created>2015-10-13</created><updated>2015-10-14</updated><authors><author><keyname>Carpin</keyname><forenames>Mattia</forenames></author><author><keyname>Rosati</keyname><forenames>Stefano</forenames></author><author><keyname>Khan</keyname><forenames>Mohammad Emtiyaz</forenames></author><author><keyname>Rimoldi</keyname><forenames>Bixio</forenames></author></authors><title>UAVs using Bayesian Optimization to Locate WiFi Devices</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of localizing non-collaborative WiFi devices in a
large region. Our main motive is to localize humans by localizing their WiFi
devices, e.g. during search-and-rescue operations after a natural disaster. We
use an active sensing approach that relies on Unmanned Aerial Vehicles (UAVs)
to collect signal-strength measurements at informative locations. The problem
is challenging since the measurement is received at arbitrary times and they
are received only when the UAV is in close proximity to the device. For these
reasons, it is extremely important to make prudent decision with very few
measurements. We use the Bayesian optimization approach based on Gaussian
process (GP) regression. This approach works well for our application since GPs
give reliable predictions with very few measurements while Bayesian
optimization makes a judicious trade-off between exploration and exploitation.
In field experiments conducted over a region of 1000 $\times$ 1000 $m^2$, we
show that our approach reduces the search area to less than 100 meters around
the WiFi device within 5 minutes only. Overall, our approach localizes the
device in less than 15 minutes with an error of less than 20 meters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03602</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03602</id><created>2015-10-13</created><authors><author><keyname>Srivastava</keyname><forenames>Brij Mohan Lal</forenames></author><author><keyname>Vydana</keyname><forenames>Hari Krishna</forenames></author><author><keyname>Vuppala</keyname><forenames>Anil Kumar</forenames></author><author><keyname>Shrivastava</keyname><forenames>Manish</forenames></author></authors><title>A language model based approach towards large scale and lightweight
  language identification systems</title><categories>cs.SD cs.CL</categories><comments>Under review at ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilingual spoken dialogue systems have gained prominence in the recent
past necessitating the requirement for a front-end Language Identification
(LID) system. Most of the existing LID systems rely on modeling the language
discriminative information from low-level acoustic features. Due to the
variabilities of speech (speaker and emotional variabilities, etc.),
large-scale LID systems developed using low-level acoustic features suffer from
a degradation in the performance. In this approach, we have attempted to model
the higher level language discriminative phonotactic information for developing
an LID system. In this paper, the input speech signal is tokenized to phone
sequences by using a language independent phone recognizer. The language
discriminative phonotactic information in the obtained phone sequences are
modeled using statistical and recurrent neural network based language modeling
approaches. As this approach, relies on higher level phonotactical information
it is more robust to variabilities of speech. Proposed approach is
computationally light weight, highly scalable and it can be used in complement
with the existing LID systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03608</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03608</id><created>2015-10-13</created><updated>2016-03-07</updated><authors><author><keyname>Tom&#xe8;</keyname><forenames>Denis</forenames></author><author><keyname>Monti</keyname><forenames>Federico</forenames></author><author><keyname>Baroffio</keyname><forenames>Luca</forenames></author><author><keyname>Bondi</keyname><forenames>Luca</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author><author><keyname>Tubaro</keyname><forenames>Stefano</forenames></author></authors><title>Deep convolutional neural networks for pedestrian detection</title><categories>cs.CV</categories><comments>submitted to Elsevier Signal Processing: Image Communication special
  Issue on Deep Learning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedestrian detection is a popular research topic due to its paramount
importance for a number of applications, especially in the fields of
automotive, surveillance and robotics. Despite the significant improvements,
pedestrian detection is still an open challenge that calls for more and more
accurate algorithms. In the last few years, deep learning and in particular
convolutional neural networks emerged as the state of the art in terms of
accuracy for a number of computer vision tasks such as image classification,
object detection and segmentation, often outperforming the previous gold
standards by a large margin. In this paper, we propose a pedestrian detection
system based on deep learning, adapting a general-purpose convolutional network
to the task at hand. By thoroughly analyzing and optimizing each step of the
detection pipeline we propose an architecture that outperforms traditional
methods, achieving a task accuracy close to that of state-of-the-art
approaches, while requiring a low computational time. Finally, we tested the
system on an NVIDIA Jetson TK1, a 192-core platform that is envisioned to be a
forerunner computational brain of future self-driving cars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03614</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03614</id><created>2015-10-13</created><authors><author><keyname>Eiben</keyname><forenames>Eduard</forenames></author><author><keyname>Ganian</keyname><forenames>Robert</forenames></author><author><keyname>Lauri</keyname><forenames>Juho</forenames></author></authors><title>On the Complexity of Rainbow Coloring Problems</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge-colored graph $G$ is said to be rainbow connected if between each
pair of vertices there exists a path which uses each color at most once. The
rainbow connection number, denoted by $rc(G)$, is the minimum number of colors
needed to make $G$ rainbow connected. Along with its variants, which consider
vertex colorings and/or so-called strong colorings, the rainbow connection
number has been studied from both the algorithmic and graph-theoretic points of
view.
  In this paper we present a range of new results on the computational
complexity of computing the four major variants of the rainbow connection
number. In particular, we prove that the \textsc{Strong Rainbow Vertex
Coloring} problem is $NP$-complete even on graphs of diameter $3$. We show that
when the number of colors is fixed, then all of the considered problems can be
solved in linear time on graphs of bounded treewidth. Moreover, we provide a
linear-time algorithm which decides whether it is possible to obtain a rainbow
coloring by saving a fixed number of colors from a trivial upper bound.
Finally, we give a linear-time algorithm for computing the exact rainbow
connection numbers for three variants of the problem on graphs of bounded
vertex cover number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03623</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03623</id><created>2015-10-13</created><updated>2015-10-21</updated><authors><author><keyname>Zhang</keyname><forenames>Sai</forenames></author></authors><title>Elastic regularization in restricted Boltzmann machines: Dealing with
  $p\gg N$</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to a critical error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Restricted Boltzmann machines (RBMs) are endowed with the universal power of
modeling (binary) joint distributions. Meanwhile, as a result of their
confining network structure, training RBMs confronts less difficulties
(compared with more complicated models, e.g., Boltzmann machines) when dealing
with approximation and inference issues. However, in certain computational
biology scenarios, such as the cancer data analysis, employing RBMs to model
data features may lose its efficacy due to the &quot;$p\gg N$&quot; problem, in which the
number of features/predictors is much larger than the sample size. The &quot;$p\gg
N$&quot; problem puts the bias-variance trade-off in a more crucial place when
designing statistical learning methods. In this manuscript, we try to address
this problem by proposing a novel RBM model, called elastic restricted
Boltzmann machine (eRBM), which incorporates the elastic regularization term
into the likelihood/cost function. We provide several theoretical analysis on
the superiority of our model. Furthermore, attributed to the classic
contrastive divergence (CD) algorithm, eRBMs can be trained efficiently. Our
novel model is a promising method for future cancer data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03634</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03634</id><created>2015-10-13</created><authors><author><keyname>Gupta</keyname><forenames>Anindya</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Decoding Network Codes using the Sum-Product Algorithm</title><categories>cs.IT math.IT</categories><comments>5 figures; 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While feasibility and obtaining a solution of a given network coding problem
are well studied, the decoding procedure and complexity have not garnered much
attention. We consider the decoding problem in a network wherein the sources
generate multiple messages and the sink nodes demand some or all of the source
messages. We consider both linear and non-linear network codes over a finite
field and propose to use the sum-product (SP) algorithm over Boolean semiring
for decoding at the sink nodes in order to reduce the computational complexity.
We use traceback to further lower the computational complexity incurred by SP
decoding. We also define and identify a sufficient condition for fast
decodability of a network code at a sink that demands all the source messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03637</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03637</id><created>2015-10-13</created><authors><author><keyname>Gabbrielli</keyname><forenames>Maurizio</forenames></author><author><keyname>Giallorenzo</keyname><forenames>Saverio</forenames></author><author><keyname>Montesi</keyname><forenames>Fabrizio</forenames></author></authors><title>Applied Choreographies</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Choreographic Programming is a methodology for the development of concurrent
software based on a correctness-by-construction approach which, given a global
description of a system (a choreography), automatically generates deadlock-free
communicating programs via an EndPoint Projection (EPP). Previous works use
target-languages for EPP that, like their source choreography languages, model
communications using channel names (e.g., variants of CCS and {\pi}-calculus).
This leaves a gap between such models and real-world implementations, where
communications are concretely supported by low-level mechanisms for message
routing.
  We bridge this gap by developing Applied Choreographies (AC), a new model for
choreographic programming. AC brings the correctness-by-construction
methodology of choreographies down to the level of a real executable language.
The key feature of AC is that its semantics is based on message correlation ---
a standard technique in Service-Oriented Computing --- while retaining the
usual simple and intuitive syntax of choreography languages. We provide AC with
a typing discipline that ensures the correct use of the low-level mechanism of
message correlation, thus avoiding communication errors. We also define a
two-step compilation from AC to a low-level Correlation Calculus, which is the
basis of a real executable language (Jolie). Finally, we prove an operational
correspondence theorem, which ensures that compiled programs behave as the
original choreography. This is the first result of such correct- ness property
in the case of a real-world implemented language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03638</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03638</id><created>2015-10-13</created><authors><author><keyname>Braham</keyname><forenames>Hajer</forenames></author><author><keyname>Jemaa</keyname><forenames>Sana Ben</forenames></author><author><keyname>Fort</keyname><forenames>Gersende</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Sayrac</keyname><forenames>Berna</forenames></author></authors><title>Spatial Prediction Under Location Uncertainty In Cellular Networks</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage prediction is one of the most important aspects of cellular network
optimization for a mobile operator. Spatial statistics can be used for coverage
prediction. This approach is based on the collected geo-located measurements
performed usually by drive test campaigns. Notice that uncertainty in reporting
the location can result in inaccurate coverage prediction. In urban
environments the location error can reach important levels up to $30$ m using
the Global Positioning System (GPS). In this paper, we propose to consider the
location uncertainty in the spatial prediction technique. We focus also on the
complexity problem. We therefore propose to use the Fixed Rank Kriging (FRK) as
spatial prediction technique. We validate the model using a field-like dataset
obtained from a sophisticated simulation/planning tool. With the location
uncertainty, the FRK proves to reduce the computational complexity of the
spatial interpolation while keeping an acceptable prediction error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03648</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03648</id><created>2015-10-13</created><authors><author><keyname>Gonzalez-Betancor</keyname><forenames>Sara M.</forenames></author><author><keyname>Dorta-Gonzalez</keyname><forenames>Pablo</forenames></author></authors><title>An indicator of journal impact that is based on calculating a journal's
  percentage of highly cited publications</title><categories>cs.DL</categories><comments>30 pages, 3 figures, 3 tables, 2 supplementary materials</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two most used citation impact indicators in the assessment of scientific
journals are, nowadays, the impact factor and the h-index. However, both
indicators are not field normalized (vary heavily depending on the scientific
category) which makes them incomparable between categories. Furthermore, the
impact factor is not robust to the presence of articles with a large number of
citations, while the h-index depends on the journal size. These limitations are
very important when comparing journals of different sizes and categories. An
alternative citation impact indicator is the percentage of highly cited
articles in a journal. This measure is field normalized (comparable between
scientific categories), independent of the journal size and also robust to the
presence of articles with a high number of citations. This paper empirically
compares this indicator with the impact factor and the h-index, considering
different time windows and citation percentiles (levels of citation for
considering an article as highly cited compared to others in the same year and
category).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03650</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03650</id><created>2015-10-13</created><authors><author><keyname>Tsuchiya</keyname><forenames>Kazuyoshi</forenames></author><author><keyname>Nogami</keyname><forenames>Yasuyuki</forenames></author></authors><title>Long Period Sequences Generated by the Logistic Map over Finite Fields
  with Control Parameter Four</title><categories>cs.IT math.IT</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, binary sequences generated by chaotic maps have been widely
studied. In particular, the logistic map is used as one of the chaotic map.
However, if the logistic map is implemented by using finite precision computer
arithmetic, rounding is required. In order to avoid rounding, Miyazaki, Araki,
Uehara and Nogami proposed the logistic map over finite fields, and show some
properties of sequences generated by the logistic map over finite fields. In
this paper, we show some properties of periods of sequences generated by the
logistic map over finite fields with control parameter four. In particular, we
show conditions for parameters and initial values to have a long period, and
asymptotic properties for periods by numerical experiments. Conditions for
initial values are described by values of the Legendre symbol. The main idea is
to introduce a structure of a hyperbola to certain sets of initial values. It
follows that periods of sequences generated by the logistic map over finite
fields on the sets of initial values are induced by periods of sequences
generated by the square map on the parameter spaces of the hyperbola.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03655</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03655</id><created>2015-10-13</created><updated>2015-10-14</updated><authors><author><keyname>D'Arco</keyname><forenames>Paolo</forenames></author><author><keyname>Esfahani</keyname><forenames>Navid Nasr</forenames></author><author><keyname>Stinson</keyname><forenames>Douglas R.</forenames></author></authors><title>All or Nothing at All</title><categories>math.CO cs.CR cs.IT math.IT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue a study of unconditionally secure all-or-nothing transforms
(AONT) begun in \cite{St}. An AONT is a bijective mapping that constructs s
outputs from s inputs. We consider the security of t inputs, when s-t outputs
are known. Previous work concerned the case t=1; here we consider the problem
for general t, focussing on the case t=2. We investigate constructions of
binary matrices for which the desired properties hold with the maximum
probability. Upper bounds on these probabilities are obtained via a quadratic
programming approach, while lower bounds can be obtained from combinatorial
constructions based on symmetric BIBDs and cyclotomy. We also report some
results on exhaustive searches and random constructions for small values of s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03676</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03676</id><created>2015-07-28</created><authors><author><keyname>Abdurachmanov</keyname><forenames>David</forenames></author><author><keyname>Elmer</keyname><forenames>Peter</forenames></author><author><keyname>Eulisse</keyname><forenames>Giulio</forenames></author><author><keyname>Knight</keyname><forenames>Robert</forenames></author></authors><title>Future Computing Platforms for Science in a Power Constrained Era</title><categories>cs.DC hep-ex</categories><comments>Submitted to proceedings of the 21st International Conference on
  Computing in High Energy and Nuclear Physics (CHEP2015), Okinawa, Japan</comments><doi>10.1088/1742-6596/664/9/092007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power consumption will be a key constraint on the future growth of
Distributed High Throughput Computing (DHTC) as used by High Energy Physics
(HEP). This makes performance-per-watt a crucial metric for selecting
cost-efficient computing solutions. For this paper, we have done a wide survey
of current and emerging architectures becoming available on the market
including x86-64 variants, ARMv7 32-bit, ARMv8 64-bit, Many-Core and GPU
solutions, as well as newer System-on-Chip (SoC) solutions. We compare
performance and energy efficiency using an evolving set of standardized
HEP-related benchmarks and power measurement techniques we have been
developing. We evaluate the potential for use of such computing solutions in
the context of DHTC systems, such as the Worldwide LHC Computing Grid (WLCG).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03678</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03678</id><created>2015-10-13</created><authors><author><keyname>Gaudiello</keyname><forenames>Ilaria</forenames></author><author><keyname>Zibetti</keyname><forenames>Elisabetta</forenames></author><author><keyname>Lefort</keyname><forenames>Sebastien</forenames></author><author><keyname>Chetouani</keyname><forenames>Mohamed</forenames></author><author><keyname>Ivaldi</keyname><forenames>Serena</forenames></author></authors><title>Trust as indicator of robot functional and social acceptance. An
  experimental study on user conformation to the iCub's answers</title><categories>cs.RO cs.CY cs.HC</categories><comments>49 pages, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To investigate the functional and social acceptance of a humanoid robot, we
carried out an experimental study with 56 adult participants and the iCub
robot. Trust in the robot has been considered as a main indicator of acceptance
in decision-making tasks characterized by perceptual uncertainty (e.g.,
evaluating the weight of two objects) and socio-cognitive uncertainty (e.g.,
evaluating which is the most suitable item in a specific context), and measured
by the participants' conformation to the iCub's answers to specific questions.
In particular, we were interested in understanding whether specific (i)
user-related features (i.e. desire for control), (ii) robot-related features
(i.e., attitude towards social influence of robots), and (iii) context-related
features (i.e., collaborative vs. competitive scenario), may influence their
trust towards the iCub robot. We found that participants conformed more to the
iCub's answers when their decisions were about functional issues than when they
were about social issues. Moreover, the few participants conforming to the
iCub's answers for social issues also conformed less for functional issues.
Trust in the robot's functional savvy does not thus seem to be a pre-requisite
for trust in its social savvy. Finally, desire for control, attitude towards
social influence of robots and type of interaction scenario did not influence
the trust in iCub. Results are discussed with relation to methodology of HRI
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03692</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03692</id><created>2015-10-13</created><authors><author><keyname>Ablinger</keyname><forenames>Jakob</forenames></author><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Algebraic independence of (cyclotomic) harmonic sums</title><categories>cs.SC math-ph math.MP math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An expression in terms of (cyclotomic) harmonic sums can be simplified by the
quasi-shuffle algebra in terms of the so-called basis sums. By construction,
these sums are algebraically independent within the quasi-shuffle algebra. In
this article we show that the basis sums can be represented within a tower of
difference ring extensions where the constants remain unchanged. This property
enables one to embed this difference ring for the (cyclotomic) harmonic sums
into the ring of sequences. This construction implies that the sequences
produced by the basis sums are algebraically independent over the rational
sequences adjoined with the alternating sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03694</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03694</id><created>2015-10-13</created><authors><author><keyname>Herrer&#xed;a-Alonso</keyname><forenames>Sergio</forenames></author><author><keyname>Rodr&#xed;guez-P&#xe9;rez</keyname><forenames>Miguel</forenames></author><author><keyname>Fern&#xe1;ndez-Veiga</keyname><forenames>Manuel</forenames></author><author><keyname>L&#xf3;pez-Garc&#xed;a</keyname><forenames>C&#xe1;ndido</forenames></author></authors><title>Frame Coalescing in Dual-Mode EEE</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE has recently released the 802.3bj standard that defines two
different low power operating modes for high speed Energy Efficient Ethernet
physical interfaces (PHYs) working at 40 and 100 Gb/s. In this paper, we
propose the use of the well-known frame coalescing algorithm to manage them and
provide an analytical model to evaluate the influence of coalescing parameters
and PHY characteristics on their power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03709</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03709</id><created>2015-10-12</created><authors><author><keyname>Dominguez</keyname><forenames>Miguel</forenames></author><author><keyname>Ghoraani</keyname><forenames>Behnaz</forenames></author><author><keyname>D</keyname><forenames>Ph.</forenames></author></authors><title>Structure-Constrained Basis Pursuit for Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, 1 table, 6 equations, submitted to ICASSP 2016
  but not yet accepted or rejected</comments><msc-class>94</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressive sensing (CS) theory, as the number of samples is decreased
below a minimum threshold, the average error of the recovery increases.
Sufficient sampling is either required for quality reconstruction or the error
is resignedly accepted. However, most CS work has not taken advantage of the
inherent structure in a variety of signals relevant to engineering
applications. Hence, this paper proposes a new method of recovery built on
basis pursuit (BP), called Structure-Constrained Basis Pursuit (SCBP), that
constrains signals based on known structure rather than through extra sampling.
Preliminary assessments of this method on TIMIT recordings of the speech
phoneme /aa/ show a substantial decrease in error: with a fixed 5:1 compression
ratio the average recovery error is 23.8% lower versus vanilla BP. More
significantly, this method can be applied to any CS application that samples
structured data, such as FSK waveforms, speech, and tones. In these cases,
higher compression ratios can be reached with comparable error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03710</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03710</id><created>2015-10-13</created><updated>2016-01-14</updated><authors><author><keyname>Vodol&#xe1;n</keyname><forenames>Miroslav</forenames></author><author><keyname>Kadlec</keyname><forenames>Rudolf</forenames></author><author><keyname>Kleindienst</keyname><forenames>Jan</forenames></author></authors><title>Hybrid Dialog State Tracker</title><categories>cs.CL</categories><comments>Accepted to Machine Learning for SLU &amp; Interaction NIPS 2015
  Workshop. Model description in Section 2.1 simplified compared to the
  previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hybrid dialog state tracker that combines a rule based
and a machine learning based approach to belief state tracking. Therefore, we
call it a hybrid tracker. The machine learning in our tracker is realized by a
Long Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker
sets a new state-of-the-art result for the Dialog State Tracking Challenge
(DSTC) 2 dataset when the system uses only live SLU as its input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03715</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03715</id><created>2015-10-13</created><authors><author><keyname>Bojic</keyname><forenames>Iva</forenames></author><author><keyname>Massaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Belyi</keyname><forenames>Alexander</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Choosing the right home location definition method for the given dataset</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ever since first mobile phones equipped with GPS came to the market, knowing
the exact user location has become a holy grail of almost every service that
lives in the digital world. Starting with the idea of location based services,
nowadays it is not only important to know where users are in real time, but
also to be able predict where they will be in future. Moreover, it is not
enough to know user location in form of latitude longitude coordinates provided
by GPS devices, but also to give a place its meaning (i.e., semantically label
it), in particular detecting the most probable home location for the given
user. The aim of this paper is to provide novel insights on differences among
the ways how different types of human digital trails represent the actual
mobility patterns and therefore the differences between the approaches
interpreting those trails for inferring said patterns. Namely, with the
emergence of different digital sources that provide information about user
mobility, it is of vital importance to fully understand that not all of them
capture exactly the same picture. With that being said, in this paper we start
from an example showing how human mobility patterns described by means of
radius of gyration are different for Flickr social network and dataset of bank
card transactions. Rather than capturing human movements closer to their homes,
Flickr more often reveals people travel mode. Consequently, home location
inferring methods used in both cases cannot be the same. We consider several
methods for home location definition known from the literature and demonstrate
that although for bank card transactions they provide highly consistent
results, home location definition detection methods applied to Flickr dataset
happen to be way more sensitive to the method selected, stressing the paramount
importance of adjusting the method to the specific dataset being used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03726</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03726</id><created>2015-10-13</created><authors><author><keyname>Martini</keyname><forenames>Simone</forenames></author></authors><title>Several types of types in programming languages</title><categories>cs.PL</categories><acm-class>D.3.3; K.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Types are an important part of any modern programming language, but we often
forget that the concept of type we understand nowadays is not the same it was
perceived in the sixties. Moreover, we conflate the concept of &quot;type&quot; in
programming languages with the concept of the same name in mathematical logic,
an identification that is only the result of the convergence of two different
paths, which started apart with different aims. The paper will present several
remarks (some historical, some of more conceptual character) on the subject, as
a basis for a further investigation. The thesis we will argue is that there are
three different characters at play in programming languages, all of them now
called types: the technical concept used in language design to guide
implementation; the general abstraction mechanism used as a modelling tool; the
classifying tool inherited from mathematical logic. We will suggest three
possible dates ad quem for their presence in the programming language
literature, suggesting that the emergence of the concept of type in computer
science is relatively independent from the logical tradition, until the
Curry-Howard isomorphism will make an explicit bridge between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03727</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03727</id><created>2015-10-13</created><authors><author><keyname>Golodetz</keyname><forenames>Stuart</forenames></author><author><keyname>Sapienza</keyname><forenames>Michael</forenames></author><author><keyname>Valentin</keyname><forenames>Julien P. C.</forenames></author><author><keyname>Vineet</keyname><forenames>Vibhav</forenames></author><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author><author><keyname>Arnab</keyname><forenames>Anurag</forenames></author><author><keyname>Prisacariu</keyname><forenames>Victor A.</forenames></author><author><keyname>K&#xe4;hler</keyname><forenames>Olaf</forenames></author><author><keyname>Ren</keyname><forenames>Carl Yuheng</forenames></author><author><keyname>Murray</keyname><forenames>David W.</forenames></author><author><keyname>Izadi</keyname><forenames>Shahram</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes</title><categories>cs.CV</categories><comments>33 pages, Project: http://www.semantic-paint.com, Code:
  https://github.com/torrvision/spaint</comments><report-no>TVG-2015-1</report-no><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03730</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03730</id><created>2015-10-13</created><authors><author><keyname>P&#xe9;rez-Gonz&#xe1;lez</keyname><forenames>Fernando</forenames></author><author><keyname>Gonz&#xe1;lez-Iglesias</keyname><forenames>Iria</forenames></author><author><keyname>Masciopinto</keyname><forenames>Miguel</forenames></author><author><keyname>Comesa&#xf1;a</keyname><forenames>Pedro</forenames></author></authors><title>Fast sequential forensic camera identification</title><categories>cs.CR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two sequential camera source identification methods are proposed. Sequential
tests implement a log-likelihood ratio test in an incremental way, thus
enabling a reliable decision with a minimal number of observations. One of our
methods adapts Goljan et al.'s to sequential operation. The second, which
offers better performance in terms of error probabilities and average number of
test observations, is based on treating the alternative hypothesis as a doubly
stochastic model. We also discuss how the standard sequential test can be
corrected to account for the event of weak fingerprints. Finally, we validate
the goodness of our methods with experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03742</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03742</id><created>2015-10-13</created><authors><author><keyname>Zhao</keyname><forenames>Liming</forenames></author><author><keyname>P&#xe9;rez-Delgado</keyname><forenames>Carlos A.</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author></authors><title>Fast graph operations in quantum computation</title><categories>quant-ph cs.DS</categories><comments>9 pages, 1 figure. Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The connection between certain entangled states and graphs has been heavily
studied in the context of measurement-based quantum computation as a tool for
understanding entanglement. Here we show that this correspondence can be
harnessed in the reverse direction to yield a graph data structure which allows
for more efficient manipulation and comparison of graphs than any possible
classical structure. We introduce efficient algorithms for many transformation
and comparison operations on graphs represented as graph states, and prove that
no classical data structure can have similar performance for the full set of
operations studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03743</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03743</id><created>2015-10-13</created><authors><author><keyname>Workman</keyname><forenames>Scott</forenames></author><author><keyname>Souvenir</keyname><forenames>Richard</forenames></author><author><keyname>Jacobs</keyname><forenames>Nathan</forenames></author></authors><title>Wide-Area Image Geolocalization with Aerial Reference Imagery</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to use deep convolutional neural networks to address the problem
of cross-view image geolocalization, in which the geolocation of a ground-level
query image is estimated by matching to georeferenced aerial images. We use
state-of-the-art feature representations for ground-level images and introduce
a cross-view training approach for learning a joint semantic feature
representation for aerial images. We also propose a network architecture that
fuses features extracted from aerial images at multiple spatial scales. To
support training these networks, we introduce a massive database that contains
pairs of aerial and ground-level images from across the United States. Our
methods significantly out-perform the state of the art on two benchmark
datasets. We also show, qualitatively, that the proposed feature
representations are discriminative at both local and continental spatial
scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03753</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03753</id><created>2015-10-13</created><updated>2015-11-03</updated><authors><author><keyname>Kadlec</keyname><forenames>Rudolf</forenames></author><author><keyname>Schmid</keyname><forenames>Martin</forenames></author><author><keyname>Kleindienst</keyname><forenames>Jan</forenames></author></authors><title>Improved Deep Learning Baselines for Ubuntu Corpus Dialogs</title><categories>cs.CL</categories><comments>Accepted to Machine Learning for SLU &amp; Interaction NIPS 2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents results of our experiments for the next utterance ranking
on the Ubuntu Dialog Corpus -- the largest publicly available multi-turn dialog
corpus. First, we use an in-house implementation of previously reported models
to do an independent evaluation using the same data. Second, we evaluate the
performances of various LSTMs, Bi-LSTMs and CNNs on the dataset. Third, we
create an ensemble by averaging predictions of multiple models. The ensemble
further improves the performance and it achieves a state-of-the-art result for
the next utterance ranking on this dataset. Finally, we discuss our future
plans using this corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03765</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03765</id><created>2015-09-08</created><authors><author><keyname>Allgaier</keyname><forenames>Nicholas</forenames></author><author><keyname>Banaschewski</keyname><forenames>Tobias</forenames></author><author><keyname>Barker</keyname><forenames>Gareth</forenames></author><author><keyname>Bokde</keyname><forenames>Arun L. W.</forenames></author><author><keyname>Bongard</keyname><forenames>Josh C.</forenames></author><author><keyname>Bromberg</keyname><forenames>Uli</forenames></author><author><keyname>B&#xfc;chel</keyname><forenames>Christian</forenames></author><author><keyname>Cattrell</keyname><forenames>Anna</forenames></author><author><keyname>Conrod</keyname><forenames>Patricia J.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Desrivi&#xe8;res</keyname><forenames>Sylvane</forenames></author><author><keyname>Dodds</keyname><forenames>Peter S.</forenames></author><author><keyname>Flor</keyname><forenames>Herta</forenames></author><author><keyname>Frouin</keyname><forenames>Vincent</forenames></author><author><keyname>Gallinat</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Gowland</keyname><forenames>Penny</forenames></author><author><keyname>Heinz</keyname><forenames>Andreas</forenames></author><author><keyname>Ittermann</keyname><forenames>Bernd</forenames></author><author><keyname>Mackey</keyname><forenames>Scott</forenames></author><author><keyname>Martinot</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Nees</keyname><forenames>Frauke</forenames></author><author><keyname>Papadopoulos-Orfanos</keyname><forenames>Dimitri</forenames></author><author><keyname>Poustka</keyname><forenames>Luise</forenames></author><author><keyname>Smolka</keyname><forenames>Michael N.</forenames></author><author><keyname>Walter</keyname><forenames>Henrik</forenames></author><author><keyname>Whelan</keyname><forenames>Robert</forenames></author><author><keyname>Schumann</keyname><forenames>Gunter</forenames></author><author><keyname>Garavan</keyname><forenames>Hugh</forenames></author><author><keyname>Consortium</keyname><forenames>IMAGEN</forenames></author></authors><title>Nonlinear functional mapping of the human brain</title><categories>q-bio.NC cs.NE</categories><comments>21 pages, 12 figures, and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of neuroimaging has truly become data rich, and novel analytical
methods capable of gleaning meaningful information from large stores of imaging
data are in high demand. Those methods that might also be applicable on the
level of individual subjects, and thus potentially useful clinically, are of
special interest. In the present study, we introduce just such a method, called
nonlinear functional mapping (NFM), and demonstrate its application in the
analysis of resting state fMRI from a 242-subject subset of the IMAGEN project,
a European study of adolescents that includes longitudinal phenotypic,
behavioral, genetic, and neuroimaging data. NFM employs a computational
technique inspired by biological evolution to discover and mathematically
characterize interactions among ROI (regions of interest), without making
linear or univariate assumptions. We show that statistics of the resulting
interaction relationships comport with recent independent work, constituting a
preliminary cross-validation. Furthermore, nonlinear terms are ubiquitous in
the models generated by NFM, suggesting that some of the interactions
characterized here are not discoverable by standard linear methods of analysis.
We discuss one such nonlinear interaction in the context of a direct comparison
with a procedure involving pairwise correlation, designed to be an analogous
linear version of functional mapping. We find another such interaction that
suggests a novel distinction in brain function between drinking and
non-drinking adolescents: a tighter coupling of ROI associated with emotion,
reward, and interoceptive processes such as thirst, among drinkers. Finally, we
outline many improvements and extensions of the methodology to reduce
computational expense, complement other analytical tools like graph-theoretic
analysis, and allow for voxel level NFM to eliminate the necessity of ROI
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03776</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03776</id><created>2015-09-30</created><authors><author><keyname>Hermans</keyname><forenames>Michiel</forenames></author><author><keyname>Van Vaerenbergh</keyname><forenames>Thomas</forenames></author></authors><title>Towards Trainable Media: Using Waves for Neural Network-Style Training</title><categories>cs.NE physics.optics</categories><comments>submitted to Scientific Reports</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the concept of using the interaction between waves and
a trainable medium in order to construct a matrix-vector multiplier. In
particular we study such a device in the context of the backpropagation
algorithm, which is commonly used for training neural networks. Here, the
weights of the connections between neurons are trained by multiplying a
`forward' signal with a backwards propagating `error' signal. We show that this
concept can be extended to trainable media, where the gradient for the local
wave number is given by multiplying signal waves and error waves. We provide a
numerical example of such a system with waves traveling freely in a trainable
medium, and we discuss a potential way to build such a device in an integrated
photonics chip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03794</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03794</id><created>2015-10-13</created><authors><author><keyname>Czajka</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>On the equivalence of different presentations of Turner's bracket
  abstraction algorithm</title><categories>math.LO cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turner's bracket abstraction algorithm is perhaps the most well-known
improvement on simple bracket abstraction algorithms. It is also one of the
most studied bracket abstraction algorithms. The definition of the algorithm in
Turner's original paper is slightly ambiguous and it has been subject to
different interpretations. It has been erroneously claimed in some papers that
certain formulations of Turner's algorithm are equivalent. In this note we
clarify the relationship between various presentations of Turner's algorithm
and we show that some of them are in fact equivalent for translating
lambda-terms in beta-normal form.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03797</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03797</id><created>2015-10-13</created><authors><author><keyname>Gurciullo</keyname><forenames>Stefano</forenames></author><author><keyname>Smallegan</keyname><forenames>Michael</forenames></author><author><keyname>Pereda</keyname><forenames>Mar&#xed;a</forenames></author><author><keyname>Battiston</keyname><forenames>Federico</forenames></author><author><keyname>Patania</keyname><forenames>Alice</forenames></author><author><keyname>Poledna</keyname><forenames>Sebastian</forenames></author><author><keyname>Hedblom</keyname><forenames>Daniel</forenames></author><author><keyname>Oztan</keyname><forenames>Bahattin Tolga</forenames></author><author><keyname>Herzog</keyname><forenames>Alexander</forenames></author><author><keyname>John</keyname><forenames>Peter</forenames></author><author><keyname>Mikhaylov</keyname><forenames>Slava</forenames></author></authors><title>Complex Politics: A Quantitative Semantic and Topological Analysis of UK
  House of Commons Debates</title><categories>physics.soc-ph cs.CL cs.SI</categories><msc-class>91F10</msc-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This study is a first, exploratory attempt to use quantitative semantics
techniques and topological analysis to analyze systemic patterns arising in a
complex political system. In particular, we use a rich data set covering all
speeches and debates in the UK House of Commons between 1975 and 2014. By the
use of dynamic topic modeling (DTM) and topological data analysis (TDA) we show
that both members and parties feature specific roles within the system,
consistent over time, and extract global patterns indicating levels of
political cohesion. Our results provide a wide array of novel hypotheses about
the complex dynamics of political systems, with valuable policy applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03814</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03814</id><created>2015-10-13</created><updated>2015-10-17</updated><authors><author><keyname>Yang</keyname><forenames>Yu</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author></authors><title>In-Network Neighborhood-Based Node Similarity Measure: A Unified
  Parametric Model</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, we need to measure similarity between nodes in a large
network based on features of their neighborhoods. Although in-network node
similarity based on proximity has been well investigated, surprisingly,
measuring in-network node similarity based on neighborhoods remains a largely
untouched problem in literature. One grand challenge is that in different
applications we may need different measurements that manifest different
meanings of similarity. In this paper, we investigate the problem in a
principled and systematic manner. We develop a unified parametric model and a
series of four instance measures. Those instance similarity measures not only
address a spectrum of various meanings of similarity, but also present a series
of tradeoffs between computational cost and strictness of matching between
neighborhoods of nodes being compared. By extensive experiments and case
studies, we demonstrate the effectiveness of the proposed model and its
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03820</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03820</id><created>2015-10-13</created><updated>2016-02-20</updated><authors><author><keyname>Zhang</keyname><forenames>Ye</forenames></author><author><keyname>Wallace</keyname><forenames>Byron</forenames></author></authors><title>A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional
  Neural Networks for Sentence Classification</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) have recently achieved remarkably strong
performance on the practically important task of sentence classification (kim
2014, kalchbrenner 2014, johnson 2014). However, these models require
practitioners to specify an exact model architecture and set accompanying
hyperparameters, including the filter region size, regularization parameters,
and so on. It is currently unknown how sensitive model performance is to
changes in these configurations for the task of sentence classification. We
thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of
architecture components on model performance; our aim is to distinguish between
important and comparatively inconsequential design decisions for sentence
classification. We focus on one-layer CNNs (to the exclusion of more complex
models) due to their comparative simplicity and strong empirical performance,
which makes it a modern standard baseline method akin to Support Vector Machine
(SVMs) and logistic regression. We derive practical advice from our extensive
empirical results for those interested in getting the most out of CNNs for
sentence classification in real world settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03822</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03822</id><created>2015-10-13</created><authors><author><keyname>Wang</keyname><forenames>Zhefeng</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Yang</keyname><forenames>Yu</forenames></author><author><keyname>Ge</keyname><forenames>Yong</forenames></author><author><keyname>Chang</keyname><forenames>Biao</forenames></author></authors><title>Information Coverage Maximization in Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks, due to their popularity, have been studied extensively these
years. A rich body of these studies is related to influence maximization, which
aims to select a set of seed nodes for maximizing the expected number of active
nodes at the end of the process. However, the set of active nodes can not fully
represent the true coverage of information propagation. A node may be informed
of the information when any of its neighbours become active and try to activate
it, though this node (namely informed node) is still inactive. Therefore, we
need to consider both active nodes and informed nodes that are aware of the
information when we study the coverage of information propagation in a network.
Along this line, in this paper we propose a new problem called Information
Coverage Maximization that aims to maximize the expected number of both active
nodes and informed ones. After we prove that this problem is NP-hard and
submodular in the independent cascade model and the linear threshold model, we
design two algorithms to solve it. Extensive experiments on three real-world
data sets demonstrate the performance of the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03826</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03826</id><created>2015-10-13</created><updated>2015-12-15</updated><authors><author><keyname>Wang</keyname><forenames>Zhiguang</forenames></author><author><keyname>Oates</keyname><forenames>Tim</forenames></author><author><keyname>Lo</keyname><forenames>James</forenames></author></authors><title>Adopting Robustness and Optimality in Fitting and Learning</title><categories>cs.LG cs.NE math.OC</categories><comments>This paper has been withdrawn by the authors due to some errors and
  confusions in terminology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalized a modified exponentialized estimator by pushing the
robust-optimal (RO) index $\lambda$ to $-\infty$ for achieving robustness to
outliers by optimizing a quasi-Minimin function. The robustness is realized and
controlled adaptively by the RO index without any predefined threshold.
Optimality is guaranteed by expansion of the convexity region in the Hessian
matrix to largely avoid local optima. Detailed quantitative analysis on both
robustness and optimality are provided. The results of proposed experiments on
fitting tasks for three noisy non-convex functions and the digits recognition
task on the MNIST dataset consolidate the conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03840</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03840</id><created>2015-09-19</created><updated>2016-02-03</updated><authors><author><keyname>Paschos</keyname><forenames>Alexandros E.</forenames></author><author><keyname>Kapinas</keyname><forenames>Vasileios M.</forenames></author><author><keyname>Hadjileontiadis</keyname><forenames>Leontios J.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Dynamic Spectrum Sensing Through Accelerated Particle Swarm Optimization</title><categories>math.OC cs.IT math.IT stat.AP</categories><comments>4 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel optimization algorithm, called accelerated particle swarm
optimization (APSO), is proposed for dynamic spectrum sensing in cognitive
radio networks. While modified swarm-based optimization algorithms focus on
slight variations of the standard mathematical formulas, in APSO, the
acceleration variable of the particles in the swarm is also considered in the
search space of the optimization problem. We show that the proposed APSO-based
dynamic spectrum sensing technique is more efficient than existing methods,
which are entirely based on the original particle swarm optimization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03857</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03857</id><created>2015-10-13</created><authors><author><keyname>Kong</keyname><forenames>Zhengmin</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Wu</keyname><forenames>Feilong</forenames></author><author><keyname>Peng</keyname><forenames>Shixin</forenames></author><author><keyname>Zhong</keyname><forenames>Liang</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Iterative distributed minimum total-MSE approach for secure
  communications in MIMO interference channels</title><categories>cs.IT math.IT</categories><comments>16 pages, 9 figures, accepted to appear on IEEE Transactions on
  Information Forensics and Security, Oct. 2015</comments><journal-ref>IEEE Transactions on Information Forensics and Security, vol. 11,
  no. 3, pp. 594-608, Mar. 2016</journal-ref><doi>10.1109/TIFS.2015.2493888</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of joint transmit precoding (TPC)
matrix and receive filter matrix design subject to both secrecy and
per-transmitter power constraints in the MIMO interference channel, where $K$
legitimate transmitter-receiver pairs communicate in the presence of an
external eavesdropper. Explicitly, we jointly design the TPC and receive filter
matrices based on the minimum total mean-squared error (MT-MSE) criterion under
a given and feasible information-theoretic degrees of freedom. More
specifically, we formulate this problem by minimizing the total MSEs of the
signals communicated between the legitimate transmitter-receiver pairs, whilst
ensuring that the MSE of the signals decoded by the eavesdropper remains higher
than a certain threshold. We demonstrate that the joint design of the TPC and
receive filter matrices subject to both secrecy and transmit power constraints
can be accomplished by an efficient iterative distributed algorithm. The
convergence of the proposed iterative algorithm is characterized as well.
Furthermore, the performance of the proposed algorithm, including both its
secrecy rate and MSE, is characterized with the aid of numerical results. We
demonstrate that the proposed algorithm outperforms the traditional
interference alignment (IA) algorithm in terms of both the achievable secrecy
rate and the MSE. As a benefit, secure communications can be guaranteed by the
proposed algorithm for the MIMO interference channel even in the presence of a
&quot;sophisticated/strong&quot; eavesdropper, whose number of antennas is much higher
than that of each legitimate transmitter and receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03888</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03888</id><created>2015-10-13</created><authors><author><keyname>Kliuchnikov</keyname><forenames>Vadym</forenames></author><author><keyname>Bocharov</keyname><forenames>Alex</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Yard</keyname><forenames>Jon</forenames></author></authors><title>A Framework for Approximating Qubit Unitaries</title><categories>quant-ph cs.ET</categories><comments>60 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for efficiently approximating of qubit unitaries over
gate sets derived from totally definite quaternion algebras. It achieves
$\varepsilon$-approximations using circuits of length $O(\log(1/\varepsilon))$,
which is asymptotically optimal. The algorithm achieves the same quality of
approximation as previously-known algorithms for Clifford+T [arXiv:1212.6253],
V-basis [arXiv:1303.1411] and Clifford+$\pi/12$ [arXiv:1409.3552], running on
average in time polynomial in $O(\log(1/\varepsilon))$ (conditional on a
number-theoretic conjecture). Ours is the first such algorithm that works for a
wide range of gate sets and provides insight into what should constitute a
&quot;good&quot; gate set for a fault-tolerant quantum computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03891</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03891</id><created>2015-10-13</created><authors><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Henriques</keyname><forenames>Julie</forenames></author><author><keyname>Larger</keyname><forenames>Laurent</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Nonlinear memory capacity of parallel time-delay reservoir computers in
  the processing of multidimensional signals</title><categories>cs.NE</categories><comments>24 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the reservoir design problem in the context of
delay-based reservoir computers for multidimensional input signals, parallel
architectures, and real-time multitasking. First, an approximating reservoir
model is presented in those frameworks that provides an explicit functional
link between the reservoir parameters and architecture and its performance in
the execution of a specific task. Second, the inference properties of the ridge
regression estimator in the multivariate context is used to assess the impact
of finite sample training on the decrease of the reservoir capacity. Finally,
an empirical study is conducted that shows the adequacy of the theoretical
results with the empirical performances exhibited by various reservoir
architectures in the execution of several nonlinear tasks with multidimensional
inputs. Our results confirm the robustness properties of the parallel reservoir
architecture with respect to task misspecification and parameter choice that
had already been documented in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03892</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03892</id><created>2015-10-13</created><updated>2016-01-13</updated><authors><author><keyname>Bombardieri</keyname><forenames>Michele</forenames></author><author><keyname>Castan&#xf2;</keyname><forenames>Salvatore</forenames></author><author><keyname>Curcio</keyname><forenames>Fabrizio</forenames></author><author><keyname>Furfaro</keyname><forenames>Angelo</forenames></author><author><keyname>Karatza</keyname><forenames>Helen D.</forenames></author></authors><title>Honeypot-powered Malware Reverse Engineering</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Honeypots, i.e. networked computer systems specially designed and crafted to
mimic the normal operations of other systems while capturing and storing
information about the interactions with the world outside, are a crucial
technology into the study of cyber threats and attacks that propagate and occur
through networks. Among them, high interaction honeypots are considered the
most efficient because the attacker (whether automated or not) perceives
realistic interactions with the target machine. In the case of automated
attacks, propagated by malwares, currently available honeypots alone are not
specialized enough to allow the analysis of their behaviors and effects on the
target system. The research presented in this paper shows how high interaction
honeypots can be enhanced by powering them with specific features that improve
the reverse engineering activities needed to effectively analyze captured
malicious entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03895</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03895</id><created>2015-10-13</created><authors><author><keyname>Karppa</keyname><forenames>Matti</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Kohonen</keyname><forenames>Jukka</forenames></author></authors><title>A faster subquadratic algorithm for finding outlier correlations</title><categories>cs.DS</categories><comments>SODA 2016, to appear</comments><msc-class>65F30, 68W20, 62H20, 68T05, 68Q32</msc-class><acm-class>F.2.1; I.1.2; G.3; H.2.8; H.3.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of detecting {\em outlier pairs} of strongly correlated
variables among a collection of $n$ variables with otherwise weak pairwise
correlations. After normalization, this task amounts to the geometric task
where we are given as input a set of $n$ vectors with unit Euclidean norm and
dimension $d$, and we are asked to find all the outlier pairs of vectors whose
inner product is at least $\rho$ in absolute value, subject to the promise that
all but at most $q$ pairs of vectors have inner product at most $\tau$ in
absolute value for some constants $0&lt;\tau&lt;\rho&lt;1$.
  Improving on an algorithm of G.~Valiant [FOCS~2012; J.\,ACM~2015], we present
a randomized algorithm that for Boolean inputs ($\{-1,1\}$-valued data
normalized to unit Euclidean length) runs in time \[ \tilde
O\bigl(n^{\max\,\{1-\gamma+M(\Delta\gamma,\gamma),\,M(1-\gamma,2\Delta\gamma)\}}+qdn^{2\gamma}\bigr)\,,
\] where $0&lt;\gamma&lt;1$ is a constant tradeoff parameter and $M(\mu,\nu)$ is the
exponent to multiply an $\lfloor n^\mu\rfloor\times\lfloor n^\nu\rfloor$ matrix
with an $\lfloor n^\nu\rfloor\times \lfloor n^\mu\rfloor$ matrix and
$\Delta=1/(1-\log_\tau\rho)$. As corollaries we obtain randomized algorithms
that run in time \[ \tilde
O\bigl(n^{\frac{2\omega}{3-\log_\tau\rho}}+qdn^{\frac{2(1-\log_\tau\rho)}{3-\log_\tau\rho}}\bigr)
\] and in time \[ \tilde
O\bigl(n^{\frac{4}{2+\alpha(1-\log_\tau\rho)}}+qdn^{\frac{2\alpha(1-\log_\tau\rho)}{2+\alpha(1-\log_\tau\rho)}}\bigr)\,,
\] where $2\leq\omega&lt;2.38$ is the exponent for square matrix multiplication
and $0.3&lt;\alpha\leq 1$ is the exponent for rectangular matrix multiplication.
We present further corollaries for the light bulb problem and for learning
sparse Boolean functions. (The notation $\tilde O(\cdot)$ hides polylogarithmic
factors in $n$ and $d$ whose degree may depend on $\rho$ and $\tau$.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03903</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03903</id><created>2015-10-13</created><authors><author><keyname>Segal-Halevi</keyname><forenames>Erel</forenames></author><author><keyname>Nitzan</keyname><forenames>Shmuel</forenames></author></authors><title>Fair Cake-Cutting among Groups</title><categories>cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper extends the classic cake-cutting problem from individual agents to
groups of agents. Applications include dividing a land-estate among families or
dividing disputed lands among states. In the standard cake-cutting model, each
*agent* should receive an individual subset of the cake with a sufficiently
high individual value. In our model, each *group* should receive a subset with
a sufficiently high &quot;group value&quot;. Six ways to define the aggregate group value
based on the values of the group members are examined: four based on cardinal
welfare functions and two based on ordinal preference relations. Our results
show that the choice of the group value function has crucial implications on
the existence and applicability of fair division protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03909</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03909</id><created>2015-10-13</created><authors><author><keyname>Walecki</keyname><forenames>Robert</forenames></author><author><keyname>Rudovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author><author><keyname>Pantic</keyname><forenames>Maja</forenames></author></authors><title>Variable-state Latent Conditional Random Fields for Facial Expression
  Recognition and Action Unit Detection</title><categories>cs.CV cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated recognition of facial expressions of emotions, and detection of
facial action units (AUs), from videos depends critically on modeling of their
dynamics. These dynamics are characterized by changes in temporal phases
(onset-apex-offset) and intensity of emotion expressions and AUs, the
appearance of which may vary considerably among target subjects, making the
recognition/detection task very challenging. The state-of-the-art Latent
Conditional Random Fields (L-CRF) framework allows one to efficiently encode
these dynamics through the latent states accounting for the temporal
consistency in emotion expression and ordinal relationships between its
intensity levels, these latent states are typically assumed to be either
unordered (nominal) or fully ordered (ordinal). Yet, such an approach is often
too restrictive. For instance, in the case of AU detection, the goal is to
discriminate between the segments of an image sequence in which this AU is
active or inactive. While the sequence segments containing activation of the
target AU may better be described using ordinal latent states, the inactive
segments better be described using unordered (nominal) latent states, as no
assumption can be made about their underlying structure (since they can contain
either neutral faces or activations of non-target AUs). To address this, we
propose the variable-state L-CRF (VSL-CRF) model that automatically selects the
optimal latent states for the target image sequence. To reduce the model
overfitting either the nominal or ordinal latent states, we propose a novel
graph-Laplacian regularization of the latent states. Our experiments on three
public expression databases show that the proposed model achieves better
generalization performance compared to traditional L-CRFs and other related
state-of-the-art models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03913</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03913</id><created>2015-10-13</created><authors><author><keyname>de Paula</keyname><forenames>Ubiratam</forenames></author><author><keyname>de Oliveira</keyname><forenames>Daniel</forenames></author><author><keyname>Frota</keyname><forenames>Yuri</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author><author><keyname>Drummond</keyname><forenames>L&#xfa;cia</forenames></author></authors><title>Detecting and Handling Flash-Crowd Events on Cloud Environments</title><categories>cs.DC</categories><comments>Submitted to the ACM Transactions on the Web (TWEB)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a highly scalable computing paradigm where resources are
delivered to users on demand via Internet. There are several areas that can
benefit from cloud computing and one in special is gaining much attention: the
flash-crowd handling. Flash-crowd events happen when servers are unable to
handle the volume of requests for a specific content (or a set of contents)
that actually reach it, thus causing some requests to be denied. For the
handling of flash-crowd events in Web applications, clouds can offer elastic
computing and storage capacity during these events in order to process all
requests. However, it is important that flash-crowd events are quickly detected
and the amount of resources to be instantiated during flash crowds is correctly
estimated. In this paper, a new mechanism for detection of flash crowds based
on concepts of entropy and total correlation is proposed. Moreover, the
Flash-Crowd Handling Problem (FCHP) is precisely defined and formulated as an
integer programming problem. A new algorithm for solving it, named FCHP-ILS, is
also proposed. With FCHP-ILS the Web provider is able to replicate contents in
the available resources and define the types and amount of resources to
instantiate in the cloud during a flash-crowd event. Finally we present a case
study, based on a synthetic dataset representing flash-crowd events in small
scenarios aiming at comparing the proposed approach with de facto standard
Amazon's Auto Scaling mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03918</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03918</id><created>2015-10-13</created><authors><author><keyname>Sojakova</keyname><forenames>Kristina</forenames></author></authors><title>The equivalence of the torus and the product of two circles in homotopy
  type theory</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homotopy type theory is a new branch of mathematics which merges insights
from abstract homotopy theory and higher category theory with those of logic
and type theory. It allows us to represent a variety of mathematical objects as
basic type-theoretic constructions, higher inductive types. We present a proof
that in homotopy type theory, the torus is equivalent to the product of two
circles. This result indicates that the synthetic definition of torus as a
higher inductive type is indeed correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03919</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03919</id><created>2015-10-13</created><updated>2016-02-18</updated><authors><author><keyname>Chalk</keyname><forenames>Cameron</forenames></author><author><keyname>Martinez</keyname><forenames>Eric</forenames></author><author><keyname>Schweller</keyname><forenames>Robert</forenames></author><author><keyname>Vega</keyname><forenames>Luis</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author><author><keyname>Wylie</keyname><forenames>Tim</forenames></author></authors><title>Optimal Staged Self-Assembly of General Shapes</title><categories>cs.CG cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the number of stages, tiles, and bins needed to construct $n
\times n$ squares and scaled shapes in the staged tile assembly model. In
particular, we prove that there exists a staged system with $b$ bins and $t$
tile types assembling an $n\times n$ square using $\mathcal{O}(\frac{\log{n} -
tb - t\log t}{b^2} + \frac{\log \log b}{\log t})$ stages, nearly matching our
lower bound of $\Omega(\frac{\log{n} - tb - t\log t}{b^2})$ for almost all $n$.
For a shape $S$, we obtain bounds of $\mathcal{O}(\frac{K(S) - tb - t\log
t}{b^2} + \frac{\log \log b}{\log t})$ and $\Omega(\frac{K(S) - tb - t\log
t}{b^2})$ for the assembly of a scaled version of $S$, where $K(S)$ denotes the
Kolmogorov complexity of $S$ with respect to some universal Turing machine.
Equally tight bounds are also obtained when more powerful \emph{flexible} glue
functions are permitted. These are the first results that hold for all choices
of $b$ and $t$, and both generalize and improve on prior results. The upper
bound constructions use a new technique for converting both sources of system
complexity (the tile types and mixing graph) into a &quot;bit string&quot; assembly with
only a constant-factor loss in information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03921</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03921</id><created>2015-10-13</created><authors><author><keyname>Park</keyname><forenames>Yongjoo</forenames></author><author><keyname>Cafarella</keyname><forenames>Michael</forenames></author><author><keyname>Mozafari</keyname><forenames>Barzan</forenames></author></authors><title>Visualization-Aware Sampling for Very Large Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive visualizations are crucial in ad hoc data exploration and
analysis. However, with the growing number of massive datasets, generating
visualizations in interactive timescales is increasingly challenging. One
approach for improving the speed of the visualization tool is via data
reduction in order to reduce the computational overhead, but at a potential
cost in visualization accuracy. Common data reduction techniques, such as
uniform and stratified sampling, do not exploit the fact that the sampled
tuples will be transformed into a visualization for human consumption.
  We propose a visualization-aware sampling (VAS) that guarantees high quality
visualizations with a small subset of the entire dataset. We validate our
method when applied to scatter and map plots for three common visualization
goals: regression, density estimation, and clustering. The key to our sampling
method's success is in choosing tuples which minimize a visualization-inspired
loss function. Our user study confirms that optimizing this loss function
correlates strongly with user success in using the resulting visualizations. We
also show the NP-hardness of our optimization problem and propose an efficient
approximation algorithm. Our experiments show that, compared to previous
methods, (i) using the same sample size, VAS improves user's success by up to
35% in various visualization tasks, and (ii) VAS can achieve a required
visualization quality up to 400 times faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03924</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03924</id><created>2015-10-13</created><authors><author><keyname>Moritz</keyname><forenames>Steffen</forenames></author><author><keyname>Sard&#xe1;</keyname><forenames>Alexis</forenames></author><author><keyname>Bartz-Beielstein</keyname><forenames>Thomas</forenames></author><author><keyname>Zaefferer</keyname><forenames>Martin</forenames></author><author><keyname>Stork</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Comparison of different Methods for Univariate Time Series Imputation in
  R</title><categories>stat.AP cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Missing values in datasets are a well-known problem and there are quite a lot
of R packages offering imputation functions. But while imputation in general is
well covered within R, it is hard to find functions for imputation of
univariate time series. The problem is, most standard imputation techniques can
not be applied directly. Most algorithms rely on inter-attribute correlations,
while univariate time series imputation needs to employ time dependencies. This
paper provides an overview of univariate time series imputation in general and
an in-detail insight into the respective implementations within R packages.
Furthermore, we experimentally compare the R functions on different time series
using four different ratios of missing data. Our results show that either an
interpolation with seasonal kalman filter from the zoo package or a linear
interpolation on seasonal loess decomposed data from the forecast package were
the most effective methods for dealing with missing data in most of the
scenarios assessed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03925</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03925</id><created>2015-10-13</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>On Equivalence of Martingale Tail Bounds and Deterministic Regret
  Inequalities</title><categories>math.PR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an equivalence of (i) deterministic pathwise statements appearing in
the online learning literature (termed \emph{regret bounds}), (ii)
high-probability tail bounds for the supremum of a collection of martingales
(of a specific form arising from uniform laws of large numbers for
martingales), and (iii) in-expectation bounds for the supremum. By virtue of
the equivalence, we prove exponential tail bounds for norms of Banach space
valued martingales via deterministic regret bounds for the online mirror
descent algorithm with an adaptive step size. We extend these results beyond
the linear structure of the Banach space: we define a notion of
\emph{martingale type} for general classes of real-valued functions and show
its equivalence (up to a logarithmic factor) to various sequential complexities
of the class (in particular, the sequential Rademacher complexity and its
offset version). For classes with the general martingale type 2, we exhibit a
finer notion of variation that allows partial adaptation to the function
indexing the martingale. Our proof technique rests on sequential symmetrization
and on certifying the \emph{existence} of regret minimization strategies for
certain online prediction problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03929</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03929</id><created>2015-10-13</created><updated>2016-02-23</updated><authors><author><keyname>Spaccasassi</keyname><forenames>Carlo</forenames></author><author><keyname>Koutavas</keyname><forenames>Vasileios</forenames></author></authors><title>Type-Based Analysis for Session Inference</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a type-based analysis to infer the session protocols of channels
in an ML-like concurrent functional language. Combining and extending
well-known techniques, we develop a type-checking system that separates the
underlying ML type system from the typing of sessions. Without using linearity,
our system guarantees communication safety and partial lock freedom. It also
supports provably complete session inference with no programmer annotations. We
exhibit the usefulness of our system with interesting examples, including one
which is not typable in substructural type systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03931</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03931</id><created>2015-10-13</created><updated>2015-10-24</updated><authors><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Yang</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Structured Memory for Neural Turing Machines</title><categories>cs.AI cs.NE</categories><comments>4 pages, accepted to Reasoning, Attention, Memory (RAM) NIPS 2015
  Workshop</comments><acm-class>I.2.6</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Neural Turing Machines (NTM) contain memory component that simulates &quot;working
memory&quot; in the brain to store and retrieve information to ease simple
algorithms learning. So far, only linearly organized memory is proposed, and
during experiments, we observed that the model does not always converge, and
overfits easily when handling certain tasks. We think memory component is key
to some faulty behaviors of NTM, and better organization of memory component
could help fight those problems. In this paper, we propose several different
structures of memory for NTM, and we proved in experiments that two of our
proposed structured-memory NTMs could lead to better convergence, in term of
speed and prediction accuracy on copy task and associative recall task as in
(Graves et al. 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03935</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03935</id><created>2015-10-13</created><authors><author><keyname>Cai</keyname><forenames>Yiqi</forenames></author><author><keyname>Guo</keyname><forenames>Xiaohu</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author><author><keyname>Mao</keyname><forenames>Weihua</forenames></author><author><keyname>Zhong</keyname><forenames>Zichun</forenames></author></authors><title>Curvature-Metric-Free Surface Remeshing via Principle Component Analysis</title><categories>cs.GR</categories><comments>14 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a surface remeshing method with high approximation
quality based on Principal Component Analysis. Given a triangular mesh and a
user assigned polygon/vertex budget, traditional methods usually require the
extra curvature metric field for the desired anisotropy to best approximate the
surface, even though the estimated curvature metric is known to be imperfect
and already self-contained in the surface. In our approach, this anisotropic
control is achieved through the optimal geometry partition without this
explicit metric field. The minimization of our proposed partition energy has
the following properties: Firstly, on a C2 surface, it is theoretically
guaranteed to have the optimal aspect ratio and cluster size as specified in
approximation theory for L1 piecewise linear approximation. Secondly, it
captures sharp features on practical models without any pre-tagging. We develop
an effective merging-swapping framework to seek the optimal partition and
construct polygonal/triangular mesh afterwards. The effectiveness and
efficiency of our method are demonstrated through the comparison with other
state-of-the-art remeshing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03938</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03938</id><created>2015-10-13</created><authors><author><keyname>Farag</keyname><forenames>Hossam M.</forenames></author><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author></authors><title>Hard Decision Cooperative Spectrum Sensing Based on Estimating the Noise
  Uncertainty Factor</title><categories>cs.IT cs.NI math.IT</categories><comments>5 pages, 4 figures, IEEE International Conference on Computer
  Engineering and Systems (ICCES 2015). arXiv admin note: text overlap with
  arXiv:1505.05580</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum Sensing (SS) is one of the most challenging issues in Cognitive
Radio (CR) systems. Cooperative Spectrum Sensing (CSS) is proposed to enhance
the detection reliability of a Primary User (PU) in fading environments. In
this paper, we propose a hard decision based CSS algorithm using energy
detection with taking into account the noise uncertainty effect. In the
proposed algorithm, two dynamic thresholds are toggled based on predicting the
current PU activity, which can be successfully expected using a simple
successive averaging process with time. Also, their values are evaluated using
an estimated value of the noise uncertainty factor. These dynamic thresholds
are used to compensate the noise uncertainty effect and increase (decrease) the
probability of detection (false alarm), respectively. Theoretical analysis is
performed on the proposed algorithm to deduce its enhanced false alarm and
detection probabilities compared to the conventional hard decision CSS.
Moreover, simulation analysis is used to confirm the theoretical claims and
prove the high performance of the proposed scheme compared to the conventional
CSS using different fusion rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03945</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03945</id><created>2015-10-13</created><authors><author><keyname>Chatzidimitriou</keyname><forenames>Dimitris</forenames></author><author><keyname>Raymond</keyname><forenames>Jean-Florent</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>An $O(\log OPT)$-approximation for covering and packing minor models of
  ${\theta}_r$</title><categories>cs.DS math.CO</categories><comments>Some of the results of this paper have been presented in WAOA 2015</comments><msc-class>05C35, 05C83, 05C85, 68R10, 68W25</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two graphs $G$ and $H$, we define ${\sf v}$-${\sf cover}_{H}(G)$ (resp.
${\sf e}$-${\sf cover}_{H}(G)$) as the minimum number of vertices (resp. edges)
whose removal from $G$ produces a graph without any minor isomorphic to ${H}$.
Also ${\sf v}$-${\sf pack}_{H}(G)$ (resp. ${\sf v}$-${\sf pack}_{H}(G)$) is the
maximum number of vertex- (resp. edge-) disjoint subgraphs of $G$ that contain
a minor isomaorphic to $H$. We denote by $\theta_{r}$ the graph with two
vertices and $r$ parallel edges between them. When $H=\theta_{r}$, the
parameters ${\sf v}$-${\sf cover}_{H}$, ${\sf e}$-${\sf cover}_{H}$, ${\sf
v}$-${\sf pack}_{H}$, and ${\sf v}$-${\sf pack}_{H}$ are {\sf NP}-hard to
compute (for sufficiently big values of~$r$). Drawing upon combinatorial
results in http://arxiv.org/abs/1510.03041, we give an algorithmic proof that
if ${\sf v}$-${\sf pack}_{{\theta_{r}}}(G)\leq k$, then ${\sf v}$-${\sf
cover}_{\theta_{r}}(G) = O(k\log k)$, and similarly for ${\sf v}$-${\sf
pack}_{\theta_{r}}$ and~ ${\sf e}$-${\sf cover}_{\theta_{r}}$. In other words,
the class of graphs containing ${\theta_{r}}$ as a minor has the vertex/edge
Erd\H{o}s-P\'osa property, for every positive integer $r$. Using the
algorithmic machinery of our proofs we introduce a unified approach for the
design of an $O(\log {\rm OPT})$-approximation algorithm for~${\sf v}$-${\sf
pack}_{{\theta_{r}}}$, ${\sf v}$-${\sf cover}_{{\theta_{r}}}$, ${\sf v}$-${\sf
pack}_{{\theta_{r}}}$, and ${\sf e}$-${\sf cover}_{{\theta_{r}}}$ that runs in
$O(n\cdot \log(n)\cdot m)$ steps. Also, we derive several new
Erd\H{o}s-P\'osa-type~results from the techniques that we introduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03947</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03947</id><created>2015-10-13</created><authors><author><keyname>Segarra</keyname><forenames>Santiago</forenames></author><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Distributed Linear Network Operators using Graph Filters</title><categories>cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of graph filters to implement arbitrary linear
transformations between graph signals. Graph filters can be represented by
matrix polynomials of the graph-shift operator, which captures the structure of
the graph and is assumed to be given. Thus, graph-filter design consists in
choosing the coefficients of these polynomials (known as filter coefficients)
to resemble desired linear transformations. Due to the local structure of the
graph-shift operator, graph filters can be implemented distributedly across
nodes, making them suitable for networked settings. We determine spectral
conditions under which a specific linear transformation can be implemented
perfectly using graph filters. Furthermore, for the cases where perfect
implementation is infeasible, the design of optimal approximations for
different error metrics is analyzed. We introduce the notion of a node-variant
graph filter, which allows the simultaneous implementation of multiple
(regular) graph filters in different nodes of the graph. This additional
flexibility enables the design of more general operators without undermining
the locality in implementation. Perfect and approximate implementation of
network operators is also studied for node-variant graph filters. We
demonstrate the practical relevance of the developed framework by studying in
detail the application of graph filters to the problems of finite-time
consensus and analog network coding. Finally, we present additional numerical
experiments comparing the performance of node-invariant and node-variant
filters when approximating arbitrary linear network operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03951</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03951</id><created>2015-10-13</created><authors><author><keyname>Kiamari</keyname><forenames>Mehrdad</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Are Generalized Cut-Set Bounds Tight for the Deterministic Interference
  Channel?</title><categories>cs.IT math.IT</categories><comments>Part of this work has been presented in the 53rd Annual Allerton
  Conference on Communication, Control, and Computing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the idea of extended networks, which is constructed by replicating
the users in the two-user deterministic interference channel (DIC) and
designing the interference structure among them, such that any rate that can be
achieved by each user in the original network can also be achieved
simultaneously by all replicas of that user in the extended network. We
demonstrate that by carefully designing extended networks and applying the
generalized cut-set (GCS) bound to them, we can derive a tight converse for the
two-user DIC. Furthermore, we generalize our techniques to the three-user DIC,
and demonstrate that the proposed approach also results in deriving a tight
converse for the three-user DIC in the symmetric case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03952</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03952</id><created>2015-10-13</created><updated>2015-10-27</updated><authors><author><keyname>Cui</keyname><forenames>Hongyan</forenames></author><author><keyname>Wu</keyname><forenames>Yuxiao</forenames></author><author><keyname>Sobolevskv</keyname><forenames>Stanislav</forenames></author><author><keyname>Xu</keyname><forenames>Shuai</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Exploring Invariants &amp; Patterns in Human Commute time</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In everyday life, the process of commuting to work from home happens every
now and then. And the research of commute characteristics is useful for urban
function planning. For humans, the commute of an individual seems revealing no
regular universal patterns, but it is true that people try to find a
satisfactory state of life regarding commute issues. Commute time and distance
are most important indicators to measure the degree of this satisfaction.
Marchetti states a certain regularity in human commute time distribution -
specifically, it states that no matter when, where and how far away people
live, they always tend to spend approximately the same average time for their
daily commute. However, will the rapid development of cities nowadays as well
as serious challenges brought by economic development affect this constant? If
there are novel characteristics? We revisit these problems using fine grained
communication data in two Chinese major cities during recent two years. The
results indicate that the commute time has been slightly increased from
Marchetti's constant with the development of society. People's overall travel
budgets have been increased, more concretely speaking, for medium and long
distance commuters, their endurance limit for commute time is enhanced during
the passing years, and fluctuates around a constant; for short distance
commuters, their commute time increases with the distance. Moreover, the
population distribution in every commute distance shows strong cross-city
similarity and does not change much over two years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03955</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03955</id><created>2015-10-13</created><authors><author><keyname>Ransford</keyname><forenames>Benjamin</forenames></author><author><keyname>Ceze</keyname><forenames>Luis</forenames></author></authors><title>SAP: an Architecture for Selectively Approximate Wireless Communication</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrity checking is ubiquitous in data networks, but not all network
traffic needs integrity protection. Many applications can tolerate slightly
damaged data while still working acceptably, trading accuracy versus efficiency
to save time and energy. Such applications should be able to receive damaged
data if they so desire. In today's network stacks, lower-layer integrity checks
discard damaged data regardless of the application's wishes, violating the
End-to-End Principle. This paper argues for optional integrity checking and
gently redesigns a commodity network architecture to support
integrity-unprotected data. Our scheme, called Selective Approximate Protocol
(SAP), allows applications to coordinate multiple network layers to accept
potentially damaged data. Unlike previous schemes that targeted video or media
streaming, SAP is generic. SAP's improved throughput and decreased
retransmission rate is a good match for applications in the domain of
approximate computing.
  Implemented atop WiFi as a case study, SAP works with existing physical
layers and requires no hardware changes. SAP's benefits increase as channel
conditions degrade. In tests of an error-tolerant file-transfer application
over WiFi, SAP sped up transmission by about 30% on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03971</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03971</id><created>2015-10-14</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Quality-Aware Popularity Based Bandwidth Allocation for Scalable Video
  Broadcast over Wireless Access Networks</title><categories>cs.NI cs.MM</categories><comments>9 pages, journal paper (Accepted) in Journal of Internet Technology,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video broadcast/multicast over wireless access networks is an attractive
research issue in the field of wireless communication. With the rapid
improvement of various wireless network technologies, it is now possible to
provide high quality video transmission over wireless networks. The high
quality video streams need higher bandwidth. Hence, during the video
transmission through wireless networks, it is very important to make the best
utilization of the limited bandwidth. Therefore, when many broadcasting video
sessions are active, the bandwidth per video session can be allocated based on
popularity of the video sessions (programs). Instead of allocating equal
bandwidth to each of them, our proposed scheme allocates bandwidth per
broadcasting video session based on popularity of the video program. When the
system bandwidth is not sufficient to allocate the demanded bandwidth for all
the active video sessions, our proposed scheme efficiently allocates the total
system bandwidth among all the scalable active video sessions in such a way
that higher bandwidth is allocated to higher popularity one. Using the
mathematical and simulation analyses, we show that the proposed scheme
maximizes the average user satisfaction level and achieves the best utilization
of bandwidth. The simulation results indicate that a large number of
subscribers can receive a significantly improved quality of video. To improve
the video quality for large number of subscribers, the only tradeoff is that a
very few subscribers receive slightly degraded video quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03973</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03973</id><created>2015-10-14</created><authors><author><keyname>Chowdhury</keyname><forenames>Mostafa Zaman</forenames></author><author><keyname>Hossain</keyname><forenames>Mohammad Arif</forenames></author><author><keyname>Ahmed</keyname><forenames>Shakil</forenames></author><author><keyname>Jang</keyname><forenames>Yeong Min</forenames></author></authors><title>Radio Resource Management Based on Reused Frequency Allocation for
  Dynamic Channel Borrowing Scheme in Wireless Networks</title><categories>cs.NI cs.MM</categories><comments>Journal paper (Wireless Networks, Online published)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the modern era, cellular communication consumers are exponentially
increasing as they find the system more user-friendly. Due to enormous users
and their numerous demands, it has become a mandate to make the best use of the
limited radio resources that assures the highest standard of Quality of Service
(QoS). To reach the guaranteed level of QoS for the maximum number of users,
maximum utilization of bandwidth is not only the key issue to be considered,
rather some other factors like interference, call blocking probability etc. are
also needed to keep under deliberation. The lower performances of these factors
may retrograde the overall cellular networks performances. Keeping these
difficulties under consideration, we propose an effective dynamic channel
borrowing model that safeguards better QoS, other factors as well. The proposed
scheme reduces the excessive overall call blocking probability and does
interference mitigation without sacrificing bandwidth utilization. The proposed
scheme is modeled in such a way that the cells are bifurcated after the channel
borrowing process if the borrowed channels have the same type of frequency band
(i.e. reused frequency). We also propose that the unoccupied interfering
channels of adjacent cells can also be inactivated, instead of cell bifurcation
for interference mitigation. The simulation endings show satisfactory
performances in terms of overall call blocking probability and bandwidth
utilization that are compared to the conventional scheme without channel
borrowing. Furthermore, signal to interference plus noise ratio (SINR) level,
capacity, and outage probability are compared to the conventional scheme
without interference mitigation after channel borrowing that may attract the
considerable concentration to the operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03979</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03979</id><created>2015-10-14</created><authors><author><keyname>Wang</keyname><forenames>Limin</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Guo</keyname><forenames>Sheng</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author></authors><title>Better Exploiting OS-CNNs for Better Event Recognition in Images</title><categories>cs.CV</categories><comments>8 pages. This work is following our previous work:
  http://arxiv.org/abs/1505.00296</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event recognition from still images is one of the most important problems for
image understanding. However, compared with object recognition and scene
recognition, event recognition has received much less research attention in
computer vision community. This paper addresses the problem of cultural event
recognition in still images and focuses on applying deep learning methods on
this problem. In particular, we utilize the successful architecture of
Object-Scene Convolutional Neural Networks (OS-CNNs) to perform event
recognition. OS-CNNs are composed of object nets and scene nets, which transfer
the learned representations from the pre-trained models on large-scale object
and scene recognition datasets, respectively. We propose four types of
scenarios to explore OS-CNNs for event recognition by treating them as either
&quot;end-to-end event predictors&quot; or &quot;generic feature extractors&quot;. Our experimental
results demonstrate that the global and local representations of OS-CNNs are
complementary to each other. Finally, based on our investigation of OS-CNNs, we
come up with a solution for the cultural event recognition track at the ICCV
ChaLearn Looking at People (LAP) challenge 2015. Our team secures the third
place at this challenge and our result is very close to the best performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03989</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03989</id><created>2015-10-14</created><authors><author><keyname>Cruz-Filipe</keyname><forenames>Lu&#xed;s</forenames></author><author><keyname>Franz</keyname><forenames>Michael</forenames></author><author><keyname>Hakhverdyan</keyname><forenames>Artavazd</forenames></author><author><keyname>Ludovico</keyname><forenames>Marta</forenames></author><author><keyname>Nunes</keyname><forenames>Isabel</forenames></author><author><keyname>Schneider-Kamp</keyname><forenames>Peter</forenames></author></authors><title>repAIrC: A Tool for Ensuring Data Consistency by Means of Active
  Integrity Constraints</title><categories>cs.DB</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistency of knowledge repositories is of prime importance in organization
management. Integrity constraints are a well-known vehicle for specifying data
consistency requirements in knowledge bases; in particular, active integrity
constraints go one step further, allowing the specification of preferred ways
to overcome inconsistent situations in the context of database management. This
paper describes a tool to validate an SQL database with respect to a given set
of active integrity constraints, proposing possible repairs in case the
database is inconsistent. The tool is able to work with the different kinds of
repairs proposed in the literature, namely simple, founded, well-founded and
justified repairs. It also implements strategies for parallelizing the search
for them, allowing the user both to compute partitions of independent or
stratified active integrity constraints, and to apply these partitions to find
repairs of inconsistent databases efficiently in parallel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03994</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03994</id><created>2015-10-14</created><authors><author><keyname>Karandikar</keyname><forenames>Prateek</forenames></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames></author></authors><title>Decidability in the logic of subsequences and supersequences</title><categories>cs.LO cs.FL</categories><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider first-order logics of sequences ordered by the subsequence
ordering, aka sequence embedding. We show that the \Sigma_2 theory is
undecidable, answering a question left open by Kuske. Regarding fragments with
a bounded number of variables, we show that the FO2 theory is decidable while
the FO3 theory is undecidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.03998</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.03998</id><created>2015-10-14</created><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Otachi</keyname><forenames>Yota</forenames></author><author><keyname>&#x160;ejnoha</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>On the Classes of Interval Graphs of Limited Nesting and Count of
  Lengths</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1969, Roberts introduced proper and unit interval graphs and proved that
these classes are equal. Natural generalizations of the second class were
considered in which the number of different lengths of intervals is limited by
k. Not much is known about them and the long-standing open problem asks how
hard is recognition even for k=2. In this paper, we show that a generalization
of recognition called partial representation extension is NP-hard even for k=2,
while a quadratic-time algorithm is known for unit interval graphs.
  We propose generalizations of proper interval graphs called k-nested interval
graphs in which there are no chains of k+1 intervals nested in each other. We
give a linear-time algorithm for recognition and a polynomial-time algorithm
for partial representation extension. These results extend a list of examples,
where problems involving geometry (k lengths) are much harder than similar
problems involving topology (nesting k).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04000</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04000</id><created>2015-10-14</created><authors><author><keyname>Carayol</keyname><forenames>Arnaud</forenames></author><author><keyname>Serre</keyname><forenames>Olivier</forenames></author></authors><title>Marking Shortest Paths On Pushdown Graphs Does Not Preserve MSO
  Decidability</title><categories>cs.FL</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider pushdown graphs, i.e. infinite graphs that can be
described as transition graphs of deterministic real-time pushdown automata. We
consider the case where some vertices are designated as being final and we
built, in a breadth-first manner, a marking of edges that lead to such vertices
(i.e., for every vertex that can reach a final one, we mark all out-going edges
laying on some shortest path to a final vertex).
  Our main result is that the edge-marked version of a pushdown graph may
itself no longer be a pushdown graph, as we prove that this enrich graph may
have an undecidable MSO theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04004</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04004</id><created>2015-10-14</created><authors><author><keyname>Nasihatkon</keyname><forenames>Behrooz</forenames></author><author><keyname>Kahl</keyname><forenames>Fredrik</forenames></author></authors><title>Multiresolution Search of the Rigid Motion Space for Intensity Based
  Registration</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relation between the target functions of low-resolution and
high-resolution intensity-based registration for the class of rigid
transformations. Our results show that low resolution target values can tightly
bound the high-resolution target function in natural images. This can help with
analyzing and better understanding the process of multiresolution image
registration. It also gives a guideline for designing multiresolution
algorithms in which the search space in higher resolution registration is
restricted given the fitness values for lower resolution image pairs. To
demonstrate this, we incorporate our multiresolution technique into a Lipschitz
global optimization framework. We show that using the multiresolution scheme
can result in large gains in the efficiency of such algorithms. The method is
evaluated by applying to 2D and 3D registration problems as well as the
detection of reflective symmetry in 2D and 3D images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04007</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04007</id><created>2015-10-14</created><authors><author><keyname>Wu</keyname><forenames>Xiugang</forenames></author><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author></authors><title>Cut-Set Bound Is Loose for Gaussian Relay Networks</title><categories>cs.IT math.IT</categories><comments>Presented at Allerton 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cut-set bound developed by Cover and El Gamal in 1979 has since remained
the best known upper bound on the capacity of the Gaussian relay channel. We
develop a new upper bound on the capacity of the Gaussian primitive relay
channel which is tighter than the cut-set bound. Our proof is based on
typicality arguments and concentration of Gaussian measure. Combined with a
simple tensorization argument proposed by Courtade and Ozgur in 2015, our
result also implies that the current capacity approximations for Gaussian relay
networks, which have linear gap to the cut-set bound in the number of nodes,
are order-optimal and leads to a lower bound on the pre-constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04015</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04015</id><created>2015-10-14</created><authors><author><keyname>Li</keyname><forenames>Jiawei</forenames></author></authors><title>On Equilibria of N-seller and N-buyer Bargaining Games</title><categories>cs.GT</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A group of players that contains n sellers and n buyers bargain over the
partitions of n pies. A seller(/buyer) has to reach an agreement with a buyer
(/seller) on the division of a pie. The players bargain in a system like the
stock market: each seller(buyer) can either offer a selling(buying) price to
all buyers(sellers) or accept a price offered by another buyer(seller). The
offered prices are known to all. Once a player accepts a price offered by
another one, the division of a pie between them is determined. Each player has
a constant discounting factor and the discounting factors of all players are
common knowledge. In this article, we prove that the equilibrium of this
bargaining problem is a unanimous division rate that is exactly equivalent to
Nash bargaining equilibrium of a two-player bargaining game in which the
discounting factors of two players are the average of n buyers and the average
of n sellers respectively. This result is nontrivial for studying general
equilibrium of markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04016</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04016</id><created>2015-10-14</created><updated>2015-12-09</updated><authors><author><keyname>Chatterjee</keyname><forenames>Kingshuk</forenames></author><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author></authors><title>Multi-head Watson-Crick automata</title><categories>cs.FL</categories><comments>arXiv admin note: text overlap with arXiv:1507.05284,
  arXiv:1510.02070</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by multi-head finite automata and Watson-Crick automata in this
paper, we introduce new structure namely multi-head Watson-Crick automata where
we replace the single tape of multi-head finite automaton by a DNA double
strand. The content of the second tape is determined using a complementarity
relation similar to Watson-Crick complementarity relation. We establish the
superiority of our model over multi-head finite automata and also show that
both the deterministic and non-deterministic variant of the model can accept
non-regular unary languages. We also compare our model with parallel
communicating Watson-Crick automata systems and prove that both of them have
the same computational power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04017</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04017</id><created>2015-10-14</created><authors><author><keyname>H&#xe9;der</keyname><forenames>Mih&#xe1;ly</forenames></author><author><keyname>Tenczer</keyname><forenames>Szabolcs</forenames></author><author><keyname>Biancini</keyname><forenames>Andrea</forenames></author></authors><title>Collaboration between SAML Federations and OpenStack Clouds</title><categories>cs.SE cs.NI</categories><comments>12 pages including 2 figures. Source code at:
  https://github.com/burgosz/openstack-horizon-shibboleth</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the design process of a novel solution for enabling
the collaboration between OpenStack cloud systems in SAML federations with
standalone attribute authorities, such as national research and education
federations or eduGAIN. The software solution that realizes the integration of
systems serves as a case study to show how abstract desirable engineering
properties fixed at the beginning of the design process can be implemented
during the development phase. An analysis of earlier generations of
OpenStack-related developments trying to tackle the same problem is given. Many
aspects of this software integration can be generalized to serve as a template
for federative cloud access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04026</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04026</id><created>2015-10-14</created><authors><author><keyname>Wang</keyname><forenames>Huandong</forenames></author><author><keyname>Xu</keyname><forenames>Fengli</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Pengyu</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author></authors><title>Understanding Mobile Traffic Patterns of Large Scale Cellular Towers in
  Urban Environment</title><categories>cs.NI</categories><comments>To appear at IMC 2015</comments><acm-class>C.2.0; C.4</acm-class><doi>10.1145/2815675.2815680</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding mobile traffic patterns of large scale cellular towers in urban
environment is extremely valuable for Internet service providers, mobile users,
and government managers of modern metropolis. This paper aims at extracting and
modeling the traffic patterns of large scale towers deployed in a metropolitan
city. To achieve this goal, we need to address several challenges, including
lack of appropriate tools for processing large scale traffic measurement data,
unknown traffic patterns, as well as handling complicated factors of urban
ecology and human behaviors that affect traffic patterns. Our core contribution
is a powerful model which combines three dimensional information (time,
locations of towers, and traffic frequency spectrum) to extract and model the
traffic patterns of thousands of cellular towers. Our empirical analysis
reveals the following important observations. First, only five basic
time-domain traffic patterns exist among the 9,600 cellular towers. Second,
each of the extracted traffic pattern maps to one type of geographical
locations related to urban ecology, including residential area, business
district, transport, entertainment, and comprehensive area. Third, our
frequency-domain traffic spectrum analysis suggests that the traffic of any
tower among the 9,600 can be constructed using a linear combination of four
primary components corresponding to human activity behaviors. We believe that
the proposed traffic patterns extraction and modeling methodology, combined
with the empirical analysis on the mobile traffic, pave the way toward a deep
understanding of the traffic patterns of large scale cellular towers in modern
metropolis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04029</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04029</id><created>2015-10-14</created><authors><author><keyname>Kroher</keyname><forenames>Nadine</forenames></author><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>Jos&#xe9;-Miguel</forenames></author><author><keyname>Mora</keyname><forenames>Joaquin</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Emilia</forenames></author></authors><title>Corpus COFLA: A research corpus for the Computational study of Flamenco
  Music</title><categories>cs.SD cs.IR</categories><comments>24 pages, submitted to the ACM Journal of Computing and Cultural
  Heritage</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flamenco is a music tradition from Southern Spain which attracts a growing
community of enthusiasts around the world. Its unique melodic and rhythmic
elements, the typically spontaneous and improvised interpretation and its
diversity regarding styles make this still largely undocumented art form a
particularly interesting material for musicological studies. In prior works it
has already been demonstrated that research on computational analysis of
flamenco music, despite it being a relatively new field, can provide powerful
tools for the discovery and diffusion of this genre. In this paper we present
corpusCOFLA, a data framework for the development of such computational tools.
The proposed collection of audio recordings and meta-data serves as a pool for
creating annotated subsets which can be used in development and evaluation of
algorithms for specific music information retrieval tasks. First, we describe
the design criteria for the corpus creation and then provide various examples
of subsets drawn from the corpus. We showcase possible research applications in
the context of computational study of flamenco music and give perspectives
regarding further development of the corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04031</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04031</id><created>2015-10-14</created><authors><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Cozza</keyname><forenames>Vittoria</forenames></author><author><keyname>Petrocchi</keyname><forenames>Marinella</forenames></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames></author></authors><title>TRAP: using TaRgeted Ads to unveil Google personal Profiles</title><categories>cs.SI cs.CR cs.CY</categories><comments>7th IEEE International Workshop on Information Forensics and Security
  (WIFS) 2015. 6 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In the last decade, the advertisement market spread significantly in the web
and mobile app system. Its effectiveness is also due thanks to the possibility
to target the advertisement on the specific interests of the actual user, other
than on the content of the website hosting the advertisement. In this scenario,
became of great value services that collect and hence can provide information
about the browsing user, like Facebook and Google. In this paper, we show how
to maliciously exploit the Google Targeted Advertising system to infer personal
information in Google user profiles. In particular, the attack we consider is
external from Google and relies on combining data from Google AdWords with
other data collected from a website of the Google Display Network. We validate
the effectiveness of our proposed attack, also discussing possible application
scenarios. The result of our research shows a significant practical privacy
issue behind such type of targeted advertising service, and call for further
investigation and the design of more privacy-aware solutions, possibly without
impeding the current business model involved in online advertisement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04035</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04035</id><created>2015-10-14</created><updated>2015-12-12</updated><authors><author><keyname>Balaji</keyname><forenames>Nikhil</forenames></author><author><keyname>Datta</keyname><forenames>Samir</forenames></author><author><keyname>Ganesan</keyname><forenames>Venkatesh</forenames></author></authors><title>Counting Euler Tours in Undirected Bounded Treewidth Graphs</title><categories>cs.CC</categories><comments>17 pages; There was an error in the proof of the GapL upper bound
  claimed in the previous version which has been subsequently removed</comments><acm-class>F.1.1; F.1.3; G.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show that counting Euler tours in undirected bounded tree-width graphs is
tractable even in parallel - by proving a $\#SAC^1$ upper bound. This is in
stark contrast to #P-completeness of the same problem in general graphs.
  Our main technical contribution is to show how (an instance of) dynamic
programming on bounded \emph{clique-width} graphs can be performed efficiently
in parallel. Thus we show that the sequential result of Espelage, Gurski and
Wanke for efficiently computing Hamiltonian paths in bounded clique-width
graphs can be adapted in the parallel setting to count the number of
Hamiltonian paths which in turn is a tool for counting the number of Euler
tours in bounded tree-width graphs. Our technique also yields parallel
algorithms for counting longest paths and bipartite perfect matchings in
bounded-clique width graphs.
  While establishing that counting Euler tours in bounded tree-width graphs can
be computed by non-uniform monotone arithmetic circuits of polynomial degree
(which characterize $\#SAC^1$) is relatively easy, establishing a uniform
$\#SAC^1$ bound needs a careful use of polynomial interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04039</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04039</id><created>2015-10-14</created><authors><author><keyname>Kroher</keyname><forenames>Nadine</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Emilia</forenames></author></authors><title>Automatic Transcription of Flamenco Singing from Polyphonic Music
  Recordings</title><categories>cs.SD cs.IR</categories><comments>Submitted to the IEEE Transactions on Audio, Speech and Language
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic note-level transcription is considered one of the most challenging
tasks in music information retrieval. The specific case of flamenco singing
transcription poses a particular challenge due to its complex melodic
progressions, intonation inaccuracies, the use of a high degree of
ornamentation and the presence of guitar accompaniment. In this study, we
explore the limitations of existing state of the art transcription systems for
the case of flamenco singing and propose a specific solution for this genre: We
first extract the predominant melody and apply a novel contour filtering
process to eliminate segments of the pitch contour which originate from the
guitar accompaniment. We formulate a set of onset detection functions based on
volume and pitch characteristics to segment the resulting vocal pitch contour
into discrete note events. A quantised pitch label is assigned to each note
event by combining global pitch class probabilities with local pitch contour
statistics. The proposed system outperforms state of the art singing
transcription systems with respect to voicing accuracy, onset detection and
overall performance when evaluated on flamenco singing datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04049</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04049</id><created>2015-10-14</created><updated>2015-10-15</updated><authors><author><keyname>Park</keyname><forenames>Kunwoo</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author><author><keyname>Lee</keyname><forenames>Chul</forenames></author></authors><title>Persistent Sharing of Fitness App Status on Twitter</title><categories>cs.SI</categories><comments>11 pages, CSCW'16</comments><acm-class>H.5.3; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the world becomes more digitized and interconnected, information that was
once considered to be private such as one's health status is now being shared
publicly. To understand this new phenomenon better, it is crucial to study what
types of health information are being shared on social media and why, as well
as by whom. In this paper, we study the traits of users who share their
personal health and fitness related information on social media by analyzing
fitness status updates that MyFitnessPal users have shared via Twitter. We
investigate how certain features like user profile, fitness activity, and
fitness network in social media can potentially impact the long-term engagement
of fitness app users. We also discuss implications of our findings to achieve a
better retention of these users and to promote more sharing of their status
updates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04053</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04053</id><created>2015-10-14</created><authors><author><keyname>Bobenko</keyname><forenames>Alexander</forenames></author><author><keyname>Dimitrov</keyname><forenames>Nikolay</forenames></author><author><keyname>Sechelmann</keyname><forenames>Stefan</forenames></author></authors><title>Discrete uniformization of finite branched covers over the Riemann
  sphere via hyper-ideal circle patterns</title><categories>math.MG cs.CG math.DG math.GT</categories><comments>43 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the help of hyper-ideal circle pattern theory, we have developed a
discrete version of the classical uniformization theorems for surfaces
represented as finite branched covers over the Riemann sphere as well as
compact polyhedral surfaces with non-positive curvature. We show that in the
case of such surfaces discrete uniformization via hyper-ideal circle patterns
always exists and is unique. We also propose a numerical algorithm, utilizing
convex optimization, that constructs the desired discrete uniformization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04068</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04068</id><created>2015-10-14</created><authors><author><keyname>B&#xe9;dorf</keyname><forenames>Jeroen</forenames></author><author><keyname>Gaburov</keyname><forenames>Evghenii</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author></authors><title>Sapporo2: A versatile direct $N$-body library</title><categories>astro-ph.IM cs.MS</categories><comments>15 pages, 7 figures. Accepted for publication in Computational
  Astrophysics and Cosmology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Astrophysical direct $N$-body methods have been one of the first production
algorithms to be implemented using NVIDIA's CUDA architecture. Now, almost
seven years later, the GPU is the most used accelerator device in astronomy for
simulating stellar systems. In this paper we present the implementation of the
Sapporo2 $N$-body library, which allows researchers to use the GPU for $N$-body
simulations with little to no effort. The first version, released five years
ago, is actively used, but lacks advanced features and versatility in numerical
precision and support for higher order integrators. In this updated version we
have rebuilt the code from scratch and added support for OpenCL,
multi-precision and higher order integrators. We show how to tune these codes
for different GPU architectures and present how to continue utilizing the GPU
optimal even when only a small number of particles ($N &lt; 100$) is integrated.
This careful tuning allows Sapporo2 to be faster than Sapporo1 even with the
added options and double precision data loads. The code runs on a range of
NVIDIA and AMD GPUs in single and double precision accuracy. With the addition
of OpenCL support the library is also able to run on CPUs and other
accelerators that support OpenCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04074</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04074</id><created>2015-10-14</created><authors><author><keyname>George</keyname><forenames>Marian</forenames></author><author><keyname>Mircic</keyname><forenames>Dejan</forenames></author><author><keyname>S&#xf6;r&#xf6;s</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Floerkemeier</keyname><forenames>Christian</forenames></author><author><keyname>Mattern</keyname><forenames>Friedemann</forenames></author></authors><title>Fine-Grained Product Class Recognition for Assisted Shopping</title><categories>cs.CV</categories><comments>Accepted at ICCV Workshop on Assistive Computer Vision and Robotics
  (ICCV-ACVR) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assistive solutions for a better shopping experience can improve the quality
of life of people, in particular also of visually impaired shoppers. We present
a system that visually recognizes the fine-grained product classes of items on
a shopping list, in shelves images taken with a smartphone in a grocery store.
Our system consists of three components: (a) We automatically recognize useful
text on product packaging, e.g., product name and brand, and build a mapping of
words to product classes based on the large-scale GroceryProducts dataset. When
the user populates the shopping list, we automatically infer the product class
of each entered word. (b) We perform fine-grained product class recognition
when the user is facing a shelf. We discover discriminative patches on product
packaging to differentiate between visually similar product classes and to
increase the robustness against continuous changes in product design. (c) We
continuously improve the recognition accuracy through active learning. Our
experiments show the robustness of the proposed method against cross-domain
challenges, and the scalability to an increasing number of products with
minimal re-training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04080</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04080</id><created>2015-10-14</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Dumont</keyname><forenames>Louis</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author></authors><title>Algebraic Diagonals and Walks</title><categories>cs.SC</categories><comments>The final version of this paper has been published in the proceedings
  ISSAC 2015, in ISSAC'15 International Symposium on Symbolic and Algebraic
  Computation. Bath, United Kingdom - July 06-09, 2015. ACM New York, NY, USA</comments><doi>10.1145/2755996.2756663</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
Generically, it is its minimal polynomial and is obtained in time quasi-linear
in its size. We show that this minimal polynomial has an exponential size with
respect to the degree of the input rational function. We then address the
related problem of enumerating directed lattice walks. The insight given by our
study leads to a new method for expanding the generating power series of
bridges, excursions and meanders. We show that their first N terms can be
computed in quasi-linear complexity in N, without first computing a very large
polynomial equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04083</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04083</id><created>2015-10-14</created><updated>2016-01-31</updated><authors><author><keyname>Onwuzurike</keyname><forenames>Lucky</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author></authors><title>Experimental Analysis of Popular Smartphone Apps Offering Anonymity,
  Ephemerality, and End-to-End Encryption</title><categories>cs.CR cs.SI</categories><comments>A preliminary version of this paper appears in the Proceedings of the
  2016 NDSS Workshop on Understanding and Enhancing Online Privacy (UEOP). This
  is the full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As social networking takes to the mobile world, smartphone apps provide users
with ever-changing ways to interact with each other. Over the past couple of
years, an increasing number of apps have entered the market offering end-to-end
encryption, self-destructing messages, or some degree of anonymity. However,
little work thus far has examined the properties they offer.
  To this end, this paper presents a taxonomy of 18 of these apps: we first
look at the features they promise in their appeal to broaden their reach and
focus on 8 of the more popular ones. We present a technical evaluation, based
on static and dynamic analysis, and identify a number of gaps between the
claims and reality of their promises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04099</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04099</id><created>2015-10-14</created><authors><author><keyname>Huang</keyname><forenames>Lingxiao</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>Canonical Paths for MCMC: from Art to Science</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Chain Monte Carlo (MCMC) method is a widely used algorithm design
scheme with many applications. To make efficient use of this method, the key
step is to prove that the Markov chain is rapid mixing. Canonical paths is one
of the two main tools to prove rapid mixing. However, there are much fewer
success examples comparing to coupling, the other main tool. The main reason is
that there is no systematic approach or general recipe to design canonical
paths. Building up on a previous exploration by McQuillan, we develop a general
theory to design canonical paths for MCMC: We reduce the task of designing
canonical paths to solving a set of linear equations, which can be
automatically done even by a machine.
  Making use of this general approach, we obtain fully polynomial-time
randomized approximation schemes (FPRAS) for counting the number of
$b$-matching with $b\leq 7$ and $b$-edge-cover with $b\leq 2$. They are natural
generalizations of matchings and edge covers for graphs. No polynomial time
approximation was previously known for these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04104</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04104</id><created>2015-09-28</created><authors><author><keyname>Kaltenbacher</keyname><forenames>Simon</forenames></author><author><keyname>Kirk</keyname><forenames>Nicholas H.</forenames></author><author><keyname>Lee</keyname><forenames>Dongheui</forenames></author></authors><title>A Preliminary Study on the Learning Informativeness of Data Subsets</title><categories>cs.CL cs.RO</categories><comments>The 8th International Workshop on Human-Friendly Robotics (HFR 2015),
  Munich, Germany</comments><doi>10.13140/RG.2.1.2213.9361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the internal state of a robotic system is complex: this is
performed from multiple heterogeneous sensor inputs and knowledge sources.
Discretization of such inputs is done to capture saliences, represented as
symbolic information, which often presents structure and recurrence. As these
sequences are used to reason over complex scenarios, a more compact
representation would aid exactness of technical cognitive reasoning
capabilities, which are today constrained by computational complexity issues
and fallback to representational heuristics or human intervention. Such
problems need to be addressed to ensure timely and meaningful human-robot
interaction. Our work is towards understanding the variability of learning
informativeness when training on subsets of a given input dataset. This is in
view of reducing the training size while retaining the majority of the symbolic
learning potential. We prove the concept on human-written texts, and conjecture
this work will reduce training data size of sequential instructions, while
preserving semantic relations, when gathering information from large remote
sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04121</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04121</id><created>2015-10-14</created><authors><author><keyname>Kurganskyy</keyname><forenames>Oleksiy</forenames></author><author><keyname>Potapov</keyname><forenames>Igor</forenames></author></authors><title>Reachability problems for PAMs</title><categories>cs.NA cs.DS cs.SY</categories><comments>16 pages</comments><msc-class>54H20, 37E05, 68Q25, 68Q60</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Piecewise affine maps (PAMs) are frequently used as a reference model to show
the openness of the reachability questions in other systems. The reachability
problem for one-dimentional PAM is still open even if we define it with only
two intervals. As the main contribution of this paper we introduce new
techniques for solving reachability problems based on p-adic norms and weights
as well as showing decidability for two classes of maps. Then we show the
connections between topological properties for PAM's orbits, reachability
problems and representation of numbers in a rational base system. Finally we
show a particular instance where the uniform distribution of the original orbit
may not remain uniform or even dense after making regular shifts and taking a
fractional part in that sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04122</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04122</id><created>2015-10-14</created><authors><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author><author><keyname>Tsitsvero</keyname><forenames>Mikhail</forenames></author></authors><title>Eigenfunctions of Underspread Linear Communication Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the eigenfunctions can be found exactly for
systems whose delay-Doppler spread function is concentrated along a straight
line and they can be found in approximate sense for systems having a spread
function maximally concentrated in regions of the Doppler-delay plane whose
area is smaller than one. The interesting results are that: i) the
instantaneous frequency of the eigenfunctions is dictated by the contour level
of the time-varying transfer function; ii) the eigenvalues are restricted
between the minimum and maximum value of the system time-varying transfer
function, but not all values are possible, as the system exhibits an inherent
quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04130</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04130</id><created>2015-10-14</created><authors><author><keyname>Fowkes</keyname><forenames>Jaroslav</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>A Bayesian Network Model for Interesting Itemsets</title><categories>stat.ML cs.DB cs.LG</categories><comments>Supplementary material attached as Ancillary File</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining itemsets that are the most interesting under a statistical model of
the underlying data is a frequently used and well-studied technique for
exploratory data analysis. The most recent models of interestingness are
predominantly based on maximum entropy distributions over items or tile entries
with itemset constraints, and while computationally tractable are not easily
interpretable. We therefore propose the first, to the best of our knowledge,
generative model over itemsets, in the form of a Bayesian network, and an
associated novel measure of interestingness. Our model is able to efficiently
infer interesting itemsets directly from the transaction database using
structural EM, in which the E-step employs the greedy approximation to weighted
set cover. Our approach is theoretically simple, straightforward to implement,
trivially parallelizable and exhibits competitive performance as we demonstrate
on both synthetic and real-world examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04132</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04132</id><created>2015-10-14</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author></authors><title>Connected Big Data Measurement</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we have summarized how resilient Big Data monetization scheme
outperforms state-of-the art schemes by maintaining a balance between CDS size
and routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04149</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04149</id><created>2015-10-14</created><authors><author><keyname>Paul</keyname><forenames>Saurabh</forenames></author><author><keyname>Magdon-Ismail</keyname><forenames>Malik</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>Column Selection via Adaptive Sampling</title><categories>cs.DS math.NA</categories><comments>To Appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selecting a good column (or row) subset of massive data matrices has found
many applications in data analysis and machine learning. We propose a new
adaptive sampling algorithm that can be used to improve any relative-error
column selection algorithm. Our algorithm delivers a tighter theoretical bound
on the approximation error which we also demonstrate empirically using two well
known relative-error column subset selection algorithms. Our experimental
results on synthetic and real-world data show that our algorithm outperforms
non-adaptive sampling as well as prior adaptive sampling approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04160</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04160</id><created>2015-10-14</created><authors><author><keyname>Simmhan</keyname><forenames>Yogesh</forenames></author><author><keyname>Shukla</keyname><forenames>Anshu</forenames></author><author><keyname>Verma</keyname><forenames>Arun</forenames></author></authors><title>Benchmarking Fast Data Platforms for the Aadhaar Biometric Database</title><categories>cs.DC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aadhaar is the world's largest biometric database with a billion records,
being compiled as an identity platform to deliver social services to residents
of India.Aadhaar processes streams of biometric data as residents are enrolled
and updated.Besides 1 million enrolments and updates per day,up to 100 million
daily biometric authentications are expected during delivery of various public
services.These form critical Big Data applications,with large volumes and high
velocity of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04163</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04163</id><created>2015-10-14</created><authors><author><keyname>Neiswanger</keyname><forenames>Willie</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author></authors><title>Embarrassingly Parallel Variational Inference in Nonconjugate Models</title><categories>stat.ML cs.AI cs.DC cs.LG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a parallel variational inference (VI) procedure for use in
data-distributed settings, where each machine only has access to a subset of
data and runs VI independently, without communicating with other machines. This
type of &quot;embarrassingly parallel&quot; procedure has recently been developed for
MCMC inference algorithms; however, in many cases it is not possible to
directly extend this procedure to VI methods without requiring certain
restrictive exponential family conditions on the form of the model.
Furthermore, most existing (nonparallel) VI methods are restricted to use on
conditionally conjugate models, which limits their applicability. To combat
these issues, we make use of the recently proposed nonparametric VI to
facilitate an embarrassingly parallel VI procedure that can be applied to a
wider scope of models, including to nonconjugate models. We derive our
embarrassingly parallel VI algorithm, analyze our method theoretically, and
demonstrate our method empirically on a few nonconjugate models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04165</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04165</id><created>2015-10-14</created><authors><author><keyname>Li</keyname><forenames>Xueliang</forenames></author><author><keyname>Gallagher</keyname><forenames>John P.</forenames></author></authors><title>A Top-to-Bottom View: Energy Analysis for Mobile Application Source Code</title><categories>cs.OH</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency significantly influences user experience of battery-driven
devices such as smartphones and tablets. The goal of an energy model of source
code is to lay a foundation for energy-saving techniques from architecture to
software development. The challenge is linking hardware energy consumption to
the high-level application source code, considering the complex run-time
context, such as thread scheduling, user inputs and the abstraction of the
virtual machine. Traditional energy modeling is bottom-to-top, but this
approach faces obstacles when software consists of a number of abstract layers.
In this paper, we propose a top-to-bottom view. We focus on identifying
valuable information from the source code, which results in the idea of
utilizing an intermediate representation, &quot;energy operation&quot;, to capture the
energy characteristics. The experiment results show that the energy model at
such a high-level can reduce the error margin to within 10% and enable energy
breakdown at function-level, which helps developers understand the
energy-related features of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04183</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04183</id><created>2015-10-14</created><authors><author><keyname>Terletskyi</keyname><forenames>D. O.</forenames></author><author><keyname>Provotar</keyname><forenames>O. I.</forenames></author></authors><title>Mathematical Foundations for Designing and Development of Intelligent
  Systems of Information Analysis</title><categories>cs.AI</categories><acm-class>I.2.4; D.1.5; D.3.2; F.4.1</acm-class><journal-ref>Problems in Programming, 2014, Vol. 15, No.2-3, pp. 233-241</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is an attempt to combine different ways of working with sets of
objects and their classes for designing and development of artificial
intelligent systems (AIS) of analysis information, using object-oriented
programming (OOP). This paper contains analysis of basic concepts of OOP and
their relation with set theory and artificial intelligence (AI). Process of
sets and multisets creation from different sides, in particular mathematical
set theory, OOP and AI is considered. Definition of object and its properties,
homogeneous and inhomogeneous classes of objects, set of objects, multiset of
objects and constructive methods of their creation and classification are
proposed. In addition, necessity of some extension of existing OOP tools for
the purpose of practical implementation AIS of analysis information, using
proposed approach, is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04188</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04188</id><created>2015-10-14</created><authors><author><keyname>Terletskyi</keyname><forenames>Dmytro</forenames></author></authors><title>Universal and Determined Constructors of Multisets of Objects</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1510.04183</comments><acm-class>I.2.4; F.4.1; D.1.5; D.3.3; E.2</acm-class><journal-ref>Information Theories and Applications, Vol. 21, Number 4, 2014,
  pp. 339-361</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains analysis of creation of sets and multisets as an approach
for modeling of some aspects of human thinking. The creation of sets is
considered within constructive object-oriented version of set theory (COOST),
from different sides, in particular classical set theory, object-oriented
programming (OOP) and development of intelligent information systems (IIS). The
main feature of COOST in contrast to other versions of set theory is an
opportunity to describe essences of objects more precisely, using their
properties and methods, which can be applied to them. That is why this version
of set theory is object-oriented and close to OOP. Within COOST, the author
proposes universal constructor of multisets of objects that gives us a
possibility to create arbitrary multisets of objects. In addition, a few
determined constructors of multisets of objects, which allow creating
multisets, using strictly defined schemas, also are proposed in the paper. Such
constructors are very useful in cases of very big cardinalities of multisets,
because they give us an opportunity to calculate a multiplicity of each object
and cardinality of multiset before its creation. The proposed constructors of
multisets of objects allow us to model in a sense corresponding processes of
human thought, that in turn give us an opportunity to develop IIS, using these
tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04189</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04189</id><created>2015-10-14</created><authors><author><keyname>N&#xf8;kland</keyname><forenames>Arild</forenames></author></authors><title>Improving Back-Propagation by Adding an Adversarial Gradient</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The back-propagation algorithm is widely used for learning in artificial
neural networks. A challenge in machine learning is to create models that
generalize to new data samples not seen in the training data. Recently, a
common flaw in several machine learning algorithms was discovered: small
perturbations added to the input data lead to consistent misclassification of
data samples. Samples that easily mislead the model are called adversarial
examples. Training a &quot;maxout&quot; network on adversarial examples has shown to
decrease this vulnerability, but also increase classification performance. This
paper shows that adversarial training has a regularizing effect also in
networks with logistic, hyperbolic tangent and rectified linear units. A simple
extension to the back-propagation method is proposed, that adds an adversarial
gradient to the training. The extension requires an additional forward and
backward pass to calculate a modified input sample, or mini batch, used as
input for standard back-propagation learning. The first experimental results on
MNIST show that the &quot;adversarial back-propagation&quot; method increases the
resistance to adversarial examples and boosts the classification performance.
The extension reduces the classification error on the permutation invariant
MNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a
network with rectified linear units. Based on these promising results,
adversarial back-propagation is proposed as a stand-alone regularizing method
that should be further investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04194</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04194</id><created>2015-10-14</created><authors><author><keyname>Terletskyi</keyname><forenames>Dmytro</forenames></author><author><keyname>Provotar</keyname><forenames>Alexandr</forenames></author></authors><title>Object-Oriented Dynamic Networks</title><categories>cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1510.04183</comments><acm-class>I.2.4; D.1.5; D.3.3; F.4.1; E.2</acm-class><journal-ref>International Book Series Information Science and Computing, Book
  30 Computational Models for Business and Engineering Domains, ITHEA, 2014,
  pp. 123-136</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains description of such knowledge representation model as
Object-Oriented Dynamic Network (OODN), which gives us an opportunity to
represent knowledge, which can be modified in time, to build new relations
between objects and classes of objects and to represent results of their
modifications. The model is based on representation of objects via their
properties and methods. It gives us a possibility to classify the objects and,
in a sense, to build hierarchy of their types. Furthermore, it enables to
represent relation of modification between concepts, to build new classes of
objects based on existing classes and to create sets and multisets of concepts.
OODN can be represented as a connected and directed graph, where nodes are
concepts and edges are relations between them. Using such model of knowledge
representation, we can consider modifications of knowledge and movement through
the graph of network as a process of logical reasoning or finding the right
solutions or creativity, etc. The proposed approach gives us an opportunity to
model some aspects of human knowledge system and main mechanisms of human
thought, in particular getting a new experience and knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04197</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04197</id><created>2015-10-14</created><authors><author><keyname>Ghaemmaghami</keyname><forenames>Seyed Salman Sajjadi</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Haghbin</keyname><forenames>Afrooz</forenames></author></authors><title>A Privacy Preserving Improvement for SRTA in Telecare Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio Frequency Identification (RFID) is a modern communication technology,
which provides authentication and identification through a nonphysical contact.
Recently, the use of this technology is almost developed in healthcare
environments. Although RFID technology can prepare sagacity in systems, privacy
and security issues ought to be considered before. Recently, in 2015, Li et al.
proposed SRTA, a hash-based RFID authentication protocol in medication
verification for healthcare. In this paper, we study this protocol and show
that SRTA protocol is vulnerable to traceability, impersonation and Dos
attacks. So it does not provide the privacy and security of RFID end users.
Therefore, we propose an improved secure and efficient RFID authentication
protocol to enhance the performance of Li et al. method. Our analyze show that
the existing weaknesses of SRTA protocol are eliminated in our proposed
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04205</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04205</id><created>2015-10-14</created><authors><author><keyname>Mohammadi</keyname><forenames>Seyed Hamidreza</forenames></author></authors><title>Reducing one-to-many problem in Voice Conversion by equalizing the
  formant locations using dynamic frequency warping</title><categories>cs.SD</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we investigate a solution to reduce the effect of one-to-many
problem in voice conversion. One-to-many problem in VC happens when two very
similar speech segments in source speaker have corresponding speech segments in
target speaker that are not similar to each other. As a result, the mapper
function usually over-smoothes the generated features in order to be similar to
both target speech segments. In this study, we propose to equalize the formant
location of source-target frame pairs using dynamic frequency warping in order
to reduce the complexity. After the conversion, another dynamic frequency
warping is further applied to reverse the effect of formant location
equalization during the training. The subjective experiments showed that the
proposed approach improves the speech quality significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04206</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04206</id><created>2015-10-14</created><authors><author><keyname>Terletskyi</keyname><forenames>Dmytro</forenames></author></authors><title>Exploiters-Based Knowledge Extraction in Object-Oriented Knowledge
  Representation</title><categories>cs.AI</categories><acm-class>I.2.4; D.1.5; D.3.3; F.4.1</acm-class><journal-ref>Proceedings of 24th International Workshop, Concurrency,
  Specification &amp; Programming 2015, Rzeszow, Poland, September 28-30, 2015,
  Vol. 2, pp. 211-221</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains the consideration of knowledge extraction mechanisms of
such object-oriented knowledge representation models as frames, object-oriented
programming and object-oriented dynamic networks. In addition, conception of
universal exploiters within object-oriented dynamic networks is also discussed.
The main result of the paper is introduction of new exploiters-based knowledge
extraction approach, which provides generation of a finite set of new classes
of objects, based on the basic set of classes. The methods for calculation of
quantity of new classes, which can be obtained using proposed approach, and of
quantity of types, which each of them describes, are proposed. Proof that basic
set of classes, extended according to proposed approach, together with union
exploiter create upper semilattice is given. The approach always allows
generating of finitely defined set of new classes of objects for any
object-oriented dynamic network. A quantity of these classes can be precisely
calculated before the generation. It allows saving of only basic set of classes
in the knowledge base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04209</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04209</id><created>2015-10-14</created><authors><author><keyname>Fan</keyname><forenames>Donglei</forenames></author><author><keyname>Tarraf</keyname><forenames>Danielle C.</forenames></author></authors><title>Finite Uniform Bisimulations for Linear Systems with Finite Input
  Alphabets</title><categories>math.OC cs.SY</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a class of systems over finite alphabets, namely discrete-time
systems with linear dynamics and a finite input alphabet. We formulate a notion
of finite uniform bisimulation, and motivate and propose a notion of regular
finite uniform bisimulation. We derive sufficient conditions for the existence
of finite uniform bisimulations, and propose and analyze algorithms to compute
finite uniform bisimulations when the sufficient conditions are satisfied. We
investigate the necessary conditions, and conclude with a set of illustrative
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04210</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04210</id><created>2015-10-14</created><authors><author><keyname>Aquino</keyname><forenames>Andre L. L.</forenames></author><author><keyname>Cavalcante</keyname><forenames>Tamer S. G.</forenames></author><author><keyname>Almeida</keyname><forenames>Eliana S.</forenames></author><author><keyname>Frery</keyname><forenames>Alejandro C.</forenames></author><author><keyname>Rosso</keyname><forenames>Osvaldo A.</forenames></author></authors><title>Characterization of Vehicle Behavior with Information Theory</title><categories>cs.IT math.IT</categories><journal-ref>The European Physical Journal B, Volume 88, number 10, 2015</journal-ref><doi>10.1140/epjb/e2015-60384-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes the use of Information Theory for the characterization of
vehicles behavior through their velocities. Three public data sets were used:
i.Mobile Century data set collected on Highway I-880, near Union City,
California; ii.Borl\&quot;ange GPS data set collected in the Swedish city of
Borl\&quot;ange; and iii.Beijing taxicabs data set collected in Beijing, China,
where each vehicle speed is stored as a time series. The Bandt-Pompe
methodology combined with the Complexity-Entropy plane were used to identify
different regimes and behaviors. The global velocity is compatible with a
correlated noise with f^{-k} Power Spectrum with k &gt;= 0. With this we identify
traffic behaviors as, for instance, random velocities (k aprox. 0) when there
is congestion, and more correlated velocities (k aprox. 3) in the presence of
free traffic flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04212</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04212</id><created>2015-10-14</created><authors><author><keyname>Terletskyi</keyname><forenames>Dmytro</forenames></author></authors><title>Inheritance in Object-Oriented Knowledge Representation</title><categories>cs.AI</categories><comments>in Information and Software Technologies, Communications in Computer
  and Information Science, Springer, 2015</comments><acm-class>I.2.4; D.1.5; D.3.3</acm-class><journal-ref>Information and Software Technologies, Volume 538 of the series
  Communications in Computer and Information Science, pp. 293-305, 2015</journal-ref><doi>10.1007/978-3-319-24770-0_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper contains the consideration of inheritance mechanism in such
knowledge representation models as object-oriented programming, frames and
object-oriented dynamic networks. In addition, inheritance within
representation of vague and imprecise knowledge are also discussed. New types
of inheritance, general classification of all known inheritance types and
approach, which allows avoiding in many cases problems with exceptions,
redundancy and ambiguity within object-oriented dynamic networks and their
fuzzy extension, are introduced in the paper. The proposed approach bases on
conception of homogeneous and inhomogeneous or heterogeneous class of objects,
which allow building of inheritance hierarchy more flexibly and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04214</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04214</id><created>2015-10-14</created><authors><author><keyname>Tanaka</keyname><forenames>Takashi</forenames></author><author><keyname>Esfahani</keyname><forenames>Peyman Mohajerin</forenames></author><author><keyname>Mitter</keyname><forenames>Sanjoy K.</forenames></author></authors><title>LQG Control with Minimal Information: Three-Stage Separation Principle
  and SDP-based Solution Synthesis</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the interest of evaluating an information-theoretic requirement for
feedback control, this paper proposes a framework to synthesize a control
policy that minimizes Massey's directed information from the state sequence to
the control sequence while attaining required Linear-Quadratic-Gaussian (LQG)
control performance. Interpretation and significance of this framework is
discussed in the context of networked control theory. As the main result, we
show that an optimal control policy can be realized by an attractively simple
three-stage decision architecture comprising (1) a linear sensor with additive
Gaussian noise, (2) a Kalman filter, and (3) a certainty equivalence
controller. This result suggests an integration of two separation principles
previously known in the literature: the filter-controller separation principle
in the LQG control theory, and the sensor-filter separation principle in
zero-delay rate-distortion theory for Gauss-Markov sources. It is also shown
that an optimal policy can be synthesized by semidefinite programming (SDP).
Both time-varying finite-horizon problems and time-invariant infinite-horizon
problems are considered. Our results can be viewed as a generalization of the
data-rate theorem for mean-square stability by Nair and Evans, extended for a
control performance analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04221</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04221</id><created>2015-10-14</created><authors><author><keyname>Garcia-Ceja</keyname><forenames>Enrique</forenames></author><author><keyname>Osmani</keyname><forenames>Venet</forenames></author><author><keyname>Mayora</keyname><forenames>Oscar</forenames></author></authors><title>Automatic Stress Detection in Working Environments from Smartphones'
  Accelerometer Data: A First Step</title><categories>cs.HC</categories><comments>in IEEE Journal of Biomedical and Health Informatics, 2015</comments><doi>10.1109/JBHI.2015.2446195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increase in workload across many organisations and consequent increase in
occupational stress is negatively affecting the health of the workforce.
Measuring stress and other human psychological dynamics is difficult due to
subjective nature of self- reporting and variability between and within
individuals. With the advent of smartphones it is now possible to monitor
diverse aspects of human behaviour, including objectively measured behaviour
related to psychological state and consequently stress. We have used data from
the smartphone's built-in accelerometer to detect behaviour that correlates
with subjects stress levels. Accelerometer sensor was chosen because it raises
fewer privacy concerns (in comparison to location, video or audio recording,
for example) and because its low power consumption makes it suitable to be
embedded in smaller wearable devices, such as fitness trackers. 30 subjects
from two different organizations were provided with smartphones. The study
lasted for 8 weeks and was conducted in real working environments, with no
constraints whatsoever placed upon smartphone usage. The subjects reported
their perceived stress levels three times during their working hours. Using
combination of statistical models to classify self reported stress levels, we
achieved a maximum overall accuracy of 71% for user-specific models and an
accuracy of 60% for the use of similar-users models, relying solely on data
from a single accelerometer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04233</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04233</id><created>2015-10-14</created><authors><author><keyname>Teixeira</keyname><forenames>Carlos H. C.</forenames></author><author><keyname>Fonseca</keyname><forenames>Alexandre J.</forenames></author><author><keyname>Serafini</keyname><forenames>Marco</forenames></author><author><keyname>Siganos</keyname><forenames>Georgos</forenames></author><author><keyname>Zaki</keyname><forenames>Mohammed J.</forenames></author><author><keyname>Aboulnaga</keyname><forenames>Ashraf</forenames></author></authors><title>Arabesque: A System for Distributed Graph Mining - Extended version</title><categories>cs.DC</categories><comments>A short version of this report appeared in the Proceedings of the
  25th ACM Symp. on Operating Systems Principles (SOSP), 2015</comments><report-no>QCRI-TR-2015-005</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed data processing platforms such as MapReduce and Pregel have
substantially simplified the design and deployment of certain classes of
distributed graph analytics algorithms. However, these platforms do not
represent a good match for distributed graph mining problems, as for example
finding frequent subgraphs in a graph. Given an input graph, these problems
require exploring a very large number of subgraphs and finding patterns that
match some &quot;interestingness&quot; criteria desired by the user. These algorithms are
very important for areas such as social net- works, semantic web, and
bioinformatics. In this paper, we present Arabesque, the first distributed data
processing platform for implementing graph mining algorithms. Arabesque
automates the process of exploring a very large number of subgraphs. It defines
a high-level filter-process computational model that simplifies the development
of scalable graph mining algorithms: Arabesque explores subgraphs and passes
them to the application, which must simply compute outputs and decide whether
the subgraph should be further extended. We use Arabesque's API to produce
distributed solutions to three fundamental graph mining problems: frequent
subgraph mining, counting motifs, and finding cliques. Our implementations
require a handful of lines of code, scale to trillions of subgraphs, and
represent in some cases the first available distributed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04235</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04235</id><created>2015-10-14</created><authors><author><keyname>Kogan</keyname><forenames>Kirill</forenames></author><author><keyname>Menikkumbura</keyname><forenames>Danushka</forenames></author><author><keyname>Petri</keyname><forenames>Gustavo</forenames></author><author><keyname>Noh</keyname><forenames>Youngtae</forenames></author><author><keyname>Nikolenko</keyname><forenames>Sergey</forenames></author><author><keyname>Eugster</keyname><forenames>Patrick</forenames></author></authors><title>BASEL (Buffering Architecture SpEcification Language)</title><categories>cs.NI</categories><comments>11 pages, 11 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Buffering architectures and policies for their efficient management
constitute one of the core ingredients of a network architecture. In this work
we introduce a new specification language, BASEL, that allows to express
virtual buffering architectures and management policies representing a variety
of economic models. BASEL does not require the user to implement policies in a
high-level language; rather, the entire buffering architecture and its policy
are reduced to several comparators and simple functions. We show examples of
buffering architectures in BASEL and demonstrate empirically the impact of
various settings on performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04238</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04238</id><created>2015-10-14</created><authors><author><keyname>Henrot</keyname><forenames>Simon</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author><author><keyname>Jutten</keyname><forenames>Christian</forenames></author></authors><title>Dynamical spectral unmixing of multitemporal hyperspectral images</title><categories>cs.CV</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of unmixing a time series of
hyperspectral images. We propose a dynamical model based on linear mixing
processes at each time instant. The spectral signatures and fractional
abundances of the pure materials in the scene are seen as latent variables, and
assumed to follow a general dynamical structure. Based on a simplified version
of this model, we derive an efficient spectral unmixing algorithm to estimate
the latent variables by performing alternating minimizations. The performance
of the proposed approach is demonstrated on synthetic and real multitemporal
hyperspectral images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04240</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04240</id><created>2015-10-14</created><authors><author><keyname>Har</keyname><forenames>Dongsoo</forenames></author></authors><title>Dementia assistive system as a dense network</title><categories>cs.CY cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As elderly population increases, portion of dementia patients becomes larger.
Thus social cost of caring dementia patients has been a major concern to many
nations. This article introduces a dementia assistive system operated by
various sensors and devices installed in body area and activity area of
patients. Since this system is served based on a network which includes a
number of nodes, it requires techniques to reduce the network performance
degradation caused by densely composed sensors and devices. This article
introduces existing protocols for communications of sensors and devices at both
low rate and high rate transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04241</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04241</id><created>2015-10-14</created><authors><author><keyname>Kadayinti</keyname><forenames>Naveen</forenames></author><author><keyname>Baghini</keyname><forenames>Maryam Shojaei</forenames></author><author><keyname>Sharma</keyname><forenames>Dinesh K.</forenames></author></authors><title>A Clock Synchronizer for Repeaterless Low Swing On-Chip Links</title><categories>cs.AR</categories><comments>11 pages, 25 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A clock synchronizing circuit for repeaterless low swing interconnects is
presented in this paper. The circuit uses a delay locked loop (DLL) to generate
multiple phases of the clock, of which the one closest to the center of the eye
is picked by a phase detector loop. The picked phase is then further fine tuned
by an analog voltage controlled delay to position the sampling clock at the
center of the eye. A clock domain transfer circuit then transfers the sampled
data to the receiver clock domain with a maximum latency of three clock cycles.
The proposed synchronizer has been designed and fabricated in 130 nm UMC MM
CMOS technology. The circuit consumes 1.4 mW from a 1.2 V supply at a data rate
of 1.3 Gbps. Further, the proposed synchronizer has been designed and simulated
in TSMC 65 nm CMOS technology. Post layout simulations show that the
synchronizer consumes 1.5 mW from a 1 V supply, at a data rate of 4 Gbps in
this technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04249</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04249</id><created>2015-10-14</created><updated>2016-01-09</updated><authors><author><keyname>Samvelyan</keyname><forenames>Mikayel</forenames></author><author><keyname>Avetisyan</keyname><forenames>Svetlana</forenames></author></authors><title>Random Irregular Block-hierarchical Networks: Algorithms for Computation
  of Main Properties</title><categories>cs.DS</categories><comments>The original is in Russian</comments><journal-ref>Proceedings of the IX Annual Scientific Conference in
  Russian-Armenian (Slavonic) University (2014) 48-60</journal-ref><doi>10.13140/RG.2.1.2528.7760</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the class of random irregular block-hierarchical networks is
defined and algorithms for generation and calculation of network properties are
described. The algorithms presented for this class of networks are more
efficient than known algorithms both in computation time and memory usage and
can be used to analyze topological properties of random irregular
block-hierarchical networks. The algorithms are implemented in the system
created by the authors for the study of topological properties of random
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04256</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04256</id><created>2015-10-14</created><authors><author><keyname>Brassard</keyname><forenames>Gilles</forenames></author></authors><title>Cryptography in a Quantum World</title><categories>quant-ph cs.CR</categories><comments>14 pages, Invited talk at SOFSEM 2016, January 2016, Harrachov, Czech
  Republic. Final publication available at link.springer.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although practised as an art and science for ages, cryptography had to wait
until the mid-twentieth century before Claude Shannon gave it a strong
mathematical foundation. However, Shannon's approach was rooted is his own
information theory, itself inspired by the classical physics of Newton and
Einstein. But our world is ruled by the laws of quantum mechanics. When
quantum-mechanical phenomena are taken into account, new vistas open up both
for codemakers and codebreakers. Is quantum mechanics a blessing or a curse for
the protection of privacy? As we shall see, the jury is still out!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04267</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04267</id><created>2015-10-14</created><authors><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Shekhtman</keyname><forenames>Louis</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Debunking in a World of Tribes</title><categories>cs.CY cs.HC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently a simple military exercise on the Internet was perceived as the
beginning of a new civil war in the US. Social media aggregate people around
common interests eliciting a collective framing of narratives and worldviews.
However, the wide availability of user-provided content and the direct path
between producers and consumers of information often foster confusion about
causations, encouraging mistrust, rumors, and even conspiracy thinking. In
order to contrast such a trend attempts to \textit{debunk} are often
undertaken. Here, we examine the effectiveness of debunking through a
quantitative analysis of 54 million users over a time span of five years (Jan
2010, Dec 2014). In particular, we compare how users interact with proven
(scientific) and unsubstantiated (conspiracy-like) information on Facebook in
the US. Our findings confirm the existence of echo chambers where users
interact primarily with either conspiracy-like or scientific pages. Both groups
interact similarly with the information within their echo chamber. We examine
47,780 debunking posts and find that attempts at debunking are largely
ineffective. For one, only a small fraction of usual consumers of
unsubstantiated information interact with the posts. Furthermore, we show that
those few are often the most committed conspiracy users and rather than
internalizing debunking information, they often react to it negatively. Indeed,
after interacting with debunking posts, users retain, or even increase, their
engagement within the conspiracy echo chamber.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04316</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04316</id><created>2015-10-14</created><authors><author><keyname>B&#xe9;rard</keyname><forenames>B&#xe9;atrice</forenames></author><author><keyname>Kouchnarenko</keyname><forenames>Olga</forenames></author><author><keyname>Mullins</keyname><forenames>John</forenames></author><author><keyname>Sassolas</keyname><forenames>Mathieu</forenames></author></authors><title>Probabilistic Opacity in Refinement-Based Modeling</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a probabilistic transition system (PTS) $\cal A$ partially observed by
an attacker, and an $\omega$-regular predicate $\varphi$over the traces of
$\cal A$, measuring the disclosure of the secret $\varphi$ in $\cal A$ means
computing the probability that an attacker who observes a run of $\cal A$ can
ascertain that its trace belongs to $\varphi$. In the context of refinement, we
consider specifications given as Interval-valued Discrete Time Markov Chains
(IDTMCs), which are underspecified Markov chains where probabilities on edges
are only required to belong to intervals. Scheduling an IDTMC $\cal S$ produces
a concrete implementation as a PTS and we define the worst case disclosure of
secret $\varphi$ in ${\cal S}$ as the maximal disclosure of $\varphi$ over all
PTSs thus produced. We compute this value for a subclass of IDTMCs and we prove
that refinement can only improve the opacity of implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04317</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04317</id><created>2015-10-14</created><authors><author><keyname>Tran</keyname><forenames>Hung Nghiep</forenames></author><author><keyname>Takasu</keyname><forenames>Atsuhiro</forenames></author></authors><title>Partitioning Algorithms for Improving Efficiency of Topic Modeling
  Parallelization</title><categories>cs.DC</categories><doi>10.1109/PACRIM.2015.7334854</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modeling is a very powerful technique in data analysis and data mining
but it is generally slow. Many parallelization approaches have been proposed to
speed up the learning process. However, they are usually not very efficient
because of the many kinds of overhead, especially the load-balancing problem.
We address this problem by proposing three partitioning algorithms, which
either run more quickly or achieve better load balance than current
partitioning algorithms. These algorithms can easily be extended to improve
parallelization efficiency on other topic models similar to LDA, e.g., Bag of
Timestamps, which is an extension of LDA with time information. We evaluate
these algorithms on two popular datasets, NIPS and NYTimes. We also build a
dataset containing over 1,000,000 scientific publications in the computer
science domain from 1951 to 2010 to experiment with Bag of Timestamps
parallelization, which we design to demonstrate the proposed algorithms'
extensibility. The results strongly confirm the advantages of these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04336</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04336</id><created>2015-10-14</created><authors><author><keyname>Malik</keyname><forenames>Avinash</forenames></author><author><keyname>Roop</keyname><forenames>Partha S</forenames></author><author><keyname>Andalam</keyname><forenames>Sidharta</forenames></author><author><keyname>Yip</keyname><forenames>Eugene</forenames></author><author><keyname>Trew</keyname><forenames>Mark</forenames></author></authors><title>A synchronous rendering of hybrid systems for designing Plant-on-a-Chip
  (PoC)</title><categories>cs.FL cs.SY</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid systems are discrete controllers that are used for controlling a
physical process (plant) exhibiting continuous dynamics. A hybrid automata (HA)
is a well known and widely used formal model for the specification of such
systems. While many methods exist for simulating hybrid automata, there are no
known approaches for the automatic code generation from HA that are semantic
preserving. If this were feasible, it would enable the design of a
plant-on-a-chip (PoC) system that could be used for the emulation of the plant
to validate discrete controllers. Such an approach would need to be
mathematically sound and should not rely on numerical solvers. We propose a
method of PoC design for plant emulation, not possible before. The approach
restricts input/output (I/O) HA models using a set of criteria for
well-formedness which are statically verified. Following verification, we use
an abstraction based on a synchronous approach to facilitate code generation.
This is feasible through a sound transformation to synchronous HA. We compare
our method (the developed tool called Piha) to the widely used Simulink R
simulation framework and show that our method is superior in both execution
time and code size. Our approach to the PoC problem paves the way for the
emulation of physical plants in diverse domains such as robotics, automation,
medical devices, and intelligent transportation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04340</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04340</id><created>2015-10-14</created><authors><author><keyname>Sun</keyname><forenames>Xiang</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author></authors><title>PRIMAL: PRofIt Maximization Avatar pLacement for Mobile Edge Computing</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a cloudlet network architecture to bring the computing resources
from the centralized cloud to the edge. Thus, each User Equipment (UE) can
communicate with its Avatar, a software clone located in a cloudlet, with lower
end-to-end (E2E) delay. However, UEs are moving over time, and so the low E2E
delay may not be maintained if UEs' Avatars stay in their original cloudlets.
Thus, live Avatar migration (i.e., migrating a UE's Avatar to a suitable
cloudlet based on the UE's location) is enabled to maintain low E2E delay
between each UE and its Avatar. On the other hand, the migration itself incurs
extra overheads in terms of resources of the Avatar, which compromise the
performance of applications running in the Avatar. By considering the gain
(i.e., the E2E delay reduction) and the cost (i.e., the migration overheads) of
the live Avatar migration, we propose a PRofIt Maximization Avatar pLacement
(PRIMAL) strategy for the cloudlet network in order to optimize the tradeoff
between the migration gain and the migration cost by selectively migrating the
Avatars to their optimal locations. Simulation results demonstrate that as
compared to the other two strategies (i.e., Follow Me Avatar and Static),
PRIMAL maximizes the profit in terms of maintaining the low average E2E delay
between UEs and their Avatars and minimizing the migration cost simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04347</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04347</id><created>2015-10-14</created><authors><author><keyname>Davoust</keyname><forenames>Alan</forenames></author><author><keyname>Esfandiari</keyname><forenames>Babak</forenames></author></authors><title>Processing Regular Path Queries on Arbitrarily Distributed Data</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular Path Queries (RPQs) are a type of graph query where answers are pairs
of nodes connected by a sequence of edges matching a regular expression. We
study the techniques to process such queries on a distributed graph of data.
While many techniques assume the location of each data element (node or edge)
is known, when the components of the distributed system are autonomous, the
data will be arbitrarily distributed. As the different query processing
strategies are equivalently costly in the worst case, we isolate
query-dependent cost factors and present a method to choose between strategies,
using new query cost estimation techniques. We evaluate our techniques using
meaningful queries on biomedical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04356</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04356</id><created>2015-10-14</created><authors><author><keyname>Aeron</keyname><forenames>Shuchin</forenames></author><author><keyname>Kernfeld</keyname><forenames>Eric</forenames></author></authors><title>Group-Invariant Subspace Clustering</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Proceedings of Allerton 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of group invariant subspace clustering
where the data is assumed to come from a union of group-invariant subspaces of
a vector space, i.e. subspaces which are invariant with respect to action of a
given group. Algebraically, such group-invariant subspaces are also referred to
as submodules. Similar to the well known Sparse Subspace Clustering approach
where the data is assumed to come from a union of subspaces, we analyze an
algorithm which, following a recent work [1], we refer to as Sparse Sub-module
Clustering (SSmC). The method is based on finding group-sparse
self-representation of data points. In this paper we primarily derive general
conditions under which such a group-invariant subspace identification is
possible. In particular we extend the geometric analysis in [2] and in the
process we identify a related problem in geometric functional analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04373</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04373</id><created>2015-10-14</created><authors><author><keyname>Ghifary</keyname><forenames>Muhammad</forenames></author><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Kleijn</keyname><forenames>W. Bastiaan</forenames></author><author><keyname>Zhang</keyname><forenames>Mengjie</forenames></author></authors><title>Scatter Component Analysis: A Unified Framework for Domain Adaptation
  and Domain Generalization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses classification tasks on a particular target domain in
which labeled training data are only available from source domains different
from (but related to) the target. Two closely related frameworks, domain
adaptation and domain generalization, are concerned with such tasks, where the
only difference between those frameworks is the availability of the unlabeled
target data: domain adaptation can leverage unlabeled target information, while
domain generalization cannot.
  We propose Scatter Component Analyis (SCA), a fast representation learning
algorithm that can be applied to both domain adaptation and domain
generalization. SCA is based on a simple geometrical measure, i.e., scatter,
which operates on reproducing kernel Hilbert space. SCA finds a representation
that trades between maximizing the separability of classes, minimizing the
mismatch between domains, and maximizing the separability of data; each of
which is quantified through scatter. The optimization problem of SCA can be
reduced to a generalized eigenvalue problem, which results in a fast and exact
solution.
  Comprehensive experiments on benchmark cross-domain object recognition
datasets verify that SCA performs much faster than several state-of-the-art
algorithms and also provides state-of-the-art classification accuracy in both
domain adaptation and domain generalization. We also show that scatter can be
used to establish a theoretical generalization bound in the case of domain
adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04374</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04374</id><created>2015-10-14</created><authors><author><keyname>Santipach</keyname><forenames>Wiroonsak</forenames></author><author><keyname>Mamat</keyname><forenames>Kritsada</forenames></author><author><keyname>Charoenlarpnopparut</keyname><forenames>Chalie</forenames></author></authors><title>Outage Bound for Max-Based Downlink Scheduling With Imperfect CSIT and
  Delay Constraint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider downlink max-based scheduling in which a base station and users
are equipped with a single antenna. In each time slot, the base station
transmits to the user whose channel state information at the transmitter (CSIT)
is the largest. We assume that the CSIT for some users is imperfect due to
quantization or estimation error and derive an analytical lower bound for the
probability of outage, which occurs if a required data rate for a user is not
satisfied within a delay constraint. The bound is tight for Rayleigh fading and
shows how the required rates and the CSIT error affect outage performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04379</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04379</id><created>2015-10-14</created><authors><author><keyname>Zhang</keyname><forenames>Yanru</forenames></author><author><keyname>Liu</keyname><forenames>Lanchao</forenames></author><author><keyname>Gu</keyname><forenames>Yunan</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Pan</keyname><forenames>Miao</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Offloading in Software Defined Network at Edge with Information
  Asymmetry: A Contract Theoretical Approach</title><categories>cs.NI</categories><comments>10 pages, 9 figures</comments><journal-ref>J Sign Process Syst 81 (2015) 1-13</journal-ref><doi>10.1007/s11265-015-1038-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of highly capable mobile devices such as smartphones and
tablets has significantly increased the demand for wireless access. Software
defined network (SDN) at edge is viewed as one promising technology to simplify
the traffic offloading process for current wireless networks. In this paper, we
investigate the incentive problem in SDN-at-edge of how to motivate a third
party access points (APs) such as WiFi and smallcells to offload traffic for
the central base stations (BSs). The APs will only admit the traffic from the
BS under the precondition that their own traffic demand is satisfied. Under the
information asymmetry that the APs know more about own traffic demands, the BS
needs to distribute the payment in accordance with the APs' idle capacity to
maintain a compatible incentive. First, we apply a contract-theoretic approach
to model and analyze the service trading between the BS and APs. Furthermore,
other two incentive mechanisms: optimal discrimination contract and linear
pricing contract are introduced to serve as the comparisons of the anti adverse
selection contract. Finally, the simulation results show that the contract can
effectively incentivize APs' participation and offload the cellular network
traffic. Furthermore, the anti adverse selection contract achieves the optimal
outcome under the information asymmetry scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04389</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04389</id><created>2015-10-14</created><authors><author><keyname>Matsui</keyname><forenames>Yusuke</forenames></author><author><keyname>Ito</keyname><forenames>Kota</forenames></author><author><keyname>Aramaki</keyname><forenames>Yuji</forenames></author><author><keyname>Yamasaki</keyname><forenames>Toshihiko</forenames></author><author><keyname>Aizawa</keyname><forenames>Kiyoharu</forenames></author></authors><title>Sketch-based Manga Retrieval using Manga109 Dataset</title><categories>cs.CV cs.IR cs.MM</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manga (Japanese comics) are popular worldwide. However, current e-manga
archives offer very limited search support, including keyword-based search by
title or author, or tag-based categorization. To make the manga search
experience more intuitive, efficient, and enjoyable, we propose a content-based
manga retrieval system. First, we propose a manga-specific image-describing
framework. It consists of efficient margin labeling, edge orientation histogram
feature description, and approximate nearest-neighbor search using product
quantization. Second, we propose a sketch-based interface as a natural way to
interact with manga content. The interface provides sketch-based querying,
relevance feedback, and query retouch. For evaluation, we built a novel dataset
of manga images, Manga109, which consists of 109 comic books of 21,142 pages
drawn by professional manga artists. To the best of our knowledge, Manga109 is
currently the biggest dataset of manga images available for research. We
conducted a comparative study, a localization evaluation, and a large-scale
qualitative study. From the experiments, we verified that: (1) the retrieval
accuracy of the proposed method is higher than those of previous methods; (2)
the proposed method can localize an object instance with reasonable runtime and
accuracy; and (3) sketch querying is useful for manga search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04390</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04390</id><created>2015-10-14</created><authors><author><keyname>Tsakiris</keyname><forenames>Manolis C.</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Dual Principal Component Pursuit</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of outlier rejection in single subspace learning.
Classical approaches work directly with a low-dimensional representation of the
subspace. Our approach works with a dual representation of the subspace and
hence aims to find its orthogonal complement. We pose this problem as an
$\ell_1$-minimization problem on the sphere and show that, under certain
conditions on the distribution of the data, any global minimizer of this
non-convex problem gives a vector orthogonal to the subspace. Moreover, we show
that such a vector can still be found by relaxing the non-convex problem with a
sequence of linear programs. Experiments on synthetic and real data show that
the proposed approach, which we call Dual Principal Component Pursuit (DPCP),
outperforms state-of-the art methods, especially in the case of
high-dimensional subspaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04393</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04393</id><created>2015-10-15</created><authors><author><keyname>Newberry</keyname><forenames>X. Y.</forenames></author></authors><title>The Diagonal Lemma Fails in Aristotelian Logic</title><categories>cs.LO</categories><journal-ref>The Reasoner Volume 10, Number 1 January 2016 pgs 102-103</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 1950-ies P. F. Strawson proposed the Logic of Presuppositions. In this
system sentences with empty subject are considered neither true nor false.
Strawson considered only simple sentences with two predicate letters. But the
concept can be extended to arbitrary monadic, even polyadic sentences utilizing
truth-relevant logic developed by Richard Diaz. It can be shown that
self-referential G\&quot;odel's sentence in fact has an empty subject, and can thus
be classified by the Logic of Presuppositions as neither true nor false.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04396</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04396</id><created>2015-10-15</created><authors><author><keyname>Tsakiris</keyname><forenames>Manolis C.</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Filtrated Spectral Algebraic Subspace Clustering</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic Subspace Clustering (ASC) is a simple and elegant method based on
polynomial fitting and differentiation for clustering noiseless data drawn from
an arbitrary union of subspaces. In practice, however, ASC is limited to
equi-dimensional subspaces because the estimation of the subspace dimension via
algebraic methods is sensitive to noise. This paper proposes a new ASC
algorithm that can handle noisy data drawn from subspaces of arbitrary
dimensions. The key ideas are (1) to construct, at each point, a decreasing
sequence of subspaces containing the subspace passing through that point; (2)
to use the distances from any other point to each subspace in the sequence to
construct a subspace clustering affinity, which is superior to alternative
affinities both in theory and in practice. Experiments on the Hopkins 155
dataset demonstrate the superiority of the proposed method with respect to
sparse and low rank subspace clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04411</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04411</id><created>2015-10-15</created><updated>2015-11-15</updated><authors><author><keyname>Wu</keyname><forenames>Angela Xiao</forenames></author><author><keyname>Taneja</keyname><forenames>Harsh</forenames></author></authors><title>Reimagining Internet Geographies: A User-Centric Ethnological Mapping of
  the World Wide Web</title><categories>cs.SI</categories><comments>Wu, Angela, Xiao &amp; Taneja. H. (Forthcoming.) Reimagining Internet
  Geographies: A User-Centric Ethnological Mapping of the World Wide Web.
  Journal of Computer Mediated Communication. Both Authors Contributed Equally
  to the Manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new user-centric imagery of the WWW that foregrounds local usage
and its shaping forces, in contrast to existing imageries that prioritize
Internet infrastructure. We construct ethnological maps of WWW usage through a
network analysis of shared global traffic between 1000 most popular websites at
three time points and develop granular measures for exploring global
participation in online communication. Our results reveal the significant
growth and thickening of online regional cultures associated with the global
South. We draw attention to how local cultural identity, affirmative state
intervention and economic contexts shape regional cultures on the global WWW.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04413</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04413</id><created>2015-10-15</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author><author><keyname>Ahmad</keyname><forenames>Jamil</forenames></author><author><keyname>Sajjad</keyname><forenames>Muhammad</forenames></author><author><keyname>Zubair</keyname><forenames>Muhammad</forenames></author></authors><title>Secure Image Steganography using Cryptography and Image Transposition</title><categories>cs.MM cs.CR</categories><comments>A simple but effective image steganographic method, providing secure
  transmission of secret data over Internet. The final published version of the
  paper can be downloaded from the link:
  (http://www.neduet.edu.pk/NED-Journal/2015/15vol4paper3.html). Please contact
  me at khan.muhammad.icp@gmail.com if you need the final formatted published
  version of the paper</comments><journal-ref>NED University Journal of Research 12.4 (2015): 81-91</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information security is one of the most challenging problems in today's
technological world. In order to secure the transmission of secret data over
the public network (Internet), various schemes have been presented over the
last decade. Steganography combined with cryptography, can be one of the best
choices for solving this problem. This paper proposes a new steganographic
method based on gray-level modification for true colour images using image
transposition, secret key and cryptography. Both the secret key and secret
information are initially encrypted using multiple encryption algorithms
(bitxor operation, bits shuffling, and stego key-based encryption); these are,
subsequently, hidden in the host image pixels. In addition, the input image is
transposed before data hiding. Image transposition, bits shuffling, bitxoring,
stego key-based encryption, and gray-level modification introduce five
different security levels to the proposed scheme, making the data recovery
extremely difficult for attackers. The proposed technique is evaluated by
objective analysis using various image quality assessment metrics, producing
promising results in terms of imperceptibility and security. Moreover, the high
quality stego images and its minimal histogram changeability, also validate the
effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04420</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04420</id><created>2015-10-15</created><authors><author><keyname>Sarao</keyname><forenames>Paramjot Kaur</forenames></author><author><keyname>Mittal</keyname><forenames>Puneet</forenames></author><author><keyname>Kaur</keyname><forenames>Rupinder</forenames></author></authors><title>Narrative Science Systems: A Review</title><categories>cs.AI</categories><journal-ref>International Journal of Research in Computer Science, 5(1), 2015,
  pp 9-14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic narration of events and entities is the need of the hour,
especially when live reporting is critical and volume of information to be
narrated is huge. This paper discusses the challenges in this context, along
with the algorithms used to build such systems. From a systematic study, we can
infer that most of the work done in this area is related to statistical data.
It was also found that subjective evaluation or contribution of experts is also
limited for narration context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04422</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04422</id><created>2015-10-15</created><authors><author><keyname>Tran</keyname><forenames>Hung Nghiep</forenames></author><author><keyname>Huynh</keyname><forenames>Tin</forenames></author><author><keyname>Hoang</keyname><forenames>Kiem</forenames></author></authors><title>A Potential Approach to Overcome Data Limitation in Scientific
  Publication Recommendation</title><categories>cs.DL</categories><doi>10.1109/KSE.2015.76</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data are essential for the experiments of relevant scientific publication
recommendation methods but it is difficult to build ground truth data. A
naturally promising solution is using publications that are referenced by
researchers to build their ground truth data. Unfortunately, this approach has
not been explored in the literature, so its applicability is still a gap in our
knowledge. In this research, we systematically study this approach by
theoretical and empirical analyses. In general, the results show that this
approach is reasonable and has many advantages. However, the empirical analysis
shows both positive and negative results. We conclude that, in some situations,
this is a useful alternative approach toward overcoming data limitation. Based
on this approach, we build and publish a dataset in computer science domain to
help advancing other researches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04436</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04436</id><created>2015-10-15</created><authors><author><keyname>Islam</keyname><forenames>Hasan M. A.</forenames></author><author><keyname>Lukyanenko</keyname><forenames>Andrey</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author><author><keyname>Yla-Jaaski</keyname><forenames>Antti</forenames></author></authors><title>Towards Disruption Tolerant ICN</title><categories>cs.NI</categories><comments>ISCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-Centric Networking (ICN) is a promi- nent topic in current
networking research. ICN design signifi- cantly considers the increased demand
of scalable and efficient content distribution for Future Internet. However,
intermittently connected mobile environments or disruptive networks present a
significant challenge to ICN deployment. In this context, delay tolerant
networking (DTN) architecture is an initiative that effec- tively deals with
network disruptions. Among all ICN proposals, Content Centric Networking (CCN)
is gaining more and more interest for its architectural design, but still has
the limitation in highly disruptive environment. In this paper, we design a
protocol stack referred as CCNDTN which integrates DTN architecture in the
native CCN to deal with network disruption. We also present the implementation
details of the proposed CCNDTN. We extend CCN routing strategies by integrating
Bundle protocol of DTN architecture. The integration of CCN and DTN enriches
the connectivity options of CCN architecture in fragmented networks.
Furthermore, CCNDTN can be beneficial through the simultaneous use of all
available connectivities and opportunistic networking of DTN for the
dissemination of larger data items. This paper also highlights the potential
use cases of CCNDTN architecture and crucial questions about integrating CCN
and DTN
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04437</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04437</id><created>2015-10-15</created><authors><author><keyname>Maity</keyname><forenames>Satyabrata</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>A Novel Approach for Human Action Recognition from Silhouette Images</title><categories>cs.CV</categories><comments>Manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel human action recognition technique from video is
presented. Any action of human is a combination of several micro action
sequences performed by one or more body parts of the human. The proposed
approach uses spatio-temporal body parts movement (STBPM) features extracted
from foreground silhouette of the human objects. The newly proposed STBPM
feature estimates the movements of different body parts for any given time
segment to classify actions. We also proposed a rule based logic named rule
action classifier (RAC), which uses a series of condition action rules based on
prior knowledge and hence does not required training to classify any action.
Since we don't require training to classify actions, the proposed approach is
view independent. The experimental results on publicly available Wizeman and
MuHVAi datasets are compared with that of the related research work in terms of
accuracy in the human action detection, and proposed technique outperforms the
others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04440</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04440</id><created>2015-10-15</created><authors><author><keyname>Crafa</keyname><forenames>Silvia</forenames></author></authors><title>Modelling the Evolution of Programming Languages</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming languages are engineered languages that allow to instruct a
machine and share algorithmic information; they have a great influence on the
society since they underlie almost every information technology artefact, and
they are at the core of the current explosion of software technology. The
history of programming languages is marked by innovations, diversifications,
lateral transfers and social influences; moreover, it represents an
intermediate case study between the evolution of human languages and the
evolution of technology. In this paper we study the application of the
Darwinian explanation to the programming languages evolution by discussing to
what extent the evolutionary mechanisms distinctive of biology can be applied
to this area. We show that a number of evolutionary building blocks can be
recognised in the realm of computer languages, but we also identify critical
issues. Far from being crystal clear, this fine-grained study shows to be a
useful tool to assess recent results about programming languages phylogenies.
Finally, we show that rich evolutionary patterns, such as co-evolution,
macro-evolutionary trends, niche construction and exaptation, can be
effectively applied to programming languages and provide for interesting
explanatory tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04445</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04445</id><created>2015-10-15</created><authors><author><keyname>Ghodrati</keyname><forenames>Amir</forenames></author><author><keyname>Diba</keyname><forenames>Ali</forenames></author><author><keyname>Pedersoli</keyname><forenames>Marco</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers</title><categories>cs.CV</categories><comments>ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we evaluate the quality of the activation layers of a
convolutional neural network (CNN) for the gen- eration of object proposals. We
generate hypotheses in a sliding-window fashion over different activation
layers and show that the final convolutional layers can find the object of
interest with high recall but poor localization due to the coarseness of the
feature maps. Instead, the first layers of the network can better localize the
object of interest but with a reduced recall. Based on this observation we
design a method for proposing object locations that is based on CNN features
and that combines the best of both worlds. We build an inverse cascade that,
going from the final to the initial convolutional layers of the CNN, selects
the most promising object locations and refines their boxes in a coarse-to-fine
manner. The method is efficient, because i) it uses the same features extracted
for detection, ii) it aggregates features using integral images, and iii) it
avoids a dense evaluation of the proposals due to the inverse coarse-to-fine
cascade. The method is also accurate; it outperforms most of the previously
proposed object proposals approaches and when plugged into a CNN-based detector
produces state-of-the- art detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04454</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04454</id><created>2015-10-15</created><authors><author><keyname>Ma</keyname><forenames>Yao</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Online Markov decision processes with policy iteration</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online Markov decision process (MDP) is a generalization of the classical
Markov decision process that incorporates changing reward functions. In this
paper, we propose practical online MDP algorithms with policy iteration and
theoretically establish a sublinear regret bound. A notable advantage of the
proposed algorithm is that it can be easily combined with function
approximation, and thus large and possibly continuous state spaces can be
efficiently handled. Through experiments, we demonstrate the usefulness of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04455</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04455</id><created>2015-10-15</created><authors><author><keyname>Oizumi</keyname><forenames>Masafumi</forenames></author><author><keyname>Tsuchiya</keyname><forenames>Naotsugu</forenames></author><author><keyname>Amari</keyname><forenames>Shun-ichi</forenames></author></authors><title>A unified framework for information integration based on information
  geometry</title><categories>q-bio.NC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a unified theoretical framework for quantifying spatio-temporal
interactions in a stochastic dynamical system based on information geometry. In
the proposed framework, the degree of interactions is quantified by the
divergence between the actual probability distribution of the system and a
constrained probability distribution where the interactions of interest are
disconnected. This framework provides novel geometric interpretations of
various information theoretic measures of interactions, such as mutual
information, transfer entropy, and stochastic interaction in terms of how
interactions are disconnected. The framework therefore provides an intuitive
understanding of the relationships between the various quantities. By extending
the concept of transfer entropy, we propose a novel measure of integrated
information which measures causal interactions between parts of a system.
Integrated information quantifies the extent to which the whole is more than
the sum of the parts and can be potentially used as a biological measure of the
levels of consciousness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04467</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04467</id><created>2015-10-15</created><authors><author><keyname>Banisch</keyname><forenames>Sven</forenames></author></authors><title>From Microscopic Heterogeneity to Macroscopic Complexity in the
  Contrarian Voter Model</title><categories>cond-mat.stat-mech cs.IT math.IT nlin.AO physics.data-an</categories><journal-ref>Advances in Complex Systems, 2014, 17, 1450025</journal-ref><doi>10.1142/S0219525914500258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An analytical treatment of a simple opinion model with contrarian behavior is
presented. The focus is on the stationary dynamics of the model and in
particular on the effect of inhomogeneities in the interaction topology on the
stationary behavior. We start from a micro-level Markov chain description of
the model. Markov chain aggregation is then used to derive a macro chain for
the complete graph as well as a meso-level description for the two-community
graph composed of two (weakly) coupled sub-communities. In both cases, a
detailed understanding of the model behavior is possible using Markov chain
tools. More importantly, however, this setting provides an analytical scenario
to study the discrepancy between the homogeneous mixing case and the model on a
slightly more complex topology. We show that memory effects are introduced at
the macro level when we aggregate over agent attributes without sensitivity to
the microscopic details and quantify these effects using concepts from
information theory. In this way, the method facilitates the analysis of the
relation between microscopic processes and a their aggregation to a macroscopic
level of description and informs about the complexity of a system introduced by
heterogeneous interaction relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04469</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04469</id><created>2015-10-15</created><authors><author><keyname>Pantelis</keyname><forenames>Garry</forenames></author></authors><title>A Formal System: Rigorous Constructions of Computer Models</title><categories>cs.LO</categories><msc-class>03B35 (Primary), 00A71, 03B15 (Secondary)</msc-class><acm-class>D.2.4; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book explores an alternative to the current dominant paradigm where a
discrete computer model is constructed as an attempt to approximate some
continuum theory. We focus on a class of discrete computer models that are
based on simple deterministic rules and finite state arithmetic. Such models
are highly compatible with the operational parameters of the real world
computer on which they are executed and hence their validation can be
associated with the allowable computations on the machine. A simple formal
system based on a language of functional programs is employed as a tool of
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04472</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04472</id><created>2015-10-15</created><updated>2015-10-26</updated><authors><author><keyname>Natali</keyname><forenames>Laura</forenames></author><author><keyname>Merani</keyname><forenames>Maria Luisa</forenames></author></authors><title>Adaptive Streaming in P2P Live Video Systems: A Distributed Rate Control
  Approach</title><categories>cs.NI</categories><comments>12 pages, 17 figures, this work has been submitted to the IEEE
  journal on selected Area in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Adaptive Streaming over HTTP (DASH) is a recently proposed standard
that offers different versions of the same media content to adapt the delivery
process over the Internet to dynamic bandwidth fluctuations and different user
device capabilities. The peer-to-peer (P2P) paradigm for video streaming allows
to leverage the cooperation among peers, guaranteeing to serve every video
request with increased scalability and reduced cost. We propose to combine
these two approaches in a P2P-DASH architecture, exploiting the potentiality of
both. The new platform is made of several swarms, and a different DASH
representation is streamed within each of them; unlike client-server DASH
architectures, where each client autonomously selects which version to download
according to current network conditions and to its device resources, we put
forth a new rate control strategy implemented at peer site to maintain a good
viewing quality to the local user and to simultaneously guarantee the
successful operation of the P2P swarms. The effectiveness of the solution is
demonstrated through simulation and it indicates that the P2P-DASH platform is
able to warrant its users a very good performance, much more satisfying than in
a conventional P2P environment where DASH is not employed. Through a comparison
with a reference DASH system modeled via the Integer Linear Programming (ILP)
approach, the new system is shown to outperform such reference architecture. To
further validate the proposal, both in terms of robustness and scalability,
system behavior is investigated in the critical condition of a flash crowd,
showing that the strong upsurge of new users can be successfully revealed and
gradually accommodated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04488</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04488</id><created>2015-10-15</created><updated>2016-02-09</updated><authors><author><keyname>Ruby</keyname><forenames>Rukhsana</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Leung</keyname><forenames>Cyril</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Performance Analysis of a Heterogeneous Traffic Scheduler using Large
  Deviation Principle</title><categories>cs.NI cs.PF</categories><comments>Number of Pages: 35</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze a scheduling algorithm which is suitable for the
heterogeneous traffic network. In the large deviation setting, we are
interested to see, how the asymptotic decay rate of maximum queue overflow
probability achieved by this algorithm.We first derive an upper bound on the
decay rate of the queue overflow probability as the queue overflow threshold
approaches infinity. Then, we study several structural properties of the
minimum cost path of the maximum queue length. Given these properties, we prove
that the maximum queue length follows a sample path with linear increment. For
certain parameter values, the scheduling algorithm is asymptotically optimal in
reducing the maximum queue length. Through numerical results, we have show the
large deviation properties of the queue length typically used in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04489</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04489</id><created>2015-10-15</created><authors><author><keyname>Af&#x15f;er</keyname><forenames>H&#xfc;seyin</forenames></author><author><keyname>Deli&#xe7;</keyname><forenames>Hakan</forenames></author></authors><title>Polar Codes With Higher-Order Memory</title><categories>cs.IT math.IT</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the design of a set of code sequences $ \{ {\mathscr
C}_{n}^{(m)} : n\geq 1, m \geq 1 \}$, with memory order $m$ and code-length
$N=O(\phi^n)$, where $ \phi \in (1,2]$ is the largest real root of the
polynomial equation $F(m,\rho)=\rho^m-\rho^{m-1}-1$ and $\phi$ is decreasing in
$m$. $\{ {\mathscr C}_{n}^{(m)}\}$ is based on the channel polarization idea,
where $ \{ {\mathscr C}_{n}^{(1)} \}$ coincides with the polar codes presented
by Ar\i kan and can be encoded and decoded with complexity $O(N \log N)$. $ \{
{\mathscr C}_{n}^{(m)} \}$ achieves the symmetric capacity, $I(W)$, of an
arbitrary binary-input, discrete-output memoryless channel, $W$, for any fixed
$m$ and its encoding and decoding complexities decrease with growing $m$. We
obtain an achievable bound on the probability of block-decoding error, $P_e$,
of $\{ {\mathscr C}_{n}^{(m)} \}$ and showed that $P_e = O (2^{-N^\beta} )$ is
achievable for $\beta &lt; \frac{\phi-1}{1+m(\phi-1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04493</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04493</id><created>2015-10-15</created><authors><author><keyname>Xenaki</keyname><forenames>Spyridoula D.</forenames></author><author><keyname>Koutroumbas</keyname><forenames>Konstantinos D.</forenames></author><author><keyname>Rontogiannis</keyname><forenames>Athanasios A.</forenames></author></authors><title>Sparsity-aware Possibilistic Clustering Algorithms</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1412.3613</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper two novel possibilistic clustering algorithms are presented,
which utilize the concept of sparsity. The first one, called sparse
possibilistic c-means, exploits sparsity and can deal well with closely located
clusters that may also be of significantly different densities. The second one,
called sparse adaptive possibilistic c-means, is an extension of the first,
where now the involved parameters are dynamically adapted. The latter can deal
well with even more challenging cases, where, in addition to the above,
clusters may be of significantly different variances. More specifically, it
provides improved estimates of the cluster representatives, while, in addition,
it has the ability to estimate the actual number of clusters, given an
overestimate of it. Extensive experimental results on both synthetic and real
data sets support the previous statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04500</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04500</id><created>2015-10-15</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author></authors><title>Noisy-parallel and comparable corpora filtering methodology for the
  extraction of bi-lingual equivalent data at sentence level</title><categories>cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:1509.09093,
  arXiv:1509.08881</comments><journal-ref>Computer Science 16.2 (2015): 169-184</journal-ref><doi>10.7494/csci.2015.16.2.169</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text alignment and text quality are critical to the accuracy of Machine
Translation (MT) systems, some NLP tools, and any other text processing tasks
requiring bilingual data. This research proposes a language independent
bi-sentence filtering approach based on Polish (not a position-sensitive
language) to English experiments. This cleaning approach was developed on the
TED Talks corpus and also initially tested on the Wikipedia comparable corpus,
but it can be used for any text domain or language pair. The proposed approach
implements various heuristics for sentence comparison. Some of them leverage
synonyms and semantic and structural analysis of text as additional
information. Minimization of data loss was ensured. An improvement in MT system
score with text processed using the tool is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04501</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04501</id><created>2015-10-15</created><authors><author><keyname>Tygel</keyname><forenames>Alan</forenames></author><author><keyname>Auer</keyname><forenames>S&#xf6;ren</forenames></author><author><keyname>Debattista</keyname><forenames>Jeremy</forenames></author><author><keyname>Orlandi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Campos</keyname><forenames>Maria Luiza Machado</forenames></author></authors><title>Towards Cleaning-up Open Data Portals: A Metadata Reconciliation
  Approach</title><categories>cs.IR cs.DB</categories><comments>8 pages,10 Figures - Under Revision for ICSC2016</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents an approach for metadata reconciliation, curation and
linking for Open Governamental Data Portals (ODPs). ODPs have been lately the
standard solution for governments willing to put their public data available
for the society. Portal managers use several types of metadata to organize the
datasets, one of the most important ones being the tags. However, the tagging
process is subject to many problems, such as synonyms, ambiguity or
incoherence, among others. As our empiric analysis of ODPs shows, these issues
are currently prevalent in most ODPs and effectively hinders the reuse of Open
Data. In order to address these problems, we develop and implement an approach
for tag reconciliation in Open Data Portals, encompassing local actions related
to individual portals, and global actions for adding a semantic metadata layer
above individual portals. The local part aims to enhance the quality of tags in
a single portal, and the global part is meant to interlink ODPs by establishing
relations between tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04503</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04503</id><created>2015-10-15</created><updated>2015-10-29</updated><authors><author><keyname>Oehmann</keyname><forenames>David</forenames></author><author><keyname>Awada</keyname><forenames>Ahmad</forenames></author><author><keyname>Viering</keyname><forenames>Ingo</forenames></author><author><keyname>Simsek</keyname><forenames>Meryem</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard P.</forenames></author></authors><title>SINR Model with Best Server Association for High Availability Studies of
  Wireless Networks</title><categories>cs.NI</categories><comments>11 pages, 4 figures, accepted for publication in IEEE Wireless
  Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The signal-to-interference-and-noise ratio (SINR) is of key importance for
the analysis and design of wireless networks. For addressing new requirements
imposed on wireless communication, in particular high availability, a highly
accurate modeling of the SINR is needed. We propose a stochastic model of the
SINR distribution where shadow fading is characterized by random variables.
Therein, the impact of shadow fading on the user association is incorporated by
modification of the distributions involved. The SINR model is capable to
describe all parts of the SINR distribution in detail, especially the left tail
which is of interest for studies of high availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04507</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04507</id><created>2015-10-15</created><authors><author><keyname>Elser</keyname><forenames>Dominique</forenames><affiliation>Quantum Information Processing Group</affiliation></author><author><keyname>G&#xfc;nthner</keyname><forenames>Kevin</forenames><affiliation>Quantum Information Processing Group</affiliation></author><author><keyname>Khan</keyname><forenames>Imran</forenames><affiliation>Quantum Information Processing Group</affiliation></author><author><keyname>Stiller</keyname><forenames>Birgit</forenames><affiliation>Quantum Information Processing Group</affiliation></author><author><keyname>Marquardt</keyname><forenames>Christoph</forenames><affiliation>Quantum Information Processing Group</affiliation></author><author><keyname>Leuchs</keyname><forenames>Gerd</forenames><affiliation>Quantum Information Processing Group</affiliation></author><author><keyname>Saucke</keyname><forenames>Karen</forenames><affiliation>Tesat-Spacecom GmbH &amp; Co. KG, Backnang, Germany</affiliation></author><author><keyname>Tr&#xf6;ndle</keyname><forenames>Daniel</forenames><affiliation>Tesat-Spacecom GmbH &amp; Co. KG, Backnang, Germany</affiliation></author><author><keyname>Heine</keyname><forenames>Frank</forenames><affiliation>Tesat-Spacecom GmbH &amp; Co. KG, Backnang, Germany</affiliation></author><author><keyname>Seel</keyname><forenames>Stefan</forenames><affiliation>Tesat-Spacecom GmbH &amp; Co. KG, Backnang, Germany</affiliation></author><author><keyname>Greulich</keyname><forenames>Peter</forenames><affiliation>Tesat-Spacecom GmbH &amp; Co. KG, Backnang, Germany</affiliation></author><author><keyname>Zech</keyname><forenames>Herwig</forenames><affiliation>Tesat-Spacecom GmbH &amp; Co. KG, Backnang, Germany</affiliation></author><author><keyname>G&#xfc;tlich</keyname><forenames>Bj&#xf6;rn</forenames><affiliation>Space Administration, German Aerospace Center</affiliation></author><author><keyname>Richter</keyname><forenames>Ines</forenames><affiliation>Space Administration, German Aerospace Center</affiliation></author><author><keyname>Meyer</keyname><forenames>Rolf</forenames><affiliation>Space Administration, German Aerospace Center</affiliation></author></authors><title>Satellite Quantum Communication via the Alphasat Laser Communication
  Terminal</title><categories>quant-ph cs.CR cs.ET physics.ao-ph physics.optics</categories><comments>International Conference on Space Optical Systems and Applications
  (IEEE ICSOS 2015), October 27 and 28, 2015, New Orleans, USA, 4 pages, 5
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By harnessing quantum effects, we nowadays can use encryption that is in
principle proven to withstand any conceivable attack. These fascinating quantum
features have been implemented in metropolitan quantum networks around the
world. In order to interconnect such networks over long distances, optical
satellite communication is the method of choice. Standard telecommunication
components allow one to efficiently implement quantum communication by
measuring field quadratures (continuous variables). This opens the possibility
to adapt our Laser Communication Terminals (LCTs) to quantum key distribution
(QKD). First satellite measurement campaigns are currently validating our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04516</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04516</id><created>2015-10-15</created><updated>2015-12-31</updated><authors><author><keyname>Guo</keyname><forenames>Yinghao</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Cooperative Local Caching and File Sharing under Heterogeneous File
  Preferences</title><categories>cs.IT cs.NI math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Wireless device-to-device (D2D) caching has been recently introduced as an
effective scheme for reducing the average download delay of mobile terminals
(MTs) in cellular network. Specifically, the MTs first cache popular files in
their local memories in off-peak hours and then exchange the requested files
with each other in the vicinity via D2D communications during the peak hours.
Prior works largely overlook MTs' heterogeneity in file preferences and their
selfish behaviors. In this paper, we practically categorize the MTs into
different interest groups according to their individual file preferences and
jointly investigate each group's caching strategies to reduce the average file
download delay. By modeling MTs' mobilities as homogeneous Poisson point
processes (HPPPs), we analytically characterize MTs' average download delay in
closed-form. We first consider the fully cooperative case where a centralizer
helps all MT groups to make caching decisions. We formulate the problem as a
weighted sum delay minimization problem, through which the minimum delay
trade-offs of different groups are characterized. Next, we study two benchmark
cases under selfish caching, namely, partial and no cooperation, with and
without inter-group file sharing, respectively. The optimal caching
distributions for these two cases are derived. Finally, extensive numerical
examples are presented to compare the delay performances of different schemes
and show the effectiveness of the cooperative D2D caching compared to the two
benchmark cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04521</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04521</id><created>2015-10-15</created><authors><author><keyname>Barto</keyname><forenames>Libor</forenames></author><author><keyname>Opr&#x161;al</keyname><forenames>Jakub</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author></authors><title>The wonderland of reflections</title><categories>math.LO cs.CC cs.LO math.RA</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental fact for the algebraic theory of constraint satisfaction
problems (CSPs) over a fixed template is that pp-interpretations between at
most countable \omega-categorical relational structures have two algebraic
counterparts for their polymorphism clones: a semantic one via the standard
algebraic operators H, S, P, and a syntactic one via clone homomorphisms
(capturing identities). We provide a similar characterization which
incorporates all relational constructions relevant for CSPs, that is,
homomorphic equivalence and adding singletons to cores in addition to
pp-interpretations. For the semantic part we introduce a new construction,
called reflection, and for the syntactic part we find an appropriate weakening
of clone homomorphisms, called h1 clone homomorphisms (capturing identities of
height 1).
  As a consequence, the complexity of the CSP of an at most countable
$\omega$-categorical structure depends only on the identities of height 1
satisfied in its polymorphism clone as well as the the natural uniformity
thereon. This allows us in turn to formulate a new elegant dichotomy conjecture
for the CSPs of reducts of finitely bounded homogeneous structures.
  Finally, we reveal a close connection between h1 clone homomorphisms and the
notion of compatibility with projections used in the study of the lattice of
interpretability types of varieties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04524</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04524</id><created>2015-10-15</created><authors><author><keyname>Fau&#xdf;</keyname><forenames>Michael</forenames></author><author><keyname>Zoubir</keyname><forenames>Abdelhak M.</forenames></author></authors><title>Old Bands, New Tracks---Revisiting the Band Model for Robust Hypothesis
  Testing</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, submitted for publication in the IEEE
  Transactions on Signal Processing</comments><msc-class>62C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The density band model proposed by Kassam for robust hypothesis testing is
revisited in this paper. First, a novel criterion for the general
characterization of least favorable distributions is proposed, which unifies
existing results. This criterion is then used to derive an implicit definition
of the least favorable distributions under band uncertainties. In contrast to
the existing solution, it only requires two scalar values to be determined and
eliminates the need for case-by-case statements. Based on this definition, a
generic fixed-point algorithm is proposed that iteratively calculates the least
favorable distributions for arbitrary band specifications. Finally, three
different types of robust tests that emerge from band models are discussed and
a numerical example is presented to illustrate their potential use in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04526</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04526</id><created>2015-10-15</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Dumont</keyname><forenames>Louis</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author></authors><title>Algebraic Diagonals and Walks: Algorithms, Bounds, Complexity</title><categories>cs.SC</categories><comments>This is an extended version of arXiv:1510.04080 with more precise
  results and more detailed proofs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diagonal of a multivariate power series F is the univariate power series
Diag(F) generated by the diagonal terms of F. Diagonals form an important class
of power series; they occur frequently in number theory, theoretical physics
and enumerative combinatorics. We study algorithmic questions related to
diagonals in the case where F is the Taylor expansion of a bivariate rational
function. It is classical that in this case Diag(F) is an algebraic function.
We propose an algorithm that computes an annihilating polynomial for Diag(F).
We give a precise bound on the size of this polynomial and show that
generically, this polynomial is the minimal polynomial and that its size
reaches the bound. The algorithm runs in time quasi-linear in this bound, which
grows exponentially with the degree of the input rational function. We then
address the related problem of enumerating directed lattice walks. The insight
given by our study leads to a new method for expanding the generating power
series of bridges, excursions and meanders. We show that their first N terms
can be computed in quasi-linear complexity in N, without first computing a very
large polynomial equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04563</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04563</id><created>2015-10-15</created><authors><author><keyname>Simon</keyname><forenames>Konrad</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author></authors><title>Elasticity-based Matching by Minimizing the Symmetric Difference of
  Shapes</title><categories>cs.CV cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of matching two shapes assuming these shapes are
related by an elastic deformation. Using linearized elasticity theory and the
finite element method we seek an elastic deformation that is caused by simple
external boundary forces and accounts for the difference between the two
shapes. Our main contribution is in proposing a cost function and an
optimization procedure to minimize the symmetric difference between the
deformed and the target shapes as an alternative to point matches that guide
the matching in other techniques. We show how to approximate the nonlinear
optimization problem by a sequence of convex problems. We demonstrate the
utility of our method in experiments and compare it to an ICP-like matching
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04565</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04565</id><created>2015-10-15</created><authors><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander G.</forenames></author></authors><title>Beyond Spatial Pyramid Matching: Space-time Extended Descriptor for
  Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of generating video features for action recognition.
The spatial pyramid and its variants have been very popular feature models due
to their success in balancing spatial location encoding and spatial invariance.
Although it seems straightforward to extend spatial pyramid to the temporal
domain (spatio-temporal pyramid), the large spatio-temporal diversity of
unconstrained videos and the resulting significantly higher dimensional
representations make it less appealing. This paper introduces the space-time
extended descriptor, a simple but efficient alternative way to include the
spatio-temporal location into the video features. Instead of only coding motion
information and leaving the spatio-temporal location to be represented at the
pooling stage, location information is used as part of the encoding step. This
method is a much more effective and efficient location encoding method as
compared to the fixed grid model because it avoids the danger of over
committing to artificial boundaries and its dimension is relatively low.
Experimental results on several benchmark datasets show that, despite its
simplicity, this method achieves comparable or better results than
spatio-temporal pyramid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04583</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04583</id><created>2015-10-15</created><authors><author><keyname>Mohammadi</keyname><forenames>Shahin</forenames></author><author><keyname>Zuckerman</keyname><forenames>Neta</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author></authors><title>A Critical Survey of Deconvolution Methods for Separating cell-types in
  Complex Tissues</title><categories>cs.CE q-bio.QM</categories><comments>Submitted to the Special Issue of Proceedings of IEEE - Foundations&amp;
  Applications of Science of Information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying concentrations of components from an observed mixture is a
fundamental problem in signal processing. It has diverse applications in fields
ranging from hyperspectral imaging to denoising biomedical sensors. This paper
focuses on in-silico deconvolution of signals associated with complex tissues
into their constitutive cell-type specific components, along with a
quantitative characterization of the cell-types. Deconvolving mixed
tissues/cell-types is useful in the removal of contaminants (e.g., surrounding
cells) from tumor biopsies, as well as in monitoring changes in the cell
population in response to treatment or infection. In these contexts, the
observed signal from the mixture of cell-types is assumed to be a linear
combination of the expression levels of genes in constitutive cell-types. The
goal is to use known signals corresponding to individual cell-types along with
a model of the mixing process to cast the deconvolution problem as a suitable
optimization problem.
  In this paper, we present a survey of models, methods, and assumptions
underlying deconvolution techniques. We investigate the choice of the different
loss functions for evaluating estimation error, constraints on solutions,
preprocessing and data filtering, feature selection, and regularization to
enhance the quality of solutions, along with the impact of these choices on the
performance of regression-based methods for deconvolution. We assess different
combinations of these factors and use detailed statistical measures to evaluate
their effectiveness. We identify shortcomings of current methods and avenues
for further investigation. For many of the identified shortcomings, such as
normalization issues and data filtering, we provide new solutions. We summarize
our findings in a prescriptive step-by-step process, which can be applied to a
wide range of deconvolution problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04585</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04585</id><created>2015-10-15</created><authors><author><keyname>Li</keyname><forenames>Kezhi</forenames></author></authors><title>A Brief Survey of Image Processing Algorithms in Electrical Capacitance
  Tomography</title><categories>cs.CV</categories><comments>Internal Report, MRRC, University of Cambridge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To study the fundamental physics of complex multiphase flow systems using
advanced measurement techniques, especially the electrical capacitance
tomography (ECT) approach, this article carries out an initial literature
review of the ECT method from a point of view of signal processing and
algorithm design. After introducing the physical laws governing the ECT system,
we will focus on various reconstruction techniques that are capable to recover
the image of the internal characteristics of a specified region based on the
measuring capacitances of multi-electrode sensors surrounding the region. Each
technique has its own advantages and limitations, and many algorithms have been
examined by simulations or experiments. Future researches in 3D reconstruction
and other potential improvements of the system are discussed in the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04586</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04586</id><created>2015-10-15</created><authors><author><keyname>McGregor</keyname><forenames>John D.</forenames></author><author><keyname>Monteith</keyname><forenames>J. Yates</forenames></author></authors><title>It Takes a Socio-Technical Ecosystem</title><categories>cs.SE</categories><comments>2 pages</comments><acm-class>D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are both technical and social issues regarding the design of
sustainable scientific software. Scientists want continuously evolving systems
that capture the most recent knowledge while developers and architects want
sufficiently stable requirements to ensure correctness and efficiency. A
socio-technical ecosystem provides the environment in which these issues can be
traded off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04589</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04589</id><created>2015-10-15</created><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Meidlinger</keyname><forenames>Michael</forenames></author><author><keyname>Ghanaatian</keyname><forenames>Reza</forenames></author><author><keyname>Matz</keyname><forenames>Gerald</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>A Fully-Unrolled LDPC Decoder Based on Quantized Message Passing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a finite alphabet message passing algorithm for
LDPC codes that replaces the standard min-sum variable node update rule by a
mapping based on generic look-up tables. This mapping is designed in a way that
maximizes the mutual information between the decoder messages and the codeword
bits. We show that our decoder can deliver the same error rate performance as
the conventional decoder with a much smaller message bit-width. Finally, we use
the proposed algorithm to design a fully unrolled LDPC decoder hardware
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04590</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04590</id><created>2015-10-15</created><authors><author><keyname>Wang</keyname><forenames>Zhengyu</forenames></author></authors><title>An Improved Randomized Data Structure for Dynamic Graph Connectivity</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a randomized algorithm for dynamic graph connectivity. With
failure probability less than $1/n^c$ (for any constant $c$ we choose), our
solution has worst case running time $O(\log^3 n)$ per edge insertion,
$O(\log^4 n)$ per edge deletion, and $O(\log n/\log\log n)$ per query, where
$n$ is the number of vertices. The previous best algorithm has worst case
running time $O(\log^4 n)$ per edge insertion and $O(\log^5 n)$ per edge
deletion. The improvement is made by reducing the randomness used in the
previous result, so that we save a $\log n$ factor in update time.
  Specifically, \cite{kapron2013dynamic} uses $\log n$ copies of a data
structure in order to boost a success probability from $1/2$ to $1-n^{-c}$. We
show that, in fact though, because of the special structure of their algorithm,
this boosting via repetition is unnecessary. Rather, we can still obtain the
same correctness guarantee with high probability by arguing via a new
invariant, without repetition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04591</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04591</id><created>2015-10-15</created><authors><author><keyname>Li</keyname><forenames>Shengguo</forenames></author><author><keyname>Liao</keyname><forenames>Xiangke</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Jiang</keyname><forenames>Hao</forenames></author></authors><title>New fast divide-and-conquer algorithms for the symmetric tridiagonal
  eigenvalue problem</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two accelerated divide-and-conquer algorithms are proposed for
the symmetric tridiagonal eigenvalue problem, which cost $O(N^2r)$ {flops} in
the worst case, where $N$ is the dimension of the matrix and $r$ is a modest
number depending on the distribution of eigenvalues. Both of these algorithms
use hierarchically semiseparable (HSS) matrices to approximate some
intermediate eigenvector matrices which are Cauchy-like matrices and are
off-diagonally low-rank. The difference of these two versions lies in using
different HSS construction algorithms, one (denoted by {ADC1}) uses a
structured low-rank approximation method and the other ({ADC2}) uses a
randomized HSS construction algorithm. For the ADC2 algorithm, a method is
proposed to estimate the off-diagonal rank. Numerous experiments have been done
to show their stability and efficiency. These algorithms are implemented in
parallel in a shared memory environment, and some parallel implementation
details are included. Comparing the ADCs with highly optimized multithreaded
libraries such as Intel MKL, we find that ADCs could be more than 6x times
faster for some large matrices with few deflations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04595</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04595</id><created>2015-10-15</created><authors><author><keyname>Kounades-Bastian</keyname><forenames>Dionyssos</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>A Variational EM Algorithm for the Separation of Time-Varying
  Convolutive Audio Mixtures</title><categories>cs.SD</categories><comments>13 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of separating audio sources from
time-varying convolutive mixtures. We propose a probabilistic framework based
on the local complex-Gaussian model combined with non-negative matrix
factorization. The time-varying mixing filters are modeled by a continuous
temporal stochastic process. We present a variational expectation-maximization
(VEM) algorithm that employs a Kalman smoother to estimate the time-varying
mixing matrix, and that jointly estimate the source parameters. The sound
sources are then separated by Wiener filters constructed with the estimators
provided by the VEM algorithm. Extensive experiments on simulated data show
that the proposed method outperforms a block-wise version of a state-of-the-art
baseline method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04597</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04597</id><created>2015-10-15</created><authors><author><keyname>Fay</keyname><forenames>Damien</forenames></author></authors><title>Predictive partitioning for efficient BFS traversal in social networks</title><categories>cs.DS cs.SI stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how graph structure can be used to drastically reduce
the computational bottleneck of the Breadth First Search algorithm (the
foundation of many graph traversal techniques). In particular, we address
parallel implementations where the bottleneck is the number of messages between
processors emitted at the peak iteration. First, we derive an expression for
the expected degree distribution of vertices in the frontier of the algorithm
which is shown to be highly skewed. Subsequently, we derive an expression for
the expected message along an edge in a particular iteration. This skew
suggests a weighted, iteration based, partition would be advantageous.
Employing the METIS algorithm we then show empirically that such partitions can
reduce the message overhead by up to 50% in some particular instances and in
the order of 20% on average. These results have implications for graph
processing in multiprocessor and distributed computing environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04600</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04600</id><created>2015-10-15</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author><author><keyname>Glinkowski</keyname><forenames>Wojciech</forenames></author></authors><title>Telemedicine as a special case of Machine Translation</title><categories>cs.CL</categories><doi>10.1016/j.compmedimag.2015.09.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine translation is evolving quite rapidly in terms of quality. Nowadays,
we have several machine translation systems available in the web, which provide
reasonable translations. However, these systems are not perfect, and their
quality may decrease in some specific domains. This paper examines the effects
of different training methods when it comes to Polish - English Statistical
Machine Translation system used for the medical data. Numerous elements of the
EMEA parallel text corpora and not related OPUS Open Subtitles project were
used as the ground for creation of phrase tables and different language models
including the development, tuning and testing of these translation systems. The
BLEU, NIST, METEOR, and TER metrics have been used in order to evaluate the
results of various systems. Our experiments deal with the systems that include
POS tagging, factored phrase models, hierarchical models, syntactic taggers,
and other alignment methods. We also executed a deep analysis of Polish data as
preparatory work before automatized data processing such as true casing or
punctuation normalization phase. Normalized metrics was used to compare
results. Scores lower than 15% mean that Machine Translation engine is unable
to provide satisfying quality, scores greater than 30% mean that translations
should be understandable without problems and scores over 50 reflect adequate
translations. The average results of Polish to English translations scores for
BLEU, NIST, METEOR, and TER were relatively high and ranged from 70,58 to
82,72. The lowest score was 64,38. The average results ranges for English to
Polish translations were little lower (67,58 - 78,97). The real-life
implementations of presented high quality Machine Translation Systems are
anticipated in general medical practice and telemedicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04601</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04601</id><created>2015-10-15</created><updated>2015-12-05</updated><authors><author><keyname>Remez</keyname><forenames>Tal</forenames></author><author><keyname>Litany</keyname><forenames>Or</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex</forenames></author></authors><title>A Picture is Worth a Billion Bits: Real-Time Image Reconstruction from
  Dense Binary Pixels</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pursuit of smaller pixel sizes at ever increasing resolution in digital
image sensors is mainly driven by the stringent price and form-factor
requirements of sensors and optics in the cellular phone market. Recently, Eric
Fossum proposed a novel concept of an image sensor with dense sub-diffraction
limit one-bit pixels jots, which can be considered a digital emulation of
silver halide photographic film. This idea has been recently embodied as the
EPFL Gigavision camera. A major bottleneck in the design of such sensors is the
image reconstruction process, producing a continuous high dynamic range image
from oversampled binary measurements. The extreme quantization of the Poisson
statistics is incompatible with the assumptions of most standard image
processing and enhancement frameworks. The recently proposed maximum-likelihood
(ML) approach addresses this difficulty, but suffers from image artifacts and
has impractically high computational complexity. In this work, we study a
variant of a sensor with binary threshold pixels and propose a reconstruction
algorithm combining an ML data fitting term with a sparse synthesis prior. We
also show an efficient hardware-friendly real-time approximation of this
inverse operator.Promising results are shown on synthetic data as well as on
HDR data emulated using multiple exposures of a regular CMOS sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04608</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04608</id><created>2015-10-15</created><authors><author><keyname>Khanimov</keyname><forenames>Michael</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Delaunay Triangulations of Degenerate Point Sets</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Delaunay triangulation (DT) is one of the most common and useful
triangulations of point sets $P$ in the plane. DT is not unique when $P$ is
degenerate, specifically when it contains quadruples of co-circular points. One
way to achieve uniqueness is by applying a small (or infinitesimal)
perturbation to $P$.
  We consider a specific perturbation of such degenerate sets, in which the
coordinates of each point are independently perturbed by normally distributed
small quantities, and investigate the effect of such perturbations on the DT of
the set. We focus on two special configurations, where (1) the points of $P$
form a uniform grid, and (2) the points of $P$ are vertices of a regular
polygon.
  We present interesting (and sometimes surprising) empirical findings and
properties of the perturbed DTs for these cases, and give theoretical
explanations to some of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04609</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04609</id><created>2015-10-15</created><authors><author><keyname>Singh</keyname><forenames>Bharat</forenames></author><author><keyname>De</keyname><forenames>Soham</forenames></author><author><keyname>Zhang</keyname><forenames>Yangmuzi</forenames></author><author><keyname>Goldstein</keyname><forenames>Thomas</forenames></author><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author></authors><title>Layer-Specific Adaptive Learning Rates for Deep Networks</title><categories>cs.CV cs.AI cs.LG cs.NE</categories><comments>ICMLA 2015, deep learning, adaptive learning rates for training,
  layer specific learning rate</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing complexity of deep learning architectures is resulting in
training time requiring weeks or even months. This slow training is due in part
to vanishing gradients, in which the gradients used by back-propagation are
extremely large for weights connecting deep layers (layers near the output
layer), and extremely small for shallow layers (near the input layer); this
results in slow learning in the shallow layers. Additionally, it has also been
shown that in highly non-convex problems, such as deep neural networks, there
is a proliferation of high-error low curvature saddle points, which slows down
learning dramatically. In this paper, we attempt to overcome the two above
problems by proposing an optimization method for training deep neural networks
which uses learning rates which are both specific to each layer in the network
and adaptive to the curvature of the function, increasing the learning rate at
low curvature points. This enables us to speed up learning in the shallow
layers of the network and quickly escape high-error low curvature saddle
points. We test our method on standard image classification datasets such as
MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy
as well as reduces the required training time over standard algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04616</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04616</id><created>2015-10-15</created><authors><author><keyname>Parada</keyname><forenames>Pablo Peso</forenames></author><author><keyname>Sharma</keyname><forenames>Dushyant</forenames></author><author><keyname>van Waterschoot</keyname><forenames>Toon</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick A.</forenames></author></authors><title>Evaluating the Non-Intrusive Room Acoustics Algorithm with the ACE
  Challenge</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a single channel data driven method for non-intrusive estimation
of full-band reverberation time and full-band direct-to-reverberant ratio. The
method extracts a number of features from reverberant speech and builds a model
using a recurrent neural network to estimate the reverberant acoustic
parameters. We explore three configurations by including different data and
also by combining the recurrent neural network estimates using a support vector
machine. Our best method to estimate DRR provides a Root Mean Square Deviation
(RMSD) of 3.84 dB and a RMSD of 43.19 % for T60 estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04617</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04617</id><created>2015-10-15</created><authors><author><keyname>Ullrich</keyname><forenames>Mario</forenames></author></authors><title>A lower bound for the dispersion on the torus</title><categories>cs.CC math.NA</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the volume of the largest axis-parallel box in the
$d$-dimensional torus that contains no point of a given point set
$\mathcal{P}_n$ with $n$ elements. We prove that, for all natural numbers $d,
n$ and every point set $\mathcal{P}_n$, this volume is bounded from below by
$\min\{1,d/n\}$. This implies the same lower bound for the discrepancy on the
torus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04620</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04620</id><created>2015-10-15</created><authors><author><keyname>Xiong</keyname><forenames>Feifei</forenames></author><author><keyname>Goetze</keyname><forenames>Stefan</forenames></author><author><keyname>Meyer</keyname><forenames>Bernd T.</forenames></author></authors><title>Joint Estimation of Reverberation Time and Direct-to-Reverberation Ratio
  from Speech using Auditory-Inspired Features</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind estimation of acoustic room parameters such as the reverberation time
$T_\mathrm{60}$ and the direct-to-reverberation ratio ($\mathrm{DRR}$) is still
a challenging task, especially in case of blind estimation from reverberant
speech signals. In this work, a novel approach is proposed for joint estimation
of $T_\mathrm{60}$ and $\mathrm{DRR}$ from wideband speech in noisy conditions.
2D Gabor filters arranged in a filterbank are exploited for extracting
features, which are then used as input to a multi-layer perceptron (MLP). The
MLP output neurons correspond to specific pairs of $(T_\mathrm{60},
\mathrm{DRR})$ estimates; the output is integrated over time, and a simple
decision rule results in our estimate. The approach is applied to
single-microphone fullband speech signals provided by the Acoustic
Characterization of Environments (ACE) Challenge. Our approach outperforms the
baseline systems with median errors of close-to-zero and -1.5 dB for the
$T_\mathrm{60}$ and $\mathrm{DRR}$ estimates, respectively, while the
calculation of estimates is 5.8 times faster compared to the baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04622</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04622</id><created>2015-10-15</created><authors><author><keyname>Abboud</keyname><forenames>Amir</forenames></author><author><keyname>Backurs</keyname><forenames>Arturs</forenames></author><author><keyname>Hansen</keyname><forenames>Thomas Dueholm</forenames></author><author><keyname>Williams</keyname><forenames>Virginia Vassilevska</forenames></author><author><keyname>Zamir</keyname><forenames>Or</forenames></author></authors><title>Subtree Isomorphism Revisited</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Subtree Isomorphism problem asks whether a given tree is contained in
another given tree. The problem is of fundamental importance and has been
studied since the 1960s. For some variants, e.g., ordered trees, near-linear
time algorithms are known, but for the general case truly subquadratic
algorithms remain elusive.
  Our first result is a reduction from the Orthogonal Vectors problem to
Subtree Isomorphism, showing that a truly subquadratic algorithm for the latter
refutes the Strong Exponential Time Hypothesis (SETH).
  In light of this conditional lower bound, we focus on natural special cases
for which no truly subquadratic algorithms are known. We classify these cases
against the quadratic barrier, showing in particular that:
  -- Even for binary, rooted trees, a truly subquadratic algorithm refutes
SETH.
  -- Even for rooted trees of depth $O(\log\log{n})$, where $n$ is the total
number of vertices, a truly subquadratic algorithm refutes SETH.
  -- For every constant $d$, there is a constant $\epsilon_d&gt;0$ and a
randomized, truly subquadratic algorithm for degree-$d$ rooted trees of depth
at most $(1+ \epsilon_d) \log_{d}{n}$. In particular, there is an $O(\min\{
2.85^h ,n^2 \})$ algorithm for binary trees of depth $h$.
  Our reductions utilize new &quot;tree gadgets&quot; that are likely useful for future
SETH-based lower bounds for problems on trees. Our upper bounds apply a
folklore result from randomized decision tree complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04629</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04629</id><created>2015-10-15</created><updated>2015-10-16</updated><authors><author><keyname>Thummala</keyname><forenames>Vamsi</forenames></author><author><keyname>Chase</keyname><forenames>Jeff</forenames></author></authors><title>SAFE: A Declarative Trust Management System with Linked Credentials</title><categories>cs.CR cs.LO</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present SAFE, an integrated system for managing trust using a logic-based
declarative language. Logical trust systems authorize each request by
constructing a proof from a context---a set of authenticated logic statements
representing credentials and policies issued by various principals in a
networked system.
  A key barrier to practical use of logical trust systems is the problem of
managing proof contexts: identifying, validating, and assembling the
credentials and policies that are relevant to each trust decision. This paper
describes a new approach to managing proof contexts using context linking and
caching. Credentials and policies are stored as certified logic sets named by
secure identifiers in a shared key-value store. SAFE offers language constructs
to build and modify logic sets, link sets to form unions, pass them by
reference, and add them to proof contexts. SAFE fetches and validates
credential sets on demand and caches them in the authorizer. We evaluate and
discuss our experience using SAFE to build secure services based on case
studies drawn from practice: a secure name service resolver, a secure proxy
shim for a key value store, and an authorization module for a networked
infrastructure-as-a-service system with a federated trust structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04632</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04632</id><created>2015-10-15</created><authors><author><keyname>Joubert</keyname><forenames>Daniel J</forenames></author><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author></authors><title>Monte Carlo Dynamically Weighted Importance Sampling For Finite Element
  Model Updating</title><categories>cs.NA</categories><comments>Submitted to the IMAC-XXXIV</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Finite Element Method (FEM) is generally unable to accurately predict
natural frequencies and mode shapes of structures (eigenvalues and
eigenvectors). Engineers develop numerical methods and a variety of techniques
to compensate for this misalignment of modal properties, between experimentally
measured data and the computed result from the FEM of structures. In this paper
we compare two indirect methods of updating namely, the Adaptive Metropolis
Hastings and a newly applied algorithm called Monte Carlo Dynamically Weighted
Importance Sampling (MCDWIS). The approximation of a posterior predictive
distribution is based on Bayesian inference of continuous multivariate Gaussian
probability density functions, defining the variability of physical properties
affected by forced vibration. The motivation behind applying MCDWIS is in the
complexity of computing normalizing constants in higher dimensional or
multimodal systems. The MCDWIS accounts for this intractability by analytically
computing importance sampling estimates at each time step of the algorithm. In
addition, a dynamic weighting step with an Adaptive Pruned Enriched Population
Control Scheme (APEPCS) allows for further control over weighted samples and
population size. The performance of the MCDWIS simulation is graphically
illustrated for all algorithm dependent parameters and show unbiased, stable
sample estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04637</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04637</id><created>2015-10-15</created><updated>2016-01-26</updated><authors><author><keyname>Nugent</keyname><forenames>Steve</forenames></author><author><keyname>Voight</keyname><forenames>John</forenames></author></authors><title>On the arithmetic dimension of triangle groups</title><categories>math.NT cs.DM</categories><comments>27 pages; several corrections, including revisions to the proof of
  Lemma 4.10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Delta=\Delta(a,b,c)$ be a hyperbolic triangle group, a Fuchsian group
obtained from reflections in the sides of a triangle with angles
$\pi/a,\pi/b,\pi/c$ drawn on the hyperbolic plane. We define the arithmetic
dimension of $\Delta$ to be the number of split real places of the quaternion
algebra generated by $\Delta$ over its (totally real) invariant trace field.
Takeuchi has determined explicitly all triples $(a,b,c)$ with arithmetic
dimension $1$, corresponding to the arithmetic triangle groups. We show more
generally that the number of triples with fixed arithmetic dimension is finite,
and we present an efficient algorithm to completely enumerate the list of
triples of bounded arithmetic dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04645</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04645</id><created>2015-10-15</created><updated>2015-12-15</updated><authors><author><keyname>Ronellenfitsch</keyname><forenames>Henrik</forenames></author><author><keyname>Timme</keyname><forenames>Marc</forenames></author><author><keyname>Witthaut</keyname><forenames>Dirk</forenames></author></authors><title>A Dual Method for Computing Power Transfer Distribution Factors</title><categories>physics.comp-ph cs.CE cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power Transfer Distribution Factors (PTDFs) play a crucial role in power grid
security analysis, planning, and redispatch. Fast calculation of the PTDFs is
therefore of great importance. In this paper, we present a non-approximative
dual method of computing PTDFs. It uses power flows along topological cycles of
the network but still relies on simple matrix algebra. At the core, our method
changes the size of the matrix that needs to be inverted to calculate the PTDFs
from $N\times N$, where $N$ is the number of buses, to $(L-N+1)\times (L-N+1)$,
where $L$ is the number of lines and $L-N+1$ is the number of independent
cycles (closed loops) in the network while remaining mathematically fully
equivalent. For power grids containing a relatively small number of cycles, the
method offers a significant speedup of numerical calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04652</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04652</id><created>2015-10-15</created><authors><author><keyname>Exman</keyname><forenames>Iaakov</forenames></author></authors><title>Linear Software Models: Key Ideas</title><categories>cs.SE</categories><comments>26 pages, 4 figures</comments><acm-class>D.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Software Models is a systematic effort to formulate a theory of
software systems neatly based upon standard mathematics, viz. linear algebra.
It has appeared in a series of papers dealing with various aspects of the
theory. But one was lacking a single source for its key ideas. This paper
concisely distills foundational ideas and results obtained within Linear
Software Models. First and foremost we claim that one must have a deep
comprehension of the theory of software - the aim of this effort - before
embarking into theory and practice of software engineering. The Modularity
Matrix is the central algebraic structure of the software theory: it is the
source of quantitative modularity criteria; it displays high cohesion, i.e.
high sparsity; a Standard Modularity Matrix is defined - square and
block-diagonal - enabling designs comparison for any software systems in a
Software Design Laboratory. It triggers formulation of novel open questions
waiting for resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04658</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04658</id><created>2015-10-15</created><updated>2016-02-02</updated><authors><author><keyname>Fairbanks</keyname><forenames>James P.</forenames></author><author><keyname>Sanders</keyname><forenames>Geoffrey D.</forenames></author><author><keyname>Bader</keyname><forenames>David A.</forenames></author></authors><title>Spectral Partitioning with Blends of Eigenvectors</title><categories>cs.NA</categories><comments>32 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many common methods for data analysis rely on linear algebra. We provide new
results connecting data analysis error to numerical accuracy, which leads to
the first meaningful stopping criterion for two way spectral partitioning. More
generally, we provide pointwise convergence guarantees so that blends (linear
combinations) of eigenvectors can be employed to solve data analysis problems
with confidence in their accuracy. We demonstrate this theory on an accessible
model problem, the Ring of Cliques, by deriving the relevant eigenpairs and
comparing the predicted results to numerical solutions. These results bridge
the gap between linear algebra based data analysis methods and the convergence
theory of iterative approximation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04661</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04661</id><created>2015-10-15</created><authors><author><keyname>Fu</keyname><forenames>Peng</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>A Type-Theoretic Approach to Resolution</title><categories>cs.LO</categories><comments>21 page (incl appendices) in LOPSTR'15 LNCS post-proceedings, 2015.
  arXiv admin note: substantial text overlap with arXiv:1506.06166</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new type-theoretic approach to SLD-resolution and Horn-clause
logic programming. It views Horn formulas as types, and derivations for a given
query as a construction of the inhabitant (a proof-term) for the type given by
the query. We propose a method of program transformation that allows to
transform logic programs in such a way that proof evidence is computed
alongside SLD-derivations. We discuss two applications of this approach: in
recently proposed productivity theory of structural resolution, and in type
class inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04676</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04676</id><created>2015-10-15</created><updated>2015-10-26</updated><authors><author><keyname>Aum&#xfc;ller</keyname><forenames>Martin</forenames></author><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author><author><keyname>Klaue</keyname><forenames>Pascal</forenames></author></authors><title>How Good is Multi-Pivot Quicksort?</title><categories>cs.DS</categories><comments>Submitted to a journal, v2: Fixed statement of Gibb's inequality</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Pivot Quicksort refers to variants of classical quicksort where in the
partitioning step $k$ pivots are used to split the input into $k + 1$ segments.
For many years, multi-pivot quicksort was regarded as impractical, but in 2009
a 2-pivot approach by Yaroslavskiy, Bentley, and Bloch was chosen as the
standard sorting algorithm in Sun's Java 7. In 2014 at ALENEX, Kushagra et al.
introduced an even faster algorithm that uses three pivots. This paper studies
what possible advantages multi-pivot quicksort might offer in general. The
contributions are as follows: Natural comparison-optimal algorithms for
multi-pivot quicksort are devised and analyzed. The analysis shows that the
benefits of using multiple pivots with respect to the average comparison count
are marginal and these strategies are inferior to simpler strategies such as
the well known median-of-$k$ approach. A substantial part of the partitioning
cost is caused by rearranging elements. A rigorous analysis of an algorithm for
rearranging elements in the partitioning step is carried out, observing mainly
how often array cells are accessed during partitioning. The algorithm behaves
best if 3 or 5 pivots are used. Experiments show that this translates into good
cache behavior and is closest to predicting observed running times of
multi-pivot quicksort algorithms. Finally, it is studied how choosing pivots
from a sample affects sorting cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04684</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04684</id><created>2015-10-15</created><authors><author><keyname>Zhang</keyname><forenames>Yanru</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Social Network Enhanced Device-to-Device Communication Underlaying
  Cellular Networks</title><categories>cs.SI cs.NI</categories><comments>5 pages, 6 figures</comments><journal-ref>IEEE/CIC International Conference on Communications in China 1
  (2013) 182 - 186 (2013)</journal-ref><doi>10.1109/ICCChinaW.2013.6670590</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communication has seen as a major technology to
overcome the imminent wireless capacity crunch and to enable new application
services. In this paper, we propose a social-aware approach for optimizing D2D
communication by exploiting two layers: the social network and the physical
wireless layers. First we formulate the physical layer D2D network according to
users' encounter histories. Subsequently, we propose an approach, based on the
so-called Indian Buffet Process, so as to model the distribution of contents in
users' online social networks. Given the social relations collected by the
Evolved Node B (eNB), we jointly optimize the traffic offloading process in D2D
communication. In addition, we give the Chernoff bound and approximated
cumulative distribution function (CDF) of the offloaded traffic. In the
simulation, we proved the effectiveness of the bound and CDF. The numerical
results based on real traces show that the proposed approach offload the
traffic of eNB's successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04686</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04686</id><created>2015-10-15</created><authors><author><keyname>Andrawis</keyname><forenames>Robert</forenames></author><author><keyname>Bermeo</keyname><forenames>Jose David</forenames></author><author><keyname>Charles</keyname><forenames>James</forenames></author><author><keyname>Fang</keyname><forenames>Jianbin</forenames></author><author><keyname>Fonseca</keyname><forenames>Jim</forenames></author><author><keyname>He</keyname><forenames>Yu</forenames></author><author><keyname>Klimeck</keyname><forenames>Gerhard</forenames></author><author><keyname>Jiang</keyname><forenames>Zhengping</forenames></author><author><keyname>Kubis</keyname><forenames>Tillmann</forenames></author><author><keyname>Mejia</keyname><forenames>Daniel</forenames></author><author><keyname>Lemus</keyname><forenames>Daniel</forenames></author><author><keyname>Povolotskyi</keyname><forenames>Michael</forenames></author><author><keyname>Rubiano</keyname><forenames>Santiago Alonso Perez</forenames></author><author><keyname>Sarangapani</keyname><forenames>Prasad</forenames></author><author><keyname>Zeng</keyname><forenames>Lang</forenames></author></authors><title>NEMO5: Achieving High-end Internode Communication for Performance
  Projection Beyond Moore's Law</title><categories>cs.DC physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic performance predictions of modern nanotransistors require
nonequilibrium Green's functions including incoherent scattering on phonons as
well as inclusion of random alloy disorder and surface roughness effects. The
solution of all these effects is numerically extremely expensive and has to be
done on the world's largest supercomputers due to the large memory requirement
and the high performance demands on the communication network between the
compute nodes. In this work, it is shown that NEMO5 covers all required
physical effects and their combination. Furthermore, it is also shown that
NEMO5's implementation of the algorithm scales very well up to about 178176CPUs
with a sustained performance of about 857 TFLOPS. Therefore, NEMO5 is ready to
simulate future nanotransistors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04692</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04692</id><created>2015-10-15</created><authors><author><keyname>Ruby</keyname><forenames>Rukhsana</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author><author><keyname>Sydor</keyname><forenames>John</forenames></author></authors><title>Reinforcement Learning Based Transmission Strategy of Cognitive User in
  IEEE 802.11 based Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional concept of cognitive radio is the coexistence of primary and
secondary user in multiplexed manner. we consider the opportunistic channel
access scheme in IEEE 802.11 based networks subject to the interference
mitigation scenario. According to the protocol rule and due to the constraint
of message passing, secondary user is unaware of the exact state of the primary
user. In this paper, we have proposed an online algorithm for the secondary
which assist determining a backoff counter or the decision of being idle for
utilizing the time/frequency slot unoccupied by the primary user. Proposed
algorithm is based on conventional reinforcement learning technique namely
Q-Learning. Simulation has been conducted in order to prove the strength of
this algorithm and also results have been compared with our contemporary
solution of this problem where secondary user is aware of some states of
primary user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04705</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04705</id><created>2015-10-15</created><authors><author><keyname>Zhang</keyname><forenames>Yanru</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Exploring Social Ties for Enhanced Device-to-Device Communications in
  Wireless Networks</title><categories>cs.SI cs.NI</categories><comments>6 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1510.04684</comments><journal-ref>Y. Zhang IEEE Global Communications Conference (GLOBECOM) (2013)
  4597-4602</journal-ref><doi>10.1109/GLOCOMW.2013.6855676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Device-to-device (D2D) communications is seen as a major technology to
overcome the imminent wireless capacity crunch and to enable novel application
services. In this paper, we propose a novel, social-aware approach for
optimizing D2D communications by exploiting two network layers: the social
network and the physical, wireless network. First we formulate the physical
layer D2D network according to users' encounter histories. Subsequently, we
propose a novel approach, based on the so-called Indian Buffet Process, so as
to model the distribution of contents in users' online social networks. Given
the online and offline social relations collected by the Evolved Node B, we
jointly optimize the traffic offload process in D2D communication. Simulation
results show that the proposed approach offload the traffic of Evolved Node B
successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04706</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04706</id><created>2015-10-15</created><authors><author><keyname>Baxter</keyname><forenames>John S. H.</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Peters</keyname><forenames>Terry M.</forenames></author></authors><title>Shape Complexes in Continuous Max-Flow Hierarchical Multi-Labeling
  Problems</title><categories>cs.CV</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although topological considerations amongst multiple labels have been
previously investigated in the context of continuous max-flow image
segmentation, similar investigations have yet to be made about shape
considerations in a general and extendable manner. This paper presents shape
complexes for segmentation, which capture more complex shapes by combining
multiple labels and super-labels constrained by geodesic star convexity. Shape
complexes combine geodesic star convexity constraints with hierarchical label
organization, which together allow for more complex shapes to be represented.
This framework avoids the use of co-ordinate system warping techniques to
convert shape constraints into topological constraints, which may be ambiguous
or ill-defined for certain segmentation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04707</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04707</id><created>2015-10-15</created><authors><author><keyname>Senoussaoui</keyname><forenames>M.</forenames></author><author><keyname>Santos</keyname><forenames>J. F.</forenames></author><author><keyname>Falk</keyname><forenames>T. H.</forenames></author></authors><title>SRMR variants for improved blind room acoustics characterization</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Chal- lenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</comments><report-no>ACEChallenge/2015/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reverberation, especially in large rooms, severely degrades speech
recognition performance and speech intelligibility. Since direct measurement of
room characteristics is usually not possible, blind estimation of
reverberation-related metrics such as the reverberation time (RT) and the
direct-to-reverberant energy ratio (DRR) can be valuable information to speech
recognition and enhancement algorithms operating in enclosed environments. The
objective of this work is to evaluate the performance of five variants of blind
RT and DRR estimators based on a modulation spectrum representation of
reverberant speech with single- and multi-channel speech data. These models are
all based on variants of the so-called Speech-to-Reverberation Modulation
Energy Ratio (SRMR). We show that these measures outperform a state-of-the-art
baseline based on maximum-likelihood estimation of sound decay rates in terms
of root-mean square error (RMSE), as well as Pearson correlation. Compared to
the baseline, the best proposed measure, called NSRMR_k , achieves a 23%
relative improvement in terms of RMSE and allows for relative correlation
improvements ranging from 13% to 47% for RT prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04709</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04709</id><created>2015-10-15</created><updated>2015-11-18</updated><authors><author><keyname>Elliott</keyname><forenames>Desmond</forenames></author><author><keyname>Frank</keyname><forenames>Stella</forenames></author><author><keyname>Hasler</keyname><forenames>Eva</forenames></author></authors><title>Multilingual Image Description with Neural Sequence Models</title><categories>cs.CL cs.CV cs.LG cs.NE</categories><comments>Under review as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an approach to multi-language image description
bringing together insights from neural machine translation and neural image
description. To create a description of an image for a given target language,
our sequence generation models condition on feature vectors from the image, the
description from the source language, and/or a multimodal vector computed over
the image and a description in the source language. In image description
experiments on the IAPR-TC12 dataset of images aligned with English and German
sentences, we find significant and substantial improvements in BLEU4 and Meteor
scores for models trained over multiple languages, compared to a monolingual
baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04712</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04712</id><created>2015-10-15</created><authors><author><keyname>Weerakkody</keyname><forenames>Sean</forenames></author><author><keyname>Liu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Son</keyname><forenames>Sang H.</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author></authors><title>A Graph Theoretic Characterization of Perfect Attackability and
  Detection in Distributed Control Systems</title><categories>cs.SY</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the analysis and design of secure Distributed
Control Systems in the face of integrity attacks on sensors and controllers by
external attackers or insiders. In general a DCS consists of many heterogenous
components and agents including sensors, actuators, controllers. Due to its
distributed nature, some agents may start misbehaving to disrupt the system.
This paper first reviews necessary and sufficient conditions for deterministic
detection of integrity attacks carried out by any number of malicious agents,
based on the concept of left invertibility of structural control systems. It
then develops a notion equivalent to structural left invertibility in terms of
vertex separators of a graph. This tool is then leveraged to design minimal
communication networks for DCSs, which ensure that an adversary cannot generate
undetectable attacks. Numerical examples are included to illustrate these
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04728</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04728</id><created>2015-10-15</created><authors><author><keyname>Puchinger</keyname><forenames>Sven</forenames></author><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author><author><keyname>Li</keyname><forenames>Wenhui</forenames></author><author><keyname>Sidorenko</keyname><forenames>Vladimir</forenames></author></authors><title>Row Reduction Applied to Decoding of Rank Metric and Subspace Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to Designs, Codes and Cryptography</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many algebraic codes the main part of decoding can be reduced to row
reduction of a module basis, enabling general, flexible and highly efficient
algorithms. Inspired by this, we develop an approach of transforming matrices
over skew polynomial rings into certain normal forms. We apply this to solve
generalised shift register problems, or Pad\'e approximations, over skew
polynomial rings which occur in error and erasure decoding $\ell$-Interleaved
Gabidulin codes. We obtain an algorithm with complexity $O(\ell \mu^2)$ where
$\mu$ measures the size of the input problem. Further, we show how to
list-$\ell$ decode Mahdavifar--Vardy subspace codes in $O(\ell r^2 m^2)$ time,
where $m$ is a parameter proportional to the dimension of the codewords'
ambient space and $r$ is the dimension of the received subspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04731</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04731</id><created>2015-10-15</created><authors><author><keyname>Joshi</keyname><forenames>Gauri</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author></authors><title>Efficient Replication of Queued Tasks for Latency Reduction in Cloud
  Systems</title><categories>cs.DC cs.IT cs.PF math.IT</categories><comments>presented at Allerton 2015. arXiv admin note: substantial text
  overlap with arXiv:1508.03599</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cloud computing systems, assigning a job to multiple servers and waiting
for the earliest copy to finish is an effective method to combat the
variability in response time of individual servers. Although adding redundant
replicas always reduces service time, the total computing time spent per job
may be higher, thus increasing waiting time in queue. The total time spent per
job is also proportional to the cost of computing resources. We analyze how
different redundancy strategies, for eg. number of replicas, and the time when
they are issued and canceled, affect the latency and computing cost. We get the
insight that the log-concavity of the service time distribution is a key factor
in determining whether adding redundancy reduces latency and cost. If the
service distribution is log-convex, then adding maximum redundancy reduces both
latency and cost. And if it is log-concave, then having fewer replicas and
canceling the redundant requests early is more effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04734</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04734</id><created>2015-10-15</created><authors><author><keyname>Subotin</keyname><forenames>Michael</forenames></author><author><keyname>Davis</keyname><forenames>Anthony R.</forenames></author></authors><title>A Method for Modeling Co-Occurrence Propensity of Clinical Codes with
  Application to ICD-10-PCS Auto-Coding</title><categories>cs.CL</categories><comments>Submitted to Journal of the American Medical Informatics Association,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective. Natural language processing methods for medical auto-coding, or
automatic generation of medical billing codes from electronic health records,
generally assign each code independently of the others. They may thus assign
codes for closely related procedures or diagnoses to the same document, even
when they do not tend to occur together in practice, simply because the right
choice can be difficult to infer from the clinical narrative.
  Materials and Methods. We propose a method that injects awareness of the
propensities for code co-occurrence into this process. First, a model is
trained to estimate the conditional probability that one code is assigned by a
human coder, given than another code is known to have been assigned to the same
document. Then, at runtime, an iterative algorithm is used to apply this model
to the output of an existing statistical auto-coder to modify the confidence
scores of the codes.
  Results. We tested this method in combination with a primary auto-coder for
ICD-10 procedure codes, achieving a 12% relative improvement in F-score over
the primary auto-coder baseline.
  Discussion. The proposed method can be used, with appropriate features, in
combination with any auto-coder that generates codes with different levels of
confidence.
  Conclusion. The promising results obtained for ICD-10 procedure codes suggest
that the proposed method may have wider applications in auto-coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04747</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04747</id><created>2015-10-15</created><updated>2016-01-22</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Shi</keyname><forenames>Yang</forenames></author><author><keyname>Niranjan</keyname><forenames>U. N.</forenames></author></authors><title>Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse
  Perturbations</title><categories>cs.LG cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust tensor CP decomposition involves decomposing a tensor into low rank
and sparse components. We propose a novel non-convex iterative algorithm with
guaranteed recovery. It alternates between low-rank CP decomposition through
gradient ascent (a variant of the tensor power method), and hard thresholding
of the residual. We prove convergence to the globally optimal solution under
natural incoherence conditions on the low rank component, and bounded level of
sparse perturbations. We compare our method with natural baselines which apply
robust matrix PCA either to the {\em flattened} tensor, or to the matrix slices
of the tensor. Our method can provably handle a far greater level of
perturbation when the sparse tensor is block-structured. This naturally occurs
in many applications such as the activity detection task in videos. Our
experiments validate these findings. Thus, we establish that tensor methods can
tolerate a higher level of gross corruptions compared to matrix methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04748</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04748</id><created>2015-10-15</created><authors><author><keyname>Ramos</keyname><forenames>Marcus V. M.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Almeida</keyname><forenames>Jos&#xe9; Carlos Bacelar</forenames></author></authors><title>Formalization of the pumping lemma for context-free languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-free languages (CFLs) are highly important in computer language
processing technology as well as in formal language theory. The Pumping Lemma
is a property that is valid for all context-free languages, and is used to show
the existence of non context-free languages. This paper presents a
formalization, using the Coq proof assistant, of the Pumping Lemma for
context-free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04763</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04763</id><created>2015-10-15</created><authors><author><keyname>Noor-A-Rahim</keyname><forenames>Md.</forenames></author><author><keyname>Lechner</keyname><forenames>Gottfried</forenames></author><author><keyname>Nguyen</keyname><forenames>Khoa D.</forenames></author></authors><title>Density Evolution Analysis of Spatially Coupled LDPC Codes Over BIAWGN
  Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the density evolution analysis of spatially coupled
low-density parity-check (SC-LDPC) codes over binary input additive white
Gaussian noise (BIAWGN) channels under the belief propagation (BP) decoding
algorithm. Using reciprocal channel approximation and Gaussian approximation,
we propose averaging techniques for the density evolution of SC-LDPC codes over
BIAWGN channels. We show that the proposed techniques can closely predict the
decoding threshold while offering reduced complexity compared to the existing
multi-edge-type density evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04767</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04767</id><created>2015-10-15</created><updated>2015-10-19</updated><authors><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author><author><keyname>Laender</keyname><forenames>Alberto H. F.</forenames></author><author><keyname>Alves</keyname><forenames>Bruno L.</forenames></author></authors><title>The H-index Paradox: Your Coauthors Have a Higher H-index than You Do</title><categories>cs.SI cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One interesting phenomenon that emerges from the typical structure of social
networks is the friendship paradox. It states that your friends have on average
more friends than you do. Recent efforts have explored variations of it, with
numerous implications for the dynamics of social networks. However, the
friendship paradox and its variations consider only the topological structure
of the networks and neglect many other characteristics that are correlated with
node degree. In this article, we take the case of scientific collaborations to
investigate whether a similar paradox also arises in terms of a researcher's
scientific productivity as measured by her H-index. The H-index is a widely
used metric in academia to capture both the quality and the quantity of a
researcher's scientific output. It is likely that a researcher may use her
coauthors' H-indexes as a way to infer whether her own H-index is adequate in
her research area. Nevertheless, in this article, we show that the average
H-index of a researcher's coauthors is usually higher than her own H-index. We
present empirical evidence of this paradox and discuss some of its potential
consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04771</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04771</id><created>2015-10-15</created><updated>2015-11-07</updated><authors><author><keyname>Lowery</keyname><forenames>Arthur James</forenames></author></authors><title>Enhanced asymmetrically-clipped optical OFDM</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 Figures (Addition of Author's Note referring to recent
  publications on Layered ACO-OFDM and SEE-OFDM Arxiv version of 28th Oct 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymmetrically clipped optical orthogonal frequency-division multiplexing
(ACO-OFDM) is a technique that sacrifices spectral efficiency in order to
transmit an orthogonally frequency-division multiplexed signal over a unipolar
channel, such as a directly modulated direct-detection fiber or free-space
channel. Several methods have been proposed to regain this spectral efficiency,
including: asymmetrically clipped DC-biased optical OFDM (ADO-OFDM), enhanced
U-OFDM (EU-OFDM), and spectral and energy efficient OFDM (SEE-OFDM). This paper
presents a new method that offers the highest receiver sensitivity for a given
optical power at spectral efficiencies above 3 bit/s/Hz, having a 7-dB
sensitivity advantage over DC-biased OFDM for 1024-QAM at 87.5% of its spectral
efficiency, at the same bit rate and optical power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04772</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04772</id><created>2015-10-15</created><authors><author><keyname>Pradhan</keyname><forenames>Chandan</forenames></author><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Analysis of Path Loss mitigation through Dynamic Spectrum Access:
  Software Defined Radio</title><categories>cs.NI</categories><comments>Accepted in ICMOCE-2015 (IIT Bhubaneswar, India)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an analysis is carried out for a method to mitigate the path
loss through the dynamic spectrum access (DSA) method. The path loss is a major
component which determines the QoS of a wireless link. Its effect is
complemented by the presence of obstruction between the transmitter and
receiver. The future cellular network (5G) focuses on operating with the
millimeter-wave (mmW). In higher frequency, path loss can play a significant
role in degrading the link quality due to higher attenuation. In a scenario,
where the operating environment is changing dynamically, sudden degradation of
operating conditions or arrival of obstruction between transmitter and receiver
may result in link failure. The method analyzed here includes dynamically
allocating spectrum at a lower frequency band for a link suffering from high
path loss. For the analysis, a wireless link was set up using Universal
Software Radio Peripherals (USRPs). The received power is observed to increase
by dynamically changing the operating frequency from 1.9 GHz to 830 MHz.
Finally the utility of software defined radio (SDR) in the RF front end, to
combat the path loss in the future cellular networks, is studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04780</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04780</id><created>2015-10-16</created><authors><author><keyname>Zhu</keyname><forenames>Chenhao</forenames></author><author><keyname>Ren</keyname><forenames>Kan</forenames></author><author><keyname>Liu</keyname><forenames>Xuan</forenames></author><author><keyname>Wang</keyname><forenames>Haofen</forenames></author><author><keyname>Tian</keyname><forenames>Yiding</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author></authors><title>A Graph Traversal Based Approach to Answer Non-Aggregation Questions
  Over DBpedia</title><categories>cs.CL cs.IR</categories><comments>In the proceedings of the 5th Joint International Semantic Technology
  (JIST2015)</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We present a question answering system over DBpedia, filling the gap between
user information needs expressed in natural language and a structured query
interface expressed in SPARQL over the underlying knowledge base (KB). Given
the KB, our goal is to comprehend a natural language query and provide
corresponding accurate answers. Focusing on solving the non-aggregation
questions, in this paper, we construct a subgraph of the knowledge base from
the detected entities and propose a graph traversal method to solve both the
semantic item mapping problem and the disambiguation problem in a joint way.
Compared with existing work, we simplify the process of query intention
understanding and pay more attention to the answer path ranking. We evaluate
our method on a non-aggregation question dataset and further on a complete
dataset. Experimental results show that our method achieves best performance
compared with several state-of-the-art systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04781</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04781</id><created>2015-10-16</created><updated>2015-11-06</updated><authors><author><keyname>Wang</keyname><forenames>Haohan</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>A Survey: Time Travel in Deep Learning Space: An Introduction to Deep
  Learning Models and How Deep Learning Models Evolved from the Initial Ideas</title><categories>cs.LG cs.NE</categories><comments>43 pages, 31 figures. Fix typos in abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report will show the history of deep learning evolves. It will trace
back as far as the initial belief of connectionism modelling of brain, and come
back to look at its early stage realization: neural networks. With the
background of neural network, we will gradually introduce how convolutional
neural network, as a representative of deep discriminative models, is developed
from neural networks, together with many practical techniques that can help in
optimization of neural networks. On the other hand, we will also trace back to
see the evolution history of deep generative models, to see how researchers
balance the representation power and computation complexity to reach Restricted
Boltzmann Machine and eventually reach Deep Belief Nets. Further, we will also
look into the development history of modelling time series data with neural
networks. We start with Time Delay Neural Networks and move further to
currently famous model named Recurrent Neural Network and its extension Long
Short Term Memory. We will also briefly look into how to construct deep
recurrent neural networks. Finally, we will conclude this report with some
interesting open-ended questions of deep neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04796</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04796</id><created>2015-10-16</created><authors><author><keyname>Mishra</keyname><forenames>Sumit</forenames></author><author><keyname>Mondal</keyname><forenames>Samrat</forenames></author><author><keyname>Saha</keyname><forenames>Sriparna</forenames></author></authors><title>Improved Solution to the Non-Domination Level Update Problem</title><categories>cs.DS</categories><comments>18 pages and 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-domination level update problem is to sort the non-dominated fronts after
insertion or deletion of a solution. Generally the solution to this problem
requires to perform the complete non-dominated sorting which is too expensive
in terms of number of comparisons. Recently an Efficient Non-domination Level
Update (ENLU) approach is proposed which does not perform the complete sorting.
For this purpose, in this paper a space efficient version of ENLU approach is
proposed without compromising the number of comparisons. However this approach
does not work satisfactorily in all the cases. So we have also proposed another
tree based approach for solving this non-domination level update problem. In
case of insertion, the tree based approach always checks for same number of
fronts unlike linear approach in which the number of fronts to be checked
depends on the inserted solution. The result shows that in case where all the
solutions are dominating in nature the maximum number of comparisons using tree
based approach is $\mathcal{O}(\log N)$ as opposed to $\mathcal{O}(N)$ in ENLU
approach. When all the solutions are equally divided into $K$ fronts such that
each solution in a front is dominated by all the solutions in the previous
front then the maximum number of comparisons to find a deleted solution in case
of tree based approach is $K{-}\log K$ less than that of ENLU approach. Using
these approaches an on-line sorting algorithm is also proposed and the
competitive analysis of this algorithm is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04802</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04802</id><created>2015-10-16</created><authors><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author><author><keyname>Achananuparp</keyname><forenames>Palakorn</forenames></author></authors><title>Insights from Machine-Learned Diet Success Prediction</title><categories>cs.HC cs.CY</categories><comments>Preprint of an article appearing at the Pacific Symposium on
  Biocomputing (PSB) 2016 in the Social Media Mining for Public Health
  Monitoring and Surveillance track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support people trying to lose weight and stay healthy, more and more
fitness apps have sprung up including the ability to track both calories intake
and expenditure. Users of such apps are part of a wider ``quantified self''
movement and many opt-in to publicly share their logged data. In this paper, we
use public food diaries of more than 4,000 long-term active MyFitnessPal users
to study the characteristics of a (un-)successful diet. Concretely, we train a
machine learning model to predict repeatedly being over or under self-set daily
calories goals and then look at which features contribute to the model's
prediction. Our findings include both expected results, such as the token
``mcdonalds'' or the category ``dessert'' being indicative for being over the
calories goal, but also less obvious ones such as the difference between pork
and poultry concerning dieting success, or the use of the ``quick added
calories'' functionality being indicative of over-shooting calorie-wise. This
study also hints at the feasibility of using such data for more in-depth data
mining, e.g., looking at the interaction between consumed foods such as mixing
protein- and carbohydrate-rich foods. To the best of our knowledge, this is the
first systematic study of public food diaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04811</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04811</id><created>2015-10-16</created><updated>2015-11-28</updated><authors><author><keyname>Beijbom</keyname><forenames>Oscar</forenames></author><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Yao</keyname><forenames>Evan</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Rodriguez-Ramirez</keyname><forenames>Alberto</forenames></author><author><keyname>Gonzalez-Rivero</keyname><forenames>Manuel</forenames></author><author><keyname>Guldberg</keyname><forenames>Ove Hoegh -</forenames></author></authors><title>Quantification in-the-wild: data-sets and baselines</title><categories>cs.LG</categories><comments>This report was prsented at the NIPS 2015 workshop on Transfer and
  Multi-Task Learning: Trends and New Perspectives. It is 4 pages + 1 page of
  references followed by a 6 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantification is the task of estimating the class-distribution of a
data-set. While typically considered as a parameter estimation problem with
strict assumptions on the data-set shift, we consider quantification
in-the-wild, on two large scale data-sets from marine ecology: a survey of
Caribbean coral reefs, and a plankton time series from Martha's Vineyard
Coastal Observatory. We investigate several quantification methods from the
literature and indicate opportunities for future work. In particular, we show
that a deep neural network can be fine-tuned on a very limited amount of data
(25 - 100 samples) to outperform alternative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04815</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04815</id><created>2015-10-16</created><updated>2015-10-21</updated><authors><author><keyname>Li</keyname><forenames>Wenzhe</forenames></author><author><keyname>Ahn</keyname><forenames>Sungjin</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Scalable MCMC for Mixed Membership Stochastic Blockmodels</title><categories>cs.LG stat.ML</categories><comments>9 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stochastic gradient Markov chain Monte Carlo (SG-MCMC) algorithm
for scalable inference in mixed-membership stochastic blockmodels (MMSB). Our
algorithm is based on the stochastic gradient Riemannian Langevin sampler and
achieves both faster speed and higher accuracy at every iteration than the
current state-of-the-art algorithm based on stochastic variational inference.
In addition we develop an approximation that can handle models that entertain a
very large number of communities. The experimental results show that SG-MCMC
strictly dominates competing algorithms in all cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04817</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04817</id><created>2015-10-16</created><authors><author><keyname>&#xc1;lvez</keyname><forenames>Javier</forenames></author><author><keyname>Lucio</keyname><forenames>Paqui</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Improving the Competency of First-Order Ontologies</title><categories>cs.AI cs.LO</categories><comments>8 pages, 2 tables</comments><acm-class>I.2.4</acm-class><journal-ref>Proceedings of the 8th International Conference on Knowledge
  Capture (K-CAP 2015). Palisades, NY. 2015</journal-ref><doi>10.1145/2815833.2815841</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework to evaluate and improve first-order (FO)
ontologies using automated theorem provers (ATPs) on the basis of competency
questions (CQs). Our framework includes both the adaptation of a methodology
for evaluating ontologies to the framework of first-order logic and a new set
of non-trivial CQs designed to evaluate FO versions of SUMO, which
significantly extends the very small set of CQs proposed in the literature.
Most of these new CQs have been automatically generated from a small set of
patterns and the mapping of WordNet to SUMO. Applying our framework, we
demonstrate that Adimen-SUMO v2.2 outperforms TPTP-SUMO. In addition, using the
feedback provided by ATPs we have set an improved version of Adimen-SUMO
(v2.4). This new version outperforms the previous ones in terms of competency.
For instance, &quot;Humans can reason&quot; is automatically inferred from Adimen-SUMO
v2.4, while it is neither deducible from TPTP-SUMO nor Adimen-SUMO v2.2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04820</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04820</id><created>2015-10-16</created><updated>2015-11-07</updated><authors><author><keyname>Gupta</keyname><forenames>Anindya</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Error-Correcting Functional Index Codes, Generalized Exclusive Laws and
  Graph Coloring</title><categories>cs.IT math.IT</categories><comments>Six tables and three figures. Bounds on the size of optimal
  functional index codes have been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the \emph{functional index coding problem} over an error-free
broadcast network in which a source generates a set of messages and there are
multiple receivers, each holding a set of functions of source messages in its
cache, called the \emph{Has-set}, and demands to know another set of functions
of messages, called the \emph{Want-set}. Cognizant of the receivers'
\emph{Has-sets}, the source aims to satisfy the demands of each receiver by
making coded transmissions, called a \emph{functional index code}. The
objective is to minimize the number of such transmissions required. The
restriction a receiver's demands pose on the code is represented via a
constraint called the \emph{generalized exclusive law} and obtain a code using
the \emph{confusion graph} constructed using these constraints. Bounds on the
size of an optimal code based on the parameters of the confusion graph are
presented. Next, we consider the case of erroneous transmissions and provide a
necessary and sufficient condition that an FIC must satisfy for correct
decoding of desired functions at each receiver and obtain a lower bound on the
length of an error-correcting FIC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04821</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04821</id><created>2015-10-16</created><updated>2015-12-05</updated><authors><author><keyname>Kotelnikov</keyname><forenames>Evgenii</forenames></author><author><keyname>Kov&#xe1;cs</keyname><forenames>Laura</forenames></author><author><keyname>Reger</keyname><forenames>Giles</forenames></author><author><keyname>Voronkov</keyname><forenames>Andrei</forenames></author></authors><title>The Vampire and the FOOL</title><categories>cs.LO</categories><acm-class>D.2.4; F.3.1; F.4.1; I.2.3; I.2.5</acm-class><doi>10.1145/2854065.2854071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents new features recently implemented in the theorem prover
Vampire, namely support for first-order logic with a first class boolean sort
(FOOL) and polymorphic arrays. In addition to having a first class boolean
sort, FOOL also contains if-then-else and let-in expressions. We argue that
presented extensions facilitate reasoning-based program analysis, both by
increasing the expressivity of first-order reasoners and by gains in
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04822</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04822</id><created>2015-10-16</created><updated>2015-10-19</updated><authors><author><keyname>Achab</keyname><forenames>Massil</forenames><affiliation>CMAP</affiliation></author><author><keyname>Guilloux</keyname><forenames>Agathe</forenames><affiliation>LSTA</affiliation></author><author><keyname>Ga&#xef;ffas</keyname><forenames>St&#xe9;phane</forenames><affiliation>CMAP</affiliation></author><author><keyname>Bacry</keyname><forenames>Emmanuel</forenames><affiliation>CMAP</affiliation></author></authors><title>SGD with Variance Reduction beyond Empirical Risk Minimization</title><categories>stat.ML cs.LG</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a doubly stochastic proximal gradient algorithm for optimizing a
finite average of smooth convex functions, whose gradients depend on
numerically expensive expectations. Our main motivation is the acceleration of
the optimization of the regularized Cox partial-likelihood (the core model used
in survival analysis), but our algorithm can be used in different settings as
well. The proposed algorithm is doubly stochastic in the sense that gradient
steps are done using stochastic gradient descent (SGD) with variance reduction,
where the inner expectations are approximated by a Monte-Carlo Markov-Chain
(MCMC) algorithm. We derive conditions on the MCMC number of iterations
guaranteeing convergence, and obtain a linear rate of convergence under strong
convexity and a sublinear rate without this assumption. We illustrate the fact
that our algorithm improves the state-of-the-art solver for regularized Cox
partial-likelihood on several datasets from survival analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04825</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04825</id><created>2015-10-16</created><authors><author><keyname>Sarkis</keyname><forenames>Mira</forenames><affiliation>LTCI</affiliation></author><author><keyname>Concolato</keyname><forenames>Cyril</forenames><affiliation>LTCI</affiliation></author><author><keyname>Dufourd</keyname><forenames>Jean-Claude</forenames><affiliation>LTCI</affiliation></author></authors><title>MSoS: A Multi-Screen-Oriented Web Page Segmentation Approach</title><categories>cs.HC cs.MM</categories><comments>DocEng'14: ACM Symposium on Document Engineering, ACM, 2015</comments><proxy>ccsd</proxy><doi>10.1145/2682571.2797090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a multiscreen-oriented approach for segmenting web
pages. The segmentation is an automatic and hybrid visual and structural
method. It aims at creating coherent blocks which have different functions
determined by the multiscreen environment. It is also characterized by a
dynamic adaptation to the page content. Experiments are conducted on a set of
existing applications that contain multimedia elements, in particular YouTube
and video player pages. Results are compared with one seg-mentation method from
the literature and with a ground truth manually created. With a 75% precision,
the MSoS is a promising method that is capable of producing good segmentation
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04826</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04826</id><created>2015-10-16</created><authors><author><keyname>&#xc1;lvez</keyname><forenames>Javier</forenames></author><author><keyname>Lucio</keyname><forenames>Paqui</forenames></author><author><keyname>Rigau</keyname><forenames>German</forenames></author></authors><title>Evaluating the Competency of a First-Order Ontology</title><categories>cs.AI cs.LO</categories><comments>4 pages, 4 figures</comments><acm-class>I.2.4</acm-class><journal-ref>Proceedings of the 8th International Conference on Knowledge
  Capture (K-CAP 2015). Palisades, NY. 2015</journal-ref><doi>10.1145/2815833.2816946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on the results of evaluating the competency of a first-order
ontology for its use with automated theorem provers (ATPs). The evaluation
follows the adaptation of the methodology based on competency questions (CQs)
[Gr\&quot;uninger&amp;Fox,1995] to the framework of first-order logic, which is
presented in [\'Alvez&amp;Lucio&amp;Rigau,2015], and is applied to Adimen-SUMO
[\'Alvez&amp;Lucio&amp;Rigau,2015]. The set of CQs used for this evaluation has been
automatically generated from a small set of semantic patterns and the mapping
of WordNet to SUMO. Analysing the results, we can conclude that it is feasible
to use ATPs for working with Adimen-SUMO v2.4, enabling the resolution of goals
by means of performing non-trivial inferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04828</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04828</id><created>2015-10-16</created><authors><author><keyname>Chang</keyname><forenames>Cheng-Shang</forenames></author><author><keyname>Lee</keyname><forenames>Duan-Shin</forenames></author><author><keyname>Liou</keyname><forenames>Li-Heng</forenames></author><author><keyname>Lu</keyname><forenames>Sheng-Min</forenames></author><author><keyname>Wu</keyname><forenames>Mu-Huan</forenames></author></authors><title>A Probabilistic Framework for Structural Analysis in Directed Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent works, we developed a probabilistic framework for structural
analysis in undirected networks. The key idea of that framework is to sample a
network by a symmetric bivariate distribution and then use that bivariate
distribution to formerly define various notions, including centrality, relative
centrality, community, and modularity. The main objective of this paper is to
extend the probabilistic framework to directed networks, where the sampling
bivariate distributions could be asymmetric. Our main finding is that we can
relax the assumption from symmetric bivariate distributions to bivariate
distributions that have the same marginal distributions. By using such a weaker
assumption, we show that various notions for structural analysis in directed
networks can also be defined in the same manner as before. However, since the
bivariate distribution could be asymmetric, the community detection algorithms
proposed in our previous work cannot be directly applied. For this, we show
that one can construct another sampled graph with a symmetric bivariate
distribution so that for any partition of the network, the modularity index
remains the same as that of the original sampled graph. Based on this, we
propose a hierarchical agglomerative algorithm that returns a partition of
communities when the algorithm converges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04833</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04833</id><created>2015-10-16</created><authors><author><keyname>Prot</keyname><forenames>Damien</forenames></author><author><keyname>Bellenguez-Morineau</keyname><forenames>Odile</forenames></author></authors><title>Impact of precedence constraints on complexity of scheduling problems: a
  survey</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey aims at proving that the structure of precedence constraints
plays a tremendous role on the complexity of scheduling problems. Indeed many
problems can be NP-hard when considering general precedence constraints, while
they become polynomially solvable for particular precedence constraints. We
also show that there still are many very exciting challenges in this research
area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04839</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04839</id><created>2015-10-16</created><authors><author><keyname>Wang</keyname><forenames>Jian-Bo</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>Identifying spatial invasion of pandemics on metapopulation networks via
  anatomizing arrival history</title><categories>cs.SI q-bio.PE</categories><comments>14pages, 8 figures; Accepted by IEEE Transactions on Cybernetics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial spread of infectious diseases among populations via the mobility of
humans is highly stochastic and heterogeneous. Accurate forecast/mining of the
spread process is often hard to be achieved by using statistical or mechanical
models. Here we propose a new reverse problem, which aims to identify the
stochastically spatial spread process itself from observable information
regarding the arrival history of infectious cases in each subpopulation. We
solved the problem by developing an efficient optimization algorithm based on
dynamical programming, which comprises three procedures: i, anatomizing the
whole spread process among all subpopulations into disjoint componential
patches; ii, inferring the most probable invasion pathways underlying each
patch via maximum likelihood estimation; iii, recovering the whole process by
assembling the invasion pathways in each patch iteratively, without burdens in
parameter calibrations and computer simulations. Based on the entropy theory,
we introduced an identifiability measure to assess the difficulty level that an
invasion pathway can be identified. Results on both artificial and empirical
metapopulation networks show the robust performance in identifying actual
invasion pathways driving pandemic spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04842</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04842</id><created>2015-10-16</created><authors><author><keyname>Varas</keyname><forenames>David</forenames></author><author><keyname>Alfaro</keyname><forenames>M&#xf3;nica</forenames></author><author><keyname>Marques</keyname><forenames>Ferran</forenames></author></authors><title>Multiresolution hierarchy co-clustering for semantic segmentation in
  sequences with small variations</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a co-clustering technique that, given a collection of
images and their hierarchies, clusters nodes from these hierarchies to obtain a
coherent multiresolution representation of the image collection. We formalize
the co-clustering as a Quadratic Semi-Assignment Problem and solve it with a
linear programming relaxation approach that makes effective use of information
from hierarchies. Initially, we address the problem of generating an optimal,
coherent partition per image and, afterwards, we extend this method to a
multiresolution framework. Finally, we particularize this framework to an
iterative multiresolution video segmentation algorithm in sequences with small
variations. We evaluate the algorithm on the Video Occlusion/Object Boundary
Detection Dataset, showing that it produces state-of-the-art results in these
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04849</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04849</id><created>2015-10-16</created><authors><author><keyname>Czarnetzki</keyname><forenames>Silke</forenames></author><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author></authors><title>Using Duality in Circuit Complexity</title><categories>cs.CC</categories><acm-class>F.1.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate in a method for proving separation results for abstract
classes of languages. A well established method to characterize varieties of
regular languages are identities. We use a recently established generalization
of these identities to non-regular languages by Gehrke, Grigorieff, and Pin: so
called equations, which are capable of describing arbitrary Boolean algebras of
languages.
  While the main concern of their result is the existence of these equations,
we investigate in a general method that could allow to find equations for
language classes in an inductive manner.
  Thereto we extend an important tool -- the block product or substitution
principle -- known from logic and algebra, to non-regular language classes.
Furthermore, we abstract this concept by defining it directly as an operation
on (non-regular) language classes. We show that this principle can be used to
obtain equations for certain circuit classes, given equations for the gate
types.
  Concretely, we demonstrate the applicability of this method by obtaining a
description via equations for all languages recognized by circuit families that
contain a constant number of (inner) gates, given a description of the gate
types via equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04853</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04853</id><created>2015-10-16</created><authors><author><keyname>Dehghani-Madiseh</keyname><forenames>Marzieh</forenames></author><author><keyname>Hlad&#xed;k</keyname><forenames>Milan</forenames></author></authors><title>Efficient Approaches for Enclosing the United Solution Set of the
  Interval Generalized Sylvester Matrix Equation</title><categories>cs.NA</categories><msc-class>65G30, 15A24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the interval generalized Sylvester matrix
equation ${\bf{A}}X{\bf{B}}+{\bf{C}}X{\bf{D}}={\bf{F}}$ and develop some
techniques for obtaining outer estimations for the so-called united solution
set of this interval system. First, we propose a modified variant of the
Krawczyk operator which causes reducing computational complexity to cubic,
compared to Kronecker product form. We then propose an iterative technique for
enclosing the solution set. These approaches are based on spectral
decompositions of the midpoints of ${\bf{A}}$, ${\bf{B}}$, ${\bf{C}}$ and
${\bf{D}}$ and in both of them we suppose that the midpoints of ${\bf{A}}$ and
${\bf{C}}$ are simultaneously diagonalizable as well as for the midpoints of
the matrices ${\bf{B}}$ and ${\bf{D}}$. Some numerical experiments are given to
illustrate the performance of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04854</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04854</id><created>2015-10-16</created><updated>2016-02-23</updated><authors><author><keyname>Castiglioni</keyname><forenames>Valentina</forenames></author><author><keyname>Lanotte</keyname><forenames>Ruggero</forenames></author><author><keyname>Merro</keyname><forenames>Massimo</forenames></author></authors><title>A Semantic Theory of the Internet of Things</title><categories>cs.LO</categories><comments>The contribution of Valentina Castiglioni is limited to an early
  writing of some of the proofs in the Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a process calculus for modelling systems in the Internet of Things
paradigm. Our systems interact both with the physical environment, via sensors
and actuators, and with smart devices, via short-range and Internet channels.
The calculus is equipped with a standard notion of bisimilarity which is a
fully abstract characterisation of a well-known contextual equivalence. We use
our semantic proof-methods to prove run-time properties as well as system
equalities of non-trivial IoT systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04860</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04860</id><created>2015-10-16</created><authors><author><keyname>Kova&#x10d;i&#x107;</keyname><forenames>Kristian</forenames></author><author><keyname>Ivanjko</keyname><forenames>Edouard</forenames></author><author><keyname>Jelu&#x161;i&#x107;</keyname><forenames>Niko</forenames></author></authors><title>Measurement of Road Traffic Parameters Based on Multi-Vehicle Tracking</title><categories>cs.CV</categories><comments>Part of the Proceedings of the Croatian Computer Vision Workshop,
  CCVW 2015, Year 3</comments><report-no>UniZg-CRV-CCVW/2015/0010</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Development of computing power and cheap video cameras enabled today's
traffic management systems to include more cameras and computer vision
applications for transportation system monitoring and control. Combined with
image processing algorithms cameras are used as sensors to measure road traffic
parameters like flow volume, origin-destination matrices, classify vehicles,
etc. In this paper we propose a system for measurement of road traffic
parameters (basic motion model parameters and macro-scopic traffic parameters).
The system is based on Local Binary Pattern (LBP) image features classification
with a cascade of Gentle Adaboost (GAB) classifiers to determine vehicle
existence and its location in an image. Additionally, vehicle tracking and
counting in a road traffic video is performed by using Extended Kalman Filter
(EKF) and virtual markers. The newly proposed system is compared with a system
based on background subtraction. Comparison is performed by the means of
execution time and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04861</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04861</id><created>2015-10-16</created><authors><author><keyname>Bla&#x17e;evi&#x107;</keyname><forenames>Martin</forenames></author><author><keyname>Brki&#x107;</keyname><forenames>Karla</forenames></author><author><keyname>Hrka&#x107;</keyname><forenames>Tomislav</forenames></author></authors><title>Towards Reversible De-Identification in Video Sequences Using 3D Avatars
  and Steganography</title><categories>cs.CV cs.MM</categories><comments>Part of the Proceedings of the Croatian Computer Vision Workshop,
  CCVW 2015, Year 3</comments><report-no>UniZg-CRV-CCVW/2015/0011</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose a de-identification pipeline that protects the privacy of humans
in video sequences by replacing them with rendered 3D human models, hence
concealing their identity while retaining the naturalness of the scene. The
original images of humans are steganographically encoded in the carrier image,
i.e. the image containing the original scene and the rendered 3D human models.
We qualitatively explore the feasibility of our approach, utilizing the Kinect
sensor and its libraries to detect and localize human joints. A 3D avatar is
rendered into the scene using the obtained joint positions, and the original
human image is steganographically encoded in the new scene. Our qualitative
evaluation shows reasonably good results that merit further exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04862</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04862</id><created>2015-10-16</created><authors><author><keyname>Damen</keyname><forenames>Dima</forenames></author><author><keyname>Leelasawassuk</keyname><forenames>Teesid</forenames></author><author><keyname>Mayol-Cuevas</keyname><forenames>Walterio</forenames></author></authors><title>You-Do, I-Learn: Unsupervised Multi-User egocentric Approach Towards
  Video-Based Guidance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an unsupervised approach towards automatically extracting
video-based guidance on object usage, from egocentric video and wearable gaze
tracking, collected from multiple users while performing tasks. The approach i)
discovers task relevant objects, ii) builds a model for each, iii)
distinguishes different ways in which each discovered object has been used and
vi) discovers the dependencies between object interactions. The work
investigates using appearance, position, motion and attention, and presents
results using each and a combination of relevant features. Moreover, an online
scalable approach is presented and is compared to offline results. In the
assistive mode, the paper proposes a method for selecting a suitable video
guide to be displayed to a novice user indicating how to use an object, purely
triggered by the user's gaze. The assistive mode also recommends an object to
be used next based on the learnt sequence of object interactions. The approach
was tested on a variety of daily tasks such as initialising a printer,
preparing a coffee and setting up a gym machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04863</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04863</id><created>2015-10-16</created><authors><author><keyname>Petkovi&#x107;</keyname><forenames>Tomislav</forenames></author><author><keyname>Lon&#x10d;ari&#x107;</keyname><forenames>Sven</forenames></author></authors><title>An Extension to Hough Transform Based on Gradient Orientation</title><categories>cs.CV</categories><comments>Part of the Proceedings of the Croatian Computer Vision Workshop,
  CCVW 2015, Year 3</comments><report-no>UniZg-CRV-CCVW/2015/0012</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Hough transform is one of the most common methods for line detection. In
this paper we propose a novel extension of the regular Hough transform. The
proposed extension combines the extension of the accumulator space and the
local gradient orientation resulting in clutter reduction and yielding more
prominent peaks, thus enabling better line identification. We demonstrate
benefits in applications such as visual quality inspection and rectangle
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04868</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04868</id><created>2015-10-16</created><authors><author><keyname>Thomasian</keyname><forenames>Alexander</forenames></author><author><keyname>Xu</keyname><forenames>Jun</forenames></author></authors><title>Data Allocation in a Heterogeneous Disk Array - HDA with Multiple RAID
  Levels for Database Applications</title><categories>cs.DC</categories><comments>IEEE 2-column format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the allocation of Virtual Arrays (VAs) in a Heterogeneous Disk
Array (HDA). Each VA holds groups of related objects and datasets such as
files, relational tables, which has similar performance and availability
characteristics. We evaluate single-pass data allocation methods for HDA using
a synthetic stream of allocation requests, where each VA is characterized by
its RAID level, disk loads and space requirements. The goal is to maximize the
number of allocated VAs and maintain high disk bandwidth and capacity
utilization, while balancing disk loads. Although only RAID1 (basic mirroring)
and RAID5 (rotated parity arrays) are considered in the experimental study, we
develop the analysis required to estimate disk loads for other RAID levels.
Since VA loads vary significantly over time, the VA allocation is carried out
at the peak load period, while ensuring that disk bandwidth is not exceeded at
other high load periods. Experimental results with a synthetic stream of
allocation requests show that allocation methods minimizing the maximum disk
bandwidth and capacity utilization or their variance across all disks yield the
maximum number of allocated VAs. HDA saves disk bandwidth, since a single RAID
level accommodating the most stringent availability requirements for a small
subset of objects would incur an unnecessarily high overhead for updating check
blocks or data replicas for all objects. The number of allocated VAs can be
increased by adopting the clustered RAID5 paradigm, which exploits the tradeoff
between redundancy and bandwidth utilization. Since rebuild can be carried out
at the level of individual VAs, prioritizing rebuild of VAs with higher access
rates can improve overall performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04873</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04873</id><created>2015-10-16</created><updated>2015-10-20</updated><authors><author><keyname>Dragomir</keyname><forenames>Iulia</forenames></author><author><keyname>Preoteasa</keyname><forenames>Viorel</forenames></author><author><keyname>Tripakis</keyname><forenames>Stavros</forenames></author></authors><title>Translating Hierarchical Block Diagrams into Composite Predicate
  Transformers</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulink is the de facto industrial standard for designing embedded control
systems. When dealing with the formal verification of Simulink models, we face
the problem of translating the graphical language of Simulink, namely,
hierarchical block diagrams (HBDs), into a formalism suitable for verification.
In this paper, we study the translation of HBDs into the compositional
refinement calculus framework for reactive systems. Specifically, we consider
as target language an algebra of atomic predicate transformers to capture basic
Simulink blocks (both stateless and stateful), composed in series, in parallel,
and in feedback. For a given HBD, there are many possible ways to translate it
into a term in this algebra, with different tradeoffs. We explore these
tradeoffs, and present three translation algorithms. We report on a prototype
implementation of these algorithms in a tool that translates Simulink models
into algebra terms implemented in the Isabelle theorem prover. We test our tool
on several case studies including a benchmark Simulink model by Toyota. We
compare the three translation algorithms, with respect to size and readability
of generated terms, simplifiability of the corresponding formulas, and other
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04880</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04880</id><created>2015-10-15</created><authors><author><keyname>Patranabis</keyname><forenames>Anirban</forenames></author><author><keyname>Banerjee</keyname><forenames>Kaushik</forenames></author><author><keyname>Midya</keyname><forenames>Vishal</forenames></author><author><keyname>Chakraborty</keyname><forenames>Sneha</forenames></author><author><keyname>Sanyal</keyname><forenames>Shankha</forenames></author><author><keyname>Banerjee</keyname><forenames>Archi</forenames></author><author><keyname>Sengupta</keyname><forenames>Ranjan</forenames></author><author><keyname>Ghosh</keyname><forenames>Dipak</forenames></author></authors><title>Harmonic and Timbre Analysis of Tabla Strokes</title><categories>cs.SD physics.class-ph</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indian twin drums mainly bayan and dayan (tabla) are the most important
percussion instruments in India popularly used for keeping rhythm. It is a twin
percussion/drum instrument of which the right hand drum is called dayan and the
left hand drum is called bayan. Tabla strokes are commonly called as `bol',
constitutes a series of syllables. In this study we have studied the timbre
characteristics of nine strokes from each of five different tablas. Timbre
parameters were calculated from LTAS of each stroke signals. Study of timbre
characteristics is one of the most important deterministic approach for
analyzing tabla and its stroke characteristics. Statistical correlations among
timbre parameters were measured and also through factor analysis we get to know
about the parameters of timbre analysis which are closely related. Tabla
strokes have unique harmonic and timbral characteristics at mid frequency range
and have no uniqueness at low frequency ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04881</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04881</id><created>2015-10-16</created><updated>2015-10-19</updated><authors><author><keyname>Osterholzer</keyname><forenames>Johannes</forenames></author><author><keyname>Dietze</keyname><forenames>Toni</forenames></author><author><keyname>Herrmann</keyname><forenames>Luisa</forenames></author></authors><title>Linear Context-Free Tree Languages and Inverse Homomorphisms</title><categories>cs.FL</categories><comments>30 pages, 1 figure; fixed a variable collision (t') in Section 7</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the class of linear context-free tree languages is not closed
under inverse linear tree homomorphisms. The proof is by contradiction: we
encode Dyck words into a context-free tree language and prove that its preimage
under a certain linear tree homomorphism cannot be generated by any
context-free tree grammar. A positive result can still be obtained: the linear
monadic context-free tree languages are closed under inverse linear tree
homomorphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04895</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04895</id><created>2015-10-16</created><authors><author><keyname>Pieper</keyname><forenames>Andreas</forenames></author><author><keyname>Kreutzer</keyname><forenames>Moritz</forenames></author><author><keyname>Galgon</keyname><forenames>Martin</forenames></author><author><keyname>Alvermann</keyname><forenames>Andreas</forenames></author><author><keyname>Fehske</keyname><forenames>Holger</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Lang</keyname><forenames>Bruno</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>High-performance implementation of Chebyshev filter diagonalization for
  interior eigenvalue computations</title><categories>math.NA cond-mat.mes-hall cs.NA</categories><comments>26 pages, 11 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Chebyshev filter diagonalization as a tool for the computation of
many interior eigenvalues of very large sparse symmetric matrices. In this
technique the subspace projection onto the target space of wanted eigenvectors
is approximated with high-order filter polynomials obtained from a regularized
Chebyshev expansion of a window function. After a short discussion of the
conceptual foundations of Chebyshev filter diagonalization we analyze the
impact of the choice of the damping kernel, search space size, and filter
polynomial degree on the computational accuracy and effort, before we describe
the necessary steps towards a parallel high-performance implementation. Because
Chebyshev filter diagonalization avoids the need for matrix inversion it can
deal with matrices and problem sizes that are presently not accessible with
rational function methods based on direct or iterative linear solvers. To
demonstrate the potential of Chebyshev filter diagonalization for large scale
problems of this kind we include as an example the computation of the $10^2$
innermost eigenpairs of a topological insulator matrix with dimension $10^9$
derived from quantum physics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04905</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04905</id><created>2015-10-16</created><authors><author><keyname>Becker</keyname><forenames>Stephen</forenames></author><author><keyname>Kawas</keyname><forenames>Ban</forenames></author><author><keyname>Petrik</keyname><forenames>Marek</forenames></author><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan N.</forenames></author></authors><title>Robust Partially-Compressed Least-Squares</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomized matrix compression techniques, such as the Johnson-Lindenstrauss
transform, have emerged as an effective and practical way for solving
large-scale problems efficiently. With a focus on computational efficiency,
however, forsaking solutions quality and accuracy becomes the trade-off. In
this paper, we investigate compressed least-squares problems and propose new
models and algorithms that address the issue of error and noise introduced by
compression. While maintaining computational efficiency, our models provide
robust solutions that are more accurate--relative to solutions of uncompressed
least-squares--than those of classical compressed variants. We introduce tools
from robust optimization together with a form of partial compression to improve
the error-time trade-offs of compressed least-squares solvers. We develop an
efficient solution algorithm for our Robust Partially-Compressed (RPC) model
based on a reduction to a one-dimensional search. We also derive the first
approximation error bounds for Partially-Compressed least-squares solutions.
Empirical results comparing numerous alternatives suggest that robust and
partially compressed solutions are effectively insulated against aggressive
randomized transforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04908</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04908</id><created>2015-10-16</created><authors><author><keyname>Mettes</keyname><forenames>Pascal</forenames></author><author><keyname>van Gemert</keyname><forenames>Jan C.</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author></authors><title>No Spare Parts: Sharing Part Detectors for Image Categorization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims for image categorization using a representation of distinctive
parts. Different from existing part-based work, we argue that parts are
naturally shared between image categories and should be modeled as such. We
motivate our approach with a quantitative and qualitative analysis by
backtracking where selected parts come from. Our analysis shows that in
addition to the category parts defining the class, the parts coming from the
background context and parts from other image categories improve categorization
performance. Part selection should not be done separately for each category,
but instead be shared and optimized over all categories. To incorporate part
sharing between categories, we present an algorithm based on AdaBoost to
jointly optimize part sharing and selection, as well as fusion with the global
image representation. We achieve results competitive to the state-of-the-art on
object, scene, and action categories, further improving over deep convolutional
neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04914</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04914</id><created>2015-10-16</created><authors><author><keyname>Vanaret</keyname><forenames>Charlie</forenames></author><author><keyname>Gotteland</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Durand</keyname><forenames>Nicolas</forenames></author><author><keyname>Alliot</keyname><forenames>Jean-Marc</forenames></author></authors><title>Hybridization of Interval CP and Evolutionary Algorithms for Optimizing
  Difficult Problems</title><categories>cs.AI cs.DC cs.MS math.NA math.OC</categories><comments>21st International Conference on Principles and Practice of
  Constraint Programming (CP 2015), 2015</comments><doi>10.1007/978-3-319-23219-5_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The only rigorous approaches for achieving a numerical proof of optimality in
global optimization are interval-based methods that interleave branching of the
search-space and pruning of the subdomains that cannot contain an optimal
solution. State-of-the-art solvers generally integrate local optimization
algorithms to compute a good upper bound of the global minimum over each
subspace. In this document, we propose a cooperative framework in which
interval methods cooperate with evolutionary algorithms. The latter are
stochastic algorithms in which a population of candidate solutions iteratively
evolves in the search-space to reach satisfactory solutions.
  Within our cooperative solver Charibde, the evolutionary algorithm and the
interval-based algorithm run in parallel and exchange bounds, solutions and
search-space in an advanced manner via message passing. A comparison of
Charibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and
NLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows
that Charibde is highly competitive against non-rigorous solvers and converges
faster than rigorous solvers by an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04921</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04921</id><created>2015-10-16</created><updated>2015-10-28</updated><authors><author><keyname>Ruoti</keyname><forenames>Scott</forenames></author><author><keyname>O'Neil</keyname><forenames>Mark</forenames></author><author><keyname>Zappala</keyname><forenames>Daniel</forenames></author><author><keyname>Seamons</keyname><forenames>Kent</forenames></author></authors><title>At Least Tell Me: User Attitudes Toward the Inspection of Encrypted
  Traffic</title><categories>cs.CR cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports the results of a survey of 1,976 individuals regarding
their opinions on TLS inspection, a controversial technique that can be used
for both benevolent and malicious purposes. Responses indicate that
participants hold nuanced opinions on security and privacy trade-offs, with
most recognizing legitimate uses for the practice, but also concerned about
threats from hackers or government surveillance. There is strong support for
notification and consent when a system is intercepting their encrypted traffic,
although this support varies depending on the situation. A significant concern
about malicious uses of TLS inspection is identity theft, and many would react
negatively and some would change their behavior if they discovered inspection
occurring without their knowledge. We also find that there are a small but
significant number of participants who are jaded by the current state of
affairs and have no expectation of privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04929</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04929</id><created>2015-10-16</created><authors><author><keyname>Kelemen</keyname><forenames>Z&#xe1;dor D&#xe1;niel</forenames></author><author><keyname>T&#xf3;dor</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Hodosi</keyname><forenames>S&#xe1;ndor</forenames></author><author><keyname>Somfai</keyname><forenames>&#xc1;kos</forenames></author></authors><title>Refactoring Technical Support to Reduce Interrupts of Developers</title><categories>cs.SI cs.SE</categories><comments>9 pages. arXiv admin note: substantial text overlap with
  arXiv:1503.05533</comments><journal-ref>EuroAsiaSPI 2015, Ankara, Turkey</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an analysis of a technical support data with the goal of
identifying process improvement actions for reducing interrupts is presented. A
technical support chat is established and used to provide internal developer
support to other development teams which use the software code developed by a
core team. The paper shows how data analysis of a 6 months support time helped
to identify gaps and action items for improving the technical support process
to minimize interrupts from other developer teams. The paper also shows effects
(advantages and drawbacks) of refactor actions taken based on this analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04931</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04931</id><created>2015-10-16</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Bad Universal Priors and Notions of Optimality</title><categories>cs.AI cs.LG</categories><comments>COLT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A big open question of algorithmic information theory is the choice of the
universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff
induction we have invariance theorems: the choice of the UTM changes bounds
only by a constant. For the universally intelligent agent AIXI (Hutter, 2005)
no invariance theorem is known. Our results are entirely negative: we discuss
cases in which unlucky or adversarial choices of the UTM cause AIXI to
misbehave drastically. We show that Legg-Hutter intelligence and thus balanced
Pareto optimality is entirely subjective, and that every policy is Pareto
optimal in the class of all computable environments. This undermines all
existing optimality properties for AIXI. While it may still serve as a gold
standard for AI, our results imply that AIXI is a relative theory, dependent on
the choice of the UTM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04935</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04935</id><created>2015-10-16</created><updated>2015-12-07</updated><authors><author><keyname>Nickel</keyname><forenames>Maximilian</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Holographic Embeddings of Knowledge Graphs</title><categories>cs.AI cs.LG stat.ML</categories><comments>To appear in AAAI-16</comments><acm-class>I.2.6; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning embeddings of entities and relations is an efficient and versatile
method to perform machine learning on relational data such as knowledge graphs.
In this work, we propose holographic embeddings (HolE) to learn compositional
vector space representations of entire knowledge graphs. The proposed method is
related to holographic models of associative memory in that it employs circular
correlation to create compositional representations. By using correlation as
the compositional operator HolE can capture rich interactions but
simultaneously remains efficient to compute, easy to train, and scalable to
very large datasets. In extensive experiments we show that holographic
embeddings are able to outperform state-of-the-art methods for link prediction
in knowledge graphs and relational learning benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04940</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04940</id><created>2015-10-16</created><authors><author><keyname>Si</keyname><forenames>Hongbo</forenames><affiliation>Charlie</affiliation></author><author><keyname>Ng</keyname><forenames>Boon Loong</forenames><affiliation>Charlie</affiliation></author><author><keyname>Rahman</keyname><forenames>Md. Saifur</forenames><affiliation>Charlie</affiliation></author><author><keyname>Jianzhong</keyname><affiliation>Charlie</affiliation></author><author><keyname>Zhang</keyname></author></authors><title>A Novel and Efficient Vector Quantization Based CPRI Compression
  Algorithm</title><categories>cs.IT math.IT</categories><comments>25 pages, 15 figures, 6 tables, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The future wireless network, such as Centralized Radio Access Network
(C-RAN), will need to deliver data rate about 100 to 1000 times the current 4G
technology. For C-RAN based network architecture, there is a pressing need for
tremendous enhancement of the effective data rate of the Common Public Radio
Interface (CPRI). Compression of CPRI data is one of the potential
enhancements. In this paper, we introduce a vector quantization based
compression algorithm for CPRI links, utilizing Lloyd algorithm. Methods to
vectorize the I/Q samples and enhanced initialization of Lloyd algorithm for
codebook training are investigated for improved performance. Multi-stage vector
quantization and unequally protected multi-group quantization are considered to
reduce codebook search complexity and codebook size. Simulation results show
that our solution can achieve compression of 4 times for uplink and 4.5 times
for downlink, within 2% Error Vector Magnitude (EVM) distortion. Remarkably,
vector quantization codebook proves to be quite robust against data modulation
mismatch, fading, signal-to-noise ratio (SNR) and Doppler spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04941</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04941</id><created>2015-10-16</created><authors><author><keyname>Couto</keyname><forenames>Rodrigo de Souza</forenames></author><author><keyname>Secci</keyname><forenames>Stefano</forenames></author><author><keyname>Campista</keyname><forenames>Miguel Elias Mitre</forenames></author><author><keyname>Costa</keyname><forenames>Lu&#xed;s Henrique Maciel Kosmalski</forenames></author></authors><title>Latency Versus Survivability in Geo-Distributed Data Center Design</title><categories>cs.NI</categories><journal-ref>2014 IEEE Global Communications Conference (GLOBECOM)</journal-ref><doi>10.1109/GLOCOM.2014.7036956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hot topic in data center design is to envision geo-distributed
architectures spanning a few sites across wide area networks, allowing more
proximity to the end users and higher survivability, defined as the capacity of
a system to operate after failures. As a shortcoming, this approach is subject
to an increase of latency between servers, caused by their geographic
distances. In this paper, we address the trade-off between latency and
survivability in geo-distributed data centers, through the formulation of an
optimization problem. Simulations considering realistic scenarios show that the
latency increase is significant only in the case of very strong survivability
requirements, whereas it is negligible for moderate survivability requirements.
For instance, the worst-case latency is less than 4~ms when guaranteeing that
80% of the servers are available after a failure, in a network where the
latency could be up to 33 ms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04951</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04951</id><created>2015-10-16</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author></authors><title>The Physical Web in Smart Cities</title><categories>cs.NI cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the physical web projects based on network
proximity for Smart Cities. In general, the Physical Web is an approach for
connecting any physical object to the web. With this approach, we can navigate
and control physical objects in the world surrounding mobile devices.
Alternatively, we can execute services on mobile devices, depending on the
surrounding physical objects. Technically, there are different ways to
enumerate physical objects. In this paper, we will target the models based on
the wireless proximity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04953</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04953</id><created>2015-10-16</created><authors><author><keyname>Krause</keyname><forenames>Ben</forenames></author></authors><title>Optimizing and Contrasting Recurrent Neural Network Architectures</title><categories>stat.ML cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) have long been recognized for their
potential to model complex time series. However, it remains to be determined
what optimization techniques and recurrent architectures can be used to best
realize this potential. The experiments presented take a deep look into Hessian
free optimization, a powerful second order optimization method that has shown
promising results, but still does not enjoy widespread use. This algorithm was
used to train to a number of RNN architectures including standard RNNs, long
short-term memory, multiplicative RNNs, and stacked RNNs on the task of
character prediction. The insights from these experiments led to the creation
of a new multiplicative LSTM hybrid architecture that outperformed both LSTM
and multiplicative RNNs. When tested on a larger scale, multiplicative LSTM
achieved character level modelling results competitive with the state of the
art for RNNs using very different methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04954</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04954</id><created>2015-10-16</created><authors><author><keyname>Hashemi</keyname><forenames>Yoones</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author></authors><title>New Characterization and Efficient Exhaustive Search Algorithm for
  Elementary Trapping Sets of Variable-Regular LDPC Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new characterization for elementary trapping sets
(ETSs) of variable-regular low-density parity-check (LDPC) codes. Recently,
Karimi and Banihashemi proposed a characterization of ETSs, which was based on
viewing an ETS as a layered superset (LSS) of a short cycle in the code's
Tanner graph. A notable advantage of LSS characterization is that it
corresponds to a simple LSS-based search algorithm that starts from short
cycles of the graph and finds the ETSs with LSS structure efficiently. Compared
to the LSS-based characterization of Karimi and Banihashemi, which is based on
a single LSS expansion technique, the new characterization involves two
additional expansion techniques. The introduction of the new techniques
mitigates two problems that LSS-based characterization/search suffers from: (1)
exhaustiveness, (2) search efficiency. We prove that using the three expansion
techniques, any ETS structure can be obtained starting from a simple cycle, no
matter how large the size of the structure $a$ or the number of its unsatisfied
check nodes $b$ are. We also demonstrate that for the proposed
characterization/search to exhaustively cover all the ETS structures within the
$(a,b)$ classes with $a \leq a_{max}, b \leq b_{max}$, for any value of
$a_{max}$ and $b_{max}$, the length of the short cycles required to be
enumerated is less than that of the LSS-based characterization/search. We also
prove that the three expansion techniques, proposed here, are the only
expansions needed for characterization of ETS structures starting from simple
cycles in the graph, if one requires each and every intermediate sub-structure
to be an ETS as well. Extensive simulation results are provided to show that,
compared to LSS-based search, significant improvement in search speed and
memory requirements can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04967</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04967</id><created>2015-10-16</created><updated>2016-02-06</updated><authors><author><keyname>Furtado</keyname><forenames>Bernardo Alves</forenames></author><author><keyname>Eberhardt</keyname><forenames>Isaque Daniel Rocha</forenames></author></authors><title>A simple agent-based spatial model of the economy: tools for policy</title><categories>cs.MA q-fin.GN</categories><comments>30 pages, 25 figures, includes ODD Protocol and pseudocodes. Version
  in Portuguese available at https://mpra.ub.uni-muenchen.de/67005/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study simulates the evolution of artificial economies in order to
understand the tax relevance of administrative boundaries in the quality of
life of its citizens. The modeling involves the construction of a computational
algorithm, which includes citizens, bounded into families; firms and
governments; all of them interacting in markets for goods, labor and real
estate. The real estate market allows families to move to dwellings with higher
quality or lower price when the families capitalize property values. The goods
market allows consumers to search on a flexible number of firms choosing by
price and proximity. The labor market entails a matching process between firms
(location and offered wage) and candidates (qualification). The government may
be configured into one, four or seven distinct sub-national governments. The
role of government is to collect taxes on the value added of firms in its
territory and invest the taxes into higher levels of quality of life for
residents. The model does not have a credit market. The results suggest that
the configuration of administrative boundaries is relevant to the levels of
quality of life arising from the reversal of taxes. The model with seven
regions is more dynamic, with higher GDP values, but more unequal and
heterogeneous across regions. The simulation with only one region is more
homogeneously poor. The study seeks to contribute to a theoretical and
methodological framework as well as to describe, operationalize and test
computer models of public finance analysis, with explicitly spatial and dynamic
emphasis. Several alternatives of expansion of the model for future research
are described. Moreover, this study adds to the existing literature in the
realm of simple microeconomic computational models, specifying structural
relationships between local governments and firms, consumers and dwellings
mediated by distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04972</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04972</id><created>2015-10-16</created><authors><author><keyname>Sun</keyname><forenames>Weiyi</forenames></author><author><keyname>Rumshisky</keyname><forenames>Anna</forenames></author><author><keyname>Uzuner</keyname><forenames>Ozlem</forenames></author></authors><title>Normalization of Relative and Incomplete Temporal Expressions in
  Clinical Narratives</title><categories>cs.CL cs.AI cs.IR</categories><comments>Draft version</comments><journal-ref>Journal of the American Medical Informatics Association (2015)</journal-ref><doi>10.1093/jamia/ocu004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the RI-TIMEXes in temporally annotated corpora and propose two
hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative
domain: the anchor point hypothesis and the anchor relation hypothesis. We
annotate the RI-TIMEXes in three corpora to study the characteristics of
RI-TMEXes in different domains. This informed the design of our RI-TIMEX
normalization system for the clinical domain, which consists of an anchor point
classifier, an anchor relation classifier and a rule-based RI-TIMEX text span
parser. We experiment with different feature sets and perform error analysis
for each system component. The annotation confirmed the hypotheses that we can
simplify the RI-TIMEXes normalization task using two multi-label classifiers.
Our system achieves anchor point classification, anchor relation classification
and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under
relaxed matching criteria) respectively on the held-out test set of the 2012
i2b2 temporal relation challenge. Experiments with feature sets reveals some
interesting findings such as the verbal tense feature does not inform the
anchor relation classification in clinical narratives as much as the tokens
near the RI-TIMEX. Error analysis shows that underrepresented anchor point and
anchor relation classes are difficult to detect. We formulate the RI-TIMEX
normalization problem as a pair of multi-label classification problems.
Considering only the RI-TIMEX extraction and normalization, the system achieves
statistically significant improvement over the RI-TIMEX results of the best
systems in the 2012 i2b2 challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04989</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04989</id><created>2015-10-16</created><authors><author><keyname>Narendra</keyname><forenames>Kumpati S.</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>The Rationale for Second Level Adaptation</title><categories>cs.SY</categories><comments>7 pages, 10 figures</comments><report-no>1501</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a new approach to the adaptive control of linear time-invariant
plants with unknown parameters (referred to as second level adaptation), was
introduced by Han and Narendra in [1]. Based on $N (\geq m+1)$ fixed or
adaptive models of the plant, where $m$ is the dimension of the unknown
parameter vector, an unknown parameter vector $\alpha\in R^{N}$ is estimated in
the new approach, and in turn, is used to control the overall system.
Simulation studies were presented in [1] to demonstrate that the new method is
significantly better than those that are currently in use.
  In this paper, we undertake a more detailed examination of the theoretical
and practical advantages claimed for the new method. In particular, the need
for many models, the proof of stability, and the improvement in performance and
robustness are explored in depth both theoretically and experimentally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04991</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04991</id><created>2015-10-16</created><updated>2015-11-30</updated><authors><author><keyname>Rubinstein</keyname><forenames>Aviad</forenames></author></authors><title>ETH-Hardness for Symmetric Signaling in Zero-Sum Games</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that, assuming the exponential time hypothesis, finding an
\epsilon-approximately optimal symmetric signaling scheme in a two-player
zero-sum game requires quasi-polynomial time. This is tight by [CCDEHT'15] and
resolves an open question of [Dughmi'14]. We also prove that finding a
multiplicative approximation is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.04995</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.04995</id><created>2015-10-16</created><authors><author><keyname>Malas</keyname><forenames>Tareq</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author><author><keyname>Keyes</keyname><forenames>David</forenames></author></authors><title>Multi-dimensional intra-tile parallelization for memory-starved stencil
  computations</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing the performance of stencil algorithms has been the subject of
intense research over the last two decades. Since many stencil schemes have low
arithmetic intensity, most optimizations focus on increasing the temporal data
access locality, thus reducing the data traffic through the main memory
interface with the ultimate goal of decoupling from this bottleneck. There are,
however, only few approaches that explicitly leverage the shared cache feature
of modern multicore chips. If every thread works on its private, separate cache
block, the available cache space can become too small, and sufficient temporal
locality may not be achieved.
  We propose a flexible multi-dimensional intra-tile parallelization method for
stencil algorithms on multicore CPUs with a shared outer-level cache. This
method leads to a significant reduction in the required cache space without
adverse effects from hardware prefetching or TLB shortage. Our \emph{Girih}
framework includes an auto-tuner to select optimal parameter configurations on
the target hardware. We conduct performance experiments on two contemporary
Intel processors and compare with the state-of-the-art stencil frameworks PLUTO
and Pochoir, using four corner-case stencil schemes and a wide range of problem
sizes. \emph{Girih} shows substantial performance advantages and best
arithmetic intensity at almost all problem sizes, especially on low-intensity
stencils with variable coefficients. We study in detail the performance
behavior at varying grid size using phenomenological performance modeling. Our
analysis of energy consumption reveals that our method can save energy by
reduced DRAM bandwidth usage even at marginal performance gain. It is thus well
suited for future architectures that will be strongly challenged by the cost of
data movement, be it in terms of performance or energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05024</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05024</id><created>2015-10-16</created><authors><author><keyname>Huck</keyname><forenames>Patrick</forenames></author><author><keyname>Jain</keyname><forenames>Anubhav</forenames></author><author><keyname>Gunter</keyname><forenames>Dan</forenames></author><author><keyname>Winston</keyname><forenames>Donald</forenames></author><author><keyname>Persson</keyname><forenames>Kristin</forenames></author></authors><title>A Community Contribution Framework for Sharing Materials Data with
  Materials Project</title><categories>cs.SE</categories><comments>7 pages, 3 figures, Proceedings of 2015 IEEE 11th International
  Conference on eScience, to be published in IEEE Computer Society</comments><doi>10.1109/eScience.2015.75</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As scientific discovery becomes increasingly data-driven, software platforms
are needed to efficiently organize and disseminate data from disparate sources.
This is certainly the case in the field of materials science. For example,
Materials Project has generated computational data on over 60,000 chemical
compounds and has made that data available through a web portal and REST
interface. However, such portals must seek to incorporate community submissions
to expand the scope of scientific data sharing. In this paper, we describe
MPContribs, a computing/software infrastructure to integrate and organize
contributions of simulated or measured materials data from users. Our solution
supports complex submissions and provides interfaces that allow contributors to
share analyses and graphs. A RESTful API exposes mechanisms for book-keeping,
retrieval and aggregation of submitted entries, as well as persistent URIs or
DOIs that can be used to reference the data in publications. Our approach
isolates contributed data from a host project's quality-controlled core data
and yet enables analyses across the entire dataset, programmatically or through
customized web apps. We expect the developed framework to enhance collaborative
determination of material properties and to maximize the impact of each
contributor's dataset. In the long-term, MPContribs seeks to make Materials
Project an institutional, and thus community-wide, memory for computational and
experimental materials science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05028</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05028</id><created>2015-10-16</created><authors><author><keyname>Shah</keyname><forenames>Peter</forenames></author><author><keyname>So</keyname><forenames>Won</forenames></author></authors><title>Lamassu: Storage-Efficient Host-Side Encryption</title><categories>cs.CR</categories><comments>13 pages, 11 figures, 2015 USENIX Annual Technical Conference (USENIX
  ATC 15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many storage customers are adopting encryption solutions to protect critical
data. Most existing encryption solutions sit in, or near, the application that
is the source of critical data, upstream of the primary storage system. Placing
encryption near the source ensures that data remains encrypted throughout the
storage stack, making it easier to use untrusted storage, such as public
clouds.
  Unfortunately, such a strategy also prevents downstream storage systems from
applying content-based features, such as deduplication, to the data. In this
paper, we present Lamassu, an encryption solution that uses block-oriented,
host-based, convergent encryption to secure data, while preserving
storage-based data deduplication. Unlike past convergent encryption systems,
which typically store encryption metadata in a dedicated store, our system
transparently inserts its metadata into each file's data stream. This allows us
to add Lamassu to an application stack without modifying either the client
application or the storage controller.
  In this paper, we lay out the architecture and security model used in our
system, and present a new model for maintaining metadata consistency and data
integrity in a convergent encryption environment. We also evaluate its storage
efficiency and I/O performance by using a variety of microbenchmarks, showing
that Lamassu provides excellent storage efficiency, while achieving I/O
throughput on par with similar conventional encryption systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05034</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05034</id><created>2015-10-16</created><updated>2015-10-29</updated><authors><author><keyname>Narendra</keyname><forenames>Kumpati S.</forenames></author><author><keyname>Mukhopadyhay</keyname><forenames>Snehasis</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author></authors><title>Improving the Speed of Response of Learning Algorithms Using Multiple
  Models</title><categories>cs.LG</categories><comments>7 Pages, submitted to American Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the first of a series of papers that the authors propose to write on
the subject of improving the speed of response of learning systems using
multiple models. During the past two decades, the first author has worked on
numerous methods for improving the stability, robustness, and performance of
adaptive systems using multiple models and the other authors have collaborated
with him on some of them. Independently, they have also worked on several
learning methods, and have considerable experience with their advantages and
limitations. In particular, they are well aware that it is common knowledge
that machine learning is in general very slow. Numerous attempts have been made
by researchers to improve the speed of convergence of algorithms in different
contexts. In view of the success of multiple model based methods in improving
the speed of convergence in adaptive systems, the authors believe that the same
approach will also prove fruitful in the domain of learning. In this paper, a
first attempt is made to use multiple models for improving the speed of
response of the simplest learning schemes that have been studied. i.e. Learning
Automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05041</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05041</id><created>2015-10-16</created><authors><author><keyname>Wang</keyname><forenames>Linnan</forenames></author><author><keyname>Wu</keyname><forenames>Wei</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author></authors><title>BLASX: A High Performance Level-3 BLAS Library for Heterogeneous
  Multi-GPU Computing</title><categories>cs.DC</categories><comments>under review for IPDPS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Basic Linear Algebra Subprograms (BLAS) are a set of low level linear algebra
kernels widely adopted by applications involved with the deep learning and
scientific computing. The massive and economic computing power brought forth by
the emerging GPU architectures drives interest in implementation of
compute-intensive level 3 BLAS on multi-GPU systems. In this paper, we
investigate existing multi-GPU level 3 BLAS and present that 1) issues, such as
the improper load balancing, inefficient communication, insufficient GPU stream
level concurrency and data caching, impede current implementations from fully
harnessing heterogeneous computing resources; 2) and the inter-GPU
Peer-to-Peer(P2P) communication remains unexplored. We then present BLASX: a
highly optimized multi-GPU level-3 BLAS. We adopt the concepts of
algorithms-by-tiles treating a matrix tile as the basic data unit and
operations on tiles as the basic task. Tasks are guided with a dynamic
asynchronous runtime, which is cache and locality aware. The communication cost
under BLASX becomes trivial as it perfectly overlaps communication and
computation across multiple streams during asynchronous task progression. It
also takes the current tile cache scheme one step further by proposing an
innovative 2-level hierarchical tile cache, taking advantage of inter-GPU P2P
communication. As a result, linear speedup is observable with BLASX under
multi-GPU configurations; and the extensive benchmarks demonstrate that BLASX
consistently outperforms the related leading industrial and academic projects
such as cuBLAS-XT, SuperMatrix, MAGMA and PaRSEC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05043</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05043</id><created>2015-10-16</created><authors><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>A cost function for similarity-based hierarchical clustering</title><categories>cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of algorithms for hierarchical clustering has been hampered
by a shortage of precise objective functions. To help address this situation,
we introduce a simple cost function on hierarchies over a set of points, given
pairwise similarities between those points. We show that this criterion behaves
sensibly in canonical instances and that it admits a top-down construction
procedure with a provably good approximation ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05044</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05044</id><created>2015-10-16</created><updated>2015-12-29</updated><authors><author><keyname>Gogioso</keyname><forenames>Stefano</forenames></author></authors><title>Device-independent quantum secret sharing using Mermin-type
  contextuality</title><categories>quant-ph cs.CR</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new quantum secret sharing protocol based on recent advances in
Mermin-type contextuality scenarios, which is provably secure against
postquantum nonsignaling attackers. It is a fundamental assumption of secret
sharing protocols that not all players are trusted parties, and that some may
collude amongst themselves and with eavesdroppers to break confidentiality. To
this extent, quantum secret sharing introduces a new layer of security,
enabling eavesdropping detection via entangled states and noncommuting
observables. A more thorough security analysis, however, becomes crucial if the
protocol relies on untrusted devices for its implementation: for example, it
cannot be excluded that some players may collude with the device supplier. In
this paper, we put recent developments in Mermin-type contextuality to work in
a new quantum secret sharing protocol. The maximal contextuality (aka maximal
non-locality, or zero local fraction) demonstrated by the measurement scenarios
results in strong device-independent security against nonsignaling attackers --
be they classical, quantum or postquantum -- which can be operationally
established by means of minimal statistical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05048</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05048</id><created>2015-10-16</created><authors><author><keyname>Fan</keyname><forenames>Cuiling</forenames></author><author><keyname>Li</keyname><forenames>Nian</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author></authors><title>A class of optimal ternary cyclic codes and their duals</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have applications in consumer
electronics, data storage systems, and communication systems as they have
efficient encoding and decoding algorithms. Let $m=2\ell+1$ for an integer
$\ell\geq 1$ and $\pi$ be a generator of $\gf(3^m)^*$. In this paper, a class
of cyclic codes $\C_{(u,v)}$ over $\gf(3)$ with two nonzeros $\pi^{u}$ and
$\pi^{v}$ is studied, where $u=(3^m+1)/2$, and $v=2\cdot 3^{\ell}+1$ is the
ternary Welch-type exponent. Based on a result on the non-existence of
solutions to certain equation over $\gf(3^m)$, the cyclic code $\C_{(u,v)}$ is
shown to have minimal distance four, which is the best minimal distance for any
linear code over $\gf(3)$ with length $3^m-1$ and dimension $3^m-1-2m$
according to the Sphere Packing bound. The duals of this class of cyclic codes
are also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05058</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05058</id><created>2015-10-16</created><authors><author><keyname>Amelkin</keyname><forenames>Victor</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj</forenames></author><author><keyname>Bogdanov</keyname><forenames>Petko</forenames></author></authors><title>A Distance Measure for the Analysis of Polar Opinion Dynamics in Social
  Networks</title><categories>cs.SI cs.DM cs.DS stat.ML</categories><acm-class>G.2.2; H.2.8; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of opinion dynamics in social networks plays an important role in
today's life. For applications such as predicting users' political preference,
it is particularly important to be able to analyze the dynamics of competing
opinions. While observing the evolution of polar opinions of a social network's
users over time, can we tell when the network &quot;behaved&quot; abnormally?
Furthermore, can we predict how the opinions of the users will change in the
future? Do opinions evolve according to existing network opinion dynamics
models? To answer such questions, it is not sufficient to study individual user
behavior, since opinions can spread far beyond users' egonets. We need a method
to analyze opinion dynamics of all network users simultaneously and capture the
effect of individuals' behavior on the global evolution pattern of the social
network.
  In this work, we introduce Social Network Distance (SND) - a distance measure
that quantifies the &quot;cost&quot; of evolution of one snapshot of a social network
into another snapshot under various models of polar opinion propagation. SND
has a rich semantics of a transportation problem, yet, is computable in time
linear in the number of users, which makes SND applicable to the analysis of
large-scale online social networks. In our experiments with synthetic and
real-world Twitter data, we demonstrate the utility of our distance measure for
anomalous event detection. It achieves a true positive rate of 0.83, twice as
high as that of alternatives. When employed for opinion prediction in Twitter,
our method's accuracy is 75.63%, which is 7.5% higher than that of the next
best method.
  Source Code: https://cs.ucsb.edu/~victor/pub/ucsb/dbl/snd/
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05063</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05063</id><created>2015-10-16</created><authors><author><keyname>Monaco</keyname><forenames>Matthew</forenames></author><author><keyname>Michel</keyname><forenames>Oliver</forenames></author><author><keyname>Keller</keyname><forenames>Eric</forenames></author></authors><title>Applying Operating System Principles to SDN Controller Design</title><categories>cs.NI</categories><comments>Proceedings of the Twelfth ACM Workshop on Hot Topics in Networks.
  ACM, 2013</comments><acm-class>C.2.4</acm-class><doi>10.1145/2535771.2535789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rather than creating yet another network controller which provides a
framework in a specific (potentially new) programming language and runs as a
monolithic application, in this paper we extend an existing operating system
and leverage its software ecosystem in order to serve as a practical SDN
controller. This paper introduces yanc, a controller platform for
software-defined networks which exposes the network configuration and state as
a file system, enabling user and system applications to interact through
standard file I/O, and to easily take advantage of the tools available on the
host operating system. In yanc, network applications are separate processes,
are provided by multiple sources, and may be written in any language.
Applications benefit from common and powerful technologies such as the virtual
file system (VFS) layer, which we leverage to layer a distributed file system
on top of, and Linux namespaces, which we use to isolate applications with
different views (e.g., slices). In this paper we present the goals and design
of yanc. Our initial prototype is built with the FUSE file system in user space
on Linux and has been demonstrated with a simple static flow pusher
application. Effectively, we are making Linux the network operating system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05064</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05064</id><created>2015-10-16</created><authors><author><keyname>Yan</keyname><forenames>Qifa</forenames></author><author><keyname>Cheng</keyname><forenames>Minquan</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author><author><keyname>Chen</keyname><forenames>Qingchun</forenames></author></authors><title>On the Placement Delivery Array Design in Centralized Coded Caching
  Scheme</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching is a promising solution to satisfy the ever-increasing demands for
the multi-media traffics. In caching networks, coded caching is a recently
proposed technique that achieves significant performance gains over the uncoded
caching schemes. However, to implement the coded caching schemes, each file has
to be split into $F$ packets, which usually increases exponentially with the
number of users $K$. Thus, designing caching schemes that decrease the order of
$F$ is meaningful for practical implementations. In this paper, by reviewing
the Ali-Niesen caching scheme, the placement delivery array (PDA) design
problem is firstly formulated to characterize the placement issue and the
delivery issue with a single array. Moreover, it is disclosed that the problem
of designing a centralized coded caching scheme can be translated into a
problem of designing an appropriate PDA. Secondly, it is shown that the
Ali-Niesen scheme corresponds to a special class of PDA, which realizes the
best coding gain with the least $F$. Thirdly, we present a new construction of
PDA for the centralized caching system, wherein the cache size of the server is
$q$ times of every user's (identical cache size is assumed at all users). The
new construction can decrease the required $F$ from the order
$O\left(e^{K\cdot\left(\frac{1}{q}\ln q+(1-\frac{1}{q})\ln
\frac{q}{q-1}\right)}\right)$ of Ali-Niesen scheme to
$O\left(e^{K\cdot\frac{1}{q}\ln q}\right)$, which saves a factor of
$O\left(e^{K\cdot(1-\frac{1}{q})\ln\frac{q}{q-1}}\right)$ that increases
exponentially with $K$, while the coding gain loss is only $1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05067</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05067</id><created>2015-10-16</created><updated>2016-02-04</updated><authors><author><keyname>Liao</keyname><forenames>Qianli</forenames></author><author><keyname>Leibo</keyname><forenames>Joel Z.</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>How Important is Weight Symmetry in Backpropagation?</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient backpropagation (BP) requires symmetric feedforward and feedback
connections -- the same weights must be used for forward and backward passes.
This &quot;weight transport problem&quot; (Grossberg 1987) is thought to be one of the
main reasons to doubt BP's biologically plausibility. Using 15 different
classification datasets, we systematically investigate to what extent BP really
depends on weight symmetry. In a study that turned out to be surprisingly
similar in spirit to Lillicrap et al.'s demonstration (Lillicrap et al. 2014)
but orthogonal in its results, our experiments indicate that: (1) the
magnitudes of feedback weights do not matter to performance (2) the signs of
feedback weights do matter -- the more concordant signs between feedforward and
their corresponding feedback connections, the better (3) with feedback weights
having random magnitudes and 100% concordant signs, we were able to achieve the
same or even better performance than SGD. (4) some
normalizations/stabilizations are indispensable for such asymmetric BP to work,
namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a &quot;Batch
Manhattan&quot; (BM) update rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05069</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05069</id><created>2015-10-17</created><updated>2015-10-21</updated><authors><author><keyname>Chen</keyname><forenames>Nan-Chen</forenames></author><author><keyname>Poon</keyname><forenames>Sarah S.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Lavanya</forenames></author><author><keyname>Aragon</keyname><forenames>Cecilia R.</forenames></author></authors><title>Considering Time in Designing Large-Scale Systems for Scientific
  Computing</title><categories>cs.HC</categories><comments>13 pages, to be published in Proceedings of the ACM Conference on
  Computer Supported Cooperative Work 2016</comments><acm-class>H.5.3</acm-class><doi>10.1145/2818048.2819988</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High performance computing (HPC) has driven collaborative science discovery
for decades. Exascale computing platforms, currently in the design stage, will
be deployed around 2022. The next generation of supercomputers is expected to
utilize radically different computational paradigms, necessitating fundamental
changes in how the community of scientific users will make the most efficient
use of these powerful machines. However, there have been few studies of how
scientists work with exascale or close-to-exascale HPC systems. Time as a
metaphor is so pervasive in the discussions and valuation of computing within
the HPC community that it is worthy of close study. We utilize time as a lens
to conduct an ethnographic study of scientists interacting with HPC systems. We
build upon recent CSCW work to consider temporal rhythms and collective time
within the HPC sociotechnical ecosystem and provide considerations for future
system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05071</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05071</id><created>2015-10-17</created><authors><author><keyname>Kim</keyname><forenames>Hunmin</forenames></author><author><keyname>Zhu</keyname><forenames>Minghui</forenames></author></authors><title>Distributed Robust Adaptive Frequency Regulation of Power Grid with
  Renewable Integration</title><categories>cs.SY</categories><comments>Submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates distributed frequency regulation of multi-machine
power systems with wind generations when their frequencies are known (and
unknown). The challenge is to regulate frequencies under fast changing and
unpredictable wind generations. We split unknown signals into three parts
according to their frequency ranges and design an (adaptive) internal model to
reconstruct the low and medium frequency signals. We integrate heterogeneous
grid components to deal with fast changing signals. At each bus, a battery
system is used to filter out high frequency components of wind generations,
demand response is utilized to deal with their medium-frequency components and
a synchronous generator is exploited to handle their low-frequency components.
The proposed controllers ensure exponential stability (and asymptotic
convergence) of system states with respect to their desired signals.
Simulations on the IEEE PES Distribution 37 bus test feeder topology are
conducted to demonstrate the effectiveness of the proposed controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05073</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05073</id><created>2015-10-17</created><authors><author><keyname>Liu</keyname><forenames>Jianming</forenames></author><author><keyname>Grant</keyname><forenames>Steven L.</forenames></author></authors><title>Block Sparse Memory Improved Proportionate Affine Projection Sign
  Algorithm</title><categories>cs.SY cs.SD</categories><comments>2 pages, accepted by Electronics Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A block sparse memory improved proportionate affine projection sign algorithm
(BS-MIP-APSA) is proposed for block sparse system identification under
impulsive noise. The new BS-MIP-APSA not only inherits the performance
improvement for block-sparse system identification, but also achieves
robustness to impulsive noise and the efficiency of the memory improved
proportionate affine projection sign algorithm (MIP-APSA). Simulations indicate
that it can provide both faster convergence rate and better tracking ability
under impulsive interference for block sparse system identification as compared
to APSA and MIP-APSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05075</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05075</id><created>2015-10-17</created><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Yilmaz</keyname><forenames>H. Birkan</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Energy Model for Vesicle-Based Active Transport Molecular Communication</title><categories>cs.ET cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In active transport molecular communication (ATMC), information particles are
actively transported from a transmitter to a receiver using special proteins.
Prior work has demonstrated that ATMC can be an attractive and viable solution
for on-chip applications. The energy consumption of an ATMC system plays a
central role in its design and engineering. In this work, an energy model is
presented for ATMC and the model is used to provide guidelines for designing
energy efficient systems. The channel capacity per unit energy is analyzed and
maximized. It is shown that based on the size of the symbol set and the symbol
duration, there is a vesicle size that maximizes rate per unit energy. It is
also demonstrated that maximizing rate per unit energy yields very different
system parameters compared to maximizing the rate only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05076</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05076</id><created>2015-10-17</created><updated>2016-02-02</updated><authors><author><keyname>Fong</keyname><forenames>Brendan</forenames></author><author><keyname>Rapisarda</keyname><forenames>Paolo</forenames></author><author><keyname>Soboci&#x144;ski</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>A categorical approach to open and interconnected dynamical systems</title><categories>cs.SY cs.LO math.CT</categories><comments>10 pages + 3 page appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a sound and complete graphical theory for discrete linear
time-invariant dynamical systems. The graphical syntax, as in previous work, is
closely related to the classical notion of signal flow diagrams, differently
from previous work, these are understood as multi-input multi-output
transducers that process streams with an \emph{infinite past} as well as an
infinite future. This extended semantics features non-controllable systems, and
we develop a novel, structural characterisation of controllability. Our
approach is formalised through the theory of props, extending the work of
Bonchi, Zanasi and the third author.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05091</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05091</id><created>2015-10-17</created><authors><author><keyname>Zhao</keyname><forenames>Yongwang</forenames></author><author><keyname>Sann</keyname><forenames>David</forenames></author><author><keyname>Zhang</keyname><forenames>Fuyuan</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author></authors><title>Reasoning About Information Flow Security of Separation Kernels with
  Channel-based Communication</title><categories>cs.SE cs.CR</categories><comments>17 pages, 4 figures</comments><acm-class>D.2.1; D.2.4; D.4.6; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assurance of information flow security by formal methods is mandated in
security certification of separation kernels. As an industrial standard for
separation kernels, ARINC 653 has been complied with by mainstream separation
kernels. Security of functionalities defined in ARINC 653 is thus very
important for the development and certification of separation kernels. This
paper presents the first effort to formally specify and verify separation
kernels with ARINC 653 channel-based communication. We provide a reusable
formal specification and security proofs for separation kernels in
Isabelle/HOL. During reasoning about information flow security, we find some
security flaws in the ARINC 653 standard, which can cause information leakage,
and fix them in our specification. We also validate the existence of the
security flaws in two open-source ARINC 653 compliant separation kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05093</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05093</id><created>2015-10-17</created><authors><author><keyname>Cochefert</keyname><forenames>Manfred</forenames></author><author><keyname>Couturier</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Kratsch</keyname><forenames>Dieter</forenames></author></authors><title>Faster algorithms to enumerate hypergraph transversals</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A transversal of a hypergraph is a set of vertices intersecting each
hyperedge. We design and analyze new exponential-time algorithms to enumerate
all inclusion-minimal transversals of a hypergraph. For each fixed k&gt;2, our
algorithms for hypergraphs of rank k, where the rank is the maximum size of a
hyperedge, outperform the previous best. This also implies improved upper
bounds on the maximum number of minimal transversals in n-vertex hypergraphs of
rank k&gt;2. Our main algorithm is a branching algorithm whose running time is
analyzed with Measure and Conquer. It enumerates all minimal transversals of
hypergraphs of rank 3 on n vertices in time O(1.6755^n). Our algorithm for
hypergraphs of rank 4 is based on iterative compression. Our enumeration
algorithms improve upon the best known algorithms for counting minimum
transversals in hypergraphs of rank k for k&gt;2 and for computing a minimum
transversal in hypergraphs of rank k for k&gt;5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05104</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05104</id><created>2015-10-17</created><authors><author><keyname>Meng</keyname><forenames>Ting Wei</forenames></author><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author></authors><title>The Theory of Computational Quasi-conformal Geometry on Point Clouds</title><categories>cs.CG cs.GR math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quasi-conformal (QC) theory is an important topic in complex analysis, which
studies geometric patterns of deformations between shapes. Recently,
computational QC geometry has been developed and has made significant
contributions to medical imaging, computer graphics and computer vision.
Existing computational QC theories and algorithms have been built on
triangulation structures. In practical situations, many 3D acquisition
techniques often produce 3D point cloud (PC) data of the object, which does not
contain connectivity information. It calls for a need to develop computational
QC theories on PCs. In this paper, we introduce the concept of computational QC
geometry on PCs. We define PC quasi-conformal (PCQC) maps and their associated
PC Beltrami coefficients (PCBCs). The PCBC is analogous to the Beltrami
differential in the continuous setting. Theoretically, we show that the PCBC
converges to its continuous counterpart as the density of the PC tends to zero.
We also theoretically and numerically validate the ability of PCBCs to measure
local geometric distortions of PC deformations. With these concepts, many
existing QC based algorithms for geometry processing and shape analysis can be
easily extended to PC data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05107</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05107</id><created>2015-10-17</created><authors><author><keyname>Quach</keyname><forenames>Willy</forenames></author><author><keyname>Langou</keyname><forenames>Julien</forenames></author></authors><title>A Makespan Lower Bound for the Scheduling of the Tiled Cholesky
  Factorization based on ALAP scheduling</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the advent of multicore architectures and massive parallelism, the
tiled Cholesky factorization algorithm has recently received plenty of
attention and is often referenced by practitioners as a case study. It is also
implemented in mainstream dense linear algebra libraries. However, we note that
theoretical study of the parallelism of this algorithm is currently lacking. In
this paper, we present new theoretical results about the tiled Cholesky
factorization in the context of a parallel homogeneous model without
communication costs. We use standard flop-based weights for the tasks. For a
$t$-by-$t$ matrix, we know that the critical path of the tiled Cholesky
algorithm is $9t-10$ and that the weight of all tasks is $t^3$. In this
context, we prove that no schedule with less than $0.185 t^2$ processing units
can finish in a time less than the critical path. In perspective, a naive bound
gives $0.11 t^2.$ We then give a schedule which needs less than $0.25
t^2+0.16t+3$ processing units to complete in the time of the critical path. In
perspective, a naive schedule gives $0.50 t^2.$ In addition, given a fixed
number of processing units, $p$, we give a lower bound on the execution time as
follows: $$\max( \frac{t^{3}}{p}, \frac{t^{3}}{p} - 3\frac{t^2}{p} + 6\sqrt{2p}
- 7 , 9t-10).$$ The interest of the latter formula lies in the middle term. Our
results stem from the observation that the tiled Cholesky factorization is much
better behaved when we schedule it with an ALAP (As Late As Possible) heuristic
than an ASAP (As Soon As Possible) heuristic. We also provide scheduling
heuristics which match closely the lower bound on execution time. We believe
that our theoretical results will help practical scheduling studies. Indeed,
our results enable to better characterize the quality of a practical schedule
with respect to an optimal schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05120</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05120</id><created>2015-10-17</created><updated>2015-12-17</updated><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Frommholz</keyname><forenames>Ingo</forenames></author><author><keyname>Cabanac</keyname><forenames>Guillaume</forenames></author></authors><title>Bibliometric-Enhanced Information Retrieval: 3rd International BIR
  Workshop</title><categories>cs.IR cs.DL</categories><comments>4 pages, 38th European Conference on IR Research, ECIR 2016, Padova,
  Italy. arXiv admin note: substantial text overlap with arXiv:1501.02646</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The BIR workshop brings together experts in Bibliometrics and Information
Retrieval. While sometimes perceived as rather loosely related, these research
areas share various interests and face similar challenges. Our motivation as
organizers of the BIR workshop stemmed from a twofold observation. First, both
communities only partly overlap, albeit sharing various interests. Second, it
will be profitable for both sides to tackle some of the emerging problems that
scholars face today when they have to identify relevant and high quality
literature in the fast growing number of electronic publications available
worldwide. Bibliometric techniques are not yet used widely to enhance retrieval
processes in digital libraries, although they offer value-added effects for
users. Information professionals working in libraries and archives, however,
are increasingly confronted with applying bibliometric techniques in their
services. The first BIR workshop in 2014 set the research agenda by introducing
each group to the other, illustrating state-of-the-art methods, reporting on
current research problems, and brainstorming about common interests. The second
workshop in 2015 further elaborated these themes. This third BIR workshop aims
to foster a common ground for the incorporation of bibliometric-enhanced
services into scholarly search engine interfaces. In particular we will address
specific communities, as well as studies on large, cross-domain collections
like Mendeley and ResearchGate. This third BIR workshop addresses explicitly
both scholarly and industrial researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05128</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05128</id><created>2015-10-17</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author></authors><title>Comprehensive indicator comparisons intelligible to non-experts: The
  case of two SNIP versions</title><categories>cs.DL</categories><comments>Author copy of an article accepted for publication in Scientometrics,
  posted on 17 Oct 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is proposed for comparing different types of bibliometric
indicators, introducing the notion of an Indicator Comparison Report. It
provides a comprehensive overview of the main differences and similarities of
indicators. The comparison shows both the strong points and the limitations of
each of the indicators at stake, rather than over-promoting one indicator and
ignoring the benefits of alternative constructs. It focuses on base notions,
assumptions, and application contexts, which makes it more intelligible to
non-experts. As an illustration, a comparison report is presented for the
original and the modified SNIP (Source Normalized Impact per Paper) indicator
of journal citation impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05129</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05129</id><created>2015-10-17</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author><author><keyname>Halevi</keyname><forenames>Gali</forenames></author></authors><title>On full text download and citation distributions in scientific-scholarly
  journals</title><categories>cs.DL</categories><comments>Author copy of an article accepted for publication in JASIST, posted
  on 17 Oct 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A statistical analysis of full text downloads of articles in Elseviers
ScienceDirect covering all disciplines reveals large differences in download
frequencies, their skewness, and their correlation with Scopus-based citation
counts, between disciplines, journals, and document types. Download counts tend
to be two orders of magnitude higher and less skewedly distributed than
citations. A mathematical model based on the sum of two exponentials does not
adequately capture monthly download counts. The degree of correlation at the
article level within a journal is similar to that at the journal level in the
discipline covered by that journal, suggesting that the differences between
journals are to a large extent discipline specific. Despite the fact that in
all study journals download and citation counts per article positively
correlate, little overlap may exist between the set of articles appearing in
the top of the citation distribution and that with the most frequently
downloaded ones. Usage and citation leaks, bulk downloading, differences
between reader and author populations in a subject field, the type of document
or its content, differences in obsolescence patterns between downloads and
citations, different functions of reading and citing in the research process,
all provide possible explanations of differences between download and citation
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05131</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05131</id><created>2015-10-17</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author></authors><title>Altmetrics as traces of the computerization of the research process</title><categories>cs.DL</categories><comments>Posted on 17 Oct 2015. Article to be published in early 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I propose a broad, multi-dimensional conception of altmetrics, namely as
traces of the computerization of the research process. Computerization should
be conceived in its broadest sense, including all recent developments in ICT
and software, taking place in society as a whole. I distinguish four aspects of
the research process: the collection of research data and development of
research methods; scientific information processing; communication and
organization; and, last but not least, research assessment. I will argue that
in each aspect, computerization plays a key role, and metrics are being
developed to describe this process. I propose to label the total collection of
such metrics as Altmetrics. I seek to provide a theoretical foundation of
altmetrics, based on notions developed by Michael Nielsen in his monograph
Reinventing Discovery: The New Era of Networked Science. Altmetrics can be
conceived as tools for the practical realization of the ethos of science and
scholarship in a computerized or digital age.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05137</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05137</id><created>2015-10-17</created><authors><author><keyname>Chen</keyname><forenames>Xue</forenames></author></authors><title>Integrality Gaps and Approximation Algorithms for Dispersers and
  Bipartite Expanders</title><categories>cs.CC cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of approximating the quality of a disperser. A bipartite
graph $G$ on $([N],[M])$ is a $(\rho N,(1-\delta)M)$-disperser if for any
subset $S\subseteq [N]$ of size $\rho N$, the neighbor set $\Gamma(S)$ contains
at least $(1-\delta)M$ distinct vertices. Our main results are strong
integrality gaps in the Lasserre hierarchy and an approximation algorithm for
dispersers.
  \begin{enumerate}
  \item For any $\alpha&gt;0$, $\delta&gt;0$, and a random bipartite graph $G$ with
left degree $D=O(\log N)$, we prove that the Lasserre hierarchy cannot
distinguish whether $G$ is an $(N^{\alpha},(1-\delta)M)$-disperser or not an
$(N^{1-\alpha},\delta M)$-disperser.
  \item For any $\rho&gt;0$, we prove that there exist infinitely many constants
$d$ such that the Lasserre hierarchy cannot distinguish whether a random
bipartite graph $G$ with right degree $d$ is a $(\rho N,
(1-(1-\rho)^d)M)$-disperser or not a $(\rho N, (1-\Omega(\frac{1-\rho}{\rho d +
1-\rho}))M)$-disperser. We also provide an efficient algorithm to find a subset
of size exact $\rho N$ that has an approximation ratio matching the integrality
gap within an extra loss of
$\frac{\min\{\frac{\rho}{1-\rho},\frac{1-\rho}{\rho}\}}{\log d}$.
\end{enumerate}
  Our method gives an integrality gap in the Lasserre hierarchy for bipartite
expanders with left degree~$D$. $G$ on $([N],[M])$ is a $(\rho N,a)$-expander
if for any subset $S\subseteq [N]$ of size $\rho N$, the neighbor set
$\Gamma(S)$ contains at least $a \cdot \rho N$ distinct vertices. We prove that
for any constant $\epsilon&gt;0$, there exist constants $\epsilon'&lt;\epsilon,\rho,$
and $D$ such that the Lasserre hierarchy cannot distinguish whether a bipartite
graph on $([N],[M])$ with left degree $D$ is a $(\rho N,
(1-\epsilon')D)$-expander or not a $(\rho N, (1-\epsilon)D)$-expander.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05138</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05138</id><created>2015-10-17</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author><author><keyname>Rehman</keyname><forenames>Naveed ur</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Integral Images: Efficient Algorithms for Their Computation and Storage
  in Resource-Constrained Embedded Vision Systems</title><categories>cs.CV</categories><journal-ref>Sensors 2015, 15, 16804-16830</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integral image, an intermediate image representation, has found extensive
use in multi-scale local feature detection algorithms, such as Speeded-Up
Robust Features (SURF), allowing fast computation of rectangular features at
constant speed, independent of filter size. For resource-constrained real-time
embedded vision systems, computation and storage of integral image presents
several design challenges due to strict timing and hardware limitations.
Although calculation of the integral image only consists of simple addition
operations, the total number of operations is large owing to the generally
large size of image data. Recursive equations allow substantial decrease in the
number of operations but require calculation in a serial fashion. This paper
presents two new hardware algorithms that are based on the decomposition of
these recursive equations, allowing calculation of up to four integral image
values in a row-parallel way without significantly increasing the number of
operations. An efficient design strategy is also proposed for a parallel
integral image computation unit to reduce the size of the required internal
memory (nearly 35% for common HD video). Addressing the storage problem of
integral image in embedded vision systems, the paper presents two algorithms
which allow substantial decrease (at least 44.44%) in the memory requirements.
Finally, the paper provides a case study that highlights the utility of the
proposed architectures in embedded vision systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05140</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05140</id><created>2015-10-17</created><updated>2015-11-04</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Proactive Eavesdropping via Jamming for Rate Maximization over Rayleigh
  Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instead of against eavesdropping, this letter proposes a new paradigm in
wireless security by studying how a legitimate monitor (e.g., government
agencies) efficiently eavesdrops a suspicious wireless communication link. The
suspicious transmitter controls its communication rate over Rayleigh fading
channels to maintain a target outage probability at the receiver, and the
legitimate monitor can successfully eavesdrop only when its achievable rate is
no smaller than the suspicious communication rate. We propose a proactive
eavesdropping via jamming approach to maximize the average eavesdropping rate,
where the legitimate monitor sends jamming signals with optimized power control
to moderate the suspicious communication rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05142</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05142</id><created>2015-10-17</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author><author><keyname>Cheung</keyname><forenames>Wah M.</forenames></author><author><keyname>Bais</keyname><forenames>Arjunsingh M.</forenames></author><author><keyname>Menzat</keyname><forenames>Bayar I.</forenames></author><author><keyname>Kanwal</keyname><forenames>Nadia</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Memory-Efficient Design Strategy for a Parallel Embedded Integral Image
  Computation Engine</title><categories>cs.CV</categories><comments>Machine Vision and Image Processing Conference (IMVIP), 2011</comments><doi>10.1109/IMVIP.2011.29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In embedded vision systems, parallel computation of the integral image
presents several design challenges in terms of hardware resources, speed and
power consumption. Although recursive equations significantly reduce the number
of operations for computing the integral image, the required internal memory
becomes prohibitively large for an embedded integral image computation engine
for increasing image sizes. With the objective of achieving high-throughput
with minimum hardware resources, this paper proposes a memory-efficient design
strategy for a parallel embedded integral image computation engine. Results
show that the design achieves nearly 35% reduction in memory for common HD
video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05145</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05145</id><created>2015-10-17</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Rapid Online Analysis of Local Feature Detectors and Their
  Complementarity</title><categories>cs.CV</categories><journal-ref>Sensors 2013, 13, 10876-10907</journal-ref><doi>10.3390/s130810876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A vision system that can assess its own performance and take appropriate
actions online to maximize its effectiveness would be a step towards achieving
the long-cherished goal of imitating humans. This paper proposes a method for
performing an online performance analysis of local feature detectors, the
primary stage of many practical vision systems. It advocates the spatial
distribution of local image features as a good performance indicator and
presents a metric that can be calculated rapidly, concurs with human visual
assessments and is complementary to existing offline measures such as
repeatability. The metric is shown to provide a measure of complementarity for
combinations of detectors, correctly reflecting the underlying principles of
individual detectors. Qualitative results on well-established datasets for
several state-of-the-art detectors are presented based on the proposed measure.
Using a hypothesis testing approach and a newly-acquired, larger image
database, statistically-significant performance differences are identified.
Different detector pairs and triplets are examined quantitatively and the
results provide a useful guideline for combining detectors in applications that
require a reasonable spatial distribution of image features. A principled
framework for combining feature detectors in these applications is also
presented. Timing results reveal the potential of the metric for online
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05154</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05154</id><created>2015-10-17</created><authors><author><keyname>Gatti</keyname><forenames>Christopher J.</forenames></author><author><keyname>Brooks</keyname><forenames>James D.</forenames></author><author><keyname>Nurre</keyname><forenames>Sarah G.</forenames></author></authors><title>A Historical Analysis of the Field of OR/MS using Topic Models</title><categories>stat.ML cs.DL stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates the content of the published scientific literature in
the fields of operations research and management science (OR/MS) since the
early 1950s. Our study is based on 80,757 published journal abstracts from 37
of the leading OR/MS journals. We have developed a topic model, using Latent
Dirichlet Allocation (LDA), and extend this analysis to reveal the temporal
dynamics of the field, journals, and topics. Our analysis shows the generality
or specificity of each of the journals, and we identify groups of journals with
similar content, which are both consistent and inconsistent with intuition. We
also show how journals have become more or less unique in their scope. A more
detailed analysis of each journals' topics over time shows significant temporal
dynamics, especially for journals with niche content. This study presents an
observational, yet objective, view of the published literature from OR/MS that
would be of interest to authors, editors, journals, and publishers.
Furthermore, this work can be used by new entrants to the fields of OR/MS to
understand the content landscape, as a starting point for discussions and
inquiry of the field at large, or as a model for other fields to perform
similar analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05156</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05156</id><created>2015-10-17</created><authors><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author><author><keyname>Ferrarini</keyname><forenames>Bruno</forenames></author><author><keyname>Rehman</keyname><forenames>Naveed Ur</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Assessing The Performance Bounds Of Local Feature Detectors: Taking
  Inspiration From Electronics Design Practices</title><categories>cs.CV</categories><comments>IWSSIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since local feature detection has been one of the most active research areas
in computer vision, a large number of detectors have been proposed. This has
rendered the task of characterizing the performance of various feature
detection methods an important issue in vision research. Inspired by the good
practices of electronic system design, a generic framework based on the
improved repeatability measure is presented in this paper that allows
assessment of the upper and lower bounds of detector performance in an effort
to design more reliable and effective vision systems. This framework is then
employed to establish operating and guarantee regions for several state-of-the
art detectors for JPEG compression and uniform light changes. The results are
obtained using a newly acquired, large image database (15092 images) with 539
different scenes. These results provide new insights into the behavior of
detectors and are also useful from the vision systems design perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05157</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05157</id><created>2015-10-17</created><authors><author><keyname>Ferrarini</keyname><forenames>Bruno</forenames></author><author><keyname>Ehsan</keyname><forenames>Shoaib</forenames></author><author><keyname>Rehman</keyname><forenames>Naveed Ur</forenames></author><author><keyname>McDonald-Maier</keyname><forenames>Klaus D.</forenames></author></authors><title>Performance Characterization of Image Feature Detectors in Relation to
  the Scene Content Utilizing a Large Image Database</title><categories>cs.CV</categories><comments>IWSSIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selecting the most suitable local invariant feature detector for a particular
application has rendered the task of evaluating feature detectors a critical
issue in vision research. No state-of-the-art image feature detector works
satisfactorily under all types of image transformations. Although the
literature offers a variety of comparison works focusing on performance
evaluation of image feature detectors under several types of image
transformation, the influence of the scene content on the performance of local
feature detectors has received little attention so far. This paper aims to
bridge this gap with a new framework for determining the type of scenes, which
maximize and minimize the performance of detectors in terms of repeatability
rate. Several state-of-the-art feature detectors have been assessed utilizing a
large database of 12936 images generated by applying uniform light and blur
changes to 539 scenes captured from the real world. The results obtained
provide new insights into the behaviour of feature detectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05162</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05162</id><created>2015-10-17</created><authors><author><keyname>Veld</keyname><forenames>Sander in 't</forenames></author></authors><title>Satisfiability of Short Circuit Logic</title><categories>cs.LO</categories><comments>Supervisors: dr. Inge Bethke, prof. dr. Jan van Eijck</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logical connectives typically found in programming languages are similar
to their mathematical counterparts, yet different due to their short-circuit
behaviour -- when evaluating them, the second argument is only evaluated if the
first argument is not sufficient to determine the result. Combined with the
possibility of side-effects, this creates a different type of logic called
Short Circuit Logic. A greater theoretical understanding of this logic can lead
to more efficient programming and faster program execution.
  In this thesis, formula satisfiability in the context of Short Circuit Logic
is discussed. A formal definition of evaluation based on valuation algebras is
presented, alongside an alternative definition based on valuation paths. The
accompanying satisfiability and `path-satisfiability' are then proven to be
equivalent, and an implementation of path-satisfiability is given. Although
five types of valuation algebras can be discerned, there are only three
corresponding types of valuation paths. From this, conclusions are drawn about
satisfiability and side-effects; the manner in which side-effects alter truth
values is relevant when analysing satisfiability, but the side-effects
themselves are not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05175</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05175</id><created>2015-10-17</created><updated>2015-12-03</updated><authors><author><keyname>Meng</keyname><forenames>Xianrui</forenames></author><author><keyname>Zhu</keyname><forenames>Haohan</forenames></author><author><keyname>Kollios</keyname><forenames>George</forenames></author></authors><title>Secure Top-k Query Processing on Encrypted Databases</title><categories>cs.CR</categories><acm-class>H.2.7; K.4.4; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy concerns in outsourced cloud databases have become more and more
important recently and many efficient and scalable query processing methods
over encrypted data have been proposed. However, there is very limited work on
how to securely process top-k ranking queries over encrypted databases in the
cloud. In this paper, we focus exactly on this problem: secure and efficient
processing of top-k queries over outsourced databases. In particular, we
propose the first efficient and provable secure top-k query processing
construction that achieves adaptively IND-CQA security. We develop an encrypted
data structure called \emph{EHL} and describe several secure sub-protocols
under our security model to answer top-k queries. Furthermore, we optimize our
query algorithms for both space and time efficiency. Finally, in the
experiments, we empirically analyze our protocol using real world datasets and
demonstrate that our construction is efficient and practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05176</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05176</id><created>2015-10-17</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Network Flows that Solve Linear Equations</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed network flows as solvers in continuous time for the
linear algebraic equation $\mathbf{z}=\mathbf{H}\mathbf{y}$. Each node $i$ has
access to a row $\mathbf{h}_i^{\rm T}$ of the matrix $\mathbf{H}$ and the
corresponding entry $z_i$ in the vector $\mathbf{z}$. The first &quot;consensus +
projection&quot; flow under investigation consists of two terms, one from standard
consensus dynamics and the other contributing to projection onto each affine
subspace specified by the $\mathbf{h}_i$ and $z_i$. The second &quot;projection
consensus&quot; flow on the other hand simply replaces the relative state feedback
in consensus dynamics with projected relative state feedback. Without
dwell-time assumption on switching graphs as well as without positively lower
bounded assumption on arc weights, we prove that all node states converge to a
common solution of the linear algebraic equation, if there is any. The
convergence is global for the &quot;consensus + projection&quot; flow while local for the
&quot;projection consensus&quot; flow in the sense that the initial values must lie on
the affine subspaces. If the linear equation has no exact solutions, we show
that the node states can converge to a ball around the least squares solution
whose radius can be made arbitrarily small through selecting a sufficiently
large gain for the &quot;consensus + projection&quot; flow under fixed bidirectional
graphs. Semi-global convergence to approximate least squares solutions is
demonstrated for general switching directed graphs under suitable conditions.
It is also shown that the &quot;projection consensus&quot; flow drives the average of the
node states to the least squares solution with complete graph. Numerical
examples are provided as illustrations of the established results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05178</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05178</id><created>2015-10-17</created><updated>2015-10-25</updated><authors><author><keyname>Jafarizadeh</keyname><forenames>Saber</forenames></author></authors><title>Optimizing the Convergence Rate of the Quantum Consensus: A Discrete
  Time Model</title><categories>cs.SY</categories><comments>37 pages, 3 figures, 4 table. arXiv admin note: substantial text
  overlap with arXiv:1509.05823</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the recent advances in the field of quantum computing, quantum
systems are modelled and analyzed as networks of decentralized quantum nodes
which employ distributed quantum consensus algorithms for coordination. In the
literature, both continuous and discrete time models have been proposed for
analyzing these algorithms. This paper aims at optimizing the convergence rate
of the discrete time quantum consensus algorithm over a quantum network with
$N$ qudits. The induced graphs are categorized in terms of the partitions of
integer $N$ by arranging them as the Schreier graphs. It is shown that the
original optimization problem reduces to optimizing the Second Largest
Eigenvalue Modulus (SLEM) of the weight matrix. Exploiting the Specht module
representation of partitions of $N$, the Aldous' conjecture is generalized to
all partitions (except ($N$)) in the Hasse diagram of integer $N$. Based on
this result, it is shown that the spectral gap of Laplacian of all induced
graphs corresponding to partitions (other than ($N$)) of $N$ are the same,
while the spectral radius of the Laplacian is obtained from the feasible least
dominant partition in the Hasse diagram of integer $N$. The semidefinite
programming formulation of the problem is addressed analytically for $N \leq
d^2 + 1$ and a wide range of topologies where closed-form expressions for the
optimal results are provided. For a quantum network with complete graph
topology, solution of the optimization problem based on group association
schemes is provided for all values of $N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05179</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05179</id><created>2015-10-17</created><authors><author><keyname>Dibert</keyname><forenames>Karia</forenames></author><author><keyname>Jansen</keyname><forenames>Hayden</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>Algebraic Conditions for Generating Accurate Adjacency Arrays</title><categories>cs.DB cs.DS math.RA</categories><comments>2015 IEEE MIT Undergraduate Research Technology Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data processing systems impose multiple views on data as it is processed by
the system. These views include spreadsheets, databases, matrices, and graphs.
Associative arrays unify and simplify these different approaches into a common
two-dimensional view of data. Graph construction, a fundamental operation in
the data processing pipeline, is typically done by multiplying the incidence
array representations of a graph, $\mathbf{E}_\mathrm{in}$ and
$\mathbf{E}_\mathrm{out}$, to produce an adjacency matrix of the graph that can
be processed with a variety of machine learning clustering techniques. This
work focuses on establishing the mathematical criteria to ensure that the
matrix product $\mathbf{E}_\mathrm{out}^\intercal\mathbf{E}_\mathrm{in}$ is the
adjacency array of the graph. It will then be shown that these criteria are
also necessary and sufficient for the remaining nonzero product of incidence
arrays, $\mathbf{E}_\mathrm{in}^\intercal\mathbf{E}_\mathrm{out}$ to be the
adjacency matrices of the reversed graph. Algebraic structures that comply with
the criteria will be identified and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05182</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05182</id><created>2015-10-17</created><authors><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Cheriat</keyname><forenames>Mohamed</forenames></author></authors><title>Sustainability-Aware Cloud Computing Using Virtual Carbon Tax</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a solution for sustainable cloud system is proposed and then
implemented on a real testbed. The solution composes of optimization of a
profit model and introduction of virtual carbon tax to limit environmental
footprint of the cloud. The proposed multi-criteria optimizer of the cloud
system suggests new optimum CPU frequencies for CPU-cores when the local grid
energy mix or the cloud workload changes. The cloud system is implemented on a
blade system, and proper middlewares are developed to interact with the blades.
The experimental results show that it is possible to significantly decrease the
targeted environmental footprint of the system and keep it profitable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05185</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05185</id><created>2015-10-17</created><authors><author><keyname>Jeub</keyname><forenames>Lucas G. S.</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author><author><keyname>Porter</keyname><forenames>Mason A.</forenames></author></authors><title>A Local Perspective on Community Structure in Multilayer Networks</title><categories>cs.SI math.PR nlin.AO physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate how to use simple dynamical processes such as random walks to
study and visualize community structure in multilayer networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05189</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05189</id><created>2015-10-17</created><authors><author><keyname>Wang</keyname><forenames>Fulton</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Causal Falling Rule Lists</title><categories>cs.AI stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A causal falling rule list (CFRL) is a sequence of if-then rules that
specifies heterogeneous treatment effects, where (i) the order of rules
determines the treatment effect subgroup a subject belongs to, and (ii) the
treatment effect decreases monotonically down the list. A given CFRL
parameterizes a hierarchical bayesian regression model in which the treatment
effects are incorporated as parameters, and assumed constant within
model-specific subgroups. We formulate the search for the CFRL best supported
by the data as a Bayesian model selection problem, where we perform a search
over the space of CFRL models, and approximate the evidence for a given CFRL
model using standard variational techniques. We apply CFRL to a census wage
dataset to identify subgroups of differing wage inequalities between men and
women.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05192</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05192</id><created>2015-10-17</created><authors><author><keyname>Bentley</keyname><forenames>Frank</forenames></author><author><keyname>Church</keyname><forenames>Karen</forenames></author><author><keyname>Harrison</keyname><forenames>Beverly</forenames></author><author><keyname>Lyons</keyname><forenames>Kent</forenames></author><author><keyname>Rafalow</keyname><forenames>Matthew</forenames></author></authors><title>Three Hours a Day: Understanding Current Teen Practices of Smartphone
  Application Use</title><categories>cs.HC cs.CY</categories><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teens are using mobile devices for an increasing number of activities.
Smartphones and a variety of mobile apps for communication, entertainment, and
productivity have become an integral part of their lives. This mobile phone use
has evolved rapidly as technology has changed and thus studies from even 2 or 3
years ago may not reflect new patterns and practices as smartphones have become
more sophisticated. In order to understand current teen's practices around
smartphone use, we conducted a two week, mixed-methods study with 14 diverse
teens. Through voicemail diaries, interviews, and real world usage data from a
logging application installed on their smartphones, we developed an
understanding of the types of apps used by teens, when they use these apps, and
their reasons for using specific apps in particular situations. We found that
the teens in our study used their smartphones for an average of almost 3 hours
per day and that two-thirds of all app use involved interacting with an average
of almost 10 distinct communications applications. From our study data, we
highlight key implications for the design of future mobile apps or services,
specifically new social and communications-related applications that allow
teens to maintain desired levels of privacy and permanence on the content that
they share.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05193</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05193</id><created>2015-10-17</created><updated>2015-10-19</updated><authors><author><keyname>Monter</keyname><forenames>Sergio A. Almada</forenames></author><author><keyname>Budhiraja</keyname><forenames>Amarjit</forenames></author><author><keyname>Hannig</keyname><forenames>Jan</forenames></author></authors><title>Source detection algorithms for dynamic contaminants based on the
  analysis of a hydrodynamic limit</title><categories>math.PR cs.CE cs.MA math.NA math.OC</categories><msc-class>60G35, 65C35, 86A22, 93E10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose and numerically analyze an algorithm for detection of
a contaminant source using a dynamic sensor network. The algorithm is motivated
using a global probabilistic optimization problem and is based on the analysis
of the hydrodynamic limit of a discrete time evolution equation on the lattice
under a suitable scaling of time and space. Numerical results illustrating the
effectiveness of the algorithm are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05198</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05198</id><created>2015-10-18</created><updated>2015-10-30</updated><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Ritter</keyname><forenames>Alan</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>Learning multi-faceted representations of individuals from heterogeneous
  evidence using neural networks</title><categories>cs.SI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inferring latent attributes of people online is an important social computing
task, but requires integrating the many heterogeneous sources of information
available on the web. We propose to learn individual representations of people
using neural nets to integrate information from social media. The algorithm is
able to combine any kind of cues, such as the text a person writes, the
person's attributes (e.g. gender, employer, school, location) and social
relations to other people (e.g., friendship, marriage), using global inference
to infer missing attributes from noisy cues. The resulting latent
representations capture homophily: people who have similar attributes, are
related socially, or write similar text are closer in vector space. We show
that these learned representations offer good performance at solving four
important tasks in social media inference on Twitter: predicting (1) gender,
(2) occupation, (3) location, and (4) friendships for users, and that we
achieve the best performance by integrating all these signals. Our approach
scales to large datasets, using parallel stochastic gradient descent for
learning. The resulting representations can be used as general features in and
have the potential to benefit a large number of downstream tasks like link
prediction, community detection, or reasoning over social networks, discovering
for example the high probability that a New York City resident is a fan of the
New York Knicks, or the greater preference for iPhones by computer
professionals than legal professionals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05201</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05201</id><created>2015-10-18</created><authors><author><keyname>Li</keyname><forenames>Yangjia</forenames></author><author><keyname>Lu</keyname><forenames>Hui</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author><author><keyname>Chen</keyname><forenames>Mingshuai</forenames></author><author><keyname>Wu</keyname><forenames>Guohua</forenames></author></authors><title>Termination Analysis of Polynomial Programs with Equality Conditions</title><categories>cs.PL cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the termination problem of a family of
polynomial programs, in which all assignments to program variables are
polynomials, and test conditions of loops and conditional statements are
polynomial equations. Our main result is that the non-terminating inputs of
such a polynomial program is algorithmically computable according to a strictly
descending chain of algebraic sets, which implies that the termination problem
of these programs is decidable. Moreover, the complexity of the algorithm
follows immediately from the length of the chain, which can be bounded using
Hilbert's function and Macaulay's theorem. To the best of our knowledge, the
family of polynomial programs under consideration should be the largest one
with a decidable termination problem so far. The experimental results indicate
the efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05203</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05203</id><created>2015-10-18</created><authors><author><keyname>Neubig</keyname><forenames>Graham</forenames></author><author><keyname>Morishita</keyname><forenames>Makoto</forenames></author><author><keyname>Nakamura</keyname><forenames>Satoshi</forenames></author></authors><title>Neural Reranking Improves Subjective Quality of Machine Translation:
  NAIST at WAT2015</title><categories>cs.CL</categories><comments>7 pages, 1 figure</comments><journal-ref>Proceedings of the 2nd Workshop on Asian Translation (WAT), pp.
  35-41, 2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This year, the Nara Institute of Science and Technology (NAIST)'s submission
to the 2015 Workshop on Asian Translation was based on syntax-based statistical
machine translation, with the addition of a reranking component using neural
attentional machine translation models. Experiments re-confirmed results from
previous work stating that neural MT reranking provides a large gain in
objective evaluation measures such as BLEU, and also confirmed for the first
time that these results also carry over to manual evaluation. We further
perform a detailed analysis of reasons for this increase, finding that the main
contributions of the neural models lie in improvement of the grammatical
correctness of the output, as opposed to improvements in lexical choice of
content words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05205</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05205</id><created>2015-10-18</created><authors><author><keyname>Liu</keyname><forenames>An</forenames></author><author><keyname>Lau</keyname><forenames>Vincent</forenames></author></authors><title>Asymptotic Scaling Laws of Wireless Adhoc Network with Physical Layer
  Caching</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages, 12 figures, accepted by IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a physical layer (PHY) caching scheme for wireless adhoc networks.
The PHY caching exploits cache-assisted multihop gain and cache-induced
dual-layer CoMP gain, which substantially improves the throughput of wireless
adhoc networks. In particular, the PHY caching scheme contains a novel PHY
transmission mode called the cache-induced dual-layer CoMP which can support
homogeneous opportunistic CoMP in the wireless adhoc network. Compared with
traditional per-node throughput scaling results of
\Theta\left(1/\sqrt{N}\right), we can achieve O(1) per node throughput for a
cached wireless adhoc network with N nodes. Moreover, we analyze the throughput
of the PHY caching scheme for regular wireless adhoc networks and study the
impact of various system parameters on the PHY caching gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05209</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05209</id><created>2015-10-18</created><updated>2015-11-01</updated><authors><author><keyname>Hirota</keyname><forenames>Osamu</forenames></author></authors><title>Towards Quantum Enigma Cipher</title><categories>quant-ph cs.CR</categories><comments>Typos were corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research note suggests a new way to realize a high speed direct
encryption based on quantum detection theory. The conventional cipher is
designed by a mathematical algorithm and its security is evaluated by the
complexity of the algorithm for cryptanalysis and ability of computers. This
kind of cipher cannot exceed the Shannon limit of cryptography,and it can be
decrypted with probability one in principle by trying all the possible keys
against the data length equal to the secret key length. A cipher with quantum
effect in physical layer may exceed the Shannon limit of cryptography. The
quantum stream cipher by $\alpha/\eta$ or Yuen-2000 protocol (Y-00) which
operates at Gbit/sec is a typical example of such a cipher. That is, ciphertext
of mathematical cipher with a secret key is masked by quantum noise of laser
light when an eavesdropper observes optical signals as a ciphertext of the
mathematical cipher, while the legitimate receiver does not suffer the quantum
noise effect. As a result, the inherent difference of accuracy of ciphertext
between eavesdropper and legitimate receiver arises. This is a necessary
condition to exceed the Shannon limit of cryptography. In this note, we present
a new method to generate an inherent difference of accuracy of the ciphertext,
taking into account a fundamental properties of quantum detection schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05214</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05214</id><created>2015-10-18</created><authors><author><keyname>Hope</keyname><forenames>Tom</forenames></author><author><keyname>Wagner</keyname><forenames>Avishai</forenames></author><author><keyname>Zuk</keyname><forenames>Or</forenames></author></authors><title>Clustering Noisy Signals with Structured Sparsity Using Time-Frequency
  Representation</title><categories>cs.LG stat.ML</categories><msc-class>62H30, 65T60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple and efficient time-series clustering framework
particularly suited for low Signal-to-Noise Ratio (SNR), by simultaneous
smoothing and dimensionality reduction aimed at preserving clustering
information. We extend the sparse K-means algorithm by incorporating structured
sparsity, and use it to exploit the multi-scale property of wavelets and group
structure in multivariate signals. Finally, we extract features invariant to
translation and scaling with the scattering transform, which corresponds to a
convolutional network with filters given by a wavelet operator, and use the
network's structure in sparse clustering. By promoting sparsity, this transform
can yield a low-dimensional representation of signals that gives improved
clustering results on several real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05216</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05216</id><created>2015-10-18</created><updated>2016-02-04</updated><authors><author><keyname>Rompf</keyname><forenames>Tiark</forenames></author><author><keyname>Amin</keyname><forenames>Nada</forenames></author></authors><title>From F to DOT: Type Soundness Proofs with Definitional Interpreters</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scala's type system unifies ML modules, object-oriented, and functional
programming. The Dependent Object Types (DOT) family of calculi has been
proposed as a new foundation for Scala and similar languages. Unfortunately, it
is not clear how DOT relates to any well-known type systems, and type soundness
has only been established for very restricted subsets. In fact, important Scala
features are known to break at least one key metatheoretic property such as
environment narrowing or subtyping transitivity, which are usually required for
a type soundness proof.
  First, and, perhaps surprisingly, we show how rich DOT calculi can still be
proved sound. The key insight is that narrowing and subtyping transitivity only
need to hold for runtime objects, but not for code that is never executed.
Alas, the dominant method of proving type soundness, Wright and Felleisen's
syntactic approach, is based on term rewriting, which does not a priori make a
distinction between runtime and type assignment time.
  Second, we demonstrate how type soundness can be proved for advanced,
polymorphic, type systems with respect to high-level, definitional
interpreters, implemented in Coq. We present the first mechanized soundness
proof in this style for System F&lt;: and several extensions, including mutable
references. Our proofs use only simple induction: another surprising result, as
the combination of big-step semantics, mutable references, and polymorphism is
commonly believed to require co-inductive proof techniques.
  Third, we show how DOT-like calculi emerge as generalizations of F&lt;:,
exposing a rich design space of calculi with path-dependent types which we
collectively call System D. Armed with insights from the definitional
interpreter semantics, we also show how equivalent small-step semantics and
soundness proofs in Wright-Felleisen-style can be derived for these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05217</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05217</id><created>2015-10-18</created><updated>2016-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Weiran</forenames></author><author><keyname>Li</keyname><forenames>Liang</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Partitioned Sampling of Public Opinions Based on Their Social Dynamics</title><categories>cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public opinion polling is typically done by random sampling from the entire
population, treating the opinions of individuals as independent. In the real
world, individuals' opinions are often correlated, especially among friends in
a social network, due to the effect of both homophily and social influence. In
this paper, we explore the idea of partitioned sampling, which partition
individuals likely holding similar opinions into groups and sample every group
separately to obtain an accurate estimate of the population opinion. We first
rigorously formulate the above idea into an optimization problem. In
particular, we characterize individuals' opinions as random variables, specify
the objective as minimizing the expected sample variance of the estimated
result, and precisely define the statistical measure of pairwise opinion
similarity, which by our analysis is enough to fully determine the solution of
the optimization problem. We show that the simple partitions that contain only
one sample in each group are always better, and reduce finding the optimal
simple partition to a well-studied Max-k-Cut problem. We adopt the
semi-definite programming algorithm for Max-k-Cut to solve our optimization
problem, and further develop a greedy heuristic to improve efficiency.
Moreover, to address the issue of how to obtain opinion similarity efficiently,
we propose an opinion evolution model based on reasonable user interaction
patterns in social network, and provide efficient and exact computation of
opinion similarity in the model. We use both synthetic and real-world datasets
to demonstrate that our partitioned sampling method results in significant
improvement in sampling quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05218</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05218</id><created>2015-10-18</created><authors><author><keyname>Malas</keyname><forenames>Tareq M.</forenames></author><author><keyname>Hornich</keyname><forenames>Julian</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author><author><keyname>Pflaum</keyname><forenames>Christoph</forenames></author><author><keyname>Keyes</keyname><forenames>David E.</forenames></author></authors><title>Optimization of an electromagnetics code with multicore wavefront
  diamond blocking and multi-dimensional intra-tile parallelization</title><categories>cs.CE cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding and optimizing the properties of solar cells is becoming a key
issue in the search for alternatives to nuclear and fossil energy sources. A
theoretical analysis via numerical simulations involves solving Maxwell's
Equations in discretized form and typically requires substantial computing
effort. We start from a hybrid-parallel (MPI+OpenMP) production code that
implements the Time Harmonic Inverse Iteration Method (THIIM) with
Finite-Difference Frequency Domain (FDFD) discretization. Although this
algorithm has the characteristics of a strongly bandwidth-bound stencil update
scheme, it is significantly different from the popular stencil types that have
been exhaustively studied in the high performance computing literature to date.
We apply a recently developed stencil optimization technique, multicore
wavefront diamond tiling with multi-dimensional cache block sharing, and
describe in detail the peculiarities that need to be considered due to the
special stencil structure. Concurrency in updating the components of the
electric and magnetic fields provides an additional level of parallelism. The
dependence of the cache size requirement of the optimized code on the blocking
parameters is modeled accurately, and an auto-tuner searches for optimal
configurations in the remaining parameter space. We were able to completely
decouple the execution from the memory bandwidth bottleneck, accelerating the
implementation by a factor of three to four compared to an optimal
implementation with pure spatial blocking on an 18-core Intel Haswell CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05229</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05229</id><created>2015-10-18</created><updated>2015-12-23</updated><authors><author><keyname>Sziklai</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Segal-Halevi</keyname><forenames>Erel</forenames></author></authors><title>Resource-monotonicity and Population-monotonicity in Cake-cutting</title><categories>cs.GT</categories><comments>Updates from previous version: replaced the whole-cake-divided axiom
  with the weak-Pareto-optimality axiom; replaced the connected-pieces axiom
  with the connected-utilities assumption</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the monotonicity properties of solutions in the classic problem of
fair cake-cutting - dividing a single heterogeneous resource among agents with
different preferences. Resource- and population-monotonicity relate to
scenarios where the cake, or the number of participants who divide the cake,
changes. It is required that the utility of all participants change in the same
direction: either all of them are better-off (if there is more to share) or all
are worse-off (if there is less to share). We formally introduce these concepts
to the cake-cutting setting and present a meticulous axiomatic analysis. We
show that classic fair cake-cutting protocols, like the Cut and Choose,
Banach-Knaster, Dubins-Spanier and many other fail to be monotonic. We then
present monotonic division rules in two utility models. When the utility
functions are additive, there are several proportional and Pareto-optimal
division rules that satisfy one or both monotonicity axioms. In contrast, when
the utility functions have a connectivity constraint such as in land division,
no proportional and Pareto-optimal rule can satisfy any monotonicity axiom.
Assuming weak-Pareto-optimality (no other allocation is better for all agents),
we provide a proportional resource-monotonic protocol for two agents and some
population-monotonic rules for $n$ agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05231</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05231</id><created>2015-10-18</created><authors><author><keyname>Lahlou</keyname><forenames>Tarek A.</forenames></author><author><keyname>Baran</keyname><forenames>Thomas A.</forenames></author></authors><title>Signal Processing Structures for Solving Conservative Constraint
  Satisfaction Problems</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This primary purpose of this paper is to succinctly state a number of
verifiable and tractable sufficient conditions under which a particular class
of conservative signal processing structures may be readily used to solve a
companion class of constraint satisfaction problems using both synchronous and
asynchronous implementation protocols. In particular, the mentioned class of
structures is shown to have desirable convergence and robustness properties
with respect to various uncertainties involving communication and processing
delays. Essential ingredients to the arguments herein involve blending together
functional composition methods, conservation principles, asynchronous signal
processing implementation protocols, and methods of homotopy. Numerical
experiments complement the theoretical presentation and connections to
optimization theory are made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05237</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05237</id><created>2015-10-18</created><authors><author><keyname>Gavin</keyname><forenames>Brendan</forenames></author><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>Large Enforced Sparse Non-Negative Matrix Factorization</title><categories>cs.LG cs.NA cs.SI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative matrix factorization (NMF) is a common method for generating
topic models from text data. NMF is widely accepted for producing good results
despite its relative simplicity of implementation and ease of computation. One
challenge with applying NMF to large datasets is that intermediate matrix
products often become dense, stressing the memory and compute elements of a
system. In this article, we investigate a simple but powerful modification of a
common NMF algorithm that enforces the generation of sparse intermediate and
output matrices. This method enables the application of NMF to large datasets
through improved memory and compute performance. Further, we demonstrate
empirically that this method of enforcing sparsity in the NMF either preserves
or improves both the accuracy of the resulting topic model and the convergence
rate of the underlying algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05245</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05245</id><created>2015-10-18</created><updated>2015-11-02</updated><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Brod</keyname><forenames>Daniel J.</forenames></author></authors><title>BosonSampling with Lost Photons</title><categories>quant-ph cs.CC</categories><comments>12 pages. v2: extended concluding section</comments><journal-ref>Phys. Rev. A 93, 012335 (2016)</journal-ref><doi>10.1103/PhysRevA.93.012335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BosonSampling is an intermediate model of quantum computation where
linear-optical networks are used to solve sampling problems expected to be hard
for classical computers. Since these devices are not expected to be universal
for quantum computation, it remains an open question of whether any
error-correction techniques can be applied to them, and thus it is important to
investigate how robust the model is under natural experimental imperfections,
such as losses and imperfect control of parameters. Here we investigate the
complexity of BosonSampling under photon losses---more specifically, the case
where an unknown subset of the photons are randomly lost at the sources. We
show that, if $k$ out of $n$ photons are lost, then we cannot sample
classically from a distribution that is $1/n^{\Theta(k)}$-close (in total
variation distance) to the ideal distribution, unless a
$\text{BPP}^{\text{NP}}$ machine can estimate the permanents of Gaussian
matrices in $n^{O(k)}$ time. In particular, if $k$ is constant, this implies
that simulating lossy BosonSampling is hard for a classical computer, under
exactly the same complexity assumption used for the original lossless case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05252</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05252</id><created>2015-10-18</created><authors><author><keyname>Shariati</keyname><forenames>Nafiseh</forenames></author><author><keyname>Zachariah</keyname><forenames>Dave</forenames></author><author><keyname>Karlsson</keyname><forenames>Johan</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Robust Optimal Power Distribution for Hyperthermia Cancer Treatment</title><categories>cs.IT math.IT</categories><comments>7 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an optimization problem for spatial power distribution generated
by an array of transmitting elements. Using ultrasound hyperthermia cancer
treatment as a motivating example, the signal design problem consists of
optimizing the power distribution across the tumor and healthy tissue regions,
respectively. The models used in the optimization problem are, however,
invariably subject to errors. deposition as well as inefficient treatment. To
combat such unknown model errors, we formulate a robust signal design framework
that can take the uncertainty into account using a worst-case approach. This
leads to a semi-infinite programming (SIP) robust design problem which we
reformulate as a tractable convex problem, potentially has a wider range of
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05263</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05263</id><created>2015-10-18</created><authors><author><keyname>Lo</keyname><forenames>Yung-Yin</forenames></author><author><keyname>Liao</keyname><forenames>Wanjiun</forenames></author><author><keyname>Chang</keyname><forenames>Cheng-Shang</forenames></author></authors><title>Temporal Matrix Factorization for Tracking Concept Drift in Individual
  User Preferences</title><categories>cs.SI</categories><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The matrix factorization (MF) technique has been widely adopted for solving
the rating prediction problem in recommender systems. The MF technique utilizes
the latent factor model to obtain static user preferences (user latent vectors)
and item characteristics (item latent vectors) based on historical rating data.
However, in the real world user preferences are not static but full of
dynamics. Though there are several previous works that addressed this time
varying issue of user preferences, it seems (to the best of our knowledge) that
none of them is specifically designed for tracking concept drift in individual
user preferences. Motivated by this, we develop a Temporal Matrix Factorization
approach (TMF) for tracking concept drift in each individual user latent
vector. There are two key innovative steps in our approach: (i) we develop a
modified stochastic gradient descent method to learn an individual user latent
vector at each time step, and (ii) by the Lasso regression we learn a linear
model for the transition of the individual user latent vectors. We test our
method on a synthetic dataset and several real datasets. In comparison with the
original MF, our experimental results show that our temporal method is able to
achieve lower root mean square errors (RMSE) for both the synthetic and real
datasets. One interesting finding is that the performance gain in RMSE is
mostly from those users who indeed have concept drift in their user latent
vectors at the time of prediction. In particular, for the synthetic dataset and
the Ciao dataset, there are quite a few users with that property and the
performance gains for these two datasets are roughly 20% and 5%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05266</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05266</id><created>2015-10-18</created><authors><author><keyname>Ronda-Pupo</keyname><forenames>Guillermo Armando</forenames></author><author><keyname>Katz</keyname><forenames>J. Sylvan</forenames></author></authors><title>The Scaling Relationship between Citation-Based Performance and
  Scientific Collaboration in Natural Sciences</title><categories>cs.DL</categories><comments>14 pages, 2 figures, 5 tables</comments><msc-class>Cs, DL</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to extend our knowledge about the power-law
relationship between citation-based performance and collaboration patterns for
papers of the Natural Sciences domain. We analyzed 829,924 articles that
received 16,490,346 citations. The number of articles published through
collaboration account for 89%. The citation-based performance and collaboration
patterns exhibit a power-law correlation with a scaling exponent of 1.20,
SD=0.07. We found that the Matthew effect is stronger for collaborated papers
than for single-authored. This means that the citations to a field research
areas articles increase 2.30 times each time it doubles the number of
collaborative papers. The scaling exponent for the power-law relationship for
single-authored papers was 0.85, SD=0.11. The citations to a field research
area single-authored articles increase 1.89 times each time the research area
doubles the number of non-collaborative papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05270</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05270</id><created>2015-10-18</created><authors><author><keyname>Oo</keyname><forenames>May Zin</forenames></author><author><keyname>Othman</keyname><forenames>Mazliza</forenames></author><author><keyname>Farrell</keyname><forenames>Timothy O</forenames></author></authors><title>A Proxy Acknowledgement Mechanism for TCP Variants in Mobile Ad Hoc
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequence number checking technique is proposed to improve the performance
of TCP connections in mobile ad hoc networks. While a TCP connection is
initialized, a routing protocol takes the responsibility for checking the hop
count between a source and destination pair. If the hop count is greater than a
predefined value, the routing protocol decides to use a proxy node. The
responsibility of a proxy node is to check the correctness of data packets and
inform the missing packets by sending an acknowledgement from a proxy node to
the source node. By doing so, the source node is able to retransmit any missing
packet in advance without waiting until an end-to-end acknowledgement is
received from the destination. Simulation results show that the proposed
mechanism is able to increase throughput up to 55% in static network and
decrease routing overhead up to 95% in mobile network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05275</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05275</id><created>2015-10-18</created><authors><author><keyname>Li</keyname><forenames>Hongmin</forenames></author><author><keyname>Jing</keyname><forenames>Pei</forenames></author><author><keyname>Li</keyname><forenames>Guoqi</forenames></author></authors><title>Real-time Tracking Based on Neuromrophic Vision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-time tracking is an important problem in computer vision in which most
methods are based on the conventional cameras. Neuromorphic vision is a concept
defined by incorporating neuromorphic vision sensors such as silicon retinas in
vision processing system. With the development of the silicon technology,
asynchronous event-based silicon retinas that mimic neuro-biological
architectures has been developed in recent years. In this work, we combine the
vision tracking algorithm of computer vision with the information encoding
mechanism of event-based sensors which is inspired from the neural rate coding
mechanism. The real-time tracking of single object with the advantage of high
speed of 100 time bins per second is successfully realized. Our method
demonstrates that the computer vision methods could be used for the
neuromorphic vision processing and we can realize fast real-time tracking using
neuromorphic vision sensors compare to the conventional camera.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05278</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05278</id><created>2015-10-18</created><updated>2016-03-02</updated><authors><author><keyname>Breuer</keyname><forenames>Peter</forenames></author><author><keyname>Bowen</keyname><forenames>Jonathan</forenames></author></authors><title>A First Practical Fully Homomorphic Crypto-Processor Design: The Secret
  Computer is Nearly Here</title><categories>cs.CR</categories><comments>6 pages, draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following a sequence of hardware designs for a fully homomorphic
crypto-processor - a general purpose processor that natively runs encrypted
machine code on encrypted data in registers and memory, resulting in encrypted
machine states - proposed by the authors in 2014, we discuss a working
prototype of the first of those, a so-called `pseudo-homomorphic' design. This
processor is in principle safe against physical or software-based attacks by
the owner/operator of the processor on user processes running in it. The
processor is intended as a more secure option for those emerging computing
paradigms that require trust to be placed in computations carried out in remote
locations or overseen by untrusted operators.
  The prototype has a single-pipeline superscalar architecture that runs
OpenRISC standard machine code in two distinct modes. The processor runs in the
encrypted mode (the unprivileged, `user' mode, with a long pipeline) at 60-70%
of the speed in the unencrypted mode (the privileged, `supervisor' mode, with a
short pipeline), emitting a completed encrypted instruction every 1.67-1.8
cycles on average in real trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05301</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05301</id><created>2015-10-18</created><authors><author><keyname>Isah</keyname><forenames>Haruna</forenames></author><author><keyname>Neagu</keyname><forenames>Daniel</forenames></author><author><keyname>Trundle</keyname><forenames>Paul</forenames></author></authors><title>Social Media Analysis for Product Safety using Text Mining and Sentiment
  Analysis</title><categories>cs.SI cs.IR</categories><comments>2014 14th UK Workshop on Computational Intelligence (UKCI)</comments><doi>10.1109/UKCI.2014.6930158</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing incidents of counterfeiting and associated economic and health
consequences necessitate the development of active surveillance systems capable
of producing timely and reliable information for all stake holders in the
anti-counterfeiting fight. User generated content from social media platforms
can provide early clues about product allergies, adverse events and product
counterfeiting. This paper reports a work in progresswith contributions
including: the development of a framework for gathering and analyzing the views
and experiences of users of drug and cosmetic products using machine learning,
text mining and sentiment analysis, the application of the proposed framework
on Facebook comments and data from Twitter for brand analysis, and the
description of how to develop a product safety lexicon and training data for
modeling a machine learning classifier for drug and cosmetic product sentiment
prediction. The initial brand and product comparison results signify the
usefulness of text mining and sentiment analysis on social media data while the
use of machine learning classifier for predicting the sentiment orientation
provides a useful tool for users, product manufacturers, regulatory and
enforcement agencies to monitor brand or product sentiment trends in order to
act in the event of sudden or significant rise in negative sentiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05311</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05311</id><created>2015-10-18</created><authors><author><keyname>Cohen</keyname><forenames>Rami</forenames></author><author><keyname>Cassuto</keyname><forenames>Yuval</forenames></author></authors><title>Iterative Decoding of LDPC Codes over the q-ary Partial Erasure Channel</title><categories>cs.IT math.IT</categories><comments>26 pages, submitted to the IEEE transactions on Information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new channel model, which we name the q-ary
partial erasure channel (QPEC). The QPEC has a q-ary input, and its output is
either one symbol or a set of M symbols, where M may be smaller than q. This
channel serves as a generalization to the binary erasure channel (BEC), and
mimics situations when a symbol output from the channel is known only
partially; that is, the output symbol contains some ambiguity, but is not fully
erased. This type of channel is motivated by non-volatile memory multi-level
read channels. In such channels the readout is obtained by a sequence of
current/voltage measurements, which may terminate with partial knowledge of the
stored level. Our investigation is concentrated on the performance of
low-density parity-check (LDPC) codes when used over this channel, thanks to
their low decoding complexity using belief propagation. We provide the
density-evolution equations that describe the decoding process, and suggest
several bounds and approximations. In addition, we provide tools for the
practical design of LDPC codes for use over the QPEC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05318</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05318</id><created>2015-10-18</created><authors><author><keyname>Cho</keyname><forenames>Yoon-Sik</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Latent Space Model for Multi-Modal Social Data</title><categories>cs.SI cs.LG physics.data-an physics.soc-ph</categories><comments>12 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of social networking services, researchers enjoy the
increasing availability of large-scale heterogenous datasets capturing online
user interactions and behaviors. Traditional analysis of techno-social systems
data has focused mainly on describing either the dynamics of social
interactions, or the attributes and behaviors of the users. However,
overwhelming empirical evidence suggests that the two dimensions affect one
another, and therefore they should be jointly modeled and analyzed in a
multi-modal framework. The benefits of such an approach include the ability to
build better predictive models, leveraging social network information as well
as user behavioral signals. To this purpose, here we propose the Constrained
Latent Space Model (CLSM), a generalized framework that combines Mixed
Membership Stochastic Blockmodels (MMSB) and Latent Dirichlet Allocation (LDA)
incorporating a constraint that forces the latent space to concurrently
describe the multiple data modalities. We derive an efficient inference
algorithm based on Variational Expectation Maximization that has a
computational cost linear in the size of the network, thus making it feasible
to analyze massive social datasets. We validate the proposed framework on two
problems: prediction of social interactions from user attributes and behaviors,
and behavior prediction exploiting network information. We perform experiments
with a variety of multi-modal social systems, spanning location-based social
networks (Gowalla), social media services (Instagram, Orkut), e-commerce and
review sites (Amazon, Ciao), and finally citation networks (Cora). The results
indicate significant improvement in prediction accuracy over state of the art
methods, and demonstrate the flexibility of the proposed approach for
addressing a variety of different learning problems commonly occurring with
multi-modal social data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05324</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05324</id><created>2015-10-18</created><authors><author><keyname>Xu</keyname><forenames>S.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Dynamic Topology Adaptation Based on Adaptive Link Selection Algorithms
  for Distributed Estimation</title><categories>cs.SY cs.IT math.IT</categories><comments>14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents adaptive link selection algorithms for distributed
estimation and considers their application to wireless sensor networks and
smart grids. In particular, exhaustive search--based
least--mean--squares(LMS)/recursive least squares(RLS) link selection
algorithms and sparsity--inspired LMS/RLS link selection algorithms that can
exploit the topology of networks with poor--quality links are considered. The
proposed link selection algorithms are then analyzed in terms of their
stability, steady--state and tracking performance, and computational
complexity. In comparison with existing centralized or distributed estimation
strategies, key features of the proposed algorithms are: 1) more accurate
estimates and faster convergence speed can be obtained; and 2) the network is
equipped with the ability of link selection that can circumvent link failures
and improve the estimation performance. The performance of the proposed
algorithms for distributed estimation is illustrated via simulations in
applications of wireless sensor networks and smart grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05326</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05326</id><created>2015-10-18</created><authors><author><keyname>Lewitzka</keyname><forenames>Steffen</forenames></author></authors><title>Combining intermediate propositional logics with classical logic</title><categories>cs.LO</categories><comments>18 pages</comments><report-no>TR-PGCOMP-001/2015. Technical Report. Computer Science Graduate
  Program. Federal University of Bahia</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [17], we introduced a modal logic, called $L$, which combines
intuitionistic propositional logic $IPC$ and classical propositional logic
$CPC$ and is complete w.r.t. an algebraic semantics. However, $L$ seems to be
too weak for Kripke-style semantics. In this paper, we add positive and
negative introspection and show that the resulting logic $L5$ has a Kripke
semantics. For intermediate logics $I$, we consider the parametrized versions
$L5(I)$ of $L5$ where $IPC$ is replaced by $I$. $L5(I)$ can be seen as a
classical modal logic for the reasoning about truth in $I$. From our results,
we derive a simple method for determining algebraic and Kripke semantics for
some specific intermediate logics. We discuss some examples which are of
interest for Computer Science, namely the Logic of Here-and-There,
G\&quot;odel-Dummett Logic and Jankov Logic. Our method provides new proofs of
completeness theorems due to Hosoi, Dummett/Horn and Jankov, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05328</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05328</id><created>2015-10-18</created><updated>2015-11-22</updated><authors><author><keyname>Tabacof</keyname><forenames>Pedro</forenames></author><author><keyname>Valle</keyname><forenames>Eduardo</forenames></author></authors><title>Exploring the Space of Adversarial Images</title><categories>cs.NE</categories><comments>Under review for ICLR 2016; EDIT: Changed abstract and introduction
  to better reflect available literature; EDIT2: Updated abstract to better
  explain findings, added convolutional network experiments, moved algorithms
  section to appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We probe the pixel space of adversarial images using noise of varying
intensity and statistic. We bring novel visualizations that showcase the
phenomenon and its high variability. We show that adversarial images appear in
large regions in the pixel space, but that, for the same task, a shallow
classifier seems more robust to adversarial images than a deep convolutional
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05336</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05336</id><created>2015-10-18</created><authors><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author></authors><title>Clustering is Easy When ....What?</title><categories>stat.ML cs.LG</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that most of the common clustering objectives are NP-hard to
optimize. In practice, however, clustering is being routinely carried out. One
approach for providing theoretical understanding of this seeming discrepancy is
to come up with notions of clusterability that distinguish realistically
interesting input data from worst-case data sets. The hope is that there will
be clustering algorithms that are provably efficient on such &quot;clusterable&quot;
instances. This paper addresses the thesis that the computational hardness of
clustering tasks goes away for inputs that one really cares about. In other
words, that &quot;Clustering is difficult only when it does not matter&quot; (the
\emph{CDNM thesis} for short).
  I wish to present a a critical bird's eye overview of the results published
on this issue so far and to call attention to the gap between available and
desirable results on this issue. A longer, more detailed version of this note
is available as arXiv:1507.05307.
  I discuss which requirements should be met in order to provide formal support
to the the CDNM thesis and then examine existing results in view of these
requirements and list some significant unsolved research challenges in that
direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05338</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05338</id><created>2015-10-18</created><authors><author><keyname>Malekshan</keyname><forenames>Kamal Rahimi</forenames></author><author><keyname>Zhuang</keyname><forenames>Weihua</forenames></author><author><keyname>Lostanlen</keyname><forenames>Yves</forenames></author></authors><title>Coordination-based Medium Access Control with Space-reservation for
  Wireless Ad Hoc Networks</title><categories>cs.NI</categories><comments>12 pages, 12 figures, IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient radio spectrum utilization and low energy consumption in mobile
devices are essential in developing next generation wireless networks. This
paper presents a new medium access control (MAC) mechanism to enhance spectrum
efficiency and reduce energy consumption in a wireless ad hoc network. A set of
coordinator nodes, distributed in the network area, periodically schedule
contention-free time slots for all data transmissions/receptions in the
network, based on transmission requests from source nodes. Adjacent
coordinators exchange scheduling information to effectively increase spatial
spectrum reuse and avoid transmission collisions. Moreover, the proposed MAC
scheme allows a node to put its radio interface into a sleep mode when it is
not transmitting/receiving a packet, in order to reduce energy consumption.
Simulation results demonstrate that the proposed scheme achieves substantially
higher throughput and has significantly lower energy consumption in comparison
with existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05344</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05344</id><created>2015-10-18</created><updated>2016-02-22</updated><authors><author><keyname>Ha</keyname><forenames>Jung-Su</forenames></author><author><keyname>Choi</keyname><forenames>Han-Lim</forenames></author></authors><title>A Topology-Guided Path Integral Approach for Stochastic Optimal Control</title><categories>cs.SY cs.RO</categories><comments>8 pages, 4 figures, accepted to IEEE International Conference on
  Robotics and Automation (ICRA) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an efficient method to solve a class of continuous-time,
continuous-space stochastic optimal control problems of robot motion in a
cluttered environment. The method builds upon a path integral representation of
the stochastic optimal control problem that allows computation of the optimal
solution through sampling and estimation process. As this sampling process
often leads to a local minimum especially when the state space is highly
non-convex due to the obstacle field, we present an efficient method to
alleviate this issue by devising a proposed topological motion planning
algorithm. Combined with a receding-horizon scheme in execution of the optimal
control solution, the proposed method can generate a dynamically feasible and
collision-free trajectory while reducing concern about local optima.
Illustrative numerical examples are presented to demonstrate the applicability
and validity of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05355</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05355</id><created>2015-10-19</created><authors><author><keyname>Heng</keyname><forenames>Ziling</forenames></author><author><keyname>Yue</keyname><forenames>Qin</forenames></author></authors><title>Several classes of cyclic codes with either optimal three weights or a
  few weights</title><categories>cs.IT math.IT</categories><comments>24 pages</comments><msc-class>11T71, 11T55, 12E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes with a few weights are very useful in the design of frequency
hopping sequences and the development of secret sharing schemes. In this paper,
we mainly use Gauss sums to represent the Hamming weights of a general
construction of cyclic codes. As applications, we obtain a class of optimal
three-weight codes achieving the Griesmer bound, which generalizes a Vega's
result in \cite{V1}, and several classes of cyclic codes with only a few
weights, which solve the open problem in \cite{V1}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05363</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05363</id><created>2015-10-19</created><authors><author><keyname>Sharma</keyname><forenames>Rohini</forenames></author><author><keyname>Loboyal</keyname><forenames>D. K.</forenames></author></authors><title>Region Based Energy Balanced Inter-cluster communication Protocol for
  Sensor networks</title><categories>cs.NI</categories><comments>NATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND INFORMATION
  PROCESSING (NCCCIP-2015) ISBN: 978-93-84935-27-6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks faces unbalanced energy consumption problem over
time. Clustering provides an energy efficient method to improve lifespan of the
sensor network. Cluster head collects data from other nodes and transmits it
towards the sink node. Cluster heads which are far-off from the sink, consumes
more power in transmission of information towards the sink. We propose Region
Based Energy Balanced Inter-cluster communication protocol (RBEBP) to improve
lifespan of the sensor network. Monitored area has been divided into regions;
cluster heads are selected from specific region based on the residual energy of
nodes in that region. If energy of nodes of the specific region is low, nodes
from another region are selected as cluster heads. Optimized selection of
cluster heads helps in improving lifespan of the sensor network. In our scheme,
cluster heads which are far-off from the sink use another cluster heads as the
relay nodes to transmit their data to the sink node. So energy of cluster heads
deplete in a uniform way and complete area remain covered by sensor nodes.
Simulation results demonstrate that RBEBP can effectively reduce total energy
depletion and considerably extend lifespan of the network as compared to LEACH
protocol. RBEBP also minimize the problem of energy holes in monitored area and
improve the throughput of the network
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05373</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05373</id><created>2015-10-19</created><authors><author><keyname>Thimm</keyname><forenames>Matthias</forenames></author><author><keyname>Villata</keyname><forenames>Serena</forenames></author></authors><title>System Descriptions of the First International Competition on
  Computational Models of Argumentation (ICCMA'15)</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the system description of the 18 solvers submitted to
the First International Competition on Computational Models of Argumentation
(ICCMA'15) and therefore gives an overview on state-of-the-art of computational
approaches to abstract argumentation problems. Further information on the
results of the competition and the performance of the individual solvers can be
found on at http://argumentationcompetition.org/2015/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05390</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05390</id><created>2015-10-19</created><updated>2015-11-26</updated><authors><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author></authors><title>Entropy and thinning of discrete random variables</title><categories>math.PR cs.IT math.IT</categories><comments>Draft for Proceedings of IMA theme year in Discrete Structures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe five types of results concerning information and concentration of
discrete random variables, and relationships between them. These results are
motivated by their counterparts in the continuous case. The results we consider
are information theoretic approaches to Poisson approximation, the maximum
entropy property of the Poisson distribution, discrete concentration (Poincare
and logarithmic Sobolev) inequalities, monotonicity of entropy and concavity of
entropy in the Shepp-Olkin regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05405</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05405</id><created>2015-10-19</created><authors><author><keyname>Sarkis</keyname><forenames>Mira</forenames><affiliation>LTCI</affiliation></author><author><keyname>Concolato</keyname><forenames>Cyril</forenames><affiliation>LTCI</affiliation></author><author><keyname>Dufourd</keyname><forenames>Jean-Claude</forenames><affiliation>LTCI</affiliation></author></authors><title>The Virtual Splitter: Refactoring Web Applications for the Multiscreen
  Environment</title><categories>cs.MM</categories><proxy>ccsd</proxy><journal-ref>DocEng'14: ACM Symposium on Document Engineering, ACM, 2014,
  pp.Pages139-142 \&amp;lt;10.1145/2644866.2644893\&amp;gt;</journal-ref><doi>10.1145/2644866.2644893</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creating web applications for the multiscreen environment is still a
challenge. One approach is to transform existing single-screen applications but
this has not been done yet automatically or generically. This paper proposes a
refactor-ing system. It consists of a generic and extensible mapping phase that
automatically analyzes the application content based on a semantic or a visual
criterion determined by the author or the user, and prepares it for the
splitting process. The system then splits the application and as a result
delivers two instrumented applications ready for distribution across devices.
During runtime, the system uses a mirroring phase to maintain the functionality
of the distributed application and to support a dynamic splitting process.
Developed as a Chrome extension, our approach is validated on several web
applications, including a YouTube page and a video application from Mozilla.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05407</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05407</id><created>2015-10-19</created><updated>2015-12-18</updated><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Sreedharan</keyname><forenames>Jithin K.</forenames><affiliation>MAESTRO</affiliation></author></authors><title>Bayesian Inference of Online Social Network Statistics via Lightweight
  Random Walk Crawls</title><categories>cs.SI physics.soc-ph stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networks (OSN) contain extensive amount of information about
the underlying society that is yet to be explored. One of the most feasible
technique to fetch information from OSN, crawling through Application
Programming Interface (API) requests, poses serious concerns over the the
guarantees of the estimates. In this work, we focus on making reliable
statistical inference with limited API crawls. Based on regenerative properties
of the random walks, we propose an unbiased estimator for the aggregated sum of
functions over edges and proved the connection between variance of the
estimator and spectral gap. In order to facilitate Bayesian inference on the
true value of the estimator, we derive the approximate posterior distribution
of the estimate. Later the proposed ideas are validated with numerical
experiments on inference problems in real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05417</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05417</id><created>2015-10-19</created><authors><author><keyname>Sato</keyname><forenames>Toshiki</forenames></author><author><keyname>Takano</keyname><forenames>Yuichi</forenames></author><author><keyname>Miyashiro</keyname><forenames>Ryuhei</forenames></author></authors><title>Piecewise-Linear Approximation for Feature Subset Selection in a
  Sequential Logit Model</title><categories>stat.ME cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns a method of selecting a subset of features for a
sequential logit model. Tanaka and Nakagawa (2014) proposed a mixed integer
quadratic optimization formulation for solving the problem based on a quadratic
approximation of the logistic loss function. However, since there is a
significant gap between the logistic loss function and its quadratic
approximation, their formulation may fail to find a good subset of features. To
overcome this drawback, we apply a piecewise-linear approximation to the
logistic loss function. Accordingly, we frame the feature subset selection
problem of minimizing an information criterion as a mixed integer linear
optimization problem. The computational results demonstrate that our
piecewise-linear approximation approach found a better subset of features than
the quadratic approximation approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05433</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05433</id><created>2015-10-19</created><authors><author><keyname>Akhremtsev</keyname><forenames>Yaroslav</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author></authors><title>Fast Parallel Operations on Search Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using (a,b)-trees as an example, we show how to perform a parallel split with
logarithmic latency and parallel join, bulk updates, intersection, union (or
merge), and (symmetric) set difference with logarithmic latency and with
information theoretically optimal work. We present both asymptotically optimal
solutions and simplified versions that perform well in practice - they are
several times faster than previous implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05435</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05435</id><created>2015-10-19</created><authors><author><keyname>Vaddi</keyname><forenames>Mahesh Babu</forenames></author><author><keyname>Bhattaram</keyname><forenames>Roop Kumar</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Optimal Scalar Linear Index Codes for Some Symmetric Multiple Unicast
  Problems</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity of symmetric instance of the multiple unicast index coding
problem with neighboring antidotes (side-information) with number of messages
equal to the number of receivers was given by Maleki, Cadambe and Jafar. In
this paper we consider ten symmetric multiple unicast problems with lesser
antidotes than considered by them and explicitly construct scalar linear codes
for these problems. These codes are shown to achieve the capacity or
equivalently these codes shown to be of optimal length. Also, the constructed
codes enable the receivers use small number of transmissions to decode their
wanted messages which is important to have the probability of message error
reduced in a noisy broadcast channel. Some of the cases considered are shown to
be critical index coding problems and these codes help to identify some of the
subclasses considered by others to be not critical index coding problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05436</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05436</id><created>2015-10-19</created><authors><author><keyname>Malek</keyname><forenames>Mohamed</forenames></author><author><keyname>Helbert</keyname><forenames>David</forenames></author><author><keyname>Carre</keyname><forenames>Philippe</forenames></author></authors><title>Color graph based wavelet transform with perceptual information</title><categories>cs.CV</categories><journal-ref>Journal of Electronic Imaging (SPIE), 24(5), 2015</journal-ref><doi>10.1117/1.JEI.24.5.053004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a numerical strategy to define a multiscale
analysis for color and multicomponent images based on the representation of
data on a graph. Our approach consists in computing the graph of an image using
the psychovisual information and analysing it by using the spectral graph
wavelet transform. We suggest introducing color dimension into the computation
of the weights of the graph and using the geodesic distance as a means of
distance measurement. We thus have defined a wavelet transform based on a graph
with perceptual information by using the CIELab color distance. This new
representation is illustrated with denoising and inpainting applications.
Overall, by introducing psychovisual information in the graph computation for
the graph wavelet transform we obtain very promising results. Therefore results
in image restoration highlight the interest of the appropriate use of color
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05437</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05437</id><created>2015-10-19</created><authors><author><keyname>Duan</keyname><forenames>Runyao</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>Activated zero-error classical capacity of quantum channels in the
  presence of quantum no-signalling correlations</title><categories>quant-ph cs.IT math.IT</categories><comments>8 pages and two lines in RevTex4, 0 figures; preliminary version;
  comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently the one-shot quantum no-signalling assisted zero-error classical
capacity of a quantum channel has been formulated as a semidefinite programming
(SDP) depending only on the Choi-Kraus operator space of the channel. In this
paper, we study the \textit{activated quantum no-signalling assisted zero-error
classical capacity} by first allowing the assistance from some noiseless
forward communication channel and later paying back the cost of the helper. We
show that the one-shot activated capacity can also be formulated as a SDP and
derive a number of striking properties of this number. In particular, this
number is additive under direct sum, and is always greater than or equal to the
super-dense coding bound. An a remarkable consequence, we find that one bit
noiseless classical communication is able to fully activate any
classical-quantum channel to achieve its asymptotic capacity, or the
semidefinite fractional packing number. We also discuss the condition under
which a noisy channel can activate an activatable channel. Interestingly, a
channel is able to activate itself if its one-shot capacity is greater than or
equal to the Golden Ratio - $(1+\sqrt{5})/2$. We also show that the asymptotic
activated capacity is still equal to the usual no-signalling assisted capacity.
Finally, we show that general the asymptotic no-signalling assisted zero-error
capacity does not equal to the semidefinite (fractional) packing number by an
explicit construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05452</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05452</id><created>2015-10-19</created><updated>2016-03-01</updated><authors><author><keyname>Alcolei</keyname><forenames>Aurore</forenames></author><author><keyname>Perrot</keyname><forenames>K&#xe9;vin</forenames></author><author><keyname>Sen&#xe9;</keyname><forenames>Sylvain</forenames></author></authors><title>On the flora of asynchronous locally non-monotonic Boolean automata
  networks</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean automata networks (BANs) are a well established model for biological
regulation systems such as neural networks or genetic networks. Studies on the
dynamics of BANs, whether it is synchronous or asynchronous, have mainly
focused on monotonic networks, where fundamental questions on the links
relating their static and dynamical properties have been raised and addressed.
This paper explores analogous questions on asynchronous non-monotonic networks,
xor-BANs, that are BANs where all the local transition functions are
xor-functions. Using algorithmic tools, we give a general characterisation of
the asynchronous transition graphs for most of the cactus xor-BANs and strongly
connected xor-BANs. As an illustration of the results, we provide a complete
description of the asynchronous dynamics of two particular classes of xor-BAN,
namely xor-Flowers and xor-Cycle Chains. This work also leads to new
bisimulation equivalences specific to xor-BANs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05454</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05454</id><created>2015-10-19</created><authors><author><keyname>Abshoff</keyname><forenames>Sebastian</forenames></author><author><keyname>Cord-Landwehr</keyname><forenames>Andreas</forenames></author><author><keyname>Fischer</keyname><forenames>Matthias</forenames></author><author><keyname>Jung</keyname><forenames>Daniel</forenames></author><author><keyname>der Heide</keyname><forenames>Friedhelm Meyer auf</forenames></author></authors><title>Gathering a Closed Chain of Robots on a Grid</title><categories>cs.DC cs.MA</categories><acm-class>F.1.2; F.2.2; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following variant of the two dimensional gathering problem
for swarms of robots: Given a swarm of $n$ indistinguishable, point shaped
robots on a two dimensional grid. Initially, the robots form a closed chain on
the grid and must keep this connectivity during the whole process of their
gathering. Connectivity means, that neighboring robots of the chain need to be
positioned at the same or neighboring points of the grid. In our model,
gathering means to keep shortening the chain until the robots are located
inside a $2\times 2$ subgrid. Our model is completely local (no global control,
no global coordinates, no compass, no global communication or vision, \ldots).
Each robot can only see its next constant number of left and right neighbors on
the chain. This fixed constant is called the \emph{viewing path length}. All
its operations and detections are restricted to this constant number of robots.
Other robots, even if located at neighboring or the same grid point cannot be
detected. Only based on the relative positions of its detectable chain
neighbors, a robot can decide to obtain a certain state. Based on this state
and their local knowledge, the robots do local modifications to the chain by
moving to neighboring grid points without breaking the chain. These
modifications are performed without the knowledge whether they lead to a global
progress or not. We assume the fully synchronous $\mathcal{FSYNC}$ model. For
this problem, we present a gathering algorithm which needs linear time. This
result generalizes the result from \cite{hopper}, where an open chain with
specified distinguishable (and fixed) endpoints is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05460</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05460</id><created>2015-10-19</created><authors><author><keyname>Chistikov</keyname><forenames>Dmitry</forenames></author><author><keyname>Czerwi&#x144;ski</keyname><forenames>Wojciech</forenames></author><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Wehar</keyname><forenames>Michael</forenames></author></authors><title>Shortest paths in one-counter systems</title><categories>cs.FL cs.LO</categories><comments>29 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any one-counter automaton with $n$ states, if its language is
non-empty, accepts some word of length at most $O(n^2)$. This closes the gap
between the previously known upper bound of $O(n^3)$ and lower bound of
$\Omega(n^2)$. More generally, we prove a tight upper bound on the length of
shortest paths between arbitrary configurations in one-counter transition
systems. Weaker bounds have previously appeared in the literature, and our
result offers an improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05461</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05461</id><created>2015-10-19</created><authors><author><keyname>Khim</keyname><forenames>Justin</forenames></author><author><keyname>Loh</keyname><forenames>Po-Ling</forenames></author></authors><title>Confidence Sets for the Source of a Diffusion in Regular Trees</title><categories>math.ST cs.DM cs.SI math.PR stat.ML stat.TH</categories><comments>23 pages</comments><msc-class>62M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of identifying the source of a diffusion spreading over
a regular tree. When the degree of each node is at least three, we show that it
is possible to construct confidence sets for the diffusion source with size
independent of the number of infected nodes. Our estimators are motivated by
analogous results in the literature concerning identification of the root node
in preferential attachment and uniform attachment trees. At the core of our
proofs is a probabilistic analysis of P\'{o}lya urns corresponding to the
number of uninfected neighbors in specific subtrees of the infection tree. We
also provide an example illustrating the shortcomings of source estimation
techniques in settings where the underlying graph is asymmetric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05464</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05464</id><created>2015-10-19</created><updated>2015-11-10</updated><authors><author><keyname>Kranich</keyname><forenames>Stefan</forenames></author></authors><title>Generation of real algebraic loci via complex detours</title><categories>math.AG cs.CG math.CV</categories><comments>revised version; 16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the locus generation algorithm used by the dynamic geometry
software Cinderella, and how it uses complex detours to resolve singularities.
We show that the algorithm is independent of the orientation of its complex
detours. We conjecture that the algorithm terminates if it takes small enough
complex detours and small enough steps on every complex detour. Moreover, we
introduce a variant of the algorithm that possibly generates entire real
connected components of real algebraic loci. Several examples illustrate its
use for organic generation of real algebraic loci. Another example shows how we
can apply the algorithm to simulate mechanical linkages. Apparently, the use of
complex detours produces physically reasonable motion of such linkages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05467</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05467</id><created>2015-10-19</created><updated>2016-03-05</updated><authors><author><keyname>Kumari</keyname><forenames>Priti</forenames></author><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author></authors><title>Autocorrelation values and Linear complexity of generalized cyclotomic
  sequence of order four, and construction of cyclic codes</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to this work has been
  done by the other author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $n_1$ and $n_2$ be two distinct primes with
$\mathrm{gcd}(n_1-1,n_2-1)=4$. In this paper, we compute the autocorrelation
values of generalized cyclotomic sequence of order $4$. Our results show that
this sequence can have very good autocorrelation property. We determine the
linear complexity and minimal polynomial of the generalized cyclotomic sequence
over $\mathrm{GF}(q)$ where $q=p^m$ and $p$ is an odd prime. Our results show
that this sequence possesses large linear complexity. So, the sequence can be
used in many domains such as cryptography and coding theory. We employ this
sequence of order $4$ to construct several classes of cyclic codes over
$\mathrm{GF}(q)$ with length $n_1n_2$. We also obtain the lower bounds on the
minimum distance of these cyclic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05477</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05477</id><created>2015-10-19</created><authors><author><keyname>Basbug</keyname><forenames>Mehmet Emin</forenames></author><author><keyname>Ozcan</keyname><forenames>Koray</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>Accelerometer based Activity Classification with Variational Inference
  on Sticky HDP-SLDS</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  As part of daily monitoring of human activities, wearable sensors and devices
are becoming increasingly popular sources of data. With the advent of
smartphones equipped with acceloremeter, gyroscope and camera; it is now
possible to develop activity classification platforms everyone can use
conveniently. In this paper, we propose a fast inference method for an
unsupervised non-parametric time series model namely variational inference for
sticky HDP-SLDS(Hierarchical Dirichlet Process Switching Linear Dynamical
System). We show that the proposed algorithm can differentiate various indoor
activities such as sitting, walking, turning, going up/down the stairs and
taking the elevator using only the acceloremeter of an Android smartphone
Samsung Galaxy S4. We used the front camera of the smartphone to annotate
activity types precisely. We compared the proposed method with Hidden Markov
Models with Gaussian emission probabilities on a dataset of 10 subjects. We
showed that the efficacy of the stickiness property. We further compared the
variational inference to the Gibbs sampler on the same model and show that
variational inference is faster in one order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05484</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05484</id><created>2015-10-19</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Zhao</keyname><forenames>Liming</forenames></author><author><keyname>Wei</keyname><forenames>Lina</forenames></author><author><keyname>Yang</keyname><forenames>MingHsuan</forenames></author><author><keyname>Wu</keyname><forenames>Fei</forenames></author><author><keyname>Zhuang</keyname><forenames>Yueting</forenames></author><author><keyname>Ling</keyname><forenames>Haibin</forenames></author><author><keyname>Wang</keyname><forenames>Jingdong</forenames></author></authors><title>DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object
  Detection</title><categories>cs.CV</categories><comments>Regular Paper Submitted to IEEE Transactions on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem in salient object detection is how to effectively model the
semantic properties of salient objects in a data-driven manner. In this paper,
we propose a multi-task deep saliency model based on a fully convolutional
neural network (FCNN) with global input (whole raw images) and global output
(whole saliency maps). In principle, the proposed saliency model takes a
data-driven strategy for encoding the underlying saliency prior information,
and then sets up a multi-task learning scheme for exploring the intrinsic
correlations between saliency detection and semantic image segmentation.
Through collaborative feature learning from such two correlated tasks, the
shared fully convolutional layers produce effective features for object
perception. Moreover, it is capable of capturing the semantic information on
salient objects across different levels using the fully convolutional layers,
which investigates the feature-sharing properties of salient object detection
with great feature redundancy reduction. Finally, we present a graph Laplacian
regularized nonlinear regression model for saliency refinement. Experimental
results demonstrate the effectiveness of our approach in comparison with the
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05486</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05486</id><created>2015-10-19</created><authors><author><keyname>Razgon</keyname><forenames>Igor</forenames></author></authors><title>Two types of branching programs with bounded repetition that cannot
  efficiently compute monotone 3-CNFs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove exponential lower bounds for two classes of non-deterministic
branching programs (NBPs) with bounded repetition computing a class of
functions representable by monotone 3-CNFs. The first class significantly
generalizes monotone read-$k$-times NBPs and the second class generalizes
oblivious read $k$ times branching programs. To the best of our knowledge, this
is the first separation of monotone NBPs with bounded repetition from monotone
CNFs as well as the first separation of oblivious read-$k$-times branching
programs (even determinisitc ones) from CNFs. The lower bounds remain
exponential for $k \leq \log n/a$ where $a$ is a sufficiently large constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05491</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05491</id><created>2015-10-19</created><authors><author><keyname>Basbug</keyname><forenames>Mehmet Emin</forenames></author><author><keyname>Engelhardt</keyname><forenames>Barbara</forenames></author></authors><title>Clustering with Beta Divergences</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Clustering algorithms start with a fixed divergence metric, which captures
the possibly asymmetric distance between two samples. In a mixture model, the
sample distribution plays the role of a divergence metric. It is often the case
that the distributional assumption is not validated, which calls for an
adaptive approach. We consider a richer model where the underlying distribution
belongs to a parametrized exponential family, called Tweedie Models. We first
show the connection between the Tweedie models and beta divergences, and derive
the corresponding hard-assignment clustering algorithm. We exploit this
connection to identify moment conditions and use Generalized Method of
Moments(GMoM) to learn the data distribution. Based on this adaptive approach,
we propose four new hard clustering algorithms and compare them to the
classical k-means and DP-means on synthetic data as well as seven UCI datasets
and one large gene expression dataset. We further compare the GMoM routine to
an approximate maximum likelihood routine and validate the computational
benefits of the GMoM approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05497</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05497</id><created>2015-10-19</created><updated>2015-10-28</updated><authors><author><keyname>Reshetova</keyname><forenames>Elena</forenames></author><author><keyname>Bonazzi</keyname><forenames>Filippo</forenames></author><author><keyname>Nyman</keyname><forenames>Thomas</forenames></author><author><keyname>Borgaonkar</keyname><forenames>Ravishankar</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Characterizing SEAndroid Policies in the Wild</title><categories>cs.CR</categories><comments>10 pages, 3 figures. v2: Added acknowledgment to Jan-Erik Ekberg. v3:
  Fixed typo in abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Starting from the 5.0 Lollipop release all Android processes must be run
inside confined SEAndroid access control domains. As a result, Android device
manufacturers were compelled to develop SEAndroid expertise in order to create
policies for their device-specific components. In this paper we analyse
SEAndroid policies from a number of 5.0 Lollipop devices on the market, and
identify patterns of common problems we found. We also suggest some practical
tools that can improve policy design and analysis. We implemented the first of
such tools, SEAL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05502</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05502</id><created>2015-10-19</created><authors><author><keyname>Brewster</keyname><forenames>Richard</forenames></author><author><keyname>Foucaud</keyname><forenames>Florent</forenames></author><author><keyname>Hell</keyname><forenames>Pavol</forenames></author><author><keyname>Naserasr</keyname><forenames>Reza</forenames></author></authors><title>The complexity of signed graph and 2-edge-coloured graph homomorphisms</title><categories>cs.DM math.CO</categories><comments>18 pages; 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to study homomorphism problems (from a computational
point of view) on two superclasses of graphs: $2$-edge-coloured graphs and
signed graphs. On the one hand, we consider the $H$-COLOURING problem when $H$
is a $2$-edge-coloured graph, and we show that a dichotomy theorem would imply
the dichotomy conjecture of Feder and Vardi. On the other hand, we prove a
dichotomy theorem for the $(H,\Pi)$-COLOURING problem for a large class of
signed graphs $(H,\Pi)$. Specifically, as long as $(H,\Pi)$ does not contain a
negative (respectively a positive) loop, the problem is polynomial-time
solvable if the core of $(H,\Pi)$ has at most two edges and is NP-complete
otherwise. (Note that this covers all simple signed graphs.) The same dichotomy
holds if $(H,\Pi)$ has no digons, and we conjecture that it holds always.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05503</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05503</id><created>2015-10-19</created><authors><author><keyname>Fafianie</keyname><forenames>Stefan</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Quyen</keyname><forenames>Voung Anh</forenames></author></authors><title>Preprocessing under uncertainty</title><categories>cs.DS</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study preprocessing for tractable problems when part of the
input is unknown or uncertain. This comes up naturally if, e.g., the load of
some machines or the congestion of some roads is not known far enough in
advance, or if we have to regularly solve a problem over instances that are
largely similar, e.g., daily airport scheduling with few charter flights.
Unlike robust optimization, which also studies settings like this, our goal
lies not in computing solutions that are (approximately) good for every
instantiation. Rather, we seek to preprocess the known parts of the input, to
speed up finding an optimal solution once the missing data is known.
  We present efficient algorithms that given an instance with partially
uncertain input generate an instance of size polynomial in the amount of
uncertain data that is equivalent for every instantiation of the unknown part.
Concretely, we obtain such algorithms for Minimum Spanning Tree, Minimum Weight
Matroid Basis, and Maximum Cardinality Bipartite Maxing, where respectively the
weight of edges, weight of elements, and the availability of vertices is
unknown for part of the input. Furthermore, we show that there are tractable
problems, such as Small Connected Vertex Cover, for which one cannot hope to
obtain similar results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05512</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05512</id><created>2015-10-19</created><updated>2016-02-01</updated><authors><author><keyname>Luccio</keyname><forenames>Fabrizio</forenames></author></authors><title>Arithmetic for Rooted Trees</title><categories>cs.DM</categories><comments>18 pages, 8 figures</comments><acm-class>E.1; G.2.0; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new arithmetic for non-empty rooted unordered trees simply
called trees. After discussing tree representation and enumeration, we define
the operations of tree addition, multiplication and stretch, prove their
properties, and show that all trees can be generated from a starting tree of
one vertex. We then show how a given tree can be obtained as the sum or product
of two trees, thus defining prime trees with respect to addition and
multiplication. In both cases we show how primality can be decided in time
polynomial in the number of vertices and we prove that factorization is unique.
We then define negative trees and suggest dealing with tree equations, giving
some preliminary results. Finally we comment on how our arithmetic might be
useful, and discuss preceding studies that have some relations with our. To the
best of our knowledge our approach and results are completely new aside for an
earlier version of this work submitte as an arXiv manuscript.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05513</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05513</id><created>2015-10-19</created><authors><author><keyname>Moll&#xe9;n</keyname><forenames>Christopher</forenames></author><author><keyname>Gustavsson</keyname><forenames>Ulf</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Out-of-Band Radiation Measure for MIMO Arrays with Beamformed
  Transmission</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spatial characteristics of the out-of-band radiation that a multiuser
MIMO system emits in the environment, due to its power amplifiers (modeled by a
polynomial model) are nonlinear, is studied by deriving an analytical
expression for the continuous-time cross-correlation of the transmit signals.
At a random spatial point, the same power is received at any frequency on
average with a MIMO base station as with a SISO base station when the two
radiate the same amount of power. For a specific channel realization however,
the received power depends on the channel. We show that the power received
out-of-band only deviates little from the average in a MIMO system with
multiple users and that the deviation can be significant with only one user.
Using an ergodicity argument, we conclude that out-of-band radiation is less of
a problem in massive MIMO, where total radiated power is lower compared to SISO
systems and that requirements on spectral regrowth can be relaxed in MIMO
systems without causing more total out-of-band radiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05524</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05524</id><created>2015-10-19</created><authors><author><keyname>Nasini</keyname><forenames>Graciela</forenames></author><author><keyname>Severin</keyname><forenames>Daniel</forenames></author><author><keyname>Torres</keyname><forenames>Pablo</forenames></author></authors><title>On the Packing Chromatic Number on Hamming Graphs and General Graphs</title><categories>cs.DM</categories><comments>Accepted in Simp\'osio Brasileiro de Pesquisa Operacional XLVII (SBPO
  2015) Web page: http://cdsid.org.br/sbpo2015</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The packing chromatic number $\chi_\rho(G)$ of a graph $G$ is the smallest
integer $k$ needed to proper color the vertices of $G$ in such a way the
distance between any two vertices having color $i$ be at least $i+1$. We obtain
$\chi_\rho(H_{q,m})$ for $m=3$, where $H_{q,m}$ is the Hamming graph of words
of length $m$ and alphabet with $q$ symbols, and tabulate bounds of them for $m
\geq 4$ up to 10000 vertices. We also give a polynomial reduction from the
problem of finding $\chi_\rho(G)$ to the Maximum Stable Set problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05527</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05527</id><created>2015-10-19</created><authors><author><keyname>Naumann</keyname><forenames>David A.</forenames></author></authors><title>Towards Patterns for Heaps and Imperative Lambdas</title><categories>cs.PL</categories><doi>10.1016/j.jlamp.2015.10.008</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In functional programming, point-free relation calculi have been fruitful for
general theories of program construction, but for specific applications
pointwise expressions can be more convenient and comprehensible. In imperative
programming, refinement calculi have been tied to pointwise expression in terms
of state variables, with the curious exception of the ubiquitous but invisible
heap. To integrate pointwise with point-free, de Moor and Gibbons extended
lambda calculus with non-injective pattern matching interpreted using
relations. This article gives a semantics of that language using ``ideal
relations'' between partial orders, and a second semantics using predicate
transformers. The second semantics is motivated by its potential use with
separation algebra, for pattern matching in programs acting on the heap. Laws
including lax beta and eta are proved in these models and a number of open
problems are posed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05533</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05533</id><created>2015-10-19</created><authors><author><keyname>Iber</keyname><forenames>Dagmar</forenames></author><author><keyname>Karimaddini</keyname><forenames>Zahra</forenames></author><author><keyname>&#xdc;nal</keyname><forenames>Erkan</forenames></author></authors><title>Image-based Modelling of Organogenesis</title><categories>cs.CE q-bio.TO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major challenges in biology concerns the integration of data
across length and time scales into a consistent framework: how do macroscopic
properties and functionalities arise from the molecular regulatory networks -
and how can they change as a result of mutations? Morphogenesis provides an
excellent model system to study how simple molecular networks robustly control
complex processes on the macroscopic scale in spite of molecular noise, and how
important functional variants can emerge from small genetic changes. Recent
advancements in 3D imaging technologies, computer algorithms, and computer
power now allow us to develop and analyse increasingly realistic models of
biological control. Here we present our pipeline for image-based modeling that
includes the segmentation of images, the determination of displacement fields,
and the solution of systems of partial differential equations (PDEs) on the
growing, embryonic domains. The development of suitable mathematical models,
the data-based inference of parameter sets, and the evaluation of competing
models are still challenging, and current approaches are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05544</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05544</id><created>2015-10-19</created><updated>2015-11-18</updated><authors><author><keyname>Shah</keyname><forenames>Neil</forenames></author><author><keyname>Beutel</keyname><forenames>Alex</forenames></author><author><keyname>Hooi</keyname><forenames>Bryan</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author><author><keyname>Gunnemann</keyname><forenames>Stephan</forenames></author><author><keyname>Makhija</keyname><forenames>Disha</forenames></author><author><keyname>Kumar</keyname><forenames>Mohit</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>EdgeCentric: Anomaly Detection in Edge-Attributed Networks</title><categories>cs.SI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a network with attributed edges, how can we identify anomalous
behavior? Networks with edge attributes are commonplace in the real world. For
example, edges in e-commerce networks often indicate how users rated products
and services in terms of number of stars, and edges in online social and
phonecall networks contain temporal information about when friendships were
formed and when users communicated with each other -- in such cases, edge
attributes capture information about how the adjacent nodes interact with other
entities in the network. In this paper, we aim to utilize exactly this
information to discern suspicious from typical node behavior. Our work has a
number of notable contributions, including (a) formulation: while most other
graph-based anomaly detection works use structural graph connectivity or node
information, we focus on the new problem of leveraging edge information, (b)
methodology: we introduce EdgeCentric, an intuitive and scalable
compression-based approach for detecting edge-attributed graph anomalies, and
(c) practicality: we show that EdgeCentric successfully spots numerous such
anomalies in several large, edge-attributed real-world graphs, including the
Flipkart e-commerce graph with over 3 million product reviews between 1.1
million users and 545 thousand products, where it achieved 0.87 precision over
the top 100 results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05546</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05546</id><created>2015-10-19</created><authors><author><keyname>Wang</keyname><forenames>Bei</forenames></author><author><keyname>Ethier</keyname><forenames>Stephane</forenames></author><author><keyname>Tang</keyname><forenames>William</forenames></author><author><keyname>Ibrahim</keyname><forenames>Khaled</forenames></author><author><keyname>Madduri</keyname><forenames>Kamesh</forenames></author><author><keyname>Williams</keyname><forenames>Samuel</forenames></author><author><keyname>Oliker</keyname><forenames>Leonid</forenames></author></authors><title>Modern Gyrokinetic Particle-In-Cell Simulation of Fusion Plasmas on Top
  Supercomputers</title><categories>cs.DC physics.comp-ph physics.plasm-ph</categories><comments>submitted to International Journal of High Performance Computing
  Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gyrokinetic Toroidal Code at Princeton (GTC-P) is a highly scalable and
portable particle-in-cell (PIC) code. It solves the 5D Vlasov-Poisson equation
featuring efficient utilization of modern parallel computer architectures at
the petascale and beyond. Motivated by the goal of developing a modern code
capable of dealing with the physics challenge of increasing problem size with
sufficient resolution, new thread-level optimizations have been introduced as
well as a key additional domain decomposition. GTC-P's multiple levels of
parallelism, including inter-node 2D domain decomposition and particle
decomposition, as well as intra-node shared memory partition and vectorization
have enabled pushing the scalability of the PIC method to extreme computational
scales. In this paper, we describe the methods developed to build a highly
parallelized PIC code across a broad range of supercomputer designs. This
particularly includes implementations on heterogeneous systems using NVIDIA GPU
accelerators and Intel Xeon Phi (MIC) co-processors and performance comparisons
with state-of-the-art homogeneous HPC systems such as Blue Gene/Q. New
discovery science capabilities in the magnetic fusion energy application domain
are enabled, including investigations of Ion-Temperature-Gradient (ITG) driven
turbulence simulations with unprecedented spatial resolution and long temporal
duration. Performance studies with realistic fusion experimental parameters are
carried out on multiple supercomputing systems spanning a wide range of cache
capacities, cache-sharing configurations, memory bandwidth, interconnects and
network topologies. These performance comparisons using a realistic
discovery-science-capable domain application code provide valuable insights on
optimization techniques across one of the broadest sets of current high-end
computing platforms worldwide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05555</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05555</id><created>2015-10-19</created><updated>2015-11-16</updated><authors><author><keyname>Boneva</keyname><forenames>Iovka</forenames></author><author><keyname>Gayo</keyname><forenames>Jose E. Labra</forenames></author><author><keyname>Prud'hommeaux</keyname><forenames>Eric G.</forenames></author><author><keyname>Staworko</keyname><forenames>S&#x142;awek</forenames></author></authors><title>Shape Expressions Schemas</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Shape Expressions (ShEx), an expressive schema language for RDF
designed to provide a high-level, user friendly syntax with intuitive
semantics. ShEx allows to describe the vocabulary and the structure of an RDF
graph, and to constrain the allowed values for the properties of a node. It
includes an algebraic grouping operator, a choice operator, cardinalitiy
constraints for the number of allowed occurrences of a property, and negation.
We define the semantics of the language and illustrate it with examples. We
then present a validation algorithm that, given a node in an RDF graph and a
constraint defined by the ShEx schema, allows to check whether the node
satisfies that constraint. The algorithm outputs a proof that contains
trivially verifiable associations of nodes and the constraints that they
satisfy. The structure can be used for complex post-processing tasks, such as
transforming the RDF graph to other graph or tree structures, verifying more
complex constraints, or debugging (w.r.t. the schema). We also show the
inherent difficulty of error identification of ShEx.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05557</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05557</id><created>2015-10-19</created><authors><author><keyname>Guruacharya</keyname><forenames>Sudarshan</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Saddle Point Approximation for Outage Probability Using Cumulant
  Generating Functions</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes the use of saddle point approximation (SPA) to evaluate
the outage probability of wireless cellular networks. Unlike traditional
numerical integration-based approaches, the SPA approach relies on cumulant
generating functions (CGFs) and eliminates the need for explicit numerical
integration. The approach is generic and can be applied to a wide variety of
distributions, given that their CGFs exist. We illustrate the usefulness of SPA
on channel fading distributions such as Nakagami-$m$, Nakagami-$q$ (Hoyt), and
Rician distributions. Numerical results validate the accuracy of the proposed
SPA approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05559</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05559</id><created>2015-10-19</created><authors><author><keyname>Jin</keyname><forenames>Kyong Hwan</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author></authors><title>Sparse + Low Rank Decomposition of Annihilating Filter-based Hankel
  Matrix for Impulse Noise Removal</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, so called annihilating filer-based low rank Hankel matrix (ALOHA)
approach was proposed as a powerful image inpainting method. Based on the
observation that smoothness or textures within an image patch corresponds to
sparse spectral components in the frequency domain, ALOHA exploits the
existence of annihilating filters and the associated rank-deficient Hankel
matrices in the image domain to estimate the missing pixels. By extending this
idea, here we propose a novel impulse noise removal algorithm using sparse +
low rank decomposition of an annihilating filter-based Hankel matrix. The new
approach, what we call the robust ALOHA, is motivated by the observation that
an image corrupted with impulse noises has intact pixels; so the impulse noises
can be modeled as sparse components, whereas the underlying image can be still
modeled using a low-rank Hankel structured matrix. To solve the sparse + low
rank decomposition problem, we propose an alternating direction method of
multiplier (ADMM) method with initial factorized matrices coming from low rank
matrix fitting (LMaFit) algorithm. To adapt the local image statistics that
have distinct spectral distributions, the robust ALOHA is applied patch by
patch. Experimental results from two types of impulse noises - random valued
impulse noises and salt/pepper noises - for both single channel and
multi-channel color images demonstrate that the robust ALOHA outperforms the
existing algorithms up to 8dB in terms of the peak signal to noise ratio
(PSNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05567</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05567</id><created>2015-10-19</created><updated>2015-11-12</updated><authors><author><keyname>Thammawichai</keyname><forenames>Mason</forenames></author><author><keyname>Kerrigan</keyname><forenames>Eric C.</forenames></author></authors><title>Energy-Efficient Scheduling for Homogeneous Multiprocessor Systems</title><categories>cs.OS cs.SY math.OC</categories><comments>Corrected typos: definition of J_i in Section 2.1; (3b)-(3c);
  definition of \Phi_A and \Phi_D in paragraph after (6b). Previous equations
  were correct only for special case of p_i=d_i</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a number of novel algorithms, based on mathematical optimization
formulations, in order to solve a homogeneous multiprocessor scheduling
problem, while minimizing the total energy consumption. In particular, for a
system with a discrete speed set, we propose solving a tractable linear
program. Our formulations are based on a fluid model and a global scheduling
scheme, i.e. tasks are allowed to migrate between processors. The new methods
are compared with three global energy/feasibility optimal workload allocation
formulations. Simulation results illustrate that our methods achieve both
feasibility and energy optimality and outperform existing methods for
constrained deadline tasksets. Specifically, the results provided by our
algorithm can achieve up to an 80% saving compared to an algorithm without a
frequency scaling scheme and up to 70% saving compared to a constant frequency
scaling scheme for some simulated tasksets. Another benefit is that our
algorithms can solve the scheduling problem in one step instead of using a
recursive scheme. Moreover, our formulations can solve a more general class of
scheduling problems, i.e. any periodic real-time taskset with arbitrary
deadline. Lastly, our algorithms can be applied to both online and offline
scheduling schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05569</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05569</id><created>2015-10-19</created><authors><author><keyname>Sharma</keyname><forenames>Amit</forenames></author><author><keyname>Hofman</keyname><forenames>Jake M.</forenames></author><author><keyname>Watts</keyname><forenames>Duncan J.</forenames></author></authors><title>Estimating the Causal Impact of Recommendation Systems from
  Observational Data</title><categories>cs.SI stat.AP</categories><comments>ACM Conference on Economics and Computation (EC 2015)</comments><acm-class>J.4</acm-class><doi>10.1145/2764468.2764488</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems are an increasingly prominent part of the web,
accounting for up to a third of all traffic on several of the world's most
popular sites. Nevertheless, little is known about how much activity such
systems actually cause over and above activity that would have occurred via
other means (e.g., search) if recommendations were absent. Although the ideal
way to estimate the causal impact of recommendations is via randomized
experiments, such experiments are costly and may inconvenience users. In this
paper, therefore, we present a method for estimating causal effects from purely
observational data. Specifically, we show that causal identification through an
instrumental variable is possible when a product experiences an instantaneous
shock in direct traffic and the products recommended next to it do not. We then
apply our method to browsing logs containing anonymized activity for 2.1
million users on Amazon.com over a 9 month period and analyze over 4,000 unique
products that experience such shocks. We find that although recommendation
click-throughs do account for a large fraction of traffic among these products,
at least 75% of this activity would likely occur in the absence of
recommendations. We conclude with a discussion about the assumptions under
which the method is appropriate and caveats around extrapolating results to
other products, sites, or settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05572</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05572</id><created>2015-10-19</created><authors><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>On the Computability of AIXI</title><categories>cs.AI</categories><comments>UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How could we solve the machine learning and the artificial intelligence
problem if we had infinite computation? Solomonoff induction and the
reinforcement learning agent AIXI are proposed answers to this question. Both
are known to be incomputable. In this paper, we quantify this using the
arithmetical hierarchy, and prove upper and corresponding lower bounds for
incomputability. We show that AIXI is not limit computable, thus it cannot be
approximated using finite computation. Our main result is a limit-computable
{\epsilon}-optimal version of AIXI with infinite horizon that maximizes
expected rewards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05577</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05577</id><created>2015-10-19</created><authors><author><keyname>Rana</keyname><forenames>Jitenkumar Babubhai</forenames></author><author><keyname>Shetty</keyname><forenames>Rashmi</forenames></author><author><keyname>Jha</keyname><forenames>Tanya</forenames></author></authors><title>Application of Machine Learning Techniques in Human Activity Recognition</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human activity detection has seen a tremendous growth in the last decade
playing a major role in the field of pervasive computing. This emerging
popularity can be attributed to its myriad of real-life applications primarily
dealing with human-centric problems like healthcare and elder care. Many
research attempts with data mining and machine learning techniques have been
undergoing to accurately detect human activities for e-health systems. This
paper reviews some of the predictive data mining algorithms and compares the
accuracy and performances of these models. A discussion on the future research
directions is subsequently offered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05594</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05594</id><created>2015-10-19</created><authors><author><keyname>Alhussein</keyname><forenames>Omar</forenames></author><author><keyname>Hammadi</keyname><forenames>Ahmed Al</forenames></author><author><keyname>Sofotasios</keyname><forenames>Paschalis C.</forenames></author><author><keyname>Muhaidat</keyname><forenames>Sami</forenames></author><author><keyname>Liang</keyname><forenames>Jie</forenames></author><author><keyname>Al-Qutayri</keyname><forenames>Mahmoud</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Performance Analysis of Energy Detection over Mixture Gamma based Fading
  Channels with Diversity Reception</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE WiMob 2015 conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper is devoted to the evaluation of energy detection based
spectrum sensing over different multipath fading and shadowing conditions. This
is realized by means of a unified and versatile approach that is based on the
particularly flexible mixture gamma distribution. To this end, novel analytic
expressions are firstly derived for the probability of detection over MG fading
channels for the conventional single-channel communication scenario. These
expressions are subsequently employed in deriving closed-form expressions for
the case of square-law combining and square-law selection diversity methods.
The validity of the offered expressions is verified through comparisons with
results from respective computer simulations. Furthermore, they are employed in
analyzing the performance of energy detection over multipath fading, shadowing
and composite fading conditions, which provides useful insighs on the
performance and design of future cognitive radio based communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05606</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05606</id><created>2015-10-19</created><authors><author><keyname>Hosseini</keyname><forenames>Mohammad</forenames></author><author><keyname>Jiang</keyname><forenames>Yu</forenames></author><author><keyname>Wu</keyname><forenames>Poliang</forenames></author><author><keyname>Berlin</keyname><forenames>Richard B.</forenames></author><author><keyname>Sha</keyname><forenames>Lui</forenames></author></authors><title>SINk: A Middleware for Synchronization of Heterogeneous Software
  Interfaces</title><categories>cs.SE</categories><comments>ARM 2015, December 07-11, 2015, Vancouver, BC, Canada Copyright 2015
  ACM 978-1-4503-3733-5/15/12</comments><acm-class>H.5.2</acm-class><doi>10.1145/2834965.2834967</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software is everywhere. The increasing requirement of supporting a wide
variety of domains has rapidly increased the complexity of software systems,
making them hard to maintain and the training process harder for end-users,
which in turn ultimately demanded the development of user-friendly application
software with simple interfaces that makes them easy, especially for
non-professional personnel, to employ, and interact with.
  However, due to the lack of source code access for third-party software and
the lack of non-graphical interfaces such as web-services or RMI (Remote Method
Invocation) access to application functionality, synchronization between
heterogeneous closed-box software interfaces and a user-friendly version of
those interfaces has become a major challenge. In this paper, we design SINk, a
middleware that enables synchronization of multiple heterogeneous software
applications, using only graphical interface, without the need for source code
access or access to the entire platform's control. SINk helps with
synchronization of closed-box industry software, where in fact the only
possible way of communication is through software interfaces. It leverages
efficient client sever architecture, socket based protocol, adaptation to
resolution changes, and parameter mapping mechanisms to transfer control events
to ensure the real-time requirements of synchronization among multiple
interfaces are met. Our proof-of-concept evaluation shows there is in fact
potential usage of our middleware in a wide variety of domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05610</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05610</id><created>2015-10-19</created><updated>2015-10-21</updated><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Guntuboyina</keyname><forenames>Adityanand</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Stochastically Transitive Models for Pairwise Comparisons: Statistical
  and Computational Issues</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are various parametric models for analyzing pairwise comparison data,
including the Bradley-Terry-Luce (BTL) and Thurstone models, but their reliance
on strong parametric assumptions is limiting. In this work, we study a flexible
model for pairwise comparisons, under which the probabilities of outcomes are
required only to satisfy a natural form of stochastic transitivity. This class
includes several parametric models including the BTL and Thurstone models as
special cases, but is considerably more general. We provide various examples of
models in this broader stochastically transitive class for which classical
parametric models provide poor fits. Despite this greater flexibility, we show
that the matrix of probabilities can be estimated at the same rate as in
standard parametric models. On the other hand, unlike in the BTL and Thurstone
models, computing the minimax-optimal estimator in the stochastically
transitive model is non-trivial, and we explore various computationally
tractable alternatives. We show that a simple singular value thresholding
algorithm is statistically consistent but does not achieve the minimax rate. We
then propose and study algorithms that achieve the minimax rate over
interesting sub-classes of the full stochastically transitive class. We
complement our theoretical results with thorough numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05613</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05613</id><created>2015-10-19</created><authors><author><keyname>Narayanan</keyname><forenames>Venkatraman</forenames></author><author><keyname>Likhachev</keyname><forenames>Maxim</forenames></author></authors><title>PERCH: Perception via Search for Multi-Object Recognition and
  Localization</title><categories>cs.CV cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many robotic domains such as flexible automated manufacturing or personal
assistance, a fundamental perception task is that of identifying and localizing
objects whose 3D models are known. Canonical approaches to this problem include
discriminative methods that find correspondences between feature descriptors
computed over the model and observed data. While these methods have been
employed successfully, they can be unreliable when the feature descriptors
cannot capture variations in observed data: a classic example being occlusion.
As a step towards deliberative reasoning, we present PERCH: PErception via
SeaRCH, an algorithm that seeks to find the best explanation of the observed
sensor data by hypothesizing possible scenes in a generative fashion. Our
contributions are: i) formulating the multi-object recognition and localization
task as an optimization problem over the space of hypothesized scenes, ii)
exploiting structure in the optimization to cast it as a combinatorial search
problem on what we call the Monotone Scene Generation Tree, and iii) leveraging
parallelization and recent advances in multi-heuristic search in making
combinatorial search tractable. We prove that our system can guaranteeably
produce the best explanation of the scene under the chosen cost function, and
validate our claims on real world RGB-D test data. Our experimental results
show that we can identify and localize objects under heavy occlusion--cases
where state-of-the-art methods struggle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05622</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05622</id><created>2015-10-19</created><authors><author><keyname>Bihan</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LAMA</affiliation></author><author><keyname>Spaenlehauer</keyname><forenames>Pierre-Jean</forenames><affiliation>CARAMEL</affiliation></author></authors><title>Sparse Polynomial Systems with many Positive Solutions from Bipartite
  Simplicial Complexes</title><categories>math.AG cs.SC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a regular triangulation of the convex-hull $P$ of a set $\mathcal A$
of $n$ points in $\mathbb R^d$, and a real matrix $C$ of size $d \times n$. A
version of Viro's method allows to construct from these data an unmixed
polynomial system with support $\mathcal A$ and coefficient matrix $C$ whose
number of positive solutions is bounded from below by the number of
$d$-simplices which are positively decorated by $C$. We show that all the
$d$-simplices of a triangulation can be positively decorated if and only if the
triangulation is balanced, which in turn is equivalent to the fact that its
dual graph is bipartite. This allows us to identify, among classical families,
monomial supports which admit maximally positive systems, i.e. systems all
toric complex solutions of which are real and positive. These families give
some evidence in favor of a conjecture due to Bihan. We also use this technique
in order to construct fewnomial systems with many positive solutions. This is
done by considering a simplicial complex with bipartite dual graph included in
a regular triangulation of the cyclic polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05681</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05681</id><created>2015-10-19</created><authors><author><keyname>Couto</keyname><forenames>Rodrigo de Souza</forenames></author><author><keyname>Secci</keyname><forenames>Stefano</forenames></author><author><keyname>Campista</keyname><forenames>Miguel Elias Mitre</forenames></author><author><keyname>Costa</keyname><forenames>Lu&#xed;s Henrique Maciel Kosmalski</forenames></author></authors><title>Server Placement with Shared Backups for Disaster-Resilient Clouds</title><categories>cs.NI</categories><comments>Computer Networks 2015</comments><doi>10.1016/j.comnet.2015.09.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key strategy to build disaster-resilient clouds is to employ backups of
virtual machines in a geo-distributed infrastructure. Today, the continuous and
acknowledged replication of virtual machines in different servers is a service
provided by different hypervisors. This strategy guarantees that the virtual
machines will have no loss of disk and memory content if a disaster occurs, at
a cost of strict bandwidth and latency requirements. Considering this kind of
service, in this work, we propose an optimization problem to place servers in a
wide area network. The goal is to guarantee that backup machines do not fail at
the same time as their primary counterparts. In addition, by using
virtualization, we also aim to reduce the amount of backup servers required.
The optimal results, achieved in real topologies, reduce the number of backup
servers by at least 40%. Moreover, this work highlights several characteristics
of the backup service according to the employed network, such as the
fulfillment of latency requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05682</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05682</id><created>2015-10-19</created><authors><author><keyname>Ma</keyname><forenames>Jianzhu</forenames></author></authors><title>Protein Structure Prediction by Protein Alignments</title><categories>cs.CE cs.LG q-bio.BM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1306.4420 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proteins are the basic building blocks of life. They usually perform
functions by folding to a particular structure. Understanding the folding
process could help the researchers to understand the functions of proteins and
could also help to develop supplemental proteins for people with deficiencies
and gain more insight into diseases associated with troublesome folding
proteins. Experimental methods are both expensive and time consuming. In this
thesis I introduce a new machine learning based method to predict the protein
structure. The new method improves the performance from two directions:
creating accurate protein alignments and predicting accurate protein contacts.
First, I present an alignment framework MRFalign which goes beyond
state-of-the-art methods and uses Markov Random Fields to model a protein
family and align two proteins by aligning two MRFs together. Compared to other
methods, that can only model local-range residue correlation, MRFs can model
long-range residue interactions and thus, encodes global information in a
protein. Secondly, I present a Group Graphical Lasso method for contact
prediction that integrates joint multi-family Evolutionary Coupling analysis
and supervised learning to improve accuracy on proteins without many sequence
homologs. Different from single-family EC analysis that uses residue
co-evolution information in only the target protein family, our joint EC
analysis uses residue co-evolution in both the target family and its related
families, which may have divergent sequences but similar folds. Our method can
also integrate supervised learning methods to further improve accuracy. We
evaluate the performance of both methods including each of its components on
large public benchmarks. Experiments show that our methods can achieve better
accuracy than existing state-of-the-art methods under all the measurements on
most of the protein classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05700</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05700</id><created>2015-10-19</created><authors><author><keyname>Souza</keyname><forenames>Fl&#xe1;vio</forenames></author><author><keyname>Casas</keyname><forenames>Diego de Las</forenames></author><author><keyname>Flores</keyname><forenames>Vin&#xed;cius</forenames></author><author><keyname>Youn</keyname><forenames>SunBum</forenames></author><author><keyname>Cha</keyname><forenames>Meeyoung</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Almeida</keyname><forenames>Virg&#xed;lio</forenames></author></authors><title>Dawn of the Selfie Era: The Whos, Wheres, and Hows of Selfies on
  Instagram</title><categories>cs.SI cs.CY</categories><comments>ACM Conference on Online Social Networks 2015, Stanford University,
  California, USA</comments><acm-class>J.4; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online interactions are increasingly involving images, especially those
containing human faces, which are naturally attention grabbing and more
effective at conveying feelings than text. To understand this new convention of
digital culture, we study the collective behavior of sharing selfies on
Instagram and present how people appear in selfies and which patterns emerge
from such interactions. Analysis of millions of photos shows that the amount of
selfies has increased by 900 times from 2012 to 2014. Selfies are an effective
medium to grab attention; they generate on average 1.1--3.2 times more likes
and comments than other types of content on Instagram. Compared to other
content, interactions involving selfies exhibit variations in homophily scores
(in terms of age and gender) that suggest they are becoming more widespread.
Their style also varies by cultural boundaries in that the average age and
majority gender seen in selfies differ from one country to another. We provide
explanations of such country-wise variations based on cultural and
socioeconomic contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05705</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05705</id><created>2015-10-19</created><authors><author><keyname>Gale</keyname><forenames>Ella</forenames></author></authors><title>Single Memristor Logic Gates: From NOT to a Full Adder</title><categories>cs.ET cond-mat.mes-hall cs.NE</categories><msc-class>03B-02, 68U02</msc-class><acm-class>B.3.1; B.4.3; B.5.1; B.6.1; C.1.3; C.1.m; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristors have been suggested as a novel route to neuromorphic computing
based on the similarity between them and neurons (specifically synapses and ion
pumps). The d.c. action of the memristor is a current spike which imparts a
short-term memory to the device. Here it is demonstrated that this short-term
memory works exactly like habituation (e.g. in \emph{Aplysia}). We elucidate
the physical rules, based on energy conservation, governing the interaction of
these current spikes: summation, `bounce-back', directionality and `diminishing
returns'. Using these rules, we introduce 4 different logical systems to
implement sequential logic in the memristor and demonstrate how sequential
logic works by instantiating a NOT gate, an AND gate, an XOR gate and a Full
Adder with a single memristor. The Full Adder makes use of the memristor's
short-term memory to add together three binary values and outputs the sum, the
carry digit and even the order they were input in. A memristor full adder also
outputs the arithmetical sum of bits, allowing for a logically (but not
physically) reversible system. Essentially, we can replace an input/output port
with an extra time-step, allowing a single memristor to do a hither-to
unexpectedly large amount of computation. This makes up for the memristor's
slow operation speed and may relate to how neurons do a similarly-large
computation with such slow operations speeds. We propose that using spiking
logic, either in gates or as neuron-analogues, with plastic rewritable
connections between them, would allow the building of a neuromorphic computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05711</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05711</id><created>2015-10-19</created><updated>2015-10-28</updated><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Qualitative Projection Using Deep Neural Networks</title><categories>cs.NE cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNN) abstract by demodulating the output of linear
filters. In this article, we refine this definition of abstraction to show that
the inputs of a DNN are abstracted with respect to the filters. Or, to restate,
the abstraction is qualified by the filters. This leads us to introduce the
notion of qualitative projection. We use qualitative projection to abstract
MNIST hand-written digits with respect to the various dogs, horses, planes and
cars of the CIFAR dataset. We then classify the MNIST digits according to the
magnitude of their dogness, horseness, planeness and carness qualities,
illustrating the generality of qualitative projection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05714</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05714</id><created>2015-10-19</created><updated>2016-01-27</updated><authors><author><keyname>Nasir</keyname><forenames>Muhammad Anis Uddin</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Serafini</keyname><forenames>Marco</forenames></author></authors><title>When Two Choices Are not Enough: Balancing at Scale in Distributed
  Stream Processing</title><categories>cs.DC</categories><comments>12 pages, 14 Figures, this paper is accepted and will be published at
  ICDE 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carefully balancing load in distributed stream processing systems has a
fundamental impact on execution latency and throughput. Load balancing is
challenging because real-world workloads are skewed: some tuples in the stream
are associated to keys which are significantly more frequent than others. Skew
is remarkably more problematic in large deployments: more workers implies fewer
keys per worker, so it becomes harder to &quot;average out&quot; the cost of hot keys
with cold keys.
  We propose a novel load balancing technique that uses a heaving hitter
algorithm to efficiently identify the hottest keys in the stream. These hot
keys are assigned to $d \geq 2$ choices to ensure a balanced load, where $d$ is
tuned automatically to minimize the memory and computation cost of operator
replication. The technique works online and does not require the use of routing
tables. Our extensive evaluation shows that our technique can balance
real-world workloads on large deployments, and improve throughput and latency
by $\mathbf{150\%}$ and $\mathbf{60\%}$ respectively over the previous
state-of-the-art when deployed on Apache Storm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05724</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05724</id><created>2015-10-19</created><updated>2016-01-09</updated><authors><author><keyname>Blondin</keyname><forenames>Michael</forenames></author><author><keyname>Finkel</keyname><forenames>Alain</forenames></author><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Haddad</keyname><forenames>Serge</forenames></author></authors><title>Approaching the Coverability Problem Continuously</title><categories>cs.LO cs.FL</categories><comments>18 pages, 4 figures</comments><acm-class>D.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coverability problem for Petri nets plays a central role in the
verification of concurrent shared-memory programs. However, its high
EXPSPACE-complete complexity poses a challenge when encountered in real-world
instances. In this paper, we develop a new approach to this problem which is
primarily based on applying forward coverability in continuous Petri nets as a
pruning criterion inside a backward coverability framework. A cornerstone of
our approach is the efficient encoding of a recently developed polynomial-time
algorithm for reachability in continuous Petri nets into SMT. We demonstrate
the effectiveness of our approach on standard benchmarks from the literature,
which shows that our approach decides significantly more instances than any
existing tool and is in addition often much faster, in particular on large
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05725</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05725</id><created>2015-10-19</created><authors><author><keyname>Yuan</keyname><forenames>Bofeng</forenames></author><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Replication-based Outer bounds and the Optimality of &quot;Half the Cake&quot; for
  Rank-Deficient MIMO Interference Networks</title><categories>cs.IT math.IT</categories><comments>Presented in part at IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to gain new insights into MIMO interference networks, the optimality
of $\sum_{k=1}^K M_k/2$ (half the cake per user) degrees of freedom is explored
for a $K$-user multiple-input-multiple-output (MIMO) interference channel where
the cross-channels have arbitrary rank constraints, and the $k^{th}$
transmitter and receiver are equipped with $M_k$ antennas each. The result
consolidates and significantly generalizes results from prior studies by
Krishnamurthy et al., of rank-deficient interference channels where all users
have $M$ antennas, and by Tang et al., of full rank interference channels where
the $k^{th}$ user pair has $M_k$ antennas. The broader outcome of this work is
a novel class of replication-based outer bounds for arbitrary rank-constrained
MIMO interference networks where replicas of existing users are added as
auxiliary users and the network connectivity is chosen to ensure that any
achievable scheme for the original network also works in the new network. The
replicated network creates a new perspective of the problem, so that even
simple arguments such as user cooperation become quite powerful when applied in
the replicated network, giving rise to stronger outer bounds, than when applied
directly in the original network. Remarkably, the replication based bounds are
broadly applicable not only to MIMO interference channels with arbitrary
rank-constraints, but much more broadly, even beyond Gaussian settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05727</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05727</id><created>2015-10-19</created><authors><author><keyname>Huck</keyname><forenames>Patrick</forenames></author><author><keyname>Gunter</keyname><forenames>Dan</forenames></author><author><keyname>Cholia</keyname><forenames>Shreyas</forenames></author><author><keyname>Winston</keyname><forenames>Donald</forenames></author><author><keyname>N'Diaye</keyname><forenames>Alpha</forenames></author><author><keyname>Persson</keyname><forenames>Kristin</forenames></author></authors><title>User Applications Driven by the Community Contribution Framework
  MPContribs in the Materials Project</title><categories>cs.CY</categories><comments>12 pages, 5 figures, Proceedings of 10th Gateway Computing
  Environments Workshop (2015), to be published in &quot;Concurrency in Computation:
  Practice and Experience&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work discusses how the MPContribs framework in the Materials Project
(MP) allows user-contributed data to be shown and analyzed alongside the core
MP database. The Materials Project is a searchable database of electronic
structure properties of over 65,000 bulk solid materials that is accessible
through a web-based science-gateway. We describe the motivation for enabling
user contributions to the materials data and present the framework's features
and challenges in the context of two real applications. These use-cases
illustrate how scientific collaborations can build applications with their own
&quot;user-contributed&quot; data using MPContribs. The Nanoporous Materials Explorer
application provides a unique search interface to a novel dataset of hundreds
of thousands of materials, each with tables of user-contributed values related
to material adsorption and density at varying temperature and pressure. The
Unified Theoretical and Experimental x-ray Spectroscopy application discusses a
full workflow for the association, dissemination and combined analyses of
experimental data from the Advanced Light Source with MP's theoretical core
data, using MPContribs tools for data formatting, management and exploration.
The capabilities being developed for these collaborations are serving as the
model for how new materials data can be incorporated into the Materials Project
website with minimal staff overhead while giving powerful tools for data search
and display to the user community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05742</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05742</id><created>2015-10-19</created><authors><author><keyname>Xu</keyname><forenames>Xiangxiang</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Zhang</keyname><forenames>Xiujun</forenames></author><author><keyname>Xu</keyname><forenames>Xibin</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author></authors><title>Joint Deployment of Small Cells and Wireless Backhaul Links in
  Next-Generation Networks</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures. This paper has been accepted by IEEE
  Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel approach for optimizing the joint deployment of small
cell base stations and wireless backhaul links is proposed. This joint
deployment scenario is cast as a multi-objective optimization problem under the
constraints of limited backhaul capacity and outage probability. To address the
problem,a novel adaptive algorithm that integrates $\epsilon$-method,
Lagrangian relaxation and tabu search is proposed to obtain the Pareto optimal
solution set. Simulation results show that the proposed algorithm is quite
effective in finding the optimal solutions. The proposed joint deployment model
can be used for planning small cell networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05746</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05746</id><created>2015-10-19</created><authors><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Yu</keyname><forenames>F. Richard</forenames></author><author><keyname>Ji</keyname><forenames>Hong</forenames></author><author><keyname>Liu</keyname><forenames>Gang</forenames></author><author><keyname>Leung</keyname><forenames>Victor C. M.</forenames></author></authors><title>Distributed Virtual Resource Allocation in Small Cell Networks with Full
  Duplex Self-backhauls and Virtualization</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless network virtualization has attracted great attentions from both
academia and industry. Another emerging technology for next generation wireless
networks is in-band full duplex (FD) communications. Due to its promising
performance, FD communication has been considered as an effective way to
achieve self-backhauls for small cells. In this paper, we introduce wireless
virtualization into small cell networks, and propose a virtualized small cell
network architecture with FD self-backhauls. We formulate the virtual resource
allocation problem in virtualized small cell networks with FD self-backhauls as
an optimization problem. Since the formulated problem is a mixed combinatorial
and non-convex optimization problem, its computational complexity is high.
Moreover, the centralized scheme may suffer from signaling overhead, outdated
dynamics information, and scalability issues. To solve it efficiently, we
divide the original problem into two subproblems. For the first subproblem, we
transfer it to a convex optimization problem, and then solve it by an efficient
alternating direction method of multipliers (ADMM)-based distributed algorithm.
The second subproblem is a convex problem, which can be solved by each
infrastructure provider. Extensive simulations are conducted with different
system configurations to show the effectiveness of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05751</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05751</id><created>2015-10-20</created><authors><author><keyname>Ghosh</keyname><forenames>Debojyoti</forenames></author><author><keyname>Constantinescu</keyname><forenames>Emil M.</forenames></author></authors><title>Semi-Implicit Time Integration of Atmospheric Flows with
  Characteristic-Based Flux Partitioning</title><categories>cs.CE math.NA</categories><msc-class>65M-06, 86A-10, 76N-15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a characteristic-based flux partitioning for the
semi-implicit time integration of atmospheric flows. Nonhydrostatic models
require the solution of the compressible Euler equations. The acoustic
time-scale is significantly faster than the advective scale, yet it is
typically not relevant to atmospheric and weather phenomena. The acoustic and
advective components of the hyperbolic flux are separated in the characteristic
space. High-order, conservative additive Runge-Kutta methods are applied to the
partitioned equations so that the acoustic component is integrated in time
implicitly with an unconditionally stable method, while the advective component
is integrated explicitly. The time step of the overall algorithm is thus
determined by the advective scale. Benchmark flow problems are used to
demonstrate the accuracy, stability, and convergence of the proposed algorithm.
The computational cost of the partitioned semi-implicit approach is compared
with that of explicit time integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05763</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05763</id><created>2015-10-20</created><authors><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Cho</keyname><forenames>Jaehee</forenames></author><author><keyname>Everett</keyname><forenames>Andr&#xe9; M.</forenames></author></authors><title>Clarifying the Role of Distance in Friendships on Twitter: Discovery of
  a Double Power-Law Relationship</title><categories>cs.SI physics.data-an</categories><comments>7 pages, 1 figure, To be presented at the 23rd ACM SIGSPATIAL
  International Conference on Advances in Geographic Information Systems (ACM
  SIGSPATIAL 2015), Seattle, WA USA, November 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study analyzes friendships in online social networks involving
geographic distance with a geo-referenced Twitter dataset, which provides the
exact distance between corresponding users. We start by introducing a strong
definition of &quot;friend&quot; on Twitter, requiring bidirectional communication. Next,
by utilizing geo-tagged mentions delivered by users to determine their
locations, we introduce a two-stage distance estimation algorithm. As our main
contribution, our study provides the following newly-discovered friendship
degree related to the issue of space: The number of friends according to
distance follows a double power-law (i.e., a double Pareto law) distribution,
indicating that the probability of befriending a particular Twitter user is
significantly reduced beyond a certain geographic distance between users,
termed the separation point. Our analysis provides much more fine-grained
social ties in space, compared to the conventional results showing a
homogeneous power-law with distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05773</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05773</id><created>2015-10-20</created><authors><author><keyname>Lou</keyname><forenames>Youcheng</forenames></author><author><keyname>Hong</keyname><forenames>Yiguang</forenames></author></authors><title>Distributed Surrounding Design of Target Region with Complex Adjacency
  Matrices</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a complete version of the 6-page IEEE TAC technical note [1]. In this
paper, we consider the distributed surrounding of a convex target set by a
group of agents with switching communication graphs. We propose a distributed
controller to surround a given set with the same distance and desired
projection angles specified by a complex-value adjacency matrix. Under mild
connectivity assumptions, we give results in both consistent and inconsistent
cases for the set surrounding in a plane. Also, we provide sufficient
conditions for the multi-agent coordination when the convex set contains only
the origin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05774</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05774</id><created>2015-10-20</created><authors><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author><author><keyname>Laursen</keyname><forenames>Simon</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Limit Your Consumption! Finding Bounds in Average-energy Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy games are infinite two-player games played in weight\-ed arenas with
quantitative objectives that restrict the consumption of a resource modeled by
the weights, e.g., a battery that is charged and drained. Typically, upper
and/or lower bounds on the battery capacity are part of the problem
description. In this work, we consider the problem of determining upper bounds
on the average accumulated energy or on the capacity while satisfying a given
lower bound, i.e., we do not determine whether a given bound is sufficient to
meet the specification, but if there exists a bound that is sufficient to meet
it.
  In the classical setting with positive and negative weights, we show that the
problem of determining the existence of a sufficient bound on the long-run
average accumulated energy can be solved in doubly-exponential time. Then, we
consider recharge games: here, all weights are negative, but there are recharge
edges that recharge the energy to some fixed capacity. We show that bounding
the long-run average energy in such games is complete for exponential time.
Then, we consider the existential version of the problem, which turns out to be
solvable in polynomial time: here, we ask whether there is a recharge capacity
that allows the system player to win the game.
  We conclude by studying tradeoffs between the memory needed to implement
strategies and the bounds they realize. We give an example showing that memory
can be traded for bounds and vice versa. Also, we show that increasing the
capacity allows to lower the average accumulated energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05784</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05784</id><created>2015-10-20</created><authors><author><keyname>Sootla</keyname><forenames>Aivar</forenames></author><author><keyname>Anderson</keyname><forenames>James</forenames></author></authors><title>Structured Projection-Based Model Reduction with Application to
  Stochastic Biochemical Networks</title><categories>math.OC cs.SY q-bio.QM</categories><comments>13 pages; 7 figures; submitted to IEEE Transaction on Automatic
  Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Chemical Master Equation (CME) is well known to provide the highest
resolution models of a biochemical reaction network. Unfortunately, even
simulating the CME can be a challenging task. For this reason more simple
approximations to the CME have been proposed. In this work we focus on one such
model, the Linear Noise Approximation. Specifically, we consider implications
of a recently proposed LNA time-scale separation method. We show that the
reduced order LNA converges to the full order model in the mean square sense.
Using this as motivation we derive a network structure preserving reduction
algorithm based on structured projections. We present convex optimisation
algorithms that describe how such projections can be computed and we discuss
when structured solutions exits. We also show that for a certain class of
systems, structured projections can be found using basic linear algebra and no
optimisation is necessary. The algorithms are then applied to a linearised
stochastic LNA model of the yeast glycolysis pathway.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05811</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05811</id><created>2015-10-20</created><updated>2015-11-11</updated><authors><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author><author><keyname>Monshizadeh</keyname><forenames>Nima</forenames></author></authors><title>A modular design of incremental Lyapunov functions for microgrid control
  with power sharing</title><categories>math.OC cs.SY</categories><comments>Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we contribute a theoretical framework that sheds a new light on
the problem of microgrid analysis and control. The starting point is an energy
function comprising the kinetic energy associated with the elements that
emulate the rotating machinery and terms taking into account the reactive power
stored in the lines and dissipated on shunt elements. We then shape this energy
function with the addition of an adjustable voltage-dependent term, and
construct incremental storage functions satisfying suitable dissipation
inequalities. Our choice of the voltage-dependent term depends on the voltage
dynamics/controller under investigation. Several microgrids dynamics that have
similarities or coincide with dynamics already considered in the literature are
captured in our incremental energy analysis framework. The twist with respect
to existing results is that our incremental storage functions allow for a large
signal analysis of the coupled microgrid obviating the need for simplifying
linearization techniques and for the restrictive decoupling assumption in which
the frequency dynamics is fully separated from the voltage one. A complete
Lyapunov stability analysis of the various systems is carried out along with a
discussion on their active and reactive power sharing properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05816</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05816</id><created>2015-10-20</created><authors><author><keyname>Park</keyname><forenames>Jeonghoon</forenames></author><author><keyname>Lee</keyname><forenames>Soojoon</forenames></author></authors><title>Quantum non-signalling assisted zero-error classical capacity of qubit
  channels</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, no figure</comments><msc-class>81P45 (Primary) 94A15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explicitly evaluate the one-shot quantum non-signalling
assisted zero-error classical capacities $\M_0^{\mathrm{QNS}}$ for qubit
channels. In particular, we show that for nonunital qubit channels,
$\M_0^{\mathrm{QNS}}=1$, which implies that in the one-shot setting, nonunital
qubit channels cannot transmit any information with zero probability of error
even when assisted by quantum non-signalling correlations. Furthermore, we show
that for qubit channels, $\M_0^{\mathrm{QNS}}$ equals to the one-shot
entanglement-assisted zero-error classical capacities. This means that for a
single use of a qubit channel, quantum non-signalling correlations are not more
powerful than shared entanglement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05822</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05822</id><created>2015-10-20</created><authors><author><keyname>Gibert</keyname><forenames>Xavier</forenames></author><author><keyname>Patel</keyname><forenames>Vishal M.</forenames></author><author><keyname>Chellappa</keyname><forenames>Rama</forenames></author></authors><title>Sequential Score Adaptation with Extreme Value Theory for Robust Railway
  Track Inspection</title><categories>cs.CV</categories><comments>To be presented at the 3rd Workshop on Computer Vision for Road Scene
  Understanding and Autonomous Driving (CVRSUAD 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Periodic inspections are necessary to keep railroad tracks in state of good
repair and prevent train accidents. Automatic track inspection using machine
vision technology has become a very effective inspection tool. Because of its
non-contact nature, this technology can be deployed on virtually any railway
vehicle to continuously survey the tracks and send exception reports to track
maintenance personnel. However, as appearance and imaging conditions vary,
false alarm rates can dramatically change, making it difficult to select a good
operating point. In this paper, we use extreme value theory (EVT) within a
Bayesian framework to optimally adjust the sensitivity of anomaly detectors. We
show that by approximating the lower tail of the probability density function
(PDF) of the scores with an Exponential distribution (a special case of the
Generalized Pareto distribution), and using the Gamma conjugate prior learned
from the training data, it is possible to reduce the variability in false alarm
rate and improve the overall performance. This method has shown an increase in
the defect detection rate of rail fasteners in the presence of clutter (at PFA
0.1%) from 95.40% to 99.26% on the 85-mile Northeast Corridor (NEC) 2012-2013
concrete tie dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05824</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05824</id><created>2015-10-20</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Finite dynamical systems, hat games, and coding theory</title><categories>cs.IT cs.DM math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamical properties of finite dynamical systems (FDSs) have been
investigated in the context of coding theoretic problems, such as network
coding and index coding, and in the context of hat games, such as the guessing
game and Winkler's hat game. In this paper, we relate the problems mentioned
above to properties of FDSs, including the number of fixed points, their
stability, and their instability. We first introduce the guessing dimension and
the coset dimension of an FDS and their counterparts for digraphs. Based on the
coset dimension, we then strengthen the existing equivalences between network
coding and index coding. We also introduce the instability of FDSs and we study
the stability and the instability of digraphs. We prove that the instability
always reaches the size of a minimum feedback vertex set. We also obtain some
non-stable bounds independent of the number of vertices of the graph. We then
relate the stability and the instability to the guessing number. We also
exhibit a class of sparse graphs with large girth that have high stability and
high instability; our approach is code-theoretic and uses the guessing
dimension. Finally, we prove that the affine instability is always
asymptotically greater than or equal to the linear guessing number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05828</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05828</id><created>2015-10-20</created><authors><author><keyname>Dewan</keyname><forenames>Prateek</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Hiding in Plain Sight: The Anatomy of Malicious Facebook Pages</title><categories>cs.SI</categories><comments>11 pages, 9 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facebook is the world's largest Online Social Network, having more than 1
billion users. Like most other social networks, Facebook is home to various
categories of hostile entities who abuse the platform by posting malicious
content. In this paper, we identify and characterize Facebook pages that engage
in spreading URLs pointing to malicious domains. We used the Web of Trust API
to determine domain reputations of URLs published by pages, and identified 627
pages publishing untrustworthy information, misleading content, adult and child
unsafe content, scams, etc. which are deemed as &quot;Page Spam&quot; by Facebook, and do
not comply with Facebook's community standards. Our findings revealed dominant
presence of politically polarized entities engaging in spreading content from
untrustworthy web domains. Anger and religion were the most prominent topics in
the textual content published by these pages. We found that at least 8% of all
malicious pages were dedicated to promote a single malicious domain. Studying
the temporal posting activity of pages revealed that malicious pages were more
active than benign pages. We further identified collusive behavior within a set
of malicious pages spreading adult and pornographic content. We believe our
findings will enable technologists to devise efficient automated solutions to
identify and curb the spread of malicious content through such pages. To the
best of our knowledge, this is the first attempt in literature, focused
exclusively on characterizing malicious Facebook pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05830</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05830</id><created>2015-10-20</created><updated>2016-02-23</updated><authors><author><keyname>Jaffe</keyname><forenames>Ariel</forenames></author><author><keyname>Fetaya</keyname><forenames>Ethan</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Jiang</keyname><forenames>Tingting</forenames></author><author><keyname>Kluger</keyname><forenames>Yuval</forenames></author></authors><title>Unsupervised Ensemble Learning with Dependent Classifiers</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In unsupervised ensemble learning, one obtains predictions from multiple
sources or classifiers, yet without knowing the reliability and expertise of
each source, and with no labeled data to assess it. The task is to combine
these possibly conflicting predictions into an accurate meta-learner. Most
works to date assumed perfect diversity between the different sources, a
property known as conditional independence. In realistic scenarios, however,
this assumption is often violated, and ensemble learners based on it can be
severely sub-optimal. The key challenges we address in this paper are:\ (i) how
to detect, in an unsupervised manner, strong violations of conditional
independence; and (ii) construct a suitable meta-learner. To this end we
introduce a statistical model that allows for dependencies between classifiers.
Our main contributions are the development of novel unsupervised methods to
detect strongly dependent classifiers, better estimate their accuracies, and
construct an improved meta-learner. Using both artificial and real datasets, we
showcase the importance of taking classifier dependencies into account and the
competitive performance of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05833</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05833</id><created>2015-10-20</created><authors><author><keyname>ShenTu</keyname><forenames>QingChun</forenames></author><author><keyname>Yu</keyname><forenames>JianPing</forenames></author></authors><title>A Blind-Mixing Scheme for Bitcoin based on an Elliptic Curve
  Cryptography Blind Digital Signature Algorithm</title><categories>cs.CR</categories><comments>17 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To strengthen the anonymity of Bitcoin, several centralized coin-mixing
providers (mixers) such as BitcoinFog.com, BitLaundry.com, and Blockchain.info
assist users to mix Bitcoins through CoinJoin transactions with multiple inputs
and multiple outputs to uncover the relationship between them. However, these
mixers know the output address of each user, such that they cannot provide true
anonymity. This paper proposes a centralized coin-mixing algorithm based on an
elliptic curve blind signature scheme (denoted as Blind-Mixing) that obstructs
mixers from linking an input address with an output address. Comparisons among
three blind signature based algorithms, Blind-Mixing, BlindCoin, and RSA
Coin-Mixing, are conducted. It is determined that BlindCoin may be deanonymized
because of its use of a public log. In RSA Coin-Mixing, a user's Bitcoins may
be falsely claimed by another. In addition, the blind signature scheme of
Blind-Mixing executes 10.5 times faster than that of RSA Coin-Mixing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05836</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05836</id><created>2015-10-20</created><authors><author><keyname>Kaplan</keyname><forenames>Marc</forenames></author><author><keyname>Leurent</keyname><forenames>Ga&#xeb;tan</forenames></author><author><keyname>Leverrier</keyname><forenames>Anthony</forenames></author><author><keyname>Naya-Plasencia</keyname><forenames>Mar&#xed;a</forenames></author></authors><title>Quantum Differential and Linear Cryptanalysis</title><categories>quant-ph cs.CR</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computers, that may become available one day, will impact many
scientific fields. Cryptography is certainly one of them since many asymmetric
primitives would become insecure against an adversary with quantum
capabilities. Cryptographers are already anticipating this threat by proposing
and studying a number of potentially quantum-safe alternatives for those
primitives. On the other hand, the situation of symmetric primitives which seem
less vulnerable against quantum computing, has received much less attention.
  We need to prepare symmetric cryptography for the eventual arrival of the
post-quantum world, as it is done with other cryptography branches.
Cryptanalysis and security analysis are the only proper way to evaluate the
security of symmetric primitives: our trust in specific ciphers relies on their
ability to resist all known cryptanalysis tools. This requires a proper
investigation of the toolkit of quantum cryptanalysis, that might include
radically new attacks. This toolkit has not been much developed so far. In this
paper, we study how some of the main cryptanalytic attacks behave in the
post-quantum world. More specifically, we consider here quantum versions of
differential and linear cryptanalysis.
  While running Grover's search algorithm on a quantum computer brings a
quadratic speedup for brute-force attacks, we show that the situation is more
subtle when considering specific cryptanalysis techniques. In particular, we
give the quantum version of various classes of differential and linear attacks
and show that the best attacks in the classical world do not necessarily lead
to the best quantum ones. Some non-intuitive examples of application on ciphers
LAC and KLEIN are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05860</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05860</id><created>2015-10-20</created><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author></authors><title>Dynamic Spectrum Management: A Complete Complexity Characterization</title><categories>cs.IT cs.CC math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a multi-user multi-carrier communication system where multiple users
share multiple discrete subcarriers. To achieve high spectrum efficiency, the
users in the system must choose their transmit power dynamically in response to
fast channel fluctuations. Assuming perfect channel state information, two
formulations for the spectrum management (power control) problem are considered
in this paper: the first is to minimize the total transmission power subject to
all users' transmission data rate constraints, and the second is to maximize
the min-rate utility subject to individual power constraints at each user. It
is known in the literature that both formulations of the problem are polynomial
time solvable when the number of subcarriers is one and strongly NP-hard when
the number of subcarriers are greater than or equal to three. However, the
complexity characterization of the problem when the number of subcarriers is
two has been missing for a long time. This paper answers this long-standing
open question: both formulations of the problem are strongly NP-hard when the
number of subcarriers is two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05862</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05862</id><created>2015-10-20</created><updated>2015-11-20</updated><authors><author><keyname>Baptista</keyname><forenames>M. S.</forenames></author><author><keyname>Szmoski</keyname><forenames>R. M.</forenames></author><author><keyname>Pereira</keyname><forenames>R. F.</forenames></author><author><keyname>Pinto</keyname><forenames>S. E. de Souza</forenames></author></authors><title>Chaotic, informational and synchronous behaviour of multiplex networks</title><categories>nlin.CD cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The understanding of the relationship between topology and behaviour in
interconnected networks would allow to characterise and predict behaviour in
many real complex networks since both are usually not simultaneously known.
Most previous studies have focused on the relationship between topology and
synchronisation. In this work, we provide analytical formulas that shows how
topology drives complex behaviour: chaos, information, and weak or strong
synchronisation; in multiplex networks with constant Jacobian. We also study
this relationship numerically in multiplex networks of Hindmarsh-Rose neurons.
Whereas behaviour in the analytically tractable network is a direct but not
trivial consequence of the spectra of eigenvalues of the Laplacian matrix,
where behaviour may strongly depend on the break of symmetry in the topology of
interconnections, in Hindmarsh-Rose neural networks the nonlinear nature of the
chemical synapses breaks the elegant mathematical connection between the
spectra of eigenvalues of the Laplacian matrix and the behaviour of the
network, creating networks whose behaviour strongly depends on the nature
(chemical or electrical) of the inter synapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05879</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05879</id><created>2015-10-20</created><authors><author><keyname>Wittner</keyname><forenames>Christian</forenames></author><author><keyname>Schauerte</keyname><forenames>Boris</forenames></author><author><keyname>Stiefelhagen</keyname><forenames>Rainer</forenames></author></authors><title>What's the point? Frame-wise Pointing Gesture Recognition with
  Latent-Dynamic Conditional Random Fields</title><categories>cs.HC cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Latent-Dynamic Conditional Random Fields to perform skeleton-based
pointing gesture classification at each time instance of a video sequence,
where we achieve a frame-wise pointing accuracy of roughly 83%. Subsequently,
we determine continuous time sequences of arbitrary length that form individual
pointing gestures and this way reliably detect pointing gestures at a false
positive detection rate of 0.63%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05880</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05880</id><created>2015-10-20</created><authors><author><keyname>Junges</keyname><forenames>Sebastian</forenames></author><author><keyname>Jansen</keyname><forenames>Nils</forenames></author><author><keyname>Dehnert</keyname><forenames>Christian</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author><author><keyname>Katoen</keyname><forenames>Joost-Pieter</forenames></author></authors><title>Safety-Constrained Reinforcement Learning for MDPs</title><categories>cs.SE cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider controller synthesis for stochastic and partially unknown
environments in which safety is essential. Specifically, we abstract the
problem as a Markov decision process in which the expected performance is
measured using a cost function that is unknown prior to run-time exploration of
the state space. Standard learning approaches synthesize cost-optimal
strategies without guaranteeing safety properties. To remedy this, we first
compute safe, permissive strategies. Then, exploration is constrained to these
strategies and thereby meets the imposed safety requirements. Exploiting an
iterative learning procedure, the resulting policy is safety-constrained and
optimal. We show correctness and completeness of the method and discuss the use
of several heuristics to increase its scalability. Finally, we demonstrate the
applicability by means of a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05886</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05886</id><created>2015-10-20</created><authors><author><keyname>Zhang</keyname><forenames>Zhao</forenames></author><author><keyname>Zhou</keyname><forenames>Jiao</forenames></author><author><keyname>Ko</keyname><forenames>Ker-I</forenames></author><author><keyname>Du</keyname><forenames>Ding-zhu</forenames></author></authors><title>Approximation Algorithm for Minimum Weight Connected $m$-Fold Dominating
  Set</title><categories>cs.DM cs.DS</categories><msc-class>68W25, 05C85</msc-class><acm-class>G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using connected dominating set (CDS) to serve as a virtual backbone in a
wireless networks can save energy and reduce interference. Since nodes may fail
due to accidental damage or energy depletion, it is desirable that the virtual
backbone has some fault-tolerance. A $k$-connected $m$-fold dominating set
($(k,m)$-CDS) of a graph $G$ is a node set $D$ such that every node in
$V\setminus D$ has at least $m$ neighbors in $D$ and the subgraph of $G$
induced by $D$ is $k$-connected. Using $(k,m)$-CDS can tolerate the failure of
$\min\{k-1,m-1\}$ nodes. In this paper, we study Minimum Weight $(1,m)$-CDS
problem ($(1,m)$-MWCDS), and present an
$(H(\delta+m)+2H(\delta-1))$-approximation algorithm, where $\delta$ is the
maximum degree of the graph and $H(\cdot)$ is the Harmonic number. Notice that
there is a $1.35\ln n$-approximation algorithm for the $(1,1)$-MWCDS problem,
where $n$ is the number of nodes in the graph. Though our constant in $O(\ln
\cdot)$ is larger than 1.35, $n$ is replaced by $\delta$. Such a replacement
enables us to obtain a $(6.67+\varepsilon)$-approximation for the $(1,m)$-MWCDS
problem on unit disk graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05891</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05891</id><created>2015-10-20</created><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Brandenburg</keyname><forenames>Franz J.</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author></authors><title>On the Book Thickness of 1-Planar Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a book embedding of a graph G, the vertices of G are placed in order along
a straight-line called spine of the book, and the edges of G are drawn on a set
of half-planes, called the pages of the book, such that two edges drawn on a
page do not cross each other. The minimum number of pages in which a graph can
be embedded is called the book-thickness or the page-number of the graph. It is
known that every planar graph has a book embedding on at most four pages. Here
we investigate the book-embeddings of 1-planar graphs. A graph is 1-planar if
it can be drawn in the plane such that each edge is crossed at most once. We
prove that every 1-planar graph has a book embedding on at most 16 pages and
every 3-connected 1-planar graph has a book embedding on at most 12 pages. The
drawings can be computed in linear time from any given 1-planar embedding of
the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05893</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05893</id><created>2015-10-20</created><updated>2016-02-23</updated><authors><author><keyname>Thouvenin</keyname><forenames>Pierre-Antoine</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Tourneret</keyname><forenames>Jean-Yves</forenames></author></authors><title>Online Unmixing of Multitemporal Hyperspectral Images accounting for
  Spectral Variability</title><categories>physics.data-an cs.CV stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral unmixing is aimed at identifying the reference spectral
signatures composing an hyperspectral image and their relative abundance
fractions in each pixel. In practice, the identified signatures may vary
spectrally from an image to another due to varying acquisition conditions, thus
inducing possibly significant estimation errors. Against this background,
hyperspectral unmixing of several images acquired over the same area is of
considerable interest. Indeed, such an analysis enables the endmembers of the
scene to be tracked and the corresponding endmember variability to be
characterized. Sequential endmember estimation from a set of hyperspectral
images is expected to provide improved performance when compared to methods
analyzing the images independently. However, the significant size of
hyperspectral data precludes the use of batch procedures to jointly estimate
the mixture parameters of a sequence of hyperspectral images. Provided that
each elementary component is present in at least one image of the sequence, we
propose to perform an online hyperspectral unmixing accounting for temporal
endmember variability. The online hyperspectral unmixing is formulated as a
two-stage stochastic program, which can be solved using a stochastic
approximation. The performance of the proposed method is evaluated on synthetic
and real data. A comparison with independent unmixings of each image by
established methods finally illustrates the interest of the proposed strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05911</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05911</id><created>2015-10-20</created><authors><author><keyname>Shi</keyname><forenames>Baoxu</forenames></author><author><keyname>Weninger</keyname><forenames>Tim</forenames></author></authors><title>Fact Checking in Large Knowledge Graphs - A Discriminative Predicate
  Path Mining Approach</title><categories>cs.DB cs.AI cs.IR cs.SI</categories><comments>15 pages</comments><acm-class>H.4; H.2.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Traditional fact checking by experts and analysts cannot keep pace with the
volume of newly created information. It is important and necessary, therefore,
to enhance our ability to computationally determine whether some statement of
fact is true or false. We view this problem as a link-prediction task in a
knowledge graph, and show that a new model of the top discriminative predicate
paths is able to understand the meaning of some statement and accurately
determine its veracity. We evaluate our approach by examining thousands of
claims related to history, geography, biology, and politics using a public,
million node knowledge graph extracted from Wikipedia and PubMedDB. Not only
does our approach significantly outperform related models, we also find that
the discriminative predicate path model is easily interpretable and provides
sensible reasons for the final determination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05937</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05937</id><created>2015-10-20</created><authors><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Xing</keyname><forenames>Chao</forenames></author><author><keyname>Yu</keyname><forenames>Kaimin</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author></authors><title>Binary Speaker Embedding</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The popular i-vector model represents speakers as lowdimensional continuous
vectors (i-vectors), and hence is a way of continuous speaker embedding. In
this paper, we investigate binary speaker embedding, which transforms ivectors
to binary vectors (codes) by a hash function. We start from locality sensitive
hashing (LSH), a simple binarization approach where binary codes are derived
from a set of random hash functions. A potential problem of LSH is that the
randomly sampled hash functions might be suboptimal, we therefore propose an
improved Hamming distance learning approach, where the hash function is learned
by a variablesized block training that projects each dimension of the original
i-vectors to variable-sized binary codes independently. Our experiments show
that binary speaker embedding can deliver competitive or even better results on
both speaker verification and identification tasks, while the memory usage and
the computation cost are significant reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05938</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05938</id><created>2015-10-20</created><authors><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Stefanatos</keyname><forenames>Stelios</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Ultra Dense Networks: The New Wireless Frontier for Enabling 5G Access</title><categories>cs.IT cs.NI math.IT</categories><comments>to appear in IEEE Vehicular Technology Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extreme traffic load that future wireless networks are expected to
accommodate requires a re-thinking of the system design. Initial estimations
indicate that, different from the evolutionary path of previous cellular
generations that was based on spectral efficiency improvements, the most
substantial amount of future system performance gains will be obtained by means
of network infrastructure densification. By increasing the density of
operator-deployed infrastructure elements, along with incorporation of
user-deployed access nodes and mobile user devices acting as &quot;infrastructure
prosumers&quot;, it is expected that having one or more access nodes exclusively
dedicated to each user will become feasible, introducing the ultra dense
network (UDN) paradigm. Although it is clear that UDNs are able to take
advantage of the significant benefits provided by proximal transmissions and
increased spatial reuse of system resources, at the same time, large node
density and irregular deployment introduce new challenges, mainly due to the
interference environment characteristics that are vastly different from
previous cellular deployments. This article attempts to provide insights on
fundamental issues related to UDN deployment, such as determining the
infrastructure density required to support given traffic load requirements and
the benefits of network-wise coordination, demonstrating the potential of UDNs
for 5G wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05940</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05940</id><created>2015-10-20</created><authors><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Xing</keyname><forenames>Chao</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author></authors><title>Max-margin Metric Learning for Speaker Recognition</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic linear discriminant analysis (PLDA) is among the most popular
methods that accompany the i-vector model to deliver state-of-the-art
performance for speaker recognition. A potential problem of the PLDA model,
however, is that it essentially assumes strong Gaussian distributions over
i-vectors as well as speaker mean vectors, and the objective function is not
directly related to the goal of the task, e.g., discriminating true speakers
and imposters. We propose a max-margin metric learning approach to solve the
problem. It learns a linear transform with the criterion that target trials and
imposter trials are discriminated from each other by a large margin.
Experiments conducted on the SRE08 core test show that this new approach
achieves a performance comparable to or even better than PLDA, though the
scoring is as simple as a cosine computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05942</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05942</id><created>2015-10-20</created><updated>2015-10-29</updated><authors><author><keyname>Kochergin</keyname><forenames>Vadim V.</forenames></author><author><keyname>Mikhailovich</keyname><forenames>Anna V.</forenames></author></authors><title>Inversion Complexity of Functions of Multi-Valued Logic</title><categories>cs.DM</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum number of NOT gates in a logic circuit computing a Boolean
function is called the inversion complexity of the function. In 1957, A. A.
Markov determined the inversion complexity of every Boolean function and proved
that $\lceil\log_{2}(d(f)+1)\rceil$ NOT gates are necessary and sufficient to
compute any Boolean function $f$ (where $d(f)$ is the maximum number of value
changes from greater to smaller over all increasing chains of tuples of
variables values). This result is extended to $k$-valued functions computing in
this paper. Thereupon one can use monotone functions &quot;for free&quot; like in the
Boolean case. It is shown that the minimum sufficient number of non-monotone
gates for the realization of the arbitrary $k$-valued logic function $f$ is
equal to $\lceil\log_{2}(d(f)+1)\rceil$ if Post negation (function $x+1
\pmod{k}$) is used in NOT nodes and is also equal to
$\lceil\log_{k}(d(f)+1)\rceil$, if {\L}ukasiewicz negation (function $k-1-x$)
is used in NOT nodes. Similar extension for another classical result of A. A.
Markov for the inversion complexity of a system of Boolean functions to
$k$-valued logic functions has been obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05956</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05956</id><created>2015-10-20</created><updated>2015-12-20</updated><authors><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Optimal Cluster Recovery in the Labeled Stochastic Block Model</title><categories>math.PR cs.LG cs.SI stat.ML</categories><comments>33 pages. arXiv admin note: text overlap with arXiv:1412.7335</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of community detection or clustering in the labeled
Stochastic Block Model (labeled SBM) with a finite number $K$ of clusters of
sizes linearly growing with the global population of items $n$. Every pair of
items is labeled independently at random, and label $\ell$ appears with
probability $p(i,j,\ell)$ between two items in clusters indexed by $i$ and $j$,
respectively. The objective is to reconstruct the clusters from the observation
of these random labels.
  Clustering under the SBM and their extensions has attracted much attention
recently. Most existing work aimed at characterizing the set of parameters such
that it is possible to infer clusters either positively correlated with the
true clusters, or with a vanishing proportion of misclassified items, or
exactly matching the true clusters.
  We address the finer and more challenging question of determining, under the
general LSBM and for any $s$, the set of parameters such that there exists a
polynomial-time clustering algorithm with at most $s$ misclassified items in
average. We prove that in the regime where it is possible to recover the
clusters with a vanishing proportion of misclassified items, a necessary and
sufficient condition to get $s=o(n)$ misclassified items in average is $\frac{n
D(\alpha,p)}{ \log (n/s)} \ge 1$, where $D(\alpha,p)$ is an appropriately
defined function of the parameters $p=(p(i,j,\ell), i,j, \ell)$, and $\alpha$
defining the sizes of the clusters. We further develop an algorithm, based on
simple spectral methods, that achieves this fundamental performance limit. The
analysis presented in this paper allows us to recover existing results for
asymptotically accurate and exact cluster recovery in the SBM, but has much
broader applications. For example, it implies that the minimal number of
misclassified items under the LSBM considered scales as
$n\exp(-nD(\alpha,p)(1+o(1)))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05961</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05961</id><created>2015-10-20</created><authors><author><keyname>Turgut</keyname><forenames>Esma</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author></authors><title>Energy Efficiency in Relay-Assisted mmWave Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, energy efficiency of relay-assisted millimeter wave (mmWave)
cellular networks with Poisson Point Process (PPP) distributed base stations
(BSs) and relay stations (RSs) is analyzed using tools from stochastic
geometry. The distinguishing features of mmWave communications such as
directional beamforming and having different path loss laws for line-of-sight
(LOS) and non-line-of-sight (NLOS) links are incorporated into the energy
efficiency analysis. Following the description of the system model for mmWave
cellular networks, coverage probabilities are computed for each link.
Subsequently, average power consumption of BSs and RSs are modeled and energy
efficiency is determined in terms of system parameters. Energy efficiency in
the presence of beamforming alignment errors is also investigated to get
insight on the performance in practical scenarios. Finally, the impact of BS
and RS densities, antenna gains, main lobe beam widths, LOS interference range,
and alignment errors on the energy efficiency is analyzed via numerical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05963</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05963</id><created>2015-10-20</created><authors><author><keyname>Sheth</keyname><forenames>Amit</forenames></author><author><keyname>Anantharam</keyname><forenames>Pramod</forenames></author><author><keyname>Henson</keyname><forenames>Cory</forenames></author></authors><title>Semantic, Cognitive, and Perceptual Computing: Advances toward Computing
  for Human Experience</title><categories>cs.AI</categories><comments>13 pages, 4 Figures, IEEE Computer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web continues to evolve and serve as the infrastructure for
carrying massive amounts of multimodal and multisensory observations. These
observations capture various situations pertinent to people's needs and
interests along with all their idiosyncrasies. To support human-centered
computing that empower people in making better and timely decisions, we look
towards computation that is inspired by human perception and cognition. Toward
this goal, we discuss computing paradigms of semantic computing, cognitive
computing, and an emerging aspect of computing, which we call perceptual
computing. In our view, these offer a continuum to make the most out of vast,
growing, and diverse data pertinent to human needs and interests. We propose
details of perceptual computing characterized by interpretation and exploration
operations comparable to the interleaving of bottom and top brain processing.
  This article consists of two parts. First we describe semantic computing,
cognitive computing, and perceptual computing to lay out distinctions while
acknowledging their complementary capabilities. We then provide a conceptual
overview of the newest of these three paradigms--perceptual computing. For
further insights, we focus on an application scenario of asthma management
converting massive, heterogeneous and multimodal (big) data into actionable
information or smart data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05970</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05970</id><created>2015-10-20</created><authors><author><keyname>&#x17d;bontar</keyname><forenames>Jure</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Stereo Matching by Training a Convolutional Neural Network to Compare
  Image Patches</title><categories>cs.CV cs.LG cs.NE</categories><comments>Submitted to the Journal of Machine Learning Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for extracting depth information from a rectified image
pair. Our approach focuses on the first stage of many stereo algorithms: the
matching cost computation. We approach the problem by learning a similarity
measure on small image patches using a convolutional neural network. Training
is carried out in a supervised manner by constructing a binary classification
data set with examples of similar and dissimilar pairs of patches. We examine
two network architectures for this task: one tuned for speed, the other for
accuracy. The output of the convolutional neural network is used to initialize
the stereo matching cost. A series of post-processing steps follow: cross-based
cost aggregation, semiglobal matching, a left-right consistency check, subpixel
enhancement, a median filter, and a bilateral filter. We evaluate our method on
the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it
outperforms other approaches on all three data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05976</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05976</id><created>2015-10-20</created><authors><author><keyname>Liu</keyname><forenames>Li-Ping</forenames></author><author><keyname>Dietterich</keyname><forenames>Thomas G.</forenames></author><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Transductive Optimization of Top k Precision</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a binary classification problem in which the learner is given a
labeled training set, an unlabeled test set, and is restricted to choosing
exactly $k$ test points to output as positive predictions. Problems of this
kind---{\it transductive precision@$k$}---arise in information retrieval,
digital advertising, and reserve design for endangered species. Previous
methods separate the training of the model from its use in scoring the test
points. This paper introduces a new approach, Transductive Top K (TTK), that
seeks to minimize the hinge loss over all training instances under the
constraint that exactly $k$ test instances are predicted as positive. The paper
presents two optimization methods for this challenging problem. Experiments and
analysis confirm the importance of incorporating the knowledge of $k$ into the
learning process. Experimental evaluations of the TTK approach show that the
performance of TTK matches or exceeds existing state-of-the-art methods on 7
UCI datasets and 3 reserve design problem instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.05981</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.05981</id><created>2015-10-20</created><authors><author><keyname>Souza</keyname><forenames>Roberto C. S. N. P.</forenames></author><author><keyname>de Brito</keyname><forenames>Denise E. F</forenames></author><author><keyname>Assun&#xe7;&#xe3;o</keyname><forenames>Renato M.</forenames></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><suffix>Jr</suffix></author></authors><title>A latent shared-component generative model for real-time disease
  surveillance using Twitter data</title><categories>cs.SI stat.ML</categories><comments>Appears in 2nd ACM SIGKDD Workshop on Connected Health at Big Data
  Era (BigCHat)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting the large amount of available data for addressing relevant social
problems has been one of the key challenges in data mining. Such efforts have
been recently named &quot;data science for social good&quot; and attracted the attention
of several researchers and institutions. We give a contribution in this
objective in this paper considering a difficult public health problem, the
timely monitoring of dengue epidemics in small geographical areas. We develop a
generative simple yet effective model to connect the fluctuations of disease
cases and disease-related Twitter posts. We considered a hidden Markov process
driving both, the fluctuations in dengue reported cases and the tweets issued
in each region. We add a stable but random source of tweets to represent the
posts when no disease cases are recorded. The model is learned through a Markov
chain Monte Carlo algorithm that produces the posterior distribution of the
relevant parameters. Using data from a significant number of large Brazilian
towns, we demonstrate empirically that our model is able to predict well the
next weeks of the disease counts using the tweets and disease cases jointly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06002</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06002</id><created>2015-10-20</created><updated>2015-10-27</updated><authors><author><keyname>Choi</keyname><forenames>Heejin</forenames></author><author><keyname>Meshi</keyname><forenames>Ofer</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Fast and Scalable Structural SVM with Slack Rescaling</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient method for training slack-rescaled structural SVM.
Although finding the most violating label in a margin-rescaled formulation is
often easy since the target function decomposes with respect to the structure,
this is not the case for a slack-rescaled formulation, and finding the most
violated label might be very difficult. Our core contribution is an efficient
method for finding the most-violating-label in a slack-rescaled formulation,
given an oracle that returns the most-violating-label in a (slightly modified)
margin-rescaled formulation. We show that our method enables accurate and
scalable training for slack-rescaled SVMs, reducing runtime by an order of
magnitude compared to previous approaches to slack-rescaled SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06020</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06020</id><created>2015-10-17</created><authors><author><keyname>Grinblat</keyname><forenames>Andrey</forenames></author></authors><title>Parallelization of Petri Nets via Polynomials</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce for any Petri net N an commutative polynomials
P(N) of two variables. We construct an one to one correspondence between these
polynomials and labelled Petri nets (i.e., any event is labeled by some natural
number). As corollary we get the following claim; the computational process can
be parallelized (i.e., it decomposes) iff for the correspondence Petri Net N,
the polynomial P(N) can be decomposed in the ring of commutative polynomials of
two variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06024</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06024</id><created>2015-10-19</created><authors><author><keyname>Ye</keyname><forenames>Junting</forenames></author><author><keyname>Akoglu</keyname><forenames>Leman</forenames></author></authors><title>Robust Semi-Supervised Classification for Multi-Relational Graphs</title><categories>cs.LG</categories><comments>14 pages, 8 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-regularized semi-supervised learning has been used effectively for
classification when (i) instances are connected through a graph, and (ii)
labeled data is scarce. If available, using multiple relations (or graphs)
between the instances can improve the prediction performance. On the other
hand, when these relations have varying levels of veracity and exhibit varying
relevance for the task, very noisy and/or irrelevant relations may deteriorate
the performance. As a result, an effective weighing scheme needs to be put in
place. In this work, we propose a robust and scalable approach for
multi-relational graph-regularized semi-supervised classification. Under a
convex optimization scheme, we simultaneously infer weights for the multiple
graphs as well as a solution. We provide a careful analysis of the inferred
weights, based on which we devise an algorithm that filters out irrelevant and
noisy graphs and produces weights proportional to the informativeness of the
remaining graphs. Moreover, the proposed method is linearly scalable w.r.t. the
number of edges in the union of the multiple graphs. Through extensive
experiments we show that our method yields superior results under different
noise models, and under increasing number of noisy graphs and intensity of
noise, as compared to a list of baselines and state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06051</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06051</id><created>2015-10-20</created><authors><author><keyname>Albert</keyname><forenames>Michael H.</forenames></author><author><keyname>Lackner</keyname><forenames>Marie-Louise</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author><author><keyname>Vatter</keyname><forenames>Vincent</forenames></author></authors><title>The Complexity of Pattern Matching for $321$-Avoiding and Skew-Merged
  Permutations</title><categories>math.CO cs.DS</categories><msc-class>05A05, 68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Permutation Pattern Matching problem, asking whether a pattern
permutation $\pi$ is contained in a permutation $\tau$, is known to be
NP-complete. In this paper we present two polynomial time algorithms for
special cases. The first algorithm is applicable if both $\pi$ and $\tau$ are
$321$-avoiding; the second is applicable if $\pi$ and $\tau$ are skew-merged.
Both algorithms have a runtime of $O(kn)$, where $k$ is the length of $\pi$ and
$n$ the length of $\tau$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06054</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06054</id><created>2015-10-20</created><authors><author><keyname>Drakopoulos</keyname><forenames>Kimon</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Tsitsiklis</keyname><forenames>John N.</forenames></author></authors><title>When is a network epidemic hard to eliminate?</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the propagation of a contagion process (epidemic) on a network
and study the problem of dynamically allocating a fixed curing budget to the
nodes of the graph, at each time instant. For bounded degree graphs, we provide
a lower bound on the expected time to extinction under any such dynamic
allocation policy, in terms of a combinatorial quantity that we call the
resistance of the set of initially infected nodes, the available budget, and
the number of nodes n. Specifically, we consider the case of bounded degree
graphs, with the resistance growing linearly in n. We show that if the curing
budget is less than a certain multiple of the resistance, then the expected
time to extinction grows exponentially with n. As a corollary, if all nodes are
initially infected and the CutWidth of the graph grows linearly, while the
curing budget is less than a certain multiple of the CutWidth, then the
expected time to extinction grows exponentially in n. The combination of the
latter with our prior work establishes a fairly sharp phase transition on the
expected time to extinction (sub-linear versus exponential) based on the
relation between the CutWidth and the curing budget.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06055</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06055</id><created>2015-10-20</created><authors><author><keyname>Drakopoulos</keyname><forenames>Kimon</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Tsitsiklis</keyname><forenames>John N.</forenames></author></authors><title>A lower bound on the performance of dynamic curing policies for
  epidemics on graphs</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an SIS-type epidemic process that evolves on a known graph. We
assume that a fixed curing budget can be allocated at each instant to the nodes
of the graph, towards the objective of minimizing the expected extinction time
of the epidemic. We provide a lower bound on the optimal expected extinction
time as a function of the available budget, the epidemic parameters, the
maximum degree, and the CutWidth of the graph. For graphs with large CutWidth
(close to the largest possible), and under a budget which is sublinear in the
number of nodes, our lower bound scales exponentially with the size of the
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06073</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06073</id><created>2015-10-20</created><authors><author><keyname>Clarkson</keyname><forenames>Kenneth L.</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Input Sparsity and Hardness for Robust Subspace Approximation</title><categories>cs.DS</categories><comments>paper appeared in FOCS, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the subspace approximation problem, we seek a k-dimensional subspace F of
R^d that minimizes the sum of p-th powers of Euclidean distances to a given set
of n points a_1, ..., a_n in R^d, for p &gt;= 1. More generally than minimizing
sum_i dist(a_i,F)^p,we may wish to minimize sum_i M(dist(a_i,F)) for some loss
function M(), for example, M-Estimators, which include the Huber and Tukey loss
functions. Such subspaces provide alternatives to the singular value
decomposition (SVD), which is the p=2 case, finding such an F that minimizes
the sum of squares of distances. For p in [1,2), and for typical M-Estimators,
the minimizing $F$ gives a solution that is more robust to outliers than that
provided by the SVD. We give several algorithmic and hardness results for these
robust subspace approximation problems.
  We think of the n points as forming an n x d matrix A, and letting nnz(A)
denote the number of non-zero entries of A. Our results hold for p in [1,2). We
use poly(n) to denote n^{O(1)} as n -&gt; infty. We obtain: (1) For minimizing
sum_i dist(a_i,F)^p, we give an algorithm running in O(nnz(A) +
(n+d)poly(k/eps) + exp(poly(k/eps))), (2) we show that the problem of
minimizing sum_i dist(a_i, F)^p is NP-hard, even to output a
(1+1/poly(d))-approximation, answering a question of Kannan and Vempala, and
complementing prior results which held for p &gt;2, (3) For loss functions for a
wide class of M-Estimators, we give a problem-size reduction: for a parameter
K=(log n)^{O(log k)}, our reduction takes O(nnz(A) log n + (n+d) poly(K/eps))
time to reduce the problem to a constrained version involving matrices whose
dimensions are poly(K eps^{-1} log n). We also give bicriteria solutions, (4)
Our techniques lead to the first O(nnz(A) + poly(d/eps)) time algorithms for
(1+eps)-approximate regression for a wide class of convex M-Estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06074</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06074</id><created>2015-10-20</created><authors><author><keyname>Pereira</keyname><forenames>Pedro O.</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Bounded Control for Double Integrator in Quadrotor Dynamics</title><categories>cs.SY</categories><comments>Companion article submitted to ECC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a trajectory tracking controller for a quadrotor system by
finding a coordinate change which transforms the quadrotor's vector field into
that of a thrust propelled system. In a thrust propelled system, the goal is to
stabilize its position around the origin, while the system is actuated by a one
dimensional acceleration/thrust along a direction vector, by a time-varying
gravity, and by the angular acceleration of the direction vector. For this
system, a solution has been proposed in a companion article, submitted to ECC
2016, based on the implicit knowledge of a bounded controller for a double
integrator system, and on the implicit knowledge of a Lyapunov function that
guarantees the origin is asymptotically stable for the double integrator
controlled by the bounded controller. We present two alternative bounded
controllers for a double integrator system, and corresponding Lyapunov
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06083</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06083</id><created>2015-10-20</created><authors><author><keyname>Dong</keyname><forenames>Hongbo</forenames></author><author><keyname>Chen</keyname><forenames>Kun</forenames></author><author><keyname>Linderoth</keyname><forenames>Jeff</forenames></author></authors><title>Regularization vs. Relaxation: A conic optimization perspective of
  statistical variable selection</title><categories>cs.LG math.NA math.OC</categories><comments>Also available on optimization online
  {http://www.optimization-online.org/DB_HTML/2015/05/4932.html}</comments><msc-class>90C22, 90C47, 62J07</msc-class><acm-class>G.1.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable selection is a fundamental task in statistical data analysis.
Sparsity-inducing regularization methods are a popular class of methods that
simultaneously perform variable selection and model estimation. The central
problem is a quadratic optimization problem with an l0-norm penalty. Exactly
enforcing the l0-norm penalty is computationally intractable for larger scale
problems, so dif- ferent sparsity-inducing penalty functions that approximate
the l0-norm have been introduced. In this paper, we show that viewing the
problem from a convex relaxation perspective offers new insights. In
particular, we show that a popular sparsity-inducing concave penalty function
known as the Minimax Concave Penalty (MCP), and the reverse Huber penalty
derived in a recent work by Pilanci, Wainwright and Ghaoui, can both be derived
as special cases of a lifted convex relaxation called the perspective
relaxation. The optimal perspective relaxation is a related minimax problem
that balances the overall convexity and tightness of approximation to the l0
norm. We show it can be solved by a semidefinite relaxation. Moreover, a
probabilistic interpretation of the semidefinite relaxation reveals connections
with the boolean quadric polytope in combinatorial optimization. Finally by
reformulating the l0-norm pe- nalized problem as a two-level problem, with the
inner level being a Max-Cut problem, our proposed semidefinite relaxation can
be realized by replacing the inner level problem with its semidefinite
relaxation studied by Goemans and Williamson. This interpretation suggests
using the Goemans-Williamson rounding procedure to find approximate solutions
to the l0-norm penalized problem. Numerical experiments demonstrate the
tightness of our proposed semidefinite relaxation, and the effectiveness of
finding approximate solutions by Goemans-Williamson rounding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06092</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06092</id><created>2015-10-20</created><updated>2016-01-22</updated><authors><author><keyname>Yun</keyname><forenames>Jinhyuk</forenames></author><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Jeong</keyname><forenames>Hawoong</forenames></author></authors><title>Intellectual interchanges in the history of the massive online
  open-editing encyclopedia, Wikipedia</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>12 pages, 9 figures</comments><journal-ref>Phys. Rev. E 93, 012307 (2016)</journal-ref><doi>10.1103/PhysRevE.93.012307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia is a free Internet encyclopedia with an enormous amount of content.
This encyclopedia is written by volunteers with various backgrounds in a
collective fashion; anyone can access and edit most of the articles. This
open-editing nature may give us prejudice that Wikipedia is an unstable and
unreliable source; yet many studies suggest that Wikipedia is even more
accurate and self-consistent than traditional encyclopedias. Scholars have
attempted to understand such extraordinary credibility, but usually used the
number of edits as the unit of time, without consideration of real time. In
this work, we probe the formation of such collective intelligence through a
systematic analysis using the entire history of 34,534,110 English Wikipedia
articles, between 2001 and 2014. From this massive data set, we observe the
universality of both timewise and lengthwise editing scales, which suggests
that it is essential to consider the real-time dynamics. By considering real
time, we find the existence of distinct growth patterns that are unobserved by
utilizing the number of edits as the unit of time. To account for these
results, we present a mechanistic model that adopts the article editing
dynamics based on both editor-editor and editor-article interactions. The model
successfully generates the key properties of real Wikipedia articles such as
distinct types of articles for the editing patterns characterized by the
interrelationship between the numbers of edits and editors, and the article
size. In addition, the model indicates that infrequently referred articles tend
to grow faster than frequently referred ones, and articles attracting a high
motivation to edit counterintuitively reduce the number of participants. We
suggest that this decay of participants eventually brings inequality among the
editors, which will become more severe with time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06093</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06093</id><created>2015-10-20</created><authors><author><keyname>Zhai</keyname><forenames>Yao</forenames></author><author><keyname>Wang</keyname><forenames>Qifei</forenames></author><author><keyname>Lu</keyname><forenames>Yan</forenames></author><author><keyname>Li</keyname><forenames>Shipeng</forenames></author></authors><title>Content adaptive screen image scaling</title><categories>cs.CV</categories><comments>ICIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient content adaptive screen image scaling scheme
for the real-time screen applications like remote desktop and screen sharing.
In the proposed screen scaling scheme, a screen content classification step is
first introduced to classify the screen image into text and pictorial regions.
Afterward, we propose an adaptive shift linear interpolation algorithm to
predict the new pixel values with the shift offset adapted to the content type
of each pixel. The shift offset for each screen content type is offline
optimized by minimizing the theoretical interpolation error based on the
training samples respectively. The proposed content adaptive screen image
scaling scheme can achieve good visual quality and also keep the low complexity
for real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06095</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06095</id><created>2015-10-20</created><authors><author><keyname>Jeon</keyname><forenames>Charles</forenames></author><author><keyname>Ghods</keyname><forenames>Ramina</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Optimality of Large MIMO Detection via Approximate Message Passing</title><categories>cs.IT math.IT</categories><comments>Presented at the 2015 IEEE International Symposium on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal data detection in multiple-input multiple-output (MIMO) communication
systems with a large number of antennas at both ends of the wireless link
entails prohibitive computational complexity. In order to reduce the
computational complexity, a variety of sub-optimal detection algorithms have
been proposed in the literature. In this paper, we analyze the optimality of a
novel data-detection method for large MIMO systems that relies on approximate
message passing (AMP). We show that our algorithm, referred to as
individually-optimal (IO) large-MIMO AMP (short IO-LAMA), is able to perform IO
data detection given certain conditions on the MIMO system and the
constellation set (e.g., QAM or PSK) are met.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06096</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06096</id><created>2015-10-20</created><authors><author><keyname>Sun</keyname><forenames>Ju</forenames></author><author><keyname>Qu</keyname><forenames>Qing</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>When Are Nonconvex Problems Not Scary?</title><categories>math.OC cs.IT math.IT stat.ML</categories><comments>6 pages, 3 figures. This is a concise expository article that avoid
  much technical rigor. We will make a separate submission with full technical
  details in future</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we focus on nonconvex optimization problems with no &quot;spurious&quot;
local minimizers, and with saddle points of at most second-order. Concrete
applications such as dictionary learning, phase retrieval, and tensor
decomposition are known to induce such structures. We describe a second-order
trust-region algorithm that provably converges to a local minimizer in
polynomial time. Finally we highlight alternatives, and open problems in this
direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06097</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06097</id><created>2015-10-20</created><authors><author><keyname>Ghods</keyname><forenames>Ramina</forenames></author><author><keyname>Jeon</keyname><forenames>Charles</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Optimal Large-MIMO Data Detection with Transmit Impairments</title><categories>cs.IT math.IT</categories><comments>Presented at the 53rd Annual Allerton Conference on Communication,
  Control, and Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world transceiver designs for multiple-input multiple-output (MIMO)
wireless communication systems are affected by a number of hardware impairments
that already appear at the transmit side, such as amplifier non-linearities,
quantization artifacts, and phase noise. While such transmit-side impairments
are routinely ignored in the data-detection literature, they often limit
reliable communication in practical systems. In this paper, we present a novel
data-detection algorithm, referred to as large-MIMO approximate message passing
with transmit impairments (short LAMA-I), which takes into account a broad
range of transmit-side impairments in wireless systems with a large number of
transmit and receive antennas. We provide conditions in the large-system limit
for which LAMA-I achieves the error-rate performance of the
individually-optimal (IO) data detector. We furthermore demonstrate that LAMA-I
achieves near-IO performance at low computational complexity in realistic,
finite dimensional large-MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06108</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06108</id><created>2015-10-20</created><updated>2016-02-09</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Hadi</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Sriram</forenames></author></authors><title>A Counter-Example Guided Framework for Robust Synthesis of Switched
  Systems Using Control Certificates</title><categories>cs.SY</categories><comments>The paper has been withdrawn by the author. The main reason is
  incorrect proof of Theorem 4 and some incorrect results in the evaluation
  parts due to some bugs in the code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the problem of synthesizing switching controllers is
considered through the synthesis of a &quot;control certificate&quot;. Control
certificates include control barrier and Lyapunov functions, which represent
control strategies, and allow for automatic controller synthesis. Our approach
encodes the controller synthesis problem as quantified nonlinear constraints.
We extend an approach called Counterexample Guided Inductive Synthesis (CEGIS),
originally proposed for program synthesis problems, to solve the resulting
constraints. The CEGIS procedure involves the use of satisfiability-modulo
theory (SMT) solvers to automate the problem of synthesizing control
certificates. In this paper, we examine generalizations of CEGIS to attempt a
richer class of specifications, including reach-while-stay with obstacles and
control under disturbances. We demonstrate the ability of our approach to
handle systems with nonpolynomial dynamics as well. The abilities of our
general framework are demonstrated through a set of interesting examples. Our
evaluation suggests that our approach is computationally feasible, and adds to
the growing body of formal approaches to controller synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06110</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06110</id><created>2015-10-20</created><authors><author><keyname>Wildman</keyname><forenames>Jeffrey</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>Utility Maximization for Single-Station User Association in Downlink
  Cellular Networks</title><categories>cs.NI</categories><comments>14 pages, 9 figures. Submitted October 20, 2015 to IEEE/ACM
  Transactions on Networking. Preliminary conference version presented at
  Allerton 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study network utility maximization (NUM) in the context of cellular single
station association (SSA) policies, which assigns each mobile user (MU) to a
single base station (BS). We measure an SSA policy in terms of the induced
\alpha-proportional fairness utility of each user's downlink rate, summed over
all users. The general SSA NUM problem involves choosing an optimal association
from MUs to BSs as well as an optimal allocation of BS resources to associated
MUs. Finding an exact solution to such centralized user association problems is
well-known to be NP-hard. Our contributions are as follows: i) we give an
explicit solution for the optimal BS allocation for a given SSA, which
establishes SSA NUM as a purely combinatiorial problem; ii) we establish the
integrality gap for the association problem to be one, and prove the relaxation
to be a non-convex optimization problem; iii) we provide both centralized and
distributed greedy algorithms for SSA, both with and without the exchange of
instantaneous rate information between users and stations. Our numerical
results illustrate performance gains of three classes of solutions: i) SSA
solutions obtained by greedy rounding of multi-station associations (a
centralized convex program), ii) our centralized and distributed greedy
algorithms with/without rate information exchanged, and iii) simple association
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06113</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06113</id><created>2015-10-20</created><updated>2016-03-01</updated><authors><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Brown</keyname><forenames>Daniel E</forenames></author><author><keyname>Angell</keyname><forenames>William</forenames></author><author><keyname>Abdi&#x107;</keyname><forenames>Irman</forenames></author><author><keyname>Reimer</keyname><forenames>Bryan</forenames></author><author><keyname>Noh</keyname><forenames>Hae Young</forenames></author></authors><title>Automated Synchronization of Driving Data Using Vibration and Steering
  Events</title><categories>cs.RO cs.DC</categories><comments>Accepted for Publication in Elsevier Pattern Recognition Letters</comments><doi>10.1016/j.patrec.2016.02.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for automated synchronization of vehicle sensors useful
for the study of multi-modal driver behavior and for the design of advanced
driver assistance systems. Multi-sensor decision fusion relies on synchronized
data streams in (1) the offline supervised learning context and (2) the online
prediction context. In practice, such data streams are often out of sync due to
the absence of a real-time clock, use of multiple recording devices, or
improper thread scheduling and data buffer management. Cross-correlation of
accelerometer, telemetry, audio, and dense optical flow from three video
sensors is used to achieve an average synchronization error of 13 milliseconds.
The insight underlying the effectiveness of the proposed approach is that the
described sensors capture overlapping aspects of vehicle vibrations and vehicle
steering allowing the cross-correlation function to serve as a way to compute
the delay shift in each sensor. Furthermore, we show the decrease in
synchronization error as a function of the duration of the data stream.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06115</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06115</id><created>2015-10-20</created><authors><author><keyname>Wan</keyname><forenames>Changjin</forenames></author><author><keyname>Zhu</keyname><forenames>Liqiang</forenames></author><author><keyname>Liu</keyname><forenames>Yanghui</forenames></author><author><keyname>Feng</keyname><forenames>Ping</forenames></author><author><keyname>Liu</keyname><forenames>Zhaoping</forenames></author><author><keyname>Cao</keyname><forenames>Hailiang</forenames></author><author><keyname>Xiao</keyname><forenames>Peng</forenames></author><author><keyname>Shi</keyname><forenames>Yi</forenames></author><author><keyname>Wan</keyname><forenames>Qing</forenames></author></authors><title>Proton Conducting Graphene Oxide Coupled Neuron Transistors for
  Brain-Inspired Cognitive Systems</title><categories>q-bio.NC cond-mat.mtrl-sci cs.ET</categories><comments>arXiv admin note: text overlap with arXiv:1506.04658</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuron is the most important building block in our brain, and information
processing in individual neuron involves the transformation of input synaptic
spike trains into an appropriate output spike train. Hardware implementation of
neuron by individual ionic/electronic hybrid device is of great significance
for enhancing our understanding of the brain and solving sensory processing and
complex recognition tasks. Here, we provide a proof-of-principle artificial
neuron based on a proton conducting graphene oxide (GO) coupled oxide-based
electric-double-layer (EDL) transistor with multiple driving inputs and one
modulatory input terminal. Paired-pulse facilitation, dendritic integration and
orientation tuning were successfully emulated. Additionally, neuronal gain
control (arithmetic) in the scheme of rate coding is also experimentally
demonstrated. Our results provide a new-concept approach for building
brain-inspired cognitive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06121</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06121</id><created>2015-10-20</created><authors><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author></authors><title>Cache-Aided Interference Channels</title><categories>cs.IT math.IT</categories><comments>17 pages, Presented in Part in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, the bulk of wireless traffic has shifted from speech to
content. This shift creates the opportunity to cache part of the content in
memories closer to the end users, for example in base stations. Most of the
prior literature focuses on the reduction of load in the backhaul and core
networks due to caching, i.e., on the benefits caching offers for the wireline
communication link between the origin server and the caches. In this paper, we
are instead interested in the benefits caching can offer for the wireless
communication link between the caches and the end users.
  To quantify the gains of caching for this wireless link, we consider an
interference channel in which each transmitter is equipped with an isolated
cache memory. Communication takes place in two phases, a content placement
phase followed by a content delivery phase. The objective is to design both the
placement and the delivery phases to maximize the rate in the delivery phase in
response to any possible user demands. Focusing on the three-user case, we show
that through careful joint design of these phases, we can reap three distinct
benefits from caching: a load balancing gain, an interference cancellation
gain, and an interference alignment gain. In our proposed scheme, load
balancing is achieved through a specific file splitting and placement,
producing a particular pattern of content overlap at the caches. This overlap
allows to implement interference cancellation. Further, it allows us to create
several virtual transmitters, each transmitting a part of the requested
content, which increases interference-alignment possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06124</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06124</id><created>2015-10-20</created><updated>2015-10-26</updated><authors><author><keyname>Fajardo</keyname><forenames>David</forenames></author><author><keyname>Castano</keyname><forenames>Victor</forenames></author></authors><title>Hierarchy of knowledge translation: from health problems to ad-hoc drug
  design</title><categories>cs.SI cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An innovative approach to analyze the complexity of translating novel
molecular entities and nanomaterials into pharmaceutical alternatives (i.e.,
knowledge translation, KT) is discussed. First, some key concepts on the
organization and translation of the biomedical knowledge (paradigms, homophily,
power law distributions, hierarchy, modularity, and research fronts) are
reviewed. Then, we propose a model for the knowledge translation (KT) in Drug
Discovery that considers the complexity of interdisciplinary communication.
Specifically, we address two highly relevant aspects: 1) A successful KT
requires the emergence of organized bodies of inter-and transdisciplinary
research, and 2) The hierarchical and modular topological organization of these
bodies of knowledge. We focused on a set of previously-published studies on KT
which rely on a combination of network analysis and computer-assisted analysis
of the contents of scientific literature and patents. The selected studies
provide a duo of complementary perspectives: the demand of knowledge (cervical
cancer and Ebola hemorrhagic fever) and the supply of knowledge (liposomes and
nanoparticles to treat cancer and the paradigmatic Doxil, the first nanodrug to
be approved).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06141</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06141</id><created>2015-10-21</created><authors><author><keyname>Basar</keyname><forenames>Ertugrul</forenames></author></authors><title>Multiple-Input Multiple-Output OFDM with Index Modulation</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Signal Processing Letters, vol. 22, no. 12, pp. 2259-2263,
  2015</journal-ref><doi>10.1109/LSP.2015.2475361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing with index modulation (OFDM-IM) is
a novel multicarrier transmission technique which has been proposed as an
alternative to classical OFDM. The main idea of OFDM-IM is the use of the
indices of the active subcarriers in an OFDM system as an additional source of
information. In this work, we propose multiple-input multiple-output OFDM-IM
(MIMO-OFDM-IM) scheme by combining OFDM-IM and MIMO transmission techniques.
The low complexity transceiver structure of the MIMO-OFDM-IM scheme is
developed and it is shown via computer simulations that the proposed
MIMO-OFDM-IM scheme achieves significantly better error performance than
classical MIMO-OFDM for several different system configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06143</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06143</id><created>2015-10-21</created><updated>2015-11-11</updated><authors><author><keyname>Li</keyname><forenames>Aaron Q.</forenames></author><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Josifovski</keyname><forenames>Vanja</forenames></author></authors><title>High Performance Latent Variable Models</title><categories>cs.LG cs.AI</categories><comments>arXiv admin note: This paper has been withdrawn due to an
  irreconcilable author dispute</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent variable models have accumulated a considerable amount of interest
from the industry and academia for their versatility in a wide range of
applications. A large amount of effort has been made to develop systems that is
able to extend the systems to a large scale, in the hope to make use of them on
industry scale data. In this paper, we describe a system that operates at a
scale orders of magnitude higher than previous works, and an order of magnitude
faster than state-of-the-art system at the same scale, at the same time showing
more robustness and more accurate results.
  Our system uses a number of advances in distributed inference: high
performance in synchronization of sufficient statistics with relaxed
consistency model; fast sampling, using the Metropolis-Hastings-Walker method
to overcome dense generative models; statistical modeling, moving beyond Latent
Dirichlet Allocation (LDA) to Pitman-Yor distributions (PDP) and Hierarchical
Dirichlet Process (HDP) models; sophisticated parameter projection schemes, to
resolve the conflicts within the constraint between parameters arising from the
relaxed consistency model.
  This work significantly extends the domain of applicability of what is
commonly known as the Parameter Server. We obtain results with up to hundreds
billion oftokens, thousands of topics, and a vocabulary of a few million
token-types, using up to 60,000 processor cores operating on a production
cluster of a large Internet company. This demonstrates the feasibility to scale
to problems orders of magnitude larger than any previously published work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06150</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06150</id><created>2015-10-21</created><authors><author><keyname>Robinson</keyname><forenames>Joseph W.</forenames></author><author><keyname>Li</keyname><forenames>Aaron Q.</forenames></author></authors><title>Matching Mechanisms For Real-Time Computational Resource Exchange
  Markets</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe matching mechanisms for a real-time computational
resource exchange market, Chital, that incentivizes participating clients to
perform computation for their peers in exchange for overall improved
performance. The system is designed to discourage dishonest behavior via a
credit system, while simultaneously minimizing the use of dedicated computing
servers and the number of verifications performed by the administrating
servers. We describe the system in the context of a pre-existing system (under
development), Vedalia \cite{715Project}, for analyzing and visualizing product
reviews, by using machine learning such as topic models. We extend this context
to general computing tasks, describe a list of matching algorithms, and
evaluate their performance in a simulated environment. In addition, we design a
matching algorithm that optimizes the amount of time a participant could save
compared to computing a task on their own, and show empirically that this
algorithm results in a situation in which it is almost always optimal for a
user to join the exchange than do computation alone. Lastly, we use a top-down
approach to derive a theoretically near-optimal matching algorithm under
certain distributional assumptions on query frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06153</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06153</id><created>2015-10-21</created><authors><author><keyname>Li</keyname><forenames>Aaron Q</forenames></author><author><keyname>Deng</keyname><forenames>Yuntian</forenames></author><author><keyname>Jing</keyname><forenames>Kublai</forenames></author><author><keyname>Robinson</keyname><forenames>Joseph W</forenames></author></authors><title>Creating Scalable and Interactive Web Applications Using High
  Performance Latent Variable Models</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this project we outline a modularized, scalable system for comparing
Amazon products in an interactive and informative way using efficient latent
variable models and dynamic visualization. We demonstrate how our system can
build on the structure and rich review information of Amazon products in order
to provide a fast, multifaceted, and intuitive comparison. By providing a
condensed per-topic comparison visualization to the user, we are able to
display aggregate information from the entire set of reviews while providing an
interface that is at least as compact as the &quot;most helpful reviews&quot; currently
displayed by Amazon, yet far more informative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06166</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06166</id><created>2015-10-21</created><authors><author><keyname>Borges</keyname><forenames>Joaquim</forenames></author><author><keyname>Fern&#xe1;ndez-C&#xf3;rdoba</keyname><forenames>Cristina</forenames></author></authors><title>There is exactly one Z2Z4-cyclic 1-perfect code</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let ${\cal C}$ be a ${\mathbb{Z}}_2{\mathbb{Z}}_4$-additive code of length $n
&gt; 3$. We prove that if the binary Gray image of ${\cal C}$, $C=\Phi({\cal C})$,
is a 1-perfect nonlinear code, then ${\cal C}$ cannot be a
${\mathbb{Z}}_2{\mathbb{Z}}_4$-cyclic code except for one case of length
$n=15$. Moreover, we give a parity check matrix for this cyclic code. Adding an
even parity check coordinate to a ${\mathbb{Z}}_2{\mathbb{Z}}_4$-additive
1-perfect code gives an extended 1-perfect code. We also prove that any such
code cannot be ${\mathbb{Z}}_2{\mathbb{Z}}_4$-cyclic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06168</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06168</id><created>2015-10-21</created><authors><author><keyname>Wang</keyname><forenames>Peilu</forenames></author><author><keyname>Qian</keyname><forenames>Yao</forenames></author><author><keyname>Soong</keyname><forenames>Frank K.</forenames></author><author><keyname>He</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Hai</forenames></author></authors><title>Part-of-Speech Tagging with Bidirectional Long Short-Term Memory
  Recurrent Neural Network</title><categories>cs.CL</categories><comments>rejected by ACL 2015 short, score: 4,3,2 (full is 5)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidirectional Long Short-Term Memory Recurrent Neural Network (BLSTM-RNN) has
been shown to be very effective for tagging sequential data, e.g. speech
utterances or handwritten documents. While word embedding has been demoed as a
powerful representation for characterizing the statistical properties of
natural language. In this study, we propose to use BLSTM-RNN with word
embedding for part-of-speech (POS) tagging task. When tested on Penn Treebank
WSJ test set, a state-of-the-art performance of 97.40 tagging accuracy is
achieved. Without using morphological features, this approach can also achieve
a good performance comparable with the Stanford POS tagger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06175</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06175</id><created>2015-10-21</created><authors><author><keyname>Cimino</keyname><forenames>Mario G. C. A.</forenames></author><author><keyname>Celandroni</keyname><forenames>Nedo</forenames></author><author><keyname>Ferro</keyname><forenames>Erina</forenames></author><author><keyname>La Rosa</keyname><forenames>Davide</forenames></author><author><keyname>Palumbo</keyname><forenames>Filippo</forenames></author><author><keyname>Vaglini</keyname><forenames>Gigliola</forenames></author></authors><title>Wireless communication, identification and sensing technologies enabling
  integrated logistics: a study in the harbor environment</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, integrated logistics has become an important challenge in
the development of wireless communication, identification and sensing
technology, due to the growing complexity of logistics processes and the
increasing demand for adapting systems to new requirements. The advancement of
wireless technology provides a wide range of options for the maritime container
terminals. Electronic devices employed in container terminals reduce the manual
effort, facilitating timely information flow and enhancing control and quality
of service and decision made. In this paper, we examine the technology that can
be used to support integration in harbor's logistics. In the literature, most
systems have been developed to address specific needs of particular harbors,
but a systematic study is missing. The purpose is to provide an overview to the
reader about which technology of integrated logistics can be implemented and
what remains to be addressed in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06178</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06178</id><created>2015-10-21</created><updated>2016-03-01</updated><authors><author><keyname>Heijltjes</keyname><forenames>Willem</forenames><affiliation>University of Bath</affiliation></author><author><keyname>Houston</keyname><forenames>Robin</forenames></author></authors><title>Proof equivalence in MLL is PSPACE-complete</title><categories>cs.LO</categories><comments>Journal version of: Willem Heijltjes and Robin Houston. No proof nets
  for MLL with units: Proof equivalence in MLL is PSPACE-complete. In Proc.
  Joint Meeting of the 23rd EACSL Annual Conference on Computer Science Logic
  and the 29th Annual ACM/IEEE Symposium on Logic in Computer Science, 2014</comments><proxy>LMCS</proxy><journal-ref>LMCS 12 (1:2) 2016</journal-ref><doi>10.2168/LMCS-12(1:2)2016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MLL proof equivalence is the problem of deciding whether two proofs in
multiplicative linear logic are related by a series of inference permutations.
It is also known as the word problem for star-autonomous categories. Previous
work has shown the problem to be equivalent to a rewiring problem on proof
nets, which are not canonical for full MLL due to the presence of the two
units. Drawing from recent work on reconfiguration problems, in this paper it
is shown that MLL proof equivalence is PSPACE-complete, using a reduction from
Nondeterministic Constraint Logic. An important consequence of the result is
that the existence of a satisfactory notion of proof nets for MLL with units is
ruled out (under current complexity assumptions). The PSPACE-hardness result
extends to equivalence of normal forms in MELL without units, where the
weakening rule for the exponentials induces a similar rewiring problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06188</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06188</id><created>2015-10-21</created><updated>2016-01-22</updated><authors><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author><author><keyname>Li</keyname><forenames>Yen-Huan</forenames></author><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>G&#xf6;zc&#xfc;</keyname><forenames>Baran</forenames></author><author><keyname>Bogunovic</keyname><forenames>Ilija</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Learning-based Compressive Subsampling</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Submitted to IEEE Journal on Selected Topics in Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of recovering a structured signal $\mathbf{x} \in \mathbb{C}^p$
from a set of dimensionality-reduced linear measurements $\mathbf{b} = \mathbf
{A}\mathbf {x}$ arises in a variety of applications, such as medical imaging,
spectroscopy, Fourier optics, and computerized tomography. Due to computational
and storage complexity or physical constraints imposed by the problem, the
measurement matrix $\mathbf{A} \in \mathbb{C}^{n \times p}$ is often of the
form $\mathbf{A} = \mathbf{P}_{\Omega}\boldsymbol{\Psi}$ for some orthonormal
basis matrix $\boldsymbol{\Psi}\in \mathbb{C}^{p \times p}$ and subsampling
operator $\mathbf{P}_{\Omega}: \mathbb{C}^{p} \rightarrow \mathbb{C}^{n}$ that
selects the rows indexed by $\Omega$. This raises the fundamental question of
how best to choose the index set $\Omega$ in order to optimize the recovery
performance. Previous approaches to addressing this question rely on
non-uniform \emph{random} subsampling using application-specific knowledge of
the structure of $\mathbf{x}$. In this paper, we instead take a principled
learning-based approach in which a \emph{fixed} index set is chosen based on a
set of training signals $\mathbf{x}_1,\dotsc,\mathbf{x}_m$. We formulate
combinatorial optimization problems seeking to maximize the energy captured in
these signals in an average-case or worst-case sense, and we show that these
can be efficiently solved either exactly or approximately via the
identification of modularity and submodularity structures. We provide both
deterministic and statistical theoretical guarantees showing how the resulting
measurement matrices perform on signals differing from the training signals,
and we provide numerical examples showing our approach to be effective on a
variety of data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06197</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06197</id><created>2015-10-21</created><authors><author><keyname>Takano</keyname><forenames>Masanori</forenames></author><author><keyname>Wada</keyname><forenames>Kazuya</forenames></author><author><keyname>Fukuda</keyname><forenames>Ichiro</forenames></author></authors><title>Reciprocal Altruism-based Cooperation in a Social Network Game</title><categories>cs.SI physics.soc-ph q-bio.PE</categories><comments>16 pages, 2 figures; Accepted for publication in New Generation
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative behaviors are common in humans and are fundamental to our
society. Theoretical and experimental studies have modeled environments in
which the behaviors of humans, or agents, have been restricted to analyze their
social behavior. However, it is important that such studies are generalized to
less restrictive environments to understand human society. Social network games
(SNGs) provide a particularly powerful tool for the quantitative study of human
behavior. In SNGs, numerous players can behave more freely than in the
environments used in previous studies; moreover, their relationships include
apparent conflicts of interest and every action can be recorded. We focused on
reciprocal altruism, one of the mechanisms that generate cooperative behavior.
This study aims to investigate cooperative behavior based on reciprocal
altruism in a less restrictive environment. For this purpose, we analyzed the
social behavior underlying such cooperative behavior in an SNG. We focused on a
game scenario in which the relationship between the players was similar to that
in the Leader game. We defined cooperative behaviors by constructing a payoff
matrix in the scenario. The results showed that players maintained cooperative
behavior based on reciprocal altruism, and cooperators received more advantages
than noncooperators. We found that players constructed reciprocal relationships
based on two types of interactions, cooperative behavior and unproductive
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06201</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06201</id><created>2015-10-21</created><authors><author><keyname>Wang</keyname><forenames>Yaxuan</forenames></author><author><keyname>Wang</keyname><forenames>Hongzhi</forenames></author><author><keyname>Li</keyname><forenames>Jianzhong</forenames></author></authors><title>Efficient Influence Maximization in Weighted Independent Cascade Model</title><categories>cs.SI</categories><comments>13 pages, 5 figures</comments><acm-class>J.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization(IM) problem is to find a seed set in a social network
which achieves the maximal influence spread. This problem plays an important
role in viral marketing. Numerous models have been proposed to solve this
problem. However, none of them considers the attributes of nodes. Paying all
attention to the structure of network causes some trouble applying these models
to real-word applications.
  Motivated by this, we present weighted independent cascade (WIC) model, a
novel cascade model which extends the applicability of independent cascade(IC)
model by attaching attributes to the nodes. The IM problem in WIC model is to
maximize the value of nodes which are influenced. This problem is NP-hard. To
solve this problem, we present a basic greedy algorithm and Weight Reset(WR)
algorithm. Moreover, we propose Bounded Weight Reset(BWR) algorithm to make
further effort to improve the efficiency by bounding the diffusion node
influence. We prove that BWR is a fully polynomial-time approximation
scheme(FPTAS). Experimentally, we show that with additional node attribute, the
solution achieved by WIC model outperforms that of IC model in nearly 90%. The
experimental results show that BWR can achieve excellent approximation and
faster than greedy algorithm more than three orders of magnitude with little
sacrifice of accuracy. Especially, BWR can handle large networks with millions
of nodes in several tens of seconds while keeping rather high accuracy. Such
result demonstrates that BWR can solve IM problem effectively and efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06222</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06222</id><created>2015-10-21</created><authors><author><keyname>Azari</keyname><forenames>Bahar</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author><author><keyname>Tulino</keyname><forenames>Antonia M.</forenames></author></authors><title>Hypergraph-Based Analysis of Clustered Cooperative Beamforming with
  Application to Edge Caching</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages, 5 figures, Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of the performance of clustered cooperative beamforming in
cellular networks generally requires the solution of complex non-convex
optimization problems. In this letter, a framework based on a hypergraph
formalism is proposed that enables the derivation of a performance
characterization of clustered cooperative beamforming in terms of per-user
degrees of freedom (DoF) via the efficient solution of a coloring problem. An
emerging scenario in which clusters of cooperative base stations (BSs) arise is
given by cellular networks with edge caching. In fact, clusters of BSs that
share the same requested files can jointly beamform the corresponding encoded
signals. Based on this observation, the proposed framework is applied to obtain
quantitative insights into the optimal use of cache and backhaul resources in
cellular systems with edge caching. Numerical examples are provided to
illustrate the merits of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06223</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06223</id><created>2015-10-21</created><updated>2016-01-19</updated><authors><author><keyname>Trzcinski</keyname><forenames>Tomasz</forenames></author><author><keyname>Rokita</keyname><forenames>Przemyslaw</forenames></author></authors><title>Predicting popularity of online videos using Support Vector Regression</title><categories>cs.SI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a regression method to predict the popularity of an
online video based on temporal and visual cues. Our method uses Support Vector
Regression with Gaussian Radial Basis Functions. We show that modelling
popularity patterns with this approach provides higher and more stable
prediction results, mainly thanks to the non-linearity character of the
proposed method as well as its resistance against overfitting. We compare our
method with the state of the art on datasets containing over 14,000 videos from
YouTube and Facebook. Furthermore, we show that results obtained relying only
on the early distribution patterns, can be improved by adding social and visual
metadata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06229</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06229</id><created>2015-10-21</created><authors><author><keyname>Neto</keyname><forenames>Crescencio Rodrigues Lima</forenames></author><author><keyname>Chavez</keyname><forenames>Christina von Flach</forenames></author><author><keyname>de Almeida</keyname><forenames>Eduardo Santana</forenames></author><author><keyname>Rost</keyname><forenames>Dominik</forenames></author><author><keyname>Naab</keyname><forenames>Matthias</forenames></author></authors><title>Unveiling Architecture Documentation: Brazilian Stakeholders in
  Perspective</title><categories>cs.SE cs.CY</categories><comments>10 pages, 15 figures, 7 tables</comments><report-no>TR-PGCOMP-002/2015. Technical Report. Computer Science Graduate
  Program. Federal University of Bahia</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, software architecture has become a established discipline,
both in academia and industry, and the interest on software architecture
documentation has increased. In this context, the improvement of methods,
tools, and techniques around architecture documentation is of paramount
importance. We conducted a survey with 147 industrial participants (31 from
Brazil), analyzing their current problems and future wishes. We identified that
Brazilian stakeholders need updated architecture documents with the right
information. Finally, the automation of some parts of the documentation will
reduce the effort during the creation of the documents. But first, is necessary
to change the culture of the stakeholders. They have to participate actively in
the architecture documents creation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06231</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06231</id><created>2015-10-21</created><authors><author><keyname>Partala</keyname><forenames>Juha</forenames></author></authors><title>Symmetric Blind Decryption with Perfect Secrecy</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A blind decryption scheme enables a user to query decryptions from a
decryption server without revealing information about the plaintext message.
Such schemes are useful, for example, for the implementation of privacy
preserving encrypted file storages and payment systems. In terms of
functionality, blind decryption is close to oblivious transfer. For noiseless
channels, information-theoretically secure oblivious transfer is impossible.
However, in this paper we show that this is not the case for blind decryption.
We formulate a definition of perfect secrecy of symmetric blind decryption for
the following setting: at most one of the scheme participants is a malicious
observer. We also devise a symmetric blind decryption scheme based on modular
arithmetic on a ring $\mathbb{Z}_{p^2}$, where $p$ is a prime, and show that it
satisfies our notion of perfect secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06233</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06233</id><created>2015-10-21</created><updated>2016-02-25</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Transformation of fractions into simple fractions in divisive meadows</title><categories>math.RA cs.LO</categories><comments>23 pages; one theorem and two corollaries of it added at end of Sect.
  5; some minor errors corrected</comments><msc-class>12E12, 12L12, 68Q65</msc-class><doi>10.1016/j.jal.2016.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meadows are alternatives for fields with a purely equational axiomatization.
At the basis of meadows lies the decision to make the multiplicative inverse
operation total by imposing that the multiplicative inverse of zero is zero.
Divisive meadows are meadows with the multiplicative inverse operation replaced
by a division operation. Viewing a fraction as a term over the signature of
divisive meadows that is of the form p / q, we investigate which divisive
meadows admit transformation of fractions into simple fractions, i.e. fractions
without proper subterms that are fractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06257</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06257</id><created>2015-10-21</created><authors><author><keyname>Prezza</keyname><forenames>Nicola</forenames></author><author><keyname>Policriti</keyname><forenames>Alberto</forenames></author></authors><title>Computing LZ77 in Run-Compressed Space</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that the LZ77 factorization of a text T {\in\Sigma^n}
can be computed in O(R log n) bits of working space and O(n log R) time, R
being the number of runs in the Burrows-Wheeler transform of T reversed. For
extremely repetitive inputs, the working space can be as low as O(log n) bits:
exponentially smaller than the text itself. As a direct consequence of our
result, we show that a class of repetition-aware self-indexes based on a
combination of run-length encoded BWT and LZ77 can be built in asymptotically
optimal O(R + z) words of working space, z being the size of the LZ77 parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06263</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06263</id><created>2015-10-21</created><updated>2015-10-28</updated><authors><author><keyname>Barrau</keyname><forenames>Axel</forenames></author><author><keyname>Bonnabel</keyname><forenames>Silvere</forenames></author></authors><title>An EKF-SLAM algorithm with consistency properties</title><categories>cs.RO cs.SY</categories><comments>11 pages double column, 9 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the inconsistency of the EKF-based SLAM algorithm
that stems from non-observability of the origin and orientation of the global
reference frame. We prove on the non-linear two-dimensional problem with point
landmarks observed that this type of inconsistency is remedied using the
Invariant EKF, a recently introduced variant ot the EKF meant to account for
the symmetries of the state space. Extensive Monte-Carlo runs illustrate the
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06298</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06298</id><created>2015-10-21</created><authors><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author><author><keyname>Zhuk</keyname><forenames>Dmitriy</forenames></author></authors><title>Switchability and collapsibility of Gap Algebras</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let A be an idempotent algebra on a 3-element domain D that omits a G-set for
a factor. Suppose A is not \alpha\beta-projective (for some alpha, beta subsets
of D) and is not collapsible. It follows that A is switchable. We prove that,
for every finite subset Delta of Inv(A), Pol(Delta) is collapsible. We also
exhibit an algebra that is collapsible from a non-singleton source but is not
collapsible from any singleton source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06335</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06335</id><created>2015-10-21</created><authors><author><keyname>Venanzi</keyname><forenames>Matteo</forenames></author><author><keyname>Guiver</keyname><forenames>John</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Jennings</keyname><forenames>Nick</forenames></author></authors><title>Time-Sensitive Bayesian Information Aggregation for Crowdsourcing
  Systems</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing systems commonly face the problem of aggregating multiple
judgments provided by possibly unreliable workers. In addition, several aspects
of the design of efficient crowdsourcing processes, such as defining worker's
bonuses, fair prices and time limits of the tasks, involve the knowledge of the
actual duration of a specific task. In this work, we introduce a new
time{sensitive Bayesian aggregation method that simultaneously estimates a
task's duration and obtains reliable aggregations of crowdsourced judgments.
Our method builds on the key insight that the time taken by a worker to perform
a task is an important indicator of the likely quality of the produced
judgment. To capture this, our model uses latent variables to represent the
uncertainty about the workers' completion time, the tasks' duration and the
workers' accuracy. To relate the quality of a judgment to the time a worker
spends on a task, our model assumes that each task is completed within a latent
time window within which all workers with a propensity to valid labelling are
expected to submit their judgments. In contrast, workers with a lower
propensity to valid labelling, such as spammers, bots or lazy labellers, are
assumed to perform tasks considerably faster or slower than the time required
by normal workers. Specifically, we use efficient message- passing Bayesian
inference to learn approximate posterior probabilities of (i) the confusion
matrix of each worker, (ii) the propensity to valid labelling of each worker,
(iii) the unbiased duration of each task and (iv) the true label of each task.
Using two real-world public datasets for entity linking tasks, we show that our
method produces up to 15% more accurate classifications and up to 100% more
informative estimates of a task's duration compared to state{of{the{art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06336</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06336</id><created>2015-10-13</created><authors><author><keyname>Mitici</keyname><forenames>Mihaela</forenames></author><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>de Graaf</keyname><forenames>Maurits</forenames></author><author><keyname>Boucherie</keyname><forenames>Richard J.</forenames></author></authors><title>Data retrieval time for energy harvesting wireless sensor networks</title><categories>cs.SY cs.NI math.PR</categories><comments>14 pages, 3 figures</comments><msc-class>93E03, 60G99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of retrieving a reliable estimate of an attribute
monitored by a wireless sensor network, where the sensors harvest energy from
the environment independently, at random. Each sensor stores the harvested
energy in batteries of limited capacity. Moreover, provided they have
sufficient energy, the sensors broadcast their measurements in a decentralized
fashion. Clients arrive at the sensor network according to a Poisson process
and are interested in retrieving a fixed number of sensor measurements, based
on which a reliable estimate is computed. We show that the time until an
arbitrary sensor broadcasts has a phase-type distribution. Based on this result
and the theory of order statistics of phase-type distributions, we determine
the probability distribution of the time needed for a client to retrieve a
reliable estimate of an attribute monitored by the sensor network. We also
provide closed-form expression for the retrieval time of a reliable estimate
when the capacity of the sensor battery or the rate at which energy is
harvested is asymptotically large. In addition, we analyze numerically the
retrieval time of a reliable estimate for various sizes of the sensor network,
maximum capacity of the sensor batteries and rate at which energy is harvested.
These results show that the energy harvesting rate and the broadcasting rate
are the main parameters that influence the retrieval time of a reliable
estimate, while deploying sensors with large batteries does not significantly
reduce the retrieval time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06342</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06342</id><created>2015-10-21</created><authors><author><keyname>Park</keyname><forenames>Jeong Joon</forenames></author><author><keyname>Boettcher</keyname><forenames>Ronnel</forenames></author><author><keyname>Zhao</keyname><forenames>Andrew</forenames></author><author><keyname>Mun</keyname><forenames>Alex</forenames></author><author><keyname>Yuh</keyname><forenames>Kevin</forenames></author><author><keyname>Kumar</keyname><forenames>Vibhor</forenames></author><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author></authors><title>Prevalence and recoverability of syntactic parameters in sparse
  distributed memories</title><categories>cs.CL cs.IT math.IT</categories><comments>13 pages, LaTeX, 4 jpeg figures</comments><msc-class>91F20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method, based on Sparse Distributed Memory (Kanerva
Networks), for studying dependency relations between different syntactic
parameters in the Principles and Parameters model of Syntax. We store data of
syntactic parameters of world languages in a Kanerva Network and we check the
recoverability of corrupted parameter data from the network. We find that
different syntactic parameters have different degrees of recoverability. We
identify two different effects: an overall underlying relation between the
prevalence of parameters across languages and their degree of recoverability,
and a finer effect that makes some parameters more easily recoverable beyond
what their prevalence would indicate. We interpret a higher recoverability for
a syntactic parameter as an indication of the existence of a dependency
relation, through which the given parameter can be determined using the
remaining uncorrupted data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06356</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06356</id><created>2015-10-21</created><authors><author><keyname>Adachi</keyname><forenames>Steven H.</forenames></author><author><keyname>Henderson</keyname><forenames>Maxwell P.</forenames></author></authors><title>Application of Quantum Annealing to Training of Deep Neural Networks</title><categories>quant-ph cs.LG stat.ML</categories><comments>18 pages</comments><report-no>DIS201510002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Deep Learning, a well-known approach for training a Deep Neural Network
starts by training a generative Deep Belief Network model, typically using
Contrastive Divergence (CD), then fine-tuning the weights using backpropagation
or other discriminative techniques. However, the generative training can be
time-consuming due to the slow mixing of Gibbs sampling. We investigated an
alternative approach that estimates model expectations of Restricted Boltzmann
Machines using samples from a D-Wave quantum annealing machine. We tested this
method on a coarse-grained version of the MNIST data set. In our tests we found
that the quantum sampling-based training approach achieves comparable or better
accuracy with significantly fewer iterations of generative training than
conventional CD-based training. Further investigation is needed to determine
whether similar improvements can be achieved for other data sets, and to what
extent these improvements can be attributed to quantum effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06358</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06358</id><created>2015-10-19</created><authors><author><keyname>Imgrund</keyname><forenames>Maximilian</forenames></author><author><keyname>Arth</keyname><forenames>Alexander</forenames></author></authors><title>Rambrain - a library for virtually extending physical memory</title><categories>cs.DC</categories><comments>36 pages, 8 coloured figures, 6 listings, 1 table; Submitted to
  Journal of Computational Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Rambrain, a user space library that manages memory consumption
of your code. Using Rambrain you can overcommit memory over the size of
physical memory present in the system. Rambrain takes care of temporarily
swapping out data to disk and can handle multiples of the physical memory size
present. Rambrain is thread-safe, OpenMP and MPI compatible and supports
Asynchronous IO. The library was designed to require minimal changes to
existing programs and to be easy to use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06359</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06359</id><created>2015-10-21</created><authors><author><keyname>Bogale</keyname><forenames>Tadilo Endeshaw</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Massive MIMO and Millimeter Wave for 5G Wireless HetNet: Potentials and
  Challenges</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Vehicular Technology Magazine (To appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been active research activities worldwide in developing the
next-generation 5G wireless network. The 5G network is expected to support
significantly large amount of mobile data traffic and huge number of wireless
connections, achieve better cost- and energy-efficiency as well as quality of
service (QoS) in terms of communication delay, reliability and security. To
this end, the 5G wireless network should exploit potential gains in different
network dimensions including super dense and heterogeneous deployment of cells
and massive antenna arrays (i.e., massive multiple input multiple output (MIMO)
technologies) and utilization of higher frequencies, in particular millimeter
wave (mmWave) frequencies. This article discusses potentials and challenges of
the 5G heterogeneous wireless network (HetNet) which incorporates massive MIMO
and mmWave technologies. We will first provide the typical requirements of the
5G wireless network. Then, the significance of massive MIMO and mmWave in
engineering the future 5G HetNet is discussed in detail. Potential challenges
associated with the design of such 5G HetNet are discussed. Finally, we provide
some case studies, which illustrate the potential benefits of the considered
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06375</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06375</id><created>2015-10-21</created><authors><author><keyname>Zhen</keyname><forenames>Xiantong</forenames></author><author><keyname>Li</keyname><forenames>Shuo</forenames></author></authors><title>Towards Direct Medical Image Analysis without Segmentation</title><categories>cs.CV</categories><comments>2 pages perspective</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct methods have recently emerged as an effective and efficient tool in
automated medical image analysis and become a trend to solve diverse
challenging tasks in clinical practise. Compared to traditional methods, direct
methods are of much more clinical significance by straightly targeting to the
final clinical goal rather than relying on any intermediate steps. These
intermediate steps, e.g., segmentation, registration and tracking, are actually
not necessary and only limited to very constrained tasks far from being used in
practical clinical applications; moreover they are computationally expensive
and time-consuming, which causes a high waste of research resources. The
advantages of direct methods stem from \textbf{1)} removal of intermediate
steps, e.g., segmentation, tracking and registration; \textbf{2)} avoidance of
user inputs and initialization; \textbf{3)} reformulation of conventional
challenging problems, e.g., inversion problem, with efficient solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06379</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06379</id><created>2015-10-21</created><updated>2015-10-28</updated><authors><author><keyname>Preoteasa</keyname><forenames>Viorel</forenames></author><author><keyname>Tripakis</keyname><forenames>Stavros</forenames></author></authors><title>Towards Compositional Feedback in Non-Deterministic and
  Non-Input-Receptive Systems</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feedback is an essential composition operator in many classes of reactive and
other systems. This paper studies feedback in the context of compositional
theories with refinement. Such theories allow to reason about systems on a
component-by-component basis, and to characterize substitutability as a
refinement relation. Although compositional theories of feedback do exist, they
are limited either to deterministic systems (functions) or input-receptive
systems (total relations). In this paper we seek a compositional theory of
feedback which applies to non-deterministic and non-input-receptive systems
(e.g., partial relations). To achieve this, we use the semantic frameworks of
predicate and property transformers, and relations with fail and unknown
values. We show how to define instantaneous feedback (e.g., for combinational
elements and Mealy-like machines) as well as feedback for systems with a unit
delay (e.g., Moore-like machines). Both operations preserve the refinement
relation, and both can be applied to non-deterministic and non-input-receptive
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06423</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06423</id><created>2015-10-21</created><updated>2015-10-31</updated><authors><author><keyname>Wang</keyname><forenames>Zi</forenames></author><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author></authors><title>Optimization as Estimation with Gaussian Processes in Bandit Settings</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been rising interest in Bayesian optimization -- the
optimization of an unknown function with assumptions usually expressed by a
Gaussian Process (GP) prior. We study an optimization strategy that directly
uses a maximum a posteriori (MAP) estimate of the argmax of the function. This
strategy offers both practical and theoretical advantages: no tradeoff
parameter needs to be selected, and, moreover, we establish close connections
to the popular GP-UCB and GP-PI strategies. The MAP criterion can be understood
as automatically and adaptively trading off exploration and exploitation in
GP-UCB and GP-PI. We illustrate the effects of this adaptive tuning via bounds
on the regret as well as an extensive empirical evaluation on robotics and
vision tasks, demonstrating the robustness of this strategy for a range of
performance criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06437</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06437</id><created>2015-10-21</created><authors><author><keyname>Trummer</keyname><forenames>Immanuel</forenames></author><author><keyname>Koch</keyname><forenames>Christoph</forenames></author></authors><title>Multiple Query Optimization on the D-Wave 2X Adiabatic Quantum Computer</title><categories>cs.DB quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The D-Wave adiabatic quantum annealer solves hard combinatorial optimization
problems leveraging quantum physics. The newest version features over 1000
qubits and was released in August 2015. We were given access to such a machine,
currently hosted at NASA Ames Research Center in California, to explore the
potential for hard optimization problems that arise in the context of
databases.
  In this paper, we tackle the problem of multiple query optimization (MQO). We
show how an MQO problem instance can be transformed into a mathematical formula
that complies with the restrictive input format accepted by the quantum
annealer. This formula is translated into weights on and between qubits such
that the configuration minimizing the input formula can be found via a process
called adiabatic quantum annealing. We analyze the asymptotic growth rate of
the number of required qubits in the MQO problem dimensions as the number of
qubits is currently the main factor restricting applicability. We
experimentally compare the performance of the quantum annealer against other
MQO algorithms executed on a traditional computer. While the problem sizes that
can be treated are currently limited, we already find a class of problem
instances where the quantum annealer is three orders of magnitude faster than
other approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06452</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06452</id><created>2015-10-21</created><updated>2015-12-07</updated><authors><author><keyname>Moeller</keyname><forenames>Daniel</forenames></author><author><keyname>Paturi</keyname><forenames>Ramamohan</forenames></author><author><keyname>Schneider</keyname><forenames>Stefan</forenames></author></authors><title>Subquadratic Algorithms for Succinct Stable Matching</title><categories>cs.DS cs.CC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the stable matching problem when the preference lists are not
given explicitly but are represented in a succinct way and ask whether the
problem becomes computationally easier. We give subquadratic algorithms for
finding a stable matching in special cases of two very natural succinct
representations of the problem, the $d$-attribute and $d$-list models. We also
give algorithms for verifying a stable matching in the same models. We further
show that for $d = \omega(\log n)$ both finding and verifying a stable matching
in the $d$-attribute model requires quadratic time assuming the Strong
Exponential Time Hypothesis. The $d$-attribute model is therefore as hard as
the general case for large enough $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06454</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06454</id><created>2015-10-21</created><authors><author><keyname>Xie</keyname><forenames>Ronggui</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Chen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Many Access for Small Packets Based on Precoding and Sparsity-aware
  Recovery</title><categories>cs.IT math.IT</categories><comments>30 pages 10 figures ,submited to twc</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern mobile terminals produce massive small data packets. For these
short-length packets, it is inefficient to follow the current medium access
control protocols to allocate transmission resources due to heavy signaling
overhead. We propose a novel many-access scheme that is well suited for future
communication systems equipped with many receive antennas. The system is
modeled as having a block-sparsity pattern with unknown sparsity level (i.e.,
unknown number of transmitted messages). Block precoding is employed at each
single-antenna transmitter to enable simultaneous transmissions of many users.
The number of simultaneously served active users could be even more than the
number of receive antennas. Sparsity-aware recovery is designed at base station
for joint user detection and symbol demodulation. To reduce the effects of
channel fading on signal recovery, normalized block orthogonal matching pursuit
(BOMP) algorithm is introduced, and based on its performance analyis, we
develop interference cancellation based BOMP (ICBOMP) algorithm which performs
error correction and detection in each iteration of normalized BOMP.
Performance of ICBOMP is also analyzed. Simulation results demonstrate the
effectiveness of the proposed scheme in small packet services,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06460</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06460</id><created>2015-10-21</created><authors><author><keyname>Jones</keyname><forenames>Austin</forenames></author><author><keyname>Aksaray</keyname><forenames>Derya</forenames></author><author><keyname>Kong</keyname><forenames>Zhaodan</forenames></author><author><keyname>Schwager</keyname><forenames>Mac</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Robust Satisfaction of Temporal Logic Specifications via Reinforcement
  Learning</title><categories>cs.SY cs.RO</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of steering a system with unknown, stochastic
dynamics to satisfy a rich, temporally layered task given as a signal temporal
logic formula. We represent the system as a Markov decision process in which
the states are built from a partition of the state space and the transition
probabilities are unknown. We present provably convergent reinforcement
learning algorithms to maximize the probability of satisfying a given formula
and to maximize the average expected robustness, i.e., a measure of how
strongly the formula is satisfied. We demonstrate via a pair of robot
navigation simulation case studies that reinforcement learning with robustness
maximization performs better than probability maximization in terms of both
probability of satisfaction and expected robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06469</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06469</id><created>2015-10-21</created><authors><author><keyname>Fu</keyname><forenames>Jie</forenames></author><author><keyname>Atanasov</keyname><forenames>Nikolay</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Optimal Temporal Logic Planning in Probabilistic Semantic Maps</title><categories>cs.RO cs.SY</categories><comments>8 pages, 6 figures. submitted to IEEE International Conference on
  Robotics and Automation 2016</comments><msc-class>68T40, 03B44</msc-class><acm-class>I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers robot motion planning under temporal logic constraints
in probabilistic maps obtained by semantic simultaneous localization and
mapping (SLAM). The uncertainty in a map distribution presents a great
challenge for obtaining correctness guarantees with respect to the linear
temporal logic (LTL) specification. We show that the problem can be formulated
as an optimal control problem in which both the semantic map and the logic
formula evaluation are stochastic. Our first contribution is to reduce the
stochastic control problem for a subclass of LTL to a deterministic shortest
path problem by introducing a confidence parameter $delta$. A robot trajectory
obtained from the deterministic problem is guaranteed to have minimum cost and
to satisfy the logic specification in the true environment with probability
$delta$. Our second contribution is to design an admissible heuristic function
that guides the planning in the deterministic problem towards satisfying the
temporal logic specification. This allows us to obtain an optimal and very
efficient solution using the A* algorithm. The performance and correctness of
our approach are demonstrated in a simulated semantic environment using a
differential-drive robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06479</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06479</id><created>2015-10-21</created><updated>2015-10-23</updated><authors><author><keyname>Horikawa</keyname><forenames>Tomoyasu</forenames></author><author><keyname>Kamitani</keyname><forenames>Yukiyasu</forenames></author></authors><title>Generic decoding of seen and imagined objects using hierarchical visual
  features</title><categories>q-bio.NC cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Object recognition is a key function in both human and machine vision. While
recent studies have achieved fMRI decoding of seen and imagined contents, the
prediction is limited to training examples. We present a decoding approach for
arbitrary objects, using the machine vision principle that an object category
is represented by a set of features rendered invariant through hierarchical
processing. We show that visual features including those from a convolutional
neural network can be predicted from fMRI patterns and that greater accuracy is
achieved for low/high-level features with lower/higher-level visual areas,
respectively. Predicted features are used to identify the target object
(extending beyond decoder training) from a set of computed features for
numerous objects. Furthermore, we demonstrate the identification of imagined
objects, suggesting the recruitment of intermediate image representations in
top-down processing. Our results demonstrate a tight link between human and
machine vision and its utility for brain-based information retrieval.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06480</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06480</id><created>2015-10-21</created><updated>2015-11-01</updated><authors><author><keyname>Zhao</keyname><forenames>Xingya</forenames></author><author><keyname>Yang</keyname><forenames>Chenchen</forenames></author><author><keyname>Yao</keyname><forenames>Yao</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author></authors><title>Cognitive and Cache-enabled D2D Communications in Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Caching popular contents for frequent access is an attractive way of
employing the redundancy of user requests. Exploiting cognition to the
cache-enabled D2D in the multichannel cellular network is the main focus of
this paper. We contribute to analyzing the cache-based content delivery in a
twotier heterogeneous network (HetNet) composed of base stations (BSs) and
device-to-device (D2D) pairs, where the D2D accesses the networks with overlay
spectrum sharing. Node locations are first modeled as mutually independent
Poisson Point Processes (PPPs), and the service queueing process is formulated.
The corresponding tier association and cognitive access protocol are developed.
The D2D transmitter (TX) performs overlay spectrum sensing within its spectrum
sensing region (SSR) to detect the idleness of cellular channels. Then the
number of BSs and D2D TXs in the SSR are analyzed. We further elaborate the
probability mass function (PMF) of the delay and the queue length, with
modeling the traffic dynamics of request arrivals and departures at the BS and
D2D TX as the discrete-time multiserver queue with priorities. Moreover,
impacts of some key network parameters, e.g., the content popularity, the
request density and the caching storage, on the system performance are
investigated to provide a valuable insight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06482</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06482</id><created>2015-10-21</created><authors><author><keyname>Mohammadi</keyname><forenames>Shahin</forenames></author><author><keyname>Gleich</keyname><forenames>David</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author></authors><title>Triangular Alignment (TAME): A Tensor-based Approach for Higher-order
  Network Alignment</title><categories>cs.CE</categories><comments>Submitted to the IEEE/ACM Transactions on Computational Biology and
  Bioinformatics (TCBB) for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network alignment is an important tool with extensive applications in
comparative interactomics. Traditional approaches aim to simultaneously
maximize the number of conserved edges and the underlying similarity of aligned
entities. We propose a novel formulation of the network alignment problem that
extends topological similarity to higher-order structures and provide a new
objective function that maximizes the number of aligned substructures. This
objective function corresponds to an integer programming problem, which is
NP-hard. Consequently, we approximate this objective function as a surrogate
function whose maximization results in a tensor eigenvalue problem. Based on
this formulation, we present an algorithm called Triangular AlignMEnt (TAME),
which attempts to maximize the number of aligned triangles across networks. We
focus on alignment of triangles because of their enrichment in complex
networks; however, our formulation and resulting algorithms can be applied to
general motifs. Using a case study on the NAPABench dataset, we show that TAME
is capable of producing alignments with up to 99\% accuracy in terms of aligned
nodes. We further evaluate our method by aligning yeast and human interactomes.
Our results indicate that TAME outperforms the state-of-art alignment methods
both in terms of biological and topological quality of the alignments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06486</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06486</id><created>2015-10-22</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Ramamohanarao</keyname><forenames>Kotagiri</forenames></author><author><keyname>Leckie</keyname><forenames>Chris</forenames></author><author><keyname>Calheiros</keyname><forenames>Rodrigo N.</forenames></author><author><keyname>Dastjerdi</keyname><forenames>Amir Vahid</forenames></author><author><keyname>Versteeg</keyname><forenames>Steve</forenames></author></authors><title>Big Data Analytics-Enhanced Cloud Computing: Challenges, Architectural
  Elements, and Future Directions</title><categories>cs.DC</categories><comments>10 pages, 2 figures, conference paper in Proceedings of the 21st IEEE
  International Conference on Parallel and Distributed Systems (ICPADS 2015,
  IEEE Press, USA), Melbourne, Australia, December 14-17, 2015</comments><acm-class>C.1.4; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of cloud computing has made dynamic provisioning of elastic
capacity to applications on-demand. Cloud data centers contain thousands of
physical servers hosting orders of magnitude more virtual machines that can be
allocated on demand to users in a pay-as-you-go model. However, not all systems
are able to scale up by just adding more virtual machines. Therefore, it is
essential, even for scalable systems, to project workloads in advance rather
than using a purely reactive approach. Given the scale of modern cloud
infrastructures generating real time monitoring information, along with all the
information generated by operating systems and applications, this data poses
the issues of volume, velocity, and variety that are addressed by Big Data
approaches. In this paper, we investigate how utilization of Big Data analytics
helps in enhancing the operation of cloud computing environments. We discuss
diverse applications of Big Data analytics in clouds, open issues for enhancing
cloud operations via Big Data analytics, and architecture for anomaly detection
and prevention in clouds along with future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06488</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06488</id><created>2015-10-22</created><authors><author><keyname>Yang</keyname><forenames>Chenchen</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author><author><keyname>Wang</keyname><forenames>Jiangzhou</forenames></author></authors><title>When ICN Meets C-RAN for HetNets: An SDN Approach</title><categories>cs.NI cs.IT math.IT</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In this paper, we contribute to novelly proposing and elaborating the
integration of the ICN, C-RAN and SDN for the HetNet to achieve win-win
situation. The vision of the proposed system is demonstrated, followed by the
advantages and challenges. We further present the hybrid system with a
large-scale wireless heterogeneous campus network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06492</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06492</id><created>2015-10-22</created><authors><author><keyname>Hermansson</keyname><forenames>Linus</forenames></author><author><keyname>Johansson</keyname><forenames>Fredrik D.</forenames></author><author><keyname>Watanabe</keyname><forenames>Osamu</forenames></author></authors><title>Generalized Shortest Path Kernel on Graphs</title><categories>cs.DS cs.LG</categories><comments>Short version presented at Discovery Science 2015 in Banff</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of classifying graphs using graph kernels. We define
a new graph kernel, called the generalized shortest path kernel, based on the
number and length of shortest paths between nodes. For our example
classification problem, we consider the task of classifying random graphs from
two well-known families, by the number of clusters they contain. We verify
empirically that the generalized shortest path kernel outperforms the original
shortest path kernel on a number of datasets. We give a theoretical analysis
for explaining our experimental results. In particular, we estimate
distributions of the expected feature vectors for the shortest path kernel and
the generalized shortest path kernel, and we show some evidence explaining why
our graph kernel outperforms the shortest path kernel for our graph
classification problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06495</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06495</id><created>2015-10-22</created><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Reduced Complexity Belief Propagation Decoders for Polar Codes</title><categories>cs.IT math.IT</categories><comments>accepted by the IEEE 2015 workshop on signal processing systems
  (SiPS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are newly discovered capacity-achieving codes, which have
attracted lots of research efforts. Polar codes can be efficiently decoded by
the low-complexity successive cancelation (SC) algorithm and the SC list (SCL)
decoding algorithm. The belief propagation (BP) decoding algorithm not only is
an alternative to the SC and SCL decoders, but also provides soft outputs that
are necessary for joint detection and decoding. Both the BP decoder and the
soft cancelation (SCAN) decoder were proposed for polar codes to output soft
information about the coded bits. In this paper, first a belief propagation
decoding algorithm, called reduced complexity soft cancelation (RCSC) decoding
algorithm, is proposed. Let $N$ denote the block length. Our RCSC decoding
algorithm needs to store only $5N-3$ log-likelihood ratios (LLRs),
significantly less than $4N-2+\frac{N\log_2N}{2}$ and $N(\log_2N+1)$ LLRs
needed by the BP and SCAN decoders, respectively, when $N\geqslant 64$.
Besides, compared to the SCAN decoding algorithm, our RCSC decoding algorithm
eliminates unnecessary additions over the real field. Then the simplified SC
(SSC) principle is applied to our RCSC decoding algorithm, and the resulting
SSC-aided RCSC (S-RCSC) decoding algorithm further reduces the computational
complexity. Finally, based on the S-RCSC decoding algorithm, we propose a
corresponding memory efficient decoder architecture, which has better error
performance than existing architectures. Besides, our decoder architecture
consumes less energy on updating LLRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06496</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06496</id><created>2015-10-22</created><authors><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Synthesizing least-limiting guidelines for safety of semi-autonomous
  systems</title><categories>cs.SY cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of synthesizing safe-by-design control strategies for
semi-autonomous systems. Our aim is to address situations when safety cannot be
guaranteed solely by the autonomous, controllable part of the system and a
certain level of collaboration is needed from the uncontrollable part, such as
the human operator. In this paper, we propose a systematic solution to
generating least-limiting guidelines, i.e. the guidelines that restrict the
human operator as little as possible in the worst-case long-term system
executions. The algorithm leverages ideas from 2-player turn-based games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06501</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06501</id><created>2015-10-22</created><authors><author><keyname>Marchal</keyname><forenames>Samuel</forenames></author><author><keyname>Saari</keyname><forenames>Kalle</forenames></author><author><keyname>Singh</keyname><forenames>Nidhi</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Know Your Phish: Novel Techniques for Detecting Phishing Sites and their
  Targets</title><categories>cs.CR</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phishing is a major problem on the Web. Despite the significant attention it
has received over the years, there has been no definitive solution. While the
state-of-the-art solutions have reasonably good performance, they require a
large amount of training data and are not adept at detecting phishing attacks
against new targets. In this paper, we begin with two core observations: (a)
although phishers try to make a phishing webpage look similar to its target,
they do not have unlimited freedom in structuring the phishing webpage; and (b)
a webpage can be characterized by a small set of key terms; how these key terms
are used in different parts of a webpage is different in the case of legitimate
and phishing webpages. Based on these observations, we develop a phishing
detection system with several notable properties: it is language-independent,
can be implemented entirely on client-side, has excellent classification
performance and is fast. In addition, we developed a target identification
component that can identify the target website that a phishing webpage is
attempting to mimic. The target detection component is faster than previously
reported systems and can help minimize false positives in our phishing
detection system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06503</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06503</id><created>2015-10-22</created><authors><author><keyname>Shu</keyname><forenames>Xiangbo</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Lai</keyname><forenames>Hanjiang</forenames></author><author><keyname>Liu</keyname><forenames>Luoqi</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Personalized Age Progression with Aging Dictionary</title><categories>cs.CV</categories><comments>in International Conference on Computer Vision, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we aim to automatically render aging faces in a personalized
way. Basically, a set of age-group specific dictionaries are learned, where the
dictionary bases corresponding to the same index yet from different
dictionaries form a particular aging process pattern cross different age
groups, and a linear combination of these patterns expresses a particular
personalized aging process. Moreover, two factors are taken into consideration
in the dictionary learning process. First, beyond the aging dictionaries, each
subject may have extra personalized facial characteristics, e.g. mole, which
are invariant in the aging process. Second, it is challenging or even
impossible to collect faces of all age groups for a particular subject, yet
much easier and more practical to get face pairs from neighboring age groups.
Thus a personality-aware coupled reconstruction loss is utilized to learn the
dictionaries based on face pairs from neighboring age groups. Extensive
experiments well demonstrate the advantages of our proposed solution over other
state-of-the-arts in term of personalized aging progression, as well as the
performance gain for cross-age face verification by synthesizing aging faces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06507</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06507</id><created>2015-10-22</created><authors><author><keyname>Oshima</keyname><forenames>Satoshi</forenames></author><author><keyname>Mochizuki</keyname><forenames>Rica</forenames></author><author><keyname>Lenz</keyname><forenames>Reiner</forenames></author><author><keyname>Chao</keyname><forenames>Jinhui</forenames></author></authors><title>Modelling, Measuring and Compensating Color Weak Vision</title><categories>cs.CV</categories><comments>Full resolution color pictures are available from the authors</comments><acm-class>I.2.10; I.4.8; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use methods from Riemann geometry to investigate transformations between
the color spaces of color-normal and color weak observers. The two main
applications are the simulation of the perception of a color weak observer for
a color normal observer and the compensation of color images in a way that a
color weak observer has approximately the same perception as a color normal
observer. The metrics in the color spaces of interest are characterized with
the help of ellipsoids defined by the just-noticable-differences between color
which are measured with the help of color-matching experiments. The constructed
mappings are isometries of Riemann spaces that preserve the perceived
color-differences for both observers. Among the two approaches to build such an
isometry, we introduce normal coordinates in Riemann spaces as a tool to
construct a global color-weak compensation map. Compared to previously used
methods this method is free from approximation errors due to local
linearizations and it avoids the problem of shifting locations of the origin of
the local coordinate system. We analyse the variations of the Riemann metrics
for different observers obtained from new color matching experiments and
describe three variations of the basic method. The performance of the methods
is evaluated with the help of semantic differential (SD) tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06516</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06516</id><created>2015-10-22</created><authors><author><keyname>van de Hoef</keyname><forenames>Sebastian</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Coordinating Truck Platooning by Clustering Pairwise Fuel-Optimal Plans</title><categories>cs.SY</categories><comments>To appear in the proceedings of Intelligent Transporation Systems
  (ITSC), 2015 IEEE 18th International Conference on, 15-18 September 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fuel-optimal coordination of trucks into platoons. Truck
platooning is a promising technology that enables trucks to save significant
amounts of fuel by driving close together and thus reducing air drag. We study
how fuel-optimal speed profiles for platooning can be computed. A first-order
fuel model is considered and pairwise optimal plans are derived. We formulate
an optimization problem that combines these pairwise plans into an overall plan
for a large number of trucks. The problem resembles a medoids clustering
problem. We propose an approximation algorithm similar to the partitioning
around medoids algorithm and discuss its convergence. The method is evaluated
with Monte Carlo simulations. We demonstrate that the proposed algorithm can
compute a plan for thousands of trucks and that significant fuel savings can be
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06527</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06527</id><created>2015-10-22</created><authors><author><keyname>Boulogeorgos</keyname><forenames>Alexandros-Apostolos A.</forenames></author><author><keyname>Chatzidiamantis</keyname><forenames>Nestor D.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Spectrum Sensing Under Hardware Constraints</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct-conversion radio (DCR) receivers can offer highly integrated low-cost
hardware solutions for spectrum sensing in cognitive radio systems. However,
DCR receivers are susceptible to radio frequency (RF) impairments, such as
in-phase and quadrature-phase imbalance, low-noise amplifier nonlinearities and
phase noise, which limit the spectrum sensing capabilities. In this paper, we
investigate the joint effects of RF impairments on energy detection based
spectrum sensing for cognitive radio (CR) systems in multi-channel
environments. In particular, we provide closed-form expressions for the
evaluation of the detection and false alarm probabilities, assuming Rayleigh
fading. Furthermore, we extend the analysis to the case of CR networks with
cooperative sensing, where the secondary users suffer from different levels of
RF imperfections, considering both scenarios of error free and imperfect
reporting channel. Numerical and simulation results demonstrate the accuracy of
the analysis as well as the detrimental effects of RF imperfections on the
spectrum sensing performance, which bring significant losses in the
spectrum~utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06530</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06530</id><created>2015-10-22</created><updated>2016-01-27</updated><authors><author><keyname>Parruca</keyname><forenames>Donald</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author></authors><title>Throughput Analysis of Proportional Fair Scheduling For Sparse and
  Ultra-Dense Interference-Limited OFDMA/LTE Networks</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equations (45)-(49)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various system tasks like interference coordination, handover decisions,
admission control etc. in current cellular networks require precise mid-term
(spanning over a few seconds) performance models. Due to channel-dependent
scheduling at the base station, these performance models are not simple to
obtain. Furthermore, LTE cellular systems are interference-limited, hence, the
way interference is modelled is crucial for the accuracy. In this paper we
present a closed-form analytical performance model for proportional fair
scheduling in OFDMA/LTE networks. The model takes into account a precise SINR
distribution into account. We refine our model with respect to uniform
modulation and coding, as applied in LTE networks. Furthermore, the analytical
analysis is extended also for ultra-dense deployments likely to happen in the
5-th generation of cellular networks. The resulting analytical performance
model is validated by means of simulations considering realistic network
deployments. Compared with related work, our model demonstrates a significantly
higher accuracy for long-term throughput estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06535</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06535</id><created>2015-10-22</created><authors><author><keyname>Hansen</keyname><forenames>Thomas Dueholm</forenames></author><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author><author><keyname>Zwick</keyname><forenames>Uri</forenames></author></authors><title>Hollow Heaps</title><categories>cs.DS</categories><comments>27 pages, 7 figures, preliminary version appeared in ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the hollow heap, a very simple data structure with the same
amortized efficiency as the classical Fibonacci heap. All heap operations
except delete and delete-min take $O(1)$ time, worst case as well as amortized;
delete and delete-min take $O(\log n)$ amortized time on a heap of $n$ items.
Hollow heaps are by far the simplest structure to achieve this. Hollow heaps
combine two novel ideas: the use of lazy deletion and re-insertion to do
decrease-key operations, and the use of a dag (directed acyclic graph) instead
of a tree or set of trees to represent a heap. Lazy deletion produces hollow
nodes (nodes without items), giving the data structure its name.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06541</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06541</id><created>2015-10-22</created><authors><author><keyname>Che</keyname><forenames>Yueling</forenames></author><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Multi-antenna Wireless Powered Communication with Co-channel Energy and
  Information Transfer</title><categories>cs.IT math.IT</categories><comments>IEEE Communications Letters. Accepted. 9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies a multi-antenna wireless powered communication (WPC)
system with co-channel energy and information transfer, where a wireless device
(WD), powered up by wireless energy transfer (WET) from an energy transmitter
(ET), communicates to an information receiver (IR) over the same frequency
band. We maximize the achievable data rate from the WD to the IR by jointly
optimizing the energy beamforming at the ET and the information beamforming at
the WD, subject to their individual transmit power constraints. We obtain the
optimal solution to this problem in closed-form, where the optimal energy
beamforming at the ET achieves a best energy/interference tradeoff between
maximizing the energy transfer efficiency to the WD and minimizing the
co-channel interference to the IR. Numerical results show that our proposed
optimal co-channel design is superior to other reference schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06547</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06547</id><created>2015-10-22</created><authors><author><keyname>Safiulin</keyname><forenames>Illia</forenames></author><author><keyname>Schwarz</keyname><forenames>Stefan</forenames></author><author><keyname>Philosof</keyname><forenames>Tal</forenames></author><author><keyname>Rupp</keyname><forenames>Markus</forenames></author></authors><title>Latency and Resource Utilization Analysis for V2X Communication over LTE
  MBSFN Transmission</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the performance of LTE Multicast-Broadcast
Single-Frequency Networks (MBSFN). LTE-MBSFN is viewed as one of the most
promising candidates for vehicular communications which can enhance reliability
of vehicular application traffic. This is achieved due to the possibility to
efficiently support message exchange in-between vehicles by multicasting
information to several vehicles in parallel (point-to-multipoint transmission)
employing an Multimedia Broadcast/Multicast Service (MBMS). We investigate two
metrics to gauge the performance of MBMS/MBSFN transmissions in comparison with
standard unicast transmissions for vehicular communications: latency of packet
delivery and overhead caused by vehicular traffic, i.e., network utilization.
Additionally, we present technique of prediction of system behaviour and
explore the influence of transmission bandwidth and transmission rate on
mentioned metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06549</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06549</id><created>2015-10-22</created><authors><author><keyname>Li</keyname><forenames>Aaron Q</forenames></author></authors><title>Multi-GPU Distributed Parallel Bayesian Differential Topic Modelling</title><categories>cs.CL cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an explosion of data, documents, and other content, and people
require tools to analyze and interpret these, tools to turn the content into
information and knowledge. Topic modeling have been developed to solve these
problems. Topic models such as LDA [Blei et. al. 2003] allow salient patterns
in data to be extracted automatically. When analyzing texts, these patterns are
called topics. Among numerous extensions of LDA, few of them can reliably
analyze multiple groups of documents and extract topic similarities. Recently,
the introduction of differential topic modeling (SPDP) [Chen et. al. 2012]
performs uniformly better than many topic models in a discriminative setting.
  There is also a need to improve the sampling speed for topic models. While
some effort has been made for distributed algorithms, there is no work
currently done using graphical processing units (GPU). Note the GPU framework
has already become the most cost-efficient platform for many problems.
  In this thesis, I propose and implement a scalable multi-GPU distributed
parallel framework which approximates SPDP. Through experiments, I have shown
my algorithms have a gain in speed of about 50 times while being almost as
accurate, with only one single cheap laptop GPU. Furthermore, I have shown the
speed improvement is sublinearly scalable when multiple GPUs are used, while
fairly maintaining the accuracy. Therefore on a medium-sized GPU cluster, the
speed improvement could potentially reach a factor of a thousand.
  Note SPDP is just a representative of other extensions of LDA. Although my
algorithm is implemented to work with SPDP, it is designed to be a general
enough to work with other topic models. The speed-up on smaller collections
(i.e., 1000s of documents), means that these more complex LDA extensions could
now be done in real-time, thus opening up a new way of using these LDA models
in industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06553</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06553</id><created>2015-10-22</created><updated>2016-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Shi</forenames></author><author><keyname>Duncan</keyname><forenames>Stephen R.</forenames></author><author><keyname>Howey</keyname><forenames>David A.</forenames></author></authors><title>Observability analysis and state estimation of lithium-ion batteries in
  the presence of sensor biases</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the observability of one of the most commonly used
equivalent circuit models (ECMs) for lithium-ion batteries and presents a
method to estimate the state of charge (SOC) in the presence of sensor biases,
highlighting the importance of observability analysis for choosing appropriate
state estimation algorithms. Using a differential geometric approach, necessary
and sufficient conditions for the nonlinear ECM to be observable are derived
and are shown to be different from the conditions for the observability of the
linearised model. It is then demonstrated that biases in the measurements, due
to sensor ageing or calibration errors, can be estimated by applying a
nonlinear Kalman filter to an augmented model where the biases are incorporated
into the state vector. Experiments are carried out on a lithium-ion pouch cell
and three types of nonlinear filters, the first-order extended Kalman filter
(EKF), the second-order EKF and the unscented Kalman filter (UKF) are applied
using experimental data. The different performances of the filters are
explained from the point of view of observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06567</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06567</id><created>2015-10-22</created><authors><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author><author><keyname>Flamary</keyname><forenames>R&#xe9;mi</forenames><affiliation>LAGRANGE, OCA</affiliation></author><author><keyname>Courty</keyname><forenames>Nicolas</forenames><affiliation>OBELIX</affiliation></author></authors><title>Generalized conditional gradient: analysis of convergence and
  applications</title><categories>cs.LG math.OC stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objectives of this technical report is to provide additional results on
the generalized conditional gradient methods introduced by Bredies et al.
[BLM05]. Indeed , when the objective function is smooth, we provide a novel
certificate of optimality and we show that the algorithm has a linear
convergence rate. Applications of this algorithm are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06572</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06572</id><created>2015-10-22</created><authors><author><keyname>Zheng</keyname><forenames>Kan</forenames></author><author><keyname>Hu</keyname><forenames>Fanglong</forenames></author><author><keyname>Xiang</keyname><forenames>Wei</forenames></author><author><keyname>Dohler</keyname><forenames>Mischa</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author></authors><title>Radio Resource Allocation in LTE-Advanced Cellular Networks with M2M
  Communications</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine-to-machine (M2M) communications are expected to provide ubiquitous
connectivity between machines without the need of human intervention. To
support such a large number of autonomous devices, the M2M system architecture
needs to be extremely power and spectrally efficient. This article thus briefly
reviews the features of M2M services in the third generation (3G) long-term
evolution and its advancement (LTE-Advanced) networks. Architectural
enhancements are then presented for supporting M2M services in LTE-Advanced
cellular networks. To increase spectral efficiency, the same spectrum is
expected to be utilized for human-to-human (H2H) communications as well as M2M
communications. We therefore present various radio resource allocation schemes
and quantify their utility in LTE-Advanced cellular networks. System-level
simulation results are provided to validate the performance effectiveness of
M2M communications in LTE-Advanced cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06578</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06578</id><created>2015-10-22</created><authors><author><keyname>Karas</keyname><forenames>Dimitrios S.</forenames></author><author><keyname>Boulogeorgos</keyname><forenames>Alexandros-Apostolos A.</forenames></author><author><keyname>Mihos</keyname><forenames>Sotirios K.</forenames></author><author><keyname>Kapinas</keyname><forenames>Vasileios M.</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author></authors><title>Securing the MIMO Wiretap Channel with Polar Codes and Encryption</title><categories>cs.IT math.IT</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes have been proven to be capacity achieving for any binary-input
discrete memoryless channel, while at the same time they can reassure secure
and reliable transmission over the single-input single-output wireless channel.
However, the use of polar codes to secure multiple-antenna transmission and
reception has not yet been reported in the open literature. In this paper, we
assume a multiple-input multiple-output wiretap channel, where the legitimate
receiver and the eavesdropper are equipped with the same number of antennas. We
introduce a protocol that exploits the properties of both physical and media
access control layer security by employing polar coding and encryption
techniques in a hybrid manner in order to guarantee secure transmission. A
novel security technique is also proposed, where a cryptographic key is
generated based on the information transmitted and renewed every transmission
block without the need for a separate key exchange method. Finally, to
illustrate the effectiveness of the proposed protocol, we prove the weak and
strong security conditions, and we provide a practical method to achieve
computational security for the cases where these conditions cannot be
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06579</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06579</id><created>2015-10-22</created><authors><author><keyname>Zheng</keyname><forenames>Kan</forenames></author><author><keyname>Hou</keyname><forenames>Lu</forenames></author><author><keyname>Meng</keyname><forenames>Hanlin</forenames></author><author><keyname>Zheng</keyname><forenames>Qiang</forenames></author><author><keyname>Lu</keyname><forenames>Ning</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author></authors><title>Soft-Defined Heterogeneous Vehicular Network: Architecture and
  Challenges</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous Vehicular NETworks (HetVNETs) can meet various
quality-of-service (QoS) requirements for intelligent transport system (ITS)
services by integrating different access networks coherently. However, the
current network architecture for HetVNET cannot efficiently deal with the
increasing demands of rapidly changing network landscape. Thanks to the
centralization and flexibility of the cloud radio access network (Cloud-RAN),
soft-defined networking (SDN) can conveniently be applied to support the
dynamic nature of future HetVNET functions and various applications while
reducing the operating costs. In this paper, we first propose the multi-layer
Cloud RAN architecture for implementing the new network, where the multi-domain
resources can be exploited as needed for vehicle users. Then, the high-level
design of soft-defined HetVNET is presented in detail. Finally, we briefly
discuss key challenges and solutions for this new network, corroborating its
feasibility in the emerging fifth-generation (5G) era.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06582</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06582</id><created>2015-10-22</created><authors><author><keyname>Hawelka</keyname><forenames>Bartosz</forenames></author><author><keyname>Sitko</keyname><forenames>Izabela</forenames></author><author><keyname>Kazakopoulos</keyname><forenames>Pavlos</forenames></author><author><keyname>Beinat</keyname><forenames>Euro</forenames></author></authors><title>Collective Prediction of Individual Mobility Traces with Exponential
  Weights</title><categories>physics.soc-ph cs.CY cs.LG stat.ML</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and test a sequential learning algorithm for the short-term
prediction of human mobility. This novel approach pairs the Exponential Weights
forecaster with a very large ensemble of experts. The experts are individual
sequence prediction algorithms constructed from the mobility traces of 10
million roaming mobile phone users in a European country. Average prediction
accuracy is significantly higher than that of individual sequence prediction
algorithms, namely constant order Markov models derived from the user's own
data, that have been shown to achieve high accuracy in previous studies of
human mobility prediction. The algorithm uses only time stamped location data,
and accuracy depends on the completeness of the expert ensemble, which should
contain redundant records of typical mobility patterns. The proposed algorithm
is applicable to the prediction of any sufficiently large dataset of sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06585</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06585</id><created>2015-10-22</created><authors><author><keyname>Soldado</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Alexandre</keyname><forenames>Fernando</forenames></author><author><keyname>Paulino</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Execution of Compound Multi-Kernel OpenCL Computations in
  Multi-CPU/Multi-GPU Environments</title><categories>cs.DC</categories><comments>in Concurrency Computat.: Pract. Exper., 2015</comments><doi>10.1002/cpe.3612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current computational systems are heterogeneous by nature, featuring a
combination of CPUs and GPUs. As the latter are becoming an established
platform for high-performance computing, the focus is shifting towards the
seamless programming of these hybrid systems as a whole. The distinct nature of
the architectural and execution models in place raises several challenges, as
the best hardware configuration is behaviour and workload dependent. In this
paper, we address the execution of compound, multi-kernel, OpenCL computations
in multi-CPU/multi-GPU environments. We address how these computations may be
efficiently scheduled onto the target hardware, and how the system may adapt
itself to changes in the workload to process and to fluctuations in the CPU's
load. An experimental evaluation attests the performance gains obtained by the
conjoined use of the CPU and GPU devices, when compared to GPU-only executions,
and also by the use of data-locality optimizations in CPU environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06587</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06587</id><created>2015-10-22</created><authors><author><keyname>Jamroga</keyname><forenames>Wojciech</forenames></author><author><keyname>Knapik</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Kurpiewski</keyname><forenames>Damian</forenames></author></authors><title>Approximating Strategic Abilities under Imperfect Information: a Naive
  Approach</title><categories>cs.MA cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Alternating-time temporal logic ATL allows to specify requirements on
abilities that different agents should (or should not) possess in a multi-agent
system. However, model checking ATL specifications in realistic systems is
computationally hard. In particular, if the agents don't have perfect
information about the global state of the system, the complexity ranges from
Delta2P to undecidable, depending on syntactic and semantic details. The
problem is also hard in practice, as evidenced by several recent attempts to
tackle it. In this work, we propose to approximate model checking of ATL with
imperfect information by verification of their &quot;naive&quot; translations to
alternating epistemic mu-calculus. In other words, we look at what happens when
one uses the (incorrect) fixpoint algorithm to verify formulae of ATLir.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06595</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06595</id><created>2015-10-22</created><authors><author><keyname>Kr&#xfc;ger</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>V&#xf6;gele</keyname><forenames>Anna</forenames></author><author><keyname>Willig</keyname><forenames>Tobias</forenames></author><author><keyname>Yao</keyname><forenames>Angela</forenames></author><author><keyname>Klein</keyname><forenames>Reinhard</forenames></author><author><keyname>Weber</keyname><forenames>Andreas</forenames></author></authors><title>Efficient Unsupervised Temporal Segmentation of Motion Data</title><categories>cs.CV</categories><comments>15 pages, submitted to TPAMI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method for automated temporal segmentation of human motion
data into distinct actions and compositing motion primitives based on
self-similar structures in the motion sequence. We use neighbourhood graphs for
the partitioning and the similarity information in the graph is further
exploited to cluster the motion primitives into larger entities of semantic
significance. The method requires no assumptions about the motion sequences at
hand and no user interaction is required for the segmentation or clustering. In
addition, we introduce a feature bundling preprocessing technique to make the
segmentation more robust to noise, as well as a notion of motion symmetry for
more refined primitive detection. We test our method on several sensor
modalities, including markered and markerless motion capture as well as on
electromyograph and accelerometer recordings. The results highlight our
system's capabilities for both segmentation and for analysis of the finer
structures of motion data, all in a completely unsupervised manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06603</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06603</id><created>2015-10-22</created><authors><author><keyname>van de Hoef</keyname><forenames>Sebastian</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Fuel-Optimal Centralized Coordination of Truck Platooning Based on
  Shortest Paths</title><categories>cs.SY</categories><journal-ref>American Control Conference (ACC), 2015, pp.3740-3745, 1-3 July
  2015</journal-ref><doi>10.1109/ACC.2015.7171911</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Platooning is a way to significantly reduce fuel consumption of trucks.
Vehicles that drive at close inter-vehicle distance assisted by automatic
controllers experience substantially lower air-drag. In this paper, we deal
with the problem of coordinating the formation and the breakup of platoons in a
fuel-optimal way. We formulate an optimization problem which accounts for
routing, speed-dependent fuel consumption, and platooning decisions. An
algorithm to obtain an approximate solution to the problem is presented. It
first determines the shortest path for each truck. Then, possible platoon
configurations are identified. For a certain platoon configuration the optimal
speed profile is the solution to a convex program. The algorithm is illustrated
by a realistic example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06607</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06607</id><created>2015-10-22</created><authors><author><keyname>Zheng</keyname><forenames>Kan</forenames></author><author><keyname>Zheng</keyname><forenames>Qiang</forenames></author><author><keyname>Yang</keyname><forenames>Haojun</forenames></author><author><keyname>Zhao</keyname><forenames>Long</forenames></author><author><keyname>Hou</keyname><forenames>Lu</forenames></author><author><keyname>Chatzimisios</keyname><forenames>Periklis</forenames></author></authors><title>Reliable and Efficient Autonomous Driving: the Need for Heterogeneous
  Vehicular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous driving technology has been regarded as a promising solution to
reduce road accidents and traffic congestion, as well as to optimize the usage
of fuel and lane. Reliable and high efficient Vehicle-to-Vehicle (V2V) and
Vehicle-to-Infrastructure (V2I) communications are essential to let commercial
autonomous driving vehicles be on the road before 2020. The current paper
firstly presents the concept of Heterogeneous Vehicular NETworks (HetVNETs) for
autonomous driving, in which an improved protocol stack is proposed to satisfy
the communication requirements of not only safety but also non-safety services.
We then consider and study in detail several typical scenarios for autonomous
driving. In order to tackle the potential challenges raised by the autonomous
driving vehicles in HetVNETs, new techniques from transmission to networking
are proposed as potential solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06623</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06623</id><created>2015-10-22</created><authors><author><keyname>Dowsley</keyname><forenames>Rafael</forenames></author><author><keyname>Lacerda</keyname><forenames>Felipe</forenames></author><author><keyname>Nascimento</keyname><forenames>Anderson C. A.</forenames></author></authors><title>Commitment and Oblivious Transfer in the Bounded Storage Model with
  Errors</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the bounded storage model the memory of the adversary is restricted,
instead of its computational power. With this different restriction it is
possible to design protocols with information-theoretical (instead of only
computational) security. We present the first protocols for commitment and
oblivious transfer in the bounded storage model with errors, i.e., the model
where the public random sources available to the two parties are not exactly
the same, but instead are only required to have a small Hamming distance
between themselves. Commitment and oblivious transfer protocols were known
previously only for the error-free variant of the bounded storage model, which
is harder to realize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06634</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06634</id><created>2015-10-22</created><authors><author><keyname>Tsakmalis</keyname><forenames>Anestis</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Centralized Power Control in Cognitive Radio Networks Using Modulation
  and Coding Classification Feedback</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a centralized Power Control (PC) scheme and an interference
channel learning method are jointly tackled to allow a Cognitive Radio Network
(CRN) access to the frequency band of a Primary User (PU) operating based on an
Adaptive Coding and Modulation (ACM) protocol. The learning process enabler is
a cooperative Modulation and Coding Classification (MCC) technique which
estimates the Modulation and Coding scheme (MCS) of the PU. Due to the lack of
cooperation between the PU and the CRN, the CRN exploits this multilevel MCC
sensing feedback as implicit channel state information (CSI) of the PU link in
order to constantly monitor the impact of the aggregated interference it
causes. In this paper, an algorithm is developed for maximizing the CRN
throughput (the PC optimization objective) and simultaneously learning how to
mitigate PU interference (the optimization problem constraint) by using only
the MCC information. Ideal approaches for this problem setting with high
convergence rate are the cutting plane methods (CPM). Here, we focus on the
analytic center cutting plane method (ACCPM) and the center of gravity cutting
plane method (CGCPM) whose effectiveness in the proposed simultaneous PC and
interference channel learning algorithm is demonstrated through numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06646</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06646</id><created>2015-10-22</created><updated>2016-02-27</updated><authors><author><keyname>Khalifa</keyname><forenames>Osama</forenames></author><author><keyname>Corne</keyname><forenames>David Wolfe</forenames></author><author><keyname>Chantler</keyname><forenames>Mike</forenames></author></authors><title>A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya
  Parameters and Topic Models</title><categories>cs.LG cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyper-parameters play a major role in the learning and inference process of
latent Dirichlet allocation (LDA). In order to begin the LDA latent variables
learning process, these hyper-parameters values need to be pre-determined. We
propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs
Newton' (LDA-GN), which places non-informative priors over these
hyper-parameters and uses Gibbs sampling to learn appropriate values for them.
At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new
technique for learning the parameters of multivariate Polya distributions. We
report Gibbs-Newton performance results compared with two prominent existing
approaches to the latter task: Minka's fixed-point iteration method and the
Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with
standard LDA in terms of the ability of the resulting topic models to
generalize to unseen documents; (ii) by comparing it with standard LDA in its
performance on a binary classification task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06649</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06649</id><created>2015-10-22</created><authors><author><keyname>Rennela</keyname><forenames>Mathys</forenames></author></authors><title>Operator Algebras in Quantum Computation</title><categories>cs.LO math.CT math.OA</categories><comments>37 pages, made under the supervision of Prof. dr. B.P.F. Jacobs in
  partial fulfillment of the Master Parisien de Recherche en Informatique
  (MPRI) of Universit\'e Paris 7 Diderot</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this master thesis, I discuss how the theory of operator algebras, also
called operator theory, can be applied in quantum computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06658</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06658</id><created>2015-10-22</created><updated>2016-01-05</updated><authors><author><keyname>Debois</keyname><forenames>S&#xf8;ren</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Hildebrandt</keyname><forenames>Thomas</forenames><affiliation>IT University of Copenhagen</affiliation></author><author><keyname>Slaats</keyname><forenames>Tijs</forenames><affiliation>IT University of Copenhagen, Exformatics A/S</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College, London, Department of Computing</affiliation></author></authors><title>Type-checking Liveness for Collaborative Processes with Bounded and
  Unbounded Recursion</title><categories>cs.LO</categories><comments>Accepted for publication in Logical Methods in Computer Science</comments><proxy>LMCS</proxy><journal-ref>LMCS 12 (1:1) 2016</journal-ref><doi>10.2168/LMCS-12(1:1)2016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first session typing system guaranteeing request-response
liveness properties for possibly non-terminating communicating processes. The
types augment the branch and select types of the standard binary session types
with a set of required responses, indicating that whenever a particular label
is selected, a set of other labels, its responses, must eventually also be
selected. We prove that these extended types are strictly more expressive than
standard session types. We provide a type system for a process calculus similar
to a subset of collaborative BPMN processes with internal (data-based) and
external (event-based) branching, message passing, bounded and unbounded
looping. We prove that this type system is sound, i.e., it guarantees
request-response liveness for dead-lock free processes. We exemplify the use of
the calculus and type system on a concrete example of an infinite state system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06659</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06659</id><created>2015-10-22</created><authors><author><keyname>Bourtsoulatze</keyname><forenames>Eirina</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Saltarin</keyname><forenames>Jonnahtan</forenames></author><author><keyname>Braun</keyname><forenames>Torsten</forenames></author></authors><title>Content-Aware Delivery of Scalable Video in Network Coding Enabled Named
  Data Networks</title><categories>cs.NI cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel network coding enabled NDN architecture for
the delivery of scalable video. Our scheme utilizes network coding in order to
address the problem that arises in the original NDN protocol, where optimal use
of the bandwidth and caching resources necessitates the coordination of the
forwarding decisions. To optimize the performance of the proposed network
coding based NDN protocol and render it appropriate for transmission of
scalable video, we devise a novel rate allocation algorithm that decides on the
optimal rates of Interest messages sent by clients and intermediate nodes. This
algorithm guarantees that the achieved flow of Data objects will maximize the
average quality of the video delivered to the client population. To support the
handling of Interest messages and Data objects when intermediate nodes perform
network coding, we modify the standard NDN protocol and introduce the use of
Bloom filters, which store efficiently additional information about the
Interest messages and Data objects. The proposed architecture is evaluated for
transmission of scalable video over PlanetLab topologies. The evaluation shows
that the proposed scheme performs very close to the optimal performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06664</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06664</id><created>2015-10-22</created><updated>2015-10-25</updated><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Caltagirone</keyname><forenames>Francesco</forenames></author><author><keyname>Carron</keyname><forenames>Igor</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author><author><keyname>Dr&#xe9;meau</keyname><forenames>Ang&#xe9;lique</forenames></author><author><keyname>Gigan</keyname><forenames>Sylvain</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author></authors><title>Random Projections through multiple optical scattering: Approximating
  kernels at the speed of light</title><categories>cs.ET cs.LG physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random projections have proven extremely useful in many signal processing and
machine learning applications. However, they often require either to store a
very large random matrix, or to use a different, structured matrix to reduce
the computational and memory costs. Here, we overcome this difficulty by
proposing an analog, optical device, that performs the random projections
literally at the speed of light without having to store any matrix in memory.
This is achieved using the physical properties of multiple coherent scattering
of coherent light in random media. We use this device on a simple task of
classification with a kernel machine, and we show that, on the MNIST database,
the experimental results closely match the theoretical performance of the
corresponding kernel. This framework can help make kernel methods practical for
applications that have large training sets and/or require real-time prediction.
We discuss possible extensions of the method in terms of a class of kernels,
speed, memory consumption and different problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06684</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06684</id><created>2015-10-22</created><authors><author><keyname>He</keyname><forenames>Xi</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Dual Free SDCA for Empirical Risk Minimization with Adaptive
  Probabilities</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop dual free SDCA with adaptive probabilities for
regularized empirical risk minimization. This extends recent work of Shai
Shalev-Shwartz [SDCA without Duality, arXiv:1502.06177] to allow non-uniform
selection of &quot;dual&quot; coordinate in SDCA. Moreover, the probability can change
over time, making it more efficient than uniform selection. Our work focuses on
generating adaptive probabilities through iterative process, preferring to
choose coordinate with highest potential to decrease sub-optimality. We also
propose a practical variant Algorithm adfSDCA+ which is more aggressive. The
work is concluded with multiple experiments which shows efficiency of proposed
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06688</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06688</id><created>2015-10-22</created><authors><author><keyname>Ma</keyname><forenames>Chenxin</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Partitioning Data on Features or Samples in Communication-Efficient
  Distributed Optimization?</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the effect of the way that the data is partitioned in
distributed optimization. The original DiSCO algorithm [Communication-Efficient
Distributed Optimization of Self-Concordant Empirical Loss, Yuchen Zhang and
Lin Xiao, 2015] partitions the input data based on samples. We describe how the
original algorithm has to be modified to allow partitioning on features and
show its efficiency both in theory and also in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06689</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06689</id><created>2015-10-22</created><updated>2016-02-23</updated><authors><author><keyname>Austin</keyname><forenames>Woody</forenames></author><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Parallel Tensor Compression for Large-Scale Scientific Data</title><categories>cs.NA cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As parallel computing trends towards the exascale, scientific data produced
by high-fidelity simulations are growing increasingly massive. For instance, a
simulation on a three-dimensional spatial grid with 512 points per dimension
that tracks 64 variables per grid point for 128 time steps yields 8~TB of data,
assuming double precision. By viewing the data as a dense five-way tensor, we
can compute a Tucker decomposition to find inherent low-dimensional multilinear
structure, achieving compression ratios of up to 5000 on real-world data sets
with negligible loss in accuracy. So that we can operate on such massive data,
we present the first-ever distributed-memory parallel implementation for the
Tucker decomposition, whose key computations correspond to parallel linear
algebra operations, albeit with nonstandard data layouts. Our approach
specifies a data distribution for tensors that avoids any tensor data
redistribution, either locally or in parallel. We provide accompanying analysis
of the computation and communication costs of the algorithms. To demonstrate
the compression and accuracy of the method, we apply our approach to real-world
data sets from combustion science simulations. We also provide detailed
performance results, including parallel performance in both weak and strong
scaling experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06700</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06700</id><created>2015-10-22</created><authors><author><keyname>Schwerdtfeger</keyname><forenames>Konrad W.</forenames></author></authors><title>Connectivity of Boolean Satisfiability</title><categories>cs.CC cs.LO</categories><comments>PhD thesis, 82 pages, contains all results from the previous papers
  arXiv:1312.4524, arXiv:1312.6679, and arXiv:1403.6165, plus additional
  findings. arXiv admin note: text overlap with arXiv:cs/0609072 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Boolean satisfiability problems, the structure of the solution space is
characterized by the solution graph, where the vertices are the solutions, and
two solutions are connected iff they differ in exactly one variable. For this
implicitly defined graph, we here study the st-connectivity and connectivity
problems.
  Building on the work of Gopalan et al. (&quot;The Connectivity of Boolean
Satisfiability: Computational and Structural Dichotomies&quot;, 2006/2009), we first
investigate satisfiability problems given by CSPs, more exactly CNF(S)-formulas
with constants (as considered in Schaefer's famous 1978 dichotomy theorem); we
prove a computational dichotomy for the st-connectivity problem, asserting that
it is either solvable in polynomial time or PSPACE-complete, and an aligned
structural dichotomy, asserting that the maximal diameter of connected
components is either linear in the number of variables, or can be exponential;
further, we show a trichotomy for the connectivity problem, asserting that it
is either in P, coNP-complete, or PSPACE-complete.
  Next we investigate two important variants: CNF(S)-formulas without
constants, and partially quantified formulas; in both cases, we prove analogous
dichotomies for st-connectivity and the diameter; for for the connectivity
problem, we show a trichotomy in the case of quantified formulas, while in the
case of formulas without constants, we identify fragments of a possible
trichotomy.
  Finally, we consider the connectivity issues for B-formulas, which are
arbitrarily nested formulas built from some fixed set B of connectives, and for
B-circuits, which are Boolean circuits where the gates are from some finite set
B; we prove a common dichotomy for both connectivity problems and the diameter;
for partially quantified B-formulas, we show an analogous dichotomy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06701</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06701</id><created>2015-10-22</created><authors><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Schnez</keyname><forenames>Stephan</forenames></author></authors><title>On the Take-off of Airborne Wind Energy Systems Based on Rigid Wings</title><categories>math.OC cs.SY</categories><comments>This is the pre-print of a paper submitted for possible publication
  to the Elsevier journal &quot;Energy&quot;. The authors are with ABB Switzerland Ltd.,
  Corporate Research. Both authors contributed equally to this publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of launching a tethered aircraft to be used for airborne wind
energy generation is investigated. Exploiting well-assessed physical
principles, an analysis of three different take-off approaches is carried out.
The approaches are then compared on the basis of quantitative and qualitative
criteria introduced to assess their technical and economic viability. Finally,
a deeper study of the concept that is deemed the most viable one, i.e. a linear
take-off maneuver combined with on-board propellers, is performed by means of
numerical simulations. The latter are used to refine the initial analysis in
terms of power required for take-off, and further confirm the viability of the
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06702</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06702</id><created>2015-10-22</created><authors><author><keyname>Wright</keyname><forenames>Matthew</forenames></author><author><keyname>Horowitz</keyname><forenames>Roberto</forenames></author></authors><title>Fusing Loop and GPS Probe Measurements to Estimate Freeway Density</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an age of ever-increasing penetration of GPS-enabled mobile devices, the
potential of real-time &quot;probe&quot; location information for estimating the state of
transportation networks is receiving increasing attention. Much work has been
done on using probe data to estimate the current speed of vehicle traffic (or
equivalently, trip travel time). While travel times are useful to individual
drivers, the state variable for a large class of traffic models and control
algorithms is vehicle density. We derive a method for using probe data to
enhance density estimates obtained using roadside sensors based on
Rao-Blackwellized particle filters, a sequential Monte Carlo scheme.
Subsequently, we present numerical results showing the utility of our scheme in
using probe data to improve vehicle density estimation, with high performance
in simulation and good performance with real data collected from a freeway in
Los Angeles, California.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06706</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06706</id><created>2015-10-22</created><authors><author><keyname>Zlateski</keyname><forenames>Aleksandar</forenames></author><author><keyname>Lee</keyname><forenames>Kisuk</forenames></author><author><keyname>Seung</keyname><forenames>H. Sebastian</forenames></author></authors><title>ZNN - A Fast and Scalable Algorithm for Training 3D Convolutional
  Networks on Multi-Core and Many-Core Shared Memory Machines</title><categories>cs.NE cs.CV cs.DC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks (ConvNets) have become a popular approach to computer
vision. It is important to accelerate ConvNet training, which is
computationally costly. We propose a novel parallel algorithm based on
decomposition into a set of tasks, most of which are convolutions or FFTs.
Applying Brent's theorem to the task dependency graph implies that linear
speedup with the number of processors is attainable within the PRAM model of
parallel computation, for wide network architectures. To attain such
performance on real shared-memory machines, our algorithm computes convolutions
converging on the same node of the network with temporal locality to reduce
cache misses, and sums the convergent convolution outputs via an almost
wait-free concurrent method to reduce time spent in critical sections. We
implement the algorithm with a publicly available software package called ZNN.
Benchmarking with multi-core CPUs shows that ZNN can attain speedup roughly
equal to the number of physical cores. We also show that ZNN can attain over
90x speedup on a many-core CPU (Xeon Phi Knights Corner). These speedups are
achieved for network architectures with widths that are in common use. The task
parallelism of the ZNN algorithm is suited to CPUs, while the SIMD parallelism
of previous algorithms is compatible with GPUs. Through examples, we show that
ZNN can be either faster or slower than certain GPU implementations depending
on specifics of the network architecture, kernel sizes, and density and size of
the output patch. ZNN may be less costly to develop and maintain, due to the
relative ease of general-purpose CPU programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06714</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06714</id><created>2015-10-22</created><authors><author><keyname>Zave</keyname><forenames>Pamela</forenames></author><author><keyname>Cheung</keyname><forenames>Eric</forenames></author><author><keyname>Yarosh</keyname><forenames>Svetlana</forenames></author></authors><title>Toward user-centric feature composition for the Internet of Things</title><categories>cs.HC cs.CY</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many user studies of home automation, as the most familiar representative of
the Internet of Things, have shown the difficulty of developing technology that
users understand and like. It helps to state requirements as
largely-independent features, but features are not truly independent, so this
incurs the cost of managing and explaining feature interactions. We propose to
compose features at runtime, resolving their interactions by means of priority.
Although the basic idea is simple, its details must be designed to make users
comfortable by balancing manual and automatic control. On the technical side,
its details must be designed to allow meaningful separation of features and
maximum generality. As evidence that our composition mechanism achieves its
goals, we present three substantive examples of home automation, and the
results of a user study to investigate comprehension of feature interactions. A
survey of related work shows that this proposal occupies a sensible place in a
design space whose dimensions include actuator type, detection versus
resolution strategies, and modularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06729</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06729</id><created>2015-10-22</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author></authors><title>Benefits of tolerance in public goods games</title><categories>physics.soc-ph cs.GT nlin.CG q-bio.PE</categories><comments>10 two-column pages, 8 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Physical Review E 92 (2015) 042813</journal-ref><doi>10.1103/PhysRevE.92.042813</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leaving the joint enterprise when defection is unveiled is always a viable
option to avoid being exploited. Although loner strategy helps the population
not to be trapped into the tragedy of the commons state, it could offer only a
modest income for non-participants. In this paper we demonstrate that showing
some tolerance toward defectors could not only save cooperation in harsh
environments, but in fact results in a surprisingly high average payoff for
group members in public goods games. Phase diagrams and the underlying spatial
patterns reveal the high complexity of evolving states where cyclic dominant
strategies or two-strategy alliances can characterize the final state of
evolution. We identify microscopic mechanisms which are responsible for the
superiority of global solutions containing tolerant players. This phenomenon is
robust and can be observed both in well-mixed and in structured populations
highlighting the importance of tolerance in our everyday life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06743</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06743</id><created>2015-10-22</created><authors><author><keyname>Arnon-Friedman</keyname><forenames>Rotem</forenames></author><author><keyname>Portmann</keyname><forenames>Christopher</forenames></author><author><keyname>Scholz</keyname><forenames>Volkher B.</forenames></author></authors><title>Quantum-proof multi-source randomness extractors in the Markov model</title><categories>quant-ph cs.CC cs.CR</categories><comments>20 pages + appendix, comments are welcome!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomness extractors, widely used in classical and quantum cryptography and
other fields of computer science, e.g., derandomization, are functions which
generate almost uniform randomness from weak sources of randomness. In the
quantum setting one must take into account the quantum side information held by
an adversary which might be used to break the security of the extractor. In the
case of seeded extractors the presence of quantum side information has been
extensively studied. For multi-source extractors one can easily see that high
conditional min-entropy is not sufficient to guarantee security against
arbitrary side information, even in the classical case. Hence, the interesting
question is under which models of (both quantum and classical) side information
multi-source extractors remain secure. In this work we suggest a natural model
of side information, which we call the Markov model, and prove that any
multi-source extractor remains secure in the presence of quantum side
information of this type (albeit with weaker parameters). This improves on
previous results in which more restricted models were considered and the
security of only some types of extractors was shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06750</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06750</id><created>2015-10-22</created><authors><author><keyname>Fefferman</keyname><forenames>Bill</forenames></author><author><keyname>Kimmel</keyname><forenames>Shelby</forenames></author></authors><title>Quantum vs Classical Proofs and Subset Verification</title><categories>quant-ph cs.CC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the ability of efficient quantum verifiers to decide properties of
exponentially large subsets given either a classical or quantum witness. We
develop a general framework that can be used to prove that QCMA machines, with
only classical witnesses, cannot verify certain properties of subsets given
implicitly via an oracle. We use this framework to prove an oracle separation
between QCMA and QMA using an ``in-place'' permutation oracle, making the first
progress on this question since Aaronson and Kuperberg in 2007. We also use the
framework to prove a particularly simple standard oracle separation between
QCMA and AM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06762</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06762</id><created>2015-10-22</created><authors><author><keyname>Blanca</keyname><forenames>Antonio</forenames></author><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author></authors><title>Random-Cluster Dynamics in $\mathbb{Z}^2$</title><categories>cs.DM math-ph math.MP math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random-cluster model has been widely studied as a unifying framework for
random graphs, spin systems and electrical networks, but its dynamics have so
far largely resisted analysis. In this paper we analyze the Glauber dynamics of
the random-cluster model in the canonical case where the underlying graph is an
$n \times n$ box in the Cartesian lattice $\mathbb{Z}^2$. Our main result is a
$O(n^2\log n)$ upper bound for the mixing time at all values of the model
parameter $p$ except the critical point $p=p_c(q)$, and for all values of the
second model parameter $q\ge 1$. We also provide a matching lower bound proving
that our result is tight. Our analysis takes as its starting point the recent
breakthrough by Beffara and Duminil-Copin on the location of the random-cluster
phase transition in $\mathbb{Z}^2$. It is reminiscent of similar results for
spin systems such as the Ising and Potts models, but requires the reworking of
several standard tools in the context of the random-cluster model, which is not
a spin system in the usual sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06767</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06767</id><created>2015-10-22</created><updated>2015-10-29</updated><authors><author><keyname>De la Calleja</keyname><forenames>E. M.</forenames></author><author><keyname>Cervantes</keyname><forenames>F.</forenames></author><author><keyname>De la Calleja</keyname><forenames>J.</forenames></author></authors><title>Order in Jackson Pollock`s paintings</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the degree of order of twenty two Jackson Pollock's paintings using
Hausdorff-Besicovitch fractal dimension. Through the maximum value of each
multi-fractal spectrum, the fractality of the artworks is presented as a
function of the year in which they were painted. It has been reported that
Pollock's paintings are fractal and it increased on his latest works. However
our results show that fractality of the paintings do not increase as a function
of the year in which they were painted, instead of that we identify a range of
fractal dimension with values close to two. This behavior can be interpreted as
the dark linked lines are structured with high degree of order. We used our
method to characterized by its fractal spectrum, the called \emph{Teri's Find}.
We obtained similar spectrums between \emph{Teri's Find} and \emph{Number 5}
from Pollock, suggesting that fractal dimension can not be completely rejected
as a quantitative parameter to authenticate this kind of artworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06769</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06769</id><created>2015-10-22</created><updated>2016-01-21</updated><authors><author><keyname>Eskimez</keyname><forenames>Sefik Emre</forenames></author><author><keyname>Imade</keyname><forenames>Kenneth</forenames></author><author><keyname>Yang</keyname><forenames>Na</forenames></author><author><keyname>Sturge-Apple</keyname><forenames>Melissa</forenames></author><author><keyname>Duan</keyname><forenames>Zhiyao</forenames></author><author><keyname>Heinzelman</keyname><forenames>Wendi</forenames></author></authors><title>Emotion Classification: How Does an Automated System Compare to Naive
  Human Coders?</title><categories>cs.HC</categories><comments>Accepted to ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fact that emotions play a vital role in social interactions, along with
the demand for novel human-computer interaction applications, have led to the
development of a number of automatic emotion classification systems. However,
it is still debatable whether the performance of such systems can compare with
human coders. To address this issue, in this study, we present a comprehensive
comparison in a speech-based emotion classification task between 138 Amazon
Mechanical Turk workers (Turkers) and a state-of-the-art automatic computer
system. The comparison includes classifying speech utterances into six emotions
(happy, neutral, sad, anger, disgust and fear), into three arousal classes
(active, passive, and neutral), and into three valence classes (positive,
negative, and neutral). The results show that the computer system outperforms
the naive Turkers in almost all cases. Furthermore, the computer system can
increase the classification accuracy by rejecting to classify utterances for
which it is not confident, while the Turkers do not show a significantly higher
classification accuracy on their confident utterances versus unconfident ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06786</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06786</id><created>2015-10-22</created><updated>2016-03-07</updated><authors><author><keyname>Kulkarni</keyname><forenames>Vivek</forenames></author><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>Freshman or Fresher? Quantifying the Geographic Variation of Internet
  Language</title><categories>cs.CL cs.IR cs.LG</categories><comments>11 pages (updated submission)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new computational technique to detect and analyze statistically
significant geographic variation in language. Our meta-analysis approach
captures statistical properties of word usage across geographical regions and
uses statistical methods to identify significant changes specific to regions.
While previous approaches have primarily focused on lexical variation between
regions, our method identifies words that demonstrate semantic and syntactic
variation as well.
  We extend recently developed techniques for neural language models to learn
word representations which capture differing semantics across geographical
regions. In order to quantify this variation and ensure robust detection of
true regional differences, we formulate a null model to determine whether
observed changes are statistically significant. Our method is the first such
approach to explicitly account for random variation due to chance while
detecting regional variation in word meaning.
  To validate our model, we study and analyze two different massive online data
sets: millions of tweets from Twitter spanning not only four different
countries but also fifty states, as well as millions of phrases contained in
the Google Book Ngrams. Our analysis reveals interesting facets of language
change at multiple scales of geographic resolution -- from neighboring states
to distant continents.
  Finally, using our model, we propose a measure of semantic distance between
languages. Our analysis of British and American English over a period of 100
years reveals that semantic variation between these dialects is shrinking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06788</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06788</id><created>2015-10-22</created><updated>2016-02-24</updated><authors><author><keyname>Chugh</keyname><forenames>Ravi</forenames></author></authors><title>Prodirect Manipulation: Bidirectional Programming for the Masses</title><categories>cs.SE</categories><comments>ICSE 2016 Companion Proceedings (Visions of 2025 Track), May 14-22,
  2016, Austin, TX, USA</comments><acm-class>D.3.3; H.5.2; D.2.6; F.3.2</acm-class><doi>10.1145/2889160.2889210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software interfaces today generally fall at either end of a spectrum. On one
end are programmable systems, which allow expert users (i.e. programmers) to
write software artifacts that describe complex abstractions, but programs are
disconnected from their eventual output. On the other end are domain-specific
graphical user interfaces (GUIs), which allow end users (i.e. non-programmers)
to easily create varied content but present insurmountable walls when a desired
feature is not built-in. Both programmatic and direct manipulation have
distinct strengths, but users must typically choose one over the other or use
some ad-hoc combination of systems. Our goal, put simply, is to bridge this
divide.
  We envision novel software systems that tightly couple programmatic and
direct manipulation --- a combination we dub prodirect manipulation --- for a
variety of use cases. This will require advances in a broad range of software
engineering disciplines, from program analysis and program synthesis technology
to user interface design and evaluation. In this extended abstract, we propose
two general strategies --- real-time program synthesis and domain-specific
synthesis of general-purpose programs --- that may prove fruitful for
overcoming the technical challenges. We also discuss metrics that will be
important in evaluating the usability and utility of prodirect manipulation
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06789</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06789</id><created>2015-10-22</created><authors><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Nagaj</keyname><forenames>Daniel</forenames></author><author><keyname>Schuch</keyname><forenames>Norbert</forenames></author></authors><title>Quantum proofs can be verified using only single qubit measurements</title><categories>quant-ph cs.CC</categories><comments>7 pages, 1 figure</comments><journal-ref>Phys. Rev. A 93, 022326 (2016)</journal-ref><doi>10.1103/PhysRevA.93.022326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QMA (Quantum Merlin Arthur) is the class of problems which, though
potentially hard to solve, have a quantum solution which can be verified
efficiently using a quantum computer. It thus forms a natural quantum version
of the classical complexity class NP (and its probabilistic variant MA,
Merlin-Arthur games), where the verifier has only classical computational
resources. In this paper, we study what happens when we restrict the quantum
resources of the verifier to the bare minimum: individual measurements on
single qubits received as they come, one-by-one. We find that despite this
grave restriction, it is still possible to soundly verify any problem in QMA
for the verifier with the minimum quantum resources possible, without using any
quantum memory or multiqubit operations. We provide two independent proofs of
this fact, based on measurement based quantum computation and the local
Hamiltonian problem, respectively. The former construction also applies to
QMA$_1$, i.e., QMA with one-sided error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06791</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06791</id><created>2015-10-22</created><authors><author><keyname>Berejuck</keyname><forenames>Marcelo Daniel</forenames></author></authors><title>Network-on-Chip with load balancing based on interleave of flits
  technique</title><categories>cs.AR</categories><comments>10 pages. arXiv admin note: substantial text overlap with
  arXiv:1411.3492</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the evaluation of a Network-on-Chip (NoC) that offers
load balancing for Systems-on-Chip (SoCs) dedicated for multimedia applications
that require high traffic of variable bitrate communication. The NoC is based
on a technique that allows the interleaving of flits from diferente flows in
the same communication channel, and keep the load balancing without a
centralized control in the network. For this purpose, all flits in the network
received extra bits, such that every flit carries routing information. The
routers use this extra information to perform arbitration and schedule the
flits to the corresponding output ports. Analytic comparisons and experimental
data show that the approach adopted in the network keeps average latency lower
for variable bitrate flows than a network based on resource reservation when
both networks are working over 80% of offered load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06799</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06799</id><created>2015-10-22</created><updated>2015-11-27</updated><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Robust Preamble Design for Synchronization, Signaling Transmission and
  Channel Estimation</title><categories>cs.IT math.IT</categories><comments>8 pages 11 figures.
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7001610&amp;tag=1</comments><journal-ref>IEEE Transactions on Broadcasting, vol. 61, no. 1, pp. 98-104,
  Mar. 2015</journal-ref><doi>10.1109/TBC.2014.2376134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The European second generation digital video broadcasting standard (DVB-T2)
introduces a P1 symbol. This P1 symbol facilitates the coarse synchronization
and carries 7-bit transmission parameter signaling (TPS), including the fast
Fourier transform size, single-input/single-output and
multiple-input/single-output transmission modes, etc. However, this P1 symbol
suffers from obvious performance loss over fading channels. In this paper, an
improved preamble scheme is proposed, where a pair of optimal m sequences are
inserted into the frequency domain. One sequence is used for carrier frequency
offset (CFO) estimation, and the other carries TPS to inform the receiver about
the transmission configuration parameters. Compared with the conventional
preamble scheme, the proposed preamble improves CFO estimation performance and
the signaling capacity. Meanwhile, without additional overhead, the proposed
scheme exploits more active pilots than the conventional schemes. In this way,
it can facilitate the channel estimation, improve the frame synchronization
accuracy as well as enhance its robustness to frequency selective fading
channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06800</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06800</id><created>2015-10-22</created><updated>2015-11-27</updated><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author></authors><title>Priori-Information Aided Iterative Hard Threshold: A Low-Complexity
  High-Accuracy Compressive Sensing Based Channel Estimation for TDS-OFDM</title><categories>cs.IT math.IT</categories><comments>9 pages 8 figures</comments><report-no>http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6856215&amp;tag=1</report-no><journal-ref>IEEE Transactions on Wireless Communications, vol. 14, no. 1, pp.
  242-251, Jan. 2015</journal-ref><doi>10.1109/TWC.2014.2339330</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a low-complexity channel estimation (CE) scheme based on
compressive sensing (CS) for time-domain synchronous (TDS) orthogonal
frequency-division multiplexing (OFDM) to overcome the performance loss under
doubly selective fading channels. Specifically, an overlap-add method of the
time-domain training sequence is first proposed to obtain the coarse estimates
of the channel length, path delays and path gains of the wireless channel, by
exploiting the channel's temporal correlation to improve the robustness of the
coarse CE under the severe fading channel with long delay spread. We then
propose the priori-information aided (PA) iterative hard threshold (IHT)
algorithm, which utilizes the priori information of the acquired coarse
estimate for the wireless channel and therefore is capable of obtaining an
accurate channel estimate of the doubly selective fading channel. Compared with
the classical IHT algorithm whose convergence requires the $l_2$ norm of the
measurement matrix being less than 1, the proposed PA-IHT algorithm exploits
the priori information acquired to remove such a limitation as well as to
reduce the number of required iterations. Compared with the existing CS based
CE method for TDS-OFDM, the proposed PA-IHT algorithm significantly reduces the
computational complexity of CE as well as enhances the CE accuracy. Simulation
results demonstrate that, without sacrificing spectral efficiency and changing
the current TDS-OFDM signal structure, the proposed scheme performs better than
the existing CE schemes for TDS-OFDM in various scenarios, especially under
severely doubly selective fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06802</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06802</id><created>2015-10-22</created><authors><author><keyname>Leahey</keyname><forenames>Erin</forenames></author><author><keyname>Beckman</keyname><forenames>Christine</forenames></author><author><keyname>Stanko</keyname><forenames>Taryn</forenames></author></authors><title>Prominent but Less Productive: The Impact of Interdisciplinarity on
  Scientists' Research</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-disciplinary research (IDR) is being promoted by federal agencies and
universities nationwide because it presumably spurs transformative, innovative
science. In this paper we bring empirical data to assess whether IDR is indeed
beneficial, and whether costs accompany potential benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06804</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06804</id><created>2015-10-22</created><authors><author><keyname>Maamari</keyname><forenames>Diana</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>Multi-user Cognitive Interference Channels: A Survey and New Capacity
  Results</title><categories>cs.IT math.IT</categories><comments>7 figures, 16 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a survey of the state-of-the-art information theoretic
analysis for overlay multi-user (more than two pairs) cognitive networks and
reports new capacity results. In an overlay scenario, cognitive / secondary
users share the same frequency band with licensed / primary users to
efficiently exploit the spectrum. They do so without degrading the performance
of the incumbent users, and may possibly even aid in transmitting their
messages as cognitive users are assumed to possess the message(s) of primary
user(s) and possibly other cognitive user(s). The survey begins with a short
overview of the two-user overlay cognitive interference channel. The evolution
from two-user to three-user overlay cognitive interference channels is
described next, followed by generalizations to multi-user (arbitrary number of
users) cognitive networks. The rest of the paper considers K-user cognitive
interference channels with different message knowledge structures at the
transmitters. Novel capacity inner and outer bounds are proposed. Channel
conditions under which the bounds meet, thus characterizing the information
theoretic capacity of the channel, for both Linear Deterministic and Gaussian
channel models, are derived. The results show that for certain channel
conditions distributed cognition, or having a cumulative message knowledge
structure at the nodes, may not be worth the overhead as (approximately) the
same capacity can be achieved by having only one global cognitive user whose
role is to manage all the interference in the network. The paper concludes with
future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06807</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06807</id><created>2015-10-22</created><authors><author><keyname>Monroe</keyname><forenames>Will</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author></authors><title>Learning in the Rational Speech Acts Model</title><categories>cs.CL</categories><comments>12 pages, 3 figures, 1 table. Preprint for Amsterdam Colloquium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rational Speech Acts (RSA) model treats language use as a recursive
process in which probabilistic speaker and listener agents reason about each
other's intentions to enrich the literal semantics of their language along
broadly Gricean lines. RSA has been shown to capture many kinds of
conversational implicature, but it has been criticized as an unrealistic model
of speakers, and it has so far required the manual specification of a semantic
lexicon, preventing its use in natural language processing applications that
learn lexical knowledge from data. We address these concerns by showing how to
define and optimize a trained statistical classifier that uses the intermediate
agents of RSA as hidden layers of representation forming a non-linear
activation function. This treatment opens up new application domains and new
possibilities for learning effectively from data. We validate the model on a
referential expression generation task, showing that the best performance is
achieved by incorporating features approximating well-established insights
about natural language generation into RSA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06812</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06812</id><created>2015-10-22</created><updated>2016-02-18</updated><authors><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>Game-theoretic Modeling of Players' Ambiguities on External Factors</title><categories>q-fin.EC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a game-theoretic framework that incorporates both incomplete
information and general ambiguity attitudes on factors external to all players.
Our starting point is players' preferences on payoff-distribution vectors,
essentially mappings from states of the world to be faced by players with
particular types to distributions of their payoffs. There are two ways to
define equilibria for this preference game. When the preferences possess ever
more features, we can gradually add ever more structures to the game. These
include real-valued utility-like functions over the aforementioned vectors,
prior sets over states of the world, and eventually the traditional
expected-utility framework. Under mild conditions, we establish equilibrium
existence results and uncover relations between the two versions of equilibria.
Particular attention is paid to what we shall call the enterprising game, in
which players bet optimistically on the favorable resolution of ambiguities.
The two solution concepts are unified at this game's pure equilibria, whose
existence is guaranteed when strategic complementarities are present. The
current framework can be applied to used-car sales involving retaliatory
buyers, auctions involving ambiguity on competitors' assessments of item
worths, and competitive pricing involving uncertain demand curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06826</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06826</id><created>2015-10-23</created><authors><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Cao</keyname><forenames>Yun</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Tellambura</keyname><forenames>Chintha</forenames></author></authors><title>Full-Duplex Radio for Uplink/Downlink Wireless Access with Spatially
  Random Nodes</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A full-duplex (FD) multiple antenna access point (AP) communicating with
single antenna half-duplex (HD) spatially random users to support simultaneous
uplink (UL)/downlink (DL) transmissions is investigated. Since FD nodes are
inherently constrained by the loopback interference (LI), we study precoding
schemes for the AP based on maximum ratio combining (MRC)/maximal ratio
transmission (MRT), zero-forcing and the optimal scheme for UL and DL sum rate
maximization using tools from stochastic geometry. In order to shed insights
into the system's performance, simple expressions for single antenna/perfect LI
cancellation/negligible internode interference cases are also presented. We
show that FD precoding at AP improves the UL/DL sum rate and hence a doubling
of the performance of the HD mode is achievable. In particular, our results
show that these impressive performance gains remain substantially intact even
if the LI cancellation is imperfect. Furthermore, relative performance gap
between FD and HD modes increases as the number of transmit/receive antennas
becomes large, while with the MRC/MRT scheme, increasing the receive antenna
number at FD AP, is more beneficial in terms of sum rate than increasing the
transmit antenna number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06827</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06827</id><created>2015-10-23</created><authors><author><keyname>Kong</keyname><forenames>Chuili</forenames></author><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios K.</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Sum-Rate and Power Scaling of Massive MIMO Systems with Channel Aging</title><categories>cs.IT math.IT</categories><comments>Accepted to appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the achievable sum-rate of massive multiple-input
multiple-output (MIMO) systems in the presence of channel aging. For the
uplink, by assuming that the base station (BS) deploys maximum ratio combining
(MRC) or zero-forcing (ZF) receivers, we present tight closed-form lower bounds
on the achievable sum-rate for both receivers with aged channel state
information (CSI). In addition, the benefit of implementing channel prediction
methods on the sum-rate is examined, and closed-form sum rate lower bounds are
derived. Moreover, the impact of channel aging and channel prediction on the
power scaling law is characterized. Extension to the downlink scenario and
multi-cell scenario are also considered. It is found that, for a system
with/without channel prediction, the transmit power of each user can be scaled
down at most by $1/\sqrt{M}$ (where $M$ is the number of BS antennas), which
indicates that aged CSI does not degrade the power scaling law, and channel
prediction does not enhance the power scaling law; instead, these phenomena
affect the achievable sum-rate by degrading or enhancing the effective signal
to interference and noise ratio, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06828</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06828</id><created>2015-10-23</created><authors><author><keyname>Pradhan</keyname><forenames>Asit Kumar</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author><author><keyname>Subramanian</keyname><forenames>Arunkumar</forenames></author></authors><title>Construction of Near-Capacity Protograph LDPC Code Sequences with
  Block-Error Thresholds</title><categories>cs.IT math.IT</categories><comments>to appear in the IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Density evolution for protograph Low-Density Parity-Check (LDPC) codes is
considered, and it is shown that the message-error rate falls
double-exponentially with iterations whenever the degree-2 subgraph of the
protograph is cycle-free and noise level is below threshold. Conditions for
stability of protograph density evolution are established and related to the
structure of the protograph. Using large-girth graphs, sequences of protograph
LDPC codes with block-error threshold equal to bit-error threshold and
block-error rate falling near-exponentially with blocklength are constructed
deterministically. Small-sized protographs are optimized to obtain thresholds
near capacity for binary erasure and binary-input Gaussian channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06837</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06837</id><created>2015-10-23</created><authors><author><keyname>Niyato</keyname><forenames>D.</forenames></author><author><keyname>Lu</keyname><forenames>X.</forenames></author><author><keyname>Wang</keyname><forenames>P.</forenames></author><author><keyname>Kim</keyname><forenames>D. I.</forenames></author><author><keyname>Han</keyname><forenames>Z.</forenames></author></authors><title>Economics of Internet of Things (IoT): An Information Market Approach</title><categories>cs.NI</categories><comments>To appear in IEEE Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of things (IoT) has been proposed to be a new paradigm of connecting
devices and providing services to various applications, e.g., transportation,
energy, smart city, and healthcare. In this paper, we focus on an important
issue, i.e., economics of IoT, that can have a great impact to the success of
IoT applications. In particular, we adopt and present the information economics
approach with its applications in IoT. We first review existing economic models
developed for IoT services. Then, we outline two important topics of
information economics which are pertinent to IoT, i.e., the value of
information and information good pricing. Finally, we propose a game theoretic
model to study the price competition of IoT sensing services. Some outlooks on
future research directions of applying information economics to IoT are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06841</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06841</id><created>2015-10-23</created><authors><author><keyname>Lieberoth</keyname><forenames>Andreas</forenames></author><author><keyname>Pedersen</keyname><forenames>Mads Kock</forenames></author><author><keyname>Sherson</keyname><forenames>Jacob</forenames></author></authors><title>Play or science?: a study of learning and framing in crowdscience games</title><categories>physics.ed-ph cs.CY quant-ph</categories><comments>26 pages, 3 figures</comments><journal-ref>Well Played 4(2), 30 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdscience games may hold unique potentials as learning opportunities
compared to games made for fun or education. They are part of an actual science
problem solving process: By playing, players help scientists, and thereby
interact with real continuous research processes. This mixes the two worlds of
play and science in new ways. During usability testing we discovered that users
of the crowdscience game Quantum Dreams tended to answer questions in game
terms, even when directed explicitly to give science explanations.We then
examined these competing frames of understanding through a mixed correlational
and grounded theory analysis. This essay presents the core ideas of
crowdscience games as learning opportunities, and reports how a group of
players used &quot;game&quot;, &quot;science&quot; and &quot;conceptual&quot; frames to interpret their
experience. Our results suggest that oscillating between the frames instead of
sticking to just one led to the largest number of correct science
interpretations, as players could participate legitimately and autonomously at
multiple levels of understanding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06855</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06855</id><created>2015-10-23</created><authors><author><keyname>Sossan</keyname><forenames>Fabrizio</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Venkatachalam</forenames></author><author><keyname>Costanzo</keyname><forenames>Giuseppe Tommaso</forenames></author><author><keyname>Marinelli</keyname><forenames>Mattia</forenames></author><author><keyname>Douglass</keyname><forenames>Philip J.</forenames></author><author><keyname>Bindner</keyname><forenames>Henrik</forenames></author></authors><title>Grey-box Modelling of a Household Refrigeration Unit Using Time Series
  Data in Application to Demand Side Management</title><categories>cs.SY</categories><comments>Submitted to Sustainable Energy Grids and Networks (SEGAN). Accepted
  for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the application of stochastic grey-box modeling to
identify electrical power consumption-to-temperature models of a domestic
freezer using experimental measurements. The models are formulated using
stochastic differential equations (SDEs), estimated by maximum likelihood
estimation (MLE), validated through the model residuals analysis and
cross-validated to detect model over-fitting. A nonlinear model based on the
reversed Carnot cycle is also presented and included in the modeling
performance analysis. As an application of the models, we apply model
predictive control (MPC) to shift the electricity consumption of a freezer in
demand response experiments, thereby addressing the model selection problem
also from the application point of view and showing in an experimental context
the ability of MPC to exploit the freezer as a demand side resource (DSR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06879</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06879</id><created>2015-10-23</created><authors><author><keyname>Lange</keyname><forenames>Julien</forenames></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames></author></authors><title>Characteristic Formulae for Session Types (extended version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subtyping is a crucial ingredient of session type theory and its
applications, notably to programming language implementations. In this paper,
we study effective ways to check whether a session type is a subtype of another
by applying a characteristic formulae approach to the problem. Our core
contribution is an algorithm to generate a modal mu-calculus formula that
characterises all the supertypes (or subtypes) of a given type. Subtyping
checks can then be off-loaded to model checkers, thus incidentally yielding an
efficient algorithm to check safety of session types, soundly and completely.
We have implemented our theory and compared its cost with other classical
subtyping algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06882</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06882</id><created>2015-10-23</created><authors><author><keyname>Imbs</keyname><forenames>Damien</forenames></author><author><keyname>Raynal</keyname><forenames>Michel</forenames></author></authors><title>Simple and Efficient Reliable Broadcast in the Presence of Byzantine
  Processes</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a surprisingly simple and efficient reliable broadcast
algorithm for asynchronous message-passing systems made up of $n$ processes,
among which up to $t&lt;n/3$ may behave arbitrarily (Byzantine processes). This
algorithm requires two communication steps and $n^2$ messages. (The best
algorithm known so far requires three communication steps and $2n^2$ messages.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06883</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06883</id><created>2015-10-23</created><authors><author><keyname>Apollonio</keyname><forenames>Nicola</forenames></author><author><keyname>Franciosa</keyname><forenames>Paolo Giulio</forenames></author></authors><title>On Computing the Galois Lattice of Bipartite Distance Hereditary Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of Bipartite Distance Hereditary (BDH) graphs is the intersection
between bipartite domino-free and chordal bipartite graphs.\ Graphs in both the
latter classes have linearly many maximal bicliques, implying the existence of
polynomial-time algorithms for computing the associated Galois lattice.\ Such a
lattice can indeed be built in $O(m\times n)$ worst case-time for a domino-free
graph with $m$ edges and $n$ vertices.\ In this paper we give a sharp estimate
on the number of the maximal bicliques of BDH graphs and exploit such result to
give an $O(m)$ worst case time algorithm for computing the Galois lattice of
BDH graphs. By relying on the fact that neighborhoods of vertices of BDH graphs
can be realized as directed paths in a arborescence, we give an $O(n)$
worst-case space and time encoding of both the input graph and its Galois
lattice, provided that the reverse of a Bandelt and Mulder building sequence is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06895</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06895</id><created>2015-10-23</created><authors><author><keyname>Lu</keyname><forenames>Canyi</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Nonconvex Nonsmooth Low-Rank Minimization via Iteratively Reweighted
  Nuclear Norm</title><categories>cs.LG cs.CV cs.NA</categories><doi>10.1109/TIP.2015.2511584</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nuclear norm is widely used as a convex surrogate of the rank function in
compressive sensing for low rank matrix recovery with its applications in image
recovery and signal processing. However, solving the nuclear norm based relaxed
convex problem usually leads to a suboptimal solution of the original rank
minimization problem. In this paper, we propose to perform a family of
nonconvex surrogates of $L_0$-norm on the singular values of a matrix to
approximate the rank function. This leads to a nonconvex nonsmooth minimization
problem. Then we propose to solve the problem by Iteratively Reweighted Nuclear
Norm (IRNN) algorithm. IRNN iteratively solves a Weighted Singular Value
Thresholding (WSVT) problem, which has a closed form solution due to the
special properties of the nonconvex surrogate functions. We also extend IRNN to
solve the nonconvex problem with two or more blocks of variables. In theory, we
prove that IRNN decreases the objective function value monotonically, and any
limit point is a stationary point. Extensive experiments on both synthesized
data and real images demonstrate that IRNN enhances the low-rank matrix
recovery compared with state-of-the-art convex algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06915</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06915</id><created>2015-10-23</created><authors><author><keyname>Sharma</keyname><forenames>Kanishka</forenames></author><author><keyname>Peter</keyname><forenames>Loic</forenames></author><author><keyname>Rupprecht</keyname><forenames>Christian</forenames></author><author><keyname>Caroli</keyname><forenames>Anna</forenames></author><author><keyname>Wang</keyname><forenames>Lichao</forenames></author><author><keyname>Remuzzi</keyname><forenames>Andrea</forenames></author><author><keyname>Baust</keyname><forenames>Maximilian</forenames></author><author><keyname>Navab</keyname><forenames>Nassir</forenames></author></authors><title>Semi-Automatic Segmentation of Autosomal Dominant Polycystic Kidneys
  using Random Forests</title><categories>cs.CV</categories><comments>5 pages, 3 Figures, parallel submission to International Symposium on
  Biomedical Imaging 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for 3D segmentation of kidneys from patients
with autosomal dominant polycystic kidney disease (ADPKD) and severe renal
insufficiency, using computed tomography (CT) data. ADPKD severely alters the
shape of the kidneys due to non-uniform formation of cysts. As a consequence,
fully automatic segmentation of such kidneys is very challenging. We present a
segmentation method with minimal user interaction based on a random forest
classifier. One of the major novelties of the proposed approach is the usage of
geodesic distance volumes as additional source of information. These volumes
contain the intensity weighted distance to a manual outline of the respective
kidney in only one slice (for each kidney) of the CT volume. We evaluate our
method qualitatively and quantitatively on 55 CT acquisitions using ground
truth annotations from clinical experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06916</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06916</id><created>2015-10-23</created><authors><author><keyname>Chi</keyname><forenames>Yuze</forenames></author><author><keyname>Dai</keyname><forenames>Guohao</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Sun</keyname><forenames>Guangyu</forenames></author><author><keyname>Li</keyname><forenames>Guoliang</forenames></author><author><keyname>Yang</keyname><forenames>Huazhong</forenames></author></authors><title>NXgraph: An Efficient Graph Processing System on a Single Machine</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies show that graph processing systems on a single machine can
achieve competitive performance compared with cluster-based graph processing
systems. In this paper, we present NXgraph, an efficient graph processing
system on a single machine. With the abstraction of vertex intervals and edge
sub-shards, we propose the Destination-Sorted Sub-Shard (DSSS) structure to
store a graph. By dividing vertices and edges into intervals and sub-shards,
NXgraph ensures graph data access locality and enables fine-grained scheduling.
By sorting edges within each sub-shard according to their destination vertices,
NXgraph reduces write conflicts among different threads and achieves a high
degree of parallelism. Then, three updating strategies, i.e., Single-Phase
Update (SPU), Double-Phase Update (DPU), and Mixed-Phase Update (MPU), are
proposed in this paper. NXgraph can adaptively choose the fastest strategy for
different graph problems according to the graph size and the available memory
resources to fully utilize the memory space and reduce the amount of data
transfer. All these three strategies exploit streamlined disk access pattern.
Extensive experiments on three real-world graphs and five synthetic graphs show
that NXgraph can outperform GraphChi, TurboGraph, VENUS, and GridGraph in
various situations. Moreover, NXgraph, running on a single commodity PC, can
finish an iteration of PageRank on the Twitter graph with 1.5 billion edges in
2.05 seconds; while PowerGraph, a distributed graph processing system, needs
3.6s to finish the same task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06920</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06920</id><created>2015-10-23</created><authors><author><keyname>Lauer</keyname><forenames>Fabien</forenames><affiliation>ABC</affiliation></author></authors><title>On the complexity of switching linear regression</title><categories>stat.ML cs.CC cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical note extends recent results on the computational complexity of
globally minimizing the error of piecewise-affine models to the related problem
of minimizing the error of switching linear regression models. In particular,
we show that, on the one hand the problem is NP-hard, but on the other hand, it
admits a polynomial-time algorithm with respect to the number of data for any
fixed data dimension and number of modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06925</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06925</id><created>2015-10-23</created><updated>2015-12-03</updated><authors><author><keyname>Robinson</keyname><forenames>Leigh</forenames></author><author><keyname>Graham</keyname><forenames>Benjamin</forenames></author></authors><title>Confusing Deep Convolution Networks by Relabelling</title><categories>cs.CV cs.NE</categories><comments>Submitted to BMVC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks have become the gold standard for image
recognition tasks, demonstrating many current state-of-the-art results and even
achieving near-human level performance on some tasks. Despite this fact it has
been shown that their strong generalisation qualities can be fooled to
misclassify previously correctly classified natural images and give erroneous
high confidence classifications to nonsense synthetic images. In this paper we
extend that work, by presenting a straightforward way to perturb an image in
such a way as to cause it to acquire any other label from within the dataset
while leaving this perturbed image visually indistinguishable from the
original.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06937</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06937</id><created>2015-10-23</created><authors><author><keyname>Markonis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Schaer</keyname><forenames>Roger</forenames></author><author><keyname>Eggel</keyname><forenames>Ivan</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Henning</forenames></author><author><keyname>Depeursinge</keyname><forenames>Adrien</forenames></author></authors><title>Using MapReduce for Large-scale Medical Image Analysis</title><categories>cs.DC</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of the amount of medical image data produced on a daily basis in
modern hospitals forces the adaptation of traditional medical image analysis
and indexing approaches towards scalable solutions. The number of images and
their dimensionality increased dramatically during the past 20 years. We
propose solutions for large-scale medical image analysis based on parallel
computing and algorithm optimization. The MapReduce framework is used to speed
up and make possible three large-scale medical image processing use-cases: (i)
parameter optimization for lung texture segmentation using support vector
machines, (ii) content-based medical image indexing, and (iii)
three-dimensional directional wavelet analysis for solid texture
classification. A cluster of heterogeneous computing nodes was set up in our
institution using Hadoop allowing for a maximum of 42 concurrent map tasks. The
majority of the machines used are desktop computers that are also used for
regular office work. The cluster showed to be minimally invasive and stable.
The runtimes of each of the three use-case have been significantly reduced when
compared to a sequential execution. Hadoop provides an easy-to-employ framework
for data analysis tasks that scales well for many tasks but requires
optimization for specific tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06939</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06939</id><created>2015-10-23</created><authors><author><keyname>Jain</keyname><forenames>Mihir</forenames></author><author><keyname>van Gemert</keyname><forenames>Jan C.</forenames></author><author><keyname>Mensink</keyname><forenames>Thomas</forenames></author><author><keyname>Snoek</keyname><forenames>Cees G. M.</forenames></author></authors><title>Objects2action: Classifying and localizing actions without any video
  example</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to recognize actions in video without the need for
examples. Different from traditional zero-shot approaches we do not demand the
design and specification of attribute classifiers and class-to-attribute
mappings to allow for transfer from seen classes to unseen classes. Our key
contribution is objects2action, a semantic word embedding that is spanned by a
skip-gram model of thousands of object categories. Action labels are assigned
to an object encoding of unseen video based on a convex combination of action
and object affinities. Our semantic embedding has three main characteristics to
accommodate for the specifics of actions. First, we propose a mechanism to
exploit multiple-word descriptions of actions and objects. Second, we
incorporate the automated selection of the most responsive objects per action.
And finally, we demonstrate how to extend our zero-shot approach to the
spatio-temporal localization of actions in video. Experiments on four action
datasets demonstrate the potential of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06964</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06964</id><created>2015-10-23</created><updated>2015-12-03</updated><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Feghali</keyname><forenames>Carl</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author></authors><title>On a conjecture of Mohar concerning Kempe equivalence of regular graphs</title><categories>cs.DM math.CO</categories><comments>added section on Wang-Swendsen-Kotecky algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph with a vertex colouring $\alpha$. Let $a$ and $b$ be two
colours. Then a connected component of the subgraph induced by those vertices
coloured either $a$ or $b$ is known as a Kempe chain. A colouring of $G$
obtained from $\alpha$ by swapping the colours on the vertices of a Kempe chain
is said to have been obtained by a Kempe change. Two colourings of $G$ are
Kempe equivalent if one can be obtained from the other by a sequence of Kempe
changes. A conjecture of Mohar (2007) asserts that, for $k \geq 3$, all
$k$-colourings of a $k$-regular graph that is not complete are Kempe
equivalent. It was later shown that all $3$-colourings of a cubic graph that is
neither$K_4$ nor the triangular prism are Kempe equivalent. In this paper, we
prove that the conjecture holds for each $k\geq 4$. We also report the
implications of this result on the ergodicity of the Wang-Swendsen-Koteck\'{y}
algorithm for the antiferromagnetic Potts model at zero-temperature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06967</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06967</id><created>2015-10-23</created><authors><author><keyname>Anand</keyname><forenames>Anshu S</forenames></author><author><keyname>Shyamasundar</keyname><forenames>R K</forenames></author><author><keyname>Peri</keyname><forenames>Sathya</forenames></author></authors><title>Opacity Proof for CaPR+ Algorithm</title><categories>cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1307.8256</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an enhanced Automatic Check- pointing and Partial
Rollback algorithm(CaP R + ) to realize Software Transactional Memory(STM) that
is based on con- tinuous conflict detection, lazy versioning with automatic
checkpointing, and partial rollback. Further, we provide a proof of correctness
of CaP R+ algorithm, in particular, Opacity, a STM correctness criterion, that
precisely captures the intuitive correctness guarantees required of
transactional memories. The algorithm provides a natural way to realize a
hybrid system of pure aborts and partial rollbacks. We have also implemented
the algorithm, and shown its effectiveness with reference to the Red-black tree
micro-benchmark and STAMP benchmarks. The results obtained demonstrate the
effectiveness of the Partial Rollback mechanism over pure abort mechanisms,
particularly in applications consisting of large transaction lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06969</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06969</id><created>2015-10-23</created><authors><author><keyname>Engelmann</keyname><forenames>Anna</forenames></author><author><keyname>Jukan</keyname><forenames>Admela</forenames></author></authors><title>Balancing the Demands of Reliability and Security with Linear Network
  Coding in Optical Networks</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, physical layer security in the optical layer has gained significant
traction. Security treats in optical networks generally impact the reliability
of optical transmission. Linear Network Coding (LNC) can protect from both the
security treats in form of eavesdropping and faulty transmission due to
jamming. LNC can mix original data to become incomprehensible for an attacker
and also extend original data by coding redundancy, thus protecting a data from
errors injected via jamming attacks. In this paper, we study the effectiveness
of LNC to balance reliable transmission and security in optical networks. To
this end, we combine the coding process with data flow parallelization of the
source and propose and compare optimal and randomized path selection methods
for parallel transmission. The study shows that a combination of data
parallelization, LNC and randomization of path selection increases security and
reliability of the transmission. We analyze the so-called catastrophic security
treat of the network and show that in case of conventional transmission scheme
and in absence of LNC, an attacker could eavesdrop or disrupt a whole secret
data by accessing only one edge in a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.06988</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.06988</id><created>2015-10-23</created><authors><author><keyname>Joblin</keyname><forenames>Mitchell</forenames></author><author><keyname>Apel</keyname><forenames>Sven</forenames></author><author><keyname>Mauerer</keyname><forenames>Wolfgang</forenames></author></authors><title>Evolutionary Trends of Developer Coordination: A Network Approach</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software evolution is a fundamental process that transcends the realm of
technical artifacts and permeates the entire organizational structure of a
software project. By means of a longitudinal empirical study of 18 large
open-source projects, we examine and discuss the evolutionary principles that
govern the coordination of developers. By applying a network-analytic approach,
we found that the implicit and self-organizing structure of developer
coordination is ubiquitously described by non-random organizational principles
that defy conventional software-engineering wisdom. In particular, we found
that: (a) developers form scale-free networks, in which the majority of
coordination is primarily managed by an extremely small number of developers,
(b) developers tend to become more coordinated over time, limited by an upper
bound, and (c) initially developers are hierarchically arranged, but over time,
form a unique hybrid structure, in which core developers are hierarchically
arranged and peripheral developers are not. Our results suggest that the
organizational structure of large projects is constrained to evolve towards a
state that balances the costs and benefits of developer coordination, and the
mechanisms used to achieve this state depend on the project's scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07001</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07001</id><created>2015-10-23</created><authors><author><keyname>Ouyang</keyname><forenames>Yi</forenames></author><author><keyname>Tavafoghi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Dynamic Games with Asymmetric Information: Common Information Based
  Perfect Bayesian Equilibria and Sequential Decomposition</title><categories>cs.GT cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and analyze a general class of stochastic dynamic games with
asymmetric information arising in dynamic systems. In such games, multiple
strategic agents control the system dynamics and have different information
about the system over time. Because of the presence of asymmetric information,
each agent needs to form beliefs about other agents' private information.
Therefore, the specification of the agents' beliefs along with their strategies
is necessary to study the dynamic game. We use Perfect Bayesian equilibrium
(PBE) as our solution concept. A PBE consists of a pair of strategy profile and
belief system. In a PBE, every agent's strategy should be a best response under
the belief system, and the belief system depends on agents' strategy profile
when there is signaling among agents. Therefore, the circular dependence
between strategy profile and belief system makes it difficult to compute PBE.
Using the common information among agents, we introduce a subclass of PBE
called common information based perfect Bayesian equilibria (CIB-PBE), and
provide a sequential decomposition of the dynamic game. Such decomposition
leads to a backward induction algorithm to compute CIB-PBE. We illustrate the
sequential decomposition with an example of a multiple access broadcast game.
We prove the existence of CIB-PBE for a subclass of dynamic games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07020</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07020</id><created>2015-10-22</created><authors><author><keyname>Torres</keyname><forenames>Yoangel</forenames></author><author><keyname>Premaratne</keyname><forenames>Kamal</forenames></author><author><keyname>Amelung</keyname><forenames>Falk</forenames></author><author><keyname>Wdowinski</keyname><forenames>Shimon</forenames></author></authors><title>An Efficient Polyphase Filter Based Resampling Method for Unifying the
  PRFs in SAR Data</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As current airborne and spaceborne synthetic aperture radar (SAR) systems aim
to produce higher resolution and wider area products, their associated
complexities call for handling stricter requirements. Variable and higher pulse
repetition frequencies (PRFs) are increasingly being used to achieve these
demanding requirements in modern radar systems. This paper presents a
resampling scheme capable of unifying and downsampling variable PRFs within a
single look complex (SLC) SAR acquisition and across a repeat pass sequence of
acquisitions down to an effective lower PRF through the use of polyphase
filters. To evaluate the performance of this resampling scheme, we use airborne
SAR raw data with variable PRFs. The data were processed with and without the
proposed resampling method as part of the flow of the imaging algorithm.
Significant improvement in the point spread function (PSF) measurement and the
visible image quality after rate conversion and normalization justify the
theoretical basis of the proposed method and the benefits it can provide in
application scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07025</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07025</id><created>2015-10-23</created><updated>2016-02-04</updated><authors><author><keyname>Liang</keyname><forenames>Dawen</forenames></author><author><keyname>Charlin</keyname><forenames>Laurent</forenames></author><author><keyname>McInerney</keyname><forenames>James</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Modeling User Exposure in Recommendation</title><categories>stat.ML cs.IR cs.LG</categories><comments>11 pages, 4 figures. WWW'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering analyzes user preferences for items (e.g., books,
movies, restaurants, academic papers) by exploiting the similarity patterns
across users. In implicit feedback settings, all the items, including the ones
that a user did not consume, are taken into consideration. But this assumption
does not accord with the common sense understanding that users have a limited
scope and awareness of items. For example, a user might not have heard of a
certain paper, or might live too far away from a restaurant to experience it.
In the language of causal analysis, the assignment mechanism (i.e., the items
that a user is exposed to) is a latent variable that may change for various
user/item combinations. In this paper, we propose a new probabilistic approach
that directly incorporates user exposure to items into collaborative filtering.
The exposure is modeled as a latent variable and the model infers its value
from data. In doing so, we recover one of the most successful state-of-the-art
approaches as a special case of our model, and provide a plug-in method for
conditioning exposure on various forms of exposure covariates (e.g., topics in
text, venue locations). We show that our scalable inference algorithm
outperforms existing benchmarks in four different domains both with and without
exposure covariates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07026</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07026</id><created>2015-10-23</created><authors><author><keyname>Ahn</keyname><forenames>Heejin</forenames></author><author><keyname>Del Vecchio</keyname><forenames>Domitilla</forenames></author></authors><title>Semi-autonomous Intersection Collision Avoidance through Job-shop
  Scheduling</title><categories>cs.SY</categories><comments>Submitted to Hybrid Systems: Computation and Control (HSCC) 2016</comments><acm-class>I.2.8; I.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design a supervisor to prevent vehicle collisions at
intersections. An intersection is modeled as an area containing multiple
conflict points where vehicle paths cross in the future. At every time step,
the supervisor determines whether there will be more than one vehicle in the
vicinity of a conflict point at the same time. If there is, then an impending
collision is detected, and the supervisor overrides the drivers to avoid
collision. A major challenge in the design of a supervisor as opposed to an
autonomous vehicle controller is to verify whether future collisions will occur
based on the current drivers choices. This verification problem is particularly
hard due to the large number of vehicles often involved in intersection
collision, to the multitude of conflict points, and to the vehicles dynamics.
In order to solve the verification problem, we translate the problem to a
job-shop scheduling problem that yields equivalent answers. The job-shop
scheduling problem can, in turn, be transformed into a mixed-integer linear
program when the vehicle dynamics are first-order dynamics, and can thus be
solved by using a commercial solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07035</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07035</id><created>2015-10-23</created><authors><author><keyname>Robinson</keyname><forenames>Joseph W</forenames></author><author><keyname>Li</keyname><forenames>Aaron Q</forenames></author></authors><title>Fast Latent Variable Models for Inference and Visualization on Mobile
  Devices</title><categories>cs.LG cs.CL cs.DC cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this project we outline Vedalia, a high performance distributed network
for performing inference on latent variable models in the context of Amazon
review visualization. We introduce a new model, RLDA, which extends Latent
Dirichlet Allocation (LDA) [Blei et al., 2003] for the review space by
incorporating auxiliary data available in online reviews to improve modeling
while simultaneously remaining compatible with pre-existing fast sampling
techniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high
performance. The network is designed such that computation is efficiently
offloaded to the client devices using the Chital system [Robinson &amp; Li, 2015],
improving response times and reducing server costs. The resulting system is
able to rapidly compute a large number of specialized latent variable models
while requiring minimal server resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07064</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07064</id><created>2015-10-23</created><authors><author><keyname>Sarkar</keyname><forenames>Somwrita</forenames></author><author><keyname>Chawla</keyname><forenames>Sanjay</forenames></author><author><keyname>Robinson</keyname><forenames>Peter A.</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Eigenvector dynamics under perturbation of modular networks</title><categories>physics.soc-ph cs.IR</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rotation dynamics of eigenvectors of modular network adjacency matrices under
random perturbations are presented. In the presence of $q$ communities, the
number of eigenvectors corresponding to the $q$ largest eigenvalues form a
&quot;community&quot; eigenspace and rotate together, but separately from that of the
&quot;bulk&quot; eigenspace spanned by all the other eigenvectors. Using this property,
the number of modules or clusters in a network can be estimated in an
algorithm-independent way. A general derivation for the theoretical
detectability limit for sparse modular networks with $q$ communities is
presented, beyond which modularity persists in the system but cannot be
detected, and estimation results are shown to hold right to this limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07090</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07090</id><created>2015-10-23</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Arora</keyname><forenames>Megha</forenames></author><author><keyname>Gallegos</keyname><forenames>Luciano</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>Garcia</keyname><forenames>David</forenames></author></authors><title>Social Ties and Emotions: Evidence from Social Media</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>submitted to WWW</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social connections, or ties, individuals create affect their life
outcomes, for example, by providing novel information that leads to new jobs or
career opportunities. A host of socioeconomic and cognitive factors are
believed to affect social interactions, but few of these factors have been
empirically validated. In this research work, we extracted a large corpus of
data from a popular social media platform that consists of geo-referenced
messages, or tweets, posted from a major US metropolitan area. We linked these
tweets to US Census data through their locations. This allowed us to measure
emotions expressed in tweets posted from a specific area, and also use that
area's socioeconomic and demographic characteristics in the analysis. We
extracted the structure of social interactions from the people mentioned in
tweets from that area. We find that at an aggregate level, areas where social
media users engage in stronger, less diverse online social interactions are
those where they express more negative emotions, like sadness and anger. With
respect to demographics, these areas have larger numbers of Hispanic residents,
lower mean household income, and lower education levels. Conversely, areas with
weaker, more diverse online interactions are associated with happier, more
positive feelings and also have better educated, younger and higher-earning
residents. Our work highlights the value of linking social media data to
traditional data sources, such as US Census, to drive novel analysis of online
behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07092</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07092</id><created>2015-10-23</created><authors><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Bailis</keyname><forenames>Peter</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M.</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>Asynchronous Complex Analytics in a Distributed Dataflow Architecture</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable distributed dataflow systems have recently experienced widespread
adoption, with commodity dataflow engines such as Hadoop and Spark, and even
commodity SQL engines routinely supporting increasingly sophisticated analytics
tasks (e.g., support vector machines, logistic regression, collaborative
filtering). However, these systems' synchronous (often Bulk Synchronous
Parallel) dataflow execution model is at odds with an increasingly important
trend in the machine learning community: the use of asynchrony via shared,
mutable state (i.e., data races) in convex programming tasks, which has---in a
single-node context---delivered noteworthy empirical performance gains and
inspired new research into asynchronous algorithms. In this work, we attempt to
bridge this gap by evaluating the use of lightweight, asynchronous state
transfer within a commodity dataflow engine. Specifically, we investigate the
use of asynchronous sideways information passing (ASIP) that presents
single-stage parallel iterators with a Volcano-like intra-operator iterator
that can be used for asynchronous information passing. We port two synchronous
convex programming algorithms, stochastic gradient descent and the alternating
direction method of multipliers (ADMM), to use ASIPs. We evaluate an
implementation of ASIPs within on Apache Spark that exhibits considerable
speedups as well as a rich set of performance trade-offs in the use of these
asynchronous algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07095</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07095</id><created>2015-10-23</created><authors><author><keyname>Georgiou</keyname><forenames>Kyriakos</forenames></author><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>On the Value and Limits of Multi-level Energy Consumption Static
  Analysis for Deeply Embedded Single and Multi-threaded Programs</title><categories>cs.PL</categories><comments>29 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is growing interest in lowering the energy consumption of computation.
Energy transparency is a concept that makes a program's energy consumption
visible from software to hardware through the different system layers. Such
transparency can enable energy optimizations at each layer and between layers,
and help both programmers and operating systems make energy aware decisions.
The common methodology of extracting the energy consumption of a program is
through direct measurement of the target hardware. This usually involves
specialized equipment and knowledge most programmers do not have. In this
paper, we examine how existing methods for static resource analysis and energy
modeling can be utilized to perform Energy Consumption Static Analysis (ECSA)
for deeply embedded programs. To investigate this, we have developed ECSA
techniques that work at the instruction set level and at a higher level, the
LLVM IR, through a novel mapping technique. We apply our ECSA to a
comprehensive set of mainly industrial benchmarks, including single-threaded
and also multi-threaded embedded programs from two commonly used concurrency
patterns, task farms and pipelines. We compare our ECSA results to hardware
measurements and predictions obtained based on simulation traces. We discuss a
number of application scenarios for which ECSA results can provide energy
transparency and conclude with a set of new research questions for future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07099</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07099</id><created>2015-10-23</created><authors><author><keyname>Yushi</keyname><forenames>Yao</forenames></author><author><keyname>Zheng</keyname><forenames>Huang</forenames></author></authors><title>Combine CRF and MMSEG to Boost Chinese Word Segmentation in Social Media</title><categories>cs.CL</categories><comments>5 pages, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a joint algorithm for the word segmentation on
Chinese social media. Previous work mainly focus on word segmentation for plain
Chinese text, in order to develop a Chinese social media processing tool, we
need to take the main features of social media into account, whose grammatical
structure is not rigorous, and the tendency of using colloquial and Internet
terms makes the existing Chinese-processing tools inefficient to obtain good
performance on social media.
  In our approach, we combine CRF and MMSEG algorithm and extend features of
traditional CRF algorithm to train the model for word segmentation, We use
Internet lexicon in order to improve the performance of our model on Chinese
social media. Our experimental result on Sina Weibo shows that our approach
outperforms the state-of-the-art model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07104</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07104</id><created>2015-10-24</created><authors><author><keyname>Fan</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Zhengkui</forenames></author><author><keyname>Chan</keyname><forenames>Chee-Yong</forenames></author><author><keyname>Tan</keyname><forenames>Kian-Lee</forenames></author></authors><title>Supporting Window Analytics over Large-scale Dynamic Graphs</title><categories>cs.DB</categories><comments>14 pages, 16 figures</comments><acm-class>H.2.4; H.2; E.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In relational DBMS, window functions have been widely used to facilitate data
analytics. Surprisingly, while similar concepts have been employed for graph
analytics, there has been no explicit notions of graph window analytic
functions. In this paper, we formally introduce window queries for graph
analytics. In such queries, for each vertex, the analysis is performed on a
window of vertices defined based on the graph structure. In particular, we
identify two instantiations, namely the k-hop window and the topological
window. We develop two novel indices, Dense Block index (DBIndex) and
Inheritance index (I-Index), to facilitate efficient processing of these two
types of windows respectively. Extensive experiments are conducted over both
real and synthetic datasets with hundreds of millions of vertices and edges.
Experimental results indicate that our proposed index-based query processing
solutions achieve four orders of magnitude of query performance gain than the
non-index algorithm and are superior over EAGR wrt scalability and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07106</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07106</id><created>2015-10-24</created><authors><author><keyname>Vasudevan</keyname><forenames>K.</forenames></author></authors><title>Design and Development of a Burst Acquisition System for Geosynchronous
  Satcom Channels</title><categories>cs.IT math.IT</categories><comments>29 pages, 14 figures, 7 tables</comments><doi>10.1007/s11760-010-0184-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The key contribution of this work is to develop transmitter and receiver
algorithms in discrete-time for turbo- coded offset QPSK signals. The proposed
synchronization and detection techniques perform effectively at an SNR per bit
close to 1.5 dB, in the presence of a frequency offset as large as 30 % of the
symbol-rate and a clock offset of 25 ppm (parts per million). Due to the use of
up-sampling and matched filtering and a feedforward approach, the acquisition
time for clock recovery is just equal to the length of the preamble. The
carrier recovery algorithm does not exhibit any phase ambiguity, alleviating
the need for differentially encoding the data at the transmitter. The proposed
techniques are well suited for discrete-time implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07112</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07112</id><created>2015-10-24</created><authors><author><keyname>Dutta</keyname><forenames>Abhishek</forenames></author></authors><title>Predicting Performance of a Face Recognition System Based on Image
  Quality</title><categories>cs.CV</categories><comments>PhD thesis publicly defended at the University of Twente
  (Netherlands) on April 24, 2015 at 12.45</comments><doi>10.3990/1.9789036538725</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this dissertation, we present a generative model to capture the relation
between facial image quality features (like pose, illumination direction, etc)
and face recognition performance. Such a model can be used to predict the
performance of a face recognition system. Since the model is based solely on
image quality features, performance predictions can be done even before the
actual recognition has taken place thereby facilitating many preemptive action.
A practical limitation of such a data driven generative model is the limited
nature of training data set. To address this limitation, we have developed a
Bayesian approach to model the distribution of recognition performance measure
based on the number of match and non-match scores in small regions of the image
quality space. Random samples drawn from these models provide the initial data
essential for training the generative model. Experiment results based on six
face recognition systems operating on three independent data sets show that the
proposed performance prediction model can accurately predict face recognition
performance using an accurate and unbiased Image Quality Assessor (IQA).
Furthermore, our results show that variability in the unaccounted quality space
-- the image quality features not considered by the IQA -- is the major factor
causing inaccuracies in predicted performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07118</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07118</id><created>2015-10-24</created><authors><author><keyname>Vogiatzian</keyname><forenames>Filippos</forenames></author></authors><title>Secure Identification in The Isolated Qubits Model</title><categories>quant-ph cs.CR cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1410.3918 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oblivious transfer is a powerful cryptographic primitive that is complete for
secure multi-party computation. In oblivious transfer protocols a user sends
one or more messages to a receiver, while the sender remains oblivious as to
which messages have been received. Protocols for oblivious transfer cannot
exist in a classical or fully-quantum world, but can be implemented by
restricting the users' power.
  The isolated qubits model is a cryptographic model in which users are
restricted to single-qubit operations and are not allowed to use entangling
operations. Furthermore, all parties are allowed to store qubits for a long
time before measuring them.
  In this model, a secure single-bit one-out-of-two randomised oblivious
transfer protocol was recently presented by Liu. Motivated by this result, we
construct a protocol for secure string one-out-of-two randomised oblivious
transfer by simplifying and generalising the existing proof.
  We then study for the first time interactive protocols for more complex
two-party functionalities in this model based on the security of our
construction. In order to guarantee the composability of our construction,
users are restricted to measurement at the end of each sub-protocol. It is then
possible to construct secure one-out-of-two and one-out-of-k oblivious transfer
protocols in the isolated qubits model.
  Moreover, we study secure password-based identification, where a user
identifies himself to another user by evaluating the equality function on their
inputs, or passwords. We use the oblivious transfer constructions mentioned
above as sub-protocols to construct a secure identification protocol.
  Finally, we prove that constructing a secure identification protocol
non-interactively is impossible, even using oblivious transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07119</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07119</id><created>2015-10-24</created><authors><author><keyname>Dutta</keyname><forenames>Abhishek</forenames></author><author><keyname>Veldhuis</keyname><forenames>Raymond</forenames></author><author><keyname>Spreeuwers</keyname><forenames>Luuk</forenames></author></authors><title>Predicting Face Recognition Performance Using Image Quality</title><categories>cs.CV</categories><comments>Submitted to TPAMI journal on Apr. 22, 2015. Decision of &quot;Revise and
  resubmit as new&quot; received on Sep. 10, 2015. At present, updating the paper to
  address the feedback and concerns of the two reviewers. The re-submitted
  paper will be uploaded as version 2 on arXiv</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a data driven model to predict the performance of a face
recognition system based on image quality features. We model the relationship
between image quality features (e.g. pose, illumination, etc.) and recognition
performance measures using a probability density function. To address the issue
of limited nature of practical training data inherent in most data driven
models, we have developed a Bayesian approach to model the distribution of
recognition performance measures in small regions of the quality space. Since
the model is based solely on image quality features, it can predict performance
even before the actual recognition has taken place. We evaluate the performance
predictive capabilities of the proposed model for six face recognition systems
(two commercial and four open source) operating on three independent data sets:
MultiPIE, FRGC and CAS-PEAL. Our results show that the proposed model can
accurately predict performance using an accurate and unbiased Image Quality
Assessor (IQA). Furthermore, our experiments highlight the impact of the
unaccounted quality space -- the image quality features not considered by IQA
-- in contributing to performance prediction errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07124</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07124</id><created>2015-10-24</created><authors><author><keyname>Egri</keyname><forenames>Laszlo</forenames></author></authors><title>Space complexity of list H-coloring revisited: the case of oriented
  trees</title><categories>cs.CC</categories><comments>30 pages, full version with algebra section included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digraphs H for which the list homomorphism problem with template H (LHOM(H))
is in logspace (L) was characterized by Egri et al. (SODA 2014): LHOM(H) is in
L if and only if H does not contain a circular N (assuming L is different from
NL). Undirected graphs for which LHOM(H) is in L can be characterized in terms
forbidden induced subgraphs, and also via a simple inductive construction (Egri
et al., STACS 2010). As a consequence, the logspace algorithm in the undirected
case is simple and easy to understand. No such forbidden subgraph or inductive
characterization, and no such simple and easy-to-understand algorithm is known
in the case of digraphs. In this paper, in the case of oriented trees, we
refine and strengthen the results of Egri et al. (SODA 2014): we give a
characterization of oriented trees T for which LHOM(T) is in L both in terms of
forbidden induced subgraphs, and also via a simple inductive construction.
Using this characterization, we obtain a simple and easy-to-analyze logspace
algorithm for LHOM(T). We also show how these oriented trees can be recognized
in time O(|V(T)|^3) (the straightforward implementation of the algorithm given
in SODA 2014 runs in time O(|V(H)|^8) for oriented trees). An algebraic
characterization of these trees is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07135</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07135</id><created>2015-10-24</created><authors><author><keyname>B&#x142;asiok</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Raymond</keyname><forenames>Jean-Florent</forenames></author><author><keyname>Trunck</keyname><forenames>Th&#xe9;ophile</forenames></author></authors><title>Induced minors and well-quasi-ordering</title><categories>math.CO cs.DM</categories><msc-class>05C, 06A07</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $H$ is an induced minor of a graph $G$ if it can be obtained from an
induced subgraph of $G$ by contracting edges. Otherwise, $G$ is said to be
$H$-induced minor-free. Robin Thomas showed that $K_4$-induced minor-free
graphs are well-quasi-ordered by induced minors [Graphs without $K_4$ and
well-quasi-ordering, Journal of Combinatorial Theory, Series B, 38(3):240 --
247, 1985].
  We provide a dichotomy theorem for $H$-induced minor-free graphs and show
that the class of $H$-induced minor-free graphs is well-quasi-ordered by the
induced minor relation if and only if $H$ is an induced minor of the gem (the
path on 4 vertices plus a dominating vertex) or of the graph obtained by adding
a vertex of degree 2 to the complete graph on 4 vertices. To this end we proved
two decomposition theorems which are of independent interest.
  Similar dichotomy results were previously given for subgraphs by Guoli Ding
in [Subgraphs and well-quasi-ordering, Journal of Graph Theory, 16(5):489--502,
1992] and for induced subgraphs by Peter Damaschke in [Induced subgraphs and
well-quasi-ordering, Journal of Graph Theory, 14(4):427--435, 1990].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07136</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07136</id><created>2015-10-24</created><authors><author><keyname>George</keyname><forenames>Marian</forenames></author></authors><title>Image Parsing with a Wide Range of Classes and Scene-Level Context</title><categories>cs.CV</categories><comments>Published at CVPR 2015, Computer Vision and Pattern Recognition
  (CVPR), 2015 IEEE Conference on</comments><doi>10.1109/CVPR.2015.7298985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a nonparametric scene parsing approach that improves the
overall accuracy, as well as the coverage of foreground classes in scene
images. We first improve the label likelihood estimates at superpixels by
merging likelihood scores from different probabilistic classifiers. This boosts
the classification performance and enriches the representation of
less-represented classes. Our second contribution consists of incorporating
semantic context in the parsing process through global label costs. Our method
does not rely on image retrieval sets but rather assigns a global likelihood
estimate to each label, which is plugged into the overall energy function. We
evaluate our system on two large-scale datasets, SIFTflow and LMSun. We achieve
state-of-the-art performance on the SIFTflow dataset and near-record results on
LMSun.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07146</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07146</id><created>2015-10-24</created><authors><author><keyname>Maiorino</keyname><forenames>Enrico</forenames></author><author><keyname>Bianchi</keyname><forenames>Filippo Maria</forenames></author><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author></authors><title>Data-driven detrending of nonstationary fractal time series with echo
  state networks</title><categories>physics.data-an cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a data-driven approach to the problem of detrending
fractal and multifractal time series. We consider a time series as the
measurements elaborated from a dynamical process over time. We assume that such
a dynamical process is predictable to a certain degree, by means of a class of
recurrent networks called echo state networks. Such networks have been shown to
be able to predict the outcome of a number of dynamical processes. Here we
propose to perform a data-driven detrending of nonstationary, fractal and
multifractal time series by using an echo state network operating as a filter.
Notably, we predict the trend component of a given input time series, which is
superimposed to the (multi)fractal component of interest. Such a (estimated)
trend is then removed from the original time series and the residual signal is
analyzed with the Multifractal Detrended Fluctuation Analysis for a
quantitative verification of the correctness of the proposed detrending
procedure. In order to demonstrate the effectiveness of the proposed technique,
we consider several synthetic time series having a self-similar noise component
with known characteristics. Such synthetic time series contain different types
of trends. We also process a real-world dataset, the sunspot time series, which
is well-known for its multifractal features and it has recently gained
attention in the complex systems field. Results demonstrate the validity and
generality of the proposed detrending method based on echo state networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07148</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07148</id><created>2015-10-24</created><authors><author><keyname>Singh</keyname><forenames>Abhinav</forenames></author><author><keyname>Singh</keyname><forenames>Awadhesh Kumar</forenames></author></authors><title>Mobility and Energy Conscious Clustering Protocol for Wireless Networks</title><categories>cs.NI cs.DC</categories><comments>10 pages, International Congress on Information and Communication
  Technology(ICICT-2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a distributed clustering protocol for mobile
wireless sensor networks. A large majority of research in clustering and
routing algorithms for WSNs assume a static network and hence are rendered
inefficient in cases of highly mobile sensor networks, which is an aspect
addressed here. MECP is an energy efficient, mobility aware protocol and
utilizes information about movement of sensor nodes and residual energy as
attributes in network formation. It also provides a mechanism for fault
tolerance to decrease packet data loss in case of cluster head failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07162</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07162</id><created>2015-10-24</created><updated>2015-11-19</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Kuilder</keyname><forenames>Jotte</forenames></author></authors><title>To know or not to know? How looking at payoffs signals selfish behavior</title><categories>q-bio.PE cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In daily life, subjects often face a social dilemma in two stages. In Stage
1, they recognize the social dilemma structure of the decision problem at hand
(a tension between personal interest and collective interest); in Stage 2, they
have to choose between gathering additional information to learn the exact
payoffs corresponding to each of the two options or making a choice without
looking at the payoffs. While previous theoretical research suggests that the
mere act of considering one's strategic options in a social dilemma will be met
with distrust, no experimental study has tested this hypothesis. What does
&quot;looking at payoffs&quot; signal in observers? Do observers' beliefs actually match
decision makers' intentions? Experiment 1 shows that the actual action of
looking at payoffs signals selfish behavior, but it does not actually mean so.
Experiments 2 and 3 show that, when the action of looking at payoffs is
replaced by a self-report question asking the extent to which participants look
at payoffs in their everyday lives, subjects in high looking mode are indeed
more selfish than those in low looking mode, and this is correctly predicted by
observers. These results support Rand and colleagues' Social Heuristics
Hypothesis and the novel &quot;cooperate without looking&quot; model by Yoeli, Hoffman,
and Nowak. However, Experiment 1 shows that actual looking may lead to
different results, possibly caused by the emergence of a moral cleansing
effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07163</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07163</id><created>2015-10-24</created><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Evolutionary Landscape and Management of Population Diversity</title><categories>cs.NE</categories><comments>Combinations of Intelligent Methods and Application, Smart
  Innovations, Systems &amp; Technologies series, Springer</comments><msc-class>68T99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The search ability of an Evolutionary Algorithm (EA) depends on the variation
among the individuals in the population [3, 4, 8]. Maintaining an optimal level
of diversity in the EA population is imperative to ensure that progress of the
EA search is unhindered by premature convergence to suboptimal solutions.
Clearer understanding of the concept of population diversity, in the context of
evolutionary search and premature convergence in particular, is the key to
designing efficient EAs. To this end, this paper first presents a brief
analysis of the EA population diversity issues. Next we present an
investigation on a counter-niching EA technique [4] that introduces and
maintains constructive diversity in the population. The proposed approach uses
informed genetic operations to reach promising, but unexplored or
under-explored areas of the search space, while discouraging premature local
convergence. Simulation runs on a suite of standard benchmark test functions
with Genetic Algorithm (GA) implementation shows promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07165</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07165</id><created>2015-10-24</created><authors><author><keyname>West</keyname><forenames>J.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author><author><keyname>Islam</keyname><forenames>R.</forenames></author></authors><title>Intelligent Financial Fraud Detection Practices: An Investigation</title><categories>cs.CR</categories><comments>Proceedings of the 10th International Conference on Security and
  Privacy in Communication Networks (SecureComm 2014)</comments><msc-class>68U01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial fraud is an issue with far reaching consequences in the finance
industry, government, corporate sectors, and for ordinary consumers. Increasing
dependence on new technologies such as cloud and mobile computing in recent
years has compounded the problem. Traditional methods of detection involve
extensive use of auditing, where a trained individual manually observes reports
or transactions in an attempt to discover fraudulent behaviour. This method is
not only time consuming, expensive and inaccurate, but in the age of big data
it is also impractical. Not surprisingly, financial institutions have turned to
automated processes using statistical and computational methods. This paper
presents a comprehensive investigation on financial fraud detection practices
using such data mining methods, with a particular focus on computational
intelligence-based techniques. Classification of the practices based on key
aspects such as detection algorithm used, fraud type investigated, and success
rate have been covered. Issues and challenges associated with the current
practices and potential future direction of research have also been identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07167</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07167</id><created>2015-10-24</created><authors><author><keyname>West</keyname><forenames>J.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Mining Financial Statement Fraud: An Analysis of Some Experimental
  Issues</title><categories>cs.CR cs.CY</categories><comments>Proceedings of The 10th IEEE Conference on Industrial Electronics and
  Applications (ICIEA 2015), IEEE Press</comments><msc-class>68U99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial statement fraud detection is an important problem with a number of
design aspects to consider. Issues such as (i) problem representation, (ii)
feature selection, and (iii) choice of performance metrics all influence the
perceived performance of detection algorithms. Efficient implementation of
financial fraud detection methods relies on a clear understanding of these
issues. In this paper we present an analysis of the three key experimental
issues associated with financial statement fraud detection, critiquing the
prevailing ideas and providing new understandings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07169</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07169</id><created>2015-10-24</created><authors><author><keyname>Frandi</keyname><forenames>Emanuele</forenames></author><author><keyname>Nanculef</keyname><forenames>Ricardo</forenames></author><author><keyname>Lodi</keyname><forenames>Stefano</forenames></author><author><keyname>Sartori</keyname><forenames>Claudio</forenames></author><author><keyname>Suykens</keyname><forenames>Johan A. K.</forenames></author></authors><title>Fast and Scalable Lasso via Stochastic Frank-Wolfe Methods with a
  Convergence Guarantee</title><categories>stat.ML cs.LG math.OC</categories><report-no>Internal Report 15-93, ESAT-STADIUS, KU Leuven, 2015</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frank-Wolfe (FW) algorithms have been often proposed over the last few years
as efficient solvers for a variety of optimization problems arising in the
field of Machine Learning. The ability to work with cheap projection-free
iterations and the incremental nature of the method make FW a very effective
choice for many large-scale problems where computing a sparse model is
desirable.
  In this paper, we present a high-performance implementation of the FW method
tailored to solve large-scale Lasso regression problems, based on a randomized
iteration, and prove that the convergence guarantees of the standard FW method
are preserved in the stochastic setting. We show experimentally that our
algorithm outperforms several existing state of the art methods, including the
Coordinate Descent algorithm by Friedman et al. (one of the fastest known Lasso
solvers), on several benchmark datasets with a very large number of features,
without sacrificing the accuracy of the model. Our results illustrate that the
algorithm is able to generate the complete regularization path on problems of
size up to four million variables in less than one minute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07170</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07170</id><created>2015-10-24</created><authors><author><keyname>Li</keyname><forenames>Simon</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Mahajan</keyname><forenames>Aditya</forenames></author></authors><title>Privacy-Optimal Strategies for Smart Metering Systems with a
  Rechargeable Battery</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In smart-metered systems, fine-grained power demand data (load profile) is
communicated from a user to the utility provider. The correlation of the load
profile with a user's private activities leaves open the possibility of
inference attacks. Using a rechargeable battery, the user can partially obscure
its load profile and provide some protection to the private information using
various battery charging policies. Using the mutual information as the privacy
metric we study a class of optimal battery charging policies.
  When the power demand is a first-order Markov process, we propose a series of
reductions on the optimization problem and ultimately recast it as a Markov
decision process. In the special case of i.i.d. demand, we explicitly
characterize the optimal policy and show that the associated privacy-leakage
can be expressed as a single-letter mutual information expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07171</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07171</id><created>2015-10-24</created><authors><author><keyname>Poetzl</keyname><forenames>Daniel</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author></authors><title>Formalizing and Checking Thread Refinement for Data-Race-Free Execution
  Models (Extended Version)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When optimizing a thread in a concurrent program (either done manually or by
the compiler), it must be guaranteed that the resulting thread is a refinement
of the original thread. Most theories of valid optimizations are formulated in
terms of valid syntactic transformations on the program code, or in terms of
valid transformations on thread execution traces. We present a new theory
formulated instead in terms of the state of threads at synchronization
operations, and show that it provides several advantages: it supports more
optimizations, and leads to more efficient and simpler procedures for
refinement checking. We develop the theory for the SC-for-DRF execution model
(using locks for synchronization), and show that its application in a compiler
testing setting leads to large performance improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07176</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07176</id><created>2015-10-24</created><authors><author><keyname>Han</keyname><forenames>Qing</forenames></author><author><keyname>Wang</keyname><forenames>Chenxi</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>On the Effect of Fronthaul Latency on ARQ in C-RAN Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Cloud Radio Access Network (C-RAN) architecture, a Control Unit (CU)
implements the baseband processing functionalities of a cluster of Base
Stations (BSs), which are connected to it through a fronthaul network. This
architecture enables centralized processing at the CU, and hence the
implementation of enhanced interference mitigation strategies, but it also
entails an increased decoding latency due to the transport on the fronthaul
network. The fronthaul latency may offset the benefits of centralized
processing when considering the performance of protocols at layer 2 and above.
This letter studies the impact of fronthaul latency on the performance of
standard Automatic Retransmission reQuest (ARQ) protocols, namely Stop and
Wait, Go-Back-N and Selective Repeat. The performance of the C-RAN architecture
in terms of throughput and efficiency is compared to the that of a conventional
cellular system with local processing, as well as with that of a proposed
hybrid C-RAN system in which BSs can perform decoding. The dynamics of the
system are modeled as a multi-dimensional Markov process that includes
sub-chains to capture the temporal correlation of interference and channel
gains. Numerical results yield insights into the impact of system parameters
such as fronthaul latency and signal-to-interference ratio on different ARQ
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07177</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07177</id><created>2015-10-24</created><authors><author><keyname>Mohammadzadeh</keyname><forenames>Nasibeh</forenames></author><author><keyname>Asghar</keyname><forenames>Mohsen Hallaj</forenames></author><author><keyname>Kisore</keyname><forenames>Raghu</forenames></author></authors><title>Implementation of Experimental Test Bed to Evaluate Security in Cellular
  Networks</title><categories>cs.CR</categories><comments>4 pages , 2 Figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The wide development of inter connectivity of cellular networks with the
internet network has made them to be vulnerable. This exposure of the cellular
networks to internet has increased threats to customer end equipment as well as
the carrier infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07182</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07182</id><created>2015-10-24</created><authors><author><keyname>Itti</keyname><forenames>Laurent</forenames></author><author><keyname>Borji</keyname><forenames>Ali</forenames></author></authors><title>Computational models of attention</title><categories>cs.CV</categories><journal-ref>Cognitive Neuroscience: The Biology of the Mind (Fifth Edition),
  (M. S. Gazzaniga, R. B. Ivry, G. R. Mangun Ed.), pp. 1-10, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter reviews recent computational models of visual attention. We
begin with models for the bottom-up or stimulus-driven guidance of attention to
salient visual items, which we examine in seven different broad categories. We
then examine more complex models which address the top-down or goal-oriented
guidance of attention towards items that are more relevant to the task at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07185</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07185</id><created>2015-10-24</created><authors><author><keyname>Darwish</keyname><forenames>Omar</forenames></author><author><keyname>Elmasry</keyname><forenames>Amr</forenames></author><author><keyname>Katajainen</keyname><forenames>Jyrki</forenames></author></authors><title>Memory-Adjustable Navigation Piles with Applications to Sorting and
  Convex Hulls</title><categories>cs.DS</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider space-bounded computations on a random-access machine (RAM) where
the input is given on a read-only random-access medium, the output is to be
produced to a write-only sequential-access medium, and the available workspace
allows random reads and writes but is of limited capacity. The length of the
input is $N$ elements, the length of the output is limited by the computation,
and the capacity of the workspace is $O(S)$ bits for some predetermined
parameter $S$. We present a state-of-the-art priority queue---called an
adjustable navigation pile---for this restricted RAM model. Under some
reasonable assumptions, our priority queue supports $\mathit{minimum}$ and
$\mathit{insert}$ in $O(1)$ worst-case time and $\mathit{extract}$ in $O(N/S +
\lg{} S)$ worst-case time for any $S \geq \lg{} N$. We show how to use this
data structure to sort $N$ elements and to compute the convex hull of $N$
points in the two-dimensional Euclidean space in $O(N^2/S + N \lg{} S)$
worst-case time for any $S \geq \lg{} N$. Following a known lower bound for the
space-time product of any branching program for finding unique elements, both
our sorting and convex-hull algorithms are optimal. The adjustable navigation
pile has turned out to be useful when designing other space-efficient
algorithms, and we expect that it will find its way to yet other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07188</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07188</id><created>2015-10-24</created><authors><author><keyname>Song</keyname><forenames>Yinglei</forenames></author></authors><title>On the Dominating Set Problem in Random Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the {\sc Dominating Set} problem in random graphs. In
a random graph, each pair of vertices are joined by an edge with a probability
of $p$, where $p$ is a positive constant less than $1$. We show that, given a
random graph in $n$ vertices, a minimum dominating set in the graph can be
computed in expected $2^{O(\log_{2}^{2}{n})}$ time. For the parameterized
dominating set problem, we show that it cannot be solved in expected
$O(f(k)n^{c})$ time unless the minimum dominating set problem can be
approximated within a ratio of $o(\log_{2}n)$ in expected polynomial time,
where $f(k)$ is a function of the parameter $k$ and $c$ is a constant
independent of $n$ and $k$. In addition, we show that the parameterized
dominating set problem can be solved in expected $O(f(k)n^{c})$ time when the
probability $p$ depends on $n$ and equals to $\frac{1}{g(n)}$, where $g(n)&lt; n$
is a monotonously increasing function of $n$ and its value approaches infinity
when $n$ approaches infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07193</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07193</id><created>2015-10-24</created><authors><author><keyname>Dukes</keyname><forenames>Kais</forenames></author></authors><title>Statistical Parsing by Machine Learning from a Classical Arabic Treebank</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research into statistical parsing for English has enjoyed over a decade of
successful results. However, adapting these models to other languages has met
with difficulties. Previous comparative work has shown that Modern Arabic is
one of the most difficult languages to parse due to rich morphology and free
word order. Classical Arabic is the ancient form of Arabic, and is understudied
in computational linguistics, relative to its worldwide reach as the language
of the Quran. The thesis is based on seven publications that make significant
contributions to knowledge relating to annotating and parsing Classical Arabic.
  A central argument of this thesis is that using a hybrid representation
closely aligned to traditional grammar leads to improved parsing for Arabic. To
test this hypothesis, two approaches are compared. As a reference, a pure
dependency parser is adapted using graph transformations, resulting in an
87.47% F1-score. This is compared to an integrated parsing model with an
F1-score of 89.03%, demonstrating that joint dependency-constituency parsing is
better suited to Classical Arabic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07197</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07197</id><created>2015-10-24</created><authors><author><keyname>Ren</keyname><forenames>Xiang</forenames></author><author><keyname>Lv</keyname><forenames>Yuanhua</forenames></author><author><keyname>Wang</keyname><forenames>Kuansan</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>Comparative Document Analysis for Large Text Corpora</title><categories>cs.IR</categories><comments>Submission to WWW'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel research problem on joint discovery of
commonalities and differences between two individual documents (or document
sets), called Comparative Document Analysis (CDA). Given any pair of documents
from a document collection, CDA aims to automatically identify sets of quality
phrases to summarize the commonalities of both documents and highlight the
distinctions of each with respect to the other informatively and concisely. Our
solution uses a general graph-based framework to derive novel measures on
phrase semantic commonality and pairwise distinction}, and guides the selection
of sets of phrases by solving two joint optimization problems. We develop an
iterative algorithm to integrate the maximization of phrase commonality or
distinction measure with the learning of phrase-document semantic relevance in
a mutually enhancing way. Experiments on text corpora from two different
domains---scientific publications and news---demonstrate the effectiveness and
robustness of the proposed method on comparing individual documents. Our case
study on comparing news articles published at different dates shows the power
of the proposed method on comparing document sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07208</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07208</id><created>2015-10-25</created><authors><author><keyname>Lemieux</keyname><forenames>Joe</forenames></author><author><keyname>Ma</keyname><forenames>Yuan</forenames></author></authors><title>Vehicle Speed Prediction using Deep Learning</title><categories>cs.LG cs.NE</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Global optimization of the energy consumption of dual power source vehicles
such as hybrid electric vehicles, plug-in hybrid electric vehicles, and plug in
fuel cell electric vehicles requires knowledge of the complete route
characteristics at the beginning of the trip. One of the main characteristics
is the vehicle speed profile across the route. The profile will translate
directly into energy requirements for a given vehicle. However, the vehicle
speed that a given driver chooses will vary from driver to driver and from time
to time, and may be slower, equal to, or faster than the average traffic flow.
If the specific driver speed profile can be predicted, the energy usage can be
optimized across the route chosen. The purpose of this paper is to research the
application of Deep Learning techniques to this problem to identify at the
beginning of a drive cycle the driver specific vehicle speed profile for an
individual driver repeated drive cycle, which can be used in an optimization
algorithm to minimize the amount of fossil fuel energy used during the trip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07211</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07211</id><created>2015-10-25</created><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Men</keyname><forenames>Rui</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>On End-to-End Program Generation from User Intention by Deep Neural
  Networks</title><categories>cs.SE cs.LG</categories><comments>Submitted to 2016 International Conference of Software Engineering
  &quot;Vision of 2025 and Beyond&quot; track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper envisions an end-to-end program generation scenario using
recurrent neural networks (RNNs): Users can express their intention in natural
language; an RNN then automatically generates corresponding code in a
characterby-by-character fashion. We demonstrate its feasibility through a case
study and empirical analysis. To fully make such technique useful in practice,
we also point out several cross-disciplinary challenges, including modeling
user intention, providing datasets, improving model architectures, etc.
Although much long-term research shall be addressed in this new field, we
believe end-to-end program generation would become a reality in future decades,
and we are looking forward to its practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07217</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07217</id><created>2015-10-25</created><updated>2015-12-04</updated><authors><author><keyname>Liu</keyname><forenames>Sixue</forenames></author></authors><title>An Efficient Implementation for WalkSAT</title><categories>cs.AI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic local search (SLS) algorithms have exhibited great effectiveness
in finding models of random instances of the Boolean satisfiability problem
(SAT). As one of the most widely known and used SLS algorithm, WalkSAT plays a
key role in the evolutions of SLS for SAT, and also hold state-of-the-art
performance on random instances. This work proposes a novel implementation for
WalkSAT which decreases the redundant calculations leading to a dramatically
speeding up, thus dominates the latest version of WalkSAT including its
advanced variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07234</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07234</id><created>2015-10-25</created><authors><author><keyname>Brad</keyname><forenames>Raluca</forenames></author><author><keyname>H&#x102;loiu</keyname><forenames>Eugen</forenames></author><author><keyname>Brad</keyname><forenames>Remus</forenames></author></authors><title>Seam Puckering Objective Evaluation Method for Sewing Process</title><categories>cs.CV cs.CE</categories><journal-ref>Annals of the University of Oradea, volume XV, no.1, pp.23-28,
  2014, ISSN 1843-813X</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an automated method for the assessment and classification
of puckering defects detected during the preproduction control stage of the
sewing machine or product inspection. In this respect, we have presented the
possible causes and remedies of the wrinkle nonconformities. Subjective factors
related to the control environment and operators during the seams evaluation
can be reduced using an automated system whose operation is based on image
processing. Our implementation involves spectral image analysis using Fourier
transform and an unsupervised neural network, the Kohonen Map, employed to
classify material specimens, the input images, into five discrete degrees of
quality, from grade 5 (best) to grade 1 (the worst).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07244</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07244</id><created>2015-10-25</created><authors><author><keyname>B&#xf6;rm</keyname><forenames>Steffen</forenames></author><author><keyname>Christophersen</keyname><forenames>Sven</forenames></author></authors><title>Approximation of BEM matrices using GPGPUs</title><categories>cs.MS</categories><msc-class>65N38, 65Y05, 65Y10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of boundary element methods depends crucially on the time
required for setting up the stiffness matrix. The far-field part of the matrix
can be approximated by compression schemes like the fast multipole method or
$\mathcal{H}$-matrix techniques. The near-field part is typically approximated
by special quadrature rules like the Sauter-Schwab technique that can handle
the singular integrals appearing in the diagonal and near-diagonal matrix
elements.
  Since computing one element of the matrix requires only a small amount of
data but a fairly large number of operations, we propose to use GPUs to handle
vectorizable portions of the computation: near-field computations are ideally
suited for vectorization and can therefore be handled very well by GPUs. Modern
far-field compression schemes can be split into a small adaptive portion that
exhibits divergent control flows and is handled by the CPU and a vectorizable
portion that can again be sent to GPUs.
  We propose a hybrid algorithm that splits the computation into tasks for CPUs
and GPUs. Our method presented in this article is able to speedup the setup
time of boundary integral operators by a significant factor of 19-30 for both
the Laplace and the Helmholtz equation in 3D when using two consumer GPGPUs
compared to a quad-core CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07246</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07246</id><created>2015-10-25</created><updated>2016-02-15</updated><authors><author><keyname>Schmuck</keyname><forenames>Anne-Kathrin</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Dynamic Hierarchical Reactive Controller Synthesis</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the formal approach to reactive controller synthesis, a symbolic
controller for a possibly hybrid system is obtained by algorithmically
computing a winning strategy in a two-player game. Such game-solving algorithms
scale poorly as the size of the game graph increases. However, in many
applications, the game graph has a natural hierarchical structure. In this
paper, we propose a modeling formalism and a synthesis algorithm that exploits
this hierarchical structure for more scalable synthesis.
  We define local games on hierarchical graphs as a modeling formalism which
decomposes a large-scale reactive synthesis problem in two dimensions. First,
the construction of a hierarchical game graph introduces abstraction layers,
where each layer is again a two-player game graph. Second, every such layer is
decomposed into multiple local game graphs, each corresponding to a node in the
higher level game graph. While local games have the potential to reduce the
state space for controller synthesis, they lead to more complex synthesis
problems where strategies computed for one local game can impose additional
requirements on lower-level local games.
  Our second contribution is a procedure to construct a dynamic controller for
local game graphs over hierarchies. The controller computes assume-admissible
winning strategies that satisfy local specifications in the presence of
environment assumptions, and dynamically updates specifications and strategies
due to interactions between games at different abstraction layers at each step
of the play. We show that our synthesis procedure is sound: the controller
constructs a play which satisfies all local specifications. We illustrate our
results through an example controlling an autonomous robot in a known,
multistory building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07247</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07247</id><created>2015-10-25</created><authors><author><keyname>Bucheli</keyname><forenames>Samuel</forenames></author></authors><title>Some Notes on Temporal Justification Logic</title><categories>cs.LO math.LO</categories><msc-class>03B42, 03B44, 03B45, 03B70</msc-class><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Justification logics are modal-like logics with the additional capability of
recording the reason, or justification, for modalities in syntactic structures,
called justification terms. Justification logics can be seen as explicit
counterparts to modal logic. The behavior and interaction of agents in
distributed system is often modeled using logics of knowledge and time. In this
paper, we sketch some preliminary ideas on how the modal knowledge part of such
logics of knowledge and time could be replaced with an appropriate
justification logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07250</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07250</id><created>2015-10-25</created><authors><author><keyname>Magurawalage</keyname><forenames>Chathura Sarathchandra</forenames></author><author><keyname>Yang</keyname><forenames>Kun</forenames></author><author><keyname>Wang</keyname><forenames>Kezhi</forenames></author></authors><title>Aqua Computing: Coupling Computing and Communications</title><categories>cs.DC cs.CY cs.NI</categories><comments>A shorter version of this paper will be submitted to an IEEE magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The authors introduce a new vision for providing computing services for
connected devices. It is based on the key concept that future computing
resources will be coupled with communication resources, for enhancing user
experience of the connected users, and also for optimising resources in the
providers' infrastructures. Such coupling is achieved by Joint/Cooperative
resource allocation algorithms, by integrating computing and communication
services and by integrating hardware in networks. Such type of computing, by
which computing services are not delivered independently but dependent of
networking services, is named Aqua Computing. The authors see Aqua Computing as
a novel approach for delivering computing resources to end devices, where
computing power of the devices are enhanced automatically once they are
connected to an Aqua Computing enabled network. The process of resource
coupling is named computation dissolving. Then, an Aqua Computing architecture
is proposed for mobile edge networks, in which computing and wireless
networking resources are allocated jointly or cooperatively by a Mobile Cloud
Controller, for the benefit of the end-users and/or for the benefit of the
service providers. Finally, a working prototype of the system is shown and the
gathered results show the performance of the Aqua Computing prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07252</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07252</id><created>2015-10-25</created><authors><author><keyname>Kuscu</keyname><forenames>M.</forenames></author><author><keyname>Akan</keyname><forenames>O. B.</forenames></author></authors><title>Modeling and Analysis of SiNW FET-Based Molecular Communication Receiver</title><categories>cs.ET</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular Communication (MC) is a bio-inspired communication method based on
the exchange of molecules for information transfer among nanoscale devices. MC
has been extensively studied from various aspects in the literature; however,
the physical design of MC transceiving units is largely neglected with the
assumption that network nodes are entirely biological devices, e.g., engineered
bacteria, which are intrinsically capable of receiving and transmitting
molecular messages. However, the low information processing capacity of
biological devices and the challenge to interface them with macroscale networks
hinder the true application potential of nanonetworks. To overcome this
problem, recently, we proposed a nanobioelectronic MC receiver architecture
exploiting the nanoscale field effect transistor-based biosensor (bioFET)
technology, which provides noninvasive and sensitive molecular detection while
producing electrical signals as the output. In this paper, we introduce a
comprehensive model for silicon nanowire (SiNW) FET-based MC receivers by
integrating the underlying processes in MC and bioFET to provide a unified
analysis framework. We derive closed-form expressions for noise statistics,
signal-to-noise ratio (SNR) at the receiver output, and symbol error
probability (SEP). Performance evaluation in terms of SNR and SEP reveals the
effects of individual system parameters on the detection performance of the
proposed MC receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07253</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07253</id><created>2015-10-25</created><authors><author><keyname>Tiezzi</keyname><forenames>Francesco</forenames></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames></author></authors><title>Reversing Single Sessions</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session-based communication has gained a widespread acceptance in practice as
a means for developing safe communicating systems via structured interactions.
In this paper, we investigate how these structured interactions are affected by
reversibility, which provides a computational model allowing executed
interactions to be undone. In particular, we provide a systematic study of the
integration of different notions of reversibility in both binary and multiparty
single sessions. The considered forms of reversibility are: one for completely
reversing a given session with one backward step, and another for also
restoring any intermediate state of the session with either one backward step
or multiple ones. We analyse the costs of reversing a session in all these
different settings. Our results show that extending binary single sessions to
multiparty ones does not affect the reversibility machinery and its costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07254</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07254</id><created>2015-10-25</created><authors><author><keyname>Chen</keyname><forenames>Jian-Jia</forenames></author></authors><title>Federated Scheduling Admits No Constant Speedup Factors for
  Constrained-Deadline DAG Task System</title><categories>cs.DS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the federated scheduling approaches in multiprocessor systems, a task (1)
either is restricted for sequential executions or (2) has exclusive access to
the assigned processors. There have been several positive results to conduct
good federated scheduling policies, with a constant speedup factor with respect
to the optimal federated scheduling. This paper answers an open question: &quot;For
constrained-deadline task systems, is federated scheduling a good strategy,
compared to the optimal schedules?&quot; The answer is &quot;No!&quot;. This paper presents an
example, which demonstrates that an optimal federated schedule in
multiprocessor scheduling has a speedup factor at least $\Omega(\min\{M, N\})$,
where $N$ is the number of the given tasks and $M$ is the number of the given
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07262</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07262</id><created>2015-10-25</created><authors><author><keyname>Arvaneh</keyname><forenames>Mahnaz</forenames></author><author><keyname>Ward</keyname><forenames>Tomas E.</forenames></author><author><keyname>Robertson</keyname><forenames>Ian H.</forenames></author></authors><title>Effects of Feedback Latency on P300-based Brain-computer Interface</title><categories>cs.HC q-bio.NC</categories><comments>Accepted for publication in IEEE EMBC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feedback has been shown to affect performance when using a Brain-Computer
Interface (BCI) based on sensorimotor rhythms. In contrast, little is known
about the influence of feedback on P300-based BCIs. There is still an open
question whether feedback affects the regulation of P300 and consequently the
operation of P300-based BCIs. In this paper, for the first time, the influence
of feedback on the P300-based BCI speller task is systematically assessed. For
this purpose, 24 healthy participants performed the classic P300-based BCI
speller task, while only half of them received feedback. Importantly, the
number of flashes per letter was reduced on a regular basis in order to
increase the frequency of providing feedback. Experimental results showed that
feedback could significantly improve the P300-based BCI speller performance, if
it was provided in short time intervals (e.g. in sequences as short as 4 to 6
flashes per row/column). Moreover, our offline analysis showed that providing
feedback remarkably enhanced the relevant ERP patterns and attenuated the
irrelevant ERP patterns, such that the discrimination between target and
nontarget EEG trials increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07263</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07263</id><created>2015-10-25</created><authors><author><keyname>Arvaneh</keyname><forenames>Mahnaz</forenames></author><author><keyname>Umilta</keyname><forenames>Alberto</forenames></author><author><keyname>Robertson</keyname><forenames>Ian H.</forenames></author></authors><title>Filter Bank Common Spatial Patterns in Mental Workload Estimation</title><categories>cs.HC</categories><comments>Accepted for publication in IEEE EMBC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EEG-based workload estimation technology provides a real time means of
assessing mental workload. Such technology can effectively enhance the
performance of the human-machine interaction and the learning process. When
designing workload estimation algorithms, a crucial signal processing component
is the feature extraction step. Despite several studies on this field, the
spatial properties of the EEG signals were mostly neglected. Since EEG
inherently has a poor spacial resolution, features extracted individually from
each EEG channel may not be sufficiently efficient. This problem becomes more
pronounced when we use low-cost but convenient EEG sensors with limited
stability which is the case in practical scenarios. To address this issue, in
this paper, we introduce a filter bank common spatial patterns algorithm
combined with a feature selection method to extract spatio-spectral features
discriminating different mental workload levels. To evaluate the proposed
algorithm, we carry out a comparative analysis between two representative types
of working memory tasks using data recorded from an Emotiv EPOC headset which
is a mobile low-cost EEG recording device. The experimental results showed that
the proposed spatial filtering algorithm outperformed the state-of-the
algorithms in terms of the classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07273</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07273</id><created>2015-10-25</created><authors><author><keyname>Sasahara</keyname><forenames>Hampei</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Nagahara</keyname><forenames>Masaaki</forenames></author></authors><title>Multiuser Detection by MAP Estimation with Sum-of-Absolute-Values
  Relaxation</title><categories>cs.IT math.IT</categories><comments>submitted; 6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider multiuser detection that copes with multiple
access interference caused in star-topology machine-to-machine (M2M)
communications. We assume that the transmitted signals are discrete-valued
(e.g. binary signals taking values of $\pm 1$), which is taken into account as
prior information in detection. We formulate the detection problem as the
maximum a posteriori (MAP) estimation, which is relaxed to a convex
optimization called the sum-of-absolute-values (SOAV) optimization. The SOAV
optimization can be efficiently solved by a proximal splitting algorithm, for
which we give the proximity operator in a closed form. Numerical simulations
are shown to illustrate the effectiveness of the proposed approach compared
with the linear minimum mean-square-error (LMMSE) and the least absolute
shrinkage and selection operator (LASSO) methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07276</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07276</id><created>2015-10-25</created><authors><author><keyname>Kop</keyname><forenames>Cynthia</forenames></author><author><keyname>Middeldorp</keyname><forenames>Aart</forenames></author><author><keyname>Sternagel</keyname><forenames>Thomas</forenames></author></authors><title>Conditional Complexiy</title><categories>cs.LO</categories><comments>This is an extended and improved version of &quot;Conditional Complexity&quot;
  as published in the proceedings of RTA 2015. It has been submitted for
  journal publication in LMCS</comments><acm-class>F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a notion of complexity for oriented conditional term rewrite
systems. This notion is realistic in the sense that it measures not only
successful computations but also partial computations that result in a failed
rule application. A transformation to unconditional context-sensitive rewrite
systems is presented which reflects this complexity notion, as well as a
technique to derive runtime and derivational complexity bounds for the result
of this transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07293</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07293</id><created>2015-10-25</created><updated>2015-12-08</updated><authors><author><keyname>Sulzmann</keyname><forenames>Martin</forenames></author><author><keyname>Thiemann</keyname><forenames>Peter</forenames></author></authors><title>Forkable Regular Expressions</title><categories>cs.FL cs.LO cs.PL</categories><comments>12 pages plus technical appendix, to appear in LATA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider forkable regular expressions, which enrich regular expressions
with a fork operator, to establish a formal basis for static and dynamic
analysis of the communication behavior of concurrent programs. We define a
novel compositional semantics for forkable expressions, establish their
fundamental properties, and define derivatives for them as a basis for the
generation of automata, for matching, and for language containment tests.
Forkable expressions may give rise to non-regular languages, in general, but we
identify sufficient conditions on expressions that guarantee finiteness of the
automata construction via derivatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07295</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07295</id><created>2015-10-25</created><authors><author><keyname>Garg</keyname><forenames>Nikhil</forenames></author><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey</forenames></author></authors><title>Impact of Dual Slope Path Loss on User Association in HetNets</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 8 figures, Accepted to IEEE Globecom 2015 Workshop on 5G
  Heterogeneous and Small Cell Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent load balancing is essential to fully realize the benefits of
dense heterogeneous networks. Current techniques have largely been studied with
single slope path loss models, though multi-slope models are known to more
closely match real deployments. This paper develops insight into the
performance of biasing and uplink/downlink decoupling for user association in
HetNets with dual slope path loss models. It is shown that dual slope path loss
models change the tradeoffs inherent in biasing and reduce gains from both
biasing and uplink/downlink decoupling. The results show that with the dual
slope path loss models, the bias maximizing the median rate is not optimal for
other users, e.g., edge users. Furthermore, optimal downlink biasing is shown
to realize most of the gains from downlink-uplink decoupling. Moreover, the
user association gains in dense networks are observed to be quite sensitive to
the path loss exponent beyond the critical distance in a dual slope model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07303</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07303</id><created>2015-10-25</created><authors><author><keyname>McLeod</keyname><forenames>Clay</forenames></author></authors><title>A Framework for Distributed Deep Learning Layer Design in Python</title><categories>cs.LG</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a framework for testing Deep Neural Network (DNN) design in
Python is presented. First, big data, machine learning (ML), and Artificial
Neural Networks (ANNs) are discussed to familiarize the reader with the
importance of such a system. Next, the benefits and detriments of implementing
such a system in Python are presented. Lastly, the specifics of the system are
explained, and some experimental results are presented to prove the
effectiveness of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07308</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07308</id><created>2015-10-25</created><authors><author><keyname>Jana</keyname><forenames>Suman</forenames></author><author><keyname>Erlingsson</keyname><forenames>&#xda;lfar</forenames></author><author><keyname>Ion</keyname><forenames>Iulia</forenames></author></authors><title>Apples and Oranges: Detecting Least-Privilege Violators with Peer Group
  Analysis</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering software into peer groups based on its apparent functionality
allows for simple, intuitive categorization of software that can, in
particular, help identify which software uses comparatively more privilege than
is necessary to implement its functionality. Such relative comparison can
improve the security of a software ecosystem in a number of ways. For example,
it can allow market operators to incentivize software developers to adhere to
the principle of least privilege, e.g., by encouraging users to use
alternative, less-privileged applications for any desired functionality. This
paper introduces software peer group analysis, a novel technique to identify
least privilege violation and rank software based on the severity of the
violation. We show that peer group analysis is an effective tool for detecting
and estimating the severity of least privilege violation. It provides
intuitive, meaningful results, even across different definitions of peer groups
and security-relevant privileges. Our evaluation is based on empirically
applying our analysis to over a million software items, in two different online
software markets, and on a validation of our assumptions in a medium-scale user
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07312</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07312</id><created>2015-10-25</created><authors><author><keyname>Bastos</keyname><forenames>Josefran de Oliveira</forenames></author><author><keyname>Coregliano</keyname><forenames>Leonardo Nagami</forenames></author></authors><title>Packing densities of layered permutations and the minimum number of
  monotone sequences in layered permutations</title><categories>math.CO cs.DM</categories><comments>19 pages</comments><msc-class>05A05 (Primary), 05D99 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present two new results of layered permutation densities.
  The first one generalizes theorems from H\&quot;ast\&quot;o [P. A. Hasto, The packing
density of other layered permutations, Electron. J. Combin. 9 (2002/03), no. 2,
Research paper 1, 16, Permutation patterns (Otago, 2003)] and Warren [D.
Warren, Optimal packing behavior of some 2-block patterns, Ann. Comb. 8 (2004),
no. 3, 355-367] to compute the permutation packing of permutations with layer
sequence $(1^a,\ell_1,\ell_2,\ldots,\ell_k)$ such that $2^a-a-1\geq k$ (and
similar permutations).
  As a second result, we prove that the minimum density of monotone sequences
of length $k+1$ in an arbitrarily large layered permutation is asymptotically
$1/k^k$. This value is compatible with a conjecture from Myers [J. S. Myers,
The minimum number of monotone subsequences, Electron. J. Combin. 9 (2002/03),
no. 2, Research paper 4, 17 pp. (electronic), Permutation patterns (Otago,
2003)] for the problem without the layered restriction (the same problem where
the monotone sequences have different lengths is also studied).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07313</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07313</id><created>2015-10-25</created><authors><author><keyname>Sadigh</keyname><forenames>Dorsa</forenames></author><author><keyname>Kapoor</keyname><forenames>Ashish</forenames></author></authors><title>Safe Control under Uncertainty</title><categories>cs.SY cs.AI cs.LO cs.RO</categories><comments>10 pages, 6 figures, Submitted to HSCC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controller synthesis for hybrid systems that satisfy temporal specifications
expressing various system properties is a challenging problem that has drawn
the attention of many researchers. However, making the assumption that such
temporal properties are deterministic is far from the reality. For example,
many of the properties the controller has to satisfy are learned through
machine learning techniques based on sensor input data. In this paper, we
propose a new logic, Probabilistic Signal Temporal Logic (PrSTL), as an
expressive language to define the stochastic properties, and enforce
probabilistic guarantees on them. We further show how to synthesize safe
controllers using this logic for cyber-physical systems under the assumption
that the stochastic properties are based on a set of Gaussian random variables.
One of the key distinguishing features of PrSTL is that the encoded logic is
adaptive and changes as the system encounters additional data and updates its
beliefs about the latent random variables that define the safety properties. We
demonstrate our approach by synthesizing safe controllers under the PrSTL
specifications for multiple case studies including control of quadrotors and
autonomous vehicles in dynamic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07315</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07315</id><created>2015-10-25</created><authors><author><keyname>Chazan</keyname><forenames>Shlomo E.</forenames></author><author><keyname>Goldberger</keyname><forenames>Jacob</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author></authors><title>A Hybrid Approach for Speech Enhancement Using MoG Model and Neural
  Network Phoneme Classifier</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a single-microphone speech enhancement algorithm. A
hybrid approach is proposed merging the generative mixture of Gaussians (MoG)
model and the discriminative neural network (NN). The proposed algorithm is
executed in two phases, the training phase, which does not recur, and the test
phase. First, the noise-free speech power spectral density (PSD) is modeled as
a MoG, representing the phoneme based diversity in the speech signal. An NN is
then trained with phoneme labeled database for phoneme classification with
mel-frequency cepstral coefficients (MFCC) as the input features. Given the
phoneme classification results, a speech presence probability (SPP) is obtained
using both the generative and discriminative models. Soft spectral subtraction
is then executed while simultaneously, the noise estimation is updated. The
discriminative NN maintain the continuity of the speech and the generative
phoneme-based MoG preserves the speech spectral structure. Extensive
experimental study using real speech and noise signals is provided. We also
compare the proposed algorithm with alternative speech enhancement algorithms.
We show that we obtain a significant improvement over previous methods in terms
of both speech quality measures and speech recognition results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07317</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07317</id><created>2015-10-25</created><authors><author><keyname>Raza</keyname><forenames>S. Hussain</forenames></author><author><keyname>Javed</keyname><forenames>Omar</forenames></author><author><keyname>Das</keyname><forenames>Aveek</forenames></author><author><keyname>Sawhney</keyname><forenames>Harpreet</forenames></author><author><keyname>Cheng</keyname><forenames>Hui</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Depth Extraction from Videos Using Geometric Context and Occlusion
  Boundaries</title><categories>cs.CV</categories><comments>British Machine Vision Conference (BMVC) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm to estimate depth in dynamic video scenes. We propose
to learn and infer depth in videos from appearance, motion, occlusion
boundaries, and geometric context of the scene. Using our method, depth can be
estimated from unconstrained videos with no requirement of camera pose
estimation, and with significant background/foreground motions. We start by
decomposing a video into spatio-temporal regions. For each spatio-temporal
region, we learn the relationship of depth to visual appearance, motion, and
geometric classes. Then we infer the depth information of new scenes using
piecewise planar parametrization estimated within a Markov random field (MRF)
framework by combining appearance to depth learned mappings and occlusion
boundary guided smoothness constraints. Subsequently, we perform temporal
smoothing to obtain temporally consistent depth maps. To evaluate our depth
estimation algorithm, we provide a novel dataset with ground truth depth for
outdoor video scenes. We present a thorough evaluation of our algorithm on our
new dataset and the publicly available Make3d static image dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07320</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07320</id><created>2015-10-25</created><authors><author><keyname>Raza</keyname><forenames>S. Hussain</forenames></author><author><keyname>Grundmann</keyname><forenames>Matthias</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Geometric Context from Videos</title><categories>cs.CV</categories><comments>Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference
  on</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm for estimating the broad 3D geometric structure
of outdoor video scenes. Leveraging spatio-temporal video segmentation, we
decompose a dynamic scene captured by a video into geometric classes, based on
predictions made by region-classifiers that are trained on appearance and
motion features. By examining the homogeneity of the prediction, we combine
predictions across multiple segmentation hierarchy levels alleviating the need
to determine the granularity a priori. We built a novel, extensive dataset on
geometric context of video to evaluate our method, consisting of over 100
ground-truth annotated outdoor videos with over 20,000 frames. To further scale
beyond this dataset, we propose a semi-supervised learning framework to expand
the pool of labeled data with high confidence predictions obtained from
unlabeled data. Our system produces an accurate prediction of geometric context
of video achieving 96% accuracy across main geometric classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07323</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07323</id><created>2015-10-25</created><authors><author><keyname>Raza</keyname><forenames>S. Hussain</forenames></author><author><keyname>Humayun</keyname><forenames>Ahmad</forenames></author><author><keyname>Grundmann</keyname><forenames>Matthias</forenames></author><author><keyname>Anderson</keyname><forenames>David</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>Finding Temporally Consistent Occlusion Boundaries in Videos using
  Geometric Context</title><categories>cs.CV</categories><comments>Applications of Computer Vision (WACV), 2015 IEEE Winter Conference
  on</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for finding temporally consistent occlusion
boundaries in videos to support segmentation of dynamic scenes. We learn
occlusion boundaries in a pairwise Markov random field (MRF) framework. We
first estimate the probability of an spatio-temporal edge being an occlusion
boundary by using appearance, flow, and geometric features. Next, we enforce
occlusion boundary continuity in a MRF model by learning pairwise occlusion
probabilities using a random forest. Then, we temporally smooth boundaries to
remove temporal inconsistencies in occlusion boundary estimation. Our proposed
framework provides an efficient approach for finding temporally consistent
occlusion boundaries in video by utilizing causality, redundancy in videos, and
semantic layout of the scene. We have developed a dataset with fully annotated
ground-truth occlusion boundaries of over 30 videos ($5000 frames). This
dataset is used to evaluate temporal occlusion boundaries and provides a much
needed baseline for future studies. We perform experiments to demonstrate the
role of scene layout, and temporal information for occlusion reasoning in
dynamic scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07325</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07325</id><created>2015-10-25</created><authors><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Chatzivasileiadis</keyname><forenames>Spyros</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Towards Electronics-based Emergency Control in Power Grids with High
  Renewable Penetration</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1504.04684</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional emergency control schemes in power systems usually accompany with
power interruption yielding severely economic damages to customers. This paper
sketches the ideas of a viable alternative for traditional remedial controls
for power grids with high penetration of renewables, in which the renewables
are integrated with synchronverters to mimic the dynamics of conventional
generators. In this novel emergency control scheme, the power electronics
resources are exploited to control the inertia and damping of the imitated
generators in order to quickly compensate for the deviations caused by fault
and thereby bound the fault-on dynamics and stabilize the power system under
emergency situations. This emergency control not only saves investments and
operating costs for modern and future power systems, but also helps to offer
seamless electricity service to customers. Simple numerical simulation will be
used to illustrate the concept of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07338</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07338</id><created>2015-10-25</created><authors><author><keyname>Miller</keyname><forenames>Brad</forenames></author><author><keyname>Kantchelian</keyname><forenames>Alex</forenames></author><author><keyname>Tschantz</keyname><forenames>Michael Carl</forenames></author><author><keyname>Afroz</keyname><forenames>Sadia</forenames></author><author><keyname>Bahwani</keyname><forenames>Rekha</forenames></author><author><keyname>Faizullabhoy</keyname><forenames>Riyaz</forenames></author><author><keyname>Huang</keyname><forenames>Ling</forenames></author><author><keyname>Shankar</keyname><forenames>Vaishaal</forenames></author><author><keyname>Wu</keyname><forenames>Tony</forenames></author><author><keyname>Yiu</keyname><forenames>George</forenames></author><author><keyname>Joseph</keyname><forenames>Anthony D.</forenames></author><author><keyname>Tygar</keyname><forenames>J. D.</forenames></author></authors><title>Back to the Future: Malware Detection with Temporally Consistent Labels</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The malware detection arms race involves constant change: malware changes to
evade detection and labels change as detection mechanisms react. Recognizing
that malware changes over time, prior work has enforced temporally consistent
samples by requiring that training binaries predate evaluation binaries. We
present temporally consistent labels, requiring that training labels also
predate evaluation binaries since training labels collected after evaluation
binaries constitute label knowledge from the future. Using a dataset containing
1.1 million binaries from over 2.5 years, we show that enforcing temporal label
consistency decreases detection from 91% to 72% at a 0.5% false positive rate
compared to temporal samples alone.
  The impact of temporal labeling demonstrates the potential of improved labels
to increase detection results. Hence, we present a detector capable of
selecting binaries for submission to an expert labeler for review. At a 0.5%
false positive rate, our detector achieves a 72% true positive rate without an
expert, which increases to 77% and 89% with 10 and 80 expert queries daily,
respectively. Additionally, we detect 42% of malicious binaries initially
undetected by all 32 antivirus vendors from VirusTotal used in our evaluation.
For evaluation at scale, we simulate the human expert labeler and show that our
approach is robust against expert labeling errors. Our novel contributions
include a scalable malware detector integrating manual review with machine
learning and the examination of temporal label consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07342</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07342</id><created>2015-10-25</created><authors><author><keyname>Ling</keyname><forenames>Ying</forenames></author><author><keyname>Wan</keyname><forenames>Tao</forenames></author><author><keyname>Qin</keyname><forenames>Zengchang</forenames></author></authors><title>Stable Matching with Incomplete Information in Structured Networks</title><categories>cs.GT cs.SI physics.soc-ph</categories><comments>13 pages; 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate stable matching in structured networks.
Consider case of matching in social networks where candidates are not fully
connected. A candidate on one side of the market gets acquaintance with which
one on the heterogeneous side depends on the structured network. We explore
four well-used structures of networks and define the social circle by the
distance between each candidate. When matching within social circle, we have
equilibrium distinguishes from each other since each social network's topology
differs. Equilibrium changes with the change on topology of each network and it
always converges to the same stable outcome as complete information algorithm
if there is no block to reach anyone in agent's social circle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07355</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07355</id><created>2015-10-25</created><authors><author><keyname>Liu</keyname><forenames>Lei</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Su</keyname><forenames>Yuping</forenames></author></authors><title>A Low-Complexity Gaussian Message Passing Iterative Detector for Massive
  MU-MIMO Systems</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Information,Communications and
  Signal Processing(ICICS), Accepted, 5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a low-complexity Gaussian Message Passing Iterative
Detection (GMPID) method over a pairwise graph for a massive Multiuser
Multiple-Input Multiple-Output (MU-MIMO) system, in which a base station with M
antennas serves K Gaussian sources simultaneously. Both K and M are large
numbers and we consider the cases that K&lt;M in this paper. The GMPID is a
message passing algorithm based on a fully connected loopy graph, which is well
known that it is not convergent in some cases. In this paper, we first analyse
the convergence of GMPID. Two sufficient conditions that the GMPID converges to
the Minimum Mean Square Error (MMSE) detection are proposed. However, the GMPID
may still not converge when $K/M&gt;(\sqrt{2}-1)^2$. Therefore, a new convergent
GMPID with equally low complexity called SA-GMPID is proposed, which converges
to the MMSE detection for any K&lt; M with a faster convergence speed. Finally,
numerical results are provided to verify the validity and accuracy of the
proposed theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07357</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07357</id><created>2015-10-25</created><authors><author><keyname>Chlebus</keyname><forenames>Bogdan S.</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Vaya</keyname><forenames>Shailesh</forenames></author></authors><title>Distributed Communication in Bare-Bones Wireless Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider wireless networks in which the effects of interference are
determined by the SINR model. We address the question of structuring
distributed communication when stations have very limited individual
capabilities. In particular, nodes do not know their geographic coordinates,
neighborhoods or even the size~$n$ of the network, nor can they sense
collisions. Each node is equipped only with its unique name from a range $\{1,
\dots, N\}$. We study the following three settings and distributed algorithms
for communication problems in each of them. In the uncoordinated-start case,
when one node starts an execution and other nodes are awoken by receiving
messages from already awoken nodes, we present a randomized broadcast algorithm
which wakes up all the nodes in $O(n \log^2 N)$ rounds with high probability.
In the synchronized-start case, when all the nodes simultaneously start an
execution, we give a randomized algorithm that computes a backbone of the
network in $O(\Delta\log^{7} N)$ rounds with high probability. Finally, in the
partly-coordinated-start case, when a number of nodes start an execution
together and other nodes are awoken by receiving messages from the already
awoken nodes, we develop an algorithm that creates a backbone network in time
$O(n\log^2 N +\Delta\log^{7} N)$ with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07358</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07358</id><created>2015-10-25</created><updated>2016-02-27</updated><authors><author><keyname>Feigenbaum</keyname><forenames>Itai</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew P.</forenames></author></authors><title>Selfish Knapsack</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a selfish variant of the knapsack problem. In our version, the
items are owned by agents, and each agent can misrepresent the set of items she
owns---either by avoiding reporting some of them (understating), or by
reporting additional ones that do not exist (overstating). Each agent's
objective is to maximize, within the items chosen for inclusion in the
knapsack, the total valuation of her own chosen items. The knapsack problem, in
this context, seeks to minimize the worst-case approximation ratio for social
welfare at equilibrium. We show that a randomized greedy mechanism has
attractive strategic properties: in general, it has a correlated price of
anarchy of $2$ (subject to a mild assumption). For overstating-only agents, it
becomes strategyproof; we also provide a matching lower bound of $2$ on the
(worst-case) approximation ratio attainable by randomized strategyproof
mechanisms, and show that no deterministic strategyproof mechanism can provide
any constant approximation ratio. We also deal with more specialized
environments. For the case of $2$ understating-only agents, we provide a
randomized strategyproof $\frac{5+4\sqrt{2}}{7} \approx 1.522$-approximate
mechanism, and a lower bound of $\frac{5\sqrt{5}-9}{2} \approx 1.09$. When all
agents but one are honest, we provide a deterministic strategyproof
$\frac{1+\sqrt{5}}{2} \approx 1.618$-approximate mechanism with a matching
lower bound. Finally, we consider a model where agents can misreport their
items' properties rather than existence. Specifically, each agent owns a single
item, whose value-to-size ratio is publicly known, but whose actual value and
size are not. We show that an adaptation of the greedy mechanism is
strategyproof and $2$-approximate, and provide a matching lower bound; we also
show that no deterministic strategyproof mechanism can provide a constant
approximation ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07363</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07363</id><created>2015-10-26</created><authors><author><keyname>Pouransari</keyname><forenames>Hadi</forenames></author><author><keyname>Coulier</keyname><forenames>Pieter</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>Fast hierarchical solvers for sparse matrices</title><categories>math.NA cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse linear systems are ubiquitous in various scientific computing
applications. Inversion of sparse matrices with standard direct solve schemes
are prohibitive for large systems due to their quadratic/cubic complexity.
Iterative solvers, on the other hand, demonstrate better scalability. However,
they suffer from poor convergence rates when used without a preconditioner.
There are many preconditioners developed for different problems, such as ILU,
AMG, Gauss-Seidel, etc. The choice of an effective preconditioner is highly
problem dependent. We propose a novel fully algebraic sparse matrix solve
algorithm, which has linear complexity with the problem size. Our scheme is
based on the Gauss elimination. For a given matrix, we approximate the LU
factorization with a tunable accuracy determined a priori. This method can be
used as a stand-alone direct solver with linear complexity and tunable
accuracy, or it can be used as a black-box preconditioner in conjunction with
iterative methods such as GMRES. The proposed solver is based on the low-rank
approximation of fill-ins generated during the elimination. Similar to
H-matrices, fill-ins corresponding to blocks that are well-separated in the
adjacency graph are represented via a hierarchical structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07369</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07369</id><created>2015-10-26</created><authors><author><keyname>So</keyname><forenames>Jungho</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author></authors><title>Enhancing Non-Orthogonal Multiple Access By Forming Relaying Broadcast
  Channels</title><categories>cs.IT math.IT</categories><comments>29 pages, 5 figures, submitted to IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, using relaying broadcast channels (RBCs) as component channels
for non-orthogonal multiple access (NOMA) is proposed to enhance the
performance of NOMA in single-input single-output (SISO) cellular downlink
systems. To analyze the performance of the proposed scheme, an achievable rate
region of a RBC with compress-and-forward (CF) relaying is newly derived based
on the recent work of noisy network coding (NNC). Based on the analysis of the
achievable rate region of a RBC with decode-and-forward (DF) relaying, CF
relaying, or CF relaying with dirty-paper coding (DPC) at the transmitter, the
overall system performance of NOMA equipped with RBC component channels is
investigated. It is shown that NOMA with RBC-DF yields marginal gain and NOMA
with RBC-CF/DPC yields drastic gain over the simple NOMA based on broadcast
component channels in a practical system setup. By going beyond simple
broadcast channel (BC)/successive interference cancellation (SIC) to advanced
multi-terminal encoding including DPC and CF/NNC, far larger gains can be
obtained for NOMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07380</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07380</id><created>2015-10-26</created><authors><author><keyname>Agha-mohammadi</keyname><forenames>Ali-akbar</forenames></author><author><keyname>Agarwal</keyname><forenames>Saurav</forenames></author><author><keyname>Chakravorty</keyname><forenames>Suman</forenames></author><author><keyname>Amato</keyname><forenames>Nancy M.</forenames></author></authors><title>Simultaneous Localization and Planning for Physical Mobile Robots via
  Enabling Dynamic Replanning in Belief Space</title><categories>cs.RO cs.SY</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous planning while localizing is a crucial ability for an autonomous
robot operating under uncertainty. This paper addresses this problem by
designing methods to dynamically replan while the localization uncertainty or
environment map is updated. In particular, relying on sampling-based methods,
the proposed online planning scheme can cope with challenging situations,
including when the localization update or changes in the environment alter the
homotopy class of trajectories, in which the optimal plan resides. The proposed
algorithm eliminates the need for stabilization in the state-of-the-art FIRM
(Feedback-based Information RoadMap) method, and outperforms its performance
and success probability. Applying belief space planning to physical systems
brings with it a plethora of challenges. Thus, designing computationally
tractable algorithms, a key focus and contribution of this paper is to
implement the proposed planner on a physical robot and show the SLAP
(simultaneous localization and planning) performance under uncertainty, in
changing environments, and in the presence of large disturbances such as
kidnapped robot situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07382</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07382</id><created>2015-10-26</created><authors><author><keyname>Anwar</keyname><forenames>Anika</forenames></author><author><keyname>Ahmed</keyname><forenames>Ishrat</forenames></author><author><keyname>Hashem</keyname><forenames>Tanzima</forenames></author><author><keyname>Mahmud</keyname><forenames>Jalal</forenames></author></authors><title>Impact of Imbalance Usage of Social Networking Sites on Families</title><categories>cs.HC cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of social networking sites (SNSs) such as Facebook and
Google+, investigating the impact of SNSs on our lives has become an important
research area in recent years. Though SNS usage plays a key role in connecting
people with friends and families from distant places, SNSs also bring concern
for families. We focus on imbalance SNS usage, i.e., an individual remains busy
in using SNSs when her family member is expecting to spend time with her. More
specifically, we investigate the cause and pattern of imbalance SNS usage and
how the emotion of family members may become affected, if they use SNSs in an
imbalanced way in a regular manner. This paper is the first attempt to identify
the relationship between an individual's imbalance SNS usage and the emotion of
her family member in the context of a developing country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07385</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07385</id><created>2015-10-26</created><authors><author><keyname>Cossu</keyname><forenames>Jean-Val&#xe8;re</forenames><affiliation>LIA</affiliation></author><author><keyname>Bonnefoy</keyname><forenames>Ludovic</forenames><affiliation>LIA</affiliation></author><author><keyname>Bost</keyname><forenames>Xavier</forenames><affiliation>LIA</affiliation></author><author><keyname>B&#xe8;ze</keyname><forenames>Marc El</forenames><affiliation>LIA</affiliation></author></authors><title>How to merge three different methods for information filtering ?</title><categories>cs.CL cs.IR</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is now a gold marketing tool for entities concerned with online
reputation. To automatically monitor online reputation of entities , systems
have to deal with ambiguous entity names, polarity detection and topic
detection. We propose three approaches to tackle the first issue: monitoring
Twitter in order to find relevant tweets about a given entity. Evaluated within
the framework of the RepLab-2013 Filtering task, each of them has been shown
competitive with state-of-the-art approaches. Mainly we investigate on how much
merging strategies may impact performances on a filtering task according to the
evaluation measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07389</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07389</id><created>2015-10-26</created><updated>2015-12-03</updated><authors><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames></author><author><keyname>Dann</keyname><forenames>Christoph</forenames></author><author><keyname>Lucas</keyname><forenames>Christopher G.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>The Human Kernel</title><categories>cs.LG cs.AI stat.ML</categories><comments>11 pages, 5 figures. To appear in Neural Information Processing
  Systems (NIPS) 2015. Version 2: Figure 2 (i)-(n) now displays the second set
  of progressive function learning experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian nonparametric models, such as Gaussian processes, provide a
compelling framework for automatic statistical modelling: these models have a
high degree of flexibility, and automatically calibrated complexity. However,
automating human expertise remains elusive; for example, Gaussian processes
with standard kernels struggle on function extrapolation problems that are
trivial for human learners. In this paper, we create function extrapolation
problems and acquire human responses, and then design a kernel learning
framework to reverse engineer the inductive biases of human learners across a
set of behavioral experiments. We use the learned kernels to gain psychological
insights and to extrapolate in human-like ways that go beyond traditional
stationary and polynomial kernels. Finally, we investigate Occam's razor in
human and Gaussian process based function learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07390</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07390</id><created>2015-10-26</created><authors><author><keyname>Sin</keyname><forenames>YongChol</forenames></author><author><keyname>Choe</keyname><forenames>MyongSong</forenames></author><author><keyname>Ryang</keyname><forenames>GyongIl</forenames></author></authors><title>Pan-Tilt Camera and PIR Sensor Fusion Based Moving Object Detection for
  Mobile Security Robots</title><categories>cs.RO cs.CV</categories><comments>13 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of fundamental issues for security robots is to detect and track people
in the surroundings. The main problems of this task are real-time constraints,
a changing background, varying illumination conditions and a non-rigid shape of
the person to be tracked. In this paper, we propose a solution for tracking
with a pan-tilt camera and a passive infrared range (PIR) sensor to detect the
moving object based on consecutive frame difference. The proposed method is
excellent in real-time performance because it requires only a little memory and
computation. Experiment results show that this method can detect the moving
object such as human efficiently and accurately in non-stationary and complex
indoor environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07391</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07391</id><created>2015-10-26</created><authors><author><keyname>Rachmadi</keyname><forenames>Reza Fuad</forenames></author><author><keyname>Purnama</keyname><forenames>I Ketut Eddy</forenames></author></authors><title>Vehicle Color Recognition using Convolutional Neural Network</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle color information is one of the important elements in ITS
(Intelligent Traffic System). In this paper, we present a vehicle color
recognition method using convolutional neural network (CNN). Naturally, CNN is
designed to learn classification method based on shape information, but we
proved that CNN can also learn classification based on color distribution. In
our method, we convert the input image to two different color spaces, HSV and
CIE Lab, and run it to some CNN architecture. The training process follow
procedure introduce by Krizhevsky, that learning rate is decreasing by factor
of 10 after some iterations. To test our method, we use publicly vehicle color
recognition dataset provided by Chen. The results, our model outperform the
original system provide by Chen with 2% higher overall accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07394</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07394</id><created>2015-10-26</created><updated>2016-01-20</updated><authors><author><keyname>Sippel</keyname><forenames>Erik</forenames></author><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Capacity of the Gaussian Full-Duplex Two-Hop Relay Channel with
  Self-Interference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the capacity of the Gaussian fullduplex (FD) two-hop relay
channel with self-interference (SI) is studied. In the two-hop relay channel,
the source transmits data to a relay, which forwards the data to the
destination, and a direct link between the source and the destination is not
present. Since the relay is assumed to be FD, the relay is transmitting and
receiving at the same time and in the same frequency band. We consider a
practical FD relay, where the SI caused by the transmitted signal to the
received signal is taken into account. Thereby, the SI is modeled as an
additive Gaussian random noise whose variance is proportional to the
instantaneous power of the transmit signal. We show that the capacity of the
Gaussian FD two-hop relay channel is achieved either by a Gaussian or by a
discrete input distribution at the relay. In contrast, the capacity achieving
distribution at the source is Gaussian with a varying variance in form of a
waterfilling power allocation across different symbols. We numerically compare
the capacity of the Gaussian FD two-hop relay channel with the achievable rates
of conventional FD relaying, optimal half-duplex (HD) relaying, and
conventional HD relaying. Here, conventional FD relaying is shown to be nearly
capacity achieving for all cases, where FD relaying significantly outperforms
HD relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07402</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07402</id><created>2015-10-26</created><authors><author><keyname>Steinby</keyname><forenames>Magnus</forenames></author><author><keyname>Jurvanen</keyname><forenames>Eija</forenames></author><author><keyname>Cano</keyname><forenames>Antonio</forenames></author></authors><title>Varieties of Unranked Tree Languages</title><categories>cs.FL</categories><msc-class>68Q70, 68Q45, 08A70</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study varieties that contain unranked tree languages over all alphabets.
Trees are labeled with symbols from two alphabets, an unranked operator
alphabet and an alphabet used for leaves only. Syntactic algebras of unranked
tree languages are defined similarly as for ranked tree languages, and an
unranked tree language is shown to be recognizable iff its syntactic algebra is
regular, i.e., a finite unranked algebra in which the operations are defined by
regular languages over its set of elements. We establish a bijective
correspondence between varieties of unranked tree languages and varieties of
regular algebras. For this, we develop a basic theory of unranked algebras in
which algebras over all operator alphabets are considered together. Finally, we
show that the natural unranked counterparts of several general varieties of
ranked tree languages form varieties in our sense.
  This work parallels closely the theory of general varieties of ranked tree
languages and general varieties of finite algebras, but many nontrivial
modifications are required. For example, principal varieties as the basic
building blocks of varieties of tree languages have to be replaced by what we
call quasi-principal varieties, and we device a general scheme for defining
these by certain systems of congruences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07410</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07410</id><created>2015-10-26</created><updated>2015-11-16</updated><authors><author><keyname>Arjmandi</keyname><forenames>Hamidreza</forenames></author><author><keyname>Ahmadzadeh</keyname><forenames>Arman</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Kenari</keyname><forenames>Masoumeh Nasiri</forenames></author></authors><title>A Bio-Synthetic Modulator Model for Diffusion-based Molecular
  Communications</title><categories>cs.ET cs.IT math.IT</categories><comments>This paper is an extended version of a paper submitted to IEEE ICC
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In diffusion-based molecular communication (DMC), one important functionality
of a transmitter nano-machine is signal modulation. In particular, the
transmitter has to be able to control the release of signaling molecules for
modulation of the information bits. An important class of control mechanisms in
natural cells for releasing molecules is based on ion channels which are
pore-forming proteins across the cell membrane whose opening and closing may be
controlled by a gating parameter. In this paper, a modulator for DMC based on
ion channels is proposed which controls the rate at which molecules are
released from the transmitter by modulating a gating parameter signal.
Exploiting the capabilities of the proposed modulator, an on-off keying
modulation scheme is introduced and the corresponding average modulated signal,
i.e., the average release rate of the molecules from the transmitter, is
derived in the Laplace domain. By making a simplifying assumption, a
closed-form expression for the average modulated signal in the time domain is
obtained which constitutes an upper bound on the total number of released
molecules regardless of this assumption. The derived average modulated signal
is compared to results obtained with a particle based simulator. The numerical
results show that the derived upper bound is tight if the number of ion
channels distributed across the transmitter (cell) membrane is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07420</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07420</id><created>2015-10-26</created><authors><author><keyname>Tanburn</keyname><forenames>Richard</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Lunt</keyname><forenames>Oliver</forenames><affiliation>Oxford University</affiliation></author><author><keyname>Dattani</keyname><forenames>Nikesh S.</forenames><affiliation>Kyoto University</affiliation></author></authors><title>Crushing runtimes in adiabatic quantum computation with Energy Landscape
  Manipulation (ELM): Application to Quantum Factoring</title><categories>quant-ph cs.DM cs.DS math.NT</categories><comments>Feedback Encouraged</comments><msc-class>05C50, 11A41, 11A51, 11N35, 11N36, 11N80, 11Y05, 65K10, 65P10,
  65Y20, 68Q12, 81P68, 81P94, 94A60, 81-08</msc-class><acm-class>B.2.4; B.8.2; C.1.3; C.1.m; F.2.1; F.2.3; F.4.1; G.1.0; G.1.3;
  G.1.5; G.1.6; G.2.0; G.2.1; I.1.2; I.6.4; C.4; E.3; G.0; J.2; K.2</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We introduce two methods for speeding up adiabatic quantum computations by
increasing the energy between the ground and first excited states. Our methods
are even more general. They can be used to shift a Hamiltonian's density of
states away from the ground state, so that fewer states occupy the low-lying
energies near the minimum, hence allowing for faster adiabatic passages to find
the ground state with less risk of getting caught in an undesired low-lying
excited state during the passage. Even more generally, our methods can be used
to transform a discrete optimization problem into a new one whose unique
minimum still encodes the desired answer, but with the objective function's
values forming a different landscape. Aspects of the landscape such as the
objective function's range, or the values of certain coefficients, or how many
different inputs lead to a given output value, can be decreased *or* increased.
One of the many examples for which these methods are useful is in finding the
ground state of a Hamiltonian using NMR: If it is difficult to find a molecule
such that the distances between the spins match the interactions in the
Hamiltonian, the interactions in the Hamiltonian can be changed without at all
changing the ground state. We apply our methods to an AQC algorithm for integer
factorization, and the first method reduces the maximum runtime in our example
by up to 754%, and the second method reduces the maximum runtime of another
example by up to 250%. These two methods may also be combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07424</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07424</id><created>2015-10-26</created><authors><author><keyname>Brandl</keyname><forenames>Florian</forenames></author><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Suksompong</keyname><forenames>Warut</forenames></author></authors><title>The Impossibility of Extending Random Dictatorship to Weak Preferences</title><categories>cs.MA cs.GT</categories><acm-class>C.6; D.7; D.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random dictatorship has been characterized as the only social decision scheme
that satisfies efficiency and strategyproofness when individual preferences are
strict. We show that no extension of random dictatorship to weak preferences
satisfies these properties, even when significantly weakening the required
degree of strategyproofness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07439</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07439</id><created>2015-10-26</created><authors><author><keyname>Tripathy</keyname><forenames>Abinash</forenames></author><author><keyname>Rath</keyname><forenames>Santanu Kumar</forenames></author></authors><title>Object Oriented Analysis using Natural Language Processing concepts: A
  Review</title><categories>cs.SE cs.CL</categories><comments>12 pages, International Journal of Information Processing, 9(3),
  38-50, 2015, ISSN : 0973-8215, IK International Publishing House Pvt. Ltd.,
  New Delhi, India</comments><journal-ref>International Journal of Information Processing, vol. 9, no. 3,
  pp. 38-50, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Software Development Life Cycle (SDLC) starts with eliciting requirements
of the customers in the form of Software Requirement Specification (SRS). SRS
document needed for software development is mostly written in Natural
Language(NL) convenient for the client. From the SRS document only, the class
name, its attributes and the functions incorporated in the body of the class
are traced based on pre-knowledge of analyst. The paper intends to present a
review on Object Oriented (OO) analysis using Natural Language Processing (NLP)
techniques. This analysis can be manual where domain expert helps to generate
the required diagram or automated system, where the system generates the
required diagram, from the input in the form of SRS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07456</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07456</id><created>2015-10-26</created><authors><author><keyname>Brands</keyname><forenames>G.</forenames></author><author><keyname>Roellgen</keyname><forenames>C. B.</forenames></author><author><keyname>Vogel</keyname><forenames>K. U.</forenames></author></authors><title>QRKE: Quantum-Resistant Public Key Exchange</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Post-Quantum Key Exchange is needed since the availability of quantum
computers that allegedly allow breaking classical algorithms like
Diffie-Hellman, El Gamal, RSA and others within a practical amount of time is
broadly assumed in literature. Although our survey suggests that practical
quantum computers appear to be by far less advanced as actually required to
break state-of-the-art key negotiation algorithms, it is of high scientific
interest to develop fundamentally immune key negotiation methods. A novel
polymorphic algorithm based on permutable functions and defined over the field
of real numbers is proposed. The proposed key exchange can operate with at
least four different strategies. The cryptosystem itself is highly variable
and, due to the fact that rounding operations are inevitable and mandatory on a
traditional computer system, decoherence of the quantum computer system would
lead to a premature end of the computation on quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07461</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07461</id><created>2015-10-26</created><authors><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>Results on the solutions of maximum weighted Renyi entropy problems</title><categories>cs.IT math.IT math.PR</categories><comments>17 pages</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, following standard arguments, the maximum Renyi entropy
problem for the weighted case is analyzed. We verify that under some constrains
on weight function, the Student-r and Student-t distributions maximize the
weighted Renyi entropy. Furthermore, an extended version of the Hadamard
inequality is derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07462</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07462</id><created>2015-10-26</created><authors><author><keyname>Gelle</keyname><forenames>Kitti</forenames></author><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author></authors><title>Recognizing Union-Find trees is NP-complete</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disjoint-Set forests, consisting of Union-Find trees are data structures
having a widespread practical application due to their efficiency. Despite them
being well-known, no exact structural characterization of these trees is known
(such a characterization exists for Union trees which are constructed without
using path compression). In this paper we provide such a characterization and
show that the decision problem whether a given tree is a Union-Find tree is
$\NP$-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07471</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07471</id><created>2015-10-26</created><authors><author><keyname>Chen</keyname><forenames>Cheng</forenames></author><author><keyname>Liu</keyname><forenames>Shuang</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author><author><keyname>Li</keyname><forenames>Wu-Jun</forenames></author></authors><title>A Parallel algorithm for $\mathcal{X}$-Armed bandits</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The target of $\mathcal{X}$-armed bandit problem is to find the global
maximum of an unknown stochastic function $f$, given a finite budget of $n$
evaluations. Recently, $\mathcal{X}$-armed bandits have been widely used in
many situations. Many of these applications need to deal with large-scale data
sets. To deal with these large-scale data sets, we study a distributed setting
of $\mathcal{X}$-armed bandits, where $m$ players collaborate to find the
maximum of the unknown function. We develop a novel anytime distributed
$\mathcal{X}$-armed bandit algorithm. Compared with prior work on
$\mathcal{X}$-armed bandits, our algorithm uses a quite different searching
strategy so as to fit distributed learning scenarios. Our theoretical analysis
shows that our distributed algorithm is $m$ times faster than the classical
single-player algorithm. Moreover, the number of communication rounds of our
algorithm is only logarithmic in $mn$. The numerical results show that our
method can make effective use of every players to minimize the loss. Thus, our
distributed approach is attractive and useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07474</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07474</id><created>2015-10-26</created><authors><author><keyname>G&#xf3;mez</keyname><forenames>Alexander</forenames></author><author><keyname>D&#xed;ez</keyname><forenames>German</forenames></author><author><keyname>Giraldo</keyname><forenames>Jhony</forenames></author><author><keyname>Salazar</keyname><forenames>Augusto</forenames></author><author><keyname>Daza</keyname><forenames>Juan M.</forenames></author></authors><title>A Markov Random Field and Active Contour Image Segmentation Model for
  Animal Spots Patterns</title><categories>cs.CV</categories><comments>11th International Symposium on Visual Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-intrusive biometrics of animals using images allows to analyze phenotypic
populations and individuals with patterns like stripes and spots without
affecting the studied subjects. However, non-intrusive biometrics demand a well
trained subject or the development of computer vision algorithms that ease the
identification task. In this work, an analysis of classic segmentation
approaches that require a supervised tuning of their parameters such as
threshold, adaptive threshold, histogram equalization, and saturation
correction is presented. In contrast, a general unsupervised algorithm using
Markov Random Fields (MRF) for segmentation of spots patterns is proposed.
Active contours are used to boost results using MRF output as seeds. As study
subject the Diploglossus millepunctatus lizard is used. The proposed method
achieved a maximum efficiency of $91.11\%$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07480</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07480</id><created>2015-10-26</created><authors><author><keyname>Olmos</keyname><forenames>Felipe</forenames></author><author><keyname>Kauffmann</keyname><forenames>Bruno</forenames></author></authors><title>An Inverse Problem Approach for Content Popularity Estimation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet increasingly focuses on content, as exemplified by the now
popular Information Centric Networking paradigm. This means, in particular,
that estimating content popularities becomes essential to manage and distribute
content pieces efficiently. In this paper, we show how to properly estimate
content popularities from a traffic trace.
  Specifically, we consider the problem of the popularity inference in order to
tune content-level performance models, e.g. caching models. In this context,
special care must be brought on the fact that an observer measures only the
flow of requests, which differs from the model parameters, though both
quantities are related by the model assumptions. Current studies, however,
ignore this difference and use the observed data as model parameters. In this
paper, we highlight the inverse problem that consists in determining parameters
so that the flow of requests is properly predicted by the model. We then show
how such an inverse problem can be solved using Maximum Likelihood Estimation.
Based on two large traces from the Orange network and two synthetic datasets,
we eventually quantify the importance of this inversion step for the
performance evaluation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07482</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07482</id><created>2015-10-26</created><updated>2016-01-06</updated><authors><author><keyname>Levi</keyname><forenames>Effi</forenames></author><author><keyname>Reichart</keyname><forenames>Roi</forenames></author><author><keyname>Rappoport</keyname><forenames>Ari</forenames></author></authors><title>Edge-Linear First-Order Dependency Parsing with Undirected Minimum
  Spanning Tree Inference</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The run time complexity of state-of-the-art inference algorithms in
graph-based dependency parsing is super-linear in the number of input words
(n). Recently, pruning algorithms for these models have shown to cut a large
portion of the graph edges, with minimal damage to the resulting parse trees.
Solving the inference problem in run time complexity determined solely by the
number of edges (m) is hence of obvious importance.
  We propose such an inference algorithm for first-order models, which encodes
the problem as a minimum spanning tree (MST) problem in an undirected graph.
This allows us to utilize state-of-the-art undirected MST algorithms whose run
time is O(m) at expectation and with a very high probability. A directed parse
tree is then inferred from the undirected MST and is subsequently improved with
respect to the directed parsing model through local greedy updates, both steps
running in O(n) time. In experiments with 18 languages, a variant of the
first-order MSTParser (McDonald et al., 2005b) that employs our algorithm
performs very similarly to the original parser that runs an O(n^2) directed MST
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07487</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07487</id><created>2015-10-26</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Lairez</keyname><forenames>Pierre</forenames></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames></author></authors><title>Multiple binomial sums</title><categories>cs.SC math.CO</categories><msc-class>05A10, 33F10, 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple binomial sums form a large class of multi-indexed sequences, closed
under partial summation, which contains most of the sequences obtained by
multiple summation of binomial coefficients and also all the sequences with
algebraic generating function. We study the representation of the generating
functions of binomial sums by integrals of rational functions. The outcome is
twofold. Firstly, we show that a univariate sequence is a multiple binomial sum
if and only if its generating function is the diagonal of a rational function.
Secondly we propose algorithms that decide the equality of multiple binomial
sums and that compute recurrence relations for them. In conjunction with
geometric simplifications of the integral representations, this approach
behaves well in practice. The process avoids the computation of certificates
and the problem of accurate summation that afflicts discrete creative
telescoping, both in theory and in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07493</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07493</id><created>2015-10-26</created><authors><author><keyname>Babenko</keyname><forenames>Artem</forenames></author><author><keyname>Lempitsky</keyname><forenames>Victor</forenames></author></authors><title>Aggregating Deep Convolutional Features for Image Retrieval</title><categories>cs.CV</categories><comments>accepted for ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent works have shown that image descriptors produced by deep
convolutional neural networks provide state-of-the-art performance for image
classification and retrieval problems. It has also been shown that the
activations from the convolutional layers can be interpreted as local features
describing particular image regions. These local features can be aggregated
using aggregation approaches developed for local features (e.g. Fisher
vectors), thus providing new powerful global descriptors.
  In this paper we investigate possible ways to aggregate local deep features
to produce compact global descriptors for image retrieval. First, we show that
deep features and traditional hand-engineered features have quite different
distributions of pairwise similarities, hence existing aggregation methods have
to be carefully re-evaluated. Such re-evaluation reveals that in contrast to
shallow features, the simple aggregation method based on sum pooling provides
arguably the best performance for deep convolutional features. This method is
efficient, has few parameters, and bears little risk of overfitting when e.g.
learning the PCA matrix. Overall, the new compact global descriptor improves
the state-of-the-art on four common benchmarks considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07499</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07499</id><created>2015-10-23</created><updated>2016-01-22</updated><authors><author><keyname>Dureisseix</keyname><forenames>David</forenames><affiliation>INSA Lyon</affiliation></author></authors><title>An example of geometric origami design with benefit of graph enumeration
  algorithms</title><categories>cs.CG math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is concerned with an example of complex planar geometry arising
from flat origami challenges. The complexity of solution algorithms is
illustrated, depending on the depth of the initial analysis of the problem,
starting from brute force enumeration, up to the equivalence to a dedicated
problem in graph theory. This leads to algorithms starting from an untractable
case on modern computers, up to a run of few seconds on a portable personal
computer. This emphasizes the need for a prior analysis by humans before
considering the assistance of computers for complex design problems. The graph
problem is an enumeration of spanning trees from a grid graph, leading to a
coarse scale description of the topology of the paper edge on the flat-folded
state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07510</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07510</id><created>2015-10-26</created><authors><author><keyname>Xiong</keyname><forenames>Chenrong</forenames></author><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>A multi-mode area-efficient SCL polar decoder</title><categories>cs.IT math.IT</categories><comments>13 pages, 9 figures, submitted to TVLSI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are of great interest since they are the first provably
capacity-achieving forward error correction codes. To improve throughput and to
reduce decoding latency of polar decoders, maximum likelihood (ML) decoding
units are used by successive cancellation list (SCL) decoders as well as
successive cancellation (SC) decoders. This paper proposes an approximate ML
(AML) decoding unit for SCL decoders first. In particular, we investigate the
distribution of frozen bits of polar codes designed for both the binary erasure
and additive white Gaussian noise channels, and take advantage of the
distribution to reduce the complexity of the AML decoding unit, improving the
area efficiency of SCL decoders. Furthermore, a multi-mode SCL decoder with
variable list sizes and parallelism is proposed. If high throughput or small
latency is required, the decoder decodes multiple received codewords in
parallel with a small list size. However, if error performance is of higher
priority, the multi-mode decoder switches to a serial mode with a bigger list
size. Therefore, the multi-mode SCL decoder provides a flexible tradeoff
between latency, throughput and error performance, and adapts to different
throughput and latency requirements at the expense of small overhead. Hardware
implementation and synthesis results show that our polar decoders not only have
a better area efficiency but also easily adapt to different communication
channels and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07517</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07517</id><created>2015-10-26</created><authors><author><keyname>Sechelea</keyname><forenames>Andrei</forenames></author><author><keyname>Cheng</keyname><forenames>Samuel</forenames></author><author><keyname>Munteanu</keyname><forenames>Adrian</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author></authors><title>Binary Rate Distortion With Side Information: The Asymmetric Correlation
  Channel Case</title><categories>cs.IT math.IT</categories><comments>Accepted at IEEE Global Conference on Signal and Information
  Processing (GlobalSIP) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancing over up-to-date information theoretic results that assume symmetric
correlation models, in this work we consider the problem of lossy binary source
coding with side information, where the correlation is expressed by a generic
binary asymmetric channel. Specifically, we present an in-depth analysis of
rate distortion with side information available to both the encoder and decoder
(conventional predictive), as well as the Wyner-Ziv problem for this particular
setup. Prompted by our recent results for the Z-channel correlation case, we
evaluate the rate loss between the Wyner-Ziv and the conventional predictive
coding, as a function of the parameters of the binary asymmetric correlation
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07526</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07526</id><created>2015-10-26</created><updated>2015-11-20</updated><authors><author><keyname>Yu</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Hang</keyname><forenames>Chung-Wei</forenames></author><author><keyname>Xiang</keyname><forenames>Bing</forenames></author><author><keyname>Zhou</keyname><forenames>Bowen</forenames></author></authors><title>Empirical Study on Deep Learning Models for Question Answering</title><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore deep learning models with memory component or
attention mechanism for question answering task. We combine and compare three
models, Neural Machine Translation, Neural Turing Machine, and Memory Networks
for a simulated QA data set. This paper is the first one that uses Neural
Machine Translation and Neural Turing Machines for solving QA tasks. Our
results suggest that the combination of attention and memory have potential to
solve certain QA problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07545</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07545</id><created>2015-10-26</created><updated>2016-02-08</updated><authors><author><keyname>Schnabel</keyname><forenames>Tobias</forenames></author><author><keyname>Bennett</keyname><forenames>Paul N.</forenames></author><author><keyname>Dumais</keyname><forenames>Susan T.</forenames></author><author><keyname>Joachims</keyname><forenames>Thorsten</forenames></author></authors><title>Using Shortlists to Support Decision Making and Improve Recommender
  System Performance</title><categories>cs.HC cs.IR cs.LG</categories><comments>11 pages in WWW 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study shortlists as an interface component for recommender
systems with the dual goal of supporting the user's decision process, as well
as improving implicit feedback elicitation for increased recommendation
quality. A shortlist is a temporary list of candidates that the user is
currently considering, e.g., a list of a few movies the user is currently
considering for viewing. From a cognitive perspective, shortlists serve as
digital short-term memory where users can off-load the items under
consideration -- thereby decreasing their cognitive load. From a machine
learning perspective, adding items to the shortlist generates a new implicit
feedback signal as a by-product of exploration and decision making which can
improve recommendation quality. Shortlisting therefore provides additional data
for training recommendation systems without the increases in cognitive load
that requesting explicit feedback would incur.
  We perform an user study with a movie recommendation setup to compare
interfaces that offer shortlist support with those that do not. From the user
studies we conclude: (i) users make better decisions with a shortlist; (ii)
users prefer an interface with shortlist support; and (iii) the additional
implicit feedback from sessions with a shortlist improves the quality of
recommendations by nearly a factor of two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07546</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07546</id><created>2015-10-26</created><authors><author><keyname>Eaton</keyname><forenames>James</forenames></author><author><keyname>Naylor</keyname><forenames>Patrick A.</forenames></author></authors><title>Direct-to-Reverberant Ratio Estimation on the ACE Corpus Using a
  Two-channel Beamformer</title><categories>cs.SD</categories><comments>In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383). arXiv admin note: text overlap with
  arXiv:1510.01193</comments><report-no>ACEChallenge/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct-to-Reverberant Ratio (DRR) is an important measure for characterizing
the properties of a room. The recently proposed DRR Estimation using a
Null-Steered Beamformer (DENBE) algorithm was originally tested on simulated
data where noise was artificially added to the speech after convolution with
impulse responses simulated using the image-source method. This paper evaluates
the performance of this algorithm on speech convolved with measured impulse
responses and noise using the Acoustic Characterization of Environments (ACE)
Evaluation corpus. The fullband DRR estimation performance of the DENBE
algorithm exceeds that of the baselines in all Signal-to-Noise Ratios (SNRs)
and noise types. In addition, estimation of the DRR in one third-octave ISO
frequency bands is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07550</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07550</id><created>2015-10-26</created><authors><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Towards an Application-Aware Resource Scheduling with Carrier
  Aggregation in Cellular Systems</title><categories>cs.NI</categories><comments>Submitted to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an application-aware approach for resource block
scheduling with carrier aggregation in Long Term Evolution Advanced
(LTE-Advanced) cellular networks. In our approach, users are partitioned in
different groups based on the carriers coverage area. In each group of users,
users equipments (UE)s are assigned resource blocks (RB)s from all in band
carriers. We use a utility proportional fairness (PF) approach in the utility
percentage of the application running on the UE. Each user is guaranteed a
minimum quality of service (QoS) with a priority criterion that is based on the
type of application running on the UE. We prove that our scheduling policy
exists and therefore the optimal solution is tractable. Simulation results are
provided to compare the performance of the proposed RB scheduling approach with
other scheduling policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07563</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07563</id><created>2015-10-26</created><updated>2016-02-11</updated><authors><author><keyname>Shaik</keyname><forenames>Altaf</forenames></author><author><keyname>Borgaonkar</keyname><forenames>Ravishankar</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author><author><keyname>Niemi</keyname><forenames>Valtteri</forenames></author><author><keyname>Seifert</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Practical Attacks Against Privacy and Availability in 4G/LTE Mobile
  Communication Systems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile communication systems now constitute an essential part of life
throughout the world. Fourth generation &quot;Long Term Evolution&quot; (LTE) mobile
communication networks are being deployed. The LTE suite of specifications is
considered to be significantly better than its predecessors not only in terms
of functionality but also with respect to security and privacy for subscribers.
  We carefully analyzed LTE access network protocol specifications and
uncovered several vulnerabilities. Using commercial LTE mobile devices in real
LTE networks, we demonstrate inexpensive, and practical attacks exploiting
these vulnerabilities. Our first class of attacks consists of three different
ways of making an LTE device leak its location: A semi-passive attacker can
locate an LTE device within a 2 sq.km area within a city whereas an active
attacker can precisely locate an LTE device using GPS co-ordinates or
trilateration via cell-tower signal strength information. Our second class of
attacks can persistently deny some or all services to a target LTE device. To
the best of our knowledge, our work constitutes the first publicly reported
practical attacks against LTE access network protocols.
  We present several countermeasures to resist our specific attacks. We also
discuss possible trade-offs that may explain why these vulnerabilities exist
and recommend that safety margins introduced into future specifications to
address such trade-offs should incorporate greater agility to accommodate
subsequent changes in the trade-off equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07565</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07565</id><created>2015-10-26</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Goharshady</keyname><forenames>Amir Kafshdar</forenames></author><author><keyname>Ibsen-Jensen</keyname><forenames>Rasmus</forenames></author><author><keyname>Pavlogiannis</keyname><forenames>Andreas</forenames></author></authors><title>Algorithms for Algebraic Path Properties in Concurrent Systems of
  Constant Treewidth Components</title><categories>cs.PL cs.DS</categories><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study algorithmic questions for concurrent systems where the transitions
are labeled from a complete, closed semiring, and path properties are algebraic
with semiring operations. The algebraic path properties can model dataflow
analysis problems, the shortest path problem, and many other natural problems
that arise in program analysis. We consider that each component of the
concurrent system is a graph with constant treewidth, a property satisfied by
the controlflow graphs of most programs. We allow for multiple possible
queries, which arise naturally in demand driven dataflow analysis. The study of
multiple queries allows us to consider the tradeoff between the resource usage
of the one-time preprocessing and for each individual query. The traditional
approach constructs the product graph of all components and applies the
best-known graph algorithm on the product. In this approach, even the answer to
a single query requires the transitive closure, which provides no room for
tradeoff between preprocessing and query time.
  Our main contributions are algorithms that significantly improve the
worst-case running time of the traditional approach, and provide various
tradeoffs depending on the number of queries. For example, in a concurrent
system of two components, the traditional approach requires hexic time in the
worst case for answering one query as well as computing the transitive closure,
whereas we show that with one-time preprocessing in almost cubic time, each
subsequent query can be answered in at most linear time, and even the
transitive closure can be computed in almost quartic time. Furthermore, we
establish conditional optimality results showing that the worst-case running
time of our algorithms cannot be improved without achieving major breakthroughs
in graph algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07566</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07566</id><created>2015-10-26</created><authors><author><keyname>Formentin</keyname><forenames>Simone</forenames></author><author><keyname>Guanetti</keyname><forenames>Jacopo</forenames></author><author><keyname>Savaresi</keyname><forenames>Sergio M.</forenames></author></authors><title>Least costly energy management for series hybrid electric vehicles</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy management of plug-in Hybrid Electric Vehicles (HEVs) has different
challenges from non-plug-in HEVs, due to bigger batteries and grid recharging.
Instead of tackling it to pursue energetic efficiency, an approach minimizing
the driving cost incurred by the user - the combined costs of fuel, grid energy
and battery degradation - is here proposed. A real-time approximation of the
resulting optimal policy is then provided, as well as some analytic insight
into its dependence on the system parameters. The advantages of the proposed
formulation and the effectiveness of the real-time strategy are shown by means
of a thorough simulation campaign.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07573</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07573</id><created>2015-10-26</created><authors><author><keyname>Chalupka</keyname><forenames>Krzysztof</forenames></author><author><keyname>Dickinson</keyname><forenames>Michael</forenames></author><author><keyname>Perona</keyname><forenames>Pietro</forenames></author></authors><title>Generalized Regressive Motion: a Visual Cue to Collision</title><categories>cs.RO cs.CV cs.MA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brains and sensory systems evolved to guide motion. Central to this task is
controlling the approach to stationary obstacles and detecting moving
organisms. Looming has been proposed as the main monocular visual cue for
detecting the approach of other animals and avoiding collisions with stationary
obstacles. Elegant neural mechanisms for looming detection have been found in
the brain of insects and vertebrates. However, looming has not been analyzed in
the context of collisions between two moving animals. We propose an alternative
strategy, Generalized Regressive Motion (GRM), which is consistent with
recently observed behavior in fruit flies. Geometric analysis proves that GRM
is a reliable cue to collision among conspecifics, whereas agent-based modeling
suggests that GRM is a better cue than looming as a means to detect approach,
prevent collisions and maintain mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07586</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07586</id><created>2015-10-26</created><authors><author><keyname>Rao</keyname><forenames>Sudha</forenames></author><author><keyname>Vyas</keyname><forenames>Yogarshi</forenames></author><author><keyname>Daume</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Resnik</keyname><forenames>Philip</forenames></author></authors><title>Parser for Abstract Meaning Representation using Learning to Search</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a novel technique to parse English sentences into Abstract Meaning
Representation (AMR) using SEARN, a Learning to Search approach, by modeling
the concept and the relation learning in a unified framework. We evaluate our
parser on multiple datasets from varied domains and show an absolute
improvement of 2% to 6% over the state-of-the-art. Additionally we show that
using the most frequent concept gives us a baseline that is stronger than the
state-of-the-art for concept prediction. We plan to release our parser for
public use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07595</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07595</id><created>2015-10-26</created><authors><author><keyname>Lessard</keyname><forenames>Steven</forenames></author><author><keyname>Bruce</keyname><forenames>Jonathan</forenames></author><author><keyname>Jung</keyname><forenames>Erik</forenames></author><author><keyname>Teodorescu</keyname><forenames>Mircea</forenames></author><author><keyname>SunSpiral</keyname><forenames>Vytas</forenames></author><author><keyname>Agogino</keyname><forenames>Adrian</forenames></author></authors><title>A light-weight, multi-axis compliant tensegrity joint</title><categories>cs.RO</categories><comments>Initial submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a light-weight, multi- axis compliant tenegrity
joint that is biologically inspired by the human elbow. This tensegrity elbow
actuates by shortening and lengthening cable in a method inspired by muscular
actuation in a person. Unlike many series elastic actuators, this joint is
structurally compliant not just along each axis of rotation, but along other
axes as well. Compliant robotic joints are indispensable in unpredictable
environments, including ones where the robot must interface with a person. The
joint also addresses the need for functional redundancy and flexibility, traits
which are required for many applications that investigate the use of
biologically accurate robotic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07603</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07603</id><created>2015-10-26</created><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>PMU-Based Estimation of Dynamic State Jacobian Matrix</title><categories>cs.SY</categories><comments>submitted to IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a hybrid measurement and model-based method is proposed which
can estimate the dynamic state Jacobian matrix in near real-time. The proposed
method is computationally efficient and robust to the variation of network
topology. Since the estimated Jacobian matrix carries significant information
on system dynamics and states, it can be utilized in various applications. In
particular, two application of the estimated Jacobian matrix in online
oscillation analysis, stability monitoring and control are illustrated with
numerical examples. In addition, a side-product of the proposed method can
facilitate model validation by approximating the damping of generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07609</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07609</id><created>2015-10-26</created><authors><author><keyname>Wang</keyname><forenames>Joseph</forenames></author><author><keyname>Trapeznikov</keyname><forenames>Kirill</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Efficient Learning by Directed Acyclic Graph For Resource Constrained
  Prediction</title><categories>stat.ML cs.LG</categories><comments>To appear in NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of reducing test-time acquisition costs in
classification systems. Our goal is to learn decision rules that adaptively
select sensors for each example as necessary to make a confident prediction. We
model our system as a directed acyclic graph (DAG) where internal nodes
correspond to sensor subsets and decision functions at each node choose whether
to acquire a new sensor or classify using the available measurements. This
problem can be naturally posed as an empirical risk minimization over training
data. Rather than jointly optimizing such a highly coupled and non-convex
problem over all decision nodes, we propose an efficient algorithm motivated by
dynamic programming. We learn node policies in the DAG by reducing the global
objective to a series of cost sensitive learning problems. Our approach is
computationally efficient and has proven guarantees of convergence to the
optimal system for a fixed architecture. In addition, we present an extension
to map other budgeted learning problems with large number of sensors to our DAG
architecture and demonstrate empirical performance exceeding state-of-the-art
algorithms for data composed of both few and many sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07623</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07623</id><created>2015-10-26</created><authors><author><keyname>Nasir</keyname><forenames>Muhammad Anis Uddin</forenames></author><author><keyname>Morales</keyname><forenames>Gianmarco De Francisci</forenames></author><author><keyname>Garcia-Soriano</keyname><forenames>David</forenames></author><author><keyname>Kourtellis</keyname><forenames>Nicolas</forenames></author><author><keyname>Serafini</keyname><forenames>Marco</forenames></author></authors><title>Partial Key Grouping: Load-Balanced Partitioning of Distributed Streams</title><categories>cs.DC</categories><comments>14 pages. arXiv admin note: substantial text overlap with
  arXiv:1504.00788</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of load balancing in distributed stream processing
engines, which is exacerbated in the presence of skew. We introduce Partial Key
Grouping (PKG), a new stream partitioning scheme that adapts the classical
&quot;power of two choices&quot; to a distributed streaming setting by leveraging two
novel techniques: key splitting and local load estimation. In so doing, it
achieves better load balancing than key grouping while being more scalable than
shuffle grouping.
  We test PKG on several large datasets, both real-world and synthetic.
Compared to standard hashing, PKG reduces the load imbalance by up to several
orders of magnitude, and often achieves nearly-perfect load balance. This
result translates into an improvement of up to 175% in throughput and up to 45%
in latency when deployed on a real Storm cluster. PKG has been integrated in
Apache Storm v0.10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07641</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07641</id><created>2015-10-26</created><authors><author><keyname>Lipton</keyname><forenames>Zachary C.</forenames></author><author><keyname>Kale</keyname><forenames>David C.</forenames></author><author><keyname>Wetzell</keyname><forenames>Randall C.</forenames></author></authors><title>Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel application of LSTM recurrent neural networks to
multilabel classification of diagnoses given variable-length time series of
clinical measurements. Our method outperforms a strong baseline on a variety of
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07672</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07672</id><created>2015-10-26</created><authors><author><keyname>Rahman</keyname><forenames>Muhammad Mahboob Ur</forenames></author><author><keyname>Ghauch</keyname><forenames>Hadi</forenames></author><author><keyname>Imtiaz</keyname><forenames>Sahar</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author></authors><title>RRH clustering and transmit precoding for interference-limited 5G CRAN
  downlink</title><categories>cs.IT math.IT</categories><comments>7 pages, to be presented at IEEE Globecom 2015 workshops</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider cloud RAN architecture and focus on the downlink of
an antenna domain (AD) exposed to external interference from neighboring ADs.
With system sum-rate as performance metric, and assuming that perfect channel
state information is available at the aggregation node (AN), we implement i) a
greedy user association algorithm, and ii) a greedy remote radio-head (RRH)
clustering algorithm at the AN. We then vary the size of individual RRH
clusters, and evaluate and compare the sum-rate gains due to two distinct
transmit precoding schemes namely i) zero forcing beamforming (ZFBF), ii)
coordinated beamforming (CB), when exposed to external interference of same
kind. From system-level simulation results, we learn that in an
interference-limited regime: i) RRH clustering helps, i.e., {\it cost-adjusted}
performance when RRHs cooperate is superior to the performance when they don't,
ii) for transmit precoding, the CB scheme is to be preferred over the ZFBF
scheme. Finally, we discuss in detail the cost of RRH clustering, i.e., the
piloting overhead (and the elements driving it), incorporate its impact on
system sum-rate, and discuss its implications on the baseband processing
capabilities of the RRHs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07676</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07676</id><created>2015-10-26</created><authors><author><keyname>Kumar</keyname><forenames>Mithilesh</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author></authors><title>Faster Exact and Parameterized Algorithm for Feedback Vertex Set in
  Tournaments</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tournament is a directed graph T such that every pair of vertices are
connected by an arc. A feedback vertex set is a set S of vertices in T such
that T - S is acyclic. In this article we consider the Feedback Vertex Set
problem in tournaments. Here input is a tournament T and integer k, and the
task is to determine whether T has a feedback vertex set of size at most k. We
give a new algorithm for Feedback Vertex Set in Tournaments. The running time
of our algorithm is upper bounded by O(1.618^k + n^{O(1)}) and by O(1.46^n).
Thus our algorithm simultaneously improves over the fastest known parameterized
algorithm for the problem by Dom et al. running in time O(2^kk^{O(1)} +
n^{O(1)}), and the fastest known exact exponential time algorithm by Gaspers
and Mnich with running time O(1.674^n).
  On the way to prove our main result we prove a new partitioning theorem for
undirected graphs. In particular we show that the vertices of any undirected
m-edge graph of maximum degree d can be colored white or black in such a way
that for each of the two colors, the number of edges with both endpoints of
that color is between m/4-d/2 and m/4+d/4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07703</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07703</id><created>2015-10-26</created><authors><author><keyname>Han</keyname><forenames>Shengqian</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author><author><keyname>Chen</keyname><forenames>Pan</forenames></author></authors><title>Full Duplex Assisted Inter-cell Interference Cancellation in
  Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>16 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies the suppression of cross-tier inter-cell interference (ICI)
generated by a macro base station (MBS) to pico user equipments (PUEs) in
heterogeneous networks (HetNets). Different from existing ICI avoidance schemes
such as enhanced ICI cancellation (eICIC) and coordinated beamforming, which
generally operate at the MBS, we propose a full duplex (FD) assisted ICI
cancellation (fICIC) scheme, which can operate at each pico BS (PBS)
individually and is transparent to the MBS. The basic idea of the fICIC is to
apply FD technique at the PBS such that the PBS can send the desired signals
and forward the listened cross-tier ICI simultaneously to PUEs. We first
consider the narrowband single-user case, where the MBS serves a single macro
UE and each PBS serves a single PUE. We obtain the closed-form solution of the
optimal fICIC scheme, and analyze its asymptotical performance in ICI-dominated
scenario. We then investigate the general narrowband multi-user case, where
both MBS and PBSs serve multiple UEs. We devise a low-complexity algorithm to
optimize the fICIC aimed at maximizing the downlink sum rate of the PUEs
subject to user fairness constraint. Finally, the generalization of the fICIC
to wideband systems is investigated. Simulations validate the analytical
results and demonstrate the advantages of the fICIC on mitigating cross-tier
ICI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07712</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07712</id><created>2015-10-26</created><authors><author><keyname>Yu</keyname><forenames>Haonan</forenames></author><author><keyname>Wang</keyname><forenames>Jiang</forenames></author><author><keyname>Huang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Video Paragraph Captioning using Hierarchical Recurrent Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach that exploits hierarchical Recurrent Neural Networks
(RNNs) to tackle the video captioning problem, i.e., generating one or multiple
sentences to describe a realistic video. Our hierarchical framework contains a
sentence generator and a paragraph generator. The sentence generator produces
one simple short sentence that describes a specific short video interval. It
exploits both temporal- and spatial-attention mechanisms to selectively focus
on visual elements during generation. The paragraph generator captures the
inter-sentence dependency by taking as input the sentential embedding produced
by the sentence generator, combining it with the paragraph history, and
outputting the new initial state for the sentence generator. We evaluate our
approach on two large-scale benchmark datasets: YouTubeClips and
TACoS-MultiLevel. The experiments demonstrate that our approach significantly
outperforms the current state-of-the-art methods with BLEU@4 scores 0.499 and
0.305 respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07713</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07713</id><created>2015-10-26</created><authors><author><keyname>Jaganathan</keyname><forenames>Kishore</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Phase Retrieval: An Overview of Recent Developments</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of phase retrieval is a classic one in optics and arises when one
is interested in recovering an unknown signal from the magnitude (intensity) of
its Fourier transform. While there have existed quite a few approaches to phase
retrieval, recent developments in compressed sensing and convex
optimization-based signal recovery have inspired a host of new ones. This work
presents an overview of these approaches.
  Since phase retrieval, by its very nature, is ill-posed, to make the problem
meaningful one needs to either assume prior structure on the signal (e.g.,
sparsity) or obtain additional measurements (e.g., masks, structured
illuminations). For both the cases, we review conditions for the
identifiability of the signal, as well as practical algorithms for signal
recovery. In particular, we demonstrate that it is possible to robustly and
efficiently identify an unknown signal solely from phaseless Fourier
measurements, a fact with potentially far-reaching implications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07714</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07714</id><created>2015-10-26</created><authors><author><keyname>Sadosky</keyname><forenames>Peter</forenames></author><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Price</keyname><forenames>Megan</forenames></author><author><keyname>Steorts</keyname><forenames>Rebecca C.</forenames></author></authors><title>Blocking Methods Applied to Casualty Records from the Syrian Conflict</title><categories>stat.AP cs.DB</categories><comments>25 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of death counts and associated standard errors is of great
importance in armed conflict such as the ongoing violence in Syria, as well as
historical conflicts in Guatemala, Per\'u, Colombia, Timor Leste, and Kosovo.
For example, statistical estimates of death counts were cited as important
evidence in the trial of General Efra\'in R\'ios Montt for acts of genocide in
Guatemala. Estimation relies on both record linkage and multiple systems
estimation. A key first step in this process is identifying ways to partition
the records such that they are computationally manageable. This step is
referred to as blocking and is a major challenge for the Syrian database since
it is sparse in the number of duplicate records and feature poor in its
attributes. As a consequence, we propose locality sensitive hashing (LSH)
methods to overcome these challenges. We demonstrate the computational
superiority and error rates of these methods by comparing our proposed approach
with others in the literature. We conclude with a discussion of many challenges
of merging LSH with record linkage to achieve an estimate of the number of
uniquely documented deaths in the Syrian conflict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07721</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07721</id><created>2015-10-26</created><authors><author><keyname>Kheirkhah</keyname><forenames>Morteza</forenames></author><author><keyname>Wakeman</keyname><forenames>Ian</forenames></author><author><keyname>Parisis</keyname><forenames>George</forenames></author></authors><title>Multipath-TCP in ns-3</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present our work on designing and implementing an ns-3 model
for MultiPath TCP (MPTCP). Our MPTCP model closely follows MPTCP
specifications, as described in RFC 6824, and supports TCP NewReno loss
recovery on a per subflow basis. Subflow management is based on MPTCP's kernel
implementation. We briefly describe how we integrate our MPTCP model with ns-3
and present example simulation results to showcase its working state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07727</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07727</id><created>2015-10-26</created><updated>2016-01-31</updated><authors><author><keyname>Owen</keyname><forenames>Art B.</forenames></author></authors><title>Statistically efficient thinning of a Markov chain sampler</title><categories>stat.CO cs.LG stat.ML</categories><comments>10 pages</comments><msc-class>65C40, 62M05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is common to subsample Markov chain samples to reduce the storage burden
of the output. It is also well known that discarding $k-1$ out of every $k$
observations will not improve statistical efficiency. It is less frequently
remarked that subsampling a Markov chain allows one to omit some of the
computation beyond that needed to simply advance the chain. When this reduced
computation is accounted for, thinning the Markov chain by subsampling it can
improve statistical efficiency. Given an autocorrelation parameter $\rho$ and a
cost ratio $\theta$, this paper shows how to compute the most efficient
subsampling frequency $k$. The optimal $k$ grows rapidly as $\rho$ increases
towards $1$. The resulting efficiency gain depends primarily on $\theta$, not
$\rho$. Taking $k=1$ (no thinning) is optimal when $\rho\le0$. For $\rho&gt;0$ it
is optimal if and only if $\theta \le (1-\rho)^2/(2\rho)$. The efficiency gain
never exceeds $1+\theta$. The derivations are exact for an AR(1)
autocorrelation which is often a good approximation to the autocorrelations one
sees in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07728</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07728</id><created>2015-10-26</created><authors><author><keyname>Shirvanimoghaddam</keyname><forenames>Mahyar</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Lance</keyname><forenames>Andrew M.</forenames></author></authors><title>Design of Raptor Codes in the Low SNR Regime with Applications in
  Quantum Key Distribution</title><categories>cs.IT math.IT</categories><comments>The paper has been submitted to IEEE International Communications
  Conference (ICC), Kuala Lumpur, Malaysia, May 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this work is on the design of Raptor codes for continuous
variable Quantum key distribution (CV-QKD) systems. We design a highly
efficient Raptor code for very low signal to noise ratios (SNRs), which enables
CV-QKD systems to operate over long distances with a significantly higher
secret key rate compared to conventional fixed rate codes. The degree
distribution design of Raptor codes in the low SNR regime is formulated as a
linear program, where a set of optimized degree distributions are also obtained
through linear programming. Simulation results show that the designed code
achieves efficiencies higher than 94\% for SNRs as low as -20 dB and -30 dB. We
further propose a new error reconciliation protocol for CV-QKD systems by using
Raptor codes and show that it can achieve higher secret key rates over long
distances compared to existing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07735</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07735</id><created>2015-10-26</created><authors><author><keyname>Cai</keyname><forenames>Y.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Qin</keyname><forenames>B.</forenames></author><author><keyname>Champagne</keyname><forenames>B.</forenames></author><author><keyname>Zhao</keyname><forenames>M.</forenames></author></authors><title>Adaptive Reduced-Rank Minimum Symbol-Error-Rate Receive Processing for
  Large-Scale Multiple-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>16 pages, 13 figures, IEEE Transactions on Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel adaptive reduced-rank receive processing
strategy based on joint preprocessing, decimation and filtering (JPDF) for
large-scale multiple-antenna systems. In this scheme, a reduced-rank framework
is employed for linear receive processing and multiuser interference
suppression based on the minimization of the symbol-error-rate (SER) cost
function. We present a structure with multiple processing branches that
performs a dimensionality reduction, where each branch contains a group of
jointly optimized preprocessing and decimation units, followed by a linear
receive filter. We then develop stochastic gradient (SG) algorithms to compute
the parameters of the preprocessing and receive filters, along with a
low-complexity decimation technique for both binary phase shift keying (BPSK)
and $M$-ary quadrature amplitude modulation (QAM) symbols. In addition, an
automatic parameter selection scheme is proposed to further improve the
convergence performance of the proposed reduced-rank algorithms. Simulation
results are presented for time-varying wireless environments and show that the
proposed JPDF minimum-SER receive processing strategy and algorithms achieve a
superior performance than existing methods with a reduced computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07740</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07740</id><created>2015-10-26</created><updated>2015-11-11</updated><authors><author><keyname>Saremi</keyname><forenames>Saeed</forenames></author><author><keyname>Sejnowski</keyname><forenames>Terrence J.</forenames></author></authors><title>The Wilson Machine for Image Modeling</title><categories>stat.ML cond-mat.stat-mech cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning the distribution of natural images is one of the hardest and most
important problems in machine learning. The problem remains open, because the
enormous complexity of the structures in natural images spans all length
scales. We break down the complexity of the problem and show that the hierarchy
of structures in natural images fuels a new class of learning algorithms based
on the theory of critical phenomena and stochastic processes. We approach this
problem from the perspective of the theory of critical phenomena, which was
developed in condensed matter physics to address problems with infinite
length-scale fluctuations, and build a framework to integrate the criticality
of natural images into a learning algorithm. The problem is broken down by
mapping images into a hierarchy of binary images, called bitplanes. In this
representation, the top bitplane is critical, having fluctuations in structures
over a vast range of scales. The bitplanes below go through a gradual
stochastic heating process to disorder. We turn this representation into a
directed probabilistic graphical model, transforming the learning problem into
the unsupervised learning of the distribution of the critical bitplane and the
supervised learning of the conditional distributions for the remaining
bitplanes. We learnt the conditional distributions by logistic regression in a
convolutional architecture. Conditioned on the critical binary image, this
simple architecture can generate large, natural-looking images, with many
shades of gray, without the use of hidden units, unprecedented in the studies
of natural images. The framework presented here is a major step in bringing
criticality and stochastic processes to machine learning and in studying
natural image statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07748</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07748</id><created>2015-10-26</created><authors><author><keyname>Itti</keyname><forenames>Laurent</forenames></author><author><keyname>Borji</keyname><forenames>Ali</forenames></author></authors><title>Computational models: Bottom-up and top-down aspects</title><categories>cs.CV</categories><journal-ref>The Oxford Handbook of Attention, (A. C. Nobre, S. Kastner Ed.),
  pp. 1-20, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational models of visual attention have become popular over the past
decade, we believe primarily for two reasons: First, models make testable
predictions that can be explored by experimentalists as well as theoreticians,
second, models have practical and technological applications of interest to the
applied science and engineering communities. In this chapter, we take a
critical look at recent attention modeling efforts. We focus on {\em
computational models of attention} as defined by Tsotsos \&amp; Rothenstein
\shortcite{Tsotsos_Rothenstein11}: Models which can process any visual stimulus
(typically, an image or video clip), which can possibly also be given some task
definition, and which make predictions that can be compared to human or animal
behavioral or physiological responses elicited by the same stimulus and task.
Thus, we here place less emphasis on abstract models, phenomenological models,
purely data-driven fitting or extrapolation models, or models specifically
designed for a single task or for a restricted class of stimuli. For
theoretical models, we refer the reader to a number of previous reviews that
address attention theories and models more generally
\cite{Itti_Koch01nrn,Paletta_etal05,Frintrop_etal10,Rothenstein_Tsotsos08,Gottlieb_Balan10,Toet11,Borji_Itti12pami}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07749</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07749</id><created>2015-10-26</created><authors><author><keyname>Gai</keyname><forenames>Lei</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Tengjiao</forenames></author></authors><title>A partition-based Summary-Graph-Driven Method for Efficient RDF Query
  Processing</title><categories>cs.DB</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDF query optimization is a challenging problem. Although considerable
factors and their impacts on query efficiency have been investigated, this
problem still needs further investigation. We identify that decomposing query
into a series of light-weight operations is also effective in boosting query
processing. Considering the linked nature of RDF data, the correlations among
operations should be carefully handled. In this paper, we present SGDQ, a novel
framework that features a partition-based Summary Graph Driven Query for
efficient query processing. Basically, SGDQ partitions data and models
partitions as a summary graph. A query is decomposed into subqueries that can
be answered without inter-partition processing. The final results are derived
by perform summary graph matching and join the results generated by all matched
subqueries. In essence, SGDQ combines the merits of graph match processing and
relational join-based query implementation. It intentionally avoids maintain
huge intermediate results by organizing sub-query processing in a summary graph
driven fashion. Our extensive evaluations show that SGDQ is an effective
framework for efficient RDF query processing. Its query performance
consistently outperforms the representative state-of-the-art systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07758</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07758</id><created>2015-10-26</created><authors><author><keyname>Deng</keyname><forenames>Yun</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author></authors><title>Fast Compatibility Testing for Rooted Phylogenetic Trees</title><categories>cs.DS</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following basic problem in phylogenetic tree construction.
Let $\mathcal{P} = \{T_1, \ldots, T_k\}$ be a collection of rooted phylogenetic
trees over various subsets of a set of species. The tree compatibility problem
asks whether there is a tree $T$ with the following property: for each $i \in
\{1, \dots, k\}$, $T_i$ can be obtained from the restriction of $T$ to the
species set of $T_i$ by contracting zero or more edges. If such a tree $T$
exists, we say that $\mathcal{P}$ is compatible.
  We give a $\tilde{O}(M_\mathcal{P})$ algorithm for the tree compatibility
problem, where $M_\mathcal{P}$ is the total number of nodes and edges in
$\mathcal{P}$. Unlike previous algorithms for this problem, the running time of
our method does not depend on the degrees of the nodes in the input trees.
Thus, it is equally fast on highly resolved and highly unresolved trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07768</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07768</id><created>2015-10-27</created><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Bhaskara</keyname><forenames>Aditya</forenames></author><author><keyname>Lattanzi</keyname><forenames>Silvio</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author></authors><title>Expanders via Local Edge Flips</title><categories>cs.DS cs.DC cs.NI math.PR</categories><comments>To appear in the proceedings of the 27th ACM-SIAM Symposium on
  Discrete Algorithms (SODA) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing distributed and scalable algorithms to improve network connectivity
is a central topic in peer-to-peer networks. In this paper we focus on the
following well-known problem: given an $n$-node $d$-regular network for
$d=\Omega(\log n)$, we want to design a decentralized, local algorithm that
transforms the graph into one that has good connectivity properties (low
diameter, expansion, etc.) without affecting the sparsity of the graph. To this
end, Mahlmann and Schindelhauer introduced the random &quot;flip&quot; transformation,
where in each time step, a random pair of vertices that have an edge decide to
`swap a neighbor'. They conjectured that performing $O(n d)$ such flips at
random would convert any connected $d$-regular graph into a $d$-regular
expander graph, with high probability. However, the best known upper bound for
the number of steps is roughly $O(n^{17} d^{23})$, obtained via a delicate
Markov chain comparison argument.
  Our main result is to prove that a natural instantiation of the random flip
produces an expander in at most $O(n^2 d^2 \sqrt{\log n})$ steps, with high
probability. Our argument uses a potential-function analysis based on the
matrix exponential, together with the recent beautiful results on the
higher-order Cheeger inequality of graphs. We also show that our technique can
be used to analyze another well-studied random process known as the `random
switch', and show that it produces an expander in $O(n d)$ steps with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07773</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07773</id><created>2015-10-27</created><authors><author><keyname>Chen</keyname><forenames>Wenbin</forenames></author></authors><title>An O(log k log^2 n)-competitive Randomized Algorithm for the k-Sever
  Problem</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1410.4955</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that there is an O(log k log^2 n)-competitive
randomized algorithm for the k-sever problem on any metric space with n points,
which improved the previous best competitive ratio O(log^2 k log^3 n log log n)
by Nikhil Bansal et al. (FOCS 2011, pages 267-276).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07774</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07774</id><created>2015-10-27</created><authors><author><keyname>Girish</keyname><forenames>K V Vijay</forenames></author><author><keyname>Ananthapadmanabha</keyname><forenames>T V</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A G</forenames></author></authors><title>A dictionary learning and source recovery based approach to classify
  diverse audio sources</title><categories>cs.SD</categories><comments>5 pages, 5 figures</comments><acm-class>H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dictionary learning based audio source classification algorithm is proposed
to classify a sample audio signal as one amongst a finite set of different
audio sources. Cosine similarity measure is used to select the atoms during
dictionary learning. Based on three objective measures proposed, namely, signal
to distortion ratio (SDR), the number of non-zero weights and the sum of
weights, a frame-wise source classification accuracy of 98.2% is obtained for
twelve different sources. Cent percent accuracy has been obtained using moving
SDR accumulated over six successive frames for ten of the audio sources tested,
while the two other sources require accumulation of 10 and 14 frames.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07782</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07782</id><created>2015-10-27</created><authors><author><keyname>ShenTu</keyname><forenames>QingChun</forenames></author><author><keyname>Yu</keyname><forenames>JianPing</forenames></author></authors><title>Research on Anonymization and De-anonymization in the Bitcoin System</title><categories>cs.CR</categories><comments>14 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1509.06160</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bitcoin system is an anonymous, decentralized crypto-currency. There are
some deanonymizating techniques to cluster Bitcoin addresses and to map them to
users' identifications in the two research directions of Analysis of
Transaction Chain (ATC) and Analysis of Bitcoin Protocol and Network (ABPN).
Nowadays, there are also some anonymization methods such as coin-mixing and
transaction remote release (TRR) to cover the relationship between Bitcoin
address and the user. This paper studies anonymization and de-anonymization
technologies and proposes some directions for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07787</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07787</id><created>2015-10-27</created><authors><author><keyname>Yoshizoe</keyname><forenames>Kazuki</forenames></author><author><keyname>Terada</keyname><forenames>Aika</forenames></author><author><keyname>Tsuda</keyname><forenames>Koji</forenames></author></authors><title>Redesigning pattern mining algorithms for supercomputers</title><categories>cs.DC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Upcoming many core processors are expected to employ a distributed memory
architecture similar to currently available supercomputers, but parallel
pattern mining algorithms amenable to the architecture are not comprehensively
studied. We present a novel closed pattern mining algorithm with a
well-engineered communication protocol, and generalize it to find statistically
significant patterns from personal genome data. For distributing communication
evenly, it employs global load balancing with multiple stacks distributed on a
set of cores organized as a hypercube with random edges. Our algorithm achieved
up to 1175-fold speedup by using 1200 cores for solving a problem with 11,914
items and 697 transactions, while the naive approach of separating the search
space failed completely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07790</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07790</id><created>2015-10-27</created><authors><author><keyname>Sun</keyname><forenames>Fei</forenames></author><author><keyname>Turkoglu</keyname><forenames>Kamran</forenames></author></authors><title>Distributed Real-Time Non-Linear Receding Horizon Control Methodology
  for Multi-Agent Consensus Problems</title><categories>math.OC cs.SY nlin.CD</categories><comments>(submitted and under review in Applied Mathematics and Computation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the consensus problem for multi-agent nonlinear
systems through the distributed real-time nonlinear receding horizon control
methodology. With this work, we develop a scheme to reach the consensus for
nonlinear multi agent systems under fixed directed/undirected graph(s) without
the need of any linearization techniques. For this purpose, the problem of
consensus is converted into an optimization problem and is directly solved by
the backwards sweep Riccati method to generate the control protocol which
results in a non-iterative algorithm. Stability analysis is conducted to
provide convergence guarantees of proposed scheme. In addition, an extension to
the leader-following consensus of nonlinear multi-agent systems is presented.
Several examples are provided to validate and demonstrate the effectiveness of
the presented scheme and the corresponding theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07795</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07795</id><created>2015-10-27</created><authors><author><keyname>Goel</keyname><forenames>Ashima</forenames></author><author><keyname>Das</keyname><forenames>Debasis</forenames></author></authors><title>Improvised Broadcast Algorithm for Wireless Networks</title><categories>cs.NI</categories><comments>4 pages</comments><journal-ref>International Conference on Electrical, Electronics, Signals,
  Communication and Optimization (EESCO) - 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcasting problem is an important issue in the wireless networks,
especially in dynamic wireless networks. In dynamic wireless networks the node
density and mobility is high, due to several problems which arise during
broadcasting. Two major problems faced are namely, Broadcast Storm Problem and
Disconnected network problem. In a highly dense network, if information is
being flooded in a loop, it could lead to broadcast storm. The broadcast storm
may eventually crash the entire network and lead to loss of information.
Mobility of the nodes may lead to the problem of Disconnected Network. If the
two nodes sending and receiving information are mobile with different speeds,
it could lead to a disconnection between them as soon as the receiver moves out
of the communication range. In this paper, we are trying to solve both the
problems based on our proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07799</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07799</id><created>2015-10-27</created><authors><author><keyname>Rohe</keyname><forenames>Daniel</forenames></author></authors><title>Hierarchical Parallelisation of Functional Renormalisation Group
  Calculations -- hp-fRG</title><categories>physics.comp-ph cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The functional renormalisation group (fRG) has become a powerful and widely
used method to study correlated electron systems. This often involves a high
numerical effort, motivating the question in how far High Performance Computing
(HPC) platforms can leverage the approach. In this work we report on a
multi-level parallelisation of the underlying computational machinery and show
that this can speed up the code by several orders of magnitude. This in turn
can extend the applicability of the method to otherwise inaccessible cases. We
exploit three levels of parallelisation: Distributed computing by means of
Message Passing (MPI), shared-memory computing using OpenMP, and vectorisation
by means of SIMD units (single-instruction-multiple-data). Results are provided
for two distinct High Performance Computing (HPC) platforms, namely the
IBM-based BlueGene/Q system JUQUEEN and an Intel Sandy-Bridge-based development
cluster. We discuss how certain issues and obstacles were overcome in the
course of adapting the code. Most importantly, we conclude that this vast
improvement can actually be accomplished by introducing only moderate changes
to the code, such that this strategy may serve as a guideline for other
researcher to likewise improve the efficiency of their codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07819</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07819</id><created>2015-10-27</created><updated>2016-01-16</updated><authors><author><keyname>Wu</keyname><forenames>Zhihao</forenames></author><author><keyname>Lin</keyname><forenames>Youfang</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author><author><keyname>Gregory</keyname><forenames>Steve</forenames></author></authors><title>Link Prediction with Node Clustering Coefficient</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting missing links in incomplete complex networks efficiently and
accurately is still a challenging problem. The recently proposed CAR
(Cannistrai-Alanis-Ravai) index shows the power of local link/triangle
information in improving link-prediction accuracy. With the information of
level-2 links, which are links between common-neighbors, most classical
similarity indices can be improved. Nevertheless, calculating the number of
level-2 links makes CAR index not efficient enough. Inspired by the idea of
employing local link/triangle information, we propose a new similarity index
with more local structure information. In our method, local link/triangle
structure information can be conveyed by clustering coefficient of common
neighbors directly. The reason why clustering coefficient has good
effectiveness in estimating the contribution of a common-neighbor is because
that it employs links existing between neighbors of the common-neighbor and
these links have the same structural position with the candidate link to this
common-neighbor. Ten real-world networks drawn from five various fields are
used to test the performance of our method against to classical similarity
indices and recently proposed CAR index. Two estimators: precision and AUP, are
used to evaluate the accuracy of link prediction algorithms. Generally
speaking, our new index only performs competitively with CAR, but it is a good
complement to CAR for networks with not very high LCP-corr, which is a measure
to estimate the correlation between number of common-neighbors and number of
links between common-neighbors. Besides, the proposed index is also more
efficient than CAR index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07825</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07825</id><created>2015-10-27</created><authors><author><keyname>&#x100;ri&#x146;&#x161;</keyname><forenames>Agnis</forenames></author></authors><title>Span-program-based quantum algorithms for graph bipartiteness and
  connectivity</title><categories>quant-ph cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Span program is a linear-algebraic model of computation which can be used to
design quantum algorithms. For any Boolean function there exists a span program
that leads to a quantum algorithm with optimal quantum query complexity. In
general, finding such span programs is not an easy task. In this work, given a
query access to the adjacency matrix of a simple graph $G$ with $n$ vertices,
we provide two new span-program-based quantum algorithms: an algorithm for
testing if the graph is bipartite that uses $O(n\sqrt{n})$ quantum queries; an
algorithm for testing if the graph is connected that uses $O(n\sqrt{n})$
quantum queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07830</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07830</id><created>2015-10-27</created><authors><author><keyname>Jha</keyname><forenames>Tanya</forenames></author><author><keyname>Shetty</keyname><forenames>Rashmi</forenames></author></authors><title>Automation of Smartphone Traffic Generation in a Virtualized Environment</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable and comprehensive analysis of rapidly evolving mobile device
application traffic is extremely important but a challenging problem for the
Deep Packet Inspection (DPI) engines to perform effective policy management. We
present a test framework in which a test driver can automate/orchestrate
traffic generation by invoking appropriate method (intent) of real mobile
applications (as opposed to traffic replay) in regression or functional testing
of mobile application traffic analysis engines in a virtualized environment,
without real hardware. We demonstrate the concept by automating a real-time
Skype call through a DPI engine in a virtual test setup using Android VMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07839</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07839</id><created>2015-10-27</created><authors><author><keyname>Alrshaha</keyname><forenames>Mohamed A.</forenames></author><author><keyname>Othmana</keyname><forenames>Mohamed</forenames></author></authors><title>Performance evaluation of parallel TCP, and its impact on bandwidth
  utilization and fairness in high-BDP networks based on test-bed</title><categories>cs.NI</categories><comments>6 pages</comments><journal-ref>IEEE Malaysia International Conference on Communications (MICC),
  2013</journal-ref><doi>10.1109/MICC.2013.6805793</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  After the presence of high Bandwidth-Delay Product (high-BDP) networks, many
researches have been conducted to prove either the existing TCP variants can
achieve an excellent performance without wasting the bandwidth of these
networks or not. In this paper, a comparative test-bed experiment on a set of
high speed TCP variants has been conducted to show their differences in
bandwidth utilization, loss ratio and TCP-Fairness. The involved TCP Variants
in this experiment are: NewReno, STCP, HS-TCP, H-TCP and CUBIC. These TCP
variants have been examined in both cases of single and parallel schemes. The
core of this work is how to evaluate these TCP variants over a single
bottleneck network using a new parallel scheme to fully utilize the bandwidth
of this network, and to show the impact of accelerating these variants on
bandwidth utilization, loss-ratio and fairness. The results of this work reveal
that, first: the proposed parallel scheme strongly outperforms the single based
TCP in terms of bandwidth utilization and fairness. Second: CUBIC achieved
better performance than NewReno, STCP, H-TCP and HS-TCP in both cases of single
and parallel schemes. Briefly, parallel TCP scheme increases the utilization of
network resources, and it is relatively good in fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07851</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07851</id><created>2015-10-27</created><authors><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>ALPAGE, CMB</affiliation></author></authors><title>Standards for language resources in ISO -- Looking back at 13 fruitful
  years</title><categories>cs.CL</categories><comments>edition - die Terminologiefachzeitschrift, Deutscher Terminologie-Tag
  e.V. (DTT), 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an overview of the various projects carried out within
ISO committee TC 37/SC 4 dealing with the management of language (digital)
resources. On the basis of the technical experience gained in the committee and
the wider standardization landscape the paper identifies some possible trends
for the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07861</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07861</id><created>2015-10-27</created><authors><author><keyname>Shah</keyname><forenames>Ayush</forenames></author><author><keyname>Acharya</keyname><forenames>H. B.</forenames></author><author><keyname>Pal</keyname><forenames>Ambar</forenames></author></authors><title>Cells in the Internet of Things</title><categories>cs.NI</categories><comments>Short report on the Internet of Things and its structure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things combines various earlier areas of research. As a
result, research on the subject is still organized around these pre-existing
areas: distributed computing with services and objects, networks (usually
combining 6lowpan with Zigbee etc. for the last-hop), artificial intelligence
and semantic web, and human-computer interaction. We are yet to create a
unified model that covers all these perspectives - domain, device, service,
agent, etc. In this paper, we propose the concept of cells as units of
structure and context in the Internet of things. This allows us to have a
unified vocabulary to refer to single entities (whether dumb motes, intelligent
spimes, or virtual services), intranets of things, and finally the complete
Internet of things. The question that naturally follows, is what criteria we
choose to demarcate boundaries; we suggest various possible answers to this
question. We also mention how this concept ties into the existing visions and
protocols, and suggest how it may be used as the foundation of a formal model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07863</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07863</id><created>2015-10-27</created><authors><author><keyname>Mergel</keyname><forenames>Janine C.</forenames></author><author><keyname>Sauer</keyname><forenames>Roger A.</forenames></author><author><keyname>Ober-Bl&#xf6;baum</keyname><forenames>Sina</forenames></author></authors><title>C1-continuous space-time discretization based on Hamilton's law of
  varying action</title><categories>math.NA cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a class of C1-continuous time integration methods that are
applicable to conservative problems in elastodynamics. These formulations are
based on Hamilton's law of varying action. From the action integral of the
continuous system we derive a spatially and temporally weak form of the
governing equilibrium equations. This expression is first discretized in space,
considering standard finite elements. The resulting system is then discretized
in time, approximating the displacement by piecewise cubic Hermite shape
functions. Within the time domain we thus achieve C1-continuity for the
displacement field and C0-continuity for the velocity field. From the discrete
virtual action we finally construct a class of different one-step schemes.
These methods are examined both analytically and numerically. Here, we study
both linear and nonlinear systems, considering either inherently continuous or
discrete structures. In the numerical examples we focus on one-dimensional
applications. The provided theory, however, is general and valid also for
problems in 2D or 3D. We show that the most favorable candidate -- denoted as
pp-scheme -- converges with order four. Thus, especially if high accuracy of
the numerical solution is required, this scheme can be more efficient than
existing methods. It further exhibits similar properties as variational
integrators: Conservation of linear momentum and, for simple linear systems,
symplecticity. While it remains to be investigated whether our method is
symplectic for arbitrary systems, all our numerical results show an excellent
long-term energy behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07864</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07864</id><created>2015-10-27</created><authors><author><keyname>Ender</keyname><forenames>Thomas Marcel</forenames></author><author><keyname>Vananti</keyname><forenames>Patrick</forenames></author></authors><title>Dignit\'e - DIGital Network Information &amp; Traces Extraction</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web-based criminality like counterfeiting uses web applications which are
hosted on web servers. Those servers contain a lot of information which can be
used to identify the owner and other connected persons like hosters, shipping
partners, money mules and more. These pieces of information reveal insights on
the owner or provider of a fraud website, thus we can call them traces. These
traces can then be used by the police, law enforcement authorities or the legal
representatives of the victim. In our project 2 we had identified a vast range
of possible traces. We had also considered their information content and
existing limitations. During our Bachelor thesis, we have selected several
traces and started the implementation of the API with its underlying library.
After the successful implementation of the selected traces, we have created a
graphical user interface to allow the use of our solution without using a
command-line interface. To do so, we have learned to use the Scala Programming
Language and its integration with Java code. The graphical user interface of
our example application is built using Scala Swing, the Scala adoption of the
Swing Framework. The test cases are defined using ScalaTest with FlatSpec and
Matchers and executed using the JUnit Runner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07865</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07865</id><created>2015-10-27</created><updated>2015-11-02</updated><authors><author><keyname>Rao</keyname><forenames>Jun</forenames></author><author><keyname>Feng</keyname><forenames>Hao</forenames></author><author><keyname>Yang</keyname><forenames>Chenchen</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xia</keyname><forenames>Bin</forenames></author></authors><title>Optimal Caching Placement for D2D Assisted Wireless Caching Networks</title><categories>cs.IT math.IT</categories><comments>7 pages,8 figures. submit to ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we devise the optimal caching placement to maximize the
offloading probability for a two-tier wireless caching system, where the
helpers and a part of users have caching ability. The offloading comes from the
local caching, D2D sharing and the helper transmission. In particular, to
maximize the offloading probability we reformulate the caching placement
problem for users and helpers into a difference of convex (DC) problem which
can be effectively solved by DC programming. Moreover, we analyze the two
extreme cases where there is only help-tier caching network and only user-tier.
Specifically, the placement problem for the helper-tier caching network is
reduced to a convex problem, and can be effectively solved by the classical
water-filling method. We notice that users and helpers prefer to cache popular
contents under low node density and prefer to cache different contents evenly
under high node density. Simulation results indicate a great performance gain
of the proposed caching placement over existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07867</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07867</id><created>2015-10-27</created><authors><author><keyname>Rothe</keyname><forenames>Rasmus</forenames></author><author><keyname>Timofte</keyname><forenames>Radu</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Some like it hot - visual guidance for preference prediction</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For people first impressions of someone are of determining importance. They
are hard to alter through further information. This begs the question if a
computer can reach the same judgement. Earlier research has already pointed out
that age, gender, and average attractiveness can be estimated with reasonable
precision. We improve the state-of-the-art, but also predict - based on
someone's known preferences - how much that particular person is attracted to a
novel face. Our computational pipeline comprises a face detector, convolutional
neural networks for the extraction of deep features, standard support vector
regression for gender, age and facial beauty, and - as the main novelties -
visual regularized collaborative filtering to infer inter-person preferences as
well as a novel regression technique for handling visual queries without rating
history. We validate the method using a very large dataset from a dating site
as well as images from celebrities. Our experiments yield convincing results,
i.e. we predict 76% of the ratings correctly solely based on an image, and
reveal some sociologically relevant conclusions. We also validate our
collaborative filtering solution on the standard MovieLens rating dataset,
augmented with movie posters, to predict an individual's movie rating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07873</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07873</id><created>2015-10-27</created><authors><author><keyname>Tucci</keyname><forenames>Michele</forenames></author><author><keyname>Floriduz</keyname><forenames>Alessandro</forenames></author><author><keyname>Riverso</keyname><forenames>Stefano</forenames></author><author><keyname>Ferrari-Trecate</keyname><forenames>Giancarlo</forenames></author></authors><title>Kron reduction methods for plug-and-play control of ac islanded
  microgrids with arbitrary topology</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide an extension of the scalable algorithm proposed in
(Riverso et al., 2015) for the design of Plug-and-Play (PnP) controllers for AC
Islanded microGrids (ImGs). The method in (Riverso et al., 2015) assumes DGUs
are arranged in a load-connected topology, i.e. loads can appear only at the
output terminals of inverters. For handling totally general interconnections of
DGUs and loads, we describe an approach based on Kron Reduction (KR), a network
reduction method giving an equivalent load-connected model of the original ImG.
However, existing KR approaches can fail in preserving the structure of
transfer functions representing transmission lines. To avoid this drawback, we
introduce an approximate KR algorithm, still capable to represent exactly the
asymptotic periodic behavior of electric signals even if they are unbalanced.
Our results are backed up with simulations illustrating features of the new KR
approach as well as its use for designing PnP controllers in a 21-bus ImG
derived from an IEEE test feeder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07880</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07880</id><created>2015-10-27</created><authors><author><keyname>Wadhwa</keyname><forenames>Mohit</forenames></author><author><keyname>Pal</keyname><forenames>Ambar</forenames></author><author><keyname>Shah</keyname><forenames>Ayush</forenames></author><author><keyname>Mittal</keyname><forenames>Paritosh</forenames></author><author><keyname>Acharya</keyname><forenames>H. B.</forenames></author></authors><title>Rules in Play: On the Complexity of Routing Tables and Firewalls</title><categories>cs.NI</categories><comments>On the complexity of Firewalls and Routing Tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A fundamental component of networking infras- tructure is the policy, used in
routing tables and firewalls. Accordingly, there has been extensive study of
policies. However, the theory of such policies indicates that the size of the
decision tree for a policy is very large ( O((2n)d), where the policy has n
rules and examines d features of packets). If this was indeed the case, the
existing algorithms to detect anomalies, conflicts, and redundancies would not
be tractable for practical policies (say, n = 1000 and d = 10). In this paper,
we clear up this apparent paradox. Using the concept of 'rules in play', we
calculate the actual upper bound on the size of the decision tree, and
demonstrate how three other factors - narrow fields, singletons, and
all-matches make the problem tractable in practice. We also show how this
concept may be used to solve an open problem: pruning a policy to the minimum
possible number of rules, without changing its meaning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07888</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07888</id><created>2015-10-27</created><authors><author><keyname>Howard</keyname><forenames>J. V.</forenames></author></authors><title>Exchanging Goods Using Valuable Money</title><categories>q-fin.EC cs.GT</categories><comments>20 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A group of people wishes to use money to exchange goods efficiently over
several time periods. However, there are disadvantages to using any of the
goods as money, and in addition fiat money issued in the form of notes or coins
will be valueless in the final time period, and hence in all earlier periods.
Also, Walrasian market prices are determined only up to an arbitrary rescaling.
Nevertheless we show that it is possible to devise a system which uses money to
exchange goods and in which money has a determinate positive value. The
mechanism controls the flow rather than the stock of money: it introduces some
trading frictions, some redistribution of wealth, and some distortion of
prices, but these effects can all be made small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07889</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07889</id><created>2015-10-27</created><updated>2015-11-02</updated><authors><author><keyname>Shi</keyname><forenames>Peizhi</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author></authors><title>Learning Constructive Primitives for Online Level Generation and
  Real-time Content Adaptation in Super Mario Bros</title><categories>cs.AI</categories><comments>v1 is invalid because a wrong license was chosen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Procedural content generation (PCG) is of great interest to game design and
development as it generates game content automatically. Motivated by the recent
learning-based PCG framework and other existing PCG works, we propose an
alternative approach to online content generation and adaptation in Super Mario
Bros (SMB). Unlike most of existing works in SMB, our approach exploits the
synergy between rule-based and learning-based methods to produce constructive
primitives, quality yet controllable game segments in SMB. As a result, a
complete quality game level can be generated online by integrating relevant
constructive primitives via controllable parameters regarding geometrical
features and procedure-level properties. Also the adaptive content can be
generated in real time by dynamically selecting proper constructive primitives
via an adaptation criterion, e.g., dynamic difficulty adjustment (DDA). Our
approach is of several favorable properties in terms of content quality
assurance, generation efficiency and controllability. Extensive simulation
results demonstrate that the proposed approach can generate controllable yet
quality game levels online and adaptable content for DDA in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07898</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07898</id><created>2015-10-27</created><authors><author><keyname>Andrei</keyname><forenames>Oana</forenames></author><author><keyname>Calder</keyname><forenames>Muffy</forenames></author><author><keyname>Chalmers</keyname><forenames>Matthew</forenames></author><author><keyname>Morrison</keyname><forenames>Alistair</forenames></author><author><keyname>Rost</keyname><forenames>Mattias</forenames></author></authors><title>Probabilistic Formal Analysis of App Usage to Inform Redesign</title><categories>cs.SE cs.LO</categories><comments>13 pages, 6 figures, 7 tables, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper sets out a process of app analysis intended to support
understanding of use but also redesign. From usage logs we infer activity
patterns - Markov models - and employ probabilistic formal analysis to ask
questions about the use of the app. The core of this paper's contribution is a
bridging of stochastic and formal modelling, but we also describe the work to
make that analytic core utile within a design team. We illustrate our work via
a case study of a mobile app presenting analytic findings and discussing how
they are feeding into redesign. We had posited that two activity patterns
indicated two separable sets of users, each of which might benefit from a
differently tailored app version, but our subsequent analysis detailed users'
interleaving of activity patterns over time - evidence speaking more in favour
of redesign that supports each pattern in an integrated way. We uncover
patterns consisting of brief glances at particular data and recommend them as
possible candidates for new design work on widget extensions: small displays
available while users use other apps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07905</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07905</id><created>2015-10-25</created><authors><author><keyname>Brad</keyname><forenames>Raluca</forenames></author><author><keyname>Barac</keyname><forenames>Lavinia</forenames></author><author><keyname>Brad</keyname><forenames>Remus</forenames></author></authors><title>Defect Detection Techniques for Airbag Production Sewing Stages</title><categories>cs.CV</categories><journal-ref>Journal of Textiles, vol. 2014, Article ID 738504, 7 pages, 2014</journal-ref><doi>10.1155/2014/738504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Airbags are subject to strict quality control in order to ensure passengers
safety. The quality of fabric and sewing thread influence the final product and
therefore, sewing defects must be early and accurately detected, in order to
remove the item from production. Airbag seams assembly can take various forms,
using linear and circle primitives, with threads of different colors and length
densities, creating lockstitch or double threads chainstitch. The paper
presents a framework for the automatic detection of defects occurring during
the airbag sewing stage. Types of defects as skipped stitch, missed stitch or
superimposed seam for lockstitch and two threads chainstitch are detected and
marked. Using image processing methods, the proposed framework follows the
seams path and determines if a color pattern of the considered stitches is
valid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07917</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07917</id><created>2015-10-27</created><authors><author><keyname>Abuzainab</keyname><forenames>Nof</forenames></author><author><keyname>Touati</keyname><forenames>Corinne</forenames></author></authors><title>Multihop Relaying in Millimeter Wave Networks: A Proportionally Fair
  Cooperative Network Formation Game</title><categories>cs.GT cs.NI</categories><comments>IEEE. 82nd Vehicular Technology Conference: VTC2015-Fall, Sep 2015,
  Boston, United States</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave channels suffer from considerable degradation in the channel
quality when the signal is Non Line of Sight (NLOS) between the source and the
destination. Multihop relaying is thus anticipated to improve the communi-
cation between a source and its destination. This is achieved by transmitting
the signal to a sequence of relays in which a Line of Sight (LOS) signal exists
between two nodes along the path, or more generally when the signal is better
than the transmitted signal directly from the source to the destination. In
this paper, we consider a millimeter wave network composed of multiple source-
destination pairs and a set of deployed relays. We formulate the problem of
multihop relaying as a cooperative network formation game in which each relay
chooses which source-destination pair to assist in order to improve the
end-to-end performance, that is, the multihop delay between the source and the
destination. Further, we present an algorithm based on the Nash Bargaining
Solution to ensure fairness among the different source-destination pairs and
assess its efficiency on numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07925</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07925</id><created>2015-10-27</created><authors><author><keyname>Huang</keyname><forenames>Yijun</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author></authors><title>Exclusive Sparsity Norm Minimization with Random Groups via Cone
  Projection</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Many practical applications such as gene expression analysis, multi-task
learning, image recognition, signal processing, and medical data analysis
pursue a sparse solution for the feature selection purpose and particularly
favor the nonzeros \emph{evenly} distributed in different groups. The exclusive
sparsity norm has been widely used to serve to this purpose. However, it still
lacks systematical studies for exclusive sparsity norm optimization. This paper
offers two main contributions from the optimization perspective: 1) We provide
several efficient algorithms to solve exclusive sparsity norm minimization with
either smooth loss or hinge loss (non-smooth loss). All algorithms achieve the
optimal convergence rate $O(1/k^2)$ ($k$ is the iteration number). To the best
of our knowledge, this is the first time to guarantee such convergence rate for
the general exclusive sparsity norm minimization; 2) When the group information
is unavailable to define the exclusive sparsity norm, we propose to use the
random grouping scheme to construct groups and prove that if the number of
groups is appropriately chosen, the nonzeros (true features) would be grouped
in the ideal way with high probability. Empirical studies validate the
efficiency of proposed algorithms, and the effectiveness of random grouping
scheme on the proposed exclusive SVM formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07926</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07926</id><created>2015-10-27</created><updated>2016-02-18</updated><authors><author><keyname>Alekseyev</keyname><forenames>Max A.</forenames></author></authors><title>Weighted de Bruijn Graphs for the Menage Problem and Its Generalizations</title><categories>math.CO cs.DM</categories><msc-class>05A15, 05B20, 05B30, 44A10</msc-class><acm-class>F.2.1; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of enumeration of seating arrangements of married
couples around a circular table such that no spouses sit next to each other and
no k consecutive persons are of the same gender. While the case of k=2
corresponds to the classical probl\`eme des m\'enages with a well-studied
solution, no closed-form expression for the number of seating arrangements is
known when k&gt;=3.
  We propose a novel approach for this type of problems based on enumeration of
walks in certain algebraically weighted de Bruijn graphs. Our approach leads to
new expressions for the menage numbers and their exponential generating
function and allows one to efficiently compute the number of seating
arrangements in general cases, which we illustrate in details for the ternary
case of k=3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07930</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07930</id><created>2015-10-27</created><authors><author><keyname>Awan</keyname><forenames>Zohaib Hassan</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Interplay Between Delayed CSIT and Network Topology for Secure MISO BC</title><categories>cs.IT math.IT</categories><comments>Under submission. arXiv admin note: text overlap with
  arXiv:1503.06333</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of secure transmission over a Gaussian two-user
multi-input single-output (MISO) broadcast channel under the assumption that
links connecting the transmitter to the two receivers may have unequal strength
statistically. In addition to this, the state of the channel to each receiver
is conveyed in a strictly causal manner to the transmitter. We focus on a two
state topological setting of strong v.s. weak links. Under these assumptions,
we first consider the MISO wiretap channel and establish bounds on generalized
secure degrees of freedom (GSDoF). Next, we extend this model to the two-user
MISO broadcast channel and establish inner and outer bounds on GSDoF region
with different topology states. The encoding scheme sheds light on the usage of
both resources, i.e., topology of the model and strictly causal channel state
information at the transmitter (CSIT); and, allows digitization and
multi-casting of overheard side information, while transmitting confidential
message over the stronger link. Furthermore, for a special class of channels,
we show that the established bounds agree and so we characterize the sum GSDoF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07932</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07932</id><created>2015-10-27</created><authors><author><keyname>Thuc</keyname><forenames>Tran Kien</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author></authors><title>Downlink Power Control in Two-Tier Cellular Networks with
  Energy-Harvesting Small Cells as Stochastic Games</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Transactions on Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting in cellular networks is an emerging technique to enhance
the sustainability of power-constrained wireless devices. This paper considers
the co-channel deployment of a macrocell overlaid with small cells. The small
cell base stations (SBSs) harvest energy from environmental sources whereas the
macrocell base station (MBS) uses conventional power supply. Given a stochastic
energy arrival process for the SBSs, we derive a power control policy for the
downlink transmission of both MBS and SBSs such that they can achieve their
objectives (e.g., maintain the signal-to-interference-plus-noise ratio (SINR)
at an acceptable level) on a given transmission channel. We consider a
centralized energy harvesting mechanism for SBSs, i.e., there is a central
energy storage (CES) where energy is harvested and then distributed to the
SBSs. When the number of SBSs is small, the game between the CES and the MBS is
modeled as a single-controller stochastic game and the equilibrium policies are
obtained as a solution of a quadratic programming problem. However, when the
number of SBSs tends to infinity (i.e., a highly dense network), the
centralized scheme becomes infeasible, and therefore, we use a mean field
stochastic game to obtain a distributed power control policy for each SBS. By
solving a system of partial differential equations, we derive the power control
policy of SBSs given the knowledge of mean field distribution and the available
harvested energy levels in the batteries of the SBSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07945</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07945</id><created>2015-10-27</created><updated>2016-01-06</updated><authors><author><keyname>Nam</keyname><forenames>Hyeonseob</forenames></author><author><keyname>Han</keyname><forenames>Bohyung</forenames></author></authors><title>Learning Multi-Domain Convolutional Neural Networks for Visual Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel visual tracking algorithm based on the representations
from a discriminatively trained Convolutional Neural Network (CNN). Our
algorithm pretrains a CNN using a large set of videos with tracking
ground-truths to obtain a generic target representation. Our network is
composed of shared layers and multiple branches of domain-specific layers,
where domains correspond to individual training sequences and each branch is
responsible for binary classification to identify the target in each domain. We
train the network with respect to each domain iteratively to obtain generic
target representations in the shared layers. When tracking a target in a new
sequence, we construct a new network by combining the shared layers in the
pretrained CNN with a new binary classification layer, which is updated online.
Online tracking is performed by evaluating the candidate windows randomly
sampled around the previous target state. The proposed algorithm illustrates
outstanding performance compared with state-of-the-art methods in existing
tracking benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07949</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07949</id><created>2015-10-27</created><updated>2015-10-28</updated><authors><author><keyname>Zhang</keyname><forenames>Zhongzhi</forenames></author><author><keyname>Wu</keyname><forenames>Shunqi</forenames></author><author><keyname>Li</keyname><forenames>Mingyun</forenames></author><author><keyname>Comellas</keyname><forenames>Francesc</forenames></author></authors><title>The number and degree distribution of spanning trees in the Tower of
  Hanoi graph</title><categories>cs.DM math.CO</categories><comments>25 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of spanning trees of a graph is an important invariant related to
topological and dynamic properties of the graph, such as its reliability,
communication aspects, synchronization, and so on. However, the practical
enumeration of spanning trees and the study of their properties remain a
challenge, particularly for large networks. In this paper, we study the num-
ber and degree distribution of the spanning trees in the Hanoi graph. We first
establish recursion relations between the number of spanning trees and other
spanning subgraphs of the Hanoi graph, from which we find an exact analytical
expression for the number of spanning trees of the n-disc Hanoi graph. This
result allows the calculation of the spanning tree entropy which is then
compared with those for other graphs with the same average degree. Then, we
introduce a vertex labeling which allows to find, for each vertex of the graph,
its degree distribution among all possible spanning trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07954</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07954</id><created>2015-10-08</created><authors><author><keyname>Estrada</keyname><forenames>Ernesto</forenames></author><author><keyname>Benzi</keyname><forenames>Michele</forenames></author></authors><title>Core-satellite Graphs. Clustering, Assortativity and Spectral Properties</title><categories>cs.SI math.CO physics.soc-ph</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Core-satellite graphs (sometimes referred to as generalized friendship
graphs) are an interesting class of graphs that generalize many well known
types of graphs. In this paper we show that two popular clustering measures,
the average Watts-Strogatz clustering coefficient and the transitivity index,
diverge when the graph size increases. We also show that these graphs are
disassortative. In addition, we completely describe the spectrum of the
adjacency and Laplacian matrices associated with core-satellite graphs.
Finally, we introduce the class of generalized core-satellite graphs, and we
analyze the spectral properties of such graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07957</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07957</id><created>2015-10-27</created><authors><author><keyname>Lessin</keyname><forenames>Dan</forenames></author><author><keyname>Fussell</keyname><forenames>Don</forenames></author><author><keyname>Miikkulainen</keyname><forenames>Risto</forenames></author><author><keyname>Risi</keyname><forenames>Sebastian</forenames></author></authors><title>Increasing Behavioral Complexity for Evolved Virtual Creatures with the
  ESP Method</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since their introduction in 1994 (Sims), evolved virtual creatures (EVCs)
have employed the coevolution of morphology and control to produce high-impact
work in multiple fields, including graphics, evolutionary computation,
robotics, and artificial life. However, in contrast to fixed-morphology
creatures, there has been no clear increase in the behavioral complexity of
EVCs in those two decades. This paper describes a method for moving beyond this
limit, making use of high-level human input in the form of a syllabus of
intermediate learning tasks--along with mechanisms for preservation, reuse, and
combination of previously learned tasks. This method--named ESP for its three
components: encapsulation, syllabus, and pandemonium--is presented in two
complementary versions: Fast ESP, which constrains later morphological changes
to achieve linear growth in computation time as behavioral complexity is added,
and General ESP, which allows this restriction to be removed when sufficient
computational resources are available. Experiments demonstrate that the ESP
method allows evolved virtual creatures to reach new levels of behavioral
complexity in the co-evolution of morphology and control, approximately
doubling the previous state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07970</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07970</id><created>2015-10-27</created><authors><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>An Application-Aware Spectrum Sharing Approach for Commercial Use of 3.5
  GHz Spectrum</title><categories>cs.NI</categories><comments>Submitted to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce an application-aware spectrum sharing approach
for sharing the Federal under-utilized 3.5 GHz spectrum with commercial users.
In our model, users are running elastic or inelastic traffic and each
application running on the user equipment (UE) is assigned a utility function
based on its type. Furthermore, each of the small cells users has a minimum
required target utility for its application. In order for users located under
the coverage area of the small cells' eNodeBs, with the 3.5 GHz band resources,
to meet their minimum required quality of experience (QoE), the network
operator makes a decision regarding the need for sharing the macro cell's
resources to obtain additional resources. Our objective is to provide each user
with a rate that satisfies its application's minimum required utility through
spectrum sharing approach and improve the overall QoE in the network. We
present an application-aware spectrum sharing algorithm that is based on
resource allocation with carrier aggregation to allocate macro cell permanent
resources and small cells' leased resources to UEs and allocate each user's
application an aggregated rate that can at minimum achieves the application's
minimum required utility. Finally, we present simulation results for the
performance of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07971</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07971</id><created>2015-10-27</created><authors><author><keyname>Bergamini</keyname><forenames>Elisabetta</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>Approximating Betweenness Centrality in Fully-dynamic Networks</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1504.07091,
  arXiv:1409.6241</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Betweenness is a well-known centrality measure that ranks the nodes of a
network according to their participation in shortest paths. Since an exact
computation is prohibitive in large networks, several approximation algorithms
have been proposed. Besides that, recent years have seen the publication of
dynamic algorithms for efficient recomputation of betweenness in networks that
change over time. In this paper we propose the first betweenness centrality
approximation algorithms with a provable guarantee on the maximum approximation
error for dynamic networks. Several new intermediate algorithmic results
contribute to the respective approximation algorithms: (i) new upper bounds on
the vertex diameter, (ii) the first fully-dynamic algorithm for updating an
approximation of the vertex diameter in undirected graphs, and (iii) an
algorithm with lower time complexity for updating single-source shortest paths
in unweighted graphs after a batch of edge actions. Using approximation, our
algorithms are the first to make in-memory computation of betweenness in
dynamic networks with millions of edges feasible. Our experiments show that our
algorithms can achieve substantial speedups compared to recomputation, up to
several orders of magnitude. Moreover, the approximation accuracy is usually
significantly better than the theoretical guarantee in terms of absolute error.
More importantly, for reasonably small approximation error thresholds, the rank
of nodes is well preserved, in particular for nodes with high betweenness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07986</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07986</id><created>2015-10-27</created><updated>2015-11-12</updated><authors><author><keyname>Wu</keyname><forenames>Huaming</forenames></author><author><keyname>Seidenst&#xfc;cker</keyname><forenames>Daniel</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Nieto</keyname><forenames>Carlos Mart&#xed;n</forenames></author><author><keyname>Knottenbelt</keyname><forenames>William</forenames></author><author><keyname>Wolter</keyname><forenames>Katinka</forenames></author></authors><title>A Novel Offloading Partitioning Algorithm in Mobile Cloud Computing</title><categories>cs.DC cs.PF</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.07995</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.07995</id><created>2015-10-27</created><authors><author><keyname>Dudka</keyname><forenames>Kamil</forenames></author><author><keyname>Hol&#xed;k</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Peringer</keyname><forenames>Petr</forenames></author><author><keyname>Trt&#xed;k</keyname><forenames>Marek</forenames></author><author><keyname>Vojnar</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>From Low-Level Pointers to High-Level Containers</title><categories>cs.PL</categories><comments>An extended version of a VMCAI'16 paper</comments><report-no>FIT BUT Technical Report Series, Technical Report No. FIT-TR-2015-03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method that transforms a C program manipulating containers using
low-level pointer statements into an equivalent program where the containers
are manipulated via calls of standard high-level container operations like
push_back or pop_front. The input of our method is a C program annotated by a
special form of shape invariants which can be obtained from current automatic
shape analysers after a slight modification. The resulting program where the
low-level pointer statements are summarized into high-level container
operations is more understandable and (among other possible benefits) better
suitable for program analysis. We have implemented our approach and
successfully tested it through a number of experiments with list-based
containers, including experiments with simplification of program analysis by
separating shape analysis from analysing data-related properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08007</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08007</id><created>2015-10-27</created><updated>2016-02-17</updated><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author></authors><title>Location-Enhanced Authenticated Key Exchange</title><categories>cs.CR</categories><comments>This is an extended version of 2016 International Conference on
  Computing, Networking and Communications (ICNC 2016), Workshop on Computing,
  Networking and Communications (CNC), 2016, pp. 201-205</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce LOCATHE (Location-Enhanced Authenticated Key Exchange), a
generic protocol that pools location, user attributes, access policy and
desired services into a multi-factor authentication, allowing two peers to
establish a secure, encrypted session and perform mutual authentication with
pre-shared keys, passwords and other authentication factors. LOCATHE
contributes to: (1) forward secrecy through ephemeral session keys; (2)
security through zero-knowledge password proofs (ZKPP), such that no passwords
can be learned from the exchange; (3) the ability to use not only location, but
also multiple authentication factors from a user to a service; (4) providing a
two-tiered privacy authentication scheme, in which a user may be authenticated
either based on her attributes (hiding her unique identification), or with a
full individual authentication; (5) employing the expressiveness and
flexibility of Decentralized or Multi-Authority Ciphertext-Policy
Attribute-Based Encryption, allowing multiple service providers to control
their respective key generation and attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08012</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08012</id><created>2015-10-27</created><authors><author><keyname>Zhang</keyname><forenames>Guofeng</forenames></author><author><keyname>Liu</keyname><forenames>Haomin</forenames></author><author><keyname>Dong</keyname><forenames>Zilong</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author><author><keyname>Wong</keyname><forenames>Tien-Tsin</forenames></author><author><keyname>Bao</keyname><forenames>Hujun</forenames></author></authors><title>ENFT: Efficient Non-Consecutive Feature Tracking for Robust
  Structure-from-Motion</title><categories>cs.CV</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structure-from-motion (SfM) largely relies on the quality of feature
tracking. In image sequences, if disjointed tracks caused by objects moving in
and out of the view, occasional occlusion, or image noise, are not handled
well, the corresponding SfM could be significantly affected. This problem
becomes more serious for accurate SfM of large-scale scenes, which typically
requires to capture multiple sequences to cover the whole scene. In this paper,
we propose an efficient non-consecutive feature tracking (ENFT) framework to
match the interrupted tracks distributed in different subsequences or even in
different videos. Our framework consists of steps of solving the feature
`dropout' problem when indistinctive structures, noise or even large image
distortion exist, and of rapidly recognizing and joining common features
located in different subsequences. In addition, we contribute an effective
segment-based coarse-to-fine SfM estimation algorithm for efficiently and
robustly handling large datasets. Experimental results on several challenging
and large video datasets demonstrate the effectiveness of the proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08018</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08018</id><created>2015-10-27</created><authors><author><keyname>Khina</keyname><forenames>Anatoly</forenames></author><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>The Dirty MIMO Multiple-Access Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the scalar dirty multiple-access channel, in addition to Gaussian noise,
two additive interference signals are present, each known non-causally to a
single transmitter. It was shown by Philosof et al. that for strong
interferences, an i.i.d. ensemble of codes does not achieve the capacity
region. Rather, a structured-codes approach was presented, which was shown to
be optimal in the limit of high signal-to-noise ratios, where the sum-capacity
is dictated by the minimal (&quot;bottleneck&quot;) channel gain. In the present work, we
consider the multiple-input multiple-output (MIMO) variant of this setting. In
order to incorporate structured codes in this case, one can utilize matrix
decompositions, which transform the channel into effective parallel scalar
dirty multiple-access channels. This approach however suffers from a
&quot;bottleneck&quot; effect for each effective scalar channel and therefore the
achievable rates strongly depend on the chosen decomposition. It is shown that
a recently proposed decomposition, where the diagonals of the effective channel
matrices are equal up to a scaling factor, is optimal at high signal-to-noise
ratios, under an equal rank assumption. This approach is then extended to any
number of users. Finally, an application to physical-layer network coding for
the MIMO two-way relay channel is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08027</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08027</id><created>2015-10-27</created><authors><author><keyname>Li</keyname><forenames>Yuanjie</forenames></author><author><keyname>Deng</keyname><forenames>Haotian</forenames></author><author><keyname>Peng</keyname><forenames>Chunyi</forenames></author><author><keyname>Tu</keyname><forenames>Guan-Hua</forenames></author><author><keyname>Li</keyname><forenames>Jiayao</forenames></author><author><keyname>Yuan</keyname><forenames>Zengwen</forenames></author><author><keyname>Lu</keyname><forenames>Songwu</forenames></author></authors><title>iCellular: Define Your Own Cellular Network Access on Commodity
  Smartphones</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leveraging multi-carrier access offers a promising approach to boosting
access quality in mobile networks. However, our experiments show that the
potential benefits are hard to fulfill due to fundamental limitations in the
network-controlled design. To overcome these limitations, we propose iCellular,
which allows users to define and intelligently select their own cellular
network access from multiple carriers. iCellular reuses the existing
device-side mechanisms and the standard cellular network procedure, but
leverages the end device's intelligence to be proactive and adaptive in
multi-carrier selection. It performs adaptive monitoring to ensure responsive
selection and minimal service disruption, and enhances carrier selection with
online learning and runtime decision fault prevention. It is deployable on
commodity phones without any infrastructure/hardware change. We implement
iCellular on commodity Nexus 6 phones and leverage Google Project-Fi's efforts
to test multi-carrier access among two top US carriers: T-Mobile and Sprint.
Our experiments confirm that iCellular helps users with up to 3.74x throughput
improvement (7x suspension and 1.9x latency reduction) over the state-of-art
selection. Moreover, iCellular locates the best-quality carrier in most cases,
with negligible overhead on CPU, memory and energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08039</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08039</id><created>2015-10-27</created><authors><author><keyname>Poier</keyname><forenames>Georg</forenames></author><author><keyname>Roditakis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Schulter</keyname><forenames>Samuel</forenames></author><author><keyname>Michel</keyname><forenames>Damien</forenames></author><author><keyname>Bischof</keyname><forenames>Horst</forenames></author><author><keyname>Argyros</keyname><forenames>Antonis A.</forenames></author></authors><title>Hybrid One-Shot 3D Hand Pose Estimation by Exploiting Uncertainties</title><categories>cs.CV</categories><comments>BMVC 2015 (oral); see also
  http://lrs.icg.tugraz.at/research/hybridhape/</comments><acm-class>I.2.10; I.4.8; I.5.4</acm-class><doi>10.5244/C.29.182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based approaches to 3D hand tracking have been shown to perform well in
a wide range of scenarios. However, they require initialisation and cannot
recover easily from tracking failures that occur due to fast hand motions.
Data-driven approaches, on the other hand, can quickly deliver a solution, but
the results often suffer from lower accuracy or missing anatomical validity
compared to those obtained from model-based approaches. In this work we propose
a hybrid approach for hand pose estimation from a single depth image. First, a
learned regressor is employed to deliver multiple initial hypotheses for the 3D
position of each hand joint. Subsequently, the kinematic parameters of a 3D
hand model are found by deliberately exploiting the inherent uncertainty of the
inferred joint proposals. This way, the method provides anatomically valid and
accurate solutions without requiring manual initialisation or suffering from
track losses. Quantitative results on several standard datasets demonstrate
that the proposed method outperforms state-of-the-art representatives of the
model-based, data-driven and hybrid paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08079</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08079</id><created>2015-10-27</created><updated>2016-02-09</updated><authors><author><keyname>Rodionova</keyname><forenames>Alena</forenames></author><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Nickovic</keyname><forenames>Dejan</forenames></author><author><keyname>Grosu</keyname><forenames>Radu</forenames></author></authors><title>Temporal Logic as Filtering</title><categories>cs.LO</categories><comments>10 pages</comments><msc-class>03B44</msc-class><acm-class>F.4.1; D.3.1</acm-class><journal-ref>HSCC 2016, April 12 - 14, 2016, Vienna, Austria</journal-ref><doi>10.1145/2883817.2883839</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that metric temporal logic can be viewed as linear time-invariant
filtering, by interpreting addition, multiplication, and their neutral
elements, over the (max,min,0,1) idempotent dioid. Moreover, by interpreting
these operators over the field of reals (+,*,0,1), one can associate various
quantitative semantics to a metric-temporal-logic formula, depending on the
filter's kernel used: square, rounded-square, Gaussian, low-pass, band-pass, or
high-pass. This remarkable connection between filtering and metric temporal
logic allows us to freely navigate between the two, and to regard
signal-feature detection as logical inference. To the best of our knowledge,
this connection has not been established before. We prove that our qualitative,
filtering semantics is identical to the classical MTL semantics. We also
provide a quantitative semantics for MTL, which measures the normalized,
maximum number of times a formula is satisfied within its associated kernel, by
a given signal. We show that this semantics is sound, in the sense that, if its
measure is 0, then the formula is not satisfied, and it is satisfied otherwise.
We have implemented both of our semantics in Matlab, and illustrate their
properties on various formulas and signals, by plotting their computed
measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1510.08103</identifier>
 <datestamp>2015-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1510.08103</id><created>2015-10-27</created><authors><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author><author><keyname>Zhang</keyname><forenames>Simpson</forenames></author></authors><title>From Acquaintances to Friends: Homophily and Learning in Networks</title><categories>physics.soc-ph cs.SI q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the evolution of a network in a discrete time,
stochastic setting in which agents learn about each other through repeated
interactions and maintain/break links on the basis of what they learn from
these interactions. Agents have homophilous preferences and limited capacity,
so they maintain links with others who are learned to be similar to themselves
and cut links to others who are learned to be dissimilar to themselves. Thus
learning influences the evolution of the network, but learning is imperfect so
the evolution is stochastic. Homophily matters. Higher levels of homophily
decrease the (average) number of links that agents form. However, the effect of
homophily is anomalous: mutually beneficial links may be dropped before
learning is completed, thereby resulting in sparser networks and less
clustering than under complete information. There may be big differences
between the networks that emerge under complete and incomplete information.
Homophily matters here as well: initially, greater levels of homophily increase
the difference between the complete and incomplete information networks, but
sufficiently high levels of homophily eventually decrease the difference.
Complete and incomplete information networks differ the most when the degree of
homophily is intermediate. With multiple stages of life, the effects of
incomplete information are large initially but fade somewhat over time.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="85000" completeListSize="102538">1122234|86001</resumptionToken>
</ListRecords>
</OAI-PMH>
