<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:58:52Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|30001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4716</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4716</id><created>2012-03-21</created><updated>2012-03-23</updated><authors><author><keyname>Abel</keyname><forenames>Andreas</forenames><affiliation>Department of Computer Science, Ludwig-Maximilians-University Munich</affiliation></author><author><keyname>Scherer</keyname><forenames>Gabriel</forenames><affiliation>Department of Computer Science, Ludwig-Maximilians-University Munich</affiliation></author></authors><title>On Irrelevance and Algorithmic Equality in Predicative Type Theory</title><categories>cs.LO cs.PL</categories><comments>36 pages, superseds the FoSSaCS 2011 paper of the first author,
  titled &quot;Irrelevance in Type Theory with a Heterogeneous Equality Judgement&quot;</comments><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 27,
  2012) lmcs:1045</journal-ref><doi>10.2168/LMCS-8(1:29)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependently typed programs contain an excessive amount of static terms which
are necessary to please the type checker but irrelevant for computation. To
separate static and dynamic code, several static analyses and type systems have
been put forward. We consider Pfenning's type theory with irrelevant
quantification which is compatible with a type-based notion of equality that
respects eta-laws. We extend Pfenning's theory to universes and large
eliminations and develop its meta-theory. Subject reduction, normalization and
consistency are obtained by a Kripke model over the typed equality judgement.
Finally, a type-directed equality algorithm is described whose completeness is
proven by a second Kripke model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4725</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4725</id><created>2012-03-21</created><updated>2012-07-02</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Opthof</keyname><forenames>Tobias</forenames></author></authors><title>Citation Analysis with Medical Subject Headings (MeSH) using the Web of
  Knowledge: A new routine</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (2012, in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation analysis of documents retrieved from the Medline database (at the
Web of Knowledge) has been possible only on a case-by-case basis. A technique
is here developed for citation analysis in batch mode using both Medical
Subject Headings (MeSH) at the Web of Knowledge and the Science Citation Index
at the Web of Science. This freeware routine is applied to the case of &quot;Brugada
Syndrome,&quot; a specific disease and field of research (since 1992). The journals
containing these publications, for example, are attributed to Web-of-Science
Categories other than &quot;Cardiac and Cardiovascular Systems&quot;), perhaps because of
the possibility of genetic testing for this syndrome in the clinic. With this
routine, all the instruments available for citation analysis can now be used on
the basis of MeSH terms. Other options for crossing between Medline, WoS, and
Scopus are also reviewed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4732</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4732</id><created>2012-03-21</created><authors><author><keyname>Bonizzoni</keyname><forenames>Paola</forenames></author><author><keyname>Cameron</keyname><forenames>Peter J.</forenames></author><author><keyname>Della Vedova</keyname><forenames>Gianluca</forenames></author><author><keyname>Leporati</keyname><forenames>Alberto</forenames></author><author><keyname>Mauri</keyname><forenames>Giancarlo</forenames></author></authors><title>A Unifying Framework to Characterize the Power of a Language to Express
  Relations</title><categories>cs.DB</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this extended abstract we provide a unifying framework that can be used to
characterize and compare the expressive power of query languages for different
data base models. The framework is based upon the new idea of valid partition,
that is a partition of the elements of a given data base, where each class of
the partition is composed by elements that cannot be separated (distinguished)
according to some level of information contained in the data base. We describe
two applications of this new framework, first by deriving a new syntactic
characterization of the expressive power of relational algebra which is
equivalent to the one given by Paredaens, and subsequently by studying the
expressive power of a simple graph-based data model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4740</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4740</id><created>2012-03-21</created><updated>2012-09-17</updated><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Christiano</keyname><forenames>Paul</forenames></author></authors><title>Quantum Money from Hidden Subspaces</title><categories>quant-ph cs.CC</categories><comments>48 pages, minor corrections and improvements, journal version to
  appear in Theory of Computing; Proceedings of ACM STOC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forty years ago, Wiesner pointed out that quantum mechanics raises the
striking possibility of money that cannot be counterfeited according to the
laws of physics. We propose the first quantum money scheme that is (1)
public-key, meaning that anyone can verify a banknote as genuine, not only the
bank that printed it, and (2) cryptographically secure, under a &quot;classical&quot;
hardness assumption that has nothing to do with quantum money. Our scheme is
based on hidden subspaces, encoded as the zero-sets of random multivariate
polynomials. A main technical advance is to show that the &quot;black-box&quot; version
of our scheme, where the polynomials are replaced by classical oracles, is
unconditionally secure. Previously, such a result had only been known relative
to a quantum oracle (and even there, the proof was never published). Even in
Wiesner's original setting -- quantum money that can only be verified by the
bank -- we are able to use our techniques to patch a major security hole in
Wiesner's scheme. We give the first private-key quantum money scheme that
allows unlimited verifications and that remains unconditionally secure, even if
the counterfeiter can interact adaptively with the bank. Our money scheme is
simpler than previous public-key quantum money schemes, including a knot-based
scheme of Farhi et al. The verifier needs to perform only two tests, one in the
standard basis and one in the Hadamard basis -- matching the original intuition
for quantum money, based on the existence of complementary observables. Our
security proofs use a new variant of Ambainis's quantum adversary method, and
several other tools that might be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4745</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4745</id><created>2012-03-20</created><authors><author><keyname>Priem</keyname><forenames>Jason</forenames></author><author><keyname>Piwowar</keyname><forenames>Heather A.</forenames></author><author><keyname>Hemminger</keyname><forenames>Bradley M.</forenames></author></authors><title>Altmetrics in the wild: Using social media to explore scholarly impact</title><categories>cs.DL</categories><comments>5 tables, 13 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In growing numbers, scholars are integrating social media tools like blogs,
Twitter, and Mendeley into their professional communications. The online,
public nature of these tools exposes and reifies scholarly processes once
hidden and ephemeral. Metrics based on this activities could inform broader,
faster measures of impact, complementing traditional citation metrics. This
study explores the properties of these social media-based metrics or
&quot;altmetrics&quot;, sampling 24,331 articles published by the Public Library of
Science.
  We find that that different indicators vary greatly in activity. Around 5% of
sampled articles are cited in Wikipedia, while close to 80% have been included
in at least one Mendeley library. There is, however, an encouraging diversity;
a quarter of articles have nonzero data from five or more different sources.
Correlation and factor analysis suggest citation and altmetrics indicators
track related but distinct impacts, with neither able to describe the complete
picture of scholarly use alone. There are moderate correlations between
Mendeley and Web of Science citation, but many altmetric indicators seem to
measure impact mostly orthogonal to citation. Articles cluster in ways that
suggest five different impact &quot;flavors&quot;, capturing impacts of different types
on different audiences; for instance, some articles may be heavily read and
saved by scholars but seldom cited. Together, these findings encourage more
research into altmetrics as complements to traditional citation measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4746</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4746</id><created>2012-03-21</created><updated>2012-06-21</updated><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Sublinear Time, Approximate Model-based Sparse Recovery For All</title><categories>cs.IT math.IT</categories><comments>This paper has been drawn by the author due to a error in the
  derivation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a probabilistic, {\it sublinear} runtime, measurement-optimal
system for model-based sparse recovery problems through dimensionality
reducing, {\em dense} random matrices. Specifically, we obtain a linear sketch
$u\in \R^M$ of a vector $\bestsignal\in \R^N$ in high-dimensions through a
matrix $\Phi \in \R^{M\times N}$ $(M&lt;N)$. We assume this vector can be well
approximated by $K$ non-zero coefficients (i.e., it is $K$-sparse). In
addition, the nonzero coefficients of $\bestsignal$ can obey additional
structure constraints such as matroid, totally unimodular, or knapsack
constraints, which dub as model-based sparsity. We construct the dense
measurement matrix using a probabilistic method so that it satisfies the
so-called restricted isometry property in the $\ell_2$-norm. While recovery
using such matrices is measurement-optimal as they require the smallest sketch
sizes $\numsam= O(\sparsity \log(\dimension/\sparsity))$, the existing
algorithms require superlinear runtime $\Omega(N\log(N/K))$ with the exception
of Porat and Strauss, which requires $O(\beta^5\epsilon^{-3}K(N/K)^{1/\beta}),
~\beta \in \mathbb{Z}_{+}, $ but provides an $\ell_1/\ell_1$ approximation
guarantee. In contrast, our approach features $ O\big(\max \lbrace \sketch
\sparsity \log^{O(1)} \dimension, ~\sketch \sparsity^2 \log^2
(\dimension/\sparsity) \rbrace\big) $ complexity where $ L \in \mathbb{Z}_{+}$
is a design parameter, independent of $\dimension$, requires a smaller sketch
size, can accommodate model sparsity, and provides a stronger $\ell_2/\ell_1$
guarantee. Our system applies to &quot;for all&quot; sparse signals, is robust against
bounded perturbations in $u$ as well as perturbations on $\bestsignal$ itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4751</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4751</id><created>2012-03-21</created><updated>2015-10-13</updated><authors><author><keyname>Gramoli</keyname><forenames>Vincent</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>Optimism for Boosting Concurrency</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern concurrent programming benefits from a large variety of
synchronization techniques. These include conventional pessimistic locking, as
well as optimistic techniques based on conditional synchronization primitives
or transactional memory. Yet, it is unclear which of these approaches better
leverage the concurrency inherent to multi-cores.
  In this paper, we compare the level of concurrency one can obtain by
converting a sequential program into a concurrent one using optimistic or
pessimistic techniques. To establish fair comparison of such implementations,
we introduce a new correctness criterion for concurrent programs, defined
independently of the synchronization techniques they use.
  We treat a program's concurrency as its ability to accept a concurrent
schedule, a metric inspired by the theories of both databases and transactional
memory. We show that pessimistic locking can provide strictly higher
concurrency than transactions for some applications whereas transactions can
provide strictly higher concurrency than pessimistic locks for others. Finally,
we show that combining the benefits of the two synchronization techniques can
provide strictly more concurrency than any of them individually. We propose a
list-based set algorithm that is optimal in the sense that it accepts all
correct concurrent schedules. As we show via experimentation, the optimality in
terms of concurrency is reflected by scalability gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4754</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4754</id><created>2012-03-21</created><authors><author><keyname>Ghilezan</keyname><forenames>Silvia</forenames><affiliation>LIP</affiliation></author><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author><author><keyname>Zunic</keyname><forenames>Dragisa</forenames></author></authors><title>Computational interpretation of classical logic with explicit structural
  rules</title><categories>cs.LO cs.DC math.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a calculus providing a Curry-Howard correspondence to classical
logic represented in the sequent calculus with explicit structural rules,
namely weakening and contraction. These structural rules introduce explicit
erasure and duplication of terms, respectively. We present a type system for
which we prove the type-preservation under reduction. A mutual relation with
classical calculus featuring implicit structural rules has been studied in
detail. From this analysis we derive strong normalisation property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4756</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4756</id><created>2012-03-11</created><authors><author><keyname>Osherovich</keyname><forenames>Eliyahu</forenames></author></authors><title>Numerical methods for phase retrieval</title><categories>physics.optics astro-ph.IM cs.NA physics.data-an</categories><comments>PhD. Thesis</comments><report-no>PHD-2012-04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of reconstruction of a signal from the
magnitude of its Fourier transform, also known as phase retrieval. The problem
arises in many areas of astronomy, crystallography, optics, and coherent
diffraction imaging (CDI). Our main goal is to develop an efficient
reconstruction method based on continuous optimization techniques. Unlike
current reconstruction methods, which are based on alternating projections, our
approach leads to a much faster and more robust method. However, all previous
attempts to employ continuous optimization methods, such as Newton-type
algorithms, to the phase retrieval problem failed. In this work we provide an
explanation for this failure, and based on this explanation we devise a
sufficient condition that allows development of new reconstruction
methods---approximately known Fourier phase. We demonstrate that a rough (up to
$\pi/2$ radians) Fourier phase estimate practically guarantees successful
reconstruction by any reasonable method. We also present a new reconstruction
method whose reconstruction time is orders of magnitude faster than that of the
current method-of-choice in phase retrieval---Hybrid Input-Output (HIO).
Moreover, our method is capable of successful reconstruction even in the
situations where HIO is known to fail. We also extended our method to other
applications: Fourier domain holography, and interferometry. Additionally we
developed a new sparsity-based method for sub-wavelength CDI. Using this method
we demonstrated experimental resolution exceeding several times the physical
limit imposed by the diffraction light properties (so called diffraction
limit).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4757</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4757</id><created>2012-03-08</created><authors><author><keyname>Osherovich</keyname><forenames>Eliyahu</forenames></author><author><keyname>Cohen</keyname><forenames>Oren</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Segev</keyname><forenames>Mordechai</forenames></author></authors><title>Designing and using prior data in Ankylography: Recovering a 3D object
  from a single diffraction intensity pattern</title><categories>physics.optics astro-ph.IM cs.NA physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method for Ankylography: three-dimensional structure
reconstruction from a single shot diffraction intensity pattern. Our approach
allows reconstruction of objects containing many more details than was ever
demonstrated, in a faster and more accurate fashion
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4764</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4764</id><created>2012-03-21</created><authors><author><keyname>Hernaez</keyname><forenames>Mikel</forenames></author><author><keyname>Crespo</keyname><forenames>Pedro M.</forenames></author><author><keyname>Del Ser</keyname><forenames>Javier</forenames></author></authors><title>On the Design of a Novel Joint Network-Channel Coding Scheme for the
  Multiple Access Relay Channel</title><categories>cs.IT math.IT</categories><comments>28 pages, 9 figures; Submitted to IEEE Journal on Selected Areas in
  Communications - Special Issue on Theories and Methods for Advanced Wireless
  Relays, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel joint non-binary network-channel code for the
Time-Division Decode-and-Forward Multiple Access Relay Channel (TD-DF-MARC),
where the relay linearly combines -- over a non-binary finite field -- the
coded sequences from the source nodes. A method based on an EXIT chart analysis
is derived for selecting the best coefficients of the linear combination.
Moreover, it is shown that for different setups of the system, different
coefficients should be chosen in order to improve the performance. This
conclusion contrasts with previous works where a random selection was
considered. Monte Carlo simulations show that the proposed scheme outperforms,
in terms of its gap to the outage probabilities, the previously published joint
network-channel coding approaches. Besides, this gain is achieved by using very
short-length codewords, which makes the scheme particularly attractive for
low-latency applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4788</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4788</id><created>2012-03-21</created><authors><author><keyname>Brusan</keyname><forenames>Altay</forenames></author></authors><title>Very Short Literature Survey From Supervised Learning To Surrogate
  Modeling</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The past century was era of linear systems. Either systems (especially
industrial ones) were simple (quasi)linear or linear approximations were
accurate enough. In addition, just at the ending decades of the century
profusion of computing devices were available, before then due to lack of
computational resources it was not easy to evaluate available nonlinear system
studies. At the moment both these two conditions changed, systems are highly
complex and also pervasive amount of computation strength is cheap and easy to
achieve. For recent era, a new branch of supervised learning well known as
surrogate modeling (meta-modeling, surface modeling) has been devised which
aimed at answering new needs of modeling realm. This short literature survey is
on to introduce surrogate modeling to whom is familiar with the concepts of
supervised learning. Necessity, challenges and visions of the topic are
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4807</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4807</id><created>2012-03-21</created><authors><author><keyname>Silva</keyname><forenames>Filipi Nascimento</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco Aparecido</forenames></author><author><keyname>Junior</keyname><forenames>Osvaldo Novais de Oliveira</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Quantifying the interdisciplinarity of scientific journals and fields</title><categories>physics.soc-ph cs.DL physics.comp-ph physics.data-an</categories><comments>23 pages, 6 figures</comments><journal-ref>Journal of Informetrics. Volume 7, Issue 2, Pages 469-477, 2003</journal-ref><doi>10.1016/j.joi.2013.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an overall perception of increased interdisciplinarity in science,
but this is difficult to confirm quantitatively owing to the lack of adequate
methods to evaluate subjective phenomena. This is no different from the
difficulties in establishing quantitative relationships in human and social
sciences. In this paper we quantified the interdisciplinarity of scientific
journals and science fields by using an entropy measurement based on the
diversity of the subject categories of journals citing a specific journal. The
methodology consisted in building citation networks using the Journal Citation
Reports database, in which the nodes were journals and edges were established
based on citations among journals. The overall network for the 11-year period
(1999-2009) studied was small-world and scale free with regard to the
in-strength. Upon visualizing the network topology an overall structure of the
various science fields could be inferred, especially their interconnections. We
confirmed quantitatively that science fields are becoming increasingly
interdisciplinary, with the degree of interdisplinarity (i.e. entropy)
correlating strongly with the in-strength of journals and with the impact
factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4810</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4810</id><created>2012-03-21</created><authors><author><keyname>Burnashev</keyname><forenames>Marat V.</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>Estimating a Random Walk First-Passage Time from Noisy or Delayed
  Observations</title><categories>cs.IT math.IT stat.OT</categories><comments>To appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random walk (or a Wiener process), possibly with drift, is observed in a
noisy or delayed fashion. The problem considered in this paper is to estimate
the first time \tau the random walk reaches a given level. Specifically, the
p-moment (p\geq 1) optimization problem \inf_\eta \ex|\eta-\tau|^p is
investigated where the infimum is taken over the set of stopping times that are
defined on the observation process.
  When there is no drift, optimal stopping rules are characterized for both
types of observations. When there is a drift, upper and lower bounds on
\inf_\eta \ex|\eta-\tau|^p are established for both types of observations. The
bounds are tight in the large-level regime for noisy observations and in the
large-level-large-delay regime for delayed observations. Noteworthy, for noisy
observations there exists an asymptotically optimal stopping rule that is a
function of a single observation.
  Simulation results are provided that corroborate the validity of the results
for non-asymptotic settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4822</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4822</id><created>2012-03-21</created><authors><author><keyname>Curtis</keyname><forenames>Andrew R.</forenames></author><author><keyname>Lin</keyname><forenames>Min Chih</forenames></author><author><keyname>McConnell</keyname><forenames>Ross M.</forenames></author><author><keyname>Nussbaum</keyname><forenames>Yahav</forenames></author><author><keyname>Soulignac</keyname><forenames>Francisco J.</forenames></author><author><keyname>Spinrad</keyname><forenames>Jeremy P.</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>Isomorphism of graph classes related to the circular-ones property</title><categories>cs.DS cs.DM</categories><comments>25 pages, 9 figures</comments><msc-class>68R10 (Primary), 05C60, 05C85 (Secondary)</msc-class><journal-ref>Discrete Math. Theor. Comput. Sci. 15 (2013), 157--182</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a linear-time algorithm that checks for isomorphism between two 0-1
matrices that obey the circular-ones property. This algorithm leads to
linear-time isomorphism algorithms for related graph classes, including Helly
circular-arc graphs, \Gamma-circular-arc graphs, proper circular-arc graphs and
convex-round graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4827</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4827</id><created>2012-03-21</created><authors><author><keyname>Erritali</keyname><forenames>Mohammed</forenames></author><author><keyname>Reda</keyname><forenames>Oussama Mohamed</forenames></author><author><keyname>Ouahidi</keyname><forenames>Bouabid El</forenames></author></authors><title>UML modelling of geographic routing protocol &quot;Greedy Perimeter Stateless
  Routing&quot; for its integration into the &quot;Java Network Simulator&quot;</title><categories>cs.NI</categories><comments>5 pages</comments><journal-ref>International Journal of Advanced Research in Computer Science and
  Software Engineering ISSN: 2277 128X Volume 2, Issue 2, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose an UML modeling of the &quot;Greedy Perimeter Stateless
Routing&quot; (GPSR) protocol that integrate this geographic routing protocol, into
&quot;JavaNetwork Simulator&quot; to simulate and study this protocol in a first time and
offer some improvement in these features. Java Network Simulator (JNS) is a
project of &quot;translation&quot; of Network Simulator (NS) in Java initiated by &quot;the
UCL Department of Computer Science&quot;. This simulator is not as complete as ns-2,
but it is much more accessible to programmers unfamiliar with Tcl. Java Network
Simulator does not support so far, no routing protocol for vehicular ad hoc
networks and all the routing decisions are made statically or using RIP and
OSPF. By modeling and integrating the routing protocol GPSR to JNS, users will
be able to understand the concept of the geographic routing and how the routing
information is transmitted and updated between nodes in vehicular ad hoc
network. The article first examines the architecture of the Java Network
Simulator, then gives a brief review of the routing protocol GPSR and finally
presents our UML modeling incorporating GPSR in the Java Network Simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4836</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4836</id><created>2012-03-21</created><authors><author><keyname>Gorbunovs</keyname><forenames>Anatolijs</forenames></author></authors><title>On a New Method of Storing a Variable Size Array</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new data structure, log_vector, with the following
properties: constant time random access to individual elements; constant time
element addition to the end; constant time element removal from the end;
constant time empty data structure creation; amortized constant space per
individual elements; constant additional space used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4841</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4841</id><created>2012-03-21</created><authors><author><keyname>Bhorkar</keyname><forenames>A. A.</forenames></author><author><keyname>Javidi</keyname><forenames>T.</forenames></author><author><keyname>Snoeren</keyname><forenames>A. C.</forenames></author></authors><title>Achieving Congestion Diversity in Multi-hop Wireless Mesh Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on the first systematic study of congestion-aware routing
algorithms for wireless mesh networks to achieve an improved end-end delay
performance. In particular, we compare 802.11 compatible implementations of a
set of congestion-aware routing protocols against our implementation of state
of the art shortest path routing protocol (SRCR). We implement congestion-aware
routing algorithms Backpressure (BP), Enhanced-Backpressure (E-BP) adapted from
[1], [2] suitably adjusted for 802.11 implementation. We then propose and
implement Congestion Diversity Protocol (CDP) adapted from [3] recognizing the
limitations of BP and E-BP for 802.11-based wireless networks. SRCR solely
utilizes link qualities, while BP relies on queue differential to route
packets. CDP and E-BP rely on distance metrics which take into account queue
backlogs and link qualities in the network. E-BP computes its metric by summing
the ETX and queue differential, while CDP determines its metric by calculating
the least draining time to the destination. Our small testbed consisting of
twelve 802.11g nodes enables us to empirically compare the performance of
congestion-aware routing protocols (BP, E-BP and CDP) against benchmark SRCR.
For medium to high load UDP traffic, we observe that CDP exhibits significant
improvement with respect to both end-end delay and throughput over other
protocols with no loss of performance for TCP traffic. Backpressure-based
routing algorithms (BP and E-BP) show poorer performance for UDP and TCP
traffic. Finally, we carefully study the effects of the modular approach to
congestion-aware routing design in which the MAC layer is left intact
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4844</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4844</id><created>2012-03-21</created><updated>2012-06-08</updated><authors><author><keyname>Kurniawan</keyname><forenames>Ernest</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author></authors><title>Practical Coding Schemes for Cognitive Overlay Radios</title><categories>cs.IT math.IT</categories><comments>Patent pending</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop practical coding schemes for the cognitive overlay radios as
modeled by the cognitive interference channel, a variation of the classical two
user interference channel where one of the transmitters has knowledge of both
messages. Inspired by information theoretical results, we develop a coding
strategy for each of the three parameter regimes where capacity is known. A key
feature of the capacity achieving schemes in these regimes is the joint
decoding of both users' codewords, which we accomplish by performing a
posteriori probability calculation over a combined trellis. The schemes are
shown to perform close to the capacity limit with low error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4855</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4855</id><created>2012-03-21</created><authors><author><keyname>Ershad</keyname><forenames>Shervan Fekri</forenames></author></authors><title>Texture Classification Approach Based on Combination of Edge &amp;
  Co-occurrence and Local Binary Pattern</title><categories>cs.CV cs.AI</categories><comments>4 pages, 6 figures, 1 tables</comments><journal-ref>Int'l Conf. IP, Comp. Vision, and Pattern Recognition, IPCV'11,
  2011, pp. 626-629</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Texture classification is one of the problems which has been paid much
attention on by computer scientists since late 90s. If texture classification
is done correctly and accurately, it can be used in many cases such as Pattern
recognition, object tracking, and shape recognition. So far, there have been so
many methods offered to solve this problem. Near all these methods have tried
to extract and define features to separate different labels of textures really
well. This article has offered an approach which has an overall process on the
images of textures based on Local binary pattern and Gray Level Co-occurrence
matrix and then by edge detection, and finally, extracting the statistical
features from the images would classify them. Although, this approach is a
general one and is could be used in different applications, the method has been
tested on the stone texture and the results have been compared with some of the
previous approaches to prove the quality of proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4863</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4863</id><created>2012-03-21</created><authors><author><keyname>Hobby</keyname><forenames>John D.</forenames></author><author><keyname>Tucci</keyname><forenames>Gabriel H.</forenames></author></authors><title>Traffic Analysis in Random Delaunay Tessellations and Other Graphs</title><categories>math.DG cs.CG cs.NI math.CO</categories><comments>Submitted to the Journal of Discrete Computational Geometry</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the degree distribution, the maximum vertex and edge
flow in non-uniform random Delaunay triangulations when geodesic routing is
used. We also investigate the vertex and edge flow in Erd\&quot;os-Renyi random
graphs, geometric random graphs, expanders and random $k$-regular graphs.
Moreover we show that adding a random matching to the original graph can
considerably reduced the maximum vertex flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4865</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4865</id><created>2012-03-21</created><authors><author><keyname>Asnani</keyname><forenames>Himanshu</forenames></author><author><keyname>Permuter</keyname><forenames>Haim</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Successive Refinement with Decoder Cooperation and its Channel Coding
  Duals</title><categories>cs.IT math.IT</categories><comments>55 pages, 15 figures, 8 tables, submitted to IEEE Transactions on
  Information Theory. A shorter version submitted to ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study cooperation in multi terminal source coding models involving
successive refinement. Specifically, we study the case of a single encoder and
two decoders, where the encoder provides a common description to both the
decoders and a private description to only one of the decoders. The decoders
cooperate via cribbing, i.e., the decoder with access only to the common
description is allowed to observe, in addition, a deterministic function of the
reconstruction symbols produced by the other. We characterize the fundamental
performance limits in the respective settings of non-causal, strictly-causal
and causal cribbing. We use a new coding scheme, referred to as Forward
Encoding and Block Markov Decoding, which is a variant of one recently used by
Cuff and Zhao for coordination via implicit communication. Finally, we use the
insight gained to introduce and solve some dual channel coding scenarios
involving Multiple Access Channels with cribbing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4867</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4867</id><created>2012-03-21</created><authors><author><keyname>Liu</keyname><forenames>Binyue</forenames></author><author><keyname>Cai</keyname><forenames>Ning</forenames></author></authors><title>Multi-hop Analog Network Coding: An Amplify-and-Forward Approach</title><categories>cs.IT math.IT</categories><comments>22 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of an amplify-and-forward (AF) based
analog network coding (ANC) relay scheme in a multi-hop wireless network under
individual power constraints. In the first part, a unicast scenario is
considered. The problem of finding the maximum achievable rate is formulated as
an optimization problem. Rather than solving this non-concave maximization
problem, we derive upper and lower bounds for the optimal rate. A cut-set like
upper bound is obtained in a closed form for a layered relay network. A
pseudo-optimal AF scheme is developed for a two-hop parallel network, which is
different from the conventional scheme with all amplification gains chosen as
the maximum possible values. The conditions under which either the novel scheme
or the conventional one achieves a rate within half a bit of the upper bound
are found. Then we provide an AF-based multi-hop ANC scheme with the two
schemes for a layered relay network. It is demonstrated that the lower bound of
the optimal rate can asymptotically achieve the upper bound when the network is
in the generalized high-SNR regime. In the second part, the optimal rate region
for a two-hop multiple access channel (MAC) via AF relays is investigated. In a
similar manner, we first derive an outer bound for it and then focus on
designing low complexity AF-based ANC schemes for different scenarios. Several
examples are given and the numerical results indicate that the achievable rate
region of the ANC schemes can perform close to the outer bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4870</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4870</id><created>2012-03-21</created><updated>2013-03-29</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Zhang</keyname><forenames>Cishen</forenames></author></authors><title>Variational Bayesian algorithm for quantized compressed sensing</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Trans. Signal Processing. 10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing (CS) is on recovery of high dimensional signals from their
low dimensional linear measurements under a sparsity prior and digital
quantization of the measurement data is inevitable in practical implementation
of CS algorithms. In the existing literature, the quantization error is modeled
typically as additive noise and the multi-bit and 1-bit quantized CS problems
are dealt with separately using different treatments and procedures. In this
paper, a novel variational Bayesian inference based CS algorithm is presented,
which unifies the multi- and 1-bit CS processing and is applicable to various
cases of noiseless/noisy environment and unsaturated/saturated quantizer. By
decoupling the quantization error from the measurement noise, the quantization
error is modeled as a random variable and estimated jointly with the signal
being recovered. Such a novel characterization of the quantization error
results in superior performance of the algorithm which is demonstrated by
extensive simulations in comparison with state-of-the-art methods for both
multi-bit and 1-bit CS problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4874</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4874</id><created>2012-03-21</created><authors><author><keyname>Thorpe</keyname><forenames>Christopher</forenames></author><author><keyname>Li</keyname><forenames>Feng</forenames></author><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Yu</keyname><forenames>Zhan</forenames></author><author><keyname>Saunders</keyname><forenames>David</forenames></author><author><keyname>Yu</keyname><forenames>Jingyi</forenames></author></authors><title>A Co-Prime Blur Scheme for Data Security in Video Surveillance</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel Coprime Blurred Pair (CBP) model for visual
data-hiding for security in camera surveillance. While most previous approaches
have focused on completely encrypting the video stream, we introduce a spatial
encryption scheme by blurring the image/video contents to create a CBP. Our
goal is to obscure detail in public video streams by blurring while allowing
behavior to be recognized and to quickly deblur the stream so that details are
available if behavior is recognized as suspicious. We create a CBP by blurring
the same latent image with two unknown kernels. The two kernels are coprime
when mapped to bivariate polynomials in the z domain. To deblur the CBP we
first use the coprime constraint to approximate the kernels and sample the
bivariate CBP polynomials in one dimension on the unit circle. At each sample
point, we factor the 1D polynomial pair and compose the results into a 2D
kernel matrix. Finally, we compute the inverse Fast Fourier Transform (FFT) of
the kernel matrices to recover the coprime kernels and then the latent video
stream. It is therefore only possible to deblur the video stream if a user has
access to both streams. To improve the practicability of our algorithm, we
implement our algorithm using a graphics processing unit (GPU) to decrypt the
blurred video streams in real-time, and extensive experimental results
demonstrate that our new scheme can effectively protect sensitive identity
information in surveillance videos and faithfully reconstruct the unblurred
video stream when two blurred sequences are available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4875</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4875</id><created>2012-03-21</created><authors><author><keyname>Jin</keyname><forenames>Qing</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author></authors><title>Spontaneous Symmetry Breaking in Interdependent Networked Game</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial evolution game has traditionally assumed that players interact with
neighbors on a single network, which is isolated and not influenced by other
systems. We introduce the simple game model into the interdependent networks
composed of two networks, and show that when the interdependent factor $\alpha$
is smaller than a particular value $\alpha_C$, homogeneous cooperation can be
guaranteed. However, as interdependent factor exceeds $\alpha_C$, spontaneous
symmetry breaking of fraction of cooperators presents itself between different
networks. In addition, our results can be well predicted by the strategy-couple
pair approximation method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4881</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4881</id><created>2012-03-22</created><authors><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>Computational Complexity Analysis of Multi-Objective Genetic Programming</title><categories>cs.NE</categories><comments>A conference version has been accepted for GECCO 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational complexity analysis of genetic programming (GP) has been
started recently by analyzing simple (1+1) GP algorithms for the problems ORDER
and MAJORITY. In this paper, we study how taking the complexity as an
additional criteria influences the runtime behavior. We consider
generalizations of ORDER and MAJORITY and present a computational complexity
analysis of (1+1) GP using multi-criteria fitness functions that take into
account the original objective and the complexity of a syntax tree as a
secondary measure. Furthermore, we study the expected time until
population-based multi-objective genetic programming algorithms have computed
the Pareto front when taking the complexity of a syntax tree as an equally
important objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4882</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4882</id><created>2012-03-22</created><updated>2012-04-17</updated><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author><author><keyname>Mueller</keyname><forenames>Ralf R.</forenames></author><author><keyname>Kawabata</keyname><forenames>Tsutomu</forenames></author></authors><title>Large-System Analysis of Joint User Selection and Vector Precoding with
  Zero-Forcing Transmit Beamforming for MIMO Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>submitted to ISITA2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) broadcast channels (BCs) (MIMO-BCs)
with perfect channel state information (CSI) at the transmitter are considered.
As joint user selection (US) and vector precoding (VP) (US-VP) with
zero-forcing transmit beamforming (ZF-BF), US and continuous VP (CVP) (US-CVP)
and data-dependent US (DD-US) are investigated. The replica method, developed
in statistical physics, is used to analyze the energy penalties for the two
US-VP schemes in the large-system limit, where the number of users, the number
of selected users, and the number of transmit antennas tend to infinity with
their ratios kept constant. Four observations are obtained in the large-system
limit: First, the assumptions of replica symmetry (RS) and 1-step replica
symmetry breaking (1RSB) for DD-US can provide acceptable approximations for
low and moderate system loads, respectively. Secondly, DD-US outperforms CVP
with random US in terms of the energy penalty for low-to-moderate system loads.
Thirdly, the asymptotic energy penalty of DD-US is indistinguishable from that
of US-CVP for low system loads. Finally, a greedy algorithm of DD-US proposed
in authors' previous work can achieve nearly optimal performance for
low-to-moderate system loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4885</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4885</id><created>2012-03-22</created><authors><author><keyname>Molina</keyname><forenames>Abel</forenames></author></authors><title>Parallel Repetition of Prover-Verifier Quantum Interactions</title><categories>quant-ph cs.CC</categories><comments>Chapter 5 includes results from arXiv 1104.1140</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we answer several questions about the behaviour of
prover-verifier interactions under parallel repetition when quantum information
is allowed, and the verifier acts independently in them.
  We first consider the case in which a value is associated with each of the
possible outcomes of an interaction. We prove that it is not possible for the
prover to improve on the optimum average value per repetition by repeating the
protocol multiple times in parallel.
  We look then at games in which the outcomes are classified into two types,
winning outcomes and losing outcomes. We ask what is the optimal probability
for the prover of winning at least k times out of n parallel repetitions, given
that the optimal probability of winning when only one repetition is considered
is $p$. A reasonable conjecture for the answer would be \sum_{m \geq k} {n
\choose m} p^m (1-p)^{n-m}, as that is the answer when it is optimal for the
prover to act independently. This is known to be the correct answer when k=n,
and also in the classical case. It is also correct in some generalizations of
the classical case that we will discuss later. We will show how this cannot be
extended to all cases, presenting an example of an interaction with k=1,n=2 in
which p\approx 0.85, but it is possible to always win at least once. We will
then give some upper bounds on the optimal probability for the prover of
winning k times out of n parallel repetitions. These bounds are expressed as a
function of p.
  Finally, we will connect our results to the study of error reduction for
quantum interactive proofs using parallel repetition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4900</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4900</id><created>2012-03-22</created><authors><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author><author><keyname>Post</keyname><forenames>Ian</forenames></author></authors><title>Single pass sparsification in the streaming model with edge deletions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a construction of cut sparsifiers of Benczur and Karger
in the {\em dynamic} streaming setting in a single pass over the data stream.
Previous constructions either required multiple passes or were unable to handle
edge deletions. We use $\tilde{O}(1/\e^2)$ time for each stream update and
$\tilde{O}(n/\e^2)$ time to construct a sparsifier. Our $\e$-sparsifiers have
$O(n\log^3 n/\e^2)$ edges. The main tools behind our result are an application
of sketching techniques of Ahn et al.[SODA'12] to estimate edge connectivity
together with a novel application of sampling with limited independence and
sparse recovery to produce the edges of the sparsifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4903</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4903</id><created>2012-03-22</created><updated>2014-06-08</updated><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author></authors><title>Distance Queries from Sampled Data: Accurate and Efficient</title><categories>cs.DS cs.DB math.ST stat.TH</categories><comments>13 pages; This is a full version of a KDD 2014 paper</comments><acm-class>E.0; G.3; H.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance queries are a basic tool in data analysis. They are used for
detection and localization of change for the purpose of anomaly detection,
monitoring, or planning. Distance queries are particularly useful when data
sets such as measurements, snapshots of a system, content, traffic matrices,
and activity logs are collected repeatedly.
  Random sampling, which can be efficiently performed over streamed or
distributed data, is an important tool for scalable data analysis. The sample
constitutes an extremely flexible summary, which naturally supports domain
queries and scalable estimation of statistics, which can be specified after the
sample is generated. The effectiveness of a sample as a summary, however,
hinges on the estimators we have.
  We derive novel estimators for estimating $L_p$ distance from sampled data.
Our estimators apply with the most common weighted sampling schemes: Poisson
Probability Proportional to Size (PPS) and its fixed sample size variants. They
also apply when the samples of different data sets are independent or
coordinated. Our estimators are admissible (Pareto optimal in terms of
variance) and have compelling properties.
  We study the performance of our Manhattan and Euclidean distance ($p=1,2$)
estimators on diverse datasets, demonstrating scalability and accuracy even
when a small fraction of the data is sampled. Our work, for the first time,
facilitates effective distance estimation over sampled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4912</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4912</id><created>2012-03-22</created><authors><author><keyname>Fulop</keyname><forenames>Sean A.</forenames></author></authors><title>A survey of proof nets and matrices for substructural logics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a survey of two kinds of &quot;compressed&quot; proof schemes, the
\emph{matrix method} and \emph{proof nets}, as applied to a variety of logics
ranging along the substructural hierarchy from classical all the way down to
the nonassociative Lambek system. A novel treatment of proof nets for the
latter is provided. Descriptions of proof nets and matrices are given in a
uniform notation based on sequents, so that the properties of the schemes for
the various logics can be easily compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4913</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4913</id><created>2012-03-22</created><updated>2012-06-13</updated><authors><author><keyname>Li</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Sihai</forenames></author><author><keyname>Wang</keyname><forenames>Kaiwei</forenames></author><author><keyname>Zhou</keyname><forenames>Wuyang</forenames></author></authors><title>Combined Channel Aggregation and Fragmentation Strategy in Cognitive
  Radio Networks</title><categories>cs.NI</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio networks, channel aggregation (CA) and channel
fragmentation (CF) techniques have been proposed to enhance the spectrum
utilization. While most of the literature studies CA and CF independently, in
this paper we combine CA and CF innovatively and present a new spectrum sharing
strategy named CAF (Channel Aggregation and Fragmentation). We elaborate on the
proposed CAF strategy and derive the balance equation by a continuous time
Markov chain (CTMC) model. Then various system performance metrics including
blocking probability, dropping probability, spectrum utilization and throughput
of the secondary network are evaluated. Both analytical and simulation results
show that our strategy lowers the blocking and dropping probabilities and
enhances the spectrum utilization and throughput effectively. Moreover, by
tuning the bandwidth requirement of each secondary user, different system
performance can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4917</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4917</id><created>2012-03-22</created><authors><author><keyname>Morcrette</keyname><forenames>Basile</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author></authors><title>Fully Analyzing an Algebraic Polya Urn Model</title><categories>math.CO cs.DM math.PR</categories><comments>LATIN 2012, Arequipa : Peru (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces and analyzes a particular class of Polya urns: balls
are of two colors, can only be added (the urns are said to be additive) and at
every step the same constant number of balls is added, thus only the color
compositions varies (the urns are said to be balanced). These properties make
this class of urns ideally suited for analysis from an &quot;analytic combinatorics&quot;
point-of-view, following in the footsteps of Flajolet-Dumas-Puyhaubert, 2006.
Through an algebraic generating function to which we apply a multiple
coalescing saddle-point method, we are able to give precise asymptotic results
for the probability distribution of the composition of the urn, as well as
local limit law and large deviation bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4920</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4920</id><created>2012-03-22</created><authors><author><keyname>Colussi</keyname><forenames>Livio</forenames></author></authors><title>Work function algorithm can forget history without losing
  competitiveness</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Work Function Algorithm is the most effective deterministic on-line
algorithm for the k-server problem. Koutsoupias and Papadimitriou proved WFA is
(2k-1) competitive. However the best known implementation of WFA requires time
O(i^2) to process request r_i and this makes WFA impractical for long sequences
of requests. The O(i^2) time is spent to compute the work function on the whole
history of past requests. In order to make constant the time to process a
request, Rudec and Menger proposed to restrict the history to a moving window
of fixed size. However WFA restricted to a moving window loses its
competitiveness. Here we give a condition that allows WFA to forget the whole
previous history and restart from scratch without losing competitiveness.
Moreover for most of the metric spaces of practical interest (finite or bounded
spaces) there is a constant bound on the length of the history before the
condition is verified and this makes O(1) the time to process each request.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4924</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4924</id><created>2012-03-22</created><authors><author><keyname>Hernaez</keyname><forenames>Mikel</forenames></author><author><keyname>crespo</keyname><forenames>Pedro M.</forenames></author><author><keyname>Del Ser</keyname><forenames>Javier</forenames></author></authors><title>A Flexible Channel Coding Approach for Short-Length Codewords</title><categories>cs.IT math.IT</categories><comments>4 pages; submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter introduces a novel channel coding design framework for
short-length codewords that permits balancing the tradeoff between the bit
error rate floor and waterfall region by modifying a single real-valued
parameter. The proposed approach is based on combining convolutional coding
with a $q$-ary linear combination and unequal energy allocation, the latter
being controlled by the aforementioned parameter. EXIT charts are used to shed
light on the convergence characteristics of the associated iterative decoder,
which is described in terms of factor graphs. Simulation results show that the
proposed scheme is able to adjust its end-to-end error rate performance
efficiently and easily, on the contrary to previous approaches that require a
full code redesign when the error rate requirements of the application change.
Simulations also show that, at mid-range bit-error rates, there is a small
performance penalty with respect to the previous approaches. However, the EXIT
chart analysis and the simulation results suggest that for very low bit-error
rates the proposed system will exhibit lower error floors than previous
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4930</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4930</id><created>2012-03-22</created><updated>2013-07-01</updated><authors><author><keyname>Dinuzzo</keyname><forenames>Francesco</forenames></author></authors><title>Kernels for linear time invariant system identification</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of identifying the impulse response of a
linear time invariant (LTI) dynamical system from the knowledge of the input
signal and a finite set of noisy output observations. We adopt an approach
based on regularization in a Reproducing Kernel Hilbert Space (RKHS) that takes
into account both continuous and discrete time systems. The focus of the paper
is on designing spaces that are well suited for temporal impulse response
modeling. To this end, we construct and characterize general families of
kernels that incorporate system properties such as stability, relative degree,
absence of oscillatory behavior, smoothness, or delay. In addition, we discuss
the possibility of automatically searching over these classes by means of
kernel learning techniques, so as to capture different modes of the system to
be identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4933</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4933</id><created>2012-03-22</created><authors><author><keyname>Nongmeikapam</keyname><forenames>Kishorjit</forenames></author><author><keyname>Nonglenjaoba</keyname><forenames>Lairenlakpam</forenames></author><author><keyname>Nirmal</keyname><forenames>Yumnam</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Sivaji</forenames></author></authors><title>Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS
  Tagger</title><categories>cs.CL</categories><comments>15 pages, 4 tables, 2 figures, the link
  http://airccse.org/journal/jcsit/1011csit05.pdf. arXiv admin note: text
  overlap with arXiv:1111.2399</comments><acm-class>I.2.7</acm-class><doi>10.5121/ijitcs.2012.210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a detail overview about the modified features selection in
CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging.
Selection of features is so important in CRF that the better are the features
then the better are the outputs. This work is an attempt or an experiment to
make the previous work more efficient. Multiple new features are tried to run
the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as
another feature. The CRF run with RMWE because Manipuri is rich of RMWE and
identification of RMWE becomes one of the necessities to bring up the result of
POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15%
and F-measure of 75.60%. With the identification of RMWE and considering it as
a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and
F-measure of 77.14%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4938</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4938</id><created>2012-03-22</created><updated>2012-03-23</updated><authors><author><keyname>Cabellos</keyname><forenames>Luis</forenames></author></authors><title>Advanced Programming Platform for efficient use of Data Parallel
  Hardware</title><categories>cs.DC</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Graphics processing units (GPU) had evolved from a specialized hardware
capable to render high quality graphics in games to a commodity hardware for
effective processing blocks of data in a parallel schema. This evolution is
particularly interesting for scientific groups, which traditionally use mainly
CPU as a work horse, and now can profit of the arrival of GPU hardware to HPC
clusters. This new GPU hardware promises a boost in peak performance, but it is
not trivial to use. In this article a programming platform designed to promote
a direct use of this specialized hardware is presented. This platform includes
a visual editor of parallel data flows and it is oriented to the execution in
distributed clusters with GPUs. Examples of application in two characteristic
problems, Fast Fourier Transform and Image Compression, are also shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4966</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4966</id><created>2012-03-22</created><authors><author><keyname>Sparavigna</keyname><forenames>A. C.</forenames></author><author><keyname>Marazzato</keyname><forenames>R.</forenames></author></authors><title>A tour about Isaac Newton's life</title><categories>cs.OH physics.hist-ph physics.pop-ph</categories><comments>Georeferencing, Satellite Maps, KML, XML, Acme Mapper, History of
  Physics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we propose a tour about the life of Isaac Newton, using a georeferenced
method, based on the free satellite maps. Our tour is modelled on the time-line
of the great scientist's life, as an ancient &quot;itinerarium&quot; was modelled on the
Roman roads, providing a listing of places and intervening distances, sometimes
with short description or symbols concerning the places. KML language and
Google Earth, with its Street View and 3D images are powerful tools to create
this virtual tour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4988</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4988</id><created>2012-03-22</created><updated>2012-04-27</updated><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author><author><keyname>Duncan</keyname><forenames>Ross</forenames></author><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author><author><keyname>Wang</keyname><forenames>Quanlong</forenames></author></authors><title>Strong Complementarity and Non-locality in Categorical Quantum Mechanics</title><categories>quant-ph cs.LO math.CT</categories><comments>15 pages (incl. 5 appendix). To appear: LiCS 2012</comments><doi>10.1109/LICS.2012.35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorical quantum mechanics studies quantum theory in the framework of
dagger-compact closed categories.
  Using this framework, we establish a tight relationship between two key
quantum theoretical notions: non-locality and complementarity. In particular,
we establish a direct connection between Mermin-type non-locality scenarios,
which we generalise to an arbitrary number of parties, using systems of
arbitrary dimension, and performing arbitrary measurements, and a new stronger
notion of complementarity which we introduce here.
  Our derivation of the fact that strong complementarity is a necessary
condition for a Mermin scenario provides a crisp operational interpretation for
strong complementarity. We also provide a complete classification of strongly
complementary observables for quantum theory, something which has not yet been
achieved for ordinary complementarity.
  Since our main results are expressed in the (diagrammatic) language of
dagger-compact categories, they can be applied outside of quantum theory, in
any setting which supports the purely algebraic notion of strongly
complementary observables. We have therefore introduced a method for discussing
non-locality in a wide variety of models in addition to quantum theory.
  The diagrammatic calculus substantially simplifies (and sometimes even
trivialises) many of the derivations, and provides new insights. In particular,
the diagrammatic computation of correlations clearly shows how local
measurements interact to yield a global overall effect. In other words, we
depict non-locality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5004</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5004</id><created>2012-03-22</created><updated>2012-03-23</updated><authors><author><keyname>Dunlaing</keyname><forenames>Colm O.</forenames></author></authors><title>CUDA implementation of Wagener's 2D convex hull PRAM algorithm</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a CUDA implementation of Wagener's PRAM convex hull
algorithm in two dimensions. It is presented in Knuth's literate programming
style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5026</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5026</id><created>2012-03-22</created><authors><author><keyname>Xu</keyname><forenames>Kuang</forenames></author></authors><title>On the Power of Centralization in Distributed Processing</title><categories>cs.DC cs.NI cs.PF math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we propose and analyze a multi-server model that captures a
performance trade-off between centralized and distributed processing. In our
model, a fraction $p$ of an available resource is deployed in a centralized
manner (e.g., to serve a most-loaded station) while the remaining fraction
$1-p$ is allocated to local servers that can only serve requests addressed
specifically to their respective stations.
  Using a fluid model approach, we demonstrate a surprising phase transition in
the steady-state delay, as $p$ changes: in the limit of a large number of
stations, and when any amount of centralization is available ($p&gt;0$), the
average queue length in steady state scales as $\log_{1/(1-p)} 1/(1-\lambda)$
when the traffic intensity $\lambda$ goes to 1. This is exponentially smaller
than the usual M/M/1-queue delay scaling of $1/(1-\lambda)$, obtained when all
resources are fully allocated to local stations ($p=0$). This indicates a
strong qualitative impact of even a small degree of centralization.
  We prove convergence to a fluid limit, and characterize both the transient
and steady-state behavior of the finite system, in the limit as the number of
stations $N$ goes to infinity. We show that the sequence of queue-length
processes converges to a unique fluid trajectory (over any finite time
interval, as $N$ approaches infinity, and that this fluid trajectory converges
to a unique invariant state $v^I$, for which a simple closed-form expression is
obtained. We also show that the steady-state distribution of the $N$-server
system concentrates on $v^I$ as $N$ goes to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5028</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5028</id><created>2012-03-22</created><authors><author><keyname>Abdoun</keyname><forenames>Otman</forenames></author><author><keyname>Tajani</keyname><forenames>Chakir</forenames></author><author><keyname>Abouchabka</keyname><forenames>Jaafar</forenames></author></authors><title>Hybridizing PSM and RSM Operator for Solving NP-Complete Problems:
  Application to Travelling Salesman Problem</title><categories>cs.NE</categories><comments>ISSN (Online): 1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, 2012, 374-378</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new mutation operator, Hybrid Mutation (HPRM),
for a genetic algorithm that generates high quality solutions to the Traveling
Salesman Problem (TSP). The Hybrid Mutation operator constructs an offspring
from a pair of parents by hybridizing two mutation operators, PSM and RSM. The
efficiency of the HPRM is compared as against some existing mutation operators;
namely, Reverse Sequence Mutation (RSM) and Partial Shuffle Mutation (PSM) for
BERLIN52 as instance of TSPLIB. Experimental results show that the new mutation
operator is better than the RSM and PSM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5037</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5037</id><created>2012-03-22</created><authors><author><keyname>Haddad</keyname><forenames>Salim</forenames></author><author><keyname>Baghdadi</keyname><forenames>Amer</forenames></author><author><keyname>Jezequel</keyname><forenames>Michel</forenames></author></authors><title>On the Convergence Speed of Turbo Demodulation with Turbo Decoding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing on April 27, 2011</comments><doi>10.1109/TSP.2012.2198550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative processing is widely adopted nowadays in modern wireless receivers
for advanced channel codes like turbo and LDPC codes. Extension of this
principle with an additional iterative feedback loop to the demapping function
has proven to provide substantial error performance gain. However, the adoption
of iterative demodulation with turbo decoding is constrained by the additional
implied implementation complexity, heavily impacting latency and power
consumption. In this paper, we analyze the convergence speed of these combined
two iterative processes in order to determine the exact required number of
iterations at each level. Extrinsic information transfer (EXIT) charts are used
for a thorough analysis at different modulation orders and code rates. An
original iteration scheduling is proposed reducing two demapping iterations
with reasonable performance loss of less than 0.15 dB. Analyzing and
normalizing the computational and memory access complexity, which directly
impact latency and power consumption, demonstrates the considerable gains of
the proposed scheduling and the promising contributions of the proposed
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5040</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5040</id><created>2012-03-22</created><authors><author><keyname>Hyyti&#xe4;</keyname><forenames>Esa</forenames></author><author><keyname>Aalto</keyname><forenames>Samuli</forenames></author><author><keyname>Penttinen</keyname><forenames>Aleksi</forenames></author></authors><title>Minimizing Slowdown in Heterogeneous Size-Aware Dispatching Systems
  (full version)</title><categories>cs.PF</categories><comments>This is the full version of a paper with the same title that appears
  in ACM SIGMETRICS 2012, with the inclusion of the appendix. 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a system of parallel queues where tasks are assigned (dispatched)
to one of the available servers upon arrival. The dispatching decision is based
on the full state information, i.e., on the sizes of the new and existing jobs.
We are interested in minimizing the so-called mean slowdown criterion
corresponding to the mean of the sojourn time divided by the processing time.
Assuming no new jobs arrive, the shortest-processing-time-product (SPTP)
schedule is known to minimize the slowdown of the existing jobs. The main
contribution of this paper is three-fold: 1) To show the optimality of SPTP
with respect to slowdown in a single server queue under Poisson arrivals; 2) to
derive the so-called size-aware value functions for
M/G/1-FIFO/LIFO/SPTP/SPT/SRPT with general holding costs of which the slowdown
criterion is a special case; and 3) to utilize the value functions to derive
efficient dispatching policies so as to minimize the mean slowdown in a
heterogeneous server system. The derived policies offer a significantly better
performance than e.g., the size-aware-task-assignment with equal load (SITA-E)
and least-work-left (LWL) policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5051</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5051</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author></authors><title>Analysing Temporally Annotated Corpora with CAVaT</title><categories>cs.CL</categories><journal-ref>Proc. LREC (2010) 398-404</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present CAVaT, a tool that performs Corpus Analysis and Validation for
TimeML. CAVaT is an open source, modular checking utility for statistical
analysis of features specific to temporally-annotated natural language corpora.
It provides reporting, highlights salient links between a variety of general
and time-specific linguistic features, and also validates a temporal annotation
to ensure that it is logically consistent and sufficiently annotated. Uniquely,
CAVaT provides analysis specific to TimeML-annotated temporal information.
TimeML is a standard for annotating temporal information in natural language
text. In this paper, we present the reporting part of CAVaT, and then its
error-checking ability, including the workings of several novel TimeML document
verification methods. This is followed by the execution of some example tasks
using the tool to show relations between times, events, signals and links. We
also demonstrate inconsistencies in a TimeML corpus (TimeBank) that have been
detected with CAVaT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5055</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5055</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author></authors><title>Using Signals to Improve Automatic Classification of Temporal Relations</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Temporal information conveyed by language describes how the world around us
changes through time. Events, durations and times are all temporal elements
that can be viewed as intervals. These intervals are sometimes temporally
related in text. Automatically determining the nature of such relations is a
complex and unsolved problem. Some words can act as &quot;signals&quot; which suggest a
temporal ordering between intervals. In this paper, we use these signal words
to improve the accuracy of a recent approach to classification of temporal
links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5060</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5060</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author></authors><title>USFD2: Annotating Temporal Expresions and TLINKs for TempEval-2</title><categories>cs.CL</categories><comments>Part of TempEval-2</comments><journal-ref>Proc. 5th International Workshop on Semantic Evaluation (2010)
  337-340</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We describe the University of Sheffield system used in the TempEval-2
challenge, USFD2. The challenge requires the automatic identification of
temporal entities and relations in text. USFD2 identifies and anchors temporal
expressions, and also attempts two of the four temporal relation assignment
tasks. A rule-based system picks out and anchors temporal expressions, and a
maximum entropy classifier assigns temporal link labels, based on features that
include descriptions of associated temporal signal words. USFD2 identified
temporal expressions successfully, and correctly classified their type in 90%
of cases. Determining the relation between an event and time expression in the
same sentence was performed at 63% accuracy, the second highest score in this
part of the challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5062</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5062</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author></authors><title>An Annotation Scheme for Reichenbach's Verbal Tense Structure</title><categories>cs.CL</categories><journal-ref>Proc. 6th Joint ACL-ISO Workshop on Interoperable Semantic
  Annotation (2011) 10-17</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we present RTMML, a markup language for the tenses of verbs and
temporal relations between verbs. There is a richness to tense in language that
is not fully captured by existing temporal annotation schemata. Following
Reichenbach we present an analysis of tense in terms of abstract time points,
with the aim of supporting automated processing of tense and temporal relations
in language. This allows for precise reasoning about tense in documents, and
the deduction of temporal relations between the times and verbal events in a
discourse. We define the syntax of RTMML, and demonstrate the markup in a range
of situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5066</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5066</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author></authors><title>A Corpus-based Study of Temporal Signals</title><categories>cs.CL</categories><comments>Proc. Corpus Linguistics (2011)</comments><journal-ref>Proceedings of the 6th Conference on Corpus Linguistics (2011),
  No. 197, pp. 1--8</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Automatic temporal ordering of events described in discourse has been of
great interest in recent years. Event orderings are conveyed in text via va
rious linguistic mechanisms including the use of expressions such as &quot;before&quot;,
&quot;after&quot; or &quot;during&quot; that explicitly assert a temporal relation -- temporal
signals. In this paper, we investigate the role of temporal signals in temporal
relation extraction and provide a quantitative analysis of these expres sions
in the TimeBank annotated corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5069</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5069</id><created>2012-03-22</created><authors><author><keyname>Tucci</keyname><forenames>Gabriel H.</forenames></author></authors><title>Random Regular Graphs are not Asymptotically Gromov Hyperbolic</title><categories>math.MG cs.NI math.CO</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove that random $d$--regular graphs with $d\geq 3$ have
traffic congestion of the order $O(n\log_{d-1}^{3}(n))$ where $n$ is the number
of nodes and geodesic routing is used. We also show that these graphs are not
asymptotically $\delta$--hyperbolic for any non--negative $\delta$ almost
surely as $n\to\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5073</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5073</id><created>2012-03-22</created><authors><author><keyname>Burman</keyname><forenames>Amev</forenames></author><author><keyname>Jayapal</keyname><forenames>Arun</forenames></author><author><keyname>Kannan</keyname><forenames>Sathish</forenames></author><author><keyname>Kavilikatta</keyname><forenames>Madhu</forenames></author><author><keyname>Alhelbawy</keyname><forenames>Ayman</forenames></author><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author></authors><title>USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding</title><categories>cs.CL</categories><comments>Proc. Text Analysis Conference (2011)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes the University of Sheffield's entry in the 2011 TAC KBP
entity linking and slot filling tasks. We chose to participate in the
monolingual entity linking task, the monolingual slot filling task and the
temporal slot filling tasks. We set out to build a framework for
experimentation with knowledge base population. This framework was created, and
applied to multiple KBP tasks. We demonstrated that our proposed framework is
effective and suitable for collaborative development efforts, as well as useful
in a teaching environment. Finally we present results that, while very modest,
provide improvements an order of magnitude greater than our 2010 attempt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5076</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5076</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Llorens</keyname><forenames>H&#xe9;ctor</forenames></author><author><keyname>Saquete</keyname><forenames>Estela</forenames></author></authors><title>Massively Increasing TIMEX3 Resources: A Transduction Approach</title><categories>cs.CL</categories><comments>Proc. LREC (2012)</comments><journal-ref>Proceedings of the 8th international conference on Language
  Resources and Evaluation (2012), pp. 3754-3761</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Automatic annotation of temporal expressions is a research challenge of great
interest in the field of information extraction. Gold standard
temporally-annotated resources are limited in size, which makes research using
them difficult. Standards have also evolved over the past decade, so not all
temporally annotated data is in the same format. We vastly increase available
human-annotated temporal expression resources by converting older format
resources to TimeML/TIMEX3. This task is difficult due to differing annotation
methods. We present a robust conversion tool and a new, large temporal
expression resource. Using this, we evaluate our conversion process by using it
as training data for an existing TimeML annotation tool, achieving a 0.87 F1
measure -- better than any system in the TempEval-2 timex recognition exercise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5078</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5078</id><created>2012-03-22</created><authors><author><keyname>Zuva</keyname><forenames>Tranos</forenames></author><author><keyname>Olugbara</keyname><forenames>Oludayo O.</forenames></author><author><keyname>Ojo</keyname><forenames>Sunday O.</forenames></author><author><keyname>Ngwira</keyname><forenames>Seleman M.</forenames></author></authors><title>Kernel Density Feature Points Estimator for Content-Based Image
  Retrieval</title><categories>cs.CV</categories><comments>ISSN 0975-5578 (Online) 0975-5934 (Print)</comments><journal-ref>Signal &amp; Image Processing: An International Journal (SIPIJ), Vol.4
  No 1, February 2012, Pages: 103-111</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research is taking place to find effective algorithms for content-based image
representation and description. There is a substantial amount of algorithms
available that use visual features (color, shape, texture). Shape feature has
attracted much attention from researchers that there are many shape
representation and description algorithms in literature. These shape image
representation and description algorithms are usually not application
independent or robust, making them undesirable for generic shape description.
This paper presents an object shape representation using Kernel Density Feature
Points Estimator (KDFPE). In this method, the density of feature points within
defined rings around the centroid of the image is obtained. The KDFPE is then
applied to the vector of the image. KDFPE is invariant to translation, scale
and rotation. This method of image representation shows improved retrieval rate
when compared to Density Histogram Feature Points (DHFP) method. Analytic
analysis is done to justify our method, which was compared with the DHFP to
prove its robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5084</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5084</id><created>2012-03-22</created><authors><author><keyname>Derczynski</keyname><forenames>Leon</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Gaizauskas</keyname><forenames>Robert</forenames></author><author><keyname>Greenwood</keyname><forenames>Mark A.</forenames></author></authors><title>A Data Driven Approach to Query Expansion in Question Answering</title><categories>cs.CL cs.IR</categories><journal-ref>Proc. IR4QA Workshop (2008) 34-41</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Automated answering of natural language questions is an interesting and
useful problem to solve. Question answering (QA) systems often perform
information retrieval at an initial stage. Information retrieval (IR)
performance, provided by engines such as Lucene, places a bound on overall
system performance. For example, no answer bearing documents are retrieved at
low ranks for almost 40% of questions.
  In this paper, answer texts from previous QA evaluations held as part of the
Text REtrieval Conferences (TREC) are paired with queries and analysed in an
attempt to identify performance-enhancing words. These words are then used to
evaluate the performance of a query expansion method.
  Data driven extension words were found to help in over 70% of difficult
questions. These words can be used to improve and evaluate query expansion
methods. Simple blind relevance feedback (RF) was correctly predicted as
unlikely to help overall performance, and an possible explanation is provided
for its low value in IR for QA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5086</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5086</id><created>2012-03-22</created><authors><author><keyname>Poroseva</keyname><forenames>Svetlana V.</forenames></author></authors><title>&quot;Selfish&quot; algorithm for optimizing the network survivability analysis</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>31 pages, 2 tables, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Nature, the primary goal of any network is to survive. This is less
obvious for engineering networks (electric power, gas, water, transportation
systems etc.) that are expected to operate under normal conditions most of
time. As a result, the ability of a network to withstand massive sudden damage
caused by adverse events (or survivability) has not been among traditional
goals in the network design. Reality, however, calls for the adjustment of
design priorities. As modern networks develop toward increasing their size,
complexity, and integration, the likelihood of adverse events increases too due
to technological development, climate change, and activities in the political
arena among other factors. Under such circumstances, a network failure has an
unprecedented effect on lives and economy. To mitigate the impact of adverse
events on the network operability, the survivability analysis must be conducted
at the early stage of the network design. Such analysis requires the
development of new analytical and computational tools. Computational analysis
of the network survivability is the exponential time problem at least. The
current paper describes a new algorithm, in which the reduction of the
computational complexity is achieved by mapping an initial network topology
with multiple sources and sinks onto a set of simpler smaller topologies with
multiple sources and a single sink. Steps for further reducing the time and
space expenses of computations are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5099</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5099</id><created>2012-03-22</created><authors><author><keyname>Alaei</keyname><forenames>Saeed</forenames></author><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Haghpanah</keyname><forenames>Nima</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Malekian</keyname><forenames>Azarakhsh</forenames></author></authors><title>Bayesian Optimal Auctions via Multi- to Single-agent Reduction</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an abstract optimal auction problem for a single good or service.
This problem includes environments where agents have budgets, risk preferences,
or multi-dimensional preferences over several possible configurations of the
good (furthermore, it allows an agent's budget and risk preference to be known
only privately to the agent). These are the main challenge areas for auction
theory. A single-agent problem is to optimize a given objective subject to a
constraint on the maximum probability with which each type is allocated,
a.k.a., an allocation rule. Our approach is a reduction from multi-agent
mechanism design problem to collection of single-agent problems. We focus on
maximizing revenue, but our results can be applied to other objectives (e.g.,
welfare).
  An optimal multi-agent mechanism can be computed by a linear/convex program
on interim allocation rules by simultaneously optimizing several single-agent
mechanisms subject to joint feasibility of the allocation rules. For
single-unit auctions, Border \citeyearpar{B91} showed that the space of all
jointly feasible interim allocation rules for $n$ agents is a
$\NumTypes$-dimensional convex polytope which can be specified by $2^\NumTypes$
linear constraints, where $\NumTypes$ is the total number of all agents' types.
Consequently, efficiently solving the mechanism design problem requires a
separation oracle for the feasibility conditions and also an algorithm for
ex-post implementation of the interim allocation rules. We show that the
polytope of jointly feasible interim allocation rules is the projection of a
higher dimensional polytope which can be specified by only $O(\NumTypes^2)$
linear constraints. Furthermore, our proof shows that finding a preimage of the
interim allocation rules in the higher dimensional polytope immediately gives
an ex-post implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5101</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5101</id><created>2012-03-22</created><updated>2012-04-02</updated><authors><author><keyname>Hinrichsen</keyname><forenames>Haye</forenames></author></authors><title>Entropy-based Tuning of Musical Instruments</title><categories>physics.class-ph cs.SD physics.pop-ph</categories><comments>13 pages, 9 figures</comments><journal-ref>Rev. Bras. Ens. Fis. 34(2) (2012), 2301</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human sense of hearing perceives a combination of sounds 'in tune' if the
corresponding harmonic spectra are correlated, meaning that the neuronal
excitation pattern in the inner ear exhibits some kind of order. Based on this
observation it is suggested that musical instruments such as pianos can be
tuned by minimizing the Shannon entropy of suitably preprocessed Fourier
spectra. This method reproduces not only the correct stretch curve but also
similar pitch fluctuations as in the case of high-quality aural tuning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5121</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5121</id><created>2012-03-22</created><updated>2012-03-27</updated><authors><author><keyname>Aoto</keyname><forenames>Takahito</forenames><affiliation>Tohoku University</affiliation></author><author><keyname>Toyama</keyname><forenames>Yoshihito</forenames><affiliation>Tohoku University</affiliation></author></authors><title>A Reduction-Preserving Completion for Proving Confluence of
  Non-Terminating Term Rewriting Systems</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.3.1, F.3.1, F.4.2, I.2.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 28,
  2012) lmcs:667</journal-ref><doi>10.2168/LMCS-8(1:31)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a method to prove confluence of term rewriting systems that contain
non-terminating rewrite rules such as commutativity and associativity. Usually,
confluence of term rewriting systems containing such rules is proved by
treating them as equational term rewriting systems and considering E-critical
pairs and/or termination modulo E. In contrast, our method is based solely on
usual critical pairs and it also (partially) works even if the system is not
terminating modulo E. We first present confluence criteria for term rewriting
systems whose rewrite rules can be partitioned into a terminating part and a
possibly non-terminating part. We then give a reduction-preserving completion
procedure so that the applicability of the criteria is enhanced. In contrast to
the well-known Knuth-Bendix completion procedure which preserves the
equivalence relation of the system, our completion procedure preserves the
reduction relation of the system, by which confluence of the original system is
inferred from that of the completed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5124</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5124</id><created>2012-03-22</created><authors><author><keyname>Khanna</keyname><forenames>Rajiv</forenames></author><author><keyname>Zhang</keyname><forenames>Liang</forenames></author><author><keyname>Agarwal</keyname><forenames>Deepak</forenames></author><author><keyname>Chen</keyname><forenames>Beechung</forenames></author></authors><title>Parallel Matrix Factorization for Binary Response</title><categories>cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting user affinity to items is an important problem in applications
like content optimization, computational advertising, and many more. While
bilinear random effect models (matrix factorization) provide state-of-the-art
performance when minimizing RMSE through a Gaussian response model on explicit
ratings data, applying it to imbalanced binary response data presents
additional challenges that we carefully study in this paper. Data in many
applications usually consist of users' implicit response that are often binary
-- clicking an item or not; the goal is to predict click rates, which is often
combined with other measures to calculate utilities to rank items at runtime of
the recommender systems. Because of the implicit nature, such data are usually
much larger than explicit rating data and often have an imbalanced distribution
with a small fraction of click events, making accurate click rate prediction
difficult. In this paper, we address two problems. First, we show previous
techniques to estimate bilinear random effect models with binary data are less
accurate compared to our new approach based on adaptive rejection sampling,
especially for imbalanced response. Second, we develop a parallel bilinear
random effect model fitting framework using Map-Reduce paradigm that scales to
massive datasets. Our parallel algorithm is based on a &quot;divide and conquer&quot;
strategy coupled with an ensemble approach. Through experiments on the
benchmark MovieLens data, a small Yahoo! Front Page data set, and a large
Yahoo! Front Page data set that contains 8M users and 1B binary observations,
we show that careful handling of binary response as well as identifiability
issues are needed to achieve good performance for click rate prediction, and
that the proposed adaptive rejection sampler and the partitioning as well as
ensemble techniques significantly improve model performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5126</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5126</id><created>2012-03-22</created><authors><author><keyname>Kawadia</keyname><forenames>Vikas</forenames></author><author><keyname>Sreenivasan</keyname><forenames>Sameet</forenames></author></authors><title>Online detection of temporal communities in evolving networks by
  estrangement confinement</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><journal-ref>Scientific Reports 2, Article number: 794, Mar 2012</journal-ref><doi>10.1038/srep00794</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal communities result from a consistent partitioning of nodes across
multiple snapshots of an evolving complex network that can help uncover how
dense clusters in a network emerge, combine, split and decay with time. Current
methods for finding communities in a single snapshot are not straightforwardly
generalizable to finding temporal communities since the quality functions used
for finding static communities have highly degenerate landscapes, and the
eventual partition chosen among the many partitions of similar quality is
highly sensitive to small changes in the network. To reliably detect temporal
communities we need not only to find a good community partition in a given
snapshot but also ensure that it bears some similarity to the partition(s)
found in immediately preceding snapshots. We present a new measure of partition
distance called &quot;estrangement&quot; motivated by the inertia of inter-node
relationships which, when incorporated into the measurement of partition
quality, facilitates the detection of meaningful temporal communities.
Specifically, we propose the estrangement confinement method, which postulates
that neighboring nodes in a community prefer to continue to share community
affiliation as the network evolves. Constraining estrangement enables us to
find meaningful temporal communities at various degrees of temporal smoothness
in diverse real-world datasets. Specifically, we study the evolution of voting
behavior of senators in the United States Congress, the evolution of proximity
in human mobility datasets, and the detection of evolving communities in
synthetic networks that are otherwise hard to find. Estrangement confinement
thus provides a principled approach to uncovering temporal communities in
evolving networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5128</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5128</id><created>2012-03-22</created><updated>2012-08-07</updated><authors><author><keyname>Chaudhury</keyname><forenames>Kunal N.</forenames></author></authors><title>Acceleration of the shiftable O(1) algorithm for bilateral filtering and
  non-local means</title><categories>cs.CV cs.DC</categories><comments>10 figures, 6 tables</comments><journal-ref>IEEE Transactions on Image Processing, vol. 22(4), pp. 1291- 1300,
  2013</journal-ref><doi>10.1109/TIP.2012.2222903</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A direct implementation of the bilateral filter [1] requires O(\sigma_s^2)
operations per pixel, where \sigma_s is the (effective) width of the spatial
kernel. A fast implementation of the bilateral filter was recently proposed in
[2] that required O(1) operations per pixel with respect to \sigma_s. This was
done by using trigonometric functions for the range kernel of the bilateral
filter, and by exploiting their so-called shiftability property. In particular,
a fast implementation of the Gaussian bilateral filter was realized by
approximating the Gaussian range kernel using raised cosines. Later, it was
demonstrated in [3] that this idea could be extended to a larger class of
filters, including the popular non-local means filter [4]. As already observed
in [2], a flip side of this approach was that the run time depended on the
width \sigma_r of the range kernel. For an image with (local) intensity
variations in the range [0,T], the run time scaled as O(T^2/\sigma^2_r) with
\sigma_r. This made it difficult to implement narrow range kernels,
particularly for images with large dynamic range. We discuss this problem in
this note, and propose some simple steps to accelerate the implementation in
general, and for small \sigma_r in particular.
  [1] C. Tomasi and R. Manduchi, &quot;Bilateral filtering for gray and color
images&quot;, Proc. IEEE International Conference on Computer Vision, 1998.
  [2] K.N. Chaudhury, Daniel Sage, and M. Unser, &quot;Fast O(1) bilateral filtering
using trigonometric range kernels&quot;, IEEE Transactions on Image Processing,
2011.
  [3] K.N. Chaudhury, &quot;Constant-time filtering using shiftable kernels&quot;, IEEE
Signal Processing Letters, 2011.
  [4] A. Buades, B. Coll, and J.M. Morel, &quot;A review of image denoising
algorithms, with a new one&quot;, Multiscale Modeling and Simulation, 2005.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5155</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5155</id><created>2012-03-22</created><authors><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>Bayesian Games and the Smoothness Framework</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a general class of Bayesian Games where each players utility
depends on his type (possibly multidimensional) and on the strategy profile and
where players' types are distributed independently. We show that if their full
information version for any fixed instance of the type profile is a smooth game
then the Price of Anarchy bound implied by the smoothness property, carries
over to the Bayes-Nash Price of Anarchy. We show how some proofs from the
literature (item bidding auctions, greedy auctions) can be cast as smoothness
proofs or be simplified using smoothness. For first price item bidding with
fractionally subadditive bidders we actually manage to improve by much the
existing result \cite{Hassidim2011a} from 4 to $\frac{e}{e-1}\approx 1.58$.
This also shows a very interesting separation between first and second price
item bidding since second price item bidding has PoA at least 2 even under
complete information. For a larger class of Bayesian Games where the strategy
space of a player also changes with his type we are able to show that a
slightly stronger definition of smoothness also implies a Bayes-Nash PoA bound.
We show how weighted congestion games actually satisfy this stronger definition
of smoothness. This allows us to show that the inefficiency bounds of weighted
congestion games known in the literature carry over to incomplete versions
where the weights of the players are private information. We also show how an
incomplete version of a natural class of monotone valid utility games, called
effort market games are universally $(1,1)$-smooth. Hence, we show that
incomplete versions of effort market games where the abilities of the players
and their budgets are private information has Bayes-Nash PoA at most 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5156</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5156</id><created>2012-03-22</created><authors><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>Jeon</keyname><forenames>Hyun-Bae</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>A New Low-Complexity Selected Mapping Scheme Using Cyclic Shifted IFFT
  for PAPR Reduction in OFDM Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new peak-to-average power ratio (PAPR) reduction scheme for
orthogonal frequency division multiplexing (OFDM) is proposed based on the
selected mapping (SLM) scheme. The proposed SLM scheme generates alternative
OFDM signal sequences by cyclically shifting the connections in each subblock
at an intermediate stage of inverse fast Fourier transform (IFFT). Compared
with the conventional SLM scheme, the proposed SLM scheme achieves similar PAPR
reduction performance with much lower computational complexity and no bit error
rate (BER) degradation. The performance of the proposed SLM scheme is verified
through numerical analysis. Also, it is shown that the proposed SLM scheme has
the lowest computational complexity among the existing low-complexity SLM
schemes exploiting the signals at an intermediate stage of IFFT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5158</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5158</id><created>2012-03-22</created><updated>2015-02-04</updated><authors><author><keyname>Brunson</keyname><forenames>Jason Cory</forenames></author><author><keyname>Fassino</keyname><forenames>Steve</forenames></author><author><keyname>McInnes</keyname><forenames>Antonio</forenames></author><author><keyname>Narayan</keyname><forenames>Monisha</forenames></author><author><keyname>Richardson</keyname><forenames>Brianna</forenames></author><author><keyname>Franck</keyname><forenames>Christopher</forenames></author><author><keyname>Ion</keyname><forenames>Patrick</forenames></author><author><keyname>Laubenbacher</keyname><forenames>Reinhard</forenames></author></authors><title>Evolutionary Events in a Mathematical Sciences Research Collaboration
  Network</title><categories>physics.soc-ph cs.DL cs.SI math.HO</categories><comments>30 pages, 14 figures, 1 table; supporting information: 5 pages, 5
  figures; published in Scientometrics</comments><msc-class>91D30</msc-class><journal-ref>Scientometrics, June 2014, Volume 99, Issue 3, pp 973-998</journal-ref><doi>10.1007/s11192-013-1209-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines long-term trends and shifting behavior in the
collaboration network of mathematics literature, using a subset of data from
Mathematical Reviews spanning 1985-2009. Rather than modeling the network
cumulatively, this study traces the evolution of the &quot;here and now&quot; using
fixed-duration sliding windows. The analysis uses a suite of common network
diagnostics, including the distributions of degrees, distances, and clustering,
to track network structure. Several random models that call these diagnostics
as parameters help tease them apart as factors from the values of others. Some
behaviors are consistent over the entire interval, but most diagnostics
indicate that the network's structural evolution is dominated by occasional
dramatic shifts in otherwise steady trends. These behaviors are not distributed
evenly across the network; stark differences in evolution can be observed
between two major subnetworks, loosely thought of as &quot;pure&quot; and &quot;applied&quot;,
which approximately partition the aggregate. The paper characterizes two major
events along the mathematics network trajectory and discusses possible
explanatory factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5160</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5160</id><created>2012-03-22</created><updated>2012-09-03</updated><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author><author><keyname>Lee</keyname><forenames>Young Choon</forenames></author><author><keyname>Boloori</keyname><forenames>Ali Javadzadeh</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author></authors><title>Multiple Frequency Selection in DVFS-Enabled Processors to Minimize
  Energy Consumption</title><categories>cs.DC</categories><comments>Chapter 17- Book title: &quot;Energy Efficient Distributed Computing&quot;,
  Edited by Albert Y.Zomaya, Young Choon Lee Wiley</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter we focus on slack reclamation and propose a new slack
reclamation technique, Multiple Frequency Selection DVFS (MFS-DVFS). The key
idea is to execute each task with a linear combination of more than one
frequency such that this combination results in using the lowest energy by
covering the whole slack time of the task. We have tested our algorithm with
both random and real-world application task graphs and compared with the
results in previous researches in [9] and [12-13]. The experimental results
show that our approach can achieve energy almost identical to the optimum
energy saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5161</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5161</id><created>2012-03-22</created><updated>2013-01-09</updated><authors><author><keyname>P&#xf3;sfai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author></authors><title>Effect of correlations on network controllability</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI cs.SY math.OC</categories><journal-ref>Sci. Rep. 3, 1067 (2013)</journal-ref><doi>10.1038/srep01067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dynamical system is controllable if by imposing appropriate external
signals on a subset of its nodes, it can be driven from any initial state to
any desired state in finite time. Here we study the impact of various network
characteristics on the minimal number of driver nodes required to control a
network. We find that clustering and modularity have no discernible impact, but
the symmetries of the underlying matching problem can produce linear, quadratic
or no dependence on degree correlation coefficients, depending on the nature of
the underlying correlations. The results are supported by numerical simulations
and help narrow the observed gap between the predicted and the observed number
of driver nodes in real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5169</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5169</id><created>2012-03-22</created><updated>2013-06-25</updated><authors><author><keyname>Horan</keyname><forenames>Victoria</forenames></author><author><keyname>Hurlbert</keyname><forenames>Glenn</forenames></author></authors><title>Universal Cycles for Weak Orders</title><categories>math.CO cs.DM</categories><comments>12 pages; final version</comments><msc-class>05A05 (Primary) 68R15, 05B30 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal cycles are generalizations of de Bruijn cycles and Gray codes that
were introduced originally by Chung, Diaconis, and Graham in 1990. They have
been developed by many authors since, for various combinatorial objects such as
strings, subsets, permutations, partitions, vector spaces, and designs. One
generalization of universal cycles, which require almost complete overlap of
consecutive words, is s-overlap cycles, which relax such a constraint. In this
paper we study weak orders, which are relations that are transitive and
complete. We prove the existence of universal and s-overlap cycles for weak
orders, as well as for fixed height and/or weight weak orders, and apply the
results to cycles for ordered partitions as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5181</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5181</id><created>2012-03-23</created><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>$k$-MLE: A fast algorithm for learning statistical mixture models</title><categories>cs.LG stat.ML</categories><comments>31 pages, Extend preliminary paper presented at IEEE ICASSP 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe $k$-MLE, a fast and efficient local search algorithm for learning
finite statistical mixtures of exponential families such as Gaussian mixture
models. Mixture models are traditionally learned using the
expectation-maximization (EM) soft clustering technique that monotonically
increases the incomplete (expected complete) likelihood. Given prescribed
mixture weights, the hard clustering $k$-MLE algorithm iteratively assigns data
to the most likely weighted component and update the component models using
Maximum Likelihood Estimators (MLEs). Using the duality between exponential
families and Bregman divergences, we prove that the local convergence of the
complete likelihood of $k$-MLE follows directly from the convergence of a dual
additively weighted Bregman hard clustering. The inner loop of $k$-MLE can be
implemented using any $k$-means heuristic like the celebrated Lloyd's batched
or Hartigan's greedy swap updates. We then show how to update the mixture
weights by minimizing a cross-entropy criterion that implies to update weights
by taking the relative proportion of cluster points, and reiterate the mixture
parameter update and mixture weight update processes until convergence. Hard EM
is interpreted as a special case of $k$-MLE when both the component update and
the weight update are performed successively in the inner loop. To initialize
$k$-MLE, we propose $k$-MLE++, a careful initialization of $k$-MLE guaranteeing
probabilistically a global bound on the best possible complete likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5184</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5184</id><created>2012-03-23</created><updated>2015-05-08</updated><authors><author><keyname>Lenormand</keyname><forenames>Maxime</forenames><affiliation>UR LISC</affiliation></author><author><keyname>Huet</keyname><forenames>Sylvie</forenames><affiliation>UR LISC</affiliation></author><author><keyname>Gargiulo</keyname><forenames>Floriana</forenames><affiliation>INED</affiliation></author><author><keyname>Deffuant</keyname><forenames>Guillaume</forenames><affiliation>UR LISC</affiliation></author></authors><title>A Universal Model of Commuting Networks</title><categories>math.ST cs.SI physics.soc-ph stat.TH</categories><comments>11 pages, 5 figures</comments><proxy>ccsd</proxy><journal-ref>Lenormand, M., Huet, S., Gargiulo, F. &amp; Deffuant, G. (2012) A
  universal model of commuting networks. PLoS ONE, 7, e45985</journal-ref><doi>10.1371/journal.pone.0045985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We test a recently proposed model of commuting networks on 80 case studies
from different regions of the world (Europe and United-States) and with
geographic units of different sizes (municipality, county, region). The model
takes as input the number of commuters coming in and out of each geographic
unit and generates the matrix of commuting flows betwen the geographic units.
We show that the single parameter of the model, which rules the compromise
between the influence of the distance and job opportunities, follows a
universal law that depends only on the average surface of the geographic units.
We verified that the law derived from a part of the case studies yields
accurate results on other case studies. We also show that our model
significantly outperforms the two other approaches proposing a universal
commuting model (Balcan et al. (2009); Simini et al. (2012)), particularly when
the geographic units are small (e.g. municipalities).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5186</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5186</id><created>2012-03-23</created><authors><author><keyname>Guan</keyname><forenames>Yue</forenames></author><author><keyname>Hou</keyname><forenames>Jianfeng</forenames></author><author><keyname>Yang</keyname><forenames>Yingyuan</forenames></author></authors><title>An improved bound on acyclic chromatic index of planar graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proper edge coloring of a graph $G$ is called acyclic if there is no
bichromatic cycle in $G$. The acyclic chromatic index of $G$, denoted by
$\chi'_a(G)$, is the least number of colors $k$ such that $G$ has an acyclic
edge $k$-coloring. Basavaraju et al. [Acyclic edge-coloring of planar graphs,
SIAM J. Discrete Math. 25 (2) (2011), 463--478] showed that $\chi'_a(G)\le
\Delta(G)+12$ for planar graphs $G$ with maximum degree $\Delta(G)$. In this
paper, the bound is improved to $\Delta(G)+10$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5188</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5188</id><created>2012-03-23</created><authors><author><keyname>Hen&#xdf;</keyname><forenames>Stefan</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Mezini</keyname><forenames>Mira</forenames></author></authors><title>Semi-Automatically Extracting FAQs to Improve Accessibility of Software
  Development Knowledge</title><categories>cs.SE cs.CL cs.IR</categories><comments>ICSE - 34th International Conference on Software Engineering (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequently asked questions (FAQs) are a popular way to document software
development knowledge. As creating such documents is expensive, this paper
presents an approach for automatically extracting FAQs from sources of software
development discussion, such as mailing lists and Internet forums, by combining
techniques of text mining and natural language processing. We apply the
approach to popular mailing lists and carry out a survey among software
developers to show that it is able to extract high-quality FAQs that may be
further improved by experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5196</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5196</id><created>2012-03-23</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Pandey</keyname><forenames>Suraj</forenames></author><author><keyname>Vecchiola</keyname><forenames>Christian</forenames></author></authors><title>Market-Oriented Cloud Computing and the Cloudbus Toolkit</title><categories>cs.DC</categories><comments>43 pages, 10 figures</comments><acm-class>D.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has penetrated the Information Technology industry deep
enough to influence major companies to adopt it into their mainstream business.
A strong thrust on the use of virtualization technology to realize
Infrastructure-as-a-Service (IaaS) has led enterprises to leverage
subscription-oriented computing capabilities of public Clouds for hosting their
application services. In parallel, research in academia has been investigating
transversal aspects such as security, software frameworks, quality of service,
and standardization. We believe that the complete realization of the Cloud
computing vision will lead to the introduction of a virtual market where Cloud
brokers, on behalf of end users, are in charge of selecting and composing the
services advertised by different Cloud vendors. In order to make this happen,
existing solutions and technologies have to be redesigned and extended from a
market-oriented perspective and integrated together, giving rise to what we
term Market-Oriented Cloud Computing.
  In this paper, we will assess the current status of Cloud computing by
providing a reference model, discuss the challenges that researchers and IT
practitioners are facing and will encounter in the near future, and present the
approach for solving them from the perspective of the Cloudbus toolkit, which
comprises of a set of technologies geared towards the realization of Market
Oriented Cloud Computing vision. We provide experimental results demonstrating
market-oriented resource provisioning and brokering within a Cloud and across
multiple distributed resources. We also include an application illustrating the
hosting of ECG analysis as SaaS on Amazon IaaS (EC2 and S3) services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5218</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5218</id><created>2012-03-23</created><authors><author><keyname>Mokken</keyname><forenames>Robert J.</forenames></author></authors><title>Coteries, Social Circles and Hamlets Close Communities: A Study of
  Acquaintance Networks</title><categories>cs.SI cs.DM math.CO physics.soc-ph</categories><comments>Keywords: Social networks, acquaintance networks, close communities,
  cliques, k-clubs, 2-clubs, diameter 2, shortest spanning trees, girth</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the analysis of social networks many relatively loose and heuristic
definitions of 'community' abound. In this paper the concept of closely knit
communities is studied as defined by the property that every pair of its
members are neighbors or has at least one common neighbor, where the
neighboring relationship is based on some more or less durable and stable
acquaintance or contact relation. In this paper these are studied in the form
of graphs or networks of diameter two (2-clubs). Their structure can be
characterized by investigating shortest spanning trees and girth leading to a
typology containing just three or, in combination, six types of close
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5235</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5235</id><created>2012-03-23</created><authors><author><keyname>Wu</keyname><forenames>Bang Ye</forenames></author><author><keyname>Guo</keyname><forenames>Jun-Lin</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>A linear time algorithm for the next-to-shortest path problem on
  undirected graphs with nonnegative edge lengths</title><categories>cs.DS</categories><msc-class>68R10, 68W05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For two vertices $s$ and $t$ in a graph $G=(V,E)$, the next-to-shortest path
is an $st$-path which length is minimum amongst all $st$-paths strictly longer
than the shortest path length. In this paper we show that, when the graph is
undirected and all edge lengths are nonnegative, the problem can be solved in
linear time if the distances from $s$ and $t$ to all other vertices are given.
This result generalizes the previous work (DOI 10.1007/s00453-011-9601-7) to
allowing zero-length edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5244</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5244</id><created>2012-03-23</created><updated>2012-10-22</updated><authors><author><keyname>Leducq</keyname><forenames>Elodie</forenames></author></authors><title>Second weight codewords of generalized Reed-Muller codes</title><categories>math.NT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give the second weight codewords of the generalized
Reed-Muller code of order r and length $q^m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5255</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5255</id><created>2012-03-23</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Alwani</keyname><forenames>Mohammad</forenames></author></authors><title>Post-Editing Error Correction Algorithm for Speech Recognition using
  Bing Spelling Suggestion</title><categories>cs.CL</categories><comments>LACSC - Lebanese Association for Computational Sciences -
  http://www.lacsc.org</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Vol.3, No.2, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ASR short for Automatic Speech Recognition is the process of converting a
spoken speech into text that can be manipulated by a computer. Although ASR has
several applications, it is still erroneous and imprecise especially if used in
a harsh surrounding wherein the input speech is of low quality. This paper
proposes a post-editing ASR error correction method and algorithm based on
Bing's online spelling suggestion. In this approach, the ASR recognized output
text is spell-checked using Bing's spelling suggestion technology to detect and
correct misrecognized words. More specifically, the proposed algorithm breaks
down the ASR output text into several word-tokens that are submitted as search
queries to Bing search engine. A returned spelling suggestion implies that a
query is misspelled; and thus it is replaced by the suggested correction;
otherwise, no correction is performed and the algorithm continues with the next
token until all tokens get validated. Experiments carried out on various
speeches in different languages indicated a successful decrease in the number
of ASR errors and an improvement in the overall error correction rate. Future
research can improve upon the proposed algorithm so much so that it can be
parallelized to take advantage of multiprocessor computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5259</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5259</id><created>2012-03-23</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Semaan</keyname><forenames>Paul</forenames></author></authors><title>Autonomic Model for Self-Configuring C#.NET Applications</title><categories>cs.OH</categories><comments>LACSC - Lebanese Association for Computational Sciences -
  http://www.lacsc.org</comments><journal-ref>International Journal of Research Studies in Computing, Vol.1,
  No.1, pp.21-34, April 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advances in computational technologies over the last decade, large
organizations have been investing in Information Technology to automate their
internal processes to cut costs and efficiently support their business
projects. However, this comes to a price. Business requirements always change.
Likewise, IT systems constantly evolves as developers make new versions of
them, which require endless administrative manual work to customize and
configure them, especially if they are being used in different contexts, by
different types of users, and for different requirements. Autonomic computing
was conceived to provide an answer to these ever-changing requirements.
Essentially, autonomic systems are self-configuring, self-healing,
self-optimizing, and self-protecting; hence, they can automate all complex IT
processes without human intervention. This paper proposes an autonomic model
based on Venn diagram and set theory for self-configuring C#.NET applications,
namely the self-customization of their GUI, event-handlers, and security
permissions. The proposed model does not require altering the source-code of
the original application; rather, it uses an XML-based customization file to
turn on and off the internal attributes of the application. Experiments
conducted on the proposed model, showed a successful automatic customization
for C# applications and an effective self-adaption based on dynamic business
requirements. As future work, other programming languages such as Java and C++
are to be supported, in addition to other operating systems such as Linux and
Mac so as to provide a standard platform-independent autonomic self-configuring
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5262</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5262</id><created>2012-03-23</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Semaan</keyname><forenames>Paul</forenames></author></authors><title>ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset</title><categories>cs.CL</categories><comments>LACSC - Lebanese Association for Computational Sciences -
  http://www.lacsc.org</comments><journal-ref>Journal of Computing, Vol.4, No.1, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the present time, computers are employed to solve complex tasks and
problems ranging from simple calculations to intensive digital image processing
and intricate algorithmic optimization problems to computationally-demanding
weather forecasting problems. ASR short for Automatic Speech Recognition is yet
another type of computational problem whose purpose is to recognize human
spoken speech and convert it into text that can be processed by a computer.
Despite that ASR has many versatile and pervasive real-world applications,it is
still relatively erroneous and not perfectly solved as it is prone to produce
spelling errors in the recognized text, especially if the ASR system is
operating in a noisy environment, its vocabulary size is limited, and its input
speech is of bad or low quality. This paper proposes a post-editing ASR error
correction method based on MicrosoftN-Gram dataset for detecting and correcting
spelling errors generated by ASR systems. The proposed method comprises an
error detection algorithm for detecting word errors; a candidate corrections
generation algorithm for generating correction suggestions for the detected
word errors; and a context-sensitive error correction algorithm for selecting
the best candidate for correction. The virtue of using the Microsoft N-Gram
dataset is that it contains real-world data and word sequences extracted from
the web which canmimica comprehensive dictionary of words having a large and
all-inclusive vocabulary. Experiments conducted on numerous speeches, performed
by different speakers, showed a remarkable reduction in ASR errors. Future
research can improve upon the proposed algorithm so much so that it can be
parallelized to take advantage of multiprocessor and distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5279</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5279</id><created>2012-03-23</created><authors><author><keyname>Saxton</keyname><forenames>Gregory D.</forenames></author><author><keyname>Guo</keyname><forenames>Chao</forenames></author><author><keyname>Chiu</keyname><forenames>I-Hsuan</forenames></author><author><keyname>Feng</keyname><forenames>Bo</forenames></author></authors><title>Social Media and the Social Good: How Nonprofits Use Facebook to
  Communicate with the Public</title><categories>cs.CY cs.HC</categories><comments>Chinese-language article</comments><journal-ref>China Third Sector Research, Vol. 1, pp. 40-54, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we examine the social networking practices of the 100 largest
nonprofit organizations in the United States. More specifically, we develop a
comprehensive classification scheme to delineate these organizations' use of
Facebook as a stakeholder engagement tool. We find that there are 5 primary
categories of Facebook &quot;statuses&quot;, which can be aggregated into three key
dimensions - &quot;information&quot;, &quot;community&quot;, and &quot;action&quot;. Our analysis reveals
that, though the &quot;informational&quot; use of Facebook is still significant,
nonprofit organizations are better at using Facebook to strategically engage
their stakeholders via &quot;dialogic&quot; and &quot;community-building&quot; practices than they
have been with traditional websites. The adoption of social media seems to have
engendered new paradigms of public engagement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5303</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5303</id><created>2012-03-23</created><authors><author><keyname>Zuleger</keyname><forenames>Florian</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author><author><keyname>Sinn</keyname><forenames>Moritz</forenames></author><author><keyname>Veith</keyname><forenames>Helmut</forenames></author></authors><title>Bound Analysis of Imperative Programs with the Size-change Abstraction
  (extended version)</title><categories>cs.PL</categories><comments>Extended version of SAS 2011 conference article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The size-change abstraction (SCA) is an important program abstraction for
termination analysis, which has been successfully implemented in many tools for
functional and logic programs. In this paper, we demonstrate that SCA is also a
highly effective abstract domain for the bound analysis of imperative programs.
  We have implemented a bound analysis tool based on SCA for imperative
programs. We abstract programs in a pathwise and context dependent manner,
which enables our tool to analyze real-world programs effectively. Our work
shows that SCA captures many of the essential ideas of previous termination and
bound analysis and goes beyond in a conceptually simpler framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5323</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5323</id><created>2012-03-23</created><authors><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>Parameterized Proof Complexity and W[1]</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a program of parameterized proof complexity that aims to provide
evidence that FPT is different from W[1]. A similar program already exists for
the classes W[2] and W[SAT]. We contrast these programs and prove upper and
lower bounds for W[1]-parameterized Resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5324</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5324</id><created>2012-03-23</created><authors><author><keyname>Vaz</keyname><forenames>Paula Cristina</forenames></author><author><keyname>de Matos</keyname><forenames>David Martins</forenames></author><author><keyname>Martins</keyname><forenames>Bruno</forenames></author><author><keyname>Calado</keyname><forenames>Pavel</forenames></author></authors><title>Improving an Hybrid Literary Book Recommendation System through Author
  Ranking</title><categories>cs.IR cs.DL</categories><comments>Submitted to JCDL 2012</comments><acm-class>H.3.3; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Literary reading is an important activity for individuals and choosing to
read a book can be a long time commitment, making book choice an important task
for book lovers and public library users. In this paper we present an hybrid
recommendation system to help readers decide which book to read next. We study
book and author recommendation in an hybrid recommendation setting and test our
approach in the LitRec data set. Our hybrid book recommendation approach
purposed combines two item-based collaborative filtering algorithms to predict
books and authors that the user will like. Author predictions are expanded in
to a book list that is subsequently aggregated with the former list generated
through the initial collaborative recommender. Finally, the resulting book list
is used to yield the top-n book recommendations. By means of various
experiments, we demonstrate that author recommendation can improve overall book
recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5325</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5325</id><created>2012-03-23</created><updated>2013-01-21</updated><authors><author><keyname>Xie</keyname><forenames>Hongmei</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Exact-Repair Minimum Bandwidth Regenerating Codes Based on Evaluation of
  Linearized Polynomials</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to incorrect
  statements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two new constructions of exact-repair minimum
storage regenerating (exact-MBR) codes. Both constructions obtain the encoded
symbols by first treating the message vector over GF(q) as a linearized
polynomial and then evaluating it over an extension field GF(q^m). The
evaluation points are chosen so that the encoded symbols at any node are
conjugates of each other, while corresponding symbols of different nodes are
linearly dependent with respect to GF(q). These properties ensure that data
repair can be carried out over the base field GF(q), instead of matrix
inversion over the extension field required by some existing exact-MBR codes.
To the best of our knowledge, this approach is novel in the construction of
exact-MBR codes. One of our constructions leads to exact-MBR codes with
arbitrary parameters. These exact-MBR codes have higher data reconstruction
complexities but lower data repair complexities than their counterparts based
on the product-matrix approach; hence they may be suitable for applications
that need a small number of data reconstructions but a large number of data
repairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5349</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5349</id><created>2012-03-22</created><authors><author><keyname>Menezo</keyname><forenames>Lucia G.</forenames></author><author><keyname>Puente</keyname><forenames>Valentin</forenames></author><author><keyname>Gregorio</keyname><forenames>Jose-Angel</forenames></author></authors><title>LOCKE Detailed Specification Tables</title><categories>cs.AR</categories><comments>3 pages</comments><acm-class>B.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document shows the detailed specification of LOCKE coherence protocol
for each cache controller, using a table-based technique. This representation
provides clear, concise visual information yet includes sufficient detail
(e.g., transient states) arguably lacking in the traditional, graphical form of
state diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5350</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5350</id><created>2012-03-23</created><authors><author><keyname>Polach</keyname><forenames>Frantisek</forenames></author></authors><title>Public-Key Cryptography Based on Modular Lattices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to generalization of practical Identity-Based
Encryption scheme of Boneh and Franklin. In particular we show how the protocol
could be used on finite modular lattices and as a special case on vector spaces
over finite field. The original proof of security for this protocol does not
hold in this general algebraic structure, thus this is still a work in
progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5351</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5351</id><created>2012-03-23</created><updated>2012-06-26</updated><authors><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Activity driven modeling of time varying networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>10 pages, 4 figures</comments><journal-ref>Nature Scientific Reports 2, 469 (2012)</journal-ref><doi>10.1038/srep00469</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network modeling plays a critical role in identifying statistical
regularities and structural principles common to many systems. The large
majority of recent modeling approaches are connectivity driven. The structural
patterns of the network are at the basis of the mechanisms ruling the network
formation. Connectivity driven models necessarily provide a time-aggregated
representation that may fail to describe the instantaneous and fluctuating
dynamics of many networks. We address this challenge by defining the activity
potential, a time invariant function characterizing the agents' interactions
and constructing an activity driven model capable of encoding the instantaneous
time description of the network dynamics. The model provides an explanation of
structural features such as the presence of hubs, which simply originate from
the heterogeneous activity of agents. Within this framework, highly dynamical
networks can be described analytically, allowing a quantitative discussion of
the biases induced by the time-aggregated representations in the analysis of
dynamical processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5353</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5353</id><created>2012-03-23</created><authors><author><keyname>Jiraskova</keyname><forenames>Galina</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>The state complexity of star-complement-star</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We resolve an open question by determining matching (asymptotic) upper and
lower bounds on the state complexity of the operation that sends a language L
to (c(L*))*, where c() denotes complement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5362</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5362</id><created>2012-03-23</created><updated>2012-03-26</updated><authors><author><keyname>Karaca</keyname><forenames>Mehmet</forenames></author><author><keyname>Sarikaya</keyname><forenames>Yunus</forenames></author><author><keyname>Ercetin</keyname><forenames>Ozgur</forenames></author><author><keyname>Alpcan</keyname><forenames>Tansu</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>Throughput Optimal Scheduling with Dynamic Channel Feedback</title><categories>cs.NI cs.IT math.IT</categories><comments>submitted</comments><journal-ref>Wireless Communications, IEEE Transactions on vol.12, No:6, 2013</journal-ref><doi>10.1109/TWC.2013.041713.121460</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that opportunistic scheduling algorithms are throughput
optimal under full knowledge of channel and network conditions. However, these
algorithms achieve a hypothetical achievable rate region which does not take
into account the overhead associated with channel probing and feedback required
to obtain the full channel state information at every slot. We adopt a channel
probing model where $\beta$ fraction of time slot is consumed for acquiring the
channel state information (CSI) of a single channel. In this work, we design a
joint scheduling and channel probing algorithm named SDF by considering the
overhead of obtaining the channel state information. We first analytically
prove SDF algorithm can support $1+\epsilon$ fraction of of the full rate
region achieved when all users are probed where $\epsilon$ depends on the
expected number of users which are not probed. Then, for homogenous channel, we
show that when the number of users in the network is greater than 3, $\epsilon
&gt; 0$, i.e., we guarantee to expand the rate region. In addition, for
heterogenous channels, we prove the conditions under which SDF guarantees to
increase the rate region. We also demonstrate numerically in a realistic
simulation setting that this rate region can be achieved by probing only less
than 50% of all channels in a CDMA based cellular network utilizing high data
rate protocol under normal channel conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5378</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5378</id><created>2012-03-23</created><authors><author><keyname>Noshad</keyname><forenames>Mohammad</forenames></author><author><keyname>Brandt-Pearce</keyname><forenames>Maite</forenames></author></authors><title>Expurgated PPM Using Symmetric Balanced Incomplete Block Designs</title><categories>cs.IT math.IT physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose a new pulse position modulation (PPM) scheme,
called expurgated PPM (EPPM), for application in peak power limited
communication systems, such as impulse radio (IR) ultra wide band (UWB) systems
and free space optical (FSO) communications. Using the proposed scheme, the
constellation size and the bit-rate can be increased significantly in these
systems. The symbols are obtained using symmetric balanced incomplete block
designs (BIBD), forming a set of pair-wise equidistance symbols. The
performance of Q-ary EPPM is better than any Q-ary pulse position-based
modulation scheme with the same symbol length. Since the code is cyclic, the
receiver for EPPM is simpler compared to multipulse PPM (MPPM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5387</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5387</id><created>2012-03-24</created><updated>2012-11-12</updated><authors><author><keyname>Rastogi</keyname><forenames>Vibhor</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Chitnis</keyname><forenames>Laukik</forenames></author><author><keyname>Sarma</keyname><forenames>Anish Das</forenames></author></authors><title>Finding Connected Components on Map-reduce in Logarithmic Rounds</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a large graph G = (V,E) with millions of nodes and edges, how do we
compute its connected components efficiently? Recent work addresses this
problem in map-reduce, where a fundamental trade-off exists between the number
of map-reduce rounds and the communication of each round. Denoting d the
diameter of the graph, and n the number of nodes in the largest component, all
prior map-reduce techniques either require d rounds, or require about n|V| +
|E| communication per round. We propose two randomized map-reduce algorithms --
(i) Hash-Greater-To-Min, which provably requires at most 3log(n) rounds with
high probability, and at most 2(|V| + |E|) communication per round, and (ii)
Hash-to-Min, which has a worse theoretical complexity, but in practice
completes in at most 2log(d) rounds and 3(|V| + |E|) communication per rounds.
  Our techniques for connected components can be applied to clustering as well.
We propose a novel algorithm for agglomerative single linkage clustering in
map-reduce. This is the first algorithm that can provably compute a clustering
in at most O(log(n)) rounds, where n is the size of the largest cluster. We
show the effectiveness of all our algorithms through detailed experiments on
large synthetic as well as real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5395</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5395</id><created>2012-03-24</created><updated>2013-12-08</updated><authors><author><keyname>Firooz</keyname><forenames>Mohammad Hamed</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author></authors><title>Data Dissemination in Wireless Networks with Network Coding</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Communications Letters, Volume:17 , Issue: 5, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of network coding for information dissemination over a
wireless network. Using network coding allows for a simple, distributed and
robust algorithm where nodes do not need any information from their neighbors.
In this paper, we analyze the time needed to diffuse information throughout a
network when network coding is implemented at all nodes. We then provide an
upper bound for the dissemination time for ad-hoc networks with general
topology. Moreover, we derive a relation between dissemination time and the
size of the wireless network. It is shown that for a wireless network with N
nodes, the dissemination latency is between O(N) and O(N^2), depending on the
reception probabilities of the nodes. These observations are validated by the
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5399</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5399</id><created>2012-03-24</created><authors><author><keyname>Ben-Zvi</keyname><forenames>Ido</forenames></author><author><keyname>Moses</keyname><forenames>Yoram</forenames></author></authors><title>Agent-time Epistemics and Coordination</title><categories>cs.MA cs.DC cs.LO</categories><comments>30 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A minor change to the standard epistemic logical language, replacing $K_{i}$
with $K_{\node{i,t}}$ where $t$ is a time instance, gives rise to a generalized
and more expressive form of knowledge and common knowledge operators. We
investigate the communication structures that are necessary for such
generalized epistemic states to arise, and the inter-agent coordination tasks
that require such knowledge. Previous work has established a relation between
linear event ordering and nested knowledge, and between simultaneous event
occurrences and common knowledge. In the new, extended, formalism, epistemic
necessity is decoupled from temporal necessity. Nested knowledge and event
ordering are shown to be related even when the nesting order does not match the
temporal order of occurrence. The generalized form of common knowledge does
{\em not} correspond to simultaneity. Rather, it corresponds to a notion of
tight coordination, of which simultaneity is an instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5403</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5403</id><created>2012-03-24</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Distributed, Cross-Platform, and Regression Testing Architecture for
  Service-Oriented Architecture</title><categories>cs.SE</categories><comments>ISSN: 2166-2924; LACSC - Lebanese Association for Computational
  Sciences - http://www.lacsc.org; Advances in Computer Science and its
  Applications (ACSA), Vol. 1, No. 1, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As per leading IT experts, today's large enterprises are going through
business transformations. They are adopting service-based IT models such as SOA
to develop their enterprise information systems and applications. In fact, SOA
is an integration of loosely-coupled interoperable components, possibly built
using heterogeneous software technologies and hardware platforms. As a result,
traditional testing architectures are no more adequate for verifying and
validating the quality of SOA systems and whether they are operating to
specifications. This paper first discusses the various state-of-the-art methods
for testing SOA applications, and then it proposes a novel automated,
distributed, cross-platform, and regression testing architecture for SOA
systems. The proposed testing architecture consists of several testing units
which include test engine, test code generator, test case generator, test
executer, and test monitor units. Experiments conducted showed that the
proposed testing architecture managed to use parallel agents to test
heterogeneous web services whose technologies were incompatible with the
testing framework. As future work, testing non-functional aspects of SOA
applications are to be investigated so as to allow the testing of such
properties as performance, security, availability, and scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5414</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5414</id><created>2012-03-24</created><updated>2012-04-15</updated><authors><author><keyname>Jukna</keyname><forenames>S.</forenames></author></authors><title>Clique problem, cutting plane proofs and communication complexity</title><categories>cs.CC cs.DM</categories><comments>10 pages. Theorem 1 in the previous version holds only for bipartite
  graphs, the non-bipartite case remains open. I now separate the bipartite and
  non-bipartite cases (by switching from independent sets to cliques, hence a
  new title). Some new open problems as well as references are added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by its relation to the length of cutting plane proofs for the
Maximum Biclique problem, we consider the following communication game on a
given graph G, known to both players. Let K be the maximal number of vertices
in a complete bipartite subgraph of G, which is not necessarily an induced
subgraph if G is not bipartite. Alice gets a set A of vertices, and Bob gets a
disjoint set B of vertices such that |A|+|B|&gt;K. The goal is to find a nonedge
of G between A and B. We show that O(\log n) bits of communication are enough
for every n-vertex graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5415</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5415</id><created>2012-03-24</created><authors><author><keyname>Wang</keyname><forenames>Yongji</forenames></author><author><keyname>Liao</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Wu</keyname><forenames>Hu</forenames></author><author><keyname>Wu</keyname><forenames>Jingzheng</forenames></author></authors><title>Incremental Collaborative Filtering Considering Temporal Effects</title><categories>cs.IR</categories><comments>18 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems require their recommendation algorithms to be accurate,
scalable and should handle very sparse training data which keep changing over
time. Inspired by ant colony optimization, we propose a novel collaborative
filtering scheme: Ant Collaborative Filtering that enjoys those favorable
characteristics above mentioned. With the mechanism of pheromone transmission
between users and items, our method can pinpoint most relative users and items
even in face of the sparsity problem. By virtue of the evaporation of existing
pheromone, we capture the evolution of user preference over time. Meanwhile,
the computation complexity is comparatively small and the incremental update
can be done online. We design three experiments on three typical recommender
systems, namely movie recommendation, book recommendation and music
recommendation, which cover both explicit and implicit rating data. The results
show that the proposed algorithm is well suited for real-world recommendation
scenarios which have a high throughput and are time sensitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5422</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5422</id><created>2012-03-24</created><authors><author><keyname>Lei</keyname><forenames>Jing</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Distribution Free Prediction Bands</title><categories>stat.ME cs.LG math.ST stat.TH</categories><comments>28 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distribution free, nonparametric prediction bands with a special
focus on their finite sample behavior. First we investigate and develop
different notions of finite sample coverage guarantees. Then we give a new
prediction band estimator by combining the idea of &quot;conformal prediction&quot; (Vovk
et al. 2009) with nonparametric conditional density estimation. The proposed
estimator, called COPS (Conformal Optimized Prediction Set), always has finite
sample guarantee in a stronger sense than the original conformal prediction
estimator. Under regularity conditions the estimator converges to an oracle
band at a minimax optimal rate. A fast approximation algorithm and a data
driven method for selecting the bandwidth are developed. The method is
illustrated first in simulated data. Then, an application shows that the
proposed method gives desirable prediction intervals in an automatic way, as
compared to the classical linear regression modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5423</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5423</id><created>2012-03-24</created><authors><author><keyname>della Rocca</keyname><forenames>Simona Ronchi</forenames><affiliation>UNITO</affiliation></author><author><keyname>Pimentel</keyname><forenames>Elaine</forenames><affiliation>UFMG</affiliation></author></authors><title>Proceedings 6th Workshop on Logical and Semantic Frameworks with
  Applications</title><categories>cs.LO cs.CC cs.PL</categories><proxy>EPTCS</proxy><acm-class>F.3.1, F.3.2, F.4.1</acm-class><journal-ref>EPTCS 81, 2012</journal-ref><doi>10.4204/EPTCS.81</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Sixth Workshop on Logical and
Semantic Frameworks with Applications (LSFA 2011). The workshop will be hold in
Belo Horizonte, on August 27th 2011.
  Logical and semantic frameworks are formal languages used to represent
logics, languages and systems. These frameworks provide foundations for formal
specification of systems and programming languages, supporting tool development
and reasoning.
  The objective of this one-day workshop is to put together theoreticians and
practitioners to promote new techniques and results, from the theoretical side,
and feedback on the implementation and the use of such techniques and results,
from the practical side.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5438</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5438</id><created>2012-03-24</created><authors><author><keyname>Richard</keyname><forenames>Emile</forenames></author><author><keyname>Argyriou</keyname><forenames>Andreas</forenames></author><author><keyname>Evgeniou</keyname><forenames>Theodoros</forenames></author><author><keyname>Vayatis</keyname><forenames>Nicolas</forenames></author></authors><title>A Regularization Approach for Prediction of Edges and Node Features in
  Dynamic Graphs</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the two problems of predicting links in a dynamic graph sequence
and predicting functions defined at each node of the graph. In many
applications, the solution of one problem is useful for solving the other.
Indeed, if these functions reflect node features, then they are related through
the graph structure. In this paper, we formulate a hybrid approach that
simultaneously learns the structure of the graph and predicts the values of the
node-related functions. Our approach is based on the optimization of a joint
regularization objective. We empirically test the benefits of the proposed
method with both synthetic and real data. The results indicate that joint
regularization improves prediction performance over the graph evolution and the
node features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5443</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5443</id><created>2012-03-24</created><updated>2012-06-21</updated><authors><author><keyname>Pelikan</keyname><forenames>Martin</forenames></author><author><keyname>Hauschild</keyname><forenames>Mark W.</forenames></author><author><keyname>Lanzi</keyname><forenames>Pier Luca</forenames></author></authors><title>Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA</title><categories>cs.NE cs.AI cs.LG</categories><comments>Accepted at Parallel Problem Solving from Nature (PPSN XII), 10
  pages. arXiv admin note: substantial text overlap with arXiv:1201.2241</comments><report-no>MEDAL Report No. 2012004</report-no><acm-class>I.2.6; I.2.8; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An automated technique has recently been proposed to transfer learning in the
hierarchical Bayesian optimization algorithm (hBOA) based on distance-based
statistics. The technique enables practitioners to improve hBOA efficiency by
collecting statistics from probabilistic models obtained in previous hBOA runs
and using the obtained statistics to bias future hBOA runs on similar problems.
The purpose of this paper is threefold: (1) test the technique on several
classes of NP-complete problems, including MAXSAT, spin glasses and minimum
vertex cover; (2) demonstrate that the technique is effective even when
previous runs were done on problems of different size; (3) provide empirical
evidence that combining transfer learning with other efficiency enhancement
techniques can often yield nearly multiplicative speedups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5446</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5446</id><created>2012-03-24</created><authors><author><keyname>Lauret</keyname><forenames>Philippe</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Rodler</keyname><forenames>Auline</forenames><affiliation>SPE</affiliation></author><author><keyname>Muselli</keyname><forenames>Marc</forenames><affiliation>SPE</affiliation></author><author><keyname>David</keyname><forenames>Mathieu</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Diagne</keyname><forenames>Hadja</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Voyant</keyname><forenames>Cyril</forenames><affiliation>SPE, CHD Castellucio</affiliation></author></authors><title>A Bayesian Model Committee Approach to Forecasting Global Solar
  Radiation</title><categories>stat.AP cs.LG</categories><comments>WREF 2012 : World Renewable Energy Forum, Denver : United States
  (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to use a rather new modelling approach in the realm of
solar radiation forecasting. In this work, two forecasting models:
Autoregressive Moving Average (ARMA) and Neural Network (NN) models are
combined to form a model committee. The Bayesian inference is used to affect a
probability to each model in the committee. Hence, each model's predictions are
weighted by their respective probability. The models are fitted to one year of
hourly Global Horizontal Irradiance (GHI) measurements. Another year (the test
set) is used for making genuine one hour ahead (h+1) out-of-sample forecast
comparisons. The proposed approach is benchmarked against the persistence
model. The very first results show an improvement brought by this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5451</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5451</id><created>2012-03-24</created><authors><author><keyname>Fliss</keyname><forenames>Imtiez</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>Multiple faults diagnosis using causal graph</title><categories>cs.SY</categories><journal-ref>In the proceedings of The 6th IEEE International Multi- Conference
  On Systems, Signals Devices- SSD'09, Djerba, Tunisia, 23-26 Mars 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes to put up a tool for diagnosing multi faults based on
model using techniques of detection and localization inspired from the
community of artificial intelligence and that of automatic. The diagnostic
procedure to be integrated into the supervisory system must therefore be
provided with explanatory features. Techniques based on causal reasoning are a
pertinent approach for this purpose. Bond graph modeling is used to describe
the cause effect relationship between process variables. Experimental results
are presented and discussed in order to compare performance of causal graph
technique and classic methods inspired from artificial intelligence (DX) and
control theory (FDI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5452</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5452</id><created>2012-03-24</created><authors><author><keyname>Yahia</keyname><forenames>Nesrine Ben</forenames></author><author><keyname>Bellamine</keyname><forenames>Narj&#xe8;s</forenames></author><author><keyname>Ghezala</keyname><forenames>Henda Ben</forenames></author></authors><title>Modeling of Mixed Decision Making Process</title><categories>cs.AI</categories><comments>Keywords-collaborative knowledge management; mixed decision making;
  dynamicity of actors; UML-G</comments><journal-ref>In Proceedings of IEEE International Conference on Information
  Technology and e-Services 2012, pp. 555-559 ISBN: 978-9938-9511-1-0</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Decision making whenever and wherever it is happened is key to organizations
success. In order to make correct decision, individuals, teams and
organizations need both knowledge management (to manage content) and
collaboration (to manage group processes) to make that more effective and
efficient. In this paper, we explain the knowledge management and collaboration
convergence. Then, we propose a formal description of mixed and multimodal
decision making (MDM) process where decision may be made by three possible
modes: individual, collective or hybrid. Finally, we explicit the MDM process
based on UML-G profile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5453</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5453</id><created>2012-03-24</created><authors><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author></authors><title>Optimal Private Halfspace Counting via Discrepancy</title><categories>cs.DS cs.CG</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A range counting problem is specified by a set $P$ of size $|P| = n$ of
points in $\mathbb{R}^d$, an integer weight $x_p$ associated to each point $p
\in P$, and a range space ${\cal R} \subseteq 2^{P}$. Given a query range $R
\in {\cal R}$, the target output is $R(\vec{x}) = \sum_{p \in R}{x_p}$. Range
counting for different range spaces is a central problem in Computational
Geometry.
  We study $(\epsilon, \delta)$-differentially private algorithms for range
counting. Our main results are for the range space given by hyperplanes, that
is, the halfspace counting problem. We present an $(\epsilon,
\delta)$-differentially private algorithm for halfspace counting in $d$
dimensions which achieves $O(n^{1-1/d})$ average squared error. This contrasts
with the $\Omega(n)$ lower bound established by the classical result of Dinur
and Nissim [PODS 2003] for arbitrary subset counting queries. We also show a
matching lower bound on average squared error for any $(\epsilon,
\delta)$-differentially private algorithm for halfspace counting. Both bounds
are obtained using discrepancy theory. For the lower bound, we use a modified
discrepancy measure and bound approximation of $(\epsilon,
\delta)$-differentially private algorithms for range counting queries in terms
of this discrepancy. We also relate the modified discrepancy measure to
classical combinatorial discrepancy, which allows us to exploit known
discrepancy lower bounds. This approach also yields a lower bound of
$\Omega((\log n)^{d-1})$ for $(\epsilon, \delta)$-differentially private
orthogonal range counting in $d$ dimensions, the first known superconstant
lower bound for this problem. For the upper bound, we use an approach inspired
by partial coloring methods for proving discrepancy upper bounds, and obtain
$(\epsilon, \delta)$-differentially private algorithms for range counting with
polynomially bounded shatter function range spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5454</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5454</id><created>2012-03-24</created><authors><author><keyname>fliss</keyname><forenames>Imtiez</forenames></author><author><keyname>Tagina</keyname><forenames>Moncef</forenames></author></authors><title>A Novel Fault Detection Approach combining Adaptive Thresholding and
  Fuzzy Reasoning</title><categories>cs.SY</categories><journal-ref>In the proceedings of The 3rd IEEE International Conference
  Electrical Engineering Design and Technologies (ICEEDT'09), Sousse, Tunisia,
  31 Octobre-02 Novembre 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault detection methods have their pros and cons. Thus, it is possible that
some methods can complement each other and offer consequently better diagnostic
systems. The integration of various characteristics is a way to develop
&quot;hybrid&quot; systems to overcome the limitations of individual strategies of each
method. In this paper a novel detection module combining the use of adaptive
threshold and fuzzy logic reasoning inspired by the Evsukoff's approach is
proposed in order to reduce the rate of false alarms, guarantee more robustness
to disturbances and assist the operator in making decisions. The proposed
approach can be used in case of multiple faults detection. This approach is
applied to a benchmark in diagnosis domain: the three-tank system. The results
of the proposed detection module are then presented through a gradual palette
of colors in the graphical interface of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5464</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5464</id><created>2012-03-24</created><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author></authors><title>A note on triangle partitions</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Koivisto studied the partitioning of sets of bounded cardinality. We improve
his time analysis somewhat, for the special case of triangle partitions, and
obtain a slight improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5467</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5467</id><created>2012-03-24</created><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Ou</keyname><forenames>Rong</forenames></author><author><keyname>Wong</keyname><forenames>Kwok-Wo</forenames></author></authors><title>Breaking a novel colour image encryption algorithm based on chaos</title><categories>cs.CR</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a colour image encryption algorithm based on chaos was proposed by
cascading two position permutation operations and one substitution operation,
which are all determined by some pseudo-random number sequences generated by
iterating the Logistic map. This paper evaluates the security level of the
encryption algorithm and finds that the position permutation-only part and the
substitution part can be separately broken with only $\lceil (\log_2(3MN))/8
\rceil$ and 2 chosen plain-images, respectively, where $MN$ is the size of the
plain-image. Concise theoretical analyses are provided to support the
chosen-plaintext attack, which are verified by experimental results also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5474</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5474</id><created>2012-03-25</created><authors><author><keyname>Li</keyname><forenames>Yanhua</forenames></author><author><keyname>Zhang</keyname><forenames>Zhi-Li</forenames></author><author><keyname>Bao</keyname><forenames>Jie</forenames></author></authors><title>Mutual or Unrequited Love: Identifying Stable Clusters in Social
  Networks with Uni- and Bi-directional Links</title><categories>cs.SI physics.soc-ph</categories><comments>10pages. A short version appears in 9th Workshop on Algorithms and
  Models for the Web Graph (WAW 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many social networks, e.g., Slashdot and Twitter, can be represented as
directed graphs (digraphs) with two types of links between entities: mutual
(bi-directional) and one-way (uni-directional) connections. Social science
theories reveal that mutual connections are more stable than one-way
connections, and one-way connections exhibit various tendencies to become
mutual connections. It is therefore important to take such tendencies into
account when performing clustering of social networks with both mutual and
one-way connections.
  In this paper, we utilize the dyadic methods to analyze social networks, and
develop a generalized mutuality tendency theory to capture the tendencies of
those node pairs which tend to establish mutual connections more frequently
than those occur by chance. Using these results, we develop a
mutuality-tendency-aware spectral clustering algorithm to identify more stable
clusters by maximizing the within-cluster mutuality tendency and minimizing the
cross-cluster mutuality tendency. Extensive simulation results on synthetic
datasets as well as real online social network datasets such as Slashdot,
demonstrate that our proposed mutuality-tendency-aware spectral clustering
algorithm extracts more stable social community structures than traditional
spectral clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5485</identifier>
 <datestamp>2012-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5485</id><created>2012-03-25</created><updated>2012-06-19</updated><authors><author><keyname>Agarwal</keyname><forenames>Sameer</forenames></author><author><keyname>Panda</keyname><forenames>Aurojit</forenames></author><author><keyname>Mozafari</keyname><forenames>Barzan</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author></authors><title>BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very
  Large Data</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present BlinkDB, a massively parallel, sampling-based
approximate query engine for running ad-hoc, interactive SQL queries on large
volumes of data. The key insight that BlinkDB builds on is that one can often
make reasonable decisions in the absence of perfect answers. For example,
reliably detecting a malfunctioning server using a distributed collection of
system logs does not require analyzing every request processed by the system.
Based on this insight, BlinkDB allows one to trade-off query accuracy for
response time, enabling interactive queries over massive data by running
queries on data samples and presenting results annotated with meaningful error
bars. To achieve this, BlinkDB uses two key ideas that differentiate it from
previous work in this area: (1) an adaptive optimization framework that builds
and maintains a set of multi-dimensional, multi-resolution samples from
original data over time, and (2) a dynamic sample selection strategy that
selects an appropriately sized sample based on a query's accuracy and/or
response time requirements. We have built an open-source version of BlinkDB and
validated its effectiveness using the well-known TPC-H benchmark as well as a
real-world analytic workload derived from Conviva Inc. Our experiments on a 100
node cluster show that BlinkDB can answer a wide range of queries from a
real-world query trace on up to 17 TBs of data in less than 2 seconds (over
100\times faster than Hive), within an error of 2 - 10%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5502</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5502</id><created>2012-03-25</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Strapparava</keyname><forenames>Carlo</forenames></author><author><keyname>Ozbal</keyname><forenames>Gozde</forenames></author></authors><title>Exploring Text Virality in Social Networks</title><categories>cs.CL cs.SI physics.soc-ph</categories><journal-ref>Proceedings of the Fifth International AAAI Conference on Weblogs
  and Social Media (ICWSM 2011), 17-21 July 2011, Barcelona, Spain</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to shed some light on the concept of virality - especially in
social networks - and to provide new insights on its structure. We argue that:
(a) virality is a phenomenon strictly connected to the nature of the content
being spread, rather than to the influencers who spread it, (b) virality is a
phenomenon with many facets, i.e. under this generic term several different
effects of persuasive communication are comprised and they only partially
overlap. To give ground to our claims, we provide initial experiments in a
machine learning framework to show how various aspects of virality can be
independently predicted according to content features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5523</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5523</id><created>2012-03-25</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author></authors><title>Wireless Video Transmission with Over-the-Air Packet Mixing</title><categories>cs.NI</categories><comments>2012 Packet Video Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a system for wireless video transmission with a
wireless physical layer (PHY) that supports cooperative forwarding of
interfered/superimposed packets. Our system model considers multiple and
independent unicast transmissions between network nodes while a number of them
serve as relays of the interfered/superimposed signals. For this new PHY the
average transmission rate that each node can achieve is estimated first. Next,
we formulate a utility optimization framework for the video transmission
problem and we show that it can be simplified due to the features of the new
PHY. Simulation results reveal the system operating regions for which
superimposing wireless packets is a better choice than a typical cooperative
PHY.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5532</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5532</id><created>2012-03-25</created><updated>2012-03-30</updated><authors><author><keyname>Scherrer</keyname><forenames>Bruno</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>On the Use of Non-Stationary Policies for Infinite-Horizon Discounted
  Markov Decision Processes</title><categories>cs.AI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider infinite-horizon $\gamma$-discounted Markov Decision Processes,
for which it is known that there exists a stationary optimal policy. We
consider the algorithm Value Iteration and the sequence of policies
$\pi_1,...,\pi_k$ it implicitely generates until some iteration $k$. We provide
performance bounds for non-stationary policies involving the last $m$ generated
policies that reduce the state-of-the-art bound for the last stationary policy
$\pi_k$ by a factor $\frac{1-\gamma}{1-\gamma^m}$. In particular, the use of
non-stationary policies allows to reduce the usual asymptotic performance
bounds of Value Iteration with errors bounded by $\epsilon$ at each iteration
from $\frac{\gamma}{(1-\gamma)^2}\epsilon$ to
$\frac{\gamma}{1-\gamma}\epsilon$, which is significant in the usual situation
when $\gamma$ is close to 1. Given Bellman operators that can only be computed
with some error $\epsilon$, a surprising consequence of this result is that the
problem of &quot;computing an approximately optimal non-stationary policy&quot; is much
simpler than that of &quot;computing an approximately optimal stationary policy&quot;,
and even slightly simpler than that of &quot;approximately computing the value of
some fixed policy&quot;, since this last problem only has a guarantee of
$\frac{1}{1-\gamma}\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5570</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5570</id><created>2012-03-25</created><authors><author><keyname>Tundjungsari</keyname><forenames>Vitri</forenames></author><author><keyname>Istiyanto</keyname><forenames>Jazi Eko</forenames></author><author><keyname>Winarko</keyname><forenames>Edi</forenames></author><author><keyname>Wardoyo</keyname><forenames>Retantyo</forenames></author></authors><title>Achieving Consensus with Individual Centrality Approach</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 4 tables, 1 figure, Tundjungsari, V., Istiyanto, J.E.,
  Winarko, E., Wardoyo, R., 2012, Achieving Consensus with Individual
  Centrality Approach, International Journal of Computer Science and
  Information Technology, Vol. 4, No. 1, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new consensus model in participatory decision making.
The model employs advice centrality approach by electing a leader and
recommender named as Supra Decision Maker (SDM). A SDM has a role as a decision
bench-marker to other decision makers in evaluating each alternative with
respect to given criteria. The weighting value for each alternative can be
obtained by considering consensus level and preferences' distances between SDM
and other Decision Makers. A social function using Social Judgment Scheme (SJS)
concept is employed when a decision does not achieve the required consensus
level. A simple example is presented here to illustrate our model.
  Keywords: Consensus, Group decision making, Centrality, Supra Decision Maker,
Social Judgment Scheme
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5572</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5572</id><created>2012-03-25</created><authors><author><keyname>Amblard</keyname><forenames>Pierre-Olivier</forenames></author><author><keyname>Michel</keyname><forenames>Olivier J. J.</forenames></author></authors><title>Causal conditioning and instantaneous coupling in causality graphs</title><categories>cs.IT math.IT</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates the link between Granger causality graphs recently
formalized by Eichler and directed information theory developed by Massey and
Kramer. We particularly insist on the implication of two notions of causality
that may occur in physical systems. It is well accepted that dynamical
causality is assessed by the conditional transfer entropy, a measure appearing
naturally as a part of directed information. Surprisingly the notion of
instantaneous causality is often overlooked, even if it was clearly understood
in early works. In the bivariate case, instantaneous coupling is measured
adequately by the instantaneous information exchange, a measure that
supplements the transfer entropy in the decomposition of directed information.
In this paper, the focus is put on the multivariate case and conditional graph
modeling issues. In this framework, we show that the decomposition of directed
information into the sum of transfer entropy and information exchange does not
hold anymore. Nevertheless, the discussion allows to put forward the two
measures as pillars for the inference of causality graphs. We illustrate this
on two synthetic examples which allow us to discuss not only the theoretical
concepts, but also the practical estimation issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5583</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5583</id><created>2012-03-26</created><authors><author><keyname>Liu</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author></authors><title>Graph-Theoretic Characterizations of Structural Controllability for
  Multi-Agent System with Switching Topology</title><categories>cs.MA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the controllability problem for multi-agent systems. In
particular, the structural controllability of multi-agent systems under
switching topologies is investigated. The structural controllability of
multi-agent systems is a generalization of the traditional controllability
concept for dynamical systems, and purely based on the communication topologies
among agents. The main contributions of the paper are graph-theoretic
characterizations of the structural controllability for multi-agent systems. It
turns out that the multi-agent system with switching topology is structurally
controllable if and only if the union graph G of the underlying communication
topologies is connected (single leader) or leader-follower connected
(multi-leader). Finally, the paper concludes with several illustrative examples
and discussions of the results and future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5602</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5602</id><created>2012-03-26</created><authors><author><keyname>Xu</keyname><forenames>Peng</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Dai</keyname><forenames>and Xuchu</forenames></author><author><keyname>Leung</keyname><forenames>Kin</forenames></author></authors><title>On the Application of Noisy Network Coding to the Relay-Eavesdropper
  Channel</title><categories>cs.IT math.IT</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the design of a new secrecy transmission scheme
for a four-node relay-eavesdropper channel. The key idea of the proposed scheme
is to combine noisy network coding with the interference assisted strategy for
wiretap channel with a helping interferer. A new achievable secrecy rate is
characterized for both discrete memoryless and Gaussian channels. Such a new
rate can be viewed as a general framework, where the existing interference
assisted schemes such as noisy-forwarding and cooperative jamming approaches
can be shown as special cases of the proposed scheme. In addition, under some
channel condition where the existing schemes can only achieve zero secrecy
rate, the proposed secrecy scheme can still offer significant performance
gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5612</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5612</id><created>2012-03-26</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Closed-Form Critical Conditions of Subharmonic Oscillations for Buck
  Converters</title><categories>cs.SY math.DS nlin.CD</categories><comments>Submitted to an IEEE Journal on Dec. 23, 2011, and resubmitted to
  IEEE Transactions on Circuits and Systems-I on Feb. 14, 2012. My current six
  papers in arXiv have a common reviewer</comments><journal-ref>IEEE Transactions on Circuits and Systems- I, Regular Papers,
  60(7), pp. 1967-1974, 2013</journal-ref><doi>10.1109/TCSI.2012.2230498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general critical condition of subharmonic oscillation in terms of the loop
gain is derived. Many closed-form critical conditions for various control
schemes in terms of converter parameters are also derived. Some previously
known critical conditions become special cases in the generalized framework.
Given an arbitrary control scheme, a systematic procedure is proposed to derive
the critical condition for that control scheme. Different control schemes share
similar forms of critical conditions. For example, both V2 control and voltage
mode control have the same form of critical condition. A peculiar phenomenon in
average current mode control where subharmonic oscillation occurs in a window
value of pole can be explained by the derived critical condition. A ripple
amplitude index to predict subharmonic oscillation proposed in the past
research has limited application and is shown invalid for a converter with a
large pole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5638</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5638</id><created>2012-03-26</created><authors><author><keyname>Bustin</keyname><forenames>Ronit</forenames><affiliation>Shitz</affiliation></author><author><keyname>Payaro</keyname><forenames>Miquel</forenames><affiliation>Shitz</affiliation></author><author><keyname>Palomar</keyname><forenames>Daniel</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On MMSE Properties and I-MMSE Implications in Parallel MIMO Gaussian
  Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The scalar additive Gaussian noise channel has the &quot;single crossing point&quot;
property between the minimum-mean square error (MMSE) in the estimation of the
input given the channel output, assuming a Gaussian input to the channel, and
the MMSE assuming an arbitrary input. This paper extends the result to the
parallel MIMO additive Gaussian channel in three phases: i) The channel matrix
is the identity matrix, and we limit the Gaussian input to a vector of Gaussian
i.i.d. elements. The &quot;single crossing point&quot; property is with respect to the
snr (as in the scalar case). ii) The channel matrix is arbitrary, the Gaussian
input is limited to an independent Gaussian input. A &quot;single crossing point&quot;
property is derived for each diagonal element of the MMSE matrix. iii) The
Gaussian input is allowed to be an arbitrary Gaussian random vector. A &quot;single
crossing point&quot; property is derived for each eigenvalue of the MMSE matrix.
  These three extensions are then translated to new information theoretic
properties on the mutual information, using the fundamental relationship
between estimation theory and information theory. The results of the last phase
are also translated to a new property of Fisher's information. Finally, the
applicability of all three extensions on information theoretic problems is
demonstrated through: a proof of a special case of Shannon's vector EPI, a
converse proof of the capacity region of the parallel degraded MIMO broadcast
channel (BC) under per-antenna power constrains and under covariance
constraints, and a converse proof of the capacity region of the compound
parallel degraded MIMO BC under covariance constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5675</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5675</id><created>2012-03-26</created><authors><author><keyname>Roy</keyname><forenames>Amitabha</forenames></author></authors><title>Memory Hierarchy Sensitive Graph Layout</title><categories>cs.DS cs.DB cs.PF</categories><comments>Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining large graphs for information is becoming an increasingly important
workload due to the plethora of graph structured data becoming available. An
aspect of graph algorithms that has hitherto not received much interest is the
effect of memory hierarchy on accesses. A typical system today has multiple
levels in the memory hierarchy with differing units of locality; ranging across
cache lines, TLB entries and DRAM pages. We postulate that it is possible to
allocate graph structured data in main memory in a way as to improve the
spatial locality of the data. Previous approaches to improving cache locality
have focused only on a single unit of locality, either the cache line or
virtual memory page. On the other hand cache oblivious algorithms can optimise
layout for all levels of the memory hierarchy but unfortunately need to be
specially designed for individual data structures. In this paper we explore
hierarchical blocking as a technique for closing this gap. We require as input
a specification of the units of locality in the memory hierarchy and lay out
the input graph accordingly by copying its nodes using a hierarchy of breadth
first searches. We start with a basic algorithm that is limited to trees and
then extend it to arbitrary graphs. Our most efficient version requires only a
constant amount of additional space. We have implemented versions of the
algorithm in various environments: for C programs interfaced with macros, as an
extension to the Boost object oriented graph library and finally as a
modification to the traversal phase of the semispace garbage collector in the
Jikes Java virtual machine. Our results show significant improvements in the
access time to graphs of various structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5683</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5683</id><created>2012-03-26</created><authors><author><keyname>Gol</keyname><forenames>Ebru Aydin</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Time-Constrained Temporal Logic Control of Multi-Affine Systems</title><categories>cs.SY</categories><acm-class>I.2.8; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of controlling a dynamical system such
that its trajectories satisfy a temporal logic property in a given amount of
time. We focus on multi-affine systems and specifications given as
syntactically co-safe linear temporal logic formulas over rectangular regions
in the state space. The proposed algorithm is based on the estimation of time
bounds for facet reachability problems and solving a time optimal reachability
problem on the product between a weighted transition system and an automaton
that enforces the satisfaction of the specification. A random optimization
algorithm is used to iteratively improve the solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5689</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5689</id><created>2012-03-26</created><authors><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author><author><keyname>L&#xfc;ke</keyname><forenames>Thomas</forenames></author><author><keyname>van Hoek</keyname><forenames>Wilko</forenames></author></authors><title>Building Custom Term Suggestion Web Services with OAI-Harvested Open
  Data</title><categories>cs.DL</categories><comments>8 pages, 5 figures, presented at 2. DGI-Konferenz / 64. Jahrestagung
  der DGI, D\&quot;usseldorf, Germany on 2012-03-23</comments><journal-ref>Social Media und Web Science: Das Web als Lebensraum. 2.
  DGI-Konferenz / 64. Jahrestagung der DGI, D\&quot;usseldorf, 22. bis 23. M\&quot;arz
  2012, Proceedings (2012), p. 389-396</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem that the same information need can be expressed in a variety of
ways is especially true for scientific literature. Each scientific discipline
has its own domain-specific language and vocabulary. This language is coded
into documentary tools like thesauri or classifications that are used to
document and describe scientific documents. When we think of information
retrieval as &quot;fundamentally a linguistic process&quot; (Blair, 2003) users have to
be aware of the most relevant search terms - which are the controlled thesauri
terms the documents are described with. This can be achieved with so-called
search-term-recommenders (STR) that map free search terms of a lay user to
controlled vocabulary terms which can then be used as a term suggestion or to
do an automatic query expansion (Hienert, Schaer, Schaible, &amp; Mayr, 2011).
State-of-the-art repository software systems like DSpace or EPrints already
offer some kind of term suggestion features in search or input forms but these
implementations only work as simple auto completion mechanisms that don't
incorporate any kind of semantic mapping. Such software systems would gain a
lot in terms of usability and data consistency if tools like the proposed
domain-specific STRs would be freely available. We aim to implement a rich
toolbox of web services (like the mentioned domain-specific STRs) to support
users and providers of online Digital Library (DL) or repository systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5706</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5706</id><created>2012-03-26</created><updated>2014-09-04</updated><authors><author><keyname>Scheiblechner</keyname><forenames>Peter</forenames></author></authors><title>Effective de Rham Cohomology - The General Case</title><categories>math.AG cs.CC math.AC</categories><comments>36 pages</comments><msc-class>14Q20, 14Q15, 68W30, 34C07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grothendieck has proved that each class in the de Rham cohomology of a smooth
complex affine variety can be represented by a differential form with
polynomial coefficients. After having proved a single exponential bound for the
degrees of these forms in the case of a hypersurface, here we generalize this
result to arbitrary codimension. More precisely, we show that the p-th de Rham
cohomology of a smooth affine variety of dimension m and degree D can be
represented by differential forms of degree (pD)^{O(pm)}. This result is
relevant for the algorithmic computation of the cohomology, but is also
motivated by questions in the theory of ordinary differential equations related
to the infinitesimal Hilbert 16th problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5715</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5715</id><created>2012-03-26</created><authors><author><keyname>&#xc0;lvarez</keyname><forenames>Carme</forenames></author><author><keyname>Fern&#xe0;ndez</keyname><forenames>Aleix</forenames></author></authors><title>Network Formation: Heterogeneous Traffic, Bilateral Contracting and
  Myopic Dynamics</title><categories>cs.GT cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a network formation game where nodes wish to send traffic to other
nodes. Nodes can contract bilaterally other nodes to form bidirectional links
as well as nodes can break unilaterally contracts to eliminate the
corresponding links. Our model is an extension of the model considered in
Arcaute et al. The novelty is that we do no require the traffic to be uniform
all-to-all. Each node specifies the amount of traffic that it wants to send to
any other node. We characterize stable topologies under a static point of view
and we also study the game under a myopic dynamics. We show its convergence to
stable networks under some natural assumptions on the contracting functions.
Finally we consider the efficiency of pairwise Nash topologies from a social
point of view and we show that the problem of deciding the existence stable
topologies of a given price is $\NP$-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5716</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5716</id><created>2012-03-26</created><updated>2012-03-27</updated><authors><author><keyname>Corani</keyname><forenames>Giorgio</forenames></author><author><keyname>Antonucci</keyname><forenames>Alessandro</forenames></author></authors><title>Credal Classification based on AODE and compression coefficients</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian model averaging (BMA) is an approach to average over alternative
models; yet, it usually gets excessively concentrated around the single most
probable model, therefore achieving only sub-optimal classification
performance. The compression-based approach (Boulle, 2007) overcomes this
problem, averaging over the different models by applying a logarithmic
smoothing over the models' posterior probabilities. This approach has shown
excellent performances when applied to ensembles of naive Bayes classifiers.
AODE is another ensemble of models with high performance (Webb, 2005), based on
a collection of non-naive classifiers (called SPODE) whose probabilistic
predictions are aggregated by simple arithmetic mean. Aggregating the SPODEs
via BMA rather than by arithmetic mean deteriorates the performance; instead,
we aggregate the SPODEs via the compression coefficients and we show that the
resulting classifier obtains a slight but consistent improvement over AODE.
However, an important issue in any Bayesian ensemble of models is the
arbitrariness in the choice of the prior over the models. We address this
problem by the paradigm of credal classification, namely by substituting the
unique prior with a set of priors. Credal classifier automatically recognize
the prior-dependent instances, namely the instances whose most probable class
varies, when different priors are considered; in these cases, credal
classifiers remain reliable by returning a set of classes rather than a single
class. We thus develop the credal version of both the BMA-based and the
compression-based ensemble of SPODEs, substituting the single prior over the
models by a set of priors. Experiments show that both credal classifiers
provide higher classification reliability than their determinate counterparts;
moreover the compression-based credal classifier compares favorably to previous
credal classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5731</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5731</id><created>2012-03-26</created><updated>2013-01-22</updated><authors><author><keyname>Mondal</keyname><forenames>Nabarun</forenames></author><author><keyname>Ghosh</keyname><forenames>Partha P.</forenames></author></authors><title>A Pseudo Random Number Generator from Chaos</title><categories>cs.DM</categories><msc-class>11K45, 03D10, 65P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random number generator is proposed based on a theorem about existence of
chaos in fixed point iteration of x= cot2(x). Digital computer simulation of
this function iteration exhibits random behavior. A method is proposed to
extract random bytes from this simulation. Diehard and NIST test suite for
randomness detection is run on this bytes, and it is found to pass all the
tests in the suite. Thus, this method qualifies even for cryptographic quality
random number generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5737</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5737</id><created>2012-03-26</created><authors><author><keyname>Heller</keyname><forenames>Martin</forenames></author><author><keyname>Oberhuber</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>Adaptive Row-grouped CSR Format for Storing of Sparse Matrices on GPU</title><categories>cs.DC cs.DS</categories><comments>9 pages, 5 figures, 1 code listing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new adaptive format for storing sparse matrices on GPU. We compare
it with several other formats including CUSPARSE which is today probably the
best choice for processing of sparse matrices on GPU in CUDA. Contrary to
CUSPARSE which works with common CSR format, our new format requires
conversion. However, multiplication of sparse-matrix and vector is
significantly faster for many atrices. We demonstrate it on set of 1 600
matrices and we show for what types of matrices our format is profitable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5742</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5742</id><created>2012-03-26</created><authors><author><keyname>Ferraz</keyname><forenames>Raul Antonio</forenames></author><author><keyname>Guerreiro</keyname><forenames>Marin&#xea;s</forenames></author><author><keyname>Milies</keyname><forenames>C&#xe9;sar Polcino</forenames></author></authors><title>G-equivalence in group algebras and minimal abelian codes</title><categories>cs.IT math.GR math.IT math.RA</categories><comments>8 pages, 4 tables</comments><msc-class>11T71, 68P30, 16S34</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a finite abelian group and F a field such that char(F) does not
divide |G|. Denote by FG the group algebra of G over F. A (semisimple) abelian
code is an ideal of FG. Two codes I and J of FG are G-equivalent if there
exists an automorphism of G whose linear extension to FG maps I onto J In this
paper we give a necessary and sufficient condition for minimal abelian codes to
be G-equivalent and show how to correct some results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5747</identifier>
 <datestamp>2012-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5747</id><created>2012-03-26</created><updated>2012-10-11</updated><authors><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>Constructive Discrepancy Minimization by Walking on The Edges</title><categories>cs.DS cs.DM math.CO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing the discrepancy of a set system is a fundamental problem in
combinatorics. One of the cornerstones in this area is the celebrated six
standard deviations result of Spencer (AMS 1985): In any system of n sets in a
universe of size n, there always exists a coloring which achieves discrepancy
6\sqrt{n}. The original proof of Spencer was existential in nature, and did not
give an efficient algorithm to find such a coloring. Recently, a breakthrough
work of Bansal (FOCS 2010) gave an efficient algorithm which finds such a
coloring. His algorithm was based on an SDP relaxation of the discrepancy
problem and a clever rounding procedure. In this work we give a new randomized
algorithm to find a coloring as in Spencer's result based on a restricted
random walk we call &quot;Edge-Walk&quot;. Our algorithm and its analysis use only basic
linear algebra and is &quot;truly&quot; constructive in that it does not appeal to the
existential arguments, giving a new proof of Spencer's theorem and the partial
coloring lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5748</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5748</id><created>2012-03-26</created><authors><author><keyname>Fuad</keyname><forenames>Mohammad Muztaba</forenames></author><author><keyname>Deb</keyname><forenames>Debzani</forenames></author><author><keyname>Baek</keyname><forenames>Jinsuk</forenames></author></authors><title>Self-Healing by Means of Runtime Execution Profiling</title><categories>cs.SE</categories><comments>Proceedings of 14th International Conference on Computer and
  Information Technology (ICCIT 2011) 22-24 December, 2011, Dhaka, Bangladesh</comments><doi>10.1109/ICCITechn.2011.6164784</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A self-healing application brings itself into a stable state after a failure
put the software into an unstable state. For such self-healing software
application, finding fix for a previously unseen fault is a grand challenge.
Asking the user to provide fixes for every fault is bad for productivity,
especially when the users are non-savvy in technical aspect of computing. If
failure scenarios come into existence, the user wants the runtime environment
to handle those situations autonomically. This paper presents a new technique
of finding self-healing actions by matching a fault scenario to already
established fault models. By profiling and capturing runtime parameters and
execution pathWays, stable execution models are established and later are used
to match with an unstable execution scenario. Experimentation and results are
presented that showed that even with additional overheads; this technique can
prove beneficial for autonomically healing faults and reliving system
administrators from mundane troubleshooting situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5754</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5754</id><created>2012-03-26</created><authors><author><keyname>Fuhs</keyname><forenames>Carsten</forenames></author><author><keyname>Kop</keyname><forenames>Cynthia</forenames></author></authors><title>Polynomial Interpretations for Higher-Order Rewriting</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The termination method of weakly monotonic algebras, which has been defined
for higher-order rewriting in the HRS formalism, offers a lot of power, but has
seen little use in recent years. We adapt and extend this method to the
alternative formalism of algebraic functional systems, where the simply-typed
lambda-calculus is combined with algebraic reduction. Using this theory, we
define higher-order polynomial interpretations, and show how the implementation
challenges of this technique can be tackled. A full implementation is provided
in the termination tool WANDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5762</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5762</id><created>2012-03-26</created><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Performance Analysis of Adaptive Physical Layer Network Coding for
  Wireless Two-way Relaying</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of modulation schemes for the physical layer network-coded two
way relaying scenario is presented which employs two phases: Multiple access
(MA) phase and Broadcast (BC) phase. It was shown by Koike-Akino et. al. that
adaptively changing the network coding map used at the relay according to the
channel conditions greatly reduces the impact of multiple access interference
which occurs at the relay during the MA phase. Depending on the signal set used
at the end nodes, deep fades occur for a finite number of channel fade states
referred as the singular fade states. The singular fade states fall into the
following two classes: The ones which are caused due to channel outage and
whose harmful effect cannot be mitigated by adaptive network coding are
referred as the \textit{non-removable singular fade states}. The ones which
occur due to the choice of the signal set and whose harmful effects can be
removed by a proper choice of the adaptive network coding map are referred as
the \textit{removable} singular fade states. In this paper, we derive an upper
bound on the average end-to-end Symbol Error Rate (SER), with and without
adaptive network coding at the relay, for a Rician fading scenario. It is shown
that without adaptive network coding, at high Signal to Noise Ratio (SNR), the
contribution to the end-to-end SER comes from the following error events which
fall as $\text{SNR}^{-1}$: the error events associated with the removable
singular fade states, the error events associated with the non-removable
singular fade states and the error event during the BC phase. In contrast, for
the adaptive network coding scheme, the error events associated with the
removable singular fade states contributing to the average end-to-end SER fall
as $\text{SNR}^{-2}$ and as a result the adaptive network coding scheme
provides a coding gain over the case when adaptive network coding is not used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5765</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5765</id><created>2012-03-26</created><authors><author><keyname>Collins</keyname><forenames>Karen L.</forenames></author><author><keyname>Trenk</keyname><forenames>Ann</forenames></author></authors><title>Nordhaus-Gaddum Theorem for the Distinguishing Chromatic Number</title><categories>math.CO cs.DM math.GR</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nordhaus and Gaddum proved, for any graph G, that the chromatic number of G
plus the chromatic number of G complement is less than or equal to the number
of vertices in G plus 1. Finck characterized the class of graphs that satisfy
equality in this bound. In this paper, we provide a new characterization of
this class of graphs, based on vertex degrees, which yields a new
polynomial-time recognition algorithm and efficient computation of the
chromatic number of graphs in this class. Our motivation comes from our theorem
that generalizes the Nordhaus-Gaddum theorem to the distinguishing chromatic
number: for any graph G, the distinguishing chromatic number of G plus the
distinguishing chromatic number of G complement is less than or equal to the
number of vertices of G plus the distinguishing number of G. Finally, we
characterize those graphs that achieve equality in the sum upper bounds
simultaneously for both the chromatic number and for our distinguishing
chromatic number analog of the Nordhaus-Gaddum inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5772</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5772</id><created>2012-03-26</created><authors><author><keyname>Bilen</keyname><forenames>Cagdas</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan</forenames></author></authors><title>Compressed Sensing for Moving Imagery in Medical Imaging</title><categories>cs.MM cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous applications in signal processing have benefited from the theory of
compressed sensing which shows that it is possible to reconstruct signals
sampled below the Nyquist rate when certain conditions are satisfied. One of
these conditions is that there exists a known transform that represents the
signal with a sufficiently small number of non-zero coefficients. However when
the signal to be reconstructed is composed of moving images or volumes, it is
challenging to form such regularization constraints with traditional transforms
such as wavelets. In this paper, we present a motion compensating prior for
such signals that is derived directly from the optical flow constraint and can
utilize the motion information during compressed sensing reconstruction.
Proposed regularization method can be used in a wide variety of applications
involving compressed sensing and images or volumes of moving and deforming
objects. It is also shown that it is possible to estimate the signal and the
motion jointly or separately. Practical examples from magnetic resonance
imaging has been presented to demonstrate the benefit of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5778</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5778</id><created>2012-03-24</created><authors><author><keyname>Salah-ddine</keyname><forenames>Krit</forenames></author><author><keyname>Kamal</keyname><forenames>Zared</forenames></author><author><keyname>Hassan</keyname><forenames>Qjidaa</forenames></author><author><keyname>Mohcine</keyname><forenames>Zouak</forenames></author></authors><title>A 100 mA Low Voltage Linear Regulators for Systems on Chip Applications
  Using 0.18 {\mu}m CMOS Technology</title><categories>cs.OH</categories><comments>7 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 3, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel design for a low dropout (LDO) voltage regulator is presented and
dedicated to power many sections of a typical cellular handset. However, these
baseband, RF, and audio sections have different requirements that influence
which LDO is most appropriate. After discussion of the specific requirements,
different LDOs are recommended. Also, some LDO design techniques are briefly
discussed to demonstrate how an LDO may be optimized for a specific level of
performance. Cellular phone designs require linear regulators with lowdropout,
low-noise, high PSRR, low quiescent current (Iq), and low-cost. They need to
deliver a stable output and use smallvalue output capacitors. Ideally, one
device would have all these characteristics and one low-dropout linear
regulator (LDO) could be used anywhere in the phone without worry. But in
practice, the various cell phone blocks are best powered by LDOs with different
performance characteristics. This paper provides a new design methodology to
choosing the right LDO to power each cell phone and especially for the Voltage
Phase-Locked loops (VPLLs) blocks. Fabricated in a 0.18 {\mu}m CMOS process,
the measured results show the adopted topology achieves a better phase noise
than the conventional saturation current source. and the spread of the current
limitation (without matching) is 100mA, the VPLLs system demonstrates a phase
noise of 782 nv/sqrtHz at 100-kHz, and 33 nv/sqrtHz at 1 MHz, while quiescent
current 33 {\mu}A from a 2.6 V supply voltage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5782</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5782</id><created>2012-03-26</created><authors><author><keyname>Cheng</keyname><forenames>Howard</forenames></author><author><keyname>Devadoss</keyname><forenames>Satyan L.</forenames></author><author><keyname>Li</keyname><forenames>Brian</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>Skeletal Rigidity of Phylogenetic Trees</title><categories>cs.CG cs.CE math.AG q-bio.PE</categories><comments>17 pages, 12 figures</comments><msc-class>05C05, 52C25, 92B10</msc-class><journal-ref>Discrete Applied Mathematics 170 (2014) 46 - 54</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by geometric origami and the straight skeleton construction, we
outline a map between spaces of phylogenetic trees and spaces of planar
polygons. The limitations of this map is studied through explicit examples,
culminating in proving a structural rigidity result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5794</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5794</id><created>2012-03-26</created><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author></authors><title>Polar codes for private classical communication</title><categories>quant-ph cs.IT math.IT</categories><comments>11 pages, 2 figures, submission to the 2012 International Symposium
  on Information Theory and its Applications (ISITA 2012), Honolulu, Hawaii,
  USA</comments><journal-ref>Proceedings of the 2012 International Symposium on Information
  Theory and its Applications, pages 745-749, October 2012. Available at
  http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=6401041</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a new secret-key assisted polar coding scheme for private
classical communication over a quantum or classical wiretap channel. The
security of our scheme rests on an entropic uncertainty relation, in addition
to the channel polarization effect. Our scheme achieves the symmetric private
information rate by synthesizing &quot;amplitude&quot; and &quot;phase&quot; channels from an
arbitrary quantum wiretap channel. We find that the secret-key consumption rate
of the scheme vanishes for an arbitrary degradable quantum wiretap channel.
Furthermore, we provide an additional sufficient condition for when the secret
key rate vanishes, and we suspect that satisfying this condition implies that
the scheme requires no secret key at all. Thus, this latter condition addresses
an open question from the Mahdavifar-Vardy scheme for polar coding over a
classical wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5822</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5822</id><created>2012-03-26</created><updated>2012-05-12</updated><authors><author><keyname>Wan</keyname><forenames>Cheng</forenames></author></authors><title>Coalitions in nonatomic network congestion games</title><categories>cs.GT cs.SI math.OC</categories><comments>22 pages, 2 figures</comments><msc-class>91A13, 90B10 (Primary) 90B20, 91B18 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work shows that the formation of a finite number of coalitions in a
nonatomic network congestion game benefits everyone. At the equilibrium of the
composite game played by coalitions and individuals, the average cost to each
coalition and the individuals' common cost are all lower than in the
corresponding nonatomic game (without coalitions). The individuals' cost is
lower than the average cost to any coalition. Similarly, the average cost to a
coalition is lower than that to any larger coalition. Whenever some members of
a coalition become individuals, the individuals' payoff is increased. In the
case of a unique coalition, both the average cost to the coalition and the
individuals' cost are decreasing with respect to the size of the coalition. In
a sequence of composite games, if a finite number of coalitions are fixed,
while the size of the remaining coalitions goes to zero, the equilibria of
these games converge to the equilibrium of a composite game played by the same
fixed coalitions and the remaining individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5830</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5830</id><created>2012-03-26</created><authors><author><keyname>Nguyen</keyname><forenames>Viet Hung</forenames></author><author><keyname>Massacci</keyname><forenames>Fabio</forenames></author></authors><title>An Independent Validation of Vulnerability Discovery Models</title><categories>cs.CR</categories><comments>This paper is to appear in ASIACCS'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Having a precise vulnerability discovery model (VDM) would provide a useful
quantitative insight to assess software security. Thus far, several models have
been proposed with some evidence supporting their goodness-of-fit.
  In this work we describe an independent validation of the applicability of
six existing VDMs in seventeen releases of the three popular browsers Firefox,
Google Chrome and Internet Explorer. We have collected five different kinds of
data sets based on different definitions of a vulnerability. We introduce two
quantitative metrics, goodness-of-fit entropy and goodness-of-fit quality, to
analyze the impact of vulnerability data sets to the stability as well as
quality of VDMs in the software life cycles.
  The experiment result shows that the &quot;confirmed-by-vendors' advisories&quot; data
sets apparently yields more stable and better results for VDMs. And the
performance of the s-shape logistic model (AML) seems to be superior
performance in overall. Meanwhile, Anderson thermodynamic model (AT) is indeed
not suitable for modeling the vulnerability discovery process. This means that
the discovery process of vulnerabilities and normal bugs are different because
the interests of people in finding security vulnerabilities are more than
finding normal programming bugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5846</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5846</id><created>2012-03-26</created><authors><author><keyname>Stoutemyer</keyname><forenames>David R.</forenames></author></authors><title>Series Crimes</title><categories>cs.SC cs.MS</categories><comments>36 pages, 5 tables, to appear in ACM Communications in Computer
  Algebra</comments><msc-class>30E10, 30E15, 40-04, 41-04</msc-class><journal-ref>ACM Communications in Computer Algebra 12/2012; 46
  (4)(182):134-153</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Puiseux series are power series in which the exponents can be fractional
and/or negative rational numbers. Several computer algebra systems have one or
more built-in or loadable functions for computing truncated Puiseux series.
Some are generalized to allow coe?cients containing functions of the series
variable that are dominated by any power of that variable, such as logarithms
and nested logarithms of the series variable. Some computer algebra systems
also have built-in or loadable functions that compute infinite Puiseux series.
Unfortunately, there are some little-known pitfalls in computing Puiseux
series. The most serious of these is expansions within branch cuts or at branch
points that are incorrect for some directions in the complex plane. For example
with each series implementation accessible to you:
  Compare the value of (z^2 + z^3)^(3/2) with that of its truncated series
expansion about z = 0, approximated at z = -0.01. Does the series converge to a
value that is the negative of the correct value?
  Compare the value of ln(z^2 + z^3) with its truncated series expansion about
z = 0, approximated at z = -0.01 + 0.1i. Does the series converge to a value
that is incorrect by 2pi i?
  Compare arctanh(-2 + ln(z)z) with its truncated series expansion about z = 0,
approximated at z = -0.01. Does the series converge to a value that is
incorrect by about pi i?
  At the time of this writing, most implementations that accommodate such
series exhibit such errors. This article describes how to avoid these errors
both for manual derivation of series and when implementing series packages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5871</identifier>
 <datestamp>2012-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5871</id><created>2012-03-27</created><updated>2012-11-13</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel</forenames></author><author><keyname>Fernandez-Granda</keyname><forenames>Carlos</forenames></author></authors><title>Towards a Mathematical Theory of Super-Resolution</title><categories>cs.IT math.IT math.NA</categories><comments>48 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a mathematical theory of super-resolution. Broadly
speaking, super-resolution is the problem of recovering the fine details of an
object---the high end of its spectrum---from coarse scale information
only---from samples at the low end of the spectrum. Suppose we have many point
sources at unknown locations in $[0,1]$ and with unknown complex-valued
amplitudes. We only observe Fourier samples of this object up until a frequency
cut-off $f_c$. We show that one can super-resolve these point sources with
infinite precision---i.e. recover the exact locations and amplitudes---by
solving a simple convex optimization problem, which can essentially be
reformulated as a semidefinite program. This holds provided that the distance
between sources is at least $2/f_c$. This result extends to higher dimensions
and other models. In one dimension for instance, it is possible to recover a
piecewise smooth function by resolving the discontinuity points with infinite
precision as well. We also show that the theory and methods are robust to
noise. In particular, in the discrete setting we develop some theoretical
results explaining how the accuracy of the super-resolved signal is expected to
degrade when both the noise level and the {\em super-resolution factor} vary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5874</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5874</id><created>2012-03-27</created><updated>2012-05-20</updated><authors><author><keyname>Shakya</keyname><forenames>Rajeev K.</forenames></author><author><keyname>Singh</keyname><forenames>Yatindra Nath</forenames></author><author><keyname>Verma</keyname><forenames>Nishchal K.</forenames></author></authors><title>Optimizing Channel Access for Event-Driven Wireless Sensor Networks:
  Analysis and Enhancements</title><categories>cs.NI</categories><report-no>tr-1225141594</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of medium access control in domain of event-driven
wireless sensor networks (WSNs). In this kind of WSN, sensor nodes send data to
sink node only when an event occurs in the monitoring area. The nodes in this
kind of WSNs encounter correlated traffic as a subset of nodes start sending
data by sensing a common event simultaneously. We wish to rethink of medium
access control (MAC) for this type of traffic characteristics. For WSNs, many
existing MAC protocols utilize the basic CSMA/CA strategy such as IEEE 802.11
Binary Exponential Backoff (BEB) algorithm to handle the collisions among
packets when more than one node need to access the channel. We show that this
BEB algorithm does not work well without incurring access delay or performance
degradation due to increased number of collisions and retransmissions when
nodes encounter correlated traffic. Based on above observations in mind, We
present a Adaptive Random Backoff (ARB) algorithm that is capable of mitigating
the impact of correlated traffic and capable of minimizing the chance of
collisions. ARB is based on minor modifications of BEB. We show using numerical
analysis that our proposals improve the channel access in terms of latency,
throughput, and frame dropping probability as compared with IEEE 802.11 DCF.
Simulations using NS-2 network simulator are conducted to validate the
analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5914</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5914</id><created>2012-03-27</created><authors><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Burger</keyname><forenames>Martin</forenames></author><author><keyname>Dieterich</keyname><forenames>Peter</forenames></author><author><keyname>Schwab</keyname><forenames>Albrecht</forenames></author></authors><title>A Framework for Automated Cell Tracking in Phase Contrast Microscopic
  Videos based on Normal Velocities</title><categories>q-bio.QM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel framework for the automated tracking of cells,
with a particular focus on the challenging situation of phase contrast
microscopic videos. Our framework is based on a topology preserving variational
segmentation approach applied to normal velocity components obtained from
optical flow computations, which appears to yield robust tracking and automated
extraction of cell trajectories. In order to obtain improved trackings of local
shape features we discuss an additional correction step based on active
contours and the image Laplacian which we optimize for an example class of
transformed renal epithelial (MDCK-F) cells. We also test the framework for
human melanoma cells and murine neutrophil granulocytes that were seeded on
different types of extracellular matrices. The results are validated with
manual tracking results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5915</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5915</id><created>2012-03-27</created><authors><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Bavirisetti</keyname><forenames>Teja Damodaram</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On the Feasibility of Network Alignment for Three-Source
  Three-Destination Multiple Unicast Networks with Delays</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A transform approach to network coding was introduced by Bavirisetti et al.
(arXiv:1103.3882v3 [cs.IT]) as a tool to view wireline networks with delays as
$k$-instantaneous networks (for some large $k$). When the local encoding
kernels (LEKs) of the network are varied with every time block of length $k &gt;
1$, the network is said to use block time varying LEKs. In this work, we
propose a Precoding Based Network Alignment (PBNA) scheme based on transform
approach and block time varying LEKs for three-source three-destination
multiple unicast network with delays (3-S 3-D MUN-D). In a recent work, Meng et
al. (arXiv:1202.3405v1 [cs.IT]) reduced the infinite set of sufficient
conditions for feasibility of PBNA in a three-source three-destination
instantaneous multiple unicast network as given by Das et al.
(arXiv:1008.0235v1 [cs.IT]) to a finite set and also showed that the conditions
are necessary. We show that the conditions of Meng et al. are also necessary
and sufficient conditions for feasibility of PBNA based on transform approach
and block time varying LEKs for 3-S 3-D MUN-D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5919</identifier>
 <datestamp>2012-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5919</id><created>2012-03-27</created><updated>2012-06-29</updated><authors><author><keyname>Borisevich</keyname><forenames>Alex</forenames></author><author><keyname>Schullerus</keyname><forenames>Gernot</forenames></author></authors><title>Switching strategy based on homotopy continuation for non-regular affine
  systems with application in induction motor control</title><categories>cs.SY</categories><comments>24 pages, extended version of paper for AT journal + experementation
  results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the article the problem of output setpoint tracking for affine non-linear
system is considered. Presented approach combines state feedback linearization
and homotopy numerical continuation in subspaces of phase space where feedback
linearization fails. The method of numerical parameter continuation for solving
systems of nonlinear equations is generalized to control affine non-linear
dynamical systems. The illustrative example of control of MIMO system which is
not static feedback linearizable is given. Application of proposed method
demonstrated on the speed and rotor magnetic flux control in the three-phase
asynchronous motor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5924</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5924</id><created>2012-03-27</created><updated>2012-10-02</updated><authors><author><keyname>Yin</keyname><forenames>Haifan</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Filippou</keyname><forenames>Miltiades</forenames></author><author><keyname>Liu</keyname><forenames>Yingzhuang</forenames></author></authors><title>A Coordinated Approach to Channel Estimation in Large-scale
  Multiple-antenna Systems</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures, to appear in IEEE Journal on Selected Areas in
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of channel estimation in multi-cell
interference-limited cellular networks. We consider systems employing multiple
antennas and are interested in both the finite and large-scale antenna number
regimes (so-called &quot;massive MIMO&quot;). Such systems deal with the multi-cell
interference by way of per-cell beamforming applied at each base station.
Channel estimation in such networks, which is known to be hampered by the pilot
contamination effect, constitute a major bottleneck for overall performance. We
present a novel approach which tackles this problem by enabling a low-rate
coordination between cells during the channel estimation phase itself. The
coordination makes use of the additional second-order statistical information
about the user channels, which are shown to offer a powerful way of
discriminating across interfering users with even strongly correlated pilot
sequences. Importantly, we demonstrate analytically that in the
large-number-of-antennas regime, the pilot contamination effect is made to
vanish completely under certain conditions on the channel covariance. Gains
over the conventional channel estimation framework are confirmed by our
simulations for even small antenna array sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5927</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5927</id><created>2012-03-27</created><authors><author><keyname>Aldridge</keyname><forenames>Matthew</forenames></author></authors><title>Adaptive group testing as channel coding with feedback</title><categories>cs.IT cs.DM math.IT</categories><comments>4 pages, 1 figure</comments><journal-ref>2012 International Symposium on Information Theory Proceedings,
  1832-1836, 2012</journal-ref><doi>10.1109/ISIT.2012.6283596</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group testing is the combinatorial problem of identifying the defective items
in a population by grouping items into test pools. Recently, nonadaptive group
testing - where all the test pools must be decided on at the start - has been
studied from an information theory point of view. Using techniques from channel
coding, upper and lower bounds have been given on the number of tests required
to accurately recover the defective set, even when the test outcomes can be
noisy.
  In this paper, we give the first information theoretic result on adaptive
group testing - where the outcome of previous tests can influence the makeup of
future tests. We show that adaptive testing does not help much, as the number
of tests required obeys the same lower bound as nonadaptive testing. Our proof
uses similar techniques to the proof that feedback does not improve channel
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5944</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5944</id><created>2012-03-27</created><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author><author><keyname>Mohar</keyname><forenames>Bojan</forenames></author></authors><title>Adding one edge to planar graphs makes crossing number and 1-planarity
  hard</title><categories>cs.CG cs.CC math.CO</categories><comments>27 pages, 10 figures. Part of the results appeared in Proceedings of
  the 26th Annual Symposium on Computational Geometry (SoCG), 68-76, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is near-planar if it can be obtained from a planar graph by adding an
edge. We show the surprising fact that it is NP-hard to compute the crossing
number of near-planar graphs. A graph is 1-planar if it has a drawing where
every edge is crossed by at most one other edge. We show that it is NP-hard to
decide whether a given near-planar graph is 1-planar. The main idea in both
reductions is to consider the problem of simultaneously drawing two planar
graphs inside a disk, with some of its vertices fixed at the boundary of the
disk. This leads to the concept of anchored embedding, which is of independent
interest. As an interesting consequence we obtain a new, geometric proof of
NP-completeness of the crossing number problem, even when restricted to cubic
graphs. This resolves a question of Hlin\v{e}n\'y.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5945</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5945</id><created>2012-03-27</created><updated>2012-04-11</updated><authors><author><keyname>Chen</keyname><forenames>Po-An</forenames></author><author><keyname>Kempe</keyname><forenames>David</forenames></author></authors><title>Bayesian Auctions with Friends and Foes</title><categories>cs.GT</categories><comments>This version was posted without enough prior discussion with my
  collaborator. My collaborator would prefer it not to be posted at this time</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study auctions whose bidders are embedded in a social or economic network.
As a result, even bidders who do not win the auction themselves might derive
utility from the auction, namely, when a friend wins. On the other hand, when
an enemy or competitor wins, a bidder might derive negative utility. Such spite
and altruism will alter the bidding strategies. A simple and natural model for
bidders' utilities in these settings posits that the utility of a losing bidder
i as a result of bidder j winning is a constant (positive or negative) fraction
of bidder j's utility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5948</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5948</id><created>2012-03-27</created><authors><author><keyname>Disanto</keyname><forenames>Filippo</forenames></author><author><keyname>Ferrari</keyname><forenames>Luca</forenames></author><author><keyname>Rinaldi</keyname><forenames>Simone</forenames></author></authors><title>A partial order structure on interval orders</title><categories>math.CO cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a partial order structure on the set of interval orders of a
given size, and prove that such a structure is in fact a lattice. We also
provide a way to compute meet and join inside this lattice. Finally, we show
that, if we restrict to series parallel interval order, what we obtain is the
classical Tamari poset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.5960</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.5960</id><created>2012-03-27</created><updated>2012-04-07</updated><authors><author><keyname>Borgohain</keyname><forenames>Rajdeep</forenames></author><author><keyname>Singh</keyname><forenames>Moirangthem Tiken</forenames></author><author><keyname>Sakharwade</keyname><forenames>Chandrakant</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>TSET: Token based Secure Electronic Transaction</title><categories>cs.CR</categories><comments>7 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security and trust are the most important factors in online transaction, this
paper introduces TSET a Token based Secure Electronic Transaction which is an
improvement over the existing SET, Secure Electronic Transaction protocol. We
take the concept of tokens in the TSET protocol to provide end to end security.
It also provides trust evaluation mechanism so that trustworthiness of the
merchants can be known by customers before being involved in the transaction.
Moreover, we also propose a grading mechanism so that quality of service in the
transactions improves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6001</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6001</id><created>2012-03-27</created><updated>2012-09-26</updated><authors><author><keyname>Pope</keyname><forenames>Graeme</forenames></author><author><keyname>Bracher</keyname><forenames>Annina</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Probabilistic Recovery Guarantees for Sparsely Corrupted Signals</title><categories>cs.IT math.IT</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the recovery of sparse signals subject to sparse interference, as
introduced in Studer et al., IEEE Trans. IT, 2012. We present novel
probabilistic recovery guarantees for this framework, covering varying degrees
of knowledge of the signal and interference support, which are relevant for a
large number of practical applications. Our results assume that the sparsifying
dictionaries are characterized by coherence parameters and we require
randomness only in the signal and/or interference. The obtained recovery
guarantees show that one can recover sparsely corrupted signals with
overwhelming probability, even if the sparsity of both the signal and
interference scale (near) linearly with the number of measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6005</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6005</id><created>2012-03-27</created><updated>2012-03-30</updated><authors><author><keyname>Piwowarski</keyname><forenames>Benjamin</forenames></author></authors><title>The Kernel Quantum Probabilities (KQP) Library</title><categories>cs.MS</categories><comments>Describes the library available at http://kqp.bpiwowar.net/</comments><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document, we show how the different quantities necessary to compute
kernel quantum probabilities can be computed. This document form the basis of
the implementation of the Kernel Quantum Probability (KQP) open source project
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6020</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6020</id><created>2012-03-23</created><updated>2012-03-31</updated><authors><author><keyname>Maknickas</keyname><forenames>Algirdas Antano</forenames></author></authors><title>How to solve kSAT in polynomial time</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With using of multi-nary logic analytic formulas proposition that &quot;kSAT is in
P and could be solved in $O(n^{3.5})$&quot; was proved
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6025</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6025</id><created>2012-03-26</created><updated>2012-06-02</updated><authors><author><keyname>Zhao</keyname><forenames>Hengjun</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author><author><keyname>Kapur</keyname><forenames>Deepak</forenames></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames></author></authors><title>A &quot;Hybrid&quot; Approach for Synthesizing Optimal Controllers of Hybrid
  Systems: A Case Study of the Oil Pump Industrial Example</title><categories>cs.SY cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an approach to reduce the optimal controller
synthesis problem of hybrid systems to quantifier elimination; furthermore, we
also show how to combine quantifier elimination with numerical computation in
order to make it more scalable but at the same time, keep arising errors due to
discretization manageable and within bounds. A major advantage of our approach
is not only that it avoids errors due to numerical computation, but it also
gives a better optimal controller. In order to illustrate our approach, we use
the real industrial example of an oil pump provided by the German company HYDAC
within the European project Quasimodo as a case study throughout this paper,
and show that our method improves (up to 7.5%) the results reported in [3]
based on game theory and model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6027</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6027</id><created>2012-03-27</created><authors><author><keyname>Choudhuri</keyname><forenames>Chiranjib</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Causal State Communication</title><categories>cs.IT math.IT</categories><comments>25 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of state communication over a discrete memoryless channel with
discrete memoryless state is studied when the state information is available
strictly causally at the encoder. It is shown that block Markov encoding, in
which the encoder communicates a description of the state sequence in the
previous block by incorporating side information about the state sequence at
the decoder, yields the minimum state estimation error. When the same channel
is used to send additional independent information at the expense of a higher
channel state estimation error, the optimal tradeoff between the rate of the
independent information and the state estimation error is characterized via the
capacity- distortion function. It is shown that any optimal tradeoff pair can
be achieved via rate-splitting. These coding theorems are then extended
optimally to the case of causal channel state information at the encoder using
the Shannon strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6028</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6028</id><created>2012-03-27</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Randomized Gossip Algorithm with Unreliable Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study an asynchronous randomized gossip algorithm under
unreliable communication. At each instance, two nodes are selected to meet with
a given probability. When nodes meet, two unreliable communication links are
established with communication in each direction succeeding with a time-varying
probability. It is shown that two particularly interesting cases arise when
these communication processes are either perfectly dependent or independent.
Necessary and sufficient conditions on the success probability sequence are
proposed to ensure almost sure consensus or $\epsilon$-consensus. Weak
connectivity is required when the communication is perfectly dependent, while
double connectivity is required when the communication is independent.
Moreover, it is proven that with odd number of nodes, average preserving turns
from almost forever (with probability one for all initial conditions) for
perfectly dependent communication, to almost never (with probability zero for
almost all initial conditions) for the independent case. This average
preserving property does not hold true for general number of nodes. These
results indicate the fundamental role the node interactions have in randomized
gossip algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6030</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6030</id><created>2012-03-27</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>Revisiting the D-iteration method: from theoretical to practical
  computation cost</title><categories>cs.NA math.NA</categories><comments>9 pages</comments><acm-class>G.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the D-iteration algorithm in order to better
explain its connection to the Gauss-Seidel method and different performance
results that were observed. In particular, we study here the practical
computation cost based on the execution runtime compared to the theoretical
number of iterations. We also propose an exact formula of the error for
PageRank class of equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6035</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6035</id><created>2012-03-27</created><authors><author><keyname>Jumadinova</keyname><forenames>Janyl</forenames></author><author><keyname>Dasgupta</keyname><forenames>Prithviraj</forenames></author></authors><title>A Multi-Agent Prediction Market based on Partially Observable Stochastic
  Game</title><categories>cs.MA</categories><comments>20 pages, 8 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel, game theoretic representation of a multi-agent prediction
market using a partially observable stochastic game with information (POSGI).
We then describe a correlated equilibrium (CE)-based solution strategy for this
game which enables each agent to dynamically calculate the prices at which it
should trade a security in the prediction market. We have extended our results
to risk averse traders and shown that a Pareto optimal correlated equilibrium
strategy can be used to incentively truthful revelations from risk averse
agents. Simulation results comparing our CE strategy with five other strategies
commonly used in similar markets, with both risk neutral and risk averse
agents, show that the CE strategy improves price predictions and provides
higher utilities to the agents as compared to other existing strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6036</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6036</id><created>2012-03-16</created><authors><author><keyname>Chowdhury</keyname><forenames>Abhijit</forenames><affiliation>Department of Computer Applications, NSHM College of Management and Technology, Durgapur, West Bengal, India</affiliation></author><author><keyname>Sinha</keyname><forenames>Angshu Kumar</forenames><affiliation>Department of Computer Applications, NSHM College of Management and Technology, Durgapur, West Bengal, India</affiliation></author><author><keyname>Dutta</keyname><forenames>Saurabh</forenames><affiliation>Dr. B. C. Roy Engineering College, Durgapur-713206, West Bengal, India</affiliation></author></authors><title>Proposal of a New Block Cipher reasonably Non-Vulnerable against
  Cryptanalytic Attacks</title><categories>cs.CR</categories><msc-class>94A60</msc-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new block cipher termed as &quot;Modular Arithmetic based
Block Cipher with Varying Key-Spaces (MABCVK)&quot; that uses private key-spaces of
varying lengths to encrypt data files. There is a simple but intelligent use of
theory of modular arithmetic in the scheme of the cipher. Based on observed
implementation of the proposed cipher on a set of real data files of several
types, all results are tabulated and analyzed.The schematic strength of the
cipher and the freedom of using a long key-space expectedly can make it
reasonably nonvulnerable against possible cryptanalytic attacks. As a part of
the future scope of the work, it is also intended to formulate and implement an
enhanced scheme that will use a carrier image to have a secure transmission of
the private key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6049</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6049</id><created>2012-03-27</created><authors><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Pang</keyname><forenames>Gene</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author></authors><title>MDCC: Multi-Data Center Consistency</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replicating data across multiple data centers not only allows moving the data
closer to the user and, thus, reduces latency for applications, but also
increases the availability in the event of a data center failure. Therefore, it
is not surprising that companies like Google, Yahoo, and Netflix already
replicate user data across geographically different regions.
  However, replication across data centers is expensive. Inter-data center
network delays are in the hundreds of milliseconds and vary significantly.
Synchronous wide-area replication is therefore considered to be unfeasible with
strong consistency and current solutions either settle for asynchronous
replication which implies the risk of losing data in the event of failures,
restrict consistency to small partitions, or give up consistency entirely. With
MDCC (Multi-Data Center Consistency), we describe the first optimistic commit
protocol, that does not require a master or partitioning, and is strongly
consistent at a cost similar to eventually consistent protocols. MDCC can
commit transactions in a single round-trip across data centers in the normal
operational case. We further propose a new programming model which empowers the
application developer to handle longer and unpredictable latencies caused by
inter-data center communication. Our evaluation using the TPC-W benchmark with
MDCC deployed across 5 geographically diverse data centers shows that MDCC is
able to achieve throughput and latency similar to eventually consistent quorum
protocols and that MDCC is able to sustain a data center outage without a
significant impact on response times while guaranteeing strong consistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6093</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6093</id><created>2012-03-27</created><authors><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Consensus clustering in complex networks</title><categories>physics.soc-ph cs.IR cs.SI</categories><comments>11 pages, 12 figures. Published in Scientific Reports</comments><journal-ref>Scientific Reports 2, 336 (2012)</journal-ref><doi>10.1038/srep00336</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The community structure of complex networks reveals both their organization
and hidden relationships among their constituents. Most community detection
methods currently available are not deterministic, and their results typically
depend on the specific random seeds, initial conditions and tie-break rules
adopted for their execution. Consensus clustering is used in data analysis to
generate stable results out of a set of partitions delivered by stochastic
methods. Here we show that consensus clustering can be combined with any
existing method in a self-consistent way, enhancing considerably both the
stability and the accuracy of the resulting partitions. This framework is also
particularly suitable to monitor the evolution of community structure in
temporal networks. An application of consensus clustering to a large citation
network of physics papers demonstrates its capability to keep track of the
birth, death and diversification of topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6096</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6096</id><created>2012-03-27</created><authors><author><keyname>Afek</keyname><forenames>Yehuda</forenames></author><author><keyname>Gafni</keyname><forenames>Eli</forenames></author></authors><title>Asynchrony from Synchrony</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider synchronous dynamic networks which like radio networks may have
asymmetric communication links, and are affected by communication rather than
processor failures. In this paper we investigate the minimal message
survivability in a per round basis that allows for the minimal global
cooperation, i.e., allows to solve any task that is wait-free read-write
solvable. The paper completely characterizes this survivability requirement.
Message survivability is formalized by considering adversaries that have a
limited power to remove messages in a round. Removal of a message on a link in
one direction does not necessarily imply the removal of the message on that
link in the other direction. Surprisingly there exist a single strongest
adversary which solves any wait-free read/write task. Any different adversary
that solves any wait-free read/write task is weaker, and any stronger adversary
will not solve any wait-free read/write task. ABD \cite{ABD} who considered
processor failure, arrived at an adversary that is $n/2$ resilient,
consequently can solve tasks, such as $n/2$-set-consensus, which are not
read/write wait-free solvable. With message adversaries, we arrive at an
adversary which has exactly the read-write wait-free power. Furthermore, this
adversary allows for a considerably simpler (simplest that we know of) proof
that the protocol complex of any read/write wait-free task is a subdivided
simplex, finally making this proof accessible for students with no
algebraic-topology prerequisites, and alternatively dispensing with the
assumption that the Immediate Snapshot complex is a subdivided simplex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6098</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6098</id><created>2012-03-27</created><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>Dynamic PageRank using Evolving Teleportation</title><categories>cs.SI cs.IR math.DS physics.soc-ph stat.ML</categories><comments>WAW 2012</comments><acm-class>G.2.2; H.2.8; G.1.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of nodes in a network constantly fluctuates based on changes
in the network structure as well as changes in external interest. We propose an
evolving teleportation adaptation of the PageRank method to capture how changes
in external interest influence the importance of a node. This framework
seamlessly generalizes PageRank because the importance of a node will converge
to the PageRank values if the external influence stops changing. We demonstrate
the effectiveness of the evolving teleportation on the Wikipedia graph and the
Twitter social network. The external interest is given by the number of hourly
visitors to each page and the number of monthly tweets for each user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6102</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6102</id><created>2012-03-27</created><authors><author><keyname>Ren</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Xi</keyname><forenames>Hongwei</forenames></author></authors><title>A Programmer-Centric Approach to Program Verification in ATS</title><categories>cs.PL cs.SE</categories><comments>15 pages, 11 figures. Examples available on-line
  http://www.ats-lang.org/EXAMPLE/PCPV</comments><acm-class>D.2.4; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal specification is widely employed in the construction of high-quality
software. However, there is often a huge gap between formal specification and
actual implementation. While there is already a vast body of work on software
testing and verification, the task to ensure that an implementation indeed
meets its specification is still undeniably of great difficulty. ATS is a
programming language equipped with a highly expressive type system that allows
the programmer to specify and implement and then verify within the language
itself that an implementation meets its specification. In this paper, we
present largely through examples a programmer-centric style of program
verification that puts emphasis on requesting the programmer to explain in a
literate fashion why his or her code works. This is a solid step in the pursuit
of software construction that is verifiably correct according to specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6119</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6119</id><created>2012-03-27</created><updated>2013-08-16</updated><authors><author><keyname>Zhang</keyname><forenames>Haotian</forenames></author><author><keyname>Fata</keyname><forenames>Elaheh</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>Robustness of Complex Networks with Implications for Consensus and
  Contagion</title><categories>cs.SI cs.SY physics.soc-ph</categories><comments>Extended version of paper appearing at the 2012 Conference on
  Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a graph-theoretic property known as robustness, which plays a key
role in certain classes of dynamics on networks (such as resilient consensus,
contagion and bootstrap percolation). This property is stronger than other
graph properties such as connectivity and minimum degree in that one can
construct graphs with high connectivity and minimum degree but low robustness.
However, we show that the notions of connectivity and robustness coincide on
common random graph models for complex networks (Erdos-Renyi, geometric random,
and preferential attachment graphs). More specifically, the properties share
the same threshold function in the Erdos-Renyi model, and have the same values
in one-dimensional geometric graphs and preferential attachment networks. This
indicates that a variety of purely local diffusion dynamics will be effective
at spreading information in such networks. Although graphs generated according
to the above constructions are inherently robust, we also show that it is
coNP-complete to determine whether any given graph is robust to a specified
extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6122</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6122</id><created>2012-03-27</created><updated>2012-03-31</updated><authors><author><keyname>Qian</keyname><forenames>Dajun</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author><author><keyname>Yang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author></authors><title>Diffusion of Real-Time Information in Social-Physical Networks</title><categories>cs.SI physics.soc-ph</categories><comments>add one more figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study the diffusion behavior of real-time information. Typically,
real-time information is valuable only for a limited time duration, and hence
needs to be delivered before its &quot;deadline.&quot; Therefore, real-time information
is much easier to spread among a group of people with frequent interactions
than between isolated individuals. With this insight, we consider a social
network which consists of many cliques and information can spread quickly
within a clique. Furthermore, information can also be shared through online
social networks, such as Facebook, twitter, Youtube, etc.
  We characterize the diffusion of real-time information by studying the phase
transition behaviors. Capitalizing on the theory of inhomogeneous random
networks, we show that the social network has a critical threshold above which
information epidemics are very likely to happen. We also theoretically quantify
the fractional size of individuals that finally receive the message. Finally,
the numerical results indicate that under certain conditions, the large size
cliques in a social network could greatly facilitate the diffusion of real-time
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6127</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6127</id><created>2012-03-27</created><updated>2016-02-18</updated><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author><author><keyname>Geil</keyname><forenames>Olav</forenames></author></authors><title>List Decoding Algorithm based on Voting in Groebner Bases for General
  One-Point AG Codes</title><categories>cs.IT cs.SC math.AC math.AG math.IT</categories><comments>Accepted for publication in J. Symbolic Computation. LaTeX2e
  article.cls, 42 pages, 4 tables, no figures. Ver. 6 added an illustrative
  example of the algorithm execution</comments><msc-class>94B35 (Primary), 13P10, 94B27, 14G50 (Secondary)</msc-class><acm-class>E.4; F.2.1; I.1.2; I.1.4</acm-class><doi>10.1016/j.jsc.2016.02.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the unique decoding algorithm for one-point AG codes over the
Miura-Kamiya Cab curves proposed by Lee, Bras-Amor\'os and O'Sullivan (2012) to
general one-point AG codes, without any assumption. We also extend their unique
decoding algorithm to list decoding, modify it so that it can be used with the
Feng-Rao improved code construction, prove equality between its error
correcting capability and half the minimum distance lower bound by Andersen and
Geil (2008) that has not been done in the original proposal except for
one-point Hermitian codes, remove the unnecessary computational steps so that
it can run faster, and analyze its computational complexity in terms of
multiplications and divisions in the finite field. As a unique decoding
algorithm, the proposed one is empirically and theoretically as fast as the BMS
algorithm for one-point Hermitian codes. As a list decoding algorithm,
extensive experiments suggest that it can be much faster for many moderate
size/usual inputs than the algorithm by Beelen and Brander (2010). It should be
noted that as a list decoding algorithm the proposed method seems to have
exponential worst-case computational complexity while the previous proposals
(Beelen and Brander, 2010; Guruswami and Sudan, 1999) have polynomial ones, and
that the proposed method is expected to be slower than the previous proposals
for very large/special inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6129</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6129</id><created>2012-03-27</created><updated>2013-03-07</updated><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Ruano</keyname><forenames>Diego</forenames></author><author><keyname>Geil</keyname><forenames>Olav</forenames></author></authors><title>Generalization of the Lee-O'Sullivan List Decoding for One-Point AG
  Codes</title><categories>cs.IT cs.SC math.AC math.AG math.IT</categories><comments>article.cls, 14 pages, no figure. The order of authors was changed.
  To appear in Journal of Symbolic Computation. This is an extended journal
  paper version of our earlier conference paper arXiv:1201.6248</comments><msc-class>94B35 (Primary) 13P10, 94B27, 14G50 (Secondary)</msc-class><acm-class>E.4; F.2.1; I.1.2; I.1.4</acm-class><journal-ref>Journal of Symbolic Computation, vol. 55, pp. 1-9, August 2013</journal-ref><doi>10.1016/j.jsc.2013.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the list decoding algorithm for Hermitian codes proposed by Lee
and O'Sullivan based on Gr\&quot;obner bases to general one-point AG codes, under an
assumption weaker than one used by Beelen and Brander. Our generalization
enables us to apply the fast algorithm to compute a Gr\&quot;obner basis of a module
proposed by Lee and O'Sullivan, which was not possible in another
generalization by Lax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6130</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6130</id><created>2012-03-27</created><authors><author><keyname>Foster</keyname><forenames>Dean P.</forenames></author><author><keyname>Rodu</keyname><forenames>Jordan</forenames></author><author><keyname>Ungar</keyname><forenames>Lyle H.</forenames></author></authors><title>Spectral dimensionality reduction for HMMs</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov Models (HMMs) can be accurately approximated using
co-occurrence frequencies of pairs and triples of observations by using a fast
spectral method in contrast to the usual slow methods like EM or Gibbs
sampling. We provide a new spectral method which significantly reduces the
number of model parameters that need to be estimated, and generates a sample
complexity that does not depend on the size of the observation vocabulary. We
present an elementary proof giving bounds on the relative accuracy of
probability estimates from our model. (Correlaries show our bounds can be
weakened to provide either L1 bounds or KL bounds which provide easier direct
comparisons to previous work.) Our theorem uses conditions that are checkable
from the data, instead of putting conditions on the unobservable Markov
transition matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6131</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6131</id><created>2012-03-27</created><authors><author><keyname>Davaslioglu</keyname><forenames>Kemal</forenames></author><author><keyname>Ayanoglu</keyname><forenames>Ender</forenames></author></authors><title>Power Control in Multi-Layer Cellular Networks</title><categories>cs.NI physics.comp-ph</categories><comments>6 pages, 3 figures, submitted to IEEE GLOBECOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the possible performance gains of power control in multi-layer
cellular systems where microcells and picocells are distributed within
macrocells. Although multilayers in cellular networks help increase system
capacity and coverage, and can reduce total energy consumption; they cause
interference, reducing the performance of the network. Therefore, downlink
transmit power levels of multi-layer hierarchical cellular networks need to be
controlled in order to fully exploit their benefits. In this work, we present
an analytical derivation to determine optimum power levels for two-layer
cellular networks and generalize our solution to multi-layer cellular networks.
We also simulate our results in a typical multi-layer network setup and observe
significant power savings compared to single-layer cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6136</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6136</id><created>2012-03-27</created><authors><author><keyname>Rudnick</keyname><forenames>Alex</forenames></author></authors><title>Tree Transducers, Machine Translation, and Cross-Language Divergences</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree transducers are formal automata that transform trees into other trees.
Many varieties of tree transducers have been explored in the automata theory
literature, and more recently, in the machine translation literature. In this
paper I review T and xT transducers, situate them among related formalisms, and
show how they can be used to implement rules for machine translation systems
that cover all of the cross-language structural divergences described in Bonnie
Dorr's influential article on the topic. I also present an implementation of xT
transduction, suitable and convenient for experimenting with translation rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6152</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6152</id><created>2012-03-28</created><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Weil</keyname><forenames>Pascal</forenames><affiliation>LaBRI</affiliation></author></authors><title>The FO^2 alternation hierarchy is decidable</title><categories>cs.LO cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the two-variable fragment FO^2[&lt;] of first-order logic over
finite words. Numerous characterizations of this class are known. Th\'erien and
Wilke have shown that it is decidable whether a given regular language is
definable in FO^2[&lt;]. From a practical point of view, as shown by Weis, FO^2[&lt;]
is interesting since its satisfiability problem is in NP. Restricting the
number of quantifier alternations yields an infinite hierarchy inside the class
of FO^2[&lt;]-definable languages. We show that each level of this hierarchy is
decidable. For this purpose, we relate each level of the hierarchy with a
decidable variety of finite monoids. Our result implies that there are many
different ways of climbing up the FO^2[&lt;]-quantifier alternation hierarchy:
deterministic and co-deterministic products, Mal'cev products with definite and
reverse definite semigroups, iterated block products with J-trivial monoids,
and some inductively defined omega-term identities. A combinatorial tool in the
process of ascension is that of condensed rankers, a refinement of the rankers
of Weis and Immerman and the turtle programs of Schwentick, Th\'erien, and
Vollmer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6157</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6157</id><created>2012-03-28</created><authors><author><keyname>Avron</keyname><forenames>Arnon</forenames><affiliation>Tel-Aviv University</affiliation></author></authors><title>A Logical Framework for Set Theories</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2011, arXiv:1203.5423</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 81, 2012, pp. 3-15</journal-ref><doi>10.4204/EPTCS.81.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Axiomatic set theory is almost universally accepted as the basic theory which
provides the foundations of mathematics, and in which the whole of present day
mathematics can be developed. As such, it is the most natural framework for
Mathematical Knowledge Management. However, in order to be used for this task
it is necessary to overcome serious gaps that exist between the &quot;official&quot;
formulations of set theory (as given e.g. by formal set theory ZF) and actual
mathematical practice.
  In this work we present a new unified framework for formalizations of
axiomatic set theories of different strength, from rudimentary set theory to
full ZF. It allows the use of set terms, but provides a static check of their
validity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6158</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6158</id><created>2012-03-28</created><authors><author><keyname>Miranda-Perea</keyname><forenames>Favio Ezequiel</forenames><affiliation>Facultad de Ciencias UNAM</affiliation></author><author><keyname>Gonz&#xe1;lez-Huesca</keyname><forenames>Lourdes del Carmen</forenames><affiliation>Facultad de Ciencias UNAM</affiliation></author></authors><title>Mendler-style Iso-(Co)inductive predicates: a strongly normalizing
  approach</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LSFA 2011, arXiv:1203.5423</comments><proxy>EPTCS</proxy><acm-class>F.4.1, F.3.1</acm-class><journal-ref>EPTCS 81, 2012, pp. 30-46</journal-ref><doi>10.4204/EPTCS.81.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an extension of the second-order logic AF2 with iso-style
inductive and coinductive definitions specifically designed to extract programs
from proofs a la Krivine-Parigot by means of primitive (co)recursion
principles. Our logic includes primitive constructors of least and greatest
fixed points of predicate transformers, but contrary to the common approach, we
do not restrict ourselves to positive operators to ensure monotonicity, instead
we use the Mendler-style, motivated here by the concept of monotonization of an
arbitrary operator on a complete lattice. We prove an adequacy theorem with
respect to a realizability semantics based on saturated sets and
saturated-valued functions and as a consequence we obtain the strong
normalization property for the proof-term reduction, an important feature which
is absent in previous related work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6159</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6159</id><created>2012-03-28</created><authors><author><keyname>Veloso</keyname><forenames>Paulo A. S.</forenames><affiliation>COPPE-UFRJ, Brazil</affiliation></author><author><keyname>Veloso</keyname><forenames>Sheila R. M.</forenames><affiliation>FEN-UERJ, Brazil</affiliation></author></authors><title>On Graph Refutation for Relational Inclusions</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2011, arXiv:1203.5423</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 81, 2012, pp. 47-62</journal-ref><doi>10.4204/EPTCS.81.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a graphical refutation calculus for relational inclusions: it
reduces establishing a relational inclusion to establishing that a graph
constructed from it has empty extension. This sound and complete calculus is
conceptually simpler and easier to use than the usual ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6160</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6160</id><created>2012-03-28</created><authors><author><keyname>Avelar</keyname><forenames>Andr&#xe9;ia B</forenames><affiliation>Universidade de Bras&#xed;lia</affiliation></author><author><keyname>Galdino</keyname><forenames>Andr&#xe9; L</forenames><affiliation>Universidade Federal de Goi&#xe1;s</affiliation></author><author><keyname>de Moura</keyname><forenames>Fl&#xe1;vio LC</forenames><affiliation>Universidade de Bras&#xed;lia</affiliation></author><author><keyname>Ayala-Rinc&#xf3;n</keyname><forenames>Mauricio</forenames><affiliation>Universidade de Bras&#xed;lia</affiliation></author></authors><title>A Formalization of the Theorem of Existence of First-Order Most General
  Unifiers</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2011, arXiv:1203.5423</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 81, 2012, pp. 63-78</journal-ref><doi>10.4204/EPTCS.81.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a formalization of the theorem of existence of most
general unifiers in first-order signatures in the higher-order proof assistant
PVS. The distinguishing feature of this formalization is that it remains close
to the textbook proofs that are based on proving the correctness of the
well-known Robinson's first-order unification algorithm. The formalization was
applied inside a PVS development for term rewriting systems that provides a
complete formalization of the Knuth-Bendix Critical Pair theorem, among other
relevant theorems of the theory of rewriting. In addition, the formalization
methodology has been proved of practical use in order to verify the correctness
of unification algorithms in the style of the original Robinson's unification
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6161</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6161</id><created>2012-03-28</created><authors><author><keyname>de Ara&#xfa;jo</keyname><forenames>Anderson</forenames><affiliation>University of S&#xe3;o Paulo</affiliation></author><author><keyname>Finger</keyname><forenames>Marcelo</forenames><affiliation>University of S&#xe3;o Paulo</affiliation></author></authors><title>Classical and quantum satisfiability</title><categories>cs.CC quant-ph</categories><comments>In Proceedings LSFA 2011, arXiv:1203.5423</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 81, 2012, pp. 79-84</journal-ref><doi>10.4204/EPTCS.81.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the linear algebraic definition of QSAT and propose a direct
logical characterization of such a definition. We then prove that this logical
version of QSAT is not an extension of classical satisfiability problem (SAT).
This shows that QSAT does not allow a direct comparison between the complexity
classes NP and QMA, for which SAT and QSAT are respectively complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6166</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6166</id><created>2012-03-28</created><authors><author><keyname>Chung</keyname><forenames>N. N.</forenames></author><author><keyname>Chew</keyname><forenames>L. Y.</forenames></author><author><keyname>Zhou</keyname><forenames>J.</forenames></author><author><keyname>Lai</keyname><forenames>C. H.</forenames></author></authors><title>Impact of edge-removal on the centrality betweenness of the best
  spreaders</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 4 figures</comments><journal-ref>EPL 98 (2012) 58004</journal-ref><doi>10.1209/0295-5075/98/58004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The control of epidemic spreading is essential to avoid potential fatal
consequences and also, to lessen unforeseen socio-economic impact. The need for
effective control is exemplified during the severe acute respiratory syndrome
(SARS) in 2003, which has inflicted near to a thousand deaths as well as
bankruptcies of airlines and related businesses. In this article, we examine
the efficacy of control strategies on the propagation of infectious diseases
based on removing connections within real world airline network with the
associated economic and social costs taken into account through defining
appropriate quantitative measures. We uncover the surprising results that
removing less busy connections can be far more effective in hindering the
spread of the disease than removing the more popular connections. Since
disconnecting the less popular routes tend to incur less socio-economic cost,
our finding suggests the possibility of trading minimal reduction in
connectivity of an important hub with efficiencies in epidemic control. In
particular, we demonstrate the performance of various local epidemic control
strategies, and show how our approach can predict their cost effectiveness
through the spreading control characteristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6177</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6177</id><created>2012-03-28</created><authors><author><keyname>Gol</keyname><forenames>Hajar Ghahremani</forenames></author><author><keyname>Razavi</keyname><forenames>Asadollah</forenames></author><author><keyname>Didehva</keyname><forenames>Farzad</forenames></author></authors><title>On Distance Function among Finite Set of Points</title><categories>cs.DM</categories><msc-class>97PXX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical purposes for some geometrical problems in computer science we
have as information the coordinates of some finite points in surface instead of
the whole body of a surface. The problem arised here is: &quot;How to define a
distance function in a finite space?&quot; as we will show the appropriate function
for this purpose is not a metric function. Here we try to define this distance
function in order to apply it in further proposes, specially in the field
setting of transportation theory and vehicle routing problem. More precisely in
this paper we consider VRP problem for two dimensional manifolds in R3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6178</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6178</id><created>2012-03-28</created><updated>2013-01-25</updated><authors><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Statistical Mechanics of Dictionary Learning</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.IT cs.LG math.IT</categories><comments>6 pages, 4 figures</comments><doi>10.1209/0295-5075/103/28008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding a basis matrix (dictionary) by which objective signals are
represented sparsely is of major relevance in various scientific and
technological fields. We consider a problem to learn a dictionary from a set of
training signals. We employ techniques of statistical mechanics of disordered
systems to evaluate the size of the training set necessary to typically succeed
in the dictionary learning. The results indicate that the necessary size is
much smaller than previously estimated, which theoretically supports and/or
encourages the use of dictionary learning in practical situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6214</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6214</id><created>2012-03-28</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Tuan</keyname><forenames>Yong Chee</forenames></author><author><keyname>Aksoy</keyname><forenames>Mehmet Sabih</forenames></author><author><keyname>Syam</keyname><forenames>Wahyudin P.</forenames></author></authors><title>Integrated Solution Modeling Software: A New Paradigm on Information
  Security Review and Assessment</title><categories>cs.CR</categories><comments>International Journal of Science and Advanced Technology (ISSN
  2221-8386)Volume 1 No 10 December 2011 http://www.ijsat.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actually Information security becomes a very important part for the
organization's intangible assets, so level of confidence and stakeholder
trusted are performance indicator as successes organization. Since information
security has a very important role in supporting the activities of the
organization, we need a standard or benchmark which regulates governance over
information security. The main objective of this paper is to implement a novel
practical approach framework to the development of information security
management system (ISMS) assessment and monitoring software, called by
I-SolFramework. System / software is expected to assist stakeholders in
assessing the level of their ISO27001 compliance readiness, the software could
help stakeholders understood security control or called by compliance
parameters, being shorter and more structured. The case study illustrated
provided to the reader with a set of guidelines, that aims easy understood and
applicable as measuring tools for ISMS standards (ISO27001) compliance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6233</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6233</id><created>2012-03-28</created><updated>2013-02-14</updated><authors><author><keyname>Motahari</keyname><forenames>Abolfazl</forenames></author><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Information Theory of DNA Shotgun Sequencing</title><categories>cs.IT math.IT q-bio.GN q-bio.QM</categories><comments>Revised Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA sequencing is the basic workhorse of modern day biology and medicine.
Shotgun sequencing is the dominant technique used: many randomly located short
fragments called reads are extracted from the DNA sequence, and these reads are
assembled to reconstruct the original sequence. A basic question is: given a
sequencing technology and the statistics of the DNA sequence, what is the
minimum number of reads required for reliable reconstruction? This number
provides a fundamental limit to the performance of {\em any} assembly
algorithm. For a simple statistical model of the DNA sequence and the read
process, we show that the answer admits a critical phenomena in the asymptotic
limit of long DNA sequences: if the read length is below a threshold,
reconstruction is impossible no matter how many reads are observed, and if the
read length is above the threshold, having enough reads to cover the DNA
sequence is sufficient to reconstruct. The threshold is computed in terms of
the Renyi entropy rate of the DNA sequence. We also study the impact of noise
in the read process on the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6242</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6242</id><created>2012-03-28</created><authors><author><keyname>Duncan</keyname><forenames>Ross</forenames></author></authors><title>A graphical approach to measurement-based quantum computing</title><categories>quant-ph cs.LO math.CT</categories><comments>To appear in &quot;Compositional methods in Physics and Linguistics&quot; OUP
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computations are easily represented in the graphical notation known
as the ZX-calculus, a.k.a. the red-green calculus. We demonstrate its use in
reasoning about measurement-based quantum computing, where the graphical syntax
directly captures the structure of the entangled states used to represent
computations, and show that the notion of information flow within the entangled
states gives rise to rewriting strategies for proving the correctness of
quantum programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6243</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6243</id><created>2012-03-28</created><updated>2012-03-29</updated><authors><author><keyname>Huber</keyname><forenames>Marco F.</forenames></author></authors><title>Optimal Pruning for Multi-Step Sensor Scheduling</title><categories>cs.SY cs.RO</categories><comments>6 pages, 3 figures, 1 algorithm, accepted for publication as
  technical correspondence in IEEE Transactions on Automatic Control</comments><doi>10.1109/TAC.2011.2175070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the considered linear Gaussian sensor scheduling problem, only one sensor
out of a set of sensors performs a measurement. To minimize the estimation
error over multiple time steps in a computationally tractable fashion, the
so-called information-based pruning algorithm is proposed. It utilizes the
information matrices of the sensors and the monotonicity of the Riccati
equation. This allows ordering sensors according to their information
contribution and excluding many of them from scheduling. Additionally, a tight
lower is calculated for branch-and-bound search, which further improves the
pruning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6246</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6246</id><created>2012-03-28</created><authors><author><keyname>Takeda</keyname><forenames>Koujin</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>A study of the universal threshold in the L1 recovery by statistical
  mechanics</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>6 pages, 3 figures, invited paper in 46th Annual Conference on
  Information Sciences and Systems 2012 (CISS 2012) at Princeton University,
  March 2012</comments><doi>10.1109/CISS.2012.6310755</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the universality of the L1 recovery threshold in compressed
sensing. Previous studies in the fields of statistical mechanics and random
matrix integration have shown that L1 recovery under a random matrix with
orthogonal symmetry has a universal threshold. This indicates that the
threshold of L1 recovery under a non-orthogonal random matrix differs from the
universal one. Taking this into account, we use a simple random matrix without
orthogonal symmetry, where the random entries are not independent, and show
analytically that the threshold of L1 recovery for such a matrix does not
coincide with the universal one. The results of an extensive numerical
experiment are in good agreement with the analytical results, which validates
our methodology. Though our analysis is based on replica heuristics in
statistical mechanics and is not rigorous, the findings nevertheless support
the fact that the universality of the threshold is strongly related to the
symmetry of the random matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6266</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6266</id><created>2012-03-28</created><authors><author><keyname>Aloupis</keyname><forenames>Greg</forenames></author><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author></authors><title>Circle separability queries in logarithmic time</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in the plane. In this paper we study a new
variant of the circular separability problem in which a point set $P$ is
preprocessed so that one can quickly answer queries of the following form:
Given a geometric object $Q$, report the minimum circle containing $P$ and
exluding $Q$. Our data structure can be constructed in $O(n\log n)$ time using
O(n) space, and can be used to answer the query when $Q$ is either a circle or
a convex $m$-gon in $O(\log n)$ or $O(\log n + \log m)$ time, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6274</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6274</id><created>2012-03-28</created><authors><author><keyname>Nutov</keyname><forenames>Zeev</forenames></author></authors><title>Small $\ell$-edge-covers in $k$-connected graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a $k$-edge-connected graph with edge costs $\{c(e):e \in
E\}$ and let $1 \leq \ell \leq k-1$. We show by a simple and short proof, that
$G$ contains an $\ell$-edge cover $I$ such that: $c(I) \leq \frac{\ell}{k}c(E)$
if $G$ is bipartite, or if $\ell |V|$ is even, or if $|E| \geq \frac{k|V|}{2}
+\frac{k}{2\ell}$; otherwise, $c(I) \leq (\frac{\ell}{k}+\frac{1}{k|V|})c(E)$.
The particular case $\ell=k-1$ and unit costs already includes a result of
Cheriyan and Thurimella, that $G$ contains a $(k-1)$-edge-cover of size
$|E|-\lfloor |V|/2 \rfloor$. Using our result, we slightly improve the
approximation ratios for the {\sf $k$-Connected Subgraph} problem (the
node-connectivity version) with uniform and $\beta$-metric costs. We then
consider the dual problem of finding a spanning subgraph of maximum
connectivity $k^*$ with a prescribed number of edges. We give an algorithm that
computes a $(k^*-1)$-connected subgraph, which is tight, since the problem is
NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6276</identifier>
 <datestamp>2013-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6276</id><created>2012-03-28</created><updated>2013-07-23</updated><authors><author><keyname>Sinha</keyname><forenames>Ankur</forenames></author><author><keyname>Malo</keyname><forenames>Pekka</forenames></author><author><keyname>Kuosmanen</keyname><forenames>Timo</forenames></author></authors><title>A Multi-objective Exploratory Procedure for Regression Model Selection</title><categories>stat.CO cs.NE stat.AP</categories><acm-class>G.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variable selection is recognized as one of the most critical steps in
statistical modeling. The problems encountered in engineering and social
sciences are commonly characterized by over-abundance of explanatory variables,
non-linearities and unknown interdependencies between the regressors. An added
difficulty is that the analysts may have little or no prior knowledge on the
relative importance of the variables. To provide a robust method for model
selection, this paper introduces the Multi-objective Genetic Algorithm for
Variable Selection (MOGA-VS) that provides the user with an optimal set of
regression models for a given data-set. The algorithm considers the regression
problem as a two objective task, and explores the Pareto-optimal (best subset)
models by preferring those models over the other which have less number of
regression coefficients and better goodness of fit. The model exploration can
be performed based on in-sample or generalization error minimization. The model
selection is proposed to be performed in two steps. First, we generate the
frontier of Pareto-optimal regression models by eliminating the dominated
models without any user intervention. Second, a decision making process is
executed which allows the user to choose the most preferred model using
visualizations and simple metrics. The method has been evaluated on a recently
published real dataset on Communities and Crime within United States.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6278</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6278</id><created>2012-03-28</created><authors><author><keyname>Frigeri</keyname><forenames>Achille</forenames></author><author><keyname>Pasquale</keyname><forenames>Liliana</forenames></author><author><keyname>Spoletini</keyname><forenames>Paola</forenames></author></authors><title>Fuzzy Time in LTL</title><categories>cs.LO</categories><comments>10 pages</comments><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years, the adoption of active systems has increased in many
fields of computer science, such as databases, sensor networks, and software
engineering. These systems are able to automatically react to events, by
collecting information from outside and internally generating new events.
However, the collection of data is often hampered by uncertainty and vagueness
that can arise from the imprecision of the monitoring infrastructure,
unreliable data sources, and networks. The decision making mechanism used to
produce a reaction is also imprecise, and cannot be evaluated in a crisp way.
It depends on the evaluation of vague temporal constraints, which are expressed
on the collected data by humans. Despite fuzzy logic has been mainly conceived
as a mathematical abstraction to express vagueness, no attempt has been made to
fuzzify the temporal modalities. Existing fuzzy languages do not allow us to
represent temporal properties, such as &quot;almost always&quot; and &quot;soon&quot;. Indeed, the
semantics of existing fuzzy temporal operators is based on the idea of
replacing classical connectives or propositions with their fuzzy counterparts.
To overcome these limitations, we propose a temporal framework, FTL (Fuzzy-time
Temporal Logic), to express vagueness on time. This framework formally defines
a set of fuzzy temporal modalities, which can be customized by choosing a
specific semantics for the connectives. The semantics of the language is sound,
and the introduced modalities respect a set of expected mutual relations. We
also prove that under the assumption that all events are crisp, FTL reduces to
LTL. Finally, for some of the possible fuzzy interpretations of the
connectives, we identify adequate sets of temporal operators, from which it is
possible to derive all the others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6286</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6286</id><created>2012-03-28</created><updated>2015-02-12</updated><authors><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author></authors><title>On the Easiest and Hardest Fitness Functions</title><categories>cs.NE</categories><journal-ref>IEEE Transactions on Evolutionary Computation, 19(2):295-305, 2015</journal-ref><doi>10.1109/TEVC.2014.2318025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hardness of fitness functions is an important research topic in the field
of evolutionary computation. In theory, the study can help understanding the
ability of evolutionary algorithms. In practice, the study may provide a
guideline to the design of benchmarks. The aim of this paper is to answer the
following research questions: Given a fitness function class, which functions
are the easiest with respect to an evolutionary algorithm? Which are the
hardest? How are these functions constructed? The paper provides theoretical
answers to these questions. The easiest and hardest fitness functions are
constructed for an elitist (1+1) evolutionary algorithm to maximise a class of
fitness functions with the same optima. It is demonstrated that the unimodal
functions are the easiest and deceptive functions are the hardest in terms of
the time-fitness landscape. The paper also reveals that the easiest fitness
function to one algorithm may become the hardest to another algorithm, and vice
versa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6316</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6316</id><created>2012-03-28</created><updated>2012-12-29</updated><authors><author><keyname>Pal</keyname><forenames>Sougata</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>Performance Optimization of Multiple Interconnected Heterogeneous Sensor
  Networks via Collaborative Information Sharing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnecting multiple sensor networks is a relatively new research field
which has emerged in the Wireless Sensor Network domain. Wireless Sensor
Networks (WSNs) have typically been seen as logically separate, and few works
have considered interconnection and interaction between them. Interconnecting
multiple heterogeneous sensor networks therefore opens up a new field besides
more traditional research on, e.g., routing, self organization, or MAC layer
development. Up to now, some approaches have been proposed for interconnecting
multiple sensor networks with goals like information sharing or monitoring
multiple sensor networks. In this paper, we propose to utilize inter-WSN
communication to enable Collaborative Performance Optimization, i.e., our
approach aims to optimize the performance of individual WSNs by taking into
account measured information from others. The parameters to be optimized are
energy consumption on the one hand and sensing quality on the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6318</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6318</id><created>2012-03-28</created><authors><author><keyname>Johannesson</keyname><forenames>Erik</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author><author><keyname>Bernhardsson</keyname><forenames>Bo</forenames></author><author><keyname>Ghulchak</keyname><forenames>Andrey</forenames></author></authors><title>Optimal Linear Joint Source-Channel Coding with Delay Constraint</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory on March 28th
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of joint source-channel coding is considered for a stationary
remote (noisy) Gaussian source and a Gaussian channel. The encoder and decoder
are assumed to be causal and their combined operations are subject to a delay
constraint. It is shown that, under the mean-square error distortion metric, an
optimal encoder-decoder pair from the linear and time-invariant (LTI) class can
be found by minimization of a convex functional and a spectral factorization.
The functional to be minimized is the sum of the well-known cost in a
corresponding Wiener filter problem and a new term, which is induced by the
channel noise and whose coefficient is the inverse of the channel's
signal-to-noise ratio. This result is shown to also hold in the case of
vector-valued signals, assuming parallel additive white Gaussian noise
channels. It is also shown that optimal LTI encoders and decoders generally
require infinite memory, which implies that approximations are necessary. A
numerical example is provided, which compares the performance to the lower
bound provided by rate-distortion theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6320</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6320</id><created>2012-03-28</created><authors><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>Dharmawansa</keyname><forenames>Prathapasinghe</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>Locally Best Invariant Test for Multiple Primary User Spectrum Sensing</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. Int. Conf. Cognitive Radio Oriented Wireless Net.
  Commun., June 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-antenna cooperative spectrum sensing in cognitive radio
networks, when there may be multiple primary users. A noise-uncertainty-free
detector that is optimal in the low signal to noise ratio regime is analyzed in
such a scenario. Specifically, we derive the exact moments of the test
statistics involved, which lead to simple and accurate analytical formulae for
the false alarm probability and the decision threshold. Simulations are
provided to examine the accuracy of the derived results, and to compare with
other detectors in realistic sensing scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6324</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6324</id><created>2012-03-28</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Tracing the Man in the Middle in Monoidal Categories</title><categories>cs.LO cs.CR math.CT</categories><comments>23 pages, 20 figures, Coalgebraic Methods in Computer Science (CMCS)
  2012</comments><msc-class>18D10, 97P20, 03G30</msc-class><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Man-in-the-Middle (MM) is not only a ubiquitous attack pattern in security,
but also an important paradigm of network computation and economics.
Recognizing ongoing MM-attacks is an important security task; modeling
MM-interactions is an interesting task for semantics of computation. Traced
monoidal categories are a natural framework for MM-modelling, as the trace
structure provides a tool to hide what happens *in the middle*. An effective
analysis of what has been traced out seems to require an additional property of
traces, called *normality*. We describe a modest model of network computation,
based on partially ordered multisets (pomsets), where basic network
interactions arise from the monoidal trace structure, and a normal trace
structure arises from an iterative, i.e. coalgebraic structure over terms and
messages used in computation and communication. The correspondence is
established using a convenient monadic description of normally traced monoidal
categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6329</identifier>
 <datestamp>2012-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6329</id><created>2012-03-28</created><updated>2012-11-05</updated><authors><author><keyname>Bhavsar</keyname><forenames>Arnav</forenames></author></authors><title>Analysis of Magnification in Depth from Defocus</title><categories>cs.CV</categories><comments>Typo fixed from the previous version: Changed 's' to 's^2' in the
  formula for 'h_r' in the line below equation (11)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In depth from defocus (DFD), when images are captured with different camera
parameters, a relative magnification is induced between them. Image warping is
a simpler solution to account for magnification than seemingly more accurate
optical approaches. This work is an investigation into the effects of
magnification on the accuracy of DFD. We comment on issues regarding scaling
effect on relative blur computation. We statistically analyze accountability of
scale factor, commenting on the bias and efficiency of the estimator that does
not consider scale. We also discuss the effect of interpolation errors on blur
estimation in a warping based solution to handle magnification and carry out
experimental analysis to comment on the blur estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6339</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6339</id><created>2012-03-28</created><authors><author><keyname>Mas</keyname><forenames>Massimiliano Dal</forenames></author></authors><title>Intelligent Interface Architectures for Folksonomy Driven Structure
  Network</title><categories>cs.HC cs.CL cs.CY cs.IR</categories><comments>*** This paper has been accepted to the 5th International Workshop on
  Intelligent Interfaces for Human-Computer Interaction (IIHCI 2012) - Palermo
  Italy, 4-6 July 2012 *** 7 pages, 7 figures; for details see:
  http://www.maxdalmas.com</comments><msc-class>03B65, 03G10, 68M11, 68P05, 68Q55, 68T30, 68U35</msc-class><acm-class>D.2.2; G.1.10; G.2.2; H.1.1; H.1.2; H.3.1; H.3.3; H.3.5; H.5.2;
  H.5.3; H.5.4; I.2.1; I.2.4; I.2.7; I.3.6; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The folksonomy is the result of free personal information or assignment of
tags to an object (determined by the URI) in order to find them. The practice
of tagging is done in a collective environment. Folksonomies are self
constructed, based on co-occurrence of definitions, rather than a hierarchical
structure of the data. The downside of this was that a few sites and
applications are able to successfully exploit the sharing of bookmarks. The
need for tools that are able to resolve the ambiguity of the definitions is
becoming urgent as the need of simple instruments for their visualization,
editing and exploitation in web applications still hinders their diffusion and
wide adoption. An intelligent interactive interface design for folksonomies
should consider the contextual design and inquiry based on a concurrent
interaction for a perceptual user interfaces. To represent folksonomies a new
concept structure called &quot;Folksodriven&quot; is used in this paper. While it is
presented the Folksodriven Structure Network (FSN) to resolve the ambiguity of
definitions of folksonomy tags suggestions for the user. On this base a
Human-Computer Interactive (HCI) systems is developed for the visualization,
navigation, updating and maintenance of folksonomies Knowledge Bases - the FSN
- through the web. System functionalities as well as its internal architecture
will be introduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6346</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6346</id><created>2012-03-28</created><updated>2012-03-29</updated><authors><author><keyname>Brandt</keyname><forenames>Christina</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Kamath</keyname><forenames>Gautam</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert</forenames></author></authors><title>An Analysis of One-Dimensional Schelling Segregation</title><categories>cs.GT</categories><comments>24 pages, to appear in STOC 2012</comments><acm-class>G.3; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the Schelling model of segregation in which a society of n
individuals live in a ring. Each individual is one of two races and is only
satisfied with his location so long as at least half his 2w nearest neighbors
are of the same race as him. In the dynamics, randomly-chosen unhappy
individuals successively swap locations. We consider the average size of
monochromatic neighborhoods in the final stable state. Our analysis is the
first rigorous analysis of the Schelling dynamics. We note that, in contrast to
prior approximate analyses, the final state is nearly integrated: the average
size of monochromatic neighborhoods is independent of n and polynomial in w.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6360</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6360</id><created>2012-03-28</created><updated>2012-04-30</updated><authors><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author><author><keyname>Cheng</keyname><forenames>Justin</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Lee</keyname><forenames>Lillian</forenames></author></authors><title>You had me at hello: How phrasing affects memorability</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>Final version of paper to appear at ACL 2012. 10pp, 1 fig. Data, demo
  memorability test and other info available at
  http://www.cs.cornell.edu/~cristian/memorability.html</comments><acm-class>I.2.7; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the ways in which information achieves widespread public
awareness is a research question of significant interest. We consider whether,
and how, the way in which the information is phrased --- the choice of words
and sentence structure --- can affect this process. To this end, we develop an
analysis framework and build a corpus of movie quotes, annotated with
memorability information, in which we are able to control for both the speaker
and the setting of the quotes. We find that there are significant differences
between memorable and non-memorable quotes in several key dimensions, even
after controlling for situational and contextual factors. One is lexical
distinctiveness: in aggregate, memorable quotes use less common word choices,
but at the same time are built upon a scaffolding of common syntactic patterns.
Another is that memorable quotes tend to be more general in ways that make them
easy to apply in new contexts --- that is, more portable. We also show how the
concept of &quot;memorable language&quot; can be extended across domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6390</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6390</id><created>2012-03-28</created><updated>2012-11-27</updated><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Sun</keyname><forenames>Ruo-Yu</forenames></author><author><keyname>Baligh</keyname><forenames>Hadi</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Joint Base Station Clustering and Beamformer Design for Partial
  Coordinated Transmission in Heterogenous Networks</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Journal on Selected Areas in Communications, special
  issues on Large-Scale multiple-antenna systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the interference management problem in a multicell MIMO
heterogenous network. Within each cell there are a large number of distributed
micro/pico base stations (BSs) that can be potentially coordinated for joint
transmission. To reduce coordination overhead, we consider user-centric BS
clustering so that each user is served by only a small number of (potentially
overlapping) BSs. Thus, given the channel state information, our objective is
to jointly design the BS clustering and the linear beamformers for all BSs in
the network. In this paper, we formulate this problem from a {sparse
optimization} perspective, and propose an efficient algorithm that is based on
iteratively solving a sequence of group LASSO problems. A novel feature of the
proposed algorithm is that it performs BS clustering and beamformer design
jointly rather than separately as is done in the existing approaches for
partial coordinated transmission. Moreover, the cluster size can be controlled
by adjusting a single penalty parameter in the nonsmooth regularized utility
function. The convergence of the proposed algorithm (to a local optimal
solution) is guaranteed, and its effectiveness is demonstrated via extensive
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6396</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6396</id><created>2012-03-28</created><authors><author><keyname>Rahmati</keyname><forenames>Mojtaba</forenames></author><author><keyname>Duman</keyname><forenames>Tolga M.</forenames></author></authors><title>Achievable Rates for Noisy Channels with Synchronization Errors</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop several lower bounds on the capacity of binary input symmetric
output channels with synchronization errors which also suffer from other types
of impairments such as substitutions, erasures, additive white Gaussian noise
(AWGN) etc. More precisely, we show that if the channel with synchronization
errors can be decomposed into a cascade of two channels where only the first
one suffers from synchronization errors and the second one is a memoryless
channel, a lower bound on the capacity of the original channel in terms of the
capacity of the synchronization error-only channel can be derived. To
accomplish this, we derive lower bounds on the mutual information rate between
the transmitted and received sequences (for the original channel) for an
arbitrary input distribution, and then relate this result to the channel
capacity. The results apply without the knowledge of the exact capacity
achieving input distributions. A primary application of our results is that we
can employ any lower bound derived on the capacity of the first channel
(synchronization error channel in the decomposition) to find lower bounds on
the capacity of the (original) noisy channel with synchronization errors. We
apply the general ideas to several specific classes of channels such as
synchronization error channels with erasures and substitutions, with symmetric
q-ary outputs and with AWGN explicitly, and obtain easy-to-compute bounds. We
illustrate that, with our approach, it is possible to derive tighter capacity
lower bounds compared to the currently available bounds in the literature for
certain classes of channels, e.g., deletion/substitution channels and
deletion/AWGN channels (for certain signal to noise ratio (SNR) ranges).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6397</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6397</id><created>2012-03-28</created><updated>2014-08-20</updated><authors><author><keyname>Borodin</keyname><forenames>Allan</forenames></author><author><keyname>Jain</keyname><forenames>Aadhar</forenames></author><author><keyname>Lee</keyname><forenames>Hyun Chul</forenames></author><author><keyname>Ye</keyname><forenames>Yuli</forenames></author></authors><title>Max-Sum Diversification, Monotone Submodular Functions and Dynamic
  Updates</title><categories>cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Result diversification is an important aspect in web-based search, document
summarization, facility location, portfolio management and other applications.
Given a set of ranked results for a set of objects (e.g. web documents,
facilities, etc.) with a distance between any pair, the goal is to select a
subset $S$ satisfying the following three criteria: (a) the subset $S$
satisfies some constraint (e.g. bounded cardinality); (b) the subset contains
results of high &quot;quality&quot;; and (c) the subset contains results that are
&quot;diverse&quot; relative to the distance measure. The goal of result diversification
is to produce a diversified subset while maintaining high quality as much as
possible. We study a broad class of problems where the distances are a metric,
where the constraint is given by independence in a matroid, where quality is
determined by a monotone submodular function, and diversity is defined as the
sum of distances between objects in $S$. Our problem is a generalization of the
{\em max sum diversification} problem studied in \cite{GoSh09} which in turn is
a generaliztion of the {\em max sum $p$-dispersion problem} studied extensively
in location theory. It is NP-hard even with the triangle inequality. We propose
two simple and natural algorithms: a greedy algorithm for a cardinality
constraint and a local search algorithm for an arbitary matroid constraint. We
prove that both algorithms achieve constant approximation ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6400</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6400</id><created>2012-03-28</created><authors><author><keyname>Khoussainova</keyname><forenames>Nodira</forenames></author><author><keyname>Balazinska</keyname><forenames>Magdalena</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>PerfXplain: Debugging MapReduce Job Performance</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  598-609 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While users today have access to many tools that assist in performing large
scale data analysis tasks, understanding the performance characteristics of
their parallel computations, such as MapReduce jobs, remains difficult. We
present PerfXplain, a system that enables users to ask questions about the
relative performances (i.e., runtimes) of pairs of MapReduce jobs. PerfXplain
provides a new query language for articulating performance queries and an
algorithm for generating explanations from a log of past MapReduce job
executions. We formally define the notion of an explanation together with three
metrics, relevance, precision, and generality, that measure explanation
quality. We present the explanation-generation algorithm based on techniques
related to decision-tree building. We evaluate the approach on a log of past
executions on Amazon EC2, and show that our approach can generate quality
explanations, outperforming two naive explanation-generation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6401</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6401</id><created>2012-03-28</created><authors><author><keyname>Gullo</keyname><forenames>Francesco</forenames></author><author><keyname>Tagarelli</keyname><forenames>Andrea</forenames></author></authors><title>Uncertain Centroid based Partitional Clustering of Uncertain Data</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  610-621 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering uncertain data has emerged as a challenging task in uncertain data
management and mining. Thanks to a computational complexity advantage over
other clustering paradigms, partitional clustering has been particularly
studied and a number of algorithms have been developed. While existing
proposals differ mainly in the notions of cluster centroid and clustering
objective function, little attention has been given to an analysis of their
characteristics and limits. In this work, we theoretically investigate major
existing methods of partitional clustering, and alternatively propose a
well-founded approach to clustering uncertain data based on a novel notion of
cluster centroid. A cluster centroid is seen as an uncertain object defined in
terms of a random variable whose realizations are derived based on all
deterministic representations of the objects to be clustered. As demonstrated
theoretically and experimentally, this allows for better representing a cluster
of uncertain objects, thus supporting a consistently improved clustering
performance while maintaining comparable efficiency with existing partitional
clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6402</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6402</id><created>2012-03-28</created><authors><author><keyname>Bahmani</keyname><forenames>Bahman</forenames></author><author><keyname>Moseley</keyname><forenames>Benjamin</forenames></author><author><keyname>Vattani</keyname><forenames>Andrea</forenames></author><author><keyname>Kumar</keyname><forenames>Ravi</forenames></author><author><keyname>Vassilvitskii</keyname><forenames>Sergei</forenames></author></authors><title>Scalable K-Means++</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  622-633 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over half a century old and showing no signs of aging, k-means remains one of
the most popular data processing algorithms. As is well-known, a proper
initialization of k-means is crucial for obtaining a good final solution. The
recently proposed k-means++ initialization algorithm achieves this, obtaining
an initial set of centers that is provably close to the optimum solution. A
major downside of the k-means++ is its inherent sequential nature, which limits
its applicability to massive data: one must make k passes over the data to find
a good initial set of centers. In this work we show how to drastically reduce
the number of passes needed to obtain, in parallel, a good initialization. This
is unlike prevailing efforts on parallelizing k-means that have mostly focused
on the post-initialization phases of k-means. We prove that our proposed
initialization algorithm k-means|| obtains a nearly optimal solution after a
logarithmic number of passes, and then show that in practice a constant number
of passes suffices. Experimental evaluation on real-world large-scale data
demonstrates that k-means|| outperforms k-means++ in both sequential and
parallel settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6403</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6403</id><created>2012-03-28</created><authors><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author><author><keyname>Bourhis</keyname><forenames>Pierre</forenames></author><author><keyname>Ley</keyname><forenames>Clemens</forenames></author></authors><title>Querying Schemas With Access Restrictions</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  634-645 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study verification of systems whose transitions consist of accesses to a
Web-based data-source. An access is a lookup on a relation within a relational
database, fixing values for a set of positions in the relation. For example, a
transition can represent access to a Web form, where the user is restricted to
filling in values for a particular set of fields. We look at verifying
properties of a schema describing the possible accesses of such a system. We
present a language where one can describe the properties of an access path, and
also specify additional restrictions on accesses that are enforced by the
schema. Our main property language, AccLTL, is based on a first-order extension
of linear-time temporal logic, interpreting access paths as sequences of
relational structures. We also present a lower-level automaton model,
Aautomata, which AccLTL specifications can compile into. We show that AccLTL
and A-automata can express static analysis problems related to &quot;querying with
limited access patterns&quot; that have been studied in the database literature in
the past, such as whether an access is relevant to answering a query, and
whether two queries are equivalent in the accessible data they can return. We
prove decidability and complexity results for several restrictions and variants
of AccLTL, and explain which properties of paths can be expressed in each
restriction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6404</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6404</id><created>2012-03-28</created><authors><author><keyname>Graefe</keyname><forenames>Goetz</forenames></author><author><keyname>Kuno</keyname><forenames>Harumi</forenames></author></authors><title>Definition, Detection, and Recovery of Single-Page Failures, a Fourth
  Class of Database Failures</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  646-655 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The three traditional failure classes are system, media, and transaction
failures. Sometimes, however, modern storage exhibits failures that differ from
all of those. In order to capture and describe such cases, single-page failures
are introduced as a fourth failure class. This class encompasses all failures
to read a data page correctly and with plausible contents despite all
correction attempts in lower system levels. Efficient recovery seems to require
a new data structure called the page recovery index. Its transactional
maintenance can be accomplished writing the same number of log records as
today's efficient implementations of logging and recovery. Detection and
recovery of a single-page failure can be sufficiently fast that the affected
data access is merely delayed, without the need to abort the transaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6405</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6405</id><created>2012-03-28</created><authors><author><keyname>Graefe</keyname><forenames>Goetz</forenames></author><author><keyname>Halim</keyname><forenames>Felix</forenames></author><author><keyname>Idreos</keyname><forenames>Stratos</forenames></author><author><keyname>Kuno</keyname><forenames>Harumi</forenames></author><author><keyname>Manegold</keyname><forenames>Stefan</forenames></author></authors><title>Concurrency Control for Adaptive Indexing</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  656-667 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive indexing initializes and optimizes indexes incrementally, as a side
effect of query processing. The goal is to achieve the benefits of indexes
while hiding or minimizing the costs of index creation. However,
index-optimizing side effects seem to turn read-only queries into update
transactions that might, for example, create lock contention. This paper
studies concurrency control in the context of adaptive indexing. We show that
the design and implementation of adaptive indexing rigorously separates index
structures from index contents; this relaxes the constraints and requirements
during adaptive indexing compared to those of traditional index updates. Our
design adapts to the fact that an adaptive index is refined continuously, and
exploits any concurrency opportunities in a dynamic way. A detailed
experimental analysis demonstrates that (a) adaptive indexing maintains its
adaptive properties even when running concurrent queries, (b) adaptive indexing
can exploit the opportunity for parallelism due to concurrent queries, (c) the
number of concurrency conflicts and any concurrency administration overheads
follow an adaptive behavior, decreasing as the workload evolves and adapting to
the workload needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6406</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6406</id><created>2012-03-28</created><authors><author><keyname>Dalvi</keyname><forenames>Nilesh</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Pang</keyname><forenames>Bo</forenames></author></authors><title>An Analysis of Structured Data on the Web</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 7, pp.
  680-691 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the nature and distribution of structured data on
the Web. Web-scale information extraction, or the problem of creating
structured tables using extraction from the entire web, is gathering lots of
research interest. We perform a study to understand and quantify the value of
Web-scale extraction, and how structured information is distributed amongst top
aggregator websites and tail sites for various interesting domains. We believe
this is the first study of its kind, and gives us new insights for information
extraction over the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6408</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6408</id><created>2012-03-28</created><authors><author><keyname>Ding</keyname><forenames>Xuchu</forenames></author><author><keyname>Lazar</keyname><forenames>Mircea</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Formal Abstraction of Linear Systems via Polyhedral Lyapunov Functions</title><categories>cs.SY math.OC</categories><comments>Technical report accompanying a paper to be presented in ADHS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an abstraction algorithm that produces a finite
bisimulation quotient for an autonomous discrete-time linear system. We assume
that the bisimulation quotient is required to preserve the observations over an
arbitrary, finite number of polytopic subsets of the system state space. We
generate the bisimulation quotient with the aid of a sequence of contractive
polytopic sublevel sets obtained via a polyhedral Lyapunov function. The
proposed algorithm guarantees that at iteration $i$, the bisimulation of the
system within the $i$-th sublevel set of the Lyapunov function is completed. We
then show how to use the obtained bisimulation quotient to verify the system
with respect to arbitrary Linear Temporal Logic formulas over the observed
regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6412</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6412</id><created>2012-03-28</created><updated>2012-04-19</updated><authors><author><keyname>Hobor</keyname><forenames>Aquinas</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Gherghina</keyname><forenames>Cristian</forenames><affiliation>National University of Singapore</affiliation></author></authors><title>Barriers in Concurrent Separation Logic: Now With Tool Support!</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.1.3 , D.2.1 D.2.4, D.4.4, F.3.1, F.3.3, F.4.1, F.4.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (April 20,
  2012) lmcs:800</journal-ref><doi>10.2168/LMCS-8(2:2)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop and prove sound a concurrent separation logic for Pthreads-style
barriers. Although Pthreads barriers are widely used in systems, and separation
logic is widely used for verification, there has not been any effort to combine
the two. Unlike locks and critical sections, Pthreads barriers enable
simultaneous resource redistribution between multiple threads and are
inherently stateful, leading to significant complications in the design of the
logic and its soundness proof. We show how our logic can be applied to a
specific example program in a modular way. Our proofs are machine-checked in
Coq. We showcase a program verification toolset that automatically applies the
logic rules and discharges the associated proof obligations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6413</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6413</id><created>2012-03-28</created><updated>2014-04-02</updated><authors><author><keyname>Azarfar</keyname><forenames>Arash</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sans&#xf2;</keyname><forenames>Brunilde</forenames></author></authors><title>Priority Queueing Models for Cognitive Radio Networks with Traffic
  Differentiation</title><categories>cs.NI</categories><comments>36 pages, 14 figures and 2 tables</comments><acm-class>B.2.2; B.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new queueing model providing the accurate average
system time for packets transmitted over a cognitive radio (CR) link for
multiple traffic classes with the preemptive and non-preemptive priority
service disciplines. The analysis considers general packet service time,
general distributions for the channel availability periods and service
interruption periods, and a service-resume transmission. We further introduce
and analyze two novel priority service disciplines for opportunistic spectrum
access (OSA) networks which take advantage of interruptions to preempt low
priority traffic at a low cost. Analytical results, in addition to simulation
results to validate their accuracy, are also provided and illustrate the impact
of different OSA network parameters on the average system time. We particularly
show that, for the same average CR transmission link availability, the packet
system time significantly increases in a semi-static network with long
operating and interruption periods compared to an OSA network with fast
alternating operating and interruption periods. We also present results
indicating that, due to the presence of interruptions, priority queueing
service disciplines provide a greater differentiated service in OSA networks
than in traditional networks. The analytical tools presented in this paper are
general and can be used to analyze the traffic metrics of most OSA networks
carrying multiple classes of traffic with priority queueing service
differentiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6438</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6438</id><created>2012-03-29</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Sampagar</keyname><forenames>Vithal. J.</forenames></author><author><keyname>Suma</keyname><forenames>V.</forenames></author><author><keyname>Maharajan</keyname><forenames>Ezhilarasan</forenames></author></authors><title>A Scheme for Automation of Telecom Data Processing for Business
  Application</title><categories>cs.OH</categories><comments>8 Pages, 8 Figures, Swarm Evolutionary and Memetric Computing
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the telecom industry is witnessing a large scale growth, one of the major
challenges faced in the domain deals with the analysis and processing of
telecom transactional data which are generated in large volumes by embedded
system communication controllers having various functions. This paper deals
with the analysis of such raw data files which are made up of the sequences of
the tokens. It also depicts the method in which the files are parsed for
extracting the information leading to the final storage in predefined data base
tables. The parser is capable of reading the file in a line structured way and
store the tokens into the predefined tables of data bases. The whole process is
automated using the SSIS tools available in the SQL server. The log table is
maintained in each step of the process which will enable tracking of the file
for any risk mitigation. It can extract, transform and load data resulting in
the processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6439</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6439</id><created>2012-03-29</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Suma</keyname><forenames>V.</forenames></author><author><keyname>Kumar</keyname><forenames>N. R. Shashi</forenames></author></authors><title>An Analytical Approach for Project Managers in Effective Defect
  Management in Software Process</title><categories>cs.SE</categories><comments>7 Pages, 8 Figures, 4 Tables 2011 5th Malaysian Conference in
  Software Engineering (MySEC)</comments><journal-ref>2011 5th Malaysian Conference in Software Engineering (MySEC)</journal-ref><doi>10.1109/MySEC.2011.6140669</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defect estimation and prediction are some of the main modulating factors for
the success of software projects in any software industry. Maturity and
competency of a project manager in efficient prediction and estimation of
resource capabilities are one of the strategic driving forces towards the
generation of high quality software. Currently, there are no estimation
techniques developed through empirical analysis to evaluate the decision
capability of a project manager towards resource allocation for effective
defect management. This paper brings out an empirical study carried out in a
product based software organization. Our deep investigation on several projects
throws light on the impact of decision capability of project manager towards
accomplishment of an aforementioned objective. The paper enables project
managers to gain further awareness towards the significance of predictive
positioning in resource allocation in order to develop high quality defect-free
software products. It also enhances the maturity level of the company and its
persistence in the competitive atmosphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6445</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6445</id><created>2012-03-29</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Suma</keyname><forenames>V.</forenames></author><author><keyname>Tiwari</keyname><forenames>Pranesh Kumar</forenames></author></authors><title>Analysis of Test Efficiency during Software Development Process</title><categories>cs.SE</categories><comments>6 Pages, 6 Figures, Proceedings of the Annual International
  Conference on Software Engineering and Applications (SEA 2010-2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the prerequisites of any organization is an unvarying sustainability
in the dynamic and competitive industrial environment. Development of high
quality software is therefore an inevitable constraint of any software
industry. Defect management being one of the highly influencing factors for the
production of high quality software, it is obligatory for the software
organizations to orient them towards effective defect management. Since, the
time of software evolution, testing is deemed a promising technique of defect
management in all IT industries. This paper provides an empirical investigation
of several projects through a case study comprising of four software companies
having various production capabilities. The aim of this investigation is to
analyze the efficiency of test team during software development process. The
study indicates very low-test efficiency at requirements analysis phase and
even lesser test efficiency at design phase of software development.
Subsequently, the study calls for a strong need to improve testing approaches
using techniques such as dynamic testing of design solutions in lieu of static
testing of design document. Dynamic testing techniques enhance the ability of
detection and elimination of design flaws right at the inception phase and
thereby reduce the cost and time of rework. It further improves productivity,
quality and sustainability of software industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6453</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6453</id><created>2012-03-29</created><authors><author><keyname>B&#xe9;rard</keyname><forenames>B&#xe9;atrice</forenames><affiliation>LIP6</affiliation></author><author><keyname>Haddad</keyname><forenames>Serge</forenames><affiliation>LSV</affiliation></author><author><keyname>Sassolas</keyname><forenames>Mathieu</forenames><affiliation>LIP6</affiliation></author></authors><title>Interrupt Timed Automata: verification and expressiveness</title><categories>cs.FL</categories><proxy>ccsd</proxy><journal-ref>Formal Methods in System Design 40, 1 (2012) 41-87</journal-ref><doi>10.1007/s10703-011-0140-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the class of Interrupt Timed Automata (ITA), a subclass of
hybrid automata well suited to the description of timed multi-task systems with
interruptions in a single processor environment. While the reachability problem
is undecidable for hybrid automata we show that it is decidable for ITA. More
precisely we prove that the untimed language of an ITA is regular, by building
a finite automaton as a generalized class graph. We then establish that the
reachability problem for ITA is in NEXPTIME and in PTIME when the number of
clocks is fixed. To prove the first result, we define a subclass ITA- of ITA,
and show that (1) any ITA can be reduced to a language-equivalent automaton in
ITA- and (2) the reachability problem in this subclass is in NEXPTIME (without
any class graph). In the next step, we investigate the verification of real
time properties over ITA. We prove that model checking SCL, a fragment of a
timed linear time logic, is undecidable. On the other hand, we give model
checking procedures for two fragments of timed branching time logic. We also
compare the expressive power of classical timed automata and ITA and prove that
the corresponding families of accepted languages are incomparable. The result
also holds for languages accepted by controlled real-time automata (CRTA), that
extend timed automata. We finally combine ITA with CRTA, in a model which
encompasses both classes and show that the reachability problem is still
decidable. Additionally we show that the languages of ITA are neither closed
under complementation nor under intersection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6454</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6454</id><created>2012-03-29</created><authors><author><keyname>Fakharaldien</keyname><forenames>Mohammed Adam Ibrahim</forenames></author><author><keyname>Zain</keyname><forenames>Jasni Mohamed</forenames></author><author><keyname>Sulaiman</keyname><forenames>Norrozila</forenames></author></authors><title>XRecursive: An Efficient Method to Store and Query XML Documents</title><categories>cs.DB</categories><journal-ref>Australian Journal of Basic and Applied Sciences Volume 5, Issue
  12, December 2011, Pages 2910-2916</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storing XML documents in a relational database is a promising solution
because relational databases are mature and scale very well and they have the
advantages that in a relational database XML data and structured data can
coexist making it possible to build application that involve both kinds of data
with little extra effort . In this paper, we propose an algorithm schema named
XRecursive that translates XML documents to relational database according to
the proposed storing structure. The steps and algorithm are given in details to
describe how to use the storing structure to storage and query XML documents in
relational database. Then we report our experimental results on a real database
to show the performance of our method in some features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6459</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6459</id><created>2012-03-29</created><authors><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Bruneau</keyname><forenames>Julien</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Consel</keyname><forenames>Charles</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Balland</keyname><forenames>Emilie</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Towards a Tool-based Development Methodology for Pervasive Computing
  Applications</title><categories>cs.PL cs.SE</categories><proxy>ccsd</proxy><journal-ref>IEEE TSE: Transactions on Software Engineering (2011)</journal-ref><doi>10.1109/TSE.2011.107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite much progress, developing a pervasive computing application remains a
challenge because of a lack of conceptual frameworks and supporting tools. This
challenge involves coping with heterogeneous devices, overcoming the
intricacies of distributed systems technologies, working out an architecture
for the application, encoding it in a program, writing specific code to test
the application, and finally deploying it. This paper presents a design
language and a tool suite covering the development life-cycle of a pervasive
computing application. The design language allows to define a taxonomy of
area-specific building-blocks, abstracting over their heterogeneity. This
language also includes a layer to define the architecture of an application,
following an architectural pattern commonly used in the pervasive computing
domain. Our underlying methodology assigns roles to the stakeholders, providing
separation of concerns. Our tool suite includes a compiler that takes design
artifacts written in our language as input and generates a programming
framework that supports the subsequent development stages, namely
implementation, testing, and deployment. Our methodology has been applied on a
wide spectrum of areas. Based on these experiments, we assess our approach
through three criteria: expressiveness, usability, and productivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6464</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6464</id><created>2012-03-29</created><authors><author><keyname>Osbild</keyname><forenames>Ralf</forenames></author></authors><title>General Analysis Tool Box for Controlled Perturbation</title><categories>cs.CG</categories><comments>90 pages, 30 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The implementation of reliable and efficient geometric algorithms is a
challenging task. The reason is the following conflict: On the one hand,
computing with rounded arithmetic may question the reliability of programs
while, on the other hand, computing with exact arithmetic may be too expensive
and hence inefficient. One solution is the implementation of controlled
perturbation algorithms which combine the speed of floating-point arithmetic
with a protection mechanism that guarantees reliability, nonetheless.
  This paper is concerned with the performance analysis of controlled
perturbation algorithms in theory. We answer this question with the
presentation of a general analysis tool box. This tool box is separated into
independent components which are presented individually with their interfaces.
This way, the tool box supports alternative approaches for the derivation of
the most crucial bounds. We present three approaches for this task.
Furthermore, we have thoroughly reworked the concept of controlled perturbation
in order to include rational function based predicates into the theory;
polynomial based predicates are included anyway. Even more we introduce
object-preserving perturbations. Moreover, the tool box is designed such that
it reflects the actual behavior of the controlled perturbation algorithm at
hand without any simplifying assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6481</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6481</id><created>2012-03-29</created><updated>2012-04-23</updated><authors><author><keyname>Das</keyname><forenames>Aparna</forenames></author><author><keyname>Fleszar</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Spoerhase</keyname><forenames>Joachim</forenames></author><author><keyname>Veeramoni</keyname><forenames>Sankar</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Polylogarithmic Approximation for Generalized Minimum Manhattan Networks</title><categories>cs.CG cs.DS</categories><comments>14 pages, 5 figures; added appendix and figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of $n$ terminals, which are points in $d$-dimensional Euclidean
space, the minimum Manhattan network problem (MMN) asks for a minimum-length
rectilinear network that connects each pair of terminals by a Manhattan path,
that is, a path consisting of axis-parallel segments whose total length equals
the pair's Manhattan distance. Even for $d=2$, the problem is NP-hard, but
constant-factor approximations are known. For $d \ge 3$, the problem is
APX-hard; it is known to admit, for any $\eps &gt; 0$, an
$O(n^\eps)$-approximation.
  In the generalized minimum Manhattan network problem (GMMN), we are given a
set $R$ of $n$ terminal pairs, and the goal is to find a minimum-length
rectilinear network such that each pair in $R$ is connected by a Manhattan
path. GMMN is a generalization of both MMN and the well-known rectilinear
Steiner arborescence problem (RSA). So far, only special cases of GMMN have
been considered.
  We present an $O(\log^{d+1} n)$-approximation algorithm for GMMN (and, hence,
MMN) in $d \ge 2$ dimensions and an $O(\log n)$-approximation algorithm for 2D.
We show that an existing $O(\log n)$-approximation algorithm for RSA in 2D
generalizes easily to $d&gt;2$ dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6534</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6534</id><created>2012-03-29</created><authors><author><keyname>Joseph</keyname><forenames>R&#xe9;my-Robert</forenames></author></authors><title>Global preferential consistency for the topological sorting-based
  maximal spanning tree problem</title><categories>cs.AI cs.DM</categories><comments>12 pages, 7 figures, conference : Workshop on modeling and solving
  problems with constraints (ECAI 2008-W31), Patras, Greece, 21 july 2008</comments><acm-class>F.4.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new type of fully computable problems, for DSS dedicated to
maximal spanning tree problems, based on deduction and choice: preferential
consistency problems. To show its interest, we describe a new compact
representation of preferences specific to spanning trees, identifying an
efficient maximal spanning tree sub-problem. Next, we compare this problem with
the Pareto-based multiobjective one. And at last, we propose an efficient
algorithm solving the associated preferential consistency problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6536</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6536</id><created>2012-03-29</created><authors><author><keyname>Calvert</keyname><forenames>Jesse A.</forenames></author><author><keyname>Schuster</keyname><forenames>Michael J.</forenames></author><author><keyname>Radziszowski</keyname><forenames>Stanis&#x142;aw P.</forenames></author></authors><title>Computing the Ramsey Number $R(K_5-P_3,K_5)$</title><categories>math.CO cs.DM</categories><msc-class>05C55</msc-class><journal-ref>Journal of Combinatorial Mathematics and Combinatorial Computing,
  82 (2012) 131-140</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a computer-assisted proof of the fact that $R(K_5-P_3, K_5)=25$. This
solves one of the three remaining open cases in Hendry's table, which listed
the Ramsey numbers for pairs of graphs on 5 vertices. We find that there exist
no $(K_5-P_3,K_5)$-good graphs containing a $K_4$ on 23 or 24 vertices, where a
graph $F$ is $(G,H)$-good if $F$ does not contain $G$ and the complement of $F$
does not contain $H$. The unique $(K_5-P_3,K_5)$-good graph containing a $K_4$
on 22 vertices is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6537</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6537</id><created>2012-03-29</created><authors><author><keyname>Ramanathan</keyname><forenames>Sakkaravarthi</forenames><affiliation>LAAS</affiliation></author><author><keyname>Kamoun</keyname><forenames>Aymen</forenames><affiliation>LAAS</affiliation></author><author><keyname>Drira</keyname><forenames>Khalil</forenames><affiliation>LAAS</affiliation></author><author><keyname>Chassot</keyname><forenames>Christophe</forenames><affiliation>LAAS</affiliation></author></authors><title>Ontology-based collaborative framework for disaster recovery scenarios</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at designing of adaptive framework for supporting
collaborative work of different actors in public safety and disaster recovery
missions. In such scenarios, firemen and robots interact to each other to reach
a common goal; firemen team is equipped with smart devices and robots team is
supplied with communication technologies, and should carry on specific tasks.
Here, reliable connection is mandatory to ensure the interaction between
actors. But wireless access network and communication resources are vulnerable
in the event of a sudden unexpected change in the environment. Also, the
continuous change in the mission requirements such as inclusion/exclusion of
new actor, changing the actor's priority and the limitations of smart devices
need to be monitored. To perform dynamically in such case, the presented
framework is based on a generic multi-level modeling approach that ensures
adaptation handled by semantic modeling. Automated self-configuration is driven
by rule-based reconfiguration policies through ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6543</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6543</id><created>2012-03-29</created><authors><author><keyname>Kuipers</keyname><forenames>J.</forenames></author><author><keyname>Ueda</keyname><forenames>T.</forenames></author><author><keyname>Vermaseren</keyname><forenames>J. A. M.</forenames></author><author><keyname>Vollinga</keyname><forenames>J.</forenames></author></authors><title>FORM version 4.0</title><categories>cs.SC hep-ph</categories><comments>26 pages. Uses axodraw</comments><doi>10.1016/j.cpc.2012.12.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present version 4.0 of the symbolic manipulation system FORM. The most
important new features are manipulation of rational polynomials and the
factorization of expressions. Many other new functions and commands are also
added; some of them are very general, while others are designed for building
specific high level packages, such as one for Groebner bases. New is also the
checkpoint facility, that allows for periodic backups during long calculations.
Lastly, FORM 4.0 has become available as open source under the GNU General
Public License version 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6559</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6559</id><created>2012-03-29</created><authors><author><keyname>de Bondt</keyname><forenames>Michiel</forenames></author></authors><title>Solving Mahjong Solitaire boards with peeking</title><categories>cs.CC cs.DM cs.DS</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first prove that solving Mahjong Solitaire boards with peeking is
NP-complete, even if one only allows isolated stacks of the forms /aab/ and
/abb/. We subsequently show that layouts of isolated stacks of heights one and
two can always be solved with peeking, and that doing so is in P, as well as
finding an optimal algorithm for such layouts without peeking.
  Next, we describe a practical algorithm for solving Mahjong Solitaire boards
with peeking, which is simple and fast. The algorithm uses an effective pruning
criterion and a heuristic to find and prioritize critical groups. The ideas of
the algorithm can also be applied to solving Shisen-Sho with peeking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6566</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6566</id><created>2012-03-29</created><updated>2012-04-17</updated><authors><author><keyname>Gruner</keyname><forenames>Alexander</forenames></author><author><keyname>Huber</keyname><forenames>Michael</forenames></author></authors><title>New Combinatorial Construction Techniques for Low-Density Parity-Check
  Codes and Systematic Repeat-Accumulate Codes</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>10 pages; to appear in &quot;IEEE Transactions on Communications&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents several new construction techniques for low-density
parity-check (LDPC) and systematic repeat-accumulate (RA) codes. Based on
specific classes of combinatorial designs, the improved code design focuses on
high-rate structured codes with constant column weights 3 and higher. The
proposed codes are efficiently encodable and exhibit good structural
properties. Experimental results on decoding performance with the sum-product
algorithm show that the novel codes offer substantial practical application
potential, for instance, in high-speed applications in magnetic recording and
optical communications channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6599</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6599</id><created>2012-03-29</created><authors><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author></authors><title>Distributed Randomized Algorithms for the PageRank Computation</title><categories>cs.SY math.OC</categories><journal-ref>IEEE Transactions on Automatic Control, 55: 1987-2002, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the search engine of Google, the PageRank algorithm plays a crucial role
in ranking the search results. The algorithm quantifies the importance of each
web page based on the link structure of the web. We first provide an overview
of the original problem setup. Then, we propose several distributed randomized
schemes for the computation of the PageRank, where the pages can locally update
their values by communicating to those connected by links. The main objective
of the paper is to show that these schemes asymptotically converge in the
mean-square sense to the true PageRank values. A detailed discussion on the
close relations to the multi-agent consensus problems is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6606</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6606</id><created>2012-03-29</created><authors><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author><author><keyname>Bai</keyname><forenames>Er-Wei</forenames></author></authors><title>A Web Aggregation Approach for Distributed Randomized PageRank
  Algorithms</title><categories>cs.SY math.OC</categories><comments>To appear in the IEEE Transactions on Automatic Control, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PageRank algorithm employed at Google assigns a measure of importance to
each web page for rankings in search results. In our recent papers, we have
proposed a distributed randomized approach for this algorithm, where web pages
are treated as agents computing their own PageRank by communicating with linked
pages. This paper builds upon this approach to reduce the computation and
communication loads for the algorithms. In particular, we develop a method to
systematically aggregate the web pages into groups by exploiting the sparsity
inherent in the web. For each group, an aggregated PageRank value is computed,
which can then be distributed among the group members. We provide a distributed
update scheme for the aggregated PageRank along with an analysis on its
convergence properties. The method is especially motivated by results on
singular perturbation techniques for large-scale Markov chains and multi-agent
consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6610</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6610</id><created>2012-03-29</created><authors><author><keyname>Polevoy</keyname><forenames>Gleb</forenames><affiliation>Industrial Engineering and Management, Technion</affiliation></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames><affiliation>Industrial Engineering and Management, Technion</affiliation></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames><affiliation>Industrial Engineering and Management, Technion</affiliation><affiliation>Microsoft Research</affiliation></author></authors><title>Signalling Competition and Social Welfare (Working Paper)</title><categories>cs.GT math.CO</categories><comments>15 pages, no figures</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an environment where sellers compete over buyers. All sellers are
a-priori identical and strategically signal buyers about the product they sell.
In a setting motivated by on-line advertising in display ad exchanges, where
firms use second price auctions, a firm's strategy is a decision about its
signaling scheme for a stream of goods (e.g. user impressions), and a buyer's
strategy is a selection among the firms. In this setting, a single seller will
typically provide partial information and consequently a product may be
allocated inefficiently. Intuitively, competition among sellers may induce
sellers to provide more information in order to attract buyers and thus
increase efficiency. Surprisingly, we show that such a competition among firms
may yield significant loss in consumers' social welfare with respect to the
monopolistic setting. Although we also show that in some cases the competitive
setting yields gain in social welfare, we provide a tight bound on that gain,
which is shown to be small in respect to the above possible loss.
  Our model is tightly connected with the literature on bundling in auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6622</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6622</id><created>2012-03-28</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Tuan</keyname><forenames>Yong Chee</forenames></author></authors><title>A Novel Method on ISO 27001 Reviews: ISMS Compliance Readiness Level
  Measurement</title><categories>cs.CR cs.SE</categories><comments>Computer Science Journal (ISSN: 2221-5905) Volume 2, Issue 1, April
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security is a hot issue to be discussed, ranging from business activities,
correspondence, banking and financial activities; it requires prudence and high
precision. Since information security has a very important role in supporting
activities of the organization, we need a standard or benchmark which regulates
governance over information security. The main objective of this paper is to
implement a novel practical approach framework to the development of
information security management system (ISMS) assessment and monitoring
software, called by I-SolFramework. System / software is expected to assist
stakeholders in assessing the level of their ISO27001 compliance readiness, the
software could help stakeholders understood security control or called by
compliance parameters, being shorter, more structured, high precision and
measured forecasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6630</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6630</id><created>2012-03-29</created><updated>2012-09-25</updated><authors><author><keyname>Tang</keyname><forenames>Junhua</forenames></author><author><keyname>Mansourifard</keyname><forenames>Parisa</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Power Allocation over Two Identical Gilbert-Elliott Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of power allocation over two identical Gilbert-Elliot
communication channels. Our goal is to maximize the expected discounted number
of bits transmitted over an infinite time horizon. This is achieved by choosing
among three possible strategies: (1) betting on channel 1 by allocating all the
power to this channel, which results in high data rate if channel 1 happens to
be in good state, and zero bits transmitted if channel 1 is in bad state (even
if channel 2 is in good state) (2) betting on channel 2 by allocating all the
power to the second channel, and (3) a balanced strategy whereby each channel
is allocated half the total power, with the effect that each channel can
transmit a low data rate if it is in good state. We assume that each channel's
state is only revealed upon transmission of data on that channel. We model this
problem as a partially observable Markov decision processes (MDP), and derive
key threshold properties of the optimal policy. Further, we show that by
formulating and solving a relevant linear program the thresholds can be
determined numerically when system parameters are known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6668</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6668</id><created>2012-03-29</created><updated>2013-01-21</updated><authors><author><keyname>Greenhill</keyname><forenames>Catherine</forenames></author></authors><title>Making Markov chains less lazy</title><categories>math.CO cs.DM</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mixing time of an ergodic, reversible Markov chain can be bounded in
terms of the eigenvalues of the chain: specifically, the second-largest
eigenvalue and the smallest eigenvalue. It has become standard to focus only on
the second-largest eigenvalue, by making the Markov chain &quot;lazy&quot;. (A lazy chain
does nothing at each step with probability at least 1/2, and has only
nonnegative eigenvalues.)
  An alternative approach to bounding the smallest eigenvalue was given by
Diaconis and Stroock and Diaconis and Saloff-Coste. We give examples to show
that using this approach it can be quite easy to obtain a bound on the smallest
eigenvalue of a combinatorial Markov chain which is several orders of magnitude
below the best-known bound on the second-largest eigenvalue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6673</identifier>
 <datestamp>2012-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6673</id><created>2012-03-29</created><authors><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>de Menezes</keyname><forenames>Marcio Argollo</forenames></author></authors><title>Critical behavior of the SIS epidemic model with time-dependent
  infection rate</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><comments>13 pages, 11 figures, Submitted for publication</comments><journal-ref>J. Stat. Mech. P05012 (2012)</journal-ref><doi>10.1088/1742-5468/2012/05/P05012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study a modified Susceptible-Infected-Susceptible (SIS) model
in which the infection rate $\lambda$ decays exponentially with the number of
reinfections $n$, saturating after $n=l$. We find a critical decaying rate
$\epsilon_{c}(l)$ above which a finite fraction of the population becomes
permanently infected. From the mean-field solution and computer simulations on
hypercubic lattices we find evidences that the upper critical dimension is 6
like in the SIR model, which can be mapped in ordinary percolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6686</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6686</id><created>2012-03-29</created><authors><author><keyname>Gauthier</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Otmani</keyname><forenames>Ayoub</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>A Distinguisher-Based Attack of a Homomorphic Encryption Scheme Relying
  on Reed-Solomon Codes</title><categories>cs.CR</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bogdanov and Lee suggested a homomorphic public-key encryption scheme based
on error correcting codes. The underlying public code is a modified
Reed-Solomon code obtained from inserting a zero submatrix in the Vandermonde
generating matrix defining it. The columns that define this submatrix are kept
secret and form a set $L$. We give here a distinguisher that detects if one or
several columns belong to $L$ or not. This distinguisher is obtained by
considering the code generated by component-wise products of codewords of the
public code (the so called &quot;square code&quot;). This operation is applied to
punctured versions of this square code obtained by picking a subset
  $I$ of the whole set of columns. It turns out that the dimension of the
punctured square code is directly related to the cardinality of the
intersection of $I$ with $L$. This allows an attack which recovers the full set
$L$ and which can then decrypt any ciphertext.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6695</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6695</id><created>2012-03-29</created><updated>2012-04-02</updated><authors><author><keyname>Bhaskar</keyname><forenames>Umang</forenames></author><author><keyname>Fleischer</keyname><forenames>Lisa</forenames></author></authors><title>Online Mixed Packing and Covering</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many problems, the inputs arrive over time, and must be dealt with
irrevocably when they arrive. Such problems are online problems. A common
method of solving online problems is to first solve the corresponding linear
program, and then round the fractional solution online to obtain an integral
solution.
  We give algorithms for solving linear programs with mixed packing and
covering constraints online. We first consider mixed packing and covering
linear programs, where packing constraints are given offline and covering
constraints are received online. The objective is to minimize the maximum
multiplicative factor by which any packing constraint is violated, while
satisfying the covering constraints. No prior sublinear competitive algorithms
are known for this problem. We give the first such --- a
polylogarithmic-competitive algorithm for solving mixed packing and covering
linear programs online. We also show a nearly tight lower bound.
  Our techniques for the upper bound use an exponential penalty function in
conjunction with multiplicative updates. While exponential penalty functions
are used previously to solve linear programs offline approximately, offline
algorithms know the constraints beforehand and can optimize greedily. In
contrast, when constraints arrive online, updates need to be more complex.
  We apply our techniques to solve two online fixed-charge problems with
congestion. These problems are motivated by applications in machine scheduling
and facility location. The linear program for these problems is more
complicated than mixed packing and covering, and presents unique challenges. We
show that our techniques combined with a randomized rounding procedure give
polylogarithmic-competitive integral solutions. These problems generalize
online set-cover, for which there is a polylogarithmic lower bound. Hence, our
results are close to tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6705</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6705</id><created>2012-03-29</created><updated>2012-04-01</updated><authors><author><keyname>Cheung</keyname><forenames>Ho Yee</forenames></author><author><keyname>Kwok</keyname><forenames>Tsz Chiu</forenames></author><author><keyname>Lau</keyname><forenames>Lap Chi</forenames></author></authors><title>Fast Matrix Rank Algorithms and Applications</title><categories>cs.DS cs.NA math.NA</categories><acm-class>F.2.1; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing the rank of an m x n matrix A over a
field. We present a randomized algorithm to find a set of r = rank(A) linearly
independent columns in \~O(|A| + r^\omega) field operations, where |A| denotes
the number of nonzero entries in A and \omega &lt; 2.38 is the matrix
multiplication exponent. Previously the best known algorithm to find a set of r
linearly independent columns is by Gaussian elimination, with running time
O(mnr^{\omega-2}). Our algorithm is faster when r &lt; max(m,n), for instance when
the matrix is rectangular. We also consider the problem of computing the rank
of a matrix dynamically, supporting the operations of rank one updates and
additions and deletions of rows and columns. We present an algorithm that
updates the rank in \~O(mn) field operations. We show that these algorithms can
be used to obtain faster algorithms for various problems in numerical linear
algebra, combinatorial optimization and dynamic data structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6713</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6713</id><created>2012-03-30</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>Sooda</keyname><forenames>Kavitha</forenames></author></authors><title>Application of Genetic Algorithm on Quality Graded Networks for
  Intelligent Routing</title><categories>cs.NI</categories><comments>6 Pages, 6 Figures, 3 Tables, 2011 World Congress on Information and
  Communication Technologies</comments><journal-ref>Information and Communication Technologies (WICT), 2011 World
  Congress on Information and Communication Technologies</journal-ref><doi>10.1109/WICT.2011.6141306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decade, significant research has been carried out for realizing
intelligent network routing using advertisement, position and near-optimum node
selection schemes. In this paper, a grade-based two-level node selection method
along with genetic algorithm (GA) is proposed for realizing an efficient
routing scheme. This method assumes that the nodes are intelligent and that
there exists a knowledge base about the environment in their local memory.
There are two levels for approaching the effective route selection process
through grading. At the first level, grade-based selection is applied and at
the second level, the optimum path is explored using GA. The simulation has
been carried out on different topological structures, and a significant
reduction in time is achieved for determining the optimal path through this
method compared to the non-graded networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6716</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6716</id><created>2012-03-30</created><authors><author><keyname>Nair</keyname><forenames>Dr T. R. Gopalakrishnan</forenames></author><author><keyname>Malhotra</keyname><forenames>Meenakshi</forenames></author></authors><title>Creating Intelligent Linking for Information Threading in Knowledge
  Networks</title><categories>cs.AI</categories><comments>5 Pages, 6 Figures, 2 Tables, India Conference (INDICON), 2011</comments><journal-ref>India Conference (INDICON), 2011</journal-ref><doi>10.1109/INDCON.2011.6139335</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Informledge System (ILS) is a knowledge network with autonomous nodes and
intelligent links that integrate and structure the pieces of knowledge. In this
paper, we aim to put forward the link dynamics involved in intelligent
processing of information in ILS. There has been advancement in knowledge
management field which involve managing information in databases from a single
domain. ILS works with information from multiple domains stored in distributed
way in the autonomous nodes termed as Knowledge Network Node (KNN). Along with
the concept under consideration, KNNs store the processed information linking
concepts and processors leading to the appropriate processing of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6719</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6719</id><created>2012-03-30</created><authors><author><keyname>Nair</keyname><forenames>Gopalakrishnan T. R.</forenames></author><author><keyname>Persya</keyname><forenames>Christy A.</forenames></author></authors><title>Critical Task Re-assignment under Hybrid Scheduling Approach in
  Multiprocessor Real-Time Systems</title><categories>cs.OH</categories><comments>8 Pages, 7 Figures, The 23rd IASTED International Conference on
  Parallel and Distributed Computing Systems (PDCS 2011)</comments><doi>10.2316/P.2011.757-071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded hard real time systems require substantial amount of emergency
processing power for the management of large scale systems like a nuclear power
plant under the threat of an earth quake or a future transport systems under a
peril. In order to meet a fully coordinated supervisory control of multiple
domains of a large scale system, it requires the scenario of engaging
multiprocessor real time design. There are various types of scheduling schemes
existing for meeting the critical task assignment in multiple processor
environments and it requires the tracking of faulty conditions of the subsystem
to avoid system underperformance from failure patterns. Hybrid scheduling
usually engages a combined scheduling philosophy comprising of a static
scheduling of a set of tasks and a highly pre-emptive scheduling for another
set of tasks in different situations of process control. There are instances
where highly critical tasks need to be introduced at a least expected
catastrophe and it cannot be ensured to meet all deadline in selected
processors because of the arrival pattern of such tasks and they bear low
tolerance of time to meet the required target. In such circumstances an
effective switching of processors for this set of task is feasible and we
describe a method to achieve this effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6722</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6722</id><created>2012-03-30</created><authors><author><keyname>Bettadapura</keyname><forenames>Vinay</forenames></author></authors><title>Face Expression Recognition and Analysis: The State of the Art</title><categories>cs.CV</categories><acm-class>I.5.4; I.2.10; I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic recognition of facial expressions has been an active research
topic since the early nineties. There have been several advances in the past
few years in terms of face detection and tracking, feature extraction
mechanisms and the techniques used for expression classification. This paper
surveys some of the published work since 2001 till date. The paper presents a
time-line view of the advances made in this field, the applications of
automatic face expression recognizers, the characteristics of an ideal system,
the databases that have been used and the advances made in terms of their
standardization and a detailed summary of the state of the art. The paper also
discusses facial parameterization using FACS Action Units (AUs) and MPEG-4
Facial Animation Parameters (FAPs) and the recent advances in face detection,
tracking and feature extraction methods. Notes have also been presented on
emotions, expressions and facial features, discussion on the six prototypic
expressions and the recent studies on expression classifiers. The paper ends
with a note on the challenges and the future work. This paper has been written
in a tutorial style with the intention of helping students and researchers who
are new to this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6728</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6728</id><created>2012-03-30</created><authors><author><keyname>M.</keyname><forenames>A. W.</forenames><affiliation>Jos</affiliation></author><author><keyname>Schijndel</keyname><forenames>van</forenames><affiliation>Paul</affiliation></author><author><keyname>H.</keyname><forenames>P. W. M.</forenames><affiliation>Paul</affiliation></author><author><keyname>Steskens</keyname></author></authors><title>System Identification for Indoor Climate Control</title><categories>cs.CE</categories><comments>Published at 7th International Conference on System Simulation in
  Buildings, Liege, December 11-13, 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study focuses on the applicability of system identification to identify
building and system dynamics for climate control design. The main problem
regarding the simulation of the dynamic response of a building using building
simulation software is that (1) the simulation of a large complex building is
time consuming, and (2) simulation results often lack information regarding
fast dynamic behaviour (in the order of seconds), since most software uses a
discrete time step, usually fixed to one hour. The first objective is to study
the applicability of system identification to reduce computing time for the
simulation of large complex buildings. The second objective is to research the
applicability of system identification to identify building dynamics based on
discrete time data (one hour) for climate control design. The study illustrates
that system identification is applicable for the identification of building
dynamics with a frequency that is smaller as the maximum sample frequency as
used for identification. The research shows that system identification offers
good perspectives for the modelling of heat, air and moisture processes in a
building. The main advantages of system identification models compared to the
modelling of building dynamics using building simulation software are, that (1)
the computing time is reduced significantly, and (2) system identification
models run in a MATLAB environment, in which many building simulation tools
have been developed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6741</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6741</id><created>2012-03-30</created><authors><author><keyname>Johannesson</keyname><forenames>Erik</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author><author><keyname>Bernhardsson</keyname><forenames>Bo</forenames></author></authors><title>Optimal Linear Control over Channels with Signal-to-Noise Ratio
  Constraints</title><categories>cs.SY math.OC</categories><comments>Submitted to the IEEE Transactions on Automatic Control on March 30th
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a networked control system where a linear time-invariant (LTI)
plant, subject to a stochastic disturbance, is controlled over a communication
channel with colored noise and a signal-to-noise ratio (SNR) constraint. The
controller is based on output feedback and consists of an encoder that measures
the plant output and transmits over the channel, and a decoder that receives
the channel output and issues the control signal. The objective is to stabilize
the plant and minimize a quadratic cost function, subject to the SNR
constraint.
  It is shown that optimal LTI controllers can be obtained by solving a convex
optimization problem in the Youla parameter and performing a spectral
factorization. The functional to minimize is a sum of two terms: the first is
the cost in the classical linear quadratic control problem and the second is a
new term that is induced by the channel noise. %todo ta bort meningen?
  A necessary and sufficient condition on the SNR for stabilization by an LTI
controller follows directly from a constraint of the optimization problem. It
is shown how the minimization can be approximated by a semidefinite program.
The solution is finally illustrated by a numerical example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6742</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6742</id><created>2012-03-30</created><authors><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author></authors><title>A reverse engineering approach to the suppression of citation biases
  reveals universal properties of citation distributions</title><categories>physics.soc-ph cs.DL</categories><comments>9 pages, 6 figures. Supporting information files available at
  http://filrad.homelinux.org</comments><journal-ref>PloS ONE 7, e33833 (2012)</journal-ref><doi>10.1371/journal.pone.0033833</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large amount of information contained in bibliographic databases has
recently boosted the use of citations, and other indicators based on citation
numbers, as tools for the quantitative assessment of scientific research.
Citations counts are often interpreted as proxies for the scientific influence
of papers, journals, scholars, and institutions. However, a rigorous and
scientifically grounded methodology for a correct use of citation counts is
still missing. In particular, cross-disciplinary comparisons in terms of raw
citation counts systematically favors scientific disciplines with higher
citation and publication rates. Here we perform an exhaustive study of the
citation patterns of millions of papers, and derive a simple transformation of
citation counts able to suppress the disproportionate citation counts among
scientific domains. We find that the transformation is well described by a
power-law function, and that the parameter values of the transformation are
typical features of each scientific discipline. Universal properties of
citation patterns descend therefore from the fact that citation distributions
for papers in a specific field are all part of the same family of univariate
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6744</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6744</id><created>2012-03-30</created><updated>2012-05-25</updated><authors><author><keyname>Gaito</keyname><forenames>Sabrina</forenames></author><author><keyname>Zignani</keyname><forenames>Matteo</forenames></author><author><keyname>Rossi</keyname><forenames>Gian Paolo</forenames></author><author><keyname>Sala</keyname><forenames>Alessandra</forenames></author><author><keyname>Wang</keyname><forenames>Xiao</forenames></author><author><keyname>Zheng</keyname><forenames>Haitao</forenames></author><author><keyname>Zhao</keyname><forenames>Ben Y.</forenames></author></authors><title>On the Bursty Evolution of Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high level of dynamics in today's online social networks (OSNs) creates
new challenges for their infrastructures and providers. In particular, dynamics
involving edge creation has direct implications on strategies for resource
allocation, data partitioning and replication. Understanding network dynamics
in the context of physical time is a critical first step towards a predictive
approach towards infrastructure management in OSNs. Despite increasing efforts
to study social network dynamics, current analyses mainly focus on change over
time of static metrics computed on snapshots of social graphs. The limited
prior work models network dynamics with respect to a logical clock. In this
paper, we present results of analyzing a large timestamped dataset describing
the initial growth and evolution of Renren, the leading social network in
China. We analyze and model the burstiness of link creation process, using the
second derivative, i.e. the acceleration of the degree. This allows us to
detect bursts, and to characterize the social activity of a OSN user as one of
four phases: acceleration at the beginning of an activity burst, where link
creation rate is increasing; deceleration when burst is ending and link
creation process is slowing; cruising, when node activity is in a steady state,
and complete inactivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6749</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6749</id><created>2012-03-30</created><authors><author><keyname>Platkowski</keyname><forenames>Tadeusz</forenames></author><author><keyname>Zakrzewski</keyname><forenames>Jan</forenames></author></authors><title>Game Dynamics for Players with Social and Material Preferences</title><categories>nlin.AO cs.GT</categories><comments>24 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dynamics, existence and stability of the equilibrium states
for large populations of individuals who can play various types of
non--cooperative games. The players imitate the most attractive strategies, and
the choice is motivated not only by the material payoffs of the strategies, but
also by their popularity in the population. The parameter which determines the
weights of both factors in the equilibrium states has the same analytical form
for all types of considered games, and is identified with the sensitivity to
reinforcements parameter in the Hernstein's Matching Law. We prove theorems of
existence and uniqueness, and discuss examples of multiple locally stable
polymorphic equilibria for the considered types of games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6750</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6750</id><created>2012-03-30</created><authors><author><keyname>Huber</keyname><forenames>Marco F.</forenames></author></authors><title>Adaptive Gaussian Mixture Filter Based on Statistical Linearization</title><categories>cs.SY stat.AP stat.CO</categories><comments>8 pages, appeared in the proceedings of the 14th International
  Conference on Information Fusion, Chicago, Illinois, USA, July 2011.
  Correction of an error in formula (22).
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5977694&amp;isnumber=5977431</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian mixtures are a common density representation in nonlinear,
non-Gaussian Bayesian state estimation. Selecting an appropriate number of
Gaussian components, however, is difficult as one has to trade of computational
complexity against estimation accuracy. In this paper, an adaptive Gaussian
mixture filter based on statistical linearization is proposed. Depending on the
nonlinearity of the considered estimation problem, this filter dynamically
increases the number of components via splitting. For this purpose, a measure
is introduced that allows for quantifying the locally induced linearization
error at each Gaussian mixture component. The deviation between the nonlinear
and the linearized state space model is evaluated for determining the splitting
direction. The proposed approach is not restricted to a specific statistical
linearization method. Simulations show the superior estimation performance
compared to related approaches and common filtering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6753</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6753</id><created>2012-03-30</created><authors><author><keyname>Zhang</keyname><forenames>Jianwei</forenames></author><author><keyname>Wang</keyname><forenames>Yongchao</forenames></author><author><keyname>Xing</keyname><forenames>Wei</forenames></author><author><keyname>Lu</keyname><forenames>Dongming</forenames></author></authors><title>Extended Equal Service and Differentiated Service Models for
  Peer-to-Peer File Sharing</title><categories>cs.NI</categories><journal-ref>Journal of Communications and Networks, vol. 15, no. 2, pp.
  228-239, Apr. 2013</journal-ref><doi>10.1109/JCN.2013.000037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer (P2P) systems have proved to be the most effective and popular
file sharing applications in recent years. Previous studies mainly focus on the
equal service and the differentiated service strategies when peers have no
initial data before their download. In an upload-constrained P2P file sharing
system, we model both the equal service process and the differentiated service
process when peers' initial data distribution satisfies some special
conditions, and also show how to minimize the time to get the file to any
number of peers. The proposed models can reveal the intrinsic relations among
the initial data amount, the size of peer set and the minimum last finish time.
By using the models, we can also provide arbitrary degree of differentiated
service to a certain number of peers. We believe that our analysis process and
achieved theoretical results could provide fundamental insights into studies on
bandwidth allocation and data scheduling, and can give helpful reference both
for improving system performance and building effective incentive mechanism in
P2P file sharing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6758</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6758</id><created>2012-03-30</created><authors><author><keyname>Bistrian</keyname><forenames>Diana Alina</forenames></author><author><keyname>Dragomirescu</keyname><forenames>Florica Ioana</forenames></author><author><keyname>Savii</keyname><forenames>George</forenames></author></authors><title>Spectral Differentiation Operators and Hydrodynamic Models for Stability
  of Swirling Fluid Systems</title><categories>math.SP cs.NA math.DS math.NA</categories><msc-class>35P15, 35P30, 35M32, 65P40</msc-class><journal-ref>Recent Advances in Applied Mathematics Proceedings of the 14th
  International Conference on Applied Mathematics, Puerto de la Cruz, 2009,
  ISBN: 978-960-474-138-0, pp.328-333</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop hydrodynamic models using spectral differential
operators to investigate the spatial stability of swirling fluid systems.
Including viscosity as a valid parameter of the fluid, the hydrodynamic model
is derived using a nodal Lagrangian basis and the polynomial eigenvalue problem
describing the viscous spatial stability is reduced to a generalized eigenvalue
problem using the companion vector method. For inviscid study the hydrodynamic
model is obtained by means of a class of shifted orthogonal expansion functions
and the spectral differentiation matrix is derived to approximate the discrete
derivatives. The models were applied to a Q-vortex structure, both schemes
providing good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6782</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6782</id><created>2012-03-30</created><authors><author><keyname>Michael</keyname><forenames>Johannes</forenames></author><author><keyname>Chudej</keyname><forenames>Kurt</forenames></author><author><keyname>Pannek</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Modelling and Optimal Control of a Docking Maneuver with an Uncontrolled
  Satellite</title><categories>math.OC cs.SY</categories><comments>6 pages, 4 figures</comments><msc-class>93C10, 34H05, 90C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capturing disused satellites in orbit and their controlled reentry is the aim
of the DEOS space mission. Satellites that ran out of fuel or got damaged pose
a threat to working projects in orbit. Additionally, the reentry of such
objects endangers the population as the place of impact cannot be controlled
anymore. This paper demonstrates the modelling of a rendezvous szenario between
a controlled service satellite and an uncontrolled target. The situation is
modelled via first order ordinary differental equations where a stable target
is considered. In order to prevent a collision of the two spacecrafts and to
ensure both satellites are docked at the end of the maneuver, additional state
constraints, box contraints for the control and a time dependent rendezvous
condition for the final time are added. The problem is formulated as an optimal
control problem with Bolza type cost functional and solved using a full
discretization approach in AMPL/IpOpt. Last, simulation results for capturing a
tumbling satellite are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6785</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6785</id><created>2012-03-30</created><updated>2012-08-29</updated><authors><author><keyname>Gr&#xfc;ne</keyname><forenames>Lars</forenames></author><author><keyname>Pannek</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Worthmann</keyname><forenames>Karl</forenames></author></authors><title>Ensuring Stability in Networked Systems with Nonlinear MPC for
  Continuous Time Systems</title><categories>math.OC cs.NI cs.SY</categories><comments>6 pages, 6 figures</comments><msc-class>93D15, 93D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For networked systems, the control law is typically subject to network flaws
such as delays and packet dropouts. Hence, the time in between updates of the
control law varies unexpectedly. Here, we present a stability theorem for
nonlinear model predictive control with varying control horizon in a continuous
time setting without stabilizing terminal constraints or costs. It turns out
that stability can be concluded under the same conditions as for a (short)
fixed control horizon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6786</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6786</id><created>2012-03-30</created><updated>2013-03-06</updated><authors><author><keyname>Sheehy</keyname><forenames>Donald R.</forenames></author></authors><title>Linear-Size Approximations to the Vietoris-Rips Filtration</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Vietoris-Rips filtration is a versatile tool in topological data
analysis. It is a sequence of simplicial complexes built on a metric space to
add topological structure to an otherwise disconnected set of points. It is
widely used because it encodes useful information about the topology of the
underlying metric space. This information is often extracted from its so-called
persistence diagram. Unfortunately, this filtration is often too large to
construct in full. We show how to construct an O(n)-size filtered simplicial
complex on an $n$-point metric space such that its persistence diagram is a
good approximation to that of the Vietoris-Rips filtration. This new filtration
can be constructed in $O(n\log n)$ time. The constant factors in both the size
and the running time depend only on the doubling dimension of the metric space
and the desired tightness of the approximation. For the first time, this makes
it computationally tractable to approximate the persistence diagram of the
Vietoris-Rips filtration across all scales for large data sets.
  We describe two different sparse filtrations. The first is a zigzag
filtration that removes points as the scale increases. The second is a
(non-zigzag) filtration that yields the same persistence diagram. Both methods
are based on a hierarchical net-tree and yield the same guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6791</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6791</id><created>2012-03-30</created><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>Relative Information Loss - An Introduction</title><categories>cs.IT math.IT</categories><comments>6 pages; submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a relative variant of information loss to characterize the
behavior of deterministic input-output systems. We show that the relative loss
is closely related to Renyi's information dimension. We provide an upper bound
for continuous input random variables and an exact result for a class of
functions (comprising quantizers) with infinite absolute information loss. A
connection between relative information loss and reconstruction error is
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6792</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6792</id><created>2012-03-30</created><authors><author><keyname>Ferrari</keyname><forenames>Luca</forenames></author><author><keyname>Munarini</keyname><forenames>Emanuele</forenames></author></authors><title>Enumeration of edges in some lattices of paths</title><categories>math.CO cs.FL</categories><comments>16 pages</comments><msc-class>05A15, 05A05, 06A07, 06D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We enumerate the edges in the Hasse diagram of several lattices arising in
the combinatorial context of lattice paths. Specifically, we will consider the
case of Dyck, Grand Dyck, Motzkin, Grand Motzkin, Schr\&quot;oder and Grand
Schr\&quot;oder lattices. Finally, we give a general formula for the number of edges
in an arbitrary Young lattice (which can be interpreted in a natural way as a
lattice of paths).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6795</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6795</id><created>2012-03-30</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>On Shift Spaces with Algebraic Structure</title><categories>math.DS cs.DM cs.FL math.RA</categories><comments>12 pages. A shortened version accepted for publication in the
  proceedings of Computability in Europe 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate subshifts with a general algebraic structure and cellular
automata on them, with an emphasis on (order-theoretic) lattices. Our main
results concern the characterization of Boolean algebraic subshifts, conditions
for algebraic subshifts to be recoded into cellwise algebras and the limit
dynamics of homomorphic cellular automata on lattice subshifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6798</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6798</id><created>2012-03-30</created><updated>2012-11-16</updated><authors><author><keyname>Christakou</keyname><forenames>Konstantina</forenames></author><author><keyname>Boudec</keyname><forenames>Jean-Yves Le</forenames></author><author><keyname>Paolone</keyname><forenames>Mario</forenames></author><author><keyname>Tomozei</keyname><forenames>Dan-Cristian</forenames></author></authors><title>Efficient Computation of Sensitivity Coefficients of Node Voltages and
  Line Currents in Unbalanced Radial Electrical Distribution Networks</title><categories>cs.SY</categories><comments>accepted for publication to IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimal control of power distribution systems is becoming
increasingly compelling due to the progressive penetration of distributed
energy resources in this specific layer of the electrical infrastructure.
Distribution systems are, indeed, experiencing significant changes in terms of
operation philosophies that are often based on optimal control strategies
relying on the computation of linearized dependencies between controlled (e.g.
voltages, frequency in case of islanding operation) and control variables (e.g.
power injections, transformers tap positions). As the implementation of these
strategies in real-time controllers imposes stringent time constraints, the
derivation of analytical dependency between controlled and control variables
becomes a non-trivial task to be solved. With reference to optimal voltage and
power flow controls, this paper aims at providing an analytical derivation of
node voltage and line current flows as a function of the nodal power injections
and transformers tap-changers positions. Compared to other approaches presented
in the literature, the one proposed here is based on the use of the [Y]
compound matrix of a generic multi-phase radial unbalanced network. In order to
estimate the computational benefits of the proposed approach, the relevant
improvements are also quantified versus traditional methods. The validation of
the proposed method is carried out by using both IEEE 13 and 34 node test
feeders. The paper finally shows the use of the proposed method for the problem
of optimal voltage control applied to the IEEE 34 node test feeder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6806</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6806</id><created>2012-03-30</created><authors><author><keyname>Bellettini</keyname><forenames>Carlo</forenames></author><author><keyname>Camilli</keyname><forenames>Matteo</forenames></author><author><keyname>Capra</keyname><forenames>Lorenzo</forenames></author><author><keyname>Monga</keyname><forenames>Mattia</forenames></author></authors><title>State Space Exploration of RT Systems in the Cloud</title><categories>cs.SE cs.DC cs.SC</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing availability of distributed and cloud computing frameworks make
it possible to face complex computational problems in a more effective and
convenient way. A notable example is state-space exploration of discrete-event
systems specified in a formal way. The exponential complexity of this task is a
major limitation to the usage of consolidated analysis techniques and tools. We
present and compare two different approaches to state-space explosion, relying
on distributed and cloud frameworks, respectively. These approaches were
designed and implemented following the same computational schema, a sort of map
&amp; fold. They are applied on symbolic state-space exploration of real-time
systems specified by (a timed extension of) Petri Nets, by readapting a
sequential algorithm implemented as a command-line Java tool. The outcome of
several tests performed on a benchmarking specification are presented, thus
showing the convenience of cloud approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6807</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6807</id><created>2012-03-30</created><authors><author><keyname>Ferrari</keyname><forenames>Luca</forenames></author><author><keyname>Munarini</keyname><forenames>Emanuele</forenames></author></authors><title>Enumeration of saturated chains in Dyck lattices</title><categories>math.CO cs.FL</categories><comments>9 pages</comments><msc-class>05A15, 05A05, 06A07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine a general formula to compute the number of saturated chains in
Dyck lattices, and we apply it to find the number of saturated chains of length
2 and 3. We also compute what we call the Hasse index (of order 2 and 3) of
Dyck lattices, which is the ratio between the total number of saturated chains
(of length 2 and 3) and the cardinality of the underlying poset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6845</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6845</id><created>2012-03-30</created><authors><author><keyname>Marrero</keyname><forenames>M&#xf3;nica</forenames></author><author><keyname>S&#xe1;nchez-Cuadrado</keyname><forenames>Sonia</forenames></author><author><keyname>Urbano</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Morato</keyname><forenames>Jorge</forenames></author><author><keyname>Moreiro</keyname><forenames>Jos&#xe9;-Antonio</forenames></author></authors><title>Information Retrieval Systems Adapted to the Biomedical Domain</title><categories>cs.CL cs.IR</categories><comments>6 pages, 4 tables</comments><journal-ref>El Profesional de la Informaci\'on (2010), 19-3</journal-ref><doi>10.3145/epi.2010.may.04</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The terminology used in Biomedicine shows lexical peculiarities that have
required the elaboration of terminological resources and information retrieval
systems with specific functionalities. The main characteristics are the high
rates of synonymy and homonymy, due to phenomena such as the proliferation of
polysemic acronyms and their interaction with common language. Information
retrieval systems in the biomedical domain use techniques oriented to the
treatment of these lexical peculiarities. In this paper we review some of the
techniques used in this domain, such as the application of Natural Language
Processing (BioNLP), the incorporation of lexical-semantic resources, and the
application of Named Entity Recognition (BioNER). Finally, we present the
evaluation methods adopted to assess the suitability of these techniques for
retrieving biomedical resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6859</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6859</id><created>2012-03-30</created><updated>2012-07-29</updated><authors><author><keyname>Parkinson</keyname><forenames>Matthew J.</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Summers</keyname><forenames>Alexander J.</forenames><affiliation>ETH Zurich</affiliation></author></authors><title>The Relationship Between Separation Logic and Implicit Dynamic Frames</title><categories>cs.PL cs.LO</categories><proxy>LMCS</proxy><acm-class>F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 3 (July 31,
  2012) lmcs:802</journal-ref><doi>10.2168/LMCS-8(3:1)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation logic is a concise method for specifying programs that manipulate
dynamically allocated storage. Partially inspired by separation logic, Implicit
Dynamic Frames has recently been proposed, aiming at first-order tool support.
In this paper, we precisely connect the semantics of these two logics. We
define a logic whose syntax subsumes both that of a standard separation logic,
and that of implicit dynamic frames as sub-syntaxes. We define a total heap
semantics for our logic, and, for the separation logic subsyntax, prove it
equivalent the standard partial heaps model. In order to define a semantics
which works uniformly for both subsyntaxes, we define the novel concept of a
minimal state extension, which provides a different (but equivalent) definition
of the semantics of separation logic implication and magic wand connectives,
while also giving a suitable semantics for these connectives in implicit
dynamic frames. We show that our resulting semantics agrees with the existing
definition of weakest pre-condition semantics for the implicit dynamic frames
fragment. Finally, we show that we can encode the separation logic fragment of
our logic into the implicit dynamic frames fragment, preserving semantics. For
the connectives typically supported by tools, this shows that separation logic
can be faithfully encoded in a first-order automatic verification tool
(Chalice).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6864</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6864</id><created>2012-03-30</created><authors><author><keyname>Sardari</keyname><forenames>Mohsen</forenames></author><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Fekri</keyname><forenames>Faramarz</forenames></author></authors><title>Memory-Assisted Universal Compression of Network Flows</title><categories>cs.IT cs.NI math.IT</categories><comments>INFOCOM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the existence of considerable amount of redundancy in the Internet
traffic has stimulated the deployment of several redundancy elimination
techniques within the network. These techniques are often based on either
packet-level Redundancy Elimination (RE) or Content-Centric Networking (CCN).
However, these techniques cannot exploit sub-packet redundancies. Further,
other alternative techniques such as the end-to-end universal compression
solutions would not perform well either over the Internet traffic, as such
techniques require infinite length traffic to effectively remove redundancy.
This paper proposes a memory-assisted universal compression technique that
holds a significant promise for reducing the amount of traffic in the networks.
The proposed work is based on the observation that if a source is to be
compressed and sent over a network, the associated universal code entails a
substantial overhead in transmission due to finite length traffic. However,
intermediate nodes can learn the source statistics and this can be used to
reduce the cost of describing the source statistics, reducing the transmission
overhead for such traffics. We present two algorithms (statistical and
dictionary-based) for the memory-assisted universal lossless compression of
information sources. These schemes are universal in the sense that they do not
require any prior knowledge of the traffic's statistical distribution. We
demonstrate the effectiveness of both algorithms and characterize the
memorization gain using the real Internet traces. Furthermore, we apply these
compression schemes to Internet-like power-law graphs and solve the routing
problem for compressed flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6866</identifier>
 <datestamp>2013-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6866</id><created>2012-03-30</created><updated>2012-04-02</updated><authors><author><keyname>Arroyo</keyname><forenames>David</forenames></author><author><keyname>Diaz</keyname><forenames>Jesus</forenames></author><author><keyname>Rodriguez</keyname><forenames>F. B.</forenames></author></authors><title>Cryptanalysis of a one round chaos-based Substitution Permutation
  Network</title><categories>nlin.CD cs.CR</categories><doi>10.1016/j.sigpro.2012.11.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interleaving of chaos and cryptography has been the aim of a large set of
works since the beginning of the nineties. Many encryption proposals have been
introduced to improve conventional cryptography. However, many proposals
possess serious problems according to the basic requirements for the secure
exchange of information. In this paper we highlight some of the main problems
of chaotic cryptography by means of the analysis of a very recent chaotic
cryptosystem based on a one round Substitution Permutation Network. More
specifically, we show that it is not possible to avoid the security problems of
that encryption architecture just by including a chaotic system as core of the
derived encryption system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6878</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6878</id><created>2012-03-30</created><authors><author><keyname>Marion</keyname><forenames>Jean-Yves</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>P&#xe9;choux</keyname><forenames>Romain</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Complexity Information Flow in a Multi-threaded Imperative Language</title><categories>cs.CC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a type system to analyze the time consumed by multi-threaded
imperative programs with a shared global memory, which delineates a class of
safe multi-threaded programs. We demonstrate that a safe multi-threaded program
runs in polynomial time if (i) it is strongly terminating wrt a
non-deterministic scheduling policy or (ii) it terminates wrt a deterministic
and quiet scheduling policy. As a consequence, we also characterize the set of
polynomial time functions. The type system presented is based on the
fundamental notion of data tiering, which is central in implicit computational
complexity. It regulates the information flow in a computation. This aspect is
interesting in that the type system bears a resemblance to typed based
information flow analysis and notions of non-interference. As far as we know,
this is the first characterization by a type system of polynomial time
multi-threaded programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.6880</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.6880</id><created>2012-03-30</created><updated>2013-12-16</updated><authors><author><keyname>Isaev</keyname><forenames>Mikhail</forenames><affiliation>CMAP</affiliation></author><author><keyname>Isaeva</keyname><forenames>K. V</forenames><affiliation>MIPT</affiliation></author></authors><title>On the class of graphs with strong mixing properties</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><journal-ref>Proceeding of MIPT 5, 6 (2013) 44-54</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study three mixing properties of a graph: large algebraic connectivity,
large Cheeger constant (isoperimetric number) and large spectral gap from 1 for
the second largest eigenvalue of the transition probability matrix of the
random walk on the graph. We prove equivalence of this properties (in some
sense). We give estimates for the probability for a random graph to satisfy
these properties. In addition, we present asymptotic formulas for the numbers
of Eulerian orientations and Eulerian circuits in an undirected simple graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0011</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0011</id><created>2012-03-30</created><authors><author><keyname>Lozano</keyname><forenames>Angel</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Fundamental Limits of Cooperation</title><categories>cs.IT math.IT</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperation is viewed as a key ingredient for interference management in
wireless systems. This paper shows that cooperation has fundamental
limitations. The main result is that even full cooperation between transmitters
cannot in general change an interference-limited network to a noise-limited
network. The key idea is that there exists a spectral efficiency upper bound
that is independent of the transmit power. First, a spectral efficiency upper
bound is established for systems that rely on pilot-assisted channel
estimation; in this framework, cooperation is shown to be possible only within
clusters of limited size, which are subject to out-of-cluster interference
whose power scales with that of the in-cluster signals. Second, an upper bound
is also shown to exist when cooperation is through noncoherent communication;
thus, the spectral efficiency limitation is not a by-product of the reliance on
pilot-assisted channel estimation. Consequently, existing literature that
routinely assumes the high-power spectral efficiency scales with the log of the
transmit power provides only a partial characterization. The complete
characterization proposed in this paper subdivides the high-power regime into a
degrees-of-freedom regime, where the scaling with the log of the transmit power
holds approximately, and a saturation regime, where the spectral efficiency
hits a ceiling that is independent of the power. Using a cellular system as an
example, it is demonstrated that the spectral efficiency saturates at power
levels of operational relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0015</identifier>
 <datestamp>2012-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0015</id><created>2012-03-30</created><updated>2012-07-21</updated><authors><author><keyname>Perony</keyname><forenames>Nicolas</forenames></author><author><keyname>Pfitzner</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Hierarchical Consensus Formation Reduces the Influence of Opinion Bias</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 5 figures</comments><journal-ref>Proceedings of the 26th European Conference on Modelling and
  Simulation (ECMS 2012)</journal-ref><doi>10.7148/2012-0662-0668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the role of hierarchical structures in a simple model of collective
consensus formation based on the bounded confidence model with continuous
individual opinions. For the particular variation of this model considered in
this paper, we assume that a bias towards an extreme opinion is introduced
whenever two individuals interact and form a common decision. As a simple proxy
for hierarchical social structures, we introduce a two-step decision making
process in which in the second step groups of like-minded individuals are
replaced by representatives once they have reached local consensus, and the
representatives in turn form a collective decision in a downstream process. We
find that the introduction of such a hierarchical decision making structure can
improve consensus formation, in the sense that the eventual collective opinion
is closer to the true average of individual opinions than without it. In
particular, we numerically study how the size of groups of like-minded
individuals being represented by delegate individuals affects the impact of the
bias on the final population-wide consensus. These results are of interest for
the design of organisational policies and the optimisation of hierarchical
structures in the context of group decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0029</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0029</id><created>2012-03-30</created><authors><author><keyname>Manolakos</keyname><forenames>Alexandros</forenames></author><author><keyname>Noam</keyname><forenames>Yair</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>Blind Null-space Tracking for MIMO Underlay Cognitive Radio Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind Null Space Learning (BNSL) has recently been proposed for fast and
accurate learning of the null-space associated with the channel matrix between
a secondary transmitter and a primary receiver. In this paper we propose a
channel tracking enhancement of the algorithm, namely the Blind Null Space
Tracking (BNST) algorithm that allows transmission of information to the
Secondary Receiver (SR) while simultaneously learning the null-space of the
time-varying target channel. Specifically, the enhanced algorithm initially
performs a BNSL sweep in order to acquire the null space. Then, it performs
modified Jacobi rotations such that the induced interference to the primary
receiver is kept lower than a given threshold $P_{Th}$ with probability $p$
while information is transmitted to the SR simultaneously. We present
simulation results indicating that the proposed approach has strictly better
performance over the BNSL algorithm for channels with independent Rayleigh
fading with a small Doppler frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0033</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0033</id><created>2012-03-30</created><authors><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>McDowell</keyname><forenames>Luke K.</forenames></author><author><keyname>Aha</keyname><forenames>David W.</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author></authors><title>Transforming Graph Representations for Statistical Relational Learning</title><categories>stat.ML cs.AI cs.LG cs.SI</categories><acm-class>I.2; I.2.6; H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational data representations have become an increasingly important topic
due to the recent proliferation of network datasets (e.g., social, biological,
information networks) and a corresponding increase in the application of
statistical relational learning (SRL) algorithms to these domains. In this
article, we examine a range of representation issues for graph-based relational
data. Since the choice of relational data representation for the nodes, links,
and features can dramatically affect the capabilities of SRL algorithms, we
survey approaches and opportunities for relational representation
transformation designed to improve the performance of these algorithms. This
leads us to introduce an intuitive taxonomy for data representation
transformations in relational domains that incorporates link transformation and
node transformation as symmetric representation tasks. In particular, the
transformation tasks for both nodes and links include (i) predicting their
existence, (ii) predicting their label or type, (iii) estimating their weight
or importance, and (iv) systematically constructing their relevant features. We
motivate our taxonomy through detailed examples and use it to survey and
compare competing approaches for each of these tasks. We also discuss general
conditions for transforming links, nodes, and features. Finally, we highlight
challenges that remain to be addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0034</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0034</id><created>2012-03-30</created><authors><author><keyname>Giacaglia</keyname><forenames>Giuliano</forenames></author><author><keyname>Shi</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Kim</keyname><forenames>MinJi</forenames></author><author><keyname>Lucani</keyname><forenames>Daniel E.</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Systematic Network Coding with the Aid of a Full-Duplex Relay</title><categories>cs.NI</categories><comments>6 pages, 5 figures, submitted to IEEE Globecom</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A characterization of systematic network coding over multi-hop wireless
networks is key towards understanding the trade-off between complexity and
delay performance of networks that preserve the systematic structure. This
paper studies the case of a relay channel, where the source's objective is to
deliver a given number of data packets to a receiver with the aid of a relay.
The source broadcasts to both the receiver and the relay using one frequency,
while the relay uses another frequency for transmissions to the receiver,
allowing for a full-duplex operation of the relay. We analyze the decoding
complexity and delay performance of two types of relays: one that preserves the
systematic structure of the code from the source; another that does not. A
systematic relay forwards uncoded packets upon reception, but transmits coded
packets to the receiver after receiving the first coded packet from the source.
On the other hand, a non-systematic relay always transmits linear combinations
of previously received packets. We compare the performance of these two
alternatives by analytically characterizing the expected transmission
completion time as well as the number of uncoded packets forwarded by the
relay. Our numerical results show that, for a poor channel between the source
and the receiver, preserving the systematic structure at the relay (i) allows a
significant increase in the number of uncoded packets received by the receiver,
thus reducing the decoding complexity, and (ii) preserves close to optimal
delay performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0047</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0047</id><created>2012-03-30</created><updated>2013-07-16</updated><authors><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Azimi</keyname><forenames>Javad</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli</forenames></author><author><keyname>Zhang</keyname><forenames>Ruofei</forenames></author></authors><title>A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization</title><categories>cs.LG stat.ML</categories><comments>ECML 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimizing unknown costly-to-evaluate functions has been
studied for a long time in the context of Bayesian Optimization. Algorithms in
this field aim to find the optimizer of the function by asking only a few
function evaluations at locations carefully selected based on a posterior
model. In this paper, we assume the unknown function is Lipschitz continuous.
Leveraging the Lipschitz property, we propose an algorithm with a distinct
exploration phase followed by an exploitation phase. The exploration phase aims
to select samples that shrink the search space as much as possible. The
exploitation phase then focuses on the reduced search space and selects samples
closest to the optimizer. Considering the Expected Improvement (EI) as a
baseline, we empirically show that the proposed algorithm significantly
outperforms EI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0052</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0052</id><created>2012-03-30</created><updated>2012-06-05</updated><authors><author><keyname>Lee</keyname><forenames>Kwankyu</forenames></author></authors><title>Unique Decoding of Plane AG Codes Revisited</title><categories>cs.IT math.IT</categories><comments>18 pages, submitted in a revised form to a journal different from AMC</comments><msc-class>Primary: 94B35, 94B27, Secondary: 13P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reformulate a recently introduced interpolation-based unique decoding
algorithm of algebraic geometry codes using the theory of Gr\&quot;obner bases of
modules on the coordinate ring of the base curve. With the same decoding
performance, the new algorithm has a more conceptual description that lets us
better understand the majority voting procedure central in the
interpolation-based unique decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0053</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0053</id><created>2012-03-30</created><updated>2012-04-16</updated><authors><author><keyname>Carette</keyname><forenames>Jacques</forenames></author><author><keyname>O'Connor</keyname><forenames>Russell</forenames></author></authors><title>Theory Presentation Combinators</title><categories>cs.MS cs.SC math.CT</categories><comments>Extended version of paper to appear in proceedings of CICM 2012</comments><journal-ref>AISC/MKM/Calculemus 2012: 202-215</journal-ref><doi>10.1007/978-3-642-31374-5_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We motivate and give semantics to theory presentation combinators as the
foundational building blocks for a scalable library of theories. The key
observation is that the category of contexts and fibered categories are the
ideal theoretical tools for this purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0056</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0056</id><created>2012-03-30</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Tuan</keyname><forenames>Yong Chee</forenames></author><author><keyname>Aksoy</keyname><forenames>Mehmet Sabih</forenames></author></authors><title>I-SolFramework: An Integrated Solution Framework Six Layers Assessment
  on Multimedia Information Security Architecture Policy Compliance</title><categories>cs.MM</categories><comments>International Journal of Electrical &amp; Computer Sciences IJECS-IJENS
  Vol: 12 No: 01 (126501-9494 IJECS-IJENS \c{opyright} February 2012 IJENS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimedia Information security becomes a important part for the
organization's intangible assets. Level of confidence and stakeholder trusted
are performance indicator as successes organization, it is imperative for
organizations to use Information Security Management System (ISMS) to
effectively manage their multimedia information assets. The main objective of
this paper is to Provide a novel practical framework approach to the
development of ISMS, Called by the I-SolFramework, implemented in multimedia
information security architecture (MISA), it divides a problem into six object
domains or six layers, namely organization,stakeholders, tool &amp; technology,
policy, knowledge, and culture. In addition, this framework also introduced
novelty algorithm and mathematic models as measurement and assessment tools of
MISA parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0062</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0062</id><created>2012-03-30</created><updated>2013-06-21</updated><authors><author><keyname>Boutsidis</keyname><forenames>Christos</forenames></author><author><keyname>Gittens</keyname><forenames>Alex</forenames></author></authors><title>Improved matrix algorithms via the Subsampled Randomized Hadamard
  Transform</title><categories>cs.DS math.NA</categories><comments>to appear in SIAM Journal on Matrix Analysis and Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent randomized linear algebra algorithms rely upon fast dimension
reduction methods. A popular choice is the Subsampled Randomized Hadamard
Transform (SRHT). In this article, we address the efficacy, in the Frobenius
and spectral norms, of an SRHT-based low-rank matrix approximation technique
introduced by Woolfe, Liberty, Rohklin, and Tygert. We establish a slightly
better Frobenius norm error bound than currently available, and a much sharper
spectral norm error bound (in the presence of reasonable decay of the singular
values). Along the way, we produce several results on matrix operations with
SRHTs (such as approximate matrix multiplication) that may be of independent
interest. Our approach builds upon Tropp's in &quot;Improved analysis of the
Subsampled Randomized Hadamard Transform&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0065</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0065</id><created>2012-03-30</created><authors><author><keyname>Lim</keyname><forenames>Ian</forenames></author></authors><title>MIMO Z Channel Interference Management</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIMO Z Channel is investigated in this paper. We focus on how to tackle the
interference when different users try to send their codewords to their
corresponding receivers while only one user will cause interference to the
other. We assume there are two transmitters and two receivers each with two
antennas. We propose a strategy to remove the interference while allowing
different users transmit at the same time. Our strategy is low-complexity while
the performance is good. Mathematical analysis is provided and simulations are
given based on our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0067</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0067</id><created>2012-03-30</created><authors><author><keyname>Zeng</keyname><forenames>Shuqing</forenames></author></authors><title>Estimating Rigid Transformation Between Two Range Maps Using Expectation
  Maximization Algorithm</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We address the problem of estimating a rigid transformation between two point
sets, which is a key module for target tracking system using Light Detection
And Ranging (LiDAR). A fast implementation of Expectation-maximization (EM)
algorithm is presented whose complexity is O(N) with $N$ the number of scan
points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0072</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0072</id><created>2012-03-31</created><updated>2013-03-29</updated><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author><author><keyname>Li</keyname><forenames>Qingguo</forenames></author><author><keyname>Guo</keyname><forenames>Lankun</forenames></author></authors><title>Generalized fuzzy rough sets based on fuzzy coverings</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper further studies the fuzzy rough sets based on fuzzy coverings. We
first present the notions of the lower and upper approximation operators based
on fuzzy coverings and derive their basic properties. To facilitate the
computation of fuzzy coverings for fuzzy covering rough sets, the concepts of
fuzzy subcoverings, the reducible and intersectional elements, the union and
intersection operations are provided and their properties are discussed in
detail. Afterwards, we introduce the concepts of consistent functions and fuzzy
covering mappings and provide a basic theoretical foundation for the
communication between fuzzy covering information systems. In addition, the
notion of homomorphisms is proposed to reveal the relationship between fuzzy
covering information systems. We show how large-scale fuzzy covering
information systems and dynamic fuzzy covering information systems can be
converted into small-scale ones by means of homomorphisms. Finally, an
illustrative example is employed to show that the attribute reduction can be
simplified significantly by our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0075</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0075</id><created>2012-03-31</created><updated>2012-04-12</updated><authors><author><keyname>&#x15a;mieja</keyname><forenames>Marek</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Weighted Approach to R\'enyi Entropy</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  R\'enyi entropy of order \alpha is a general measure of entropy. In this
paper we derive estimations for the R\'enyi entropy of the mixture of sources
in terms of the entropy of the single sources. These relations allow to compute
the R\'enyi entropy dimension of arbitrary order of a mixture of measures.
  The key for obtaining these results is our new definition of the weighted
R\'enyi entropy. It is shown that weighted entropy is equal to the classical
R\'enyi entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0077</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0077</id><created>2012-03-31</created><updated>2013-02-15</updated><authors><author><keyname>Genest</keyname><forenames>Blaise</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gimbert</keyname><forenames>Hugo</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Muscholl</keyname><forenames>Anca</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames><affiliation>LaBRI</affiliation></author></authors><title>Asynchronous Games over Tree Architectures</title><categories>cs.FL cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of controlling in a distributed way a Zielonka
asynchronous automaton. Every process of a controller has access to its causal
past to determine the next set of actions it proposes to play. An action can be
played only if every process controlling this action proposes to play it. We
consider reachability objectives: every process should reach its set of final
states. We show that this control problem is decidable for tree architectures,
where every process can communicate with its parent, its children, and with the
environment. The complexity of our algorithm is l-fold exponential with l being
the height of the tree representing the architecture. We show that this is
unavoidable by showing that even for three processes the problem is
EXPTIME-complete, and that it is non-elementary in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0078</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0078</id><created>2012-03-31</created><authors><author><keyname>&#x15a;mieja</keyname><forenames>Marek</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>Partition Reduction for Lossy Data Compression Problem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the computational aspects of lossy data compression problem,
where the compression error is determined by a cover of the data space. We
propose an algorithm which reduces the number of partitions needed to find the
entropy with respect to the compression error. In particular, we show that, in
the case of finite cover, the entropy is attained on some partition. We give an
algorithmic construction of such partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0094</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0094</id><created>2012-03-31</created><authors><author><keyname>Iyer</keyname><forenames>Thava</forenames></author><author><keyname>Hsieh</keyname><forenames>Robert</forenames></author><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Varghese</keyname><forenames>Benoy</forenames></author><author><keyname>Boreli</keyname><forenames>Roksana</forenames></author></authors><title>Mobile P2P Trusted On-Demand Video Streaming</title><categories>cs.NI</categories><comments>Published as demo in Local Computer Network conference (LCN 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to demonstrate a mobile server assisted P2P system for on-demand
video streaming. Our proposed solution uses a combination of 3G and ad-hoc
Wi-Fi connections, to enable mobile devices to download content from a
centralised server in a way that minimises the 3G bandwidth use and cost. On
the customised GUI, we show the corresponding reduction in 3G bandwidth
achieved by increasing the number of participating mobile devices in the
combined P2P and ad-hoc Wi- Fi network, while demonstrating the good video
playout quality on each of the mobiles. We also demonstrate the implemented
trust mechanism which enables mobiles to only use trusted adhoc connections.
The system has been implemented on Android based smartphones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0100</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0100</id><created>2012-03-31</created><authors><author><keyname>Cui</keyname><forenames>Ai-Xiang</forenames></author><author><keyname>Yang</keyname><forenames>Zimo</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Roles of Ties in Spreading</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Controlling global epidemics in the real world and accelerating
information propagation in the artificial world are of great significance,
which have activated an upsurge in the studies on networked spreading dynamics.
Lots of efforts have been made to understand the impacts of macroscopic
statistics (e.g., degree distribution and average distance) and mesoscopic
structures (e.g., communities and rich clubs) on spreading processes while the
microscopic elements are less concerned. In particular, roles of ties are not
yet clear to the academic community.
  Methodology/Principle Findings: Every edges is stamped by its strength that
is defined solely based on the local topology. According to a weighted
susceptible-infected-susceptible model, the steady-state infected density and
spreading speed are respectively optimized by adjusting the relationship
between edge's strength and spreading ability. Experiments on six real networks
show that the infected density is increased when strong ties are favored in the
spreading, while the speed is enhanced when weak ties are favored. Significance
of these findings is further demonstrated by comparing with a null model.
  Conclusions/Significance: Experimental results indicate that strong and weak
ties play distinguishable roles in spreading dynamics: the former enlarge the
infected density while the latter fasten the process. The proposed method
provides a quantitative way to reveal the qualitatively different roles of
ties, which could find applications in analyzing many networked dynamical
processes with multiple performance indices, such as synchronizability and
converging time in synchronization and throughput and delivering time in
transportation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0111</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0111</id><created>2012-03-31</created><updated>2013-02-05</updated><authors><author><keyname>Poulson</keyname><forenames>Jack</forenames></author><author><keyname>Engquist</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Li</keyname><forenames>Siwei</forenames></author><author><keyname>Ying</keyname><forenames>Lexing</forenames></author></authors><title>A parallel sweeping preconditioner for heterogeneous 3D Helmholtz
  equations</title><categories>cs.NA math.NA</categories><comments>Final version of document</comments><doi>10.1137/120871985</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A parallelization of a sweeping preconditioner for 3D Helmholtz equations
without large cavities is introduced and benchmarked for several challenging
velocity models. The setup and application costs of the sequential
preconditioner are shown to be O({\gamma}^2 N^{4/3}) and O({\gamma} N log N),
where {\gamma}({\omega}) denotes the modestly frequency-dependent number of
grid points per Perfectly Matched Layer. Several computational and memory
improvements are introduced relative to using black-box sparse-direct solvers
for the auxiliary problems, and competitive runtimes and iteration counts are
reported for high-frequency problems distributed over thousands of cores. Two
open-source packages are released along with this paper: &quot;Parallel Sweeping
Preconditioner (PSP)&quot; and the underlying distributed multifrontal solver,
&quot;Clique&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0127</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0127</id><created>2012-03-31</created><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Multidisciplinary Cognitive Content of Nanoscience and Nanotechnology</title><categories>physics.soc-ph cs.DL</categories><journal-ref>J Nanopart Res (2012) 14:685 Journal of Nanoparticle Research,
  Volume 14, Number 1, 685</journal-ref><doi>10.1007/s11051-011-0685-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article examines the cognitive evolution and disciplinary diversity of
nanotechnology as expressed through the terminology used in titles of nano
journal articles. The analysis is based on the NanoBank bibliographic database
of 287,106 nano articles published between 1981 and 2004. We perform
multifaceted analyses of title words, focusing on 100 most frequent terms.
Hierarchical clustering of title terms reveals three distinct time periods of
cognitive development of nano research: formative (1981-1990), early
(1991-1998), and current (after 1998). Early period is characterized by the
introduction of thin film deposition techniques, while the current period is
characterized by the increased focus on carbon nanotube and nanoparticle
research. We introduce a method to identify disciplinary components of
nanotechnology. It shows that the nano research is being carried out in a
number of diverse parent disciplines. Currently only 5% of articles are
published in dedicated nano-only journals. We find that some 85% of nano
research today is multidisciplinary. Hierarchical clustering of disciplinary
components reveals that the cognitive content of current nanoscience can be
divided into nine clusters. Some clusters account for a large fraction of nano
research and are identified with such parent disciplines as the condensed
matter and applied physics, materials science, and analytical chemistry. Other
clusters represent much smaller parts of nano research, but are as cognitively
distinct. In the decreasing order of size, these fields are: polymer science,
biotechnology, general chemistry, surface science, and pharmacology. Cognitive
content of research published in nano-only journals is closest to nano research
published in condensed matter and applied physics journals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0128</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0128</id><created>2012-03-31</created><authors><author><keyname>Wang</keyname><forenames>Chunyan</forenames></author><author><keyname>Ye</keyname><forenames>Mao</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author></authors><title>From User Comments to On-line Conversations</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an analysis of user conversations in on-line social media and
their evolution over time. We propose a dynamic model that accurately predicts
the growth dynamics and structural properties of conversation threads. The
model successfully reconciles the differing observations that have been
reported in existing studies. By separating artificial factors from user
behaviors, we show that there are actually underlying rules in common for
on-line conversations in different social media websites. Results of our model
are supported by empirical measurements throughout a number of different social
media websites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0131</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0131</id><created>2012-03-31</created><authors><author><keyname>Rezine</keyname><forenames>Ahmed</forenames></author></authors><title>Ordered Counter-Abstraction</title><categories>cs.LO</categories><acm-class>D.2.4; D.3.1; F.4.3; I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new symbolic representation based on an original
generalization of counter abstraction. Unlike classical counter abstraction
(used in the analysis of parameterized systems with unordered or unstructured
topologies) the new representation is tailored for proving properties of
linearly ordered parameterized systems, i.e., systems with arbitrary many
finite processes placed in an array. The relative positions in the array
capture the relative priorities of the processes. Configurations of such
systems are finite words of arbitrary lengths. The processes communicate using
global transitions constrained by their relative priorities. Intuitively, an
element of the symbolic representation has a base and a set of counters. It
denotes configurations that respect the constraints imposed by the counters and
that have the base as a sub word. We use the new representation in a uniform
and automatic Counter Example Guided Refinement scheme. We introduce a
relaxation operator that allows a well quasi ordering argument for the
termination of each iteration of the refinement loop. We explain how to refine
the relaxation to systematically prune out false positives. We implemented a
tool to illustrate the approach on a number of parameterized systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0133</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0133</id><created>2012-03-31</created><authors><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author><author><keyname>Steinbring</keyname><forenames>Jannik</forenames></author></authors><title>Progressive Gaussian Filtering</title><categories>cs.SY cs.IT cs.RO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a progressive Bayesian procedure, where the
measurement information is continuously included into the given prior estimate
(although we perform observations at discrete time steps). The key idea is to
derive a system of ordinary first-order differential equations (ODE) by
employing a new coupled density representation comprising a Gaussian density
and its Dirac Mixture approximation. The ODE is used for continuously tracking
the true non-Gaussian posterior by its best matching Gaussian approximation.
The performance of the new filter is evaluated in comparison with
state-of-the-art filters by means of a canonical benchmark example, the
discrete-time cubic sensor problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0136</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0136</id><created>2012-03-31</created><authors><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>Near-Optimal Algorithms for Online Matrix Prediction</title><categories>cs.LG cs.DS</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In several online prediction problems of recent interest the comparison class
is composed of matrices with bounded entries. For example, in the online
max-cut problem, the comparison class is matrices which represent cuts of a
given graph and in online gambling the comparison class is matrices which
represent permutations over n teams. Another important example is online
collaborative filtering in which a widely used comparison class is the set of
matrices with a small trace norm. In this paper we isolate a property of
matrices, which we call (beta,tau)-decomposability, and derive an efficient
online learning algorithm, that enjoys a regret bound of O*(sqrt(beta tau T))
for all problems in which the comparison class is composed of
(beta,tau)-decomposable matrices. By analyzing the decomposability of cut
matrices, triangular matrices, and low trace-norm matrices, we derive near
optimal regret bounds for online max-cut, online gambling, and online
collaborative filtering. In particular, this resolves (in the affirmative) an
open problem posed by Abernethy (2010); Kleinberg et al (2010). Finally, we
derive lower bounds for the three problems and show that our upper bounds are
optimal up to logarithmic factors. In particular, our lower bound for the
online collaborative filtering problem resolves another open problem posed by
Shamir and Srebro (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0140</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0140</id><created>2012-03-31</created><authors><author><keyname>Jarmasz</keyname><forenames>Mario</forenames></author></authors><title>Roget's Thesaurus as a Lexical Resource for Natural Language Processing</title><categories>cs.CL</categories><comments>Thesis submitted to the Faculty of Graduate and Postdoctoral Studies
  in partial fulfillment of the requirements for the degree of Master of
  Computer Science July, 2003. Ottawa-Carleton Institute for Computer Science,
  School of Information Technology and Engineering, University of Ottawa,
  Ottawa, Ontario, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WordNet proved that it is possible to construct a large-scale electronic
lexical database on the principles of lexical semantics. It has been accepted
and used extensively by computational linguists ever since it was released.
Inspired by WordNet's success, we propose as an alternative a similar resource,
based on the 1987 Penguin edition of Roget's Thesaurus of English Words and
Phrases.
  Peter Mark Roget published his first Thesaurus over 150 years ago. Countless
writers, orators and students of the English language have used it.
Computational linguists have employed Roget's for almost 50 years in Natural
Language Processing, however hesitated in accepting Roget's Thesaurus because a
proper machine tractable version was not available.
  This dissertation presents an implementation of a machine-tractable version
of the 1987 Penguin edition of Roget's Thesaurus - the first implementation of
its kind to use an entire current edition. It explains the steps necessary for
taking a machine-readable file and transforming it into a tractable system.
This involves converting the lexical material into a format that can be more
easily exploited, identifying data structures and designing classes to
computerize the Thesaurus. Roget's organization is studied in detail and
contrasted with WordNet's.
  We show two applications of the computerized Thesaurus: computing semantic
similarity between words and phrases, and building lexical chains in a text.
The experiments are performed using well-known benchmarks and the results are
compared to those of other systems that use Roget's, WordNet and statistical
techniques. Roget's has turned out to be an excellent resource for measuring
semantic similarity; lexical chains are easily built but more difficult to
evaluate. We also explain ways in which Roget's Thesaurus and WordNet can be
combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0147</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0147</id><created>2012-03-31</created><authors><author><keyname>Guntuboyina</keyname><forenames>Adityanand</forenames></author><author><keyname>Sen</keyname><forenames>Bodhisattva</forenames></author></authors><title>Covering Numbers for Convex Functions</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the covering numbers of the space of convex and
uniformly bounded functions in multi-dimension. We find optimal upper and lower
bounds for the $\epsilon$-covering number of $\C([a, b]^d, B)$, in the
$L_p$-metric, $1 \le p &lt; \infty$, in terms of the relevant constants, where $d
\geq 1$, $a &lt; b \in \mathbb{R}$, $B&gt;0$, and $\C([a,b]^d, B)$ denotes the set of
all convex functions on $[a, b]^d$ that are uniformly bounded by $B$. We
summarize previously known results on covering numbers for convex functions and
also provide alternate proofs of some known results. Our results have direct
implications in the study of rates of convergence of empirical minimization
procedures as well as optimal convergence rates in the numerous convexity
constrained function estimation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0153</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0153</id><created>2012-03-31</created><authors><author><keyname>Khiabani</keyname><forenames>Yahya S.</forenames></author><author><keyname>Wei</keyname><forenames>Shuangqing</forenames></author><author><keyname>Yuan</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author></authors><title>Enhancement of Secrecy of Block Ciphered Systems by Deliberate Noise</title><categories>cs.CR</categories><comments>11 pages, 8 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of end-end security enhancement by resorting
to deliberate noise injected in ciphertexts. The main goal is to generate a
degraded wiretap channel in application layer over which Wyner-type secrecy
encoding is invoked to deliver additional secure information. More
specifically, we study secrecy enhancement of DES block cipher working in
cipher feedback model (CFB) when adjustable and intentional noise is introduced
into encrypted data in application layer. A verification strategy in exhaustive
search step of linear attack is designed to allow Eve to mount a successful
attack in the noisy environment. Thus, a controllable wiretap channel is
created over multiple frames by taking advantage of errors in Eve's
cryptanalysis, whose secrecy capacity is found for the case of known channel
states at receivers. As a result, additional secure information can be
delivered by performing Wyner type secrecy encoding over super-frames ahead of
encryption, namely, our proposed secrecy encoding-then-encryption scheme. These
secrecy bits could be taken as symmetric keys for upcoming frames. Numerical
results indicate that a sufficiently large secrecy rate can be achieved by
selective noise addition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0156</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0156</id><created>2012-03-31</created><authors><author><keyname>Ravikumar</keyname><forenames>Srijith</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Raju</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Ranking Tweets Considering Trust and Relevance</title><categories>cs.SI cs.IR</categories><comments>four pages short paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing popularity of Twitter and other microblogs makes improved
trustworthiness and relevance assessment of microblogs evermore important. We
propose a method of ranking of tweets considering trustworthiness and content
based popularity. The analysis of trustworthiness and popularity exploits the
implicit relationships between the tweets. We model microblog ecosystem as a
three-layer graph consisting of : (i) users (ii) tweets and (iii) web pages. We
propose to derive trust and popularity scores of entities in these three
layers, and propagate the scores to tweets considering the inter-layer
relations. Our preliminary evaluations show improvement in precision and
trustworthiness over the baseline methods and acceptable computation timings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0161</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0161</id><created>2012-04-01</created><authors><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author><author><keyname>Yang</keyname><forenames>Mingmin</forenames></author><author><keyname>Qu</keyname><forenames>Xinglong</forenames></author><author><keyname>Yang</keyname><forenames>Xiaoguang</forenames></author></authors><title>Rebels Lead to the Doctrine of the Mean: Opinion Dynamic in a
  Heterogeneous DeGroot Model</title><categories>cs.SI physics.soc-ph</categories><comments>7 pages, Proceedings of The 6th International Conference on
  Knowledge, Information and Creativity Support Systems, Beijing, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an extension of the DeGroot model where part of the players may be
rebels. The updating rule for rebels is quite different with that of normal
players (which are referred to as conformists): at each step a rebel first
takes the opposite value of the weighted average of her neighbors' opinions,
i.e. 1 minus that average (the opinion space is assumed to be [0,1] as usual),
and then updates her opinion by taking another weighted average between that
value and her own opinion in the last round. We find that the effect of rebels
is rather significant: as long as there is at least one rebel in every closed
and strongly connected group, under very weak conditions, the opinion of each
player in the whole society will eventually tend to 0.5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0163</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0163</id><created>2012-04-01</created><updated>2012-10-18</updated><authors><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author><author><keyname>Gao</keyname><forenames>Haoyu</forenames></author><author><keyname>Qu</keyname><forenames>Xinglong</forenames></author><author><keyname>Yang</keyname><forenames>Mingmin</forenames></author><author><keyname>Yang</keyname><forenames>Xiaoguang</forenames></author></authors><title>Fashion, Cooperation, and Social Interactions</title><categories>cs.MA cs.SI physics.soc-ph</categories><comments>21 pages, 12 figures</comments><journal-ref>PLoS ONE 8(1): e49441. 2013</journal-ref><doi>10.1371/journal.pone.0049441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fashion plays such a crucial rule in the evolution of culture and society
that it is regarded as a second nature to the human being. Also, its impact on
economy is quite nontrivial. On what is fashionable, interestingly, there are
two viewpoints that are both extremely widespread but almost opposite:
conformists think that what is popular is fashionable, while rebels believe
that being different is the essence. Fashion color is fashionable in the first
sense, and Lady Gaga in the second. We investigate a model where the population
consists of the afore-mentioned two groups of people that are located on social
networks (a spatial cellular automata network and small-world networks). This
model captures two fundamental kinds of social interactions (coordination and
anti-coordination) simultaneously, and also has its own interest to game
theory: it is a hybrid model of pure competition and pure cooperation. This is
true because when a conformist meets a rebel, they play the zero sum matching
pennies game, which is pure competition. When two conformists (rebels) meet,
they play the (anti-) coordination game, which is pure cooperation. Simulation
shows that simple social interactions greatly promote cooperation: in most
cases people can reach an extraordinarily high level of cooperation, through a
selfish, myopic, naive, and local interacting dynamic (the best response
dynamic). We find that degree of synchronization also plays a critical role,
but mostly on the negative side. Four indices, namely cooperation degree,
average satisfaction degree, equilibrium ratio and complete ratio, are defined
and applied to measure people's cooperation levels from various angles. Phase
transition, as well as emergence of many interesting geographic patterns in the
cellular automata network, is also observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0165</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0165</id><created>2012-04-01</created><updated>2015-02-26</updated><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Analytical Models for Power Networks: The case of the Western US and
  ERCOT grids</title><categories>cs.SI physics.soc-ph stat.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topological structure of the power grid plays a key role in the reliable
delivery of electricity and price settlement in the electricity market.
Incorporation of new energy sources and loads into the grid over time has led
to its structural and geographical expansion and can affect its stable
operation. This paper presents an intuitive analytical model for the temporal
evolution of large grids and uses it to understand common structural features
observed in grids across America. In particular, key graph parameters like
degree distribution, graph diameter, betweenness centralities, eigen-spread and
clustering coefficients, as well as graph processes like infection propagation
are used to quantify the model's benefits through comparison with the Western
US and ERCOT power grids. The most significant contribution of the developed
model is its analytical tractability, that provides a closed form expression
for the nodal degree distribution observed in large grids. The discussed model
can be used to generate realistic test cases to analyze topological effects on
grid functioning and new grid technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0166</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0166</id><created>2012-04-01</created><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author></authors><title>Worst-Case Robust Multiuser Transmit Beamforming Using Semidefinite
  Relaxation: Duality and Implications</title><categories>cs.IT math.IT</categories><comments>2011 IEEE Asilomar Conference on Signals, Systems, and Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a downlink multiuser transmit beamforming design under
spherical channel uncertainties, using a worst-case robust formulation. This
robust design problem is nonconvex. Recently, a convex approximation
formulation based on semidefinite relaxation (SDR) has been proposed to handle
the problem. Curiously, simulation results have consistently indicated that SDR
can attain the global optimum of the robust design problem. This paper intends
to provide some theoretical insights into this important empirical finding. Our
main result is a dual representation of the SDR formulation, which reveals an
interesting linkage to a different robust design problem, and the possibility
of SDR optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0168</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0168</id><created>2012-04-01</created><updated>2014-10-11</updated><authors><author><keyname>Dong</keyname><forenames>Wen</forenames></author><author><keyname>Heller</keyname><forenames>Katherine A.</forenames></author><author><keyname>Pentland</keyname><forenames>Alex Sandy</forenames></author></authors><title>Modeling Infection with Multi-agent Dynamics</title><categories>stat.AP cs.MA cs.SI physics.soc-ph</categories><doi>10.1007/978-3-642-29047-3_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing the ability to comprehensively study infections in small
populations enables us to improve epidemic models and better advise individuals
about potential risks to their health. We currently have a limited
understanding of how infections spread within a small population because it has
been difficult to closely track an infection within a complete community. The
paper presents data closely tracking the spread of an infection centered on a
student dormitory, collected by leveraging the residents' use of cellular
phones. The data are based on daily symptom surveys taken over a period of four
months and proximity tracking through cellular phones. We demonstrate that
using a Bayesian, discrete-time multi-agent model of infection to model
real-world symptom reports and proximity tracking records gives us important
insights about infec-tions in small populations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0170</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0170</id><created>2012-04-01</created><updated>2014-04-07</updated><authors><author><keyname>Zeng</keyname><forenames>Jia</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Cao</keyname><forenames>Xiao-Qin</forenames></author></authors><title>A New Approach to Speeding Up Topic Modeling</title><categories>cs.LG cs.IR</categories><comments>14 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet allocation (LDA) is a widely-used probabilistic topic
modeling paradigm, and recently finds many applications in computer vision and
computational biology. In this paper, we propose a fast and accurate batch
algorithm, active belief propagation (ABP), for training LDA. Usually batch LDA
algorithms require repeated scanning of the entire corpus and searching the
complete topic space. To process massive corpora having a large number of
topics, the training iteration of batch LDA algorithms is often inefficient and
time-consuming. To accelerate the training speed, ABP actively scans the subset
of corpus and searches the subset of topic space for topic modeling, therefore
saves enormous training time in each iteration. To ensure accuracy, ABP selects
only those documents and topics that contribute to the largest residuals within
the residual belief propagation (RBP) framework. On four real-world corpora,
ABP performs around $10$ to $100$ times faster than state-of-the-art batch LDA
algorithms with a comparable topic modeling accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0171</identifier>
 <datestamp>2013-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0171</id><created>2012-04-01</created><updated>2013-08-12</updated><authors><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Vural</keyname><forenames>Fatos T. Yarman</forenames></author></authors><title>A New Fuzzy Stacked Generalization Technique and Analysis of its
  Performance</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, a new Stacked Generalization technique called Fuzzy Stacked
Generalization (FSG) is proposed to minimize the difference between N -sample
and large-sample classification error of the Nearest Neighbor classifier. The
proposed FSG employs a new hierarchical distance learning strategy to minimize
the error difference. For this purpose, we first construct an ensemble of
base-layer fuzzy k- Nearest Neighbor (k-NN) classifiers, each of which receives
a different feature set extracted from the same sample set. The fuzzy
membership values computed at the decision space of each fuzzy k-NN classifier
are concatenated to form the feature vectors of a fusion space. Finally, the
feature vectors are fed to a meta-layer classifier to learn the degree of
accuracy of the decisions of the base-layer classifiers for meta-layer
classification. Rather than the power of the individual base layer-classifiers,
diversity and cooperation of the classifiers become an important issue to
improve the overall performance of the proposed FSG. A weak base-layer
classifier may boost the overall performance more than a strong classifier, if
it is capable of recognizing the samples, which are not recognized by the rest
of the classifiers, in its own feature space. The experiments explore the type
of the collaboration among the individual classifiers required for an improved
performance of the suggested architecture. Experiments on multiple feature
real-world datasets show that the proposed FSG performs better than the state
of the art ensemble learning algorithms such as Adaboost, Random Subspace and
Rotation Forest. On the other hand, compatible performances are observed in the
experiments on single feature multi-attribute datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0173</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0173</id><created>2012-04-01</created><authors><author><keyname>Bafghi</keyname><forenames>Hamid G.</forenames></author><author><keyname>Seyfe</keyname><forenames>Babak</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Aref</keyname><forenames>M. Reza</forenames></author></authors><title>On The Achievable Rate Region of a New Wiretap Channel With Side
  Information</title><categories>cs.IT math.IT</categories><report-no>Rep. No. 1, March 22, 2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new applicable wiretap channel with separated side information is
considered here which consist of a sender, a legitimate receiver and a
wiretapper. In the considered scenario, the links from the transmitter to the
legitimate receiver and the eavesdropper experience different conditions or
channel states. So, the legitimate receiver and the wiretapper listen to the
transmitted signal through the channels with different channel states which may
have some correlation to each other. It is assumed that the transmitter knows
the state of the main channel non-causally and uses this knowledge to encode
its message. The state of the wiretap channel is not known anywhere. An
achievable equivocation rate region is derived for this model and is compared
to the existing works. In some special cases, the results are extended to the
Gaussian wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0176</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0176</id><created>2012-04-01</created><authors><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author><author><keyname>Sharif</keyname><forenames>Mehboob</forenames></author><author><keyname>Iqbal</keyname><forenames>Nayyar</forenames></author></authors><title>Using Fuzzy Logic to Evaluate Normalization Completeness for An Improved
  Database Design</title><categories>cs.DB</categories><comments>8 Pages</comments><journal-ref>International Journal of Information Technology and Computer
  Science (IJTCS), Hong Kong, Vol. 4/2, pp. 48-55, March 2012</journal-ref><doi>10.5815/ijitcs.2012.02.07</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach, to measure normalization completeness for conceptual model,
is introduced using quantitative fuzzy functionality in this paper. We measure
the normalization completeness of the conceptual model in two steps. In the
first step, different normalization techniques are analyzed up to Boyce Codd
Normal Form (BCNF) to find the current normal form of the relation. In the
second step, fuzzy membership values are used to scale the normal form between
0 and 1. Case studies to explain schema transformation rules and measurements.
Normalization completeness is measured by considering completeness attributes,
preventing attributes of the functional dependencies and total number of
attributes such as if the functional dependency is non-preventing then the
attributes of that functional dependency are completeness attributes. The
attributes of functional dependency which prevent to go to the next normal form
are called preventing attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0179</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0179</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Service-Oriented Architecture for Weaponry and Battle Command and
  Control Systems in Warfighting</title><categories>cs.RO</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Information and Communication
  Technology Research, Vol. 2, No. 2, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Military is one of many industries that is more computer-dependent than ever
before, from soldiers with computerized weapons, and tactical wireless devices,
to commanders with advanced battle management, command and control systems.
Fundamentally, command and control is the process of planning, monitoring, and
commanding military personnel, weaponry equipment, and combating vehicles to
execute military missions. In fact, command and control systems are
revolutionizing as war fighting is changing into cyber, technology,
information, and unmanned warfare. As a result, a new design model that
supports scalability, reusability, maintainability, survivability, and
interoperability is needed to allow commanders, hundreds of miles away from the
battlefield, to plan, monitor, evaluate, and control the war events in a
dynamic, robust, agile, and reliable manner. This paper proposes a
service-oriented architecture for weaponry and battle command and control
systems, made out of loosely-coupled and distributed web services. The proposed
architecture consists of three elementary tiers: the client tier that
corresponds to any computing military equipment; the server tier that
corresponds to the web services that deliver the basic functionalities for the
client tier; and the middleware tier that corresponds to an enterprise service
bus that promotes interoperability between all the interconnected entities. A
command and control system was simulated and experimented and it successfully
exhibited the desired features of SOA. Future research can improve upon the
proposed architecture so much so that it supports encryption for securing the
exchange of data between the various communicating entities of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0181</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0181</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Expert PC Troubleshooter With Fuzzy-Logic And Self-Learning Support</title><categories>cs.AI</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Artificial Intelligence &amp;
  Applications (IJAIA), Vol.3, No.2, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expert systems use human knowledge often stored as rules within the computer
to solve problems that generally would entail human intelligence. Today, with
information systems turning out to be more pervasive and with the myriad
advances in information technologies, automating computer fault diagnosis is
becoming so fundamental that soon every enterprise has to endorse it. This
paper proposes an expert system called Expert PC Troubleshooter for diagnosing
computer problems. The system is composed of a user interface, a rule-base, an
inference engine, and an expert interface. Additionally, the system features a
fuzzy-logic module to troubleshoot POST beep errors, and an intelligent agent
that assists in the knowledge acquisition process. The proposed system is meant
to automate the maintenance, repair, and operations (MRO) process, and free-up
human technicians from manually performing routine, laborious, and
timeconsuming maintenance tasks. As future work, the proposed system is to be
parallelized so as to boost its performance and speed-up its various
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0182</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0182</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Hybrid Information Retrieval Model For Web Images</title><categories>cs.IR</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Computer Science &amp; Emerging
  Technologies (IJCSET), Vol. 3, No. 1, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bing Bang of the Internet in the early 90's increased dramatically the
number of images being distributed and shared over the web. As a result, image
information retrieval systems were developed to index and retrieve image files
spread over the Internet. Most of these systems are keyword-based which search
for images based on their textual metadata; and thus, they are imprecise as it
is vague to describe an image with a human language. Besides, there exist the
content-based image retrieval systems which search for images based on their
visual information. However, content-based type systems are still immature and
not that effective as they suffer from low retrieval recall/precision rate.
This paper proposes a new hybrid image information retrieval model for indexing
and retrieving web images published in HTML documents. The distinguishing mark
of the proposed model is that it is based on both graphical content and textual
metadata. The graphical content is denoted by color features and color
histogram of the image; while textual metadata are denoted by the terms that
surround the image in the HTML document, more particularly, the terms that
appear in the tags p, h1, and h2, in addition to the terms that appear in the
image's alt attribute, filename, and class-label. Moreover, this paper presents
a new term weighting scheme called VTF-IDF short for Variable Term
Frequency-Inverse Document Frequency which unlike traditional schemes, it
exploits the HTML tag structure and assigns an extra bonus weight for terms
that appear within certain particular HTML tags that are correlated to the
semantics of the image. Experiments conducted to evaluate the proposed IR model
showed a high retrieval precision rate that outpaced other current models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0183</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0183</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Neural Network Model for Path-Planning of Robotic Rover Systems</title><categories>cs.NE</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Science and Technology
  (IJST), Vol. 2, No. 2, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, robotics is an auspicious and fast-growing branch of technology that
involves the manufacturing, design, and maintenance of robot machines that can
operate in an autonomous fashion and can be used in a wide variety of
applications including space exploration, weaponry, household, and
transportation. More particularly, in space applications, a common type of
robots has been of widespread use in the recent years. It is called planetary
rover which is a robot vehicle that moves across the surface of a planet and
conducts detailed geological studies pertaining to the properties of the
landing cosmic environment. However, rovers are always impeded by obstacles
along the traveling path which can destabilize the rover's body and prevent it
from reaching its goal destination. This paper proposes an ANN model that
allows rover systems to carry out autonomous path-planning to successfully
navigate through challenging planetary terrains and follow their goal location
while avoiding dangerous obstacles. The proposed ANN is a multilayer network
made out of three layers: an input, a hidden, and an output layer. The network
is trained in offline mode using back-propagation supervised learning
algorithm. A software-simulated rover was experimented and it revealed that it
was able to follow the safest trajectory despite existing obstacles. As future
work, the proposed ANN is to be parallelized so as to speed-up the execution
time of the training process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0184</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0184</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Parallel Spell-Checking Algorithm Based on Yahoo! N-Grams Dataset</title><categories>cs.CL</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Research and Reviews in
  Computer Science (IJRRCS), Vol. 3, No. 1, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spell-checking is the process of detecting and sometimes providing
suggestions for incorrectly spelled words in a text. Basically, the larger the
dictionary of a spell-checker is, the higher is the error detection rate;
otherwise, misspellings would pass undetected. Unfortunately, traditional
dictionaries suffer from out-of-vocabulary and data sparseness problems as they
do not encompass large vocabulary of words indispensable to cover proper names,
domain-specific terms, technical jargons, special acronyms, and terminologies.
As a result, spell-checkers will incur low error detection and correction rate
and will fail to flag all errors in the text. This paper proposes a new
parallel shared-memory spell-checking algorithm that uses rich real-world word
statistics from Yahoo! N-Grams Dataset to correct non-word and real-word errors
in computer text. Essentially, the proposed algorithm can be divided into three
sub-algorithms that run in a parallel fashion: The error detection algorithm
that detects misspellings, the candidates generation algorithm that generates
correction suggestions, and the error correction algorithm that performs
contextual error correction. Experiments conducted on a set of text articles
containing misspellings, showed a remarkable spelling error correction rate
that resulted in a radical reduction of both non-word and real-word errors in
electronic text. In a further study, the proposed algorithm is to be optimized
for message-passing systems so as to become more flexible and less costly to
scale over distributed machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0185</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0185</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Service-Oriented Architecture for Space Exploration Robotic Rover
  Systems</title><categories>cs.RO</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; International Journal of Science &amp; Emerging
  Technologies (IJSET), Vol. 3, No. 2, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, industrial sectors are transforming their business processes into
e-services and component-based architectures to build flexible, robust, and
scalable systems, and reduce integration-related maintenance and development
costs. Robotics is yet another promising and fast-growing industry that deals
with the creation of machines that operate in an autonomous fashion and serve
for various applications including space exploration, weaponry, laboratory
research, and manufacturing. It is in space exploration that the most common
type of robots is the planetary rover which moves across the surface of a
planet and conducts a thorough geological study of the celestial surface. This
type of rover system is still ad-hoc in that it incorporates its software into
its core hardware making the whole system cohesive, tightly-coupled, more
susceptible to shortcomings, less flexible, hard to be scaled and maintained,
and impossible to be adapted to other purposes. This paper proposes a
service-oriented architecture for space exploration robotic rover systems made
out of loosely-coupled and distributed web services. The proposed architecture
consists of three elementary tiers: the client tier that corresponds to the
actual rover; the server tier that corresponds to the web services; and the
middleware tier that corresponds to an Enterprise Service Bus which promotes
interoperability between the interconnected entities. The niche of this
architecture is that rover's software components are decoupled and isolated
from the rover's body and possibly deployed at a distant location. A
service-oriented architecture promotes integrate-ability, scalability,
reusability, maintainability, and interoperability for client-to-server
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0186</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0186</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Semaan</keyname><forenames>Paul</forenames></author></authors><title>Semantic-Sensitive Web Information Retrieval Model for HTML Documents</title><categories>cs.IR</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; European Journal of Scientific Research, Vol. 69, No.
  4, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of the Internet, a new era of digital information exchange
has begun. Currently, the Internet encompasses more than five billion online
sites and this number is exponentially increasing every day. Fundamentally,
Information Retrieval (IR) is the science and practice of storing documents and
retrieving information from within these documents. Mathematically, IR systems
are at the core based on a feature vector model coupled with a term weighting
scheme that weights terms in a document according to their significance with
respect to the context in which they appear. Practically, Vector Space Model
(VSM), Term Frequency (TF), and Inverse Term Frequency (IDF) are among other
long-established techniques employed in mainstream IR systems. However, present
IR models only target generic-type text documents, in that, they do not
consider specific formats of files such as HTML web documents. This paper
proposes a new semantic-sensitive web information retrieval model for HTML
documents. It consists of a vector model called SWVM and a weighting scheme
called BTF-IDF, particularly designed to support the indexing and retrieval of
HTML web documents. The chief advantage of the proposed model is that it
assigns extra weights for terms that appear in certain pre-specified HTML tags
that are correlated to the semantics of the document. Additionally, the model
is semantic-sensitive as it generates synonyms for every term being indexed and
later weights them appropriately to increase the likelihood of retrieving
documents with similar context but different vocabulary terms. Experiments
conducted, revealed a momentous enhancement in the precision of web IR systems
and a radical increase in the number of relevant documents being retrieved. As
further research, the proposed model is to be upgraded so as to support the
indexing and retrieval of web images in multimedia-rich web documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0188</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0188</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Alwani</keyname><forenames>Mohammad</forenames></author></authors><title>OCR Context-Sensitive Error Correction Based on Google Web 1T 5-Gram
  Data Set</title><categories>cs.CL cs.IR</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; American Journal of Scientific Research, Issue. 50,
  February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the dawn of the computing era, information has been represented
digitally so that it can be processed by electronic computers. Paper books and
documents were abundant and widely being published at that time; and hence,
there was a need to convert them into digital format. OCR, short for Optical
Character Recognition was conceived to translate paper-based books into digital
e-books. Regrettably, OCR systems are still erroneous and inaccurate as they
produce misspellings in the recognized text, especially when the source
document is of low printing quality. This paper proposes a post-processing OCR
context-sensitive error correction method for detecting and correcting non-word
and real-word OCR errors. The cornerstone of this proposed approach is the use
of Google Web 1T 5-gram data set as a dictionary of words to spell-check OCR
text. The Google data set incorporates a very large vocabulary and word
statistics entirely reaped from the Internet, making it a reliable source to
perform dictionary-based error correction. The core of the proposed solution is
a combination of three algorithms: The error detection, candidate spellings
generator, and error correction algorithms, which all exploit information
extracted from Google Web 1T 5-gram data set. Experiments conducted on scanned
images written in different languages showed a substantial improvement in the
OCR error correction rate. As future developments, the proposed algorithm is to
be parallelised so as to support parallel and distributed computing
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0191</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0191</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Alwani</keyname><forenames>Mohammad</forenames></author></authors><title>OCR Post-Processing Error Correction Algorithm using Google Online
  Spelling Suggestion</title><categories>cs.CL</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; Journal of Emerging Trends in Computing and
  Information Sciences, Vol. 3, No. 1, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of digital optical scanners, a lot of paper-based books,
textbooks, magazines, articles, and documents are being transformed into an
electronic version that can be manipulated by a computer. For this purpose,
OCR, short for Optical Character Recognition was developed to translate scanned
graphical text into editable computer text. Unfortunately, OCR is still
imperfect as it occasionally mis-recognizes letters and falsely identifies
scanned text, leading to misspellings and linguistics errors in the OCR output
text. This paper proposes a post-processing context-based error correction
algorithm for detecting and correcting OCR non-word and real-word errors. The
proposed algorithm is based on Google's online spelling suggestion which
harnesses an internal database containing a huge collection of terms and word
sequences gathered from all over the web, convenient to suggest possible
replacements for words that have been misspelled during the OCR process.
Experiments carried out revealed a significant improvement in OCR error
correction rate. Future research can improve upon the proposed algorithm so
much so that it can be parallelized and executed over multiprocessing
platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0193</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0193</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Communication Language Specifications For Digital Ecosystems</title><categories>cs.SE</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/</comments><journal-ref>International Journal of Advanced Research in Computer Science,
  Vol. 3, No. 1, Jan-Feb 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-based IT infrastructures are today's trend and the future for every
enterprise willing to support dynamic and agile business to contend with the
ever changing e-demands and requirements. A digital ecosystem is an emerging
business IT model for developing agile e-enterprises made out of
self-adaptable, self-manageable, self-organizing, and sustainable service
components. This paper defines the specifications of a communication language
for exchanging data between connecting entities in digital ecosystems. It is
called ECL short for Ecosystem Communication Language and is based on XML to
format its request and response messages. An ECU short for Ecosystem
Communication Unit is also presented which interprets, validates, parses ECL
messages and routes them to their destination entities. ECL is open and
provides transparent, portable, and interoperable communication between the
different heterogeneous distributed components to send requests, and receive
responses from each other, regardless of their incompatible protocols,
standards, and technologies. As future research, digital signature for ECL is
to be investigated so as to deliver data integrity as well as message
authenticity for the digital ecosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0195</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0195</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Management Language Specifications For Digital Ecosystems</title><categories>cs.SE</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; Journal of Global Research in Computer Science, Vol.
  3, No. 1, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines the specifications of a management language intended to
automate the control and administration of various service components connected
to a digital ecosystem. It is called EML short for Ecosystem Management
Language and it is based on proprietary syntax and notation and contains a set
of managerial commands issued by the system's administrator via a command
console. Additionally, EML is shipped with a collection of self-adaptation
procedures called SAP. Their purpose is to provide self-adaptation properties
to the ecosystem allowing it to self-optimize itself based on the state of its
execution environment. On top of that, there exists the EMU short for Ecosystem
Management Unit which interprets, validates, parses, and executes EML commands
and SAP procedures. Future research can improve upon EML so much so that it can
be extended to support a larger set of commands in addition to a larger set of
SAP procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0197</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0197</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Windows And Linux Operating Systems From A Security Perspective</title><categories>cs.OS</categories><comments>LACSC - Lebanese Association for Computational Sciences,
  http://www.lacsc.org/; Journal of Global Research in Computer Science, Vol.
  3, No. 2, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating systems are vital system software that, without them, humans would
not be able to manage and use computer systems. In essence, an operating system
is a collection of software programs whose role is to manage computer resources
and provide an interface for client applications to interact with the different
computer hardware. Most of the commercial operating systems available today on
the market have buggy code and they exhibit security flaws and vulnerabilities.
In effect, building a trusted operating system that can mostly resist attacks
and provide a secure computing environment to protect the important assets of a
computer is the goal of every operating system manufacturer. This paper deeply
investigates the various security features of the two most widespread and
successful operating systems, Microsoft Windows and Linux. The different
security features, designs, and components of the two systems are to be covered
elaborately, pin-pointing the key similarities and differences between them. In
due course, a head-to-head comparison is to be drawn for each security aspect,
exposing the advantage of one system over the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0198</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0198</id><created>2012-04-01</created><updated>2012-09-10</updated><authors><author><keyname>Muchnik</keyname><forenames>Andrej</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author><author><keyname>Vyugin</keyname><forenames>Mikhail</forenames></author></authors><title>Game arguments in computability theory and algorithmic information
  theory</title><categories>math.LO cs.GT cs.IT math.IT</categories><comments>Extended version of a talk given at CiE2012 (Turing centennial)
  conference [see previous version for conference text]</comments><msc-class>68Q30</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide some examples showing how game-theoretic arguments can be used in
computability theory and algorithmic information theory: unique numbering
theorem (Friedberg), the gap between conditional complexity and total
conditional complexity, Epstein--Levin theorem and some (yet unpublished)
result of Muchnik and Vyugin
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0199</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0199</id><created>2012-04-01</created><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Wu</keyname><forenames>Yueping</forenames></author></authors><title>Delay-aware BS Discontinuous Transmission Control and User Scheduling
  for Energy Harvesting Downlink Coordinated MIMO Systems</title><categories>cs.SY</categories><comments>Transaction on Signal Processing 2012</comments><doi>10.1109/TSP.2012.2194291</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a two-timescale delay-optimal base station
Discontinuous Transmission (BS-DTX) control and user scheduling for downlink
coordinated MIMO systems with energy harvesting capability. To reduce the
complexity and signaling overhead in practical systems, the BS-DTX control is
adaptive to both the energy state information (ESI) and the data queue state
information (QSI) over a longer timescale. The user scheduling is adaptive to
the ESI, the QSI and the channel state information (CSI) over a shorter
timescale. We show that the two-timescale delay-optimal control problem can be
modeled as an infinite horizon average cost Partially Observed Markov Decision
Problem (POMDP), which is well-known to be a difficult problem in general. By
using sample-path analysis and exploiting specific problem structure, we first
obtain some structural results on the optimal control policy and derive an
equivalent Bellman equation with reduced state space. To reduce the complexity
and facilitate distributed implementation, we obtain a delay-aware distributed
solution with the BS-DTX control at the BS controller (BSC) and the user
scheduling at each cluster manager (CM) using approximate dynamic programming
and distributed stochastic learning. We show that the proposed distributed
two-timescale algorithm converges almost surely. Furthermore, using queueing
theory, stochastic geometry and optimization techniques, we derive sufficient
conditions for the data queues to be stable in the coordinated MIMO network and
discuss various design insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0201</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0201</id><created>2012-04-01</created><authors><author><keyname>Bienvenu</keyname><forenames>Laurent</forenames></author><author><keyname>Muchnik</keyname><forenames>Andrej</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author><author><keyname>Vereshchagin</keyname><forenames>Nikolai</forenames></author></authors><title>Limit complexities revisited [once more]</title><categories>math.LO cs.IT math.IT</categories><comments>See http://arxiv.org/abs/0802.2833 for the old paper</comments><msc-class>68Q30</msc-class><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this article is to put some known results in a common
perspective and to simplify their proofs.
  We start with a simple proof of a result of Vereshchagin saying that
$\limsup_n C(x|n)$ equals $C^{0'}(x)$. Then we use the same argument to prove
similar results for prefix complexity, a priori probability on binary tree, to
prove Conidis' theorem about limits of effectively open sets, and also to
improve the results of Muchnik about limit frequencies. As a by-product, we get
a criterion of 2-randomness proved by Miller: a sequence $X$ is 2-random if and
only if there exists $c$ such that any prefix $x$ of $X$ is a prefix of some
string $y$ such that $C(y)\ge |y|-c$. (In the 1960ies this property was
suggested in Kolmogorov as one of possible randomness definitions.) We also get
another 2-randomness criterion by Miller and Nies: $X$ is 2-random if and only
if $C(x)\ge |x|-c$ for some $c$ and infinitely many prefixes $x$ of $X$.
  This is a modified version of our old paper that contained a weaker (and
cumbersome) version of Conidis' result, and the proof used low basis theorem
(in quite a strange way). The full version was formulated there as a
conjecture. This conjecture was later proved by Conidis. Bruno Bauwens
(personal communication) noted that the proof can be obtained also by a simple
modification of our original argument, and we reproduce Bauwens' argument with
his permission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0219</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0219</id><created>2012-04-01</created><authors><author><keyname>Gamzu</keyname><forenames>Iftah</forenames></author><author><keyname>Medina</keyname><forenames>Moti</forenames></author></authors><title>Improved Approximation for Orienting Mixed Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An instance of the maximum mixed graph orientation problem consists of a
mixed graph and a collection of source-target vertex pairs. The objective is to
orient the undirected edges of the graph so as to maximize the number of pairs
that admit a directed source-target path. This problem has recently arisen in
the study of biological networks, and it also has applications in communication
networks.
  In this paper, we identify an interesting local-to-global orientation
property. This property enables us to modify the best known algorithms for
maximum mixed graph orientation and some of its special structured instances,
due to Elberfeld et al. (CPM '11), and obtain improved approximation ratios. We
further proceed by developing an algorithm that achieves an even better
approximation guarantee for the general setting of the problem. Finally, we
study several well-motivated variants of this orientation problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0220</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0220</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Barbar</keyname><forenames>Aziz</forenames></author></authors><title>Sequential &amp; Parallel Algorithms for Big-Integer Numbers Subtraction</title><categories>cs.DS</categories><comments>Global Journal of Computer Science and Technology, Vol. 9, Issue 5,
  January 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many emerging computer applications require the processing of large numbers,
larger than what a CPU can handle. In fact, the top of the line PCs can only
manipulate numbers not longer than 32 bits or 64 bits. This is due to the size
of the registers and the data-path inside the CPU. As a result, performing
arithmetic operations such as subtraction on big-integer numbers is to some
extend limited. Different algorithms were designed in an attempt to solve this
problem; they all operate on big-integer numbers by first converting them into
a binary representation then performing bitwise operations on single bits. Such
algorithms are of complexity O(n) where n is the total number of bits in each
operand. This paper proposes two new algorithms for performing arithmetic
subtraction on big-integer numbers. The two algorithms are different in that
one is sequential while the other is parallel. The similarity between them is
that both follow the same concept of dividing the big-integer inputs into
several blocks or tokens of 60 bits (18 digits) each; thus reducing the input
size n in O(n) by a factor of 60. Subtraction of corresponding tokens, one from
each operand, is performed as humans perform subtraction, using a pencil and a
paper in the decimal system. Both algorithms are to be implemented using MS
C#.NET 2005 and tested over a multiple processor system. Further studies can be
done on other arithmetic operations such as addition and multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0221</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0221</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Barbar</keyname><forenames>Aziz</forenames></author></authors><title>MyProLang - My Programming Language: A Template-Driven Automatic Natural
  Programming Language</title><categories>cs.PL</categories><comments>WCECS 2008, October 22-24, 2008, San Francisco, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern computer programming languages are governed by complex syntactic
rules. They are unlike natural languages; they require extensive manual work
and a significant amount of learning and practicing for an individual to become
skilled at and to write correct programs. Computer programming is a difficult,
complicated, unfamiliar, non-automated, and a challenging discipline for
everyone; especially, for students, new programmers and end-users. This paper
proposes a new programming language and an environment for writing computer
applications based on source-code generation. It is mainly a template-driven
automatic natural imperative programming language called MyProLang. It
harnesses GUI templates to generate proprietary natural language source-code,
instead of having computer programmers write the code manually. MyProLang is a
blend of five elements. A proprietary natural programming language with
unsophisticated grammatical rules and expressive syntax; automation templates
that automate the generation of instructions and thereby minimizing the
learning and training time; an NLG engine to generate natural instructions; a
source-to-source compiler that analyzes, parses, and build executables; and an
ergonomic IDE that houses diverse functions whose role is to simplify the
software development process. MyProLang is expected to make programming open to
everyone including students, programmers and end-users. In that sense, anyone
can start programming systematically, in an automated manner and in natural
language; without wasting time in learning how to formulate instructions and
arrange expressions, without putting up with unfamiliar structures and symbols,
and without being annoyed by syntax errors. In the long run, this increases the
productivity, quality and time-to-market in software development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0222</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0222</id><created>2012-03-29</created><updated>2013-09-30</updated><authors><author><keyname>Ionica</keyname><forenames>Sorina</forenames><affiliation>INRIA Nancy - Grand Est / LORIA, INRIA Rocquencourt</affiliation></author></authors><title>Pairing-based algorithms for jacobians of genus 2 curves with maximal
  endomorphism ring</title><categories>math.NT cs.CR</categories><proxy>ccsd</proxy><journal-ref>Journal of Number Theory 133 (2013) 3755-3770</journal-ref><doi>10.1016/j.jnt.2013.04.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Galois cohomology, Schmoyer characterizes cryptographic non-trivial
self-pairings of the $\ell$-Tate pairing in terms of the action of the
Frobenius on the $\ell$-torsion of the Jacobian of a genus 2 curve. We apply
similar techniques to study the non-degeneracy of the $\ell$-Tate pairing
restrained to subgroups of the $\ell$-torsion which are maximal isotropic with
respect to the Weil pairing. First, we deduce a criterion to verify whether the
jacobian of a genus 2 curve has maximal endomorphism ring. Secondly, we derive
a method to construct horizontal $(\ell,\ell)$-isogenies starting from a
jacobian with maximal endomorphism ring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0225</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0225</id><created>2012-03-28</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Aksoy</keyname><forenames>Mehmet Sabih</forenames></author><author><keyname>Tuan</keyname><forenames>Yong Chee</forenames></author></authors><title>A Simulation Approach Paradigm: An Optimization and Inventory Challenge
  Case Study</title><categories>cs.OH</categories><comments>Computer Science Journal (ISSN: 2221-5905)Volume 2, Issue 1, April
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a simulation on automotive inventory and stock issue,
followed by evaluated performance of automotif Sector Company, focused on
getting optimum profit from supply and demand balancing. Starting by evaluating
and verification of customer's document until car delivered to customer.
Simulation method of performance is used to evaluate company activity. excess
demand of car by customer, not eligible customer to rented a car, number of
customer who served and number of customer who served including the driver, the
last result is number of optimum demand that match with the stock or supply of
car by the company. Finally, board of management should be making decision; the
first decision is buy the new car for meet with the demand or second decision
is recruit new staff for increasing customer service or customer care.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0232</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0232</id><created>2012-04-01</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author><author><keyname>Barbar</keyname><forenames>Aziz</forenames></author></authors><title>Sequential and Parallel Algorithms for the Addition of Big-Integer
  Numbers</title><categories>cs.DS</categories><journal-ref>International Journal of Computational Science, vol. 4, no. 1, pp.
  52-69, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's PCs can directly manipulate numbers not longer than 64 bits because
the size of the CPU registers and the data-path are limited. Consequently,
arithmetic operations such as addition, can only be performed on numbers of
that length. To solve the problem of computation on big-integer numbers,
different algorithms were developed. However, these algorithms are considerably
slow because they operate on individual bits; and are only designed to run over
single-processor computers. In this paper, two algorithms for handling
arithmetic addition on big-integer numbers are presented. The first algorithm
is sequential while the second is parallel. Both algorithms, unlike existing
ones, perform addition on blocks or tokens of 60 bits (18 digits), and thus
boosting the execution time by a factor of 60.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0240</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0240</id><created>2012-04-01</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Tuan</keyname><forenames>Yong Chee</forenames></author><author><keyname>Aksoy</keyname><forenames>Mehmet Sabih</forenames></author><author><keyname>Syam</keyname><forenames>Wahyudin P</forenames></author></authors><title>Integrated Solution Modeling Software: A New Paradigm on Information
  Security Review</title><categories>cs.CR cs.SE</categories><comments>International Journal of Science and Advanced Technology (ISSN
  2221-8386)Volume 1 No 10 December 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actually Information security becomes a very important part for the
organization's intangible assets, so level of confidence and stakeholder
trusted are performance indicator as successes organization. Since information
security has a very important role in supporting the activities of the
organization, we need a standard or benchmark which regulates governance over
information security. The main objective of this paper is to implement a novel
practical approach framework to the development of information security
management system (ISMS) assessment and monitoring software, called by
I-SolFramework. System / software is expected to assist stakeholders in
assessing the level of their ISO27001 compliance readiness, the software could
help stakeholders understood security control or called by compliance
parameters, being shorter and more structured. The case study illustrated
provided to the reader with a set of guidelines, that aims easy understood and
applicable as measuring tools for ISMS standards (ISO27001) compliance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0245</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0245</id><created>2012-04-01</created><authors><author><keyname>Jarmasz</keyname><forenames>Mario</forenames></author><author><keyname>Szpakowicz</keyname><forenames>Stan</forenames></author></authors><title>Roget's Thesaurus and Semantic Similarity</title><categories>cs.CL</categories><comments>8 pages</comments><journal-ref>Proceedings of Conference on Recent Advances in Natural Language
  Processing (RANLP 2003), Borovets, Bulgaria, September 2003, 212-219</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have implemented a system that measures semantic similarity using a
computerized 1987 Roget's Thesaurus, and evaluated it by performing a few
typical tests. We compare the results of these tests with those produced by
WordNet-based similarity measures. One of the benchmarks is Miller and Charles'
list of 30 noun pairs to which human judges had assigned similarity measures.
We correlate these measures with those computed by several NLP systems. The 30
pairs can be traced back to Rubenstein and Goodenough's 65 pairs, which we have
also studied. Our Roget's-based system gets correlations of .878 for the
smaller and .818 for the larger list of noun pairs; this is quite close to the
.885 that Resnik obtained when he employed humans to replicate the Miller and
Charles experiment. We further evaluate our measure by using Roget's and
WordNet to answer 80 TOEFL, 50 ESL and 300 Reader's Digest questions: the
correct synonym must be selected amongst a group of four words. Our system gets
78.75%, 82.00% and 74.33% of the questions respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0248</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0248</id><created>2012-04-01</created><authors><author><keyname>Brown</keyname><forenames>Gavin</forenames></author><author><keyname>Kasprzyk</keyname><forenames>Alexander M.</forenames></author></authors><title>Small polygons and toric codes</title><categories>math.CO cs.IT math.IT</categories><comments>9 pages, 4 tables, 3 figures</comments><msc-class>14G50 (Primary) 52B20, 14M25 (Secondary)</msc-class><journal-ref>Journal of Symbolic Computation, 51 (2013), 55-62</journal-ref><doi>10.1016/j.jsc.2012.07.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two different approaches to making systematic classifications of
plane lattice polygons, and recover the toric codes they generate, over small
fields, where these match or exceed the best known minimum distance. This
includes a [36,19,12]-code over F_7 whose minimum distance 12 exceeds that of
all previously known codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0255</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0255</id><created>2012-04-01</created><authors><author><keyname>Jarmasz</keyname><forenames>Mario</forenames></author><author><keyname>Barri&#xe8;re</keyname><forenames>Caroline</forenames></author></authors><title>Keyphrase Extraction : Enhancing Lists</title><categories>cs.CL cs.IR</categories><comments>8 pages; Proceedings of the 2nd Conference on Computational
  Linguistics in the North-East (CLiNE 2004), Montr\'eal, Canada, August</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes some modest improvements to Extractor, a state-of-the-art
keyphrase extraction system, by using a terabyte-sized corpus to estimate the
informativeness and semantic similarity of keyphrases. We present two
techniques to improve the organization and remove outliers of lists of
keyphrases. The first is a simple ordering according to their occurrences in
the corpus; the second is clustering according to semantic similarity.
Evaluation issues are discussed. We present a novel technique of comparing
extracted keyphrases to a gold standard which relies on semantic similarity
rather than string matching or an evaluation involving human judges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0257</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0257</id><created>2012-04-01</created><authors><author><keyname>Jarmasz</keyname><forenames>Mario</forenames></author><author><keyname>Szpakowicz</keyname><forenames>Stan</forenames></author></authors><title>Not As Easy As It Seems: Automating the Construction of Lexical Chains
  Using Roget's Thesaurus</title><categories>cs.CL</categories><comments>5 pages</comments><journal-ref>Proceedings of the 16th Canadian Conference on Artificial
  Intelligence (AI 2003), Halifax, Canada, June 2003. Lecture Notes in Computer
  Science 2671, Springer-Verlag 2003, 544-549</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morris and Hirst present a method of linking significant words that are about
the same topic. The resulting lexical chains are a means of identifying
cohesive regions in a text, with applications in many natural language
processing tasks, including text summarization. The first lexical chains were
constructed manually using Roget's International Thesaurus. Morris and Hirst
wrote that automation would be straightforward given an electronic thesaurus.
All applications so far have used WordNet to produce lexical chains, perhaps
because adequate electronic versions of Roget's were not available until
recently. We discuss the building of lexical chains using an electronic version
of Roget's Thesaurus. We implement a variant of the original algorithm, and
explain the necessary design decisions. We include a comparison with other
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0258</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0258</id><created>2012-04-01</created><authors><author><keyname>Jarmasz</keyname><forenames>Mario</forenames></author><author><keyname>Szpakowicz</keyname><forenames>Stan</forenames></author></authors><title>Roget's Thesaurus: a Lexical Resource to Treasure</title><categories>cs.CL</categories><comments>6 pages</comments><journal-ref>Proceedings of the NAACL WordNet and Other Lexical Resources
  workshop. Pittsburgh, June 2001, 186 - 188</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the steps involved in creating an electronic lexical
knowledge base from the 1987 Penguin edition of Roget's Thesaurus. Semantic
relations are labelled with the help of WordNet. The two resources are compared
in a qualitative and quantitative manner. Differences in the organization of
the lexical material are discussed, as well as the possibility of merging both
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0262</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0262</id><created>2012-04-01</created><updated>2012-04-26</updated><authors><author><keyname>Fish</keyname><forenames>Greg</forenames></author></authors><title>Managing contextual artificial neural networks with a service-based
  mediator</title><categories>cs.NE</categories><comments>12 pages, 5 figures</comments><msc-class>97R40</msc-class><acm-class>B.6.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, a wide variety of probabilistic and expert AI systems used to analyze
real world inputs such as unstructured text, sounds, images, and statistical
data. However, all these systems exist on different platforms, with different
implementations, and with very different, often very specific goals in mind.
This paper introduces a concept for a mediator framework for such systems and
seeks to show several architectures which would support it, potential benefits
in combining the signals of disparate networks for formalized, high level logic
and signal processing, and its possible academic and industrial uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0266</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0266</id><created>2012-04-01</created><updated>2013-02-21</updated><authors><author><keyname>Litvak</keyname><forenames>Nelly</forenames></author><author><keyname>van der Hofstad</keyname><forenames>Remco</forenames></author></authors><title>Uncovering disassortativity in large scale-free networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><doi>10.1103/PhysRevE.87.022801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixing patterns in large self-organizing networks, such as the Internet, the
World Wide Web, social and biological networks are often characterized by
degree-degree dependencies between neighbouring nodes. In this paper we propose
a new way of measuring degree-degree dependencies. One of the problems with the
commonly used assortativity coefficient is that in disassortative networks its
magnitude decreases with the network size. We mathematically explain this
phenomenon and validate the results on synthetic graphs and real-world network
data. As an alternative, we suggest to use rank correlation measures such as
Spearman's rho. Our experiments convincingly show that Spearman's rho produces
consistent values in graphs of different sizes but similar structure, and it is
able to reveal strong (positive or negative) dependencies in large graphs. In
particular, we discover much stronger negative degree-degree dependencies} in
Web graphs than was previously thought. {Rank correlations allow us to compare
the assortativity of networks of different sizes, which is impossible with the
assortativity coefficient due to its genuine dependence on the network size. We
conclude that rank correlations provide a suitable and informative method for
uncovering network mixing patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0267</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0267</id><created>2012-04-01</created><updated>2012-04-03</updated><authors><author><keyname>Bardhan</keyname><forenames>Jaydeep P.</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author></authors><title>Computational science and re-discovery: open-source implementations of
  ellipsoidal harmonics for problems in potential theory</title><categories>cs.CE cs.MS physics.chem-ph physics.comp-ph</categories><comments>25 pages, 3 figures</comments><journal-ref>Computational Science &amp; Discovery, 5:014006, 2012</journal-ref><doi>10.1088/1749-4699/5/1/014006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two open-source (BSD) implementations of ellipsoidal harmonic
expansions for solving problems of potential theory using separation of
variables. Ellipsoidal harmonics are used surprisingly infrequently,
considering their substantial value for problems ranging in scale from
molecules to the entire solar system. In this article, we suggest two possible
reasons for the paucity relative to spherical harmonics. The first is
essentially historical---ellipsoidal harmonics developed during the late 19th
century and early 20th, when it was found that only the lowest-order harmonics
are expressible in closed form. Each higher-order term requires the solution of
an eigenvalue problem, and tedious manual computation seems to have discouraged
applications and theoretical studies. The second explanation is practical: even
with modern computers and accurate eigenvalue algorithms, expansions in
ellipsoidal harmonics are significantly more challenging to compute than those
in Cartesian or spherical coordinates. The present implementations reduce the
&quot;barrier to entry&quot; by providing an easy and free way for the community to begin
using ellipsoidal harmonics in actual research. We demonstrate our
implementation using the specific and physiologically crucial problem of how
charged proteins interact with their environment, and ask: what other
analytical tools await re-discovery in an era of inexpensive computation?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0274</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0274</id><created>2012-04-01</created><authors><author><keyname>Woodward</keyname><forenames>Mark P.</forenames></author><author><keyname>Wood</keyname><forenames>Robert J.</forenames></author></authors><title>Learning from Humans as an I-POMDP</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interactive partially observable Markov decision process (I-POMDP) is a
recently developed framework which extends the POMDP to the multi-agent setting
by including agent models in the state space. This paper argues for formulating
the problem of an agent learning interactively from a human teacher as an
I-POMDP, where the agent \emph{programming} to be learned is captured by random
variables in the agent's state space, all \emph{signals} from the human teacher
are treated as observed random variables, and the human teacher, modeled as a
distinct agent, is explicitly represented in the agent's state space. The main
benefits of this approach are: i. a principled action selection mechanism, ii.
a principled belief update mechanism, iii. support for the most common teacher
\emph{signals}, and iv. the anticipated production of complex beneficial
interactions. The proposed formulation, its benefits, and several open
questions are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0280</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0280</id><created>2012-04-01</created><authors><author><keyname>Woodward</keyname><forenames>Mark P.</forenames></author><author><keyname>Wood</keyname><forenames>Robert J.</forenames></author></authors><title>Framing Human-Robot Task Communication as a POMDP</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As general purpose robots become more capable, pre-programming of all tasks
at the factory will become less practical. We would like for non-technical
human owners to be able to communicate, through interaction with their robot,
the details of a new task; we call this interaction &quot;task communication&quot;.
During task communication the robot must infer the details of the task from
unstructured human signals and it must choose actions that facilitate this
inference. In this paper we propose the use of a partially observable Markov
decision process (POMDP) for representing the task communication problem; with
the unobservable task details and unobservable intentions of the human teacher
captured in the state, with all signals from the human represented as
observations, and with the cost function chosen to penalize uncertainty. We
work through an example representation of task communication as a POMDP, and
present results from a user experiment on an interactive virtual robot,
compared with a human controlled virtual robot, for a task involving a single
object movement and binary approval input from the teacher. The results suggest
that the proposed POMDP representation produces robots that are robust to
teacher error, that can accurately infer task details, and that are perceived
to be intelligent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0281</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0281</id><created>2012-04-01</created><authors><author><keyname>Spurek</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Tabor</keyname><forenames>Jacek</forenames></author></authors><title>The memory centre</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $x \in \R$ be given. As we know the, amount of bits needed to binary code
$x$ with given accuracy ($h \in \R$) is approximately $ \m_{h}(x) \approx
\log_{2}(\max {1, |\frac{x}{h}|}). $ We consider the problem where we should
translate the origin $a$ so that the mean amount of bits needed to code
randomly chosen element from a realization of a random variable $X$ is minimal.
In other words, we want to find $a \in \R$ such that $$ \R \ni a \to \mathrm{E}
(\m_{h} (X-a)) $$ attains minimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0301</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0301</id><created>2012-04-01</created><authors><author><keyname>Sukhavasi</keyname><forenames>Ravi Teja</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Tree Codes Improve Convergence Rate of Consensus Over Erasure Channels</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of achieving average consensus between a group of agents
over a network with erasure links. In the context of consensus problems, the
unreliability of communication links between nodes has been traditionally
modeled by allowing the underlying graph to vary with time. In other words,
depending on the realization of the link erasures, the underlying graph at each
time instant is assumed to be a subgraph of the original graph. Implicit in
this model is the assumption that the erasures are symmetric: if at time t the
packet from node i to node j is dropped, the same is true for the packet
transmitted from node j to node i. However, in practical wireless communication
systems this assumption is unreasonable and, due to the lack of symmetry,
standard averaging protocols cannot guarantee that the network will reach
consensus to the true average. In this paper we explore the use of channel
coding to improve the performance of consensus algorithms. For symmetric
erasures, we show that, for certain ranges of the system parameters, repetition
codes can speed up the convergence rate. For asymmetric erasures we show that
tree codes (which have recently been designed for erasure channels) can be used
to simulate the performance of the original &quot;unerased&quot; graph. Thus, unlike
conventional consensus methods, we can guarantee convergence to the average in
the asymmetric case. The price is a slowdown in the convergence rate, relative
to the unerased network, which is still often faster than the convergence rate
of conventional consensus algorithms over noisy links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0304</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0304</id><created>2012-04-01</created><updated>2012-12-21</updated><authors><author><keyname>Gharesifard</keyname><forenames>Bahman</forenames></author><author><keyname>Cortes</keyname><forenames>Jorge</forenames></author></authors><title>Distributed continuous-time convex optimization on weight-balanced
  digraphs</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the continuous-time distributed optimization of a sum of
convex functions over directed graphs. Contrary to what is known in the
consensus literature, where the same dynamics works for both undirected and
directed scenarios, we show that the consensus-based dynamics that solves the
continuous-time distributed optimization problem for undirected graphs fails to
converge when transcribed to the directed setting. This study sets the basis
for the design of an alternative distributed dynamics which we show is
guaranteed to converge, on any strongly connected weight-balanced digraph, to
the set of minimizers of a sum of convex differentiable functions with globally
Lipschitz gradients. Our technical approach combines notions of invariance and
cocoercivity with the positive definiteness properties of graph matrices to
establish the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0309</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0309</id><created>2012-04-02</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>A Model for Personalized Keyword Extraction from Web Pages using
  Segmentation</title><categories>cs.IR</categories><comments>6 Pages, 2 Figures</comments><msc-class>68P20</msc-class><journal-ref>International Journal of Computer Applications (0975 - 8887),
  Volume 42 - No.4, March 2012</journal-ref><doi>10.5120/5682-7720</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web caters to the needs of billions of users in heterogeneous
groups. Each user accessing the World Wide Web might have his / her own
specific interest and would expect the web to respond to the specific
requirements. The process of making the web to react in a customized manner is
achieved through personalization. This paper proposes a novel model for
extracting keywords from a web page with personalization being incorporated
into it. The keyword extraction problem is approached with the help of web page
segmentation which facilitates in making the problem simpler and solving it
effectively. The proposed model is implemented as a prototype and the
experiments conducted on it empirically validate the model's efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0323</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0323</id><created>2012-04-02</created><authors><author><keyname>Renault</keyname><forenames>Jerome</forenames></author><author><keyname>Solan</keyname><forenames>Eilon</forenames></author><author><keyname>Vieille</keyname><forenames>Nicolas</forenames></author></authors><title>Dynamic Sender-Receiver Games</title><categories>math.PR cs.GT</categories><msc-class>60J10, 91A05, 91A10, 91A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic version of sender-receiver games, where the sequence of
states follows an irreducible Markov chain observed by the sender. Under mild
assumptions, we provide a simple characterization of the limit set of
equilibrium payoffs, as players become very patient. Under these assumptions,
the limit set depends on the Markov chain only through its invariant measure.
The (limit) equilibrium payoffs are the feasible payoffs that satisfy an
individual rationality condition for the receiver, and an incentive
compatibility condition for the sender.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0334</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0334</id><created>2012-04-02</created><updated>2012-07-26</updated><authors><author><keyname>Zhao</keyname><forenames>Yue</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>Implementation Of Decoders for LDPC Block Codes and LDPC Convolutional
  Codes Based on GPUs</title><categories>cs.IT cs.DC math.IT</categories><comments>9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the use of belief propagation (BP) decoding algorithm, low-density
parity-check (LDPC) codes can achieve near-Shannon limit performance. In order
to evaluate the error performance of LDPC codes, simulators running on CPUs are
commonly used. However, the time taken to evaluate LDPC codes with very good
error performance is excessive. In this paper, efficient LDPC block-code
decoders/simulators which run on graphics processing units (GPUs) are proposed.
We also implement the decoder for the LDPC convolutional code (LDPCCC). The
LDPCCC is derived from a pre-designed quasi-cyclic LDPC block code with good
error performance. Compared to the decoder based on the randomly constructed
LDPCCC code, the complexity of the proposed LDPCCC decoder is reduced due to
the periodicity of the derived LDPCCC and the properties of the quasi-cyclic
structure. In our proposed decoder architecture, $\Gamma$ (a multiple of a
warp) codewords are decoded together and hence the messages of $\Gamma$
codewords are also processed together. Since all the $\Gamma$ codewords share
the same Tanner graph, messages of the $\Gamma$ distinct codewords
corresponding to the same edge can be grouped into one package and stored
linearly. By optimizing the data structures of the messages used in the
decoding process, both the read and write processes can be performed in a
highly parallel manner by the GPUs. In addition, a thread hierarchy minimizing
the divergence of the threads is deployed, and it can maximize the efficiency
of the parallel execution. With the use of a large number of cores in the GPU
to perform the simple computations simultaneously, our GPU-based LDPC decoder
can obtain hundreds of times speedup compared with a serial CPU-based simulator
and over 40 times speedup compared with an 8-thread CPU-based simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0343</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0343</id><created>2012-04-02</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Comments on &quot;Prediction of Subharmonic Oscillation in Switching
  Converters Under Different Control Strategies&quot;</title><categories>cs.SY math.DS nlin.CD</categories><comments>Submitted to IEEE Transactions on Circuits and Systems II: Express
  Briefs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent paper [1] (El Aroudi, 2012) misapplied a critical condition (Fang
and Abed, 2001) to a well-known example. Even if the mistake is corrected, the
results in [1] are applicable only to buck converters and period-doubling
bifurcation. Actually, these results are known in Fang's works a decade ago
which have broader critical conditions applicable to other converters and
bifurcations. The flaws in [1] are identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0347</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0347</id><created>2012-04-02</created><authors><author><keyname>Geuvers</keyname><forenames>Herman</forenames></author><author><keyname>Krebbers</keyname><forenames>Robbert</forenames></author><author><keyname>McKinna</keyname><forenames>James</forenames></author></authors><title>The lambda-mu-T-calculus</title><categories>cs.LO</categories><doi>10.1016/j.apal.2012.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculi with control operators have been studied as extensions of simple type
theory. Real programming languages contain datatypes, so to really understand
control operators, one should also include these in the calculus. As a first
step in that direction, we introduce lambda-mu-T, a combination of Parigot's
lambda-mu-calculus and G\&quot;odel's T, to extend a calculus with control operators
with a datatype of natural numbers with a primitive recursor.
  We consider the problem of confluence on raw terms, and that of strong
normalization for the well-typed terms. Observing some problems with extending
the proofs of Baba at al. and Parigot's original confluence proof, we provide
new, and improved, proofs of confluence (by complete developments) and strong
normalization (by reducibility and a postponement argument) for our system.
  We conclude with some remarks about extensions, choices, and prospects for an
improved presentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0354</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0354</id><created>2012-04-02</created><updated>2013-03-19</updated><authors><author><keyname>Luo</keyname><forenames>Wuqiong</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Leng</keyname><forenames>Mei</forenames></author></authors><title>Identifying Infection Sources and Regions in Large Networks</title><categories>cs.DM cs.SI physics.soc-ph</categories><doi>10.1109/TSP.2013.2256902</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Identifying the infection sources in a network, including the index cases
that introduce a contagious disease into a population network, the servers that
inject a computer virus into a computer network, or the individuals who started
a rumor in a social network, plays a critical role in limiting the damage
caused by the infection through timely quarantine of the sources. We consider
the problem of estimating the infection sources and the infection regions
(subsets of nodes infected by each source) in a network, based only on
knowledge of which nodes are infected and their connections, and when the
number of sources is unknown a priori. We derive estimators for the infection
sources and their infection regions based on approximations of the infection
sequences count. We prove that if there are at most two infection sources in a
geometric tree, our estimator identifies the true source or sources with
probability going to one as the number of infected nodes increases. When there
are more than two infection sources, and when the maximum possible number of
infection sources is known, we propose an algorithm with quadratic complexity
to estimate the actual number and identities of the infection sources.
Simulations on various kinds of networks, including tree networks, small-world
networks and real world power grid networks, and tests on two real data sets
are provided to verify the performance of our estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0357</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0357</id><created>2012-04-02</created><authors><author><keyname>Bauer</keyname><forenames>Stefan</forenames><affiliation>Institute for Surgical Technology and Biomechanics, University of Bern, Switzerland</affiliation></author><author><keyname>Nolte</keyname><forenames>Lutz-P.</forenames><affiliation>Institute for Surgical Technology and Biomechanics, University of Bern, Switzerland</affiliation></author><author><keyname>Reyes</keyname><forenames>Mauricio</forenames><affiliation>Institute for Surgical Technology and Biomechanics, University of Bern, Switzerland</affiliation></author></authors><title>Skull-stripping for Tumor-bearing Brain Images</title><categories>cs.CV cs.CE</categories><comments>Swiss Society of Biomedical Engineering, Annual Meeting 2011, Bern,
  Switzerland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skull-stripping separates the skull region of the head from the soft brain
tissues. In many cases of brain image analysis, this is an essential
preprocessing step in order to improve the final result. This is true for both
registration and segmentation tasks. In fact, skull-stripping of magnetic
resonance images (MRI) is a well-studied problem with numerous publications in
recent years. Many different algorithms have been proposed, a summary and
comparison of which can be found in [Fennema-Notestine, 2006]. Despite the
abundance of approaches, we discovered that the algorithms which had been
suggested so far, perform poorly when dealing with tumor-bearing brain images.
This is mostly due to additional difficulties in separating the brain from the
skull in this case, especially when the lesion is located very close to the
skull border. Additionally, images acquired according to standard clinical
protocols, often exhibit anisotropic resolution and only partial coverage,
which further complicates the task. Therefore, we developed a method which is
dedicated to skull-stripping for clinically acquired tumor-bearing brain
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0368</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0368</id><created>2012-04-02</created><authors><author><keyname>Shahrbanoo</keyname><forenames>Majlesi</forenames></author><author><keyname>Ali</keyname><forenames>Mehrpour</forenames></author><author><keyname>Mehran</keyname><forenames>Mohsenzadeh</forenames></author></authors><title>An Approach for Agile SOA Development using Agile Principals</title><categories>cs.SE</categories><comments>8 pages, 1 figure, 1 table;
  http://airccse.org/journal/ijcsit2012_curr.html</comments><doi>10.5121/ijcsit.2012.4118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In dynamic and turbulent business environment, the need for success and
survival of any organization is the ability of adapting to changes efficiently
and cost-effectively. So, for developing software applications, one of the
methods is Service Oriented Architecture (SOA) methodology and other is Agile
Methodology. Since embracing changes is the indispensable concept of SOA
development as well as Agile Development, using an appropriate SOA methodology
able to adapt changes even during system development with the preservation of
software quality is necessary. In this paper, a new approach consisted of five
steps is presented to add agility to SOA methodologies. This approach, before
any SOA-based development, helps architect(s) to determine Core Business
Processes (CBPs) by using agile principals for establishing Core Architecture.
The most important advantage of this approach according to the results of case
study is possibility of embracing changes with the preservation of software
quality in SOA developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0375</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0375</id><created>2012-04-02</created><authors><author><keyname>Laaraiedh</keyname><forenames>Mohamed</forenames><affiliation>IETR</affiliation></author></authors><title>Implementation of Kalman Filter with Python Language</title><categories>cs.NI</categories><comments>The Python Papers (2009)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the implementation of a Python code for a
Kalman Filter using the Numpy package. A Kalman Filtering is carried out in two
steps: Prediction and Update. Each step is investigated and coded as a function
with matrix input and output. These different functions are explained and an
example of a Kalman Filter application for the localization of mobile in
wireless networks is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0386</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0386</id><created>2012-04-02</created><authors><author><keyname>Lima</keyname><forenames>F. W. S.</forenames></author></authors><title>Tax evasion dynamics and Zaklan model on Opinion-dependent Network</title><categories>physics.soc-ph cs.SI</categories><comments>14 page, 4 figures</comments><doi>10.1142/S0129183112500477</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the context of agent-based Monte-Carlo simulations, we study the
well-known majority-vote model (MVM) with noise applied to tax evasion on
Stauffer-Hohnisch-Pittnauer (SHP) networks. To control the fluctuations for tax
evasion in the economics model proposed by Zaklan, MVM is applied in the
neighborhood of the critical noise $q_{c}$ to evolve the Zaklan model. The
Zaklan model had been studied recently using the equilibrium Ising model. Here
we show that the Zaklan model is robust because this can be studied besides
using equilibrium dynamics of Ising model also through the nonequilibrium MVM
and on various topologies giving the same behavior regardless of dynamic or
topology used here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0414</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0414</id><created>2012-04-02</created><authors><author><keyname>Fajstrup</keyname><forenames>Lisbeth</forenames><affiliation>CEA LIST</affiliation></author><author><keyname>Goubault</keyname><forenames>Eric</forenames><affiliation>CEA LIST</affiliation></author><author><keyname>Haucourt</keyname><forenames>Emmanuel</forenames><affiliation>CEA LIST</affiliation></author><author><keyname>Mimram</keyname><forenames>Samuel</forenames><affiliation>CEA LIST</affiliation></author><author><keyname>Raussen</keyname><forenames>Martin</forenames></author></authors><title>Trace Spaces: an Efficient New Technique for State-Space Reduction</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>ESOP - 21st European Symposium on Programming 7211 (2012) 274-294</journal-ref><doi>10.1007/978-3-642-28869-2_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  State-space reduction techniques, used primarily in model-checkers, all rely
on the idea that some actions are independent, hence could be taken in any
(respective) order while put in parallel, without changing the semantics. It is
thus not necessary to consider all execution paths in the interleaving
semantics of a concurrent program, but rather some equivalence classes. The
purpose of this paper is to describe a new algorithm to compute such
equivalence classes, and a representative per class, which is based on ideas
originating in algebraic topology. We introduce a geometric semantics of
concurrent languages, where programs are interpreted as directed topological
spaces, and study its properties in order to devise an algorithm for computing
dihomotopy classes of execution paths. In particular, our algorithm is able to
compute a control-flow graph for concurrent programs, possibly containing
loops, which is &quot;as reduced as possible&quot; in the sense that it generates traces
modulo equivalence. A preliminary implementation was achieved, showing
promising results towards efficient methods to analyze concurrent programs,
with very promising results compared to partial-order reduction techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0416</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0416</id><created>2012-04-02</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Jacko</keyname><forenames>Peter</forenames><affiliation>BCAM</affiliation></author></authors><title>CCN Interest Forwarding Strategy as Multi-Armed Bandit Model with Delays</title><categories>cs.NI</categories><comments>No. RR-7917 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Content Centric Network (CCN) interest forwarding problem as a
Multi-Armed Bandit (MAB) problem with delays. We investigate the transient
behaviour of the $\eps$-greedy, tuned $\eps$-greedy and Upper Confidence Bound
(UCB) interest forwarding policies. Surprisingly, for all the three policies
very short initial exploratory phase is needed. We demonstrate that the tuned
$\eps$-greedy algorithm is nearly as good as the UCB algorithm, the best
currently available algorithm. We prove the uniform logarithmic bound for the
tuned $\eps$-greedy algorithm. In addition to its immediate application to CCN
interest forwarding, the new theoretical results for MAB problem with delays
represent significant theoretical advances in machine learning discipline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0423</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0423</id><created>2012-04-02</created><updated>2012-05-21</updated><authors><author><keyname>Lampos</keyname><forenames>Vasileios</forenames></author></authors><title>On voting intentions inference from Twitter content: a case study on UK
  2010 General Election</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 2 figures, 7 tables</comments><acm-class>J.4; I.7.0; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a report, where preliminary work regarding the topic of voting
intention inference from Social Media - such as Twitter - is presented. Our
case study is the UK 2010 General Election and we are focusing on predicting
the percentages of voting intention polls (conducted by YouGov) for the three
major political parties - Conservatives, Labours and Liberal Democrats - during
a 5-month period before the election date (May 6, 2010). We form three
methodologies for extracting positive or negative sentiment from tweets, which
build on each other, and then propose two supervised models for turning
sentiment into voting intention percentages. Interestingly, when the content of
tweets is enriched by attaching synonymous words, a significant improvement on
inference performance is achieved reaching a mean absolute error of 4.34% +/-
2.13%; in that case, the predictions are also shown to be statistically
significant. The presented methods should be considered as work-in-progress;
limitations and suggestions for future work appear in the final section of this
script.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0429</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0429</id><created>2012-04-02</created><updated>2012-07-31</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>Relative Information Loss in the PCA</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figure; extended version of a paper accepted for
  publication</comments><journal-ref>Proc. IEEE Information Theory Workshop, 2012, pp. 562 - 566</journal-ref><doi>10.1109/ITW.2012.6404738</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyze principle component analysis (PCA) as a deterministic
input-output system. We show that the relative information loss induced by
reducing the dimensionality of the data after performing the PCA is the same as
in dimensionality reduction without PCA. Finally, we analyze the case where the
PCA uses the sample covariance matrix to compute the rotation. If the rotation
matrix is not available at the output, we show that an infinite amount of
information is lost. The relative information loss is shown to decrease with
increasing sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0431</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0431</id><created>2012-04-02</created><updated>2013-11-11</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>On Dispersions of Discrete Memoryless Channels with Noncausal State
  Information at the Encoder</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn due to an error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the finite blocklength limits of state-dependent
discrete memoryless channels where the discrete memoryless state is known
noncausally at the encoder. For the point-to-point case, this is known as the
Gel'fand-Pinsker channel model. We define the (n,\epsilon)-capacity of the
Gel'fand-Pinsker channel as the maximal rate of transmission of a message
subject to the condition that the length of the block-code is n and the average
error probability is no larger than \epsilon. This paper provides a lower bound
for the (n,\epsilon)-capacity of the Gel'fand-Pinsker channel model, and hence
an upper bound on the dispersion, a fundamental second-order quantity in the
study of the performance limits of discrete memoryless channels. In addition,
we extend the work of Y. Steinberg (2005), in which the (degraded) broadcast
channel extension of the Gel'fand-Pinsker model was studied. We provide and
inner bound to the (n,\epsilon)-capacity region for this broadcast channel
model using a combination of ideas of Gel'fand-Pinsker coding, superposition
coding and dispersion (finite blocklength) analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0447</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0447</id><created>2012-04-02</created><authors><author><keyname>Winter</keyname><forenames>Philipp</forenames></author><author><keyname>Lindskog</keyname><forenames>Stefan</forenames></author></authors><title>How China Is Blocking Tor</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Not only the free web is victim to China's excessive censorship, but also the
Tor anonymity network: the Great Firewall of China prevents thousands of
potential Tor users from accessing the network. In this paper, we investigate
how the blocking mechanism is implemented, we conjecture how China's Tor
blocking infrastructure is designed and we propose countermeasures. Our work
bolsters the understanding of China's censorship capabilities and thus paves
the way towards more effective evasion techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0448</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0448</id><created>2012-04-02</created><updated>2014-04-02</updated><authors><author><keyname>Alqaralleh</keyname><forenames>Bassam</forenames></author><author><keyname>Almi'ani</keyname><forenames>K.</forenames></author></authors><title>Data Gathering Scheme for Wireless SensorNetworks Using a Single Mobile
  Element</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the experiment section</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we investigate the problem of gathering the data in wireless
sensor network using a single Mobile Element. In particular we consider the
case where the data are produced by measurements and they need to be delivered
to a predefined sink within a given time interval from the time the measurement
takes place. A mobile element travels the network in predefined paths, collect
the data from the nodes, and deliver them to the sink by a single long-distance
transmission. In this problem, the length of the mobile element path is bounded
by pre-determined length. This path will visit a subset of the nodes. These
selected nodes will work as caching points and will aggregate the other nodes'
data. The caching point nodes are selected with the aim of reducing the energy
expenditures due to multi-hop forwarding. We provide a heuristic-based solution
for this problem. We evaluate the performance of our algorithm by comparing it
to the best well-known algorithms from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0459</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0459</id><created>2012-04-02</created><authors><author><keyname>Zhang</keyname><forenames>Zhenghao</forenames></author><author><keyname>Gong</keyname><forenames>Shuping</forenames></author><author><keyname>Dimitrovski</keyname><forenames>Aleksandar D.</forenames></author><author><keyname>Li</keyname><forenames>Husheng</forenames></author></authors><title>Time Synchronization Attack in Smart Grid-Part I: Impact and Analysis</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many operations in power grids, such as fault detection and event location
estimation, depend on precise timing information. In this paper, a novel Time
Synchronization Attack (TSA) is proposed to attack the timing information in
smart grid. Since many applications in smart grid utilize synchronous
measurements and most of the measurement devices are equipped with global
positioning system (GPS) for precise timing, it is highly probable to attack
the measurement system by spoofing the GPS. The effectiveness of TSA is
demonstrated for three applications of phasor measurement unit (PMU) in smart
grid, namely transmission line fault detection, voltage stability monitoring
and event locationing. The validity of TSA is demonstrated by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0462</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0462</id><created>2012-04-02</created><authors><author><keyname>Zhang</keyname><forenames>Zhenghao</forenames></author><author><keyname>Trinkle</keyname><forenames>Matthew</forenames></author><author><keyname>Dimitrovski</keyname><forenames>Aleksandar D.</forenames></author><author><keyname>Li</keyname><forenames>Husheng</forenames></author></authors><title>Time Synchronization Attack in Smart Grid-Part II: Cross Layer Detection
  Mechanism</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel time synchronization attack (TSA) on wide area monitoring systems in
smart grid has been identified in the first part of this paper. A cross layer
detection mechanism is proposed to combat TSA in part II of this paper. In the
physical layer, we propose a GPS carrier signal noise ratio (C/No) based
spoofing detection technique. In addition, a patch-monopole hybrid antenna is
applied to receive GPS signal. By computing the standard deviation of the C/No
difference from two GPS receivers, a priori probability of spoofing detection
is fed to the upper layer, where power system state is estimated and
controlled. A trustworthiness based evaluation method is applied to identify
the PMU being under TSA. Both the physical layer and upper layer algorithms are
integrated to detect the TSA, thus forming a cross layer mechanism. Experiment
is carried out to verify the effectiveness of the proposed TSA detection
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0469</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0469</id><created>2012-03-30</created><authors><author><keyname>Bertrand</keyname><forenames>Nathalie</forenames></author><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author></authors><title>Bounded Satisfiability for PCTL</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While model checking PCTL for Markov chains is decidable in polynomial-time,
the decidability of PCTL satisfiability, as well as its finite model property,
are long standing open problems. While general satisfiability is an intriguing
challenge from a purely theoretical point of view, we argue that general
solutions would not be of interest to practitioners: such solutions could be
too big to be implementable or even infinite. Inspired by bounded synthesis
techniques, we turn to the more applied problem of seeking models of a bounded
size: we restrict our search to implementable -- and therefore reasonably
simple -- models. We propose a procedure to decide whether or not a given PCTL
formula has an implementable model by reducing it to an SMT problem. We have
implemented our techniques and found that they can be applied to the practical
problem of sanity checking -- a procedure that allows a system designer to
check whether their formula has an unexpectedly small model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0479</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0479</id><created>2012-04-02</created><authors><author><keyname>Buer</keyname><forenames>Tobias</forenames></author><author><keyname>Homberger</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Gehring</keyname><forenames>Hermann</forenames></author></authors><title>A collaborative ant colony metaheuristic for distributed multi-level
  lot-sizing</title><categories>cs.AI cs.DC</categories><msc-class>91B14, 68W15, 90B30 (Primary) 68T20, 91B10, 91B12 (Secondary)</msc-class><acm-class>I.2.11; G.1.6; I.2.8; G.2.3</acm-class><journal-ref>International Journal of Production Research 51 (17) 2013,
  5253-5270</journal-ref><doi>10.1080/00207543.2013.802822</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an ant colony optimization metaheuristic for collaborative
planning. Collaborative planning is used to coordinate individual plans of
self-interested decision makers with private information in order to increase
the overall benefit of the coalition. The method consists of a new search graph
based on encoded solutions. Distributed and private information is integrated
via voting mechanisms and via a simple but effective collaborative local search
procedure. The approach is applied to a distributed variant of the multi-level
lot-sizing problem and evaluated by means of 352 benchmark instances from the
literature. The proposed approach clearly outperforms existing approaches on
the sets of medium and large sized instances. While the best method in the
literature so far achieves an average deviation from the best known
non-distributed solutions of 46 percent for the set of the largest instances,
for example, the presented approach reduces the average deviation to only 5
percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0480</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0480</id><created>2012-04-02</created><updated>2014-11-17</updated><authors><author><keyname>Ramsdell</keyname><forenames>John D.</forenames></author></authors><title>Deducing Security Goals From Shape Analysis Sentences</title><categories>cs.CR cs.LO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Guttman presented a model-theoretic approach to establishing security goals
in the context of Strand Space theory. In his approach, a run of the
Cryptographic Protocol Shapes Analyzer (CPSA) produces models that determine if
a goal is satisfied. This paper presents a method for extracting a sentence
that completely characterizes a run of CPSA. Logical deduction can then be used
to determine if a goal is satisfied. This method has been implemented and is
available to all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0491</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0491</id><created>2012-04-02</created><updated>2012-06-08</updated><authors><author><keyname>Yagan</keyname><forenames>Osman</forenames></author><author><keyname>Gligor</keyname><forenames>Virgil</forenames></author></authors><title>Analysis of complex contagions in random multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>Revised 06/08/12. 11 Pages, 3 figures</comments><journal-ref>Phys. Rev. E 86, 036103 (2012)</journal-ref><doi>10.1103/PhysRevE.86.036103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the diffusion of influence in random multiplex networks where links
can be of $r$ different types, and for a given content (e.g., rumor, product,
political view), each link type is associated with a content dependent
parameter $c_i$ in $[0,\infty]$ that measures the relative bias type-$i$ links
have in spreading this content. In this setting, we propose a linear threshold
model of contagion where nodes switch state if their &quot;perceived&quot; proportion of
active neighbors exceeds a threshold \tau. Namely, a node connected to $m_i$
active neighbors and $k_i-m_i$ inactive neighbors via type-$i$ links will turn
active if $\sum{c_i m_i}/\sum{c_i k_i}$ exceeds its threshold \tau. Under this
model, we obtain the condition, probability and expected size of global
spreading events. Our results extend the existing work on complex contagions in
several directions by i) providing solutions for coupled random networks whose
vertices are neither identical nor disjoint, (ii) highlighting the effect of
content on the dynamics of complex contagions, and (iii) showing that
content-dependent propagation over a multiplex network leads to a subtle
relation between the giant vulnerable component of the graph and the global
cascade condition that is not seen in the existing models in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0521</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0521</id><created>2012-04-02</created><updated>2012-04-30</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author></authors><title>Explicit receivers for pure-interference bosonic multiple access
  channels</title><categories>quant-ph cs.IT math.IT</categories><comments>v1: 9 pages, 2 figures, submission to the 2012 International
  Symposium on Information Theory and its Applications (ISITA 2012), Honolulu,
  Hawaii, USA; v2: minor changes</comments><journal-ref>Proceedings of the 2012 International Symposium on Information
  Theory and its Applications, pages 303-307, October 2012. Available at
  http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=6400941</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pure-interference bosonic multiple access channel has two senders and one
receiver, such that the senders each communicate with multiple temporal modes
of a single spatial mode of light. The channel mixes the input modes from the
two users pairwise on a lossless beamsplitter, and the receiver has access to
one of the two output ports. In prior work, Yen and Shapiro found the capacity
region of this channel if encodings consist of coherent-state preparations.
Here, we demonstrate how to achieve the coherent-state Yen-Shapiro region (for
a range of parameters) using a sequential decoding strategy, and we show that
our strategy outperforms the rate regions achievable using conventional
receivers. Our receiver performs binary-outcome quantum measurements for every
codeword pair in the senders' codebooks. A crucial component of this scheme is
a non-destructive &quot;vacuum-or-not&quot; measurement that projects an n-symbol
modulated codeword onto the n-fold vacuum state or its orthogonal complement,
such that the post-measurement state is either the n-fold vacuum or has the
vacuum removed from the support of the n symbols' joint quantum state. This
receiver requires the additional ability to perform multimode optical
phase-space displacements which are realizable using a beamsplitter and a
laser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0535</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0535</id><created>2012-04-02</created><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Doubleclick Ad Exchange Auction</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Display advertisements on the web are sold via ad exchanges that use real
time auction. We describe the challenges of designing a suitable auction, and
present a simple auction called the Optional Second Price (OSP) auction that is
currently used in Doubleclick Ad Exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0543</identifier>
 <datestamp>2012-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0543</id><created>2012-04-02</created><updated>2012-08-15</updated><authors><author><keyname>Kane</keyname><forenames>Daniel M.</forenames></author></authors><title>A Structure Theorem for Poorly Anticoncentrated Gaussian Chaoses and
  Applications to the Study of Polynomial Threshold Functions</title><categories>math.PR cs.CC</categories><msc-class>60G15, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a structural result for degree-$d$ polynomials. In particular, we
show that any degree-$d$ polynomial, $p$ can be approximated by another
polynomial, $p_0$, which can be decomposed as some function of polynomials
$q_1,...,q_m$ with $q_i$ normalized and $m=O_d(1)$, so that if $X$ is a
Gaussian random variable, the probability distribution on $(q_1(X),...,q_m(X))$
does not have too much mass in any small box.
  Using this result, we prove improved versions of a number of results about
polynomial threshold functions, including producing better pseudorandom
generators, obtaining a better invariance principle, and proving improved
bounds on noise sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0547</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0547</id><created>2012-04-02</created><authors><author><keyname>D&#xed;az-Ba&#xf1;ez</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>Pablo</forenames></author></authors><title>On the number of radial orderings of planar point sets</title><categories>cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $S$ of $n$ points in the plane, a \emph{radial ordering} of $S$
with respect to a point $p$ (not in $S$) is a clockwise circular ordering of
the elements in $S$ by angle around $p$. If $S$ is two-colored, a \emph{colored
radial ordering} is a radial ordering of $S$ in which only the colors of the
points are considered. In this paper, we obtain bounds on the number of
distinct non-colored and colored radial orderings of $S$. We assume a strong
general position on $S$, not three points are collinear and not three
lines---each passing through a pair of points in $S$---intersect in a point of
$\R^2\setminus S$. In the colored case, $S$ is a set of $2n$ points partitioned
into $n$ red and $n$ blue points, and $n$ is even. We prove that: the number of
distinct radial orderings of $S$ is at most $O(n^4)$ and at least
$\Omega(n^3)$; the number of colored radial orderings of $S$ is at most
$O(n^4)$ and at least $\Omega(n)$; there exist sets of points with
$\Theta(n^4)$ colored radial orderings and sets of points with only $O(n^2)$
colored radial orderings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0556</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0556</id><created>2012-04-02</created><updated>2013-09-23</updated><authors><author><keyname>Barman</keyname><forenames>Siddharth</forenames></author><author><keyname>Liu</keyname><forenames>Xishuo</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Decomposition Methods for Large Scale LP Decoding</title><categories>cs.IT math.IT math.OC</categories><comments>35 pages, 11 figures. An early version of this work appeared at the
  49th Annual Allerton Conference, September 2011. This version to appear in
  IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When binary linear error-correcting codes are used over symmetric channels, a
relaxed version of the maximum likelihood decoding problem can be stated as a
linear program (LP). This LP decoder can be used to decode error-correcting
codes at bit-error-rates comparable to state-of-the-art belief propagation (BP)
decoders, but with significantly stronger theoretical guarantees. However, LP
decoding when implemented with standard LP solvers does not easily scale to the
block lengths of modern error correcting codes. In this paper we draw on
decomposition methods from optimization theory, specifically the Alternating
Directions Method of Multipliers (ADMM), to develop efficient distributed
algorithms for LP decoding.
  The key enabling technical result is a &quot;two-slice&quot; characterization of the
geometry of the parity polytope, which is the convex hull of all codewords of a
single parity check code. This new characterization simplifies the
representation of points in the polytope. Using this simplification, we develop
an efficient algorithm for Euclidean norm projection onto the parity polytope.
This projection is required by ADMM and allows us to use LP decoding, with all
its theoretical guarantees, to decode large-scale error correcting codes
efficiently.
  We present numerical results for LDPC codes of lengths more than 1000. The
waterfall region of LP decoding is seen to initiate at a slightly higher
signal-to-noise ratio than for sum-product BP, however an error floor is not
observed for LP decoding, which is not the case for BP. Our implementation of
LP decoding using ADMM executes as fast as our baseline sum-product BP decoder,
is fully parallelizable, and can be seen to implement a type of message-passing
with a particularly simple schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0562</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0562</id><created>2012-04-02</created><updated>2013-02-16</updated><authors><author><keyname>Bhaskar</keyname><forenames>Badri Narayan</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Atomic norm denoising with applications to line spectral estimation</title><categories>cs.IT math.IT</categories><comments>27 pages, 10 figures. A preliminary version of this work appeared in
  the Proceedings of the 49th Annual Allerton Conference in September 2011.
  Numerous numerical experiments added to this version in accordance with
  suggestions by anonymous reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by recent work on atomic norms in inverse problems, we propose a
new approach to line spectral estimation that provides theoretical guarantees
for the mean-squared-error (MSE) performance in the presence of noise and
without knowledge of the model order. We propose an abstract theory of
denoising with atomic norms and specialize this theory to provide a convex
optimization problem for estimating the frequencies and phases of a mixture of
complex exponentials. We show that the associated convex optimization problem
can be solved in polynomial time via semidefinite programming (SDP). We also
show that the SDP can be approximated by an l1-regularized least-squares
problem that achieves nearly the same error rate as the SDP but can scale to
much larger problems. We compare both SDP and l1-based approaches with
classical line spectral analysis methods and demonstrate that the SDP
outperforms the l1 optimization which outperforms MUSIC, Cadzow's, and Matrix
Pencil approaches in terms of MSE over a wide range of signal-to-noise ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0566</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0566</id><created>2012-04-02</created><updated>2012-06-21</updated><authors><author><keyname>Cotter</keyname><forenames>Andrew</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>The Kernelized Stochastic Batch Perceptron</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach for training kernel Support Vector Machines,
establish learning runtime guarantees for our method that are better then those
of any other known kernelized SVM optimization approach, and show that our
method works well in practice compared to existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0590</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0590</id><created>2012-04-03</created><authors><author><keyname>Shah</keyname><forenames>Parikshit</forenames></author><author><keyname>Bhaskar</keyname><forenames>Badri Narayan</forenames></author><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Linear System Identification via Atomic Norm Regularization</title><categories>math.OC cs.IT math.IT</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new algorithm for linear system identification from
noisy measurements. The proposed algorithm balances a data fidelity term with a
norm induced by the set of single pole filters. We pose a convex optimization
problem that approximately solves the atomic norm minimization problem and
identifies the unknown system from noisy linear measurements. This problem can
be solved efficiently with standard, freely available software. We provide
rigorous statistical guarantees that explicitly bound the estimation error (in
the H_2-norm) in terms of the stability radius, the Hankel singular values of
the true system and the number of measurements. These results in turn yield
complexity bounds and asymptotic consistency. We provide numerical experiments
demonstrating the efficacy of our method for estimating linear systems from a
variety of linear measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0634</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0634</id><created>2012-04-03</created><authors><author><keyname>Morvan</keyname><forenames>Gildas</forenames></author><author><keyname>Jolly</keyname><forenames>Daniel</forenames></author></authors><title>Multi-level agent-based modeling with the Influence Reaction principle</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the specification and the implementation of multi-level
agent-based models, using a formal model, IRM4MLS (an Influence Reaction Model
for Multi-Level Simulation), based on the Influence Reaction principle.
Proposed examples illustrate forms of top-down control in (multi-level)
multi-agent based-simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0636</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0636</id><created>2012-04-03</created><authors><author><keyname>Ivan</keyname><forenames>Gheorghe</forenames></author></authors><title>Matrix algorithm for determination of the elementary paths and
  elementary circuits using exotic semirings</title><categories>math.CO cs.DM</categories><comments>12 pages, 1 figure</comments><msc-class>16Y60, 15A09, 05C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for determining the elementary paths and elementary
circuits in a directed graph. Also, the Hamiltonian paths and Hamiltonian
circuits are enumerated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0641</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0641</id><created>2012-04-03</created><updated>2013-05-08</updated><authors><author><keyname>Biely</keyname><forenames>Martin</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author></authors><title>Agreement in Directed Dynamic Networks</title><categories>cs.DC</categories><comments>A preliminary version of this paper appeared in SIROCCO 2012</comments><msc-class>68W15</msc-class><acm-class>C.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study distributed computation in synchronous dynamic networks where an
omniscient adversary controls the unidirectional communication links. Its
behavior is modeled as a sequence of directed graphs representing the active
(i.e. timely) communication links per round. We prove that consensus is
impossible under some natural weak connectivity assumptions, and introduce
vertex-stable root components as a means for circumventing this impossibility.
Essentially, we assume that there is a short period of time during which an
arbitrary part of the network remains strongly connected, while its
interconnect topology may keep changing continuously. We present a consensus
algorithm that works under this assumption, and prove its correctness. Our
algorithm maintains a local estimate of the communication graphs, and applies
techniques for detecting stable network properties and univalent system
configurations. Our possibility results are complemented by several
impossibility results and lower bounds for consensus and other distributed
computing problems like leader election, revealing that our algorithm is
asymptotically optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0643</identifier>
 <datestamp>2012-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0643</id><created>2012-04-03</created><updated>2012-07-26</updated><authors><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Staehle</keyname><forenames>Dirk</forenames></author><author><keyname>Vinel</keyname><forenames>Alexey</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>On the Performance of Packet Aggregation in IEEE 802.11ac MU-MIMO WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-user spatial multiplexing combined with packet aggregation can
significantly increase the performance of Wireless Local Area Networks (WLANs).
In this letter, we present and evaluate a simple technique to perform packet
aggregation in IEEE 802.11ac MU-MIMO (Multi-user Multiple Input Multiple
Output) WLANs. Results show that in non-saturation conditions both the number
of active stations (STAs) and the queue size have a significant impact on the
system performance. If the number of stations is excessively high, the
heterogeneity of destinations in the packets contained in the queue makes it
difficult to take full advantage of packet aggregation. This effect can be
alleviated by increasing the queue size, which increases the chances to
schedule a large number of packets at each transmission, hence improving the
system throughput at the cost of a higher delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0650</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0650</id><created>2012-04-03</created><authors><author><keyname>Gong</keyname><forenames>Kai</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Yang</keyname><forenames>Hui</forenames></author><author><keyname>Shang</keyname><forenames>Mingsheng</forenames></author></authors><title>Variability of Contact Process in Complex Networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 4 figures</comments><msc-class>82C05</msc-class><journal-ref>CHAOS 21, 043130 (2011)</journal-ref><doi>10.1063/1.3664403</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study numerically how the structures of distinct networks influence the
epidemic dynamics in contact process. We first find that the variability
difference between homogeneous and heterogeneous networks is very narrow,
although the heterogeneous structures can induce the lighter prevalence.
Contrary to non-community networks, strong community structures can cause the
secondary outbreak of prevalence and two peaks of variability appeared.
Especially in the local community, the extraordinarily large variability in
early stage of the outbreak makes the prediction of epidemic spreading hard.
Importantly, the bridgeness plays a significant role in the predictability,
meaning the further distance of the initial seed to the bridgeness, the less
accurate the predictability is. Also, we investigate the effect of different
disease reaction mechanisms on variability, and find that the different
reaction mechanisms will result in the distinct variabilities at the end of
epidemic spreading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0660</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0660</id><created>2012-04-03</created><authors><author><keyname>Cabello</keyname><forenames>Sergio</forenames></author></authors><title>Hardness of approximation for crossing number</title><categories>cs.CG cs.CC math.CO</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that, if P\not=NP, there is a constant c &gt; 1 such that there is no
c-approximation algorithm for the crossing number, even when restricted to
3-regular graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0684</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0684</id><created>2012-04-03</created><authors><author><keyname>Scholz</keyname><forenames>Matthias</forenames></author></authors><title>Validation of nonlinear PCA</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>12 pages, 5 figures</comments><journal-ref>Neural Processing Letters, 2012</journal-ref><doi>10.1007/s11063-012-9220-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear principal component analysis (PCA) can be extended to a nonlinear PCA
by using artificial neural networks. But the benefit of curved components
requires a careful control of the model complexity. Moreover, standard
techniques for model selection, including cross-validation and more generally
the use of an independent test set, fail when applied to nonlinear PCA because
of its inherent unsupervised characteristics. This paper presents a new
approach for validating the complexity of nonlinear PCA models by using the
error in missing data estimation as a criterion for model selection. It is
motivated by the idea that only the model of optimal complexity is able to
predict missing values with the highest accuracy. While standard test set
validation usually favours over-fitted nonlinear PCA models, the proposed model
validation approach correctly selects the optimal model complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0706</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0706</id><created>2012-04-03</created><authors><author><keyname>Zhao</keyname><forenames>Zhi-Dan</forenames></author><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author></authors><title>Epidemic Variability in Hierarchical Geographical Networks with Human
  Activity Patterns</title><categories>physics.soc-ph cs.SI</categories><doi>10.1063/1.4730750</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, some studies have revealed that non-Poissonian statistics of human
behaviors stem from the hierarchical geographical network structure. On this
view, we focus on epidemic spreading in the hierarchical geographical networks,
and study how two distinct contact patterns (i. e., homogeneous time delay
(HOTD) and heterogeneous time delay (HETD) associated with geographical
distance) influence the spreading speed and the variability of outbreaks. We
find that, compared with HOTD and null model, correlations between time delay
and network hierarchy in HETD remarkably slow down epidemic spreading, and
result in a upward cascading multi-modal phenomenon. Proportionately, the
variability of outbreaks in HETD has the lower value, but several comparable
peaks for a long time, which makes the long-term prediction of epidemic
spreading hard. When a seed (i. e., the initial infected node) is from the high
layers of networks, epidemic spreading is remarkably promoted. Interestingly,
distinct trends of variabilities in two contact patterns emerge: high-layer
seeds in HOTD result in the lower variabilities, the case of HETD is opposite.
More importantly, the variabilities of high-layer seeds in HETD are much
greater than that in HOTD, which implies the unpredictability of epidemic
spreading in hierarchical geographical networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0707</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0707</id><created>2012-04-03</created><updated>2014-12-02</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author><author><keyname>S&#xf8;rensen</keyname><forenames>Troels Bjerre</forenames></author></authors><title>Approximate Well-supported Nash Equilibria below Two-thirds</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an epsilon-Nash equilibrium, a player can gain at most epsilon by changing
his behaviour. Recent work has addressed the question of how best to compute
epsilon-Nash equilibria, and for what values of epsilon a polynomial-time
algorithm exists. An epsilon-well-supported Nash equilibrium (epsilon-WSNE) has
the additional requirement that any strategy that is used with non-zero
probability by a player must have payoff at most epsilon less than the best
response. A recent algorithm of Kontogiannis and Spirakis shows how to compute
a 2/3-WSNE in polynomial time, for bimatrix games. Here we introduce a new
technique that leads to an improvement to the worst-case approximation
guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0731</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0731</id><created>2012-04-03</created><authors><author><keyname>Bailleux</keyname><forenames>Olivier</forenames></author></authors><title>Unit contradiction versus unit propagation</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some aspects of the result of applying unit resolution on a CNF formula can
be formalized as functions with domain a set of partial truth assignments. We
are interested in two ways for computing such functions, depending on whether
the result is the production of the empty clause or the assignment of a
variable with a given truth value. We show that these two models can compute
the same functions with formulae of polynomially related sizes, and we explain
how this result is related to the CNF encoding of Boolean constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0734</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0734</id><created>2012-04-03</created><authors><author><keyname>Laurent</keyname><forenames>Monique</forenames></author><author><keyname>Varvitsiotis</keyname><forenames>Antonios</forenames></author></authors><title>A new graph parameter related to bounded rank positive semidefinite
  matrix completions</title><categories>math.OC cs.DM math.CO</categories><comments>31 pages, 6 Figures. arXiv admin note: substantial text overlap with
  arXiv:1112.5960</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gram dimension $\gd(G)$ of a graph $G$ is the smallest integer $k\ge 1$
such that any partial real symmetric matrix, whose entries are specified on the
diagonal and at the off-diagonal positions corresponding to edges of $G$, can
be completed to a positive semidefinite matrix of rank at most $k$ (assuming a
positive semidefinite completion exists). For any fixed $k$ the class of graphs
satisfying $\gd(G) \le k$ is minor closed, hence it can characterized by a
finite list of forbidden minors. We show that the only minimal forbidden minor
is $K_{k+1}$ for $k\le 3$ and that there are two minimal forbidden minors:
$K_5$ and $K_{2,2,2}$ for $k=4$. We also show some close connections to
Euclidean realizations of graphs and to the graph parameter $\nu^=(G)$ of
\cite{H03}. In particular, our characterization of the graphs with $\gd(G)\le
4$ implies the forbidden minor characterization of the 3-realizable graphs of
Belk and Connelly \cite{Belk,BC} and of the graphs with $\nu^=(G) \le 4$ of van
der Holst \cite{H03}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0746</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0746</id><created>2012-04-03</created><authors><author><keyname>Hosseini</keyname><forenames>Seyed Hossein</forenames></author><author><keyname>Shayesteh</keyname><forenames>Mahrokh G.</forenames></author></authors><title>Gradually Atom Pruning for Sparse Reconstruction and Extension to
  Correlated Sparsity</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, to be included in 20th Iranian Conference on
  Electrical Engineering, IEEE, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for recovery of sparse signals from their
compressively sensed samples. The proposed algorithm benefits from the strategy
of gradual movement to estimate the positions of non-zero samples of sparse
signal. We decompose each sample of signal into two variables, namely &quot;value&quot;
and &quot;detector&quot;, by a weighted exponential function. We update these new
variables using gradient descent method. Like the traditional compressed
sensing algorithms, the first variable is used to solve the Least Absolute
Shrinkage and Selection Operator (Lasso) problem. As a new strategy, the second
variable participates in the regularization term of the Lasso (l1 norm) that
gradually detects the non-zero elements. The presence of the second variable
enables us to extend the corresponding vector of the first variable to matrix
form. This makes possible use of the correlation matrix for a heuristic search
in the case that there are correlations among the samples of signal. We compare
the performance of the new algorithm with various algorithms for uncorrelated
and correlated sparsity. The results indicate the efficiency of the proposed
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0747</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0747</id><created>2012-04-03</created><updated>2012-08-10</updated><authors><author><keyname>Hirani</keyname><forenames>Anil N.</forenames></author><author><keyname>Kalyanaraman</keyname><forenames>Kaushik</forenames></author><author><keyname>VanderZee</keyname><forenames>Evan B.</forenames></author></authors><title>Delaunay Hodge Star</title><categories>cs.CG math.NA</categories><comments>Added some references and clarified connection to Gabriel property.
  This version (without the appendix) will appear in SPM'2012: Symposium on
  Solid and Physical Modeling</comments><msc-class>65N30, 53-04</msc-class><acm-class>I.3.5; G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define signed dual volumes at all dimensions for circumcentric dual
meshes. We show that for pairwise Delaunay triangulations with mild boundary
assumptions these signed dual volumes are positive. This allows the use of such
Delaunay meshes for Discrete Exterior Calculus (DEC) because the discrete Hodge
star operator can now be correctly defined for such meshes. This operator is
crucial for DEC and is a diagonal matrix with the ratio of primal and dual
volumes along the diagonal. A correct definition requires that all entries be
positive. DEC is a framework for numerically solving differential equations on
meshes and for geometry processing tasks and has had considerable impact in
computer graphics and scientific computing. Our result allows the use of DEC
with a much larger class of meshes than was previously considered possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0752</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0752</id><created>2012-04-03</created><authors><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Pammolli</keyname><forenames>Fabio</forenames></author></authors><title>Persistence and Uncertainty in the Academic Career</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>29 pages total: 8 main manuscript + 4 figs, 21 SI text + figs</comments><journal-ref>Proceedings of the National Academy of Sciences USA 109, 5213 -
  5218 (2012)</journal-ref><doi>10.1073/pnas.1121429109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how institutional changes within academia may affect the
overall potential of science requires a better quantitative representation of
how careers evolve over time. Since knowledge spillovers, cumulative advantage,
competition, and collaboration are distinctive features of the academic
profession, both the employment relationship and the procedures for assigning
recognition and allocating funding should be designed to account for these
factors. We study the annual production n_{i}(t) of a given scientist i by
analyzing longitudinal career data for 200 leading scientists and 100 assistant
professors from the physics community. We compare our results with 21,156
sports careers. Our empirical analysis of individual productivity dynamics
shows that (i) there are increasing returns for the top individuals within the
competitive cohort, and that (ii) the distribution of production growth is a
leptokurtic &quot;tent-shaped&quot; distribution that is remarkably symmetric. Our
methodology is general, and we speculate that similar features appear in other
disciplines where academic publication is essential and collaboration is a key
feature. We introduce a model of proportional growth which reproduces these two
observations, and additionally accounts for the significantly right-skewed
distributions of career longevity and achievement in science. Using this
theoretical model, we show that short-term contracts can amplify the effects of
competition and uncertainty making careers more vulnerable to early
termination, not necessarily due to lack of individual talent and persistence,
but because of random negative production shocks. We show that fluctuations in
scientific production are quantitatively related to a scientist's collaboration
radius and team efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0764</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0764</id><created>2012-04-03</created><updated>2012-05-21</updated><authors><author><keyname>Bhadauria</keyname><forenames>Rohit</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Survey on Security Issues in Cloud Computing and Associated Mitigation
  Techniques</title><categories>cs.CR</categories><comments>20 pages, 2 Figures, 1 Table. arXiv admin note: substantial text
  overlap with arXiv:1109.5388</comments><doi>10.5120/7292-0578</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud Computing holds the potential to eliminate the requirements for setting
up of high-cost computing infrastructure for IT-based solutions and services
that the industry uses. It promises to provide a flexible IT architecture,
accessible through internet for lightweight portable devices. This would allow
multi-fold increase in the capacity or capabilities of the existing and new
software. In a cloud computing environment, the entire data reside over a set
of networked resources, enabling the data to be accessed through virtual
machines. Since these data-centers may lie in any corner of the world beyond
the reach and control of users, there are multifarious security and privacy
challenges that need to be understood and taken care of. Also, one can never
deny the possibility of a server breakdown that has been witnessed, rather
quite often in the recent times. There are various issues that need to be dealt
with respect to security and privacy in a cloud computing scenario. This
extensive survey paper aims to elaborate and analyze the numerous unresolved
issues threatening the cloud computing adoption and diffusion affecting the
various stake-holders linked to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0767</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0767</id><created>2012-04-03</created><updated>2012-04-04</updated><authors><author><keyname>Katyal</keyname><forenames>Vini</forenames></author><author><keyname>Srivastava</keyname><forenames>Deepesh</forenames></author></authors><title>Efficient Fruit Defect Detection and Glare removal Algorithm by
  anisotropic diffusion and 2D Gabor filter</title><categories>cs.CV</categories><comments>Errors in material</comments><journal-ref>International Journal of Engineering Science &amp; Advanced Technology
  (IJESAT), Volume-2, Issue-2, 352 - 357 Mar-Apr 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on fruit defect detection and glare removal using
morphological operations, Glare removal can be considered as an important
preprocessing step as uneven lighting may introduce it in images, which hamper
the results produced through segmentation by Gabor filters .The problem of
glare in images is very pronounced sometimes due to the unusual reflectance
from the camera sensor or stray light entering, this method counteracts this
problem and makes the defect detection much more pronounced. Anisotropic
diffusion is used for further smoothening of the images and removing the high
energy regions in an image for better defect detection and makes the defects
more retrievable. Our algorithm is robust and scalable the employability of a
particular mask for glare removal has been checked and proved useful for
counteracting.this problem, anisotropic diffusion further enhances the defects
with its use further Optimal Gabor filter at various orientations is used for
defect detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0775</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0775</id><created>2012-04-03</created><updated>2012-09-11</updated><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames></author></authors><title>On the Complexity of Finding Narrow Proofs</title><categories>cs.LO cs.CC</categories><comments>Full version of the FOCS 2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the following &quot;resolution width problem&quot;: Does a
given 3-CNF have a resolution refutation of width k? We prove that the problem
cannot be decided in time O(n^((k-3)/12)). This lower bound is unconditional
and does not rely on any unproven complexity theoretic assumptions. The lower
bound is matched by a trivial upper bound of n^O(k).
  We also prove that the resolution width problem is EXPTIME-complete (if k is
part of the input). This confirms a conjecture by Vardi, who has first raised
the question for the complexity of the resolution width problem. Furthermore,
we prove that the variant of the resolution width problem for regular
resolution is PSPACE-complete, confirming a conjecture by Urquhart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0776</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0776</id><created>2012-04-03</created><authors><author><keyname>Wang</keyname><forenames>Shanshan</forenames></author><author><keyname>Murugesan</keyname><forenames>Sugumar</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author></authors><title>Exploiting Channel Correlation and PU Traffic Memory for Opportunistic
  Spectrum Scheduling</title><categories>cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cognitive radio network with multiple primary users (PUs) and
one secondary user (SU), where a spectrum server is utilized for spectrum
sensing and scheduling the SU to transmit over one of the PU channels
opportunistically. One practical yet challenging scenario is when \textit{both}
the PU occupancy and the channel fading vary over time and exhibit temporal
correlations. Little work has been done for exploiting such temporal memory in
the channel fading and the PU occupancy simultaneously for opportunistic
spectrum scheduling. A main goal of this work is to understand the intricate
tradeoffs resulting from the interactions of the two sets of system states -
the channel fading and the PU occupancy, by casting the problem as a partially
observable Markov decision process. We first show that a simple greedy policy
is optimal in some special cases. To build a clear understanding of the
tradeoffs, we then introduce a full-observation genie-aided system, where the
spectrum server collects channel fading states from all PU channels. The
genie-aided system is used to decompose the tradeoffs in the original system
into multiple tiers, which are examined progressively. Numerical examples
indicate that the optimal scheduler in the original system, with observation on
the scheduled channel only, achieves a performance very close to the
genie-aided system. Further, as expected, the optimal policy in the original
system significantly outperforms randomized scheduling, pointing to the merit
of exploiting the temporal correlation structure in both channel fading and PU
occupancy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0803</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0803</id><created>2012-04-03</created><authors><author><keyname>Hosseini</keyname><forenames>Seyed Hossein</forenames></author><author><keyname>Shayesteh</keyname><forenames>Mahrokh G.</forenames></author></authors><title>Compressed Sensing for Denoising in Adaptive System Identification</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, to be published in proceedings of 20th Iranian
  Conference on Electrical Engineering, IEEE, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new technique for adaptive identification of sparse systems
based on the compressed sensing (CS) theory. We manipulate the transmitted
pilot (input signal) and the received signal such that the weights of adaptive
filter approach the compressed version of the sparse system instead of the
original system. To this end, we use random filter structure at the transmitter
to form the measurement matrix according to the CS framework. The original
sparse system can be reconstructed by the conventional recovery algorithms. As
a result, the denoising property of CS can be deployed in the proposed method
at the recovery stage. The experiments indicate significant performance
improvement of proposed method compared to the conventional LMS method which
directly identifies the sparse system. Furthermore, at low levels of sparsity,
our method outperforms a specialized identification algorithm that promotes
sparsity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0816</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0816</id><created>2012-04-03</created><authors><author><keyname>Kintali</keyname><forenames>Shiva</forenames></author><author><keyname>Shapira</keyname><forenames>Asaf</forenames></author></authors><title>A Note on the Balanced ST-Connectivity</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every YES instance of Balanced ST-Connectivity has a balanced
path of polynomial length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0824</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0824</id><created>2012-04-03</created><authors><author><keyname>Clarkson</keyname><forenames>Kenneth L.</forenames></author><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Self-improving Algorithms for Coordinate-wise Maxima</title><categories>cs.CG cs.DS</categories><comments>To appear in Symposium of Computational Geometry 2012 (17 pages, 2
  figures)</comments><journal-ref>SIAM Journal on Computing (SICOMP), 43(2), 2014, pp. 617-653</journal-ref><doi>10.1137/12089702X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the coordinate-wise maxima of a planar point set is a classic and
well-studied problem in computational geometry. We give an algorithm for this
problem in the \emph{self-improving setting}. We have $n$ (unknown) independent
distributions $\cD_1, \cD_2, ..., \cD_n$ of planar points. An input pointset
$(p_1, p_2, ..., p_n)$ is generated by taking an independent sample $p_i$ from
each $\cD_i$, so the input distribution $\cD$ is the product $\prod_i \cD_i$. A
self-improving algorithm repeatedly gets input sets from the distribution $\cD$
(which is \emph{a priori} unknown) and tries to optimize its running time for
$\cD$. Our algorithm uses the first few inputs to learn salient features of the
distribution, and then becomes an optimal algorithm for distribution $\cD$. Let
$\OPT_\cD$ denote the expected depth of an \emph{optimal} linear comparison
tree computing the maxima for distribution $\cD$. Our algorithm eventually has
an expected running time of $O(\text{OPT}_\cD + n)$, even though it did not
know $\cD$ to begin with.
  Our result requires new tools to understand linear comparison trees for
computing maxima. We show how to convert general linear comparison trees to
very restricted versions, which can then be related to the running time of our
algorithm. An interesting feature of our algorithm is an interleaved search,
where the algorithm tries to determine the likeliest point to be maximal with
minimal computation. This allows the running time to be truly optimal for the
distribution $\cD$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0830</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0830</id><created>2012-04-03</created><updated>2014-10-07</updated><authors><author><keyname>Yousefi</keyname><forenames>Mansoor I.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Information Transmission using the Nonlinear Fourier Transform, Part II:
  Numerical Methods</title><categories>cs.IT math.IT</categories><comments>Minor updates to IEEE Transactions on Information Theory, vol. 60,
  no. 7, pp. 4329--4345, July 2014</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 7, pp.
  4329--4345, July 2014</journal-ref><doi>10.1109/TIT.2014.2321151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, numerical methods are suggested to compute the discrete and
the continuous spectrum of a signal with respect to the Zakharov-Shabat system,
a Lax operator underlying numerous integrable communication channels including
the nonlinear Schr\&quot;odinger channel, modeling pulse propagation in optical
fibers. These methods are subsequently tested and their ability to estimate the
spectrum are compared against each other. These methods are used to compute the
spectrum of various signals commonly used in the optical fiber communications.
It is found that the layer-peeling and the spectral methods are suitable
schemes to estimate the nonlinear spectra with good accuracy. To illustrate the
structure of the spectrum, the locus of the eigenvalues is determined under
amplitude and phase modulation in a number of examples. It is observed that in
some cases, as signal parameters vary, eigenvalues collide and change their
course of motion. The real axis is typically the place from which new
eigenvalues originate or are absorbed into after traveling a trajectory in the
complex plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0833</identifier>
 <datestamp>2014-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0833</id><created>2012-04-03</created><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>Bounded Counter Languages</title><categories>cs.FL</categories><doi>10.1007/978-3-642-31623-4_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that deterministic finite automata equipped with $k$ two-way heads
are equivalent to deterministic machines with a single two-way input head and
$k-1$ linearly bounded counters if the accepted language is strictly bounded,
i.e., a subset of $a_1^*a_2^*... a_m^*$ for a fixed sequence of symbols $a_1,
a_2,..., a_m$. Then we investigate linear speed-up for counter machines. Lower
and upper time bounds for concrete recognition problems are shown, implying
that in general linear speed-up does not hold for counter machines. For bounded
languages we develop a technique for speeding up computations by any constant
factor at the expense of adding a fixed number of counters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0839</identifier>
 <datestamp>2012-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0839</id><created>2012-04-03</created><updated>2012-12-09</updated><authors><author><keyname>Harms</keyname><forenames>Andrew</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>A Constrained Random Demodulator for Sub-Nyquist Sampling</title><categories>cs.IT math.IT</categories><comments>Preprint of a journal paper accepted for publication in: IEEE Trans.
  Signal Processing, 2013 (copyright has been transferred to IEEE)</comments><doi>10.1109/TSP.2012.2231077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a significant modification to the Random Demodulator (RD)
of Tropp et al. for sub-Nyquist sampling of frequency-sparse signals. The
modification, termed constrained random demodulator, involves replacing the
random waveform, essential to the operation of the RD, with a constrained
random waveform that has limits on its switching rate because fast switching
waveforms may be hard to generate cleanly. The result is a relaxation on the
hardware requirements with a slight, but manageable, decrease in the recovery
guarantees. The paper also establishes the importance of properly choosing the
statistics of the constrained random waveform. If the power spectrum of the
random waveform matches the distribution on the tones of the input signal
(i.e., the distribution is proportional to the power spectrum), then recovery
of the input signal tones is improved. The theoretical guarantees provided in
the paper are validated through extensive numerical simulations and phase
transition plots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0844</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0844</id><created>2012-04-03</created><authors><author><keyname>Ghosh</keyname><forenames>Abhishek</forenames></author><author><keyname>Pamarti</keyname><forenames>Sudhakar</forenames></author></authors><title>Mitigating Timing Errors in Time-Interleaved ADCs: a signal conditioning
  approach</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, submitted to Mid-West Symposium on Circuits and
  Systems, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel techniques based on signal-conditioning are presented to mitigate
timing errors in time-interleaved ADCs. A theoretical bound on the achievable
spurious signal content, on applying the techniques, is also derived.
Behavioral simulations corroborating the same are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0849</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0849</id><created>2012-04-03</created><updated>2014-04-02</updated><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Optimal bounds for monotonicity and Lipschitz testing over hypercubes
  and hypergrids</title><categories>cs.DM cs.CC cs.DS</categories><comments>Cleaner proof and much better presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of monotonicity testing over the hypergrid and its special case,
the hypercube, is a classic, well-studied, yet unsolved question in property
testing. We are given query access to $f:[k]^n \mapsto \R$ (for some ordered
range $\R$). The hypergrid/cube has a natural partial order given by
coordinate-wise ordering, denoted by $\prec$. A function is \emph{monotone} if
for all pairs $x \prec y$, $f(x) \leq f(y)$. The distance to monotonicity,
$\eps_f$, is the minimum fraction of values of $f$ that need to be changed to
make $f$ monotone.
  For $k=2$ (the boolean hypercube), the usual tester is the \emph{edge
tester}, which checks monotonicity on adjacent pairs of domain points. It is
known that the edge tester using $O(\eps^{-1}n\log|\R|)$ samples can
distinguish a monotone function from one where $\eps_f &gt; \eps$. On the other
hand, the best lower bound for monotonicity testing over the hypercube is
$\min(|\R|^2,n)$. This leaves a quadratic gap in our knowledge, since $|\R|$
can be $2^n$. We resolve this long standing open problem and prove that
$O(n/\eps)$ samples suffice for the edge tester. For hypergrids, known testers
require $O(\eps^{-1}n\log k\log |\R|)$ samples, while the best known
(non-adaptive) lower bound is $\Omega(\eps^{-1} n\log k)$. We give a
(non-adaptive) monotonicity tester for hypergrids running in $O(\eps^{-1} n\log
k)$ time.
  Our techniques lead to optimal property testers (with the same running time)
for the natural \emph{Lipschitz property} on hypercubes and hypergrids. (A
$c$-Lipschitz function is one where $|f(x) - f(y)| \leq c\|x-y\|_1$.) In fact,
we give a general unified proof for $O(\eps^{-1}n\log k)$-query testers for a
class of &quot;bounded-derivative&quot; properties, a class containing both monotonicity
and Lipschitz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0852</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0852</id><created>2012-04-03</created><updated>2012-12-21</updated><authors><author><keyname>Gharesifard</keyname><forenames>Bahman</forenames></author><author><keyname>Cortes</keyname><forenames>Jorge</forenames></author></authors><title>Distributed convergence to Nash equilibria in two-network zero-sum games</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a class of strategic scenarios in which two networks of
agents have opposing objectives with regards to the optimization of a common
objective function. In the resulting zero-sum game, individual agents
collaborate with neighbors in their respective network and have only partial
knowledge of the state of the agents in the other network. For the case when
the interaction topology of each network is undirected, we synthesize a
distributed saddle-point strategy and establish its convergence to the Nash
equilibrium for the class of strictly concave-convex and locally Lipschitz
objective functions. We also show that this dynamics does not converge in
general if the topologies are directed. This justifies the introduction, in the
directed case, of a generalization of this distributed dynamics which we show
converges to the Nash equilibrium for the class of strictly concave-convex
differentiable functions with locally Lipschitz gradients. The technical
approach combines tools from algebraic graph theory, nonsmooth analysis,
set-valued dynamical systems, and game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0856</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0856</id><created>2012-04-03</created><updated>2012-04-10</updated><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail</forenames></author></authors><title>The Cerny Conjecture</title><categories>cs.FL</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the proof of Corollary 2</comments><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \v{C}ern\'y conjecture (\v{C}ern\'y, 1964) states that each n-state \san\
possess a \sw\ of length $(n-1)^2$. From the other side the best upper bound
for the \rl\ of n-state \sa\ known so far is equal to $\frac{n^3-n}6$ (Pin,
1983) and so is cubic (a slightly better though still cubic upper bound
$\frac{n(7n^2+6n-16)}{48}$ has been claimed in Trahtman but the published proof
of this result contains an unclear place) in $n$. In the paper the \v{C}ern\'y
conjecture is reduced to a simpler conjecture. In particular, we prove
\v{C}ern\'y conjecture for one-cluster automata and quadratic upper bounds for
automata closed to one-cluster automata. Our approach utilize theory of Markov
chains and one simple fact from linear programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0864</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0864</id><created>2012-04-04</created><authors><author><keyname>Hai</keyname><forenames>Phan Nhat</forenames></author><author><keyname>Poncelet</keyname><forenames>Pascal</forenames></author><author><keyname>Teisseire</keyname><forenames>Maguelonne</forenames></author></authors><title>GeT_Move: An Efficient and Unifying Spatio-Temporal Pattern Mining
  Algorithm for Moving Objects</title><categories>cs.DB</categories><comments>17 pages, 24 figures, submitted to KDD, TKDD</comments><acm-class>H.2.8</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Recent improvements in positioning technology has led to a much wider
availability of massive moving object data. A crucial task is to find the
moving objects that travel together. Usually, these object sets are called
spatio-temporal patterns. Due to the emergence of many different kinds of
spatio-temporal patterns in recent years, different approaches have been
proposed to extract them. However, each approach only focuses on mining a
specific kind of pattern. In addition to being a painstaking task due to the
large number of algorithms used to mine and manage patterns, it is also time
consuming. Moreover, we have to execute these algorithms again whenever new
data are added to the existing database. To address these issues, we ?first
redefine spatio-temporal patterns in the itemset context. Secondly, we propose
a unifying approach, named GeT_Move, which uses a frequent closed itemset-based
spatio-temporal pattern-mining algorithm to mine and manage different
spatio-temporal patterns. GeT_Move is implemented in two versions which are
GeT_Move and Incremental GeT_Move. To optimize the efficiency and to free the
parameters setting, we also propose a Parameter Free Incremental GeT_Move
algorithm. Comprehensive experiments are performed on real datasets as well as
large synthetic datasets to demonstrate the effectiveness and efficiency of our
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0867</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0867</id><created>2012-04-04</created><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Ho</keyname><forenames>Chin Keong</forenames></author></authors><title>Optimal Index Codes for a Class of Multicast Networks with Receiver Side
  Information</title><categories>cs.IT math.IT</categories><comments>Author's final version (to be presented at ICC 2012)</comments><journal-ref>Proceedings of the 2012 IEEE International Conference on
  Communications (ICC 2012), Ottawa, Canada, pp. 2213-2218, June 10-15, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a special class of multicast index coding problems where a
sender transmits messages to multiple receivers, each with some side
information. Here, each receiver knows a unique message a priori, and there is
no restriction on how many messages each receiver requests from the sender. For
this class of multicast index coding problems, we obtain the optimal index
code, which has the shortest codelength for which the sender needs to send in
order for all receivers to obtain their (respective) requested messages. This
is the first class of index coding problems where the optimal index codes are
found. In addition, linear index codes are shown to be optimal for this class
of index coding problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0870</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0870</id><created>2012-04-04</created><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Relax and Localize: From Value to Algorithms</title><categories>cs.LG cs.GT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a principled way of deriving online learning algorithms from a
minimax analysis. Various upper bounds on the minimax value, previously thought
to be non-constructive, are shown to yield algorithms. This allows us to
seamlessly recover known methods and to derive new ones. Our framework also
captures such &quot;unorthodox&quot; methods as Follow the Perturbed Leader and the R^2
forecaster. We emphasize that understanding the inherent complexity of the
learning problem leads to the development of algorithms.
  We define local sequential Rademacher complexities and associated algorithms
that allow us to obtain faster rates in online learning, similarly to
statistical learning theory. Based on these localized complexities we build a
general adaptive method that can take advantage of the suboptimality of the
observed sequence.
  We present a number of new algorithms, including a family of randomized
methods that use the idea of a &quot;random playout&quot;. Several new versions of the
Follow-the-Perturbed-Leader algorithms are presented, as well as methods based
on the Littlestone's dimension, efficient methods for matrix completion with
trace norm, and algorithms for the problems of transductive learning and
prediction with static experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0885</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0885</id><created>2012-04-04</created><authors><author><keyname>Mirzal</keyname><forenames>Andri</forenames></author><author><keyname>Yoshii</keyname><forenames>Shinichiro</forenames></author><author><keyname>Furukawa</keyname><forenames>Masashi</forenames></author></authors><title>PID Parameters Optimization by Using Genetic Algorithm</title><categories>cs.SY cs.LG cs.NE</categories><comments>12 pages, 4 figures</comments><journal-ref>ISTECS Journal, Vol. 8, pp. 34-43, 2006</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Time delays are components that make time-lag in systems response. They arise
in physical, chemical, biological and economic systems, as well as in the
process of measurement and computation. In this work, we implement Genetic
Algorithm (GA) in determining PID controller parameters to compensate the delay
in First Order Lag plus Time Delay (FOLPD) and compare the results with
Iterative Method and Ziegler-Nichols rule results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0897</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0897</id><created>2012-04-04</created><updated>2012-10-31</updated><authors><author><keyname>G&#xfc;nther</keyname><forenames>Elisabeth</forenames></author><author><keyname>Maurer</keyname><forenames>Olaf</forenames></author><author><keyname>Megow</keyname><forenames>Nicole</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>A New Approach to Online Scheduling: Approximating the Optimal
  Competitive Ratio</title><categories>cs.DS</categories><comments>24 pages; short version to appear in SODA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach to competitive analysis in online scheduling by
introducing the novel concept of competitive-ratio approximation schemes. Such
a scheme algorithmically constructs an online algorithm with a competitive
ratio arbitrarily close to the best possible competitive ratio for any online
algorithm. We study the problem of scheduling jobs online to minimize the
weighted sum of completion times on parallel, related, and unrelated machines,
and we derive both deterministic and randomized algorithms which are almost
best possible among all online algorithms of the respective settings. We also
generalize our techniques to arbitrary monomial cost functions and apply them
to the makespan objective. Our method relies on an abstract characterization of
online algorithms combined with various simplifications and transformations. We
also contribute algorithmic means to compute the actual value of the best
possi- ble competitive ratio up to an arbitrary accuracy. This strongly
contrasts all previous manually obtained competitiveness results for algorithms
and, most importantly, it reduces the search for the optimal com- petitive
ratio to a question that a computer can answer. We believe that our concept can
also be applied to many other problems and yields a new perspective on online
algorithms in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0901</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0901</id><created>2012-04-04</created><updated>2012-04-13</updated><authors><author><keyname>Alama</keyname><forenames>Jesse</forenames></author></authors><title>Tipi: A TPTP-based theory development environment emphasizing proof
  analysis</title><categories>math.LO cs.LO</categories><comments>10 pages, 3 tables. Submitted to ATX 2012 (Automated Theory
  Exploration, http://dream.inf.ed.ac.uk/events/atx2012/)</comments><msc-class>68T15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some theory development tasks, a problem is satisfactorily solved once it
is shown that a theorem (conjecture) is derivable from the background theory
(premises). Depending on one's motivations, the details of the derivation of
the conjecture from the premises may or may not be important. In some contexts,
though, one wants more from theory development than simply derivability of the
target theorems from the background theory. One may want to know which premises
of the background theory were used in the course of a proof output by an
automated theorem prover (when a proof is available), whether they are all, in
suitable senses, necessary (and why), whether alternative proofs can be found,
and so forth. The problem, then, is to support proof analysis in theory
development; the tool described in this paper, Tipi, aims to provide precisely
that.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0905</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0905</id><created>2012-04-04</created><authors><author><keyname>Cheng</keyname><forenames>Jin-San</forenames></author><author><keyname>Jin</keyname><forenames>Kai</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author><author><keyname>Lazard</keyname><forenames>Daniel</forenames></author></authors><title>Certified Rational Parametric Approximation of Real Algebraic Space
  Curves with Local Generic Position Method</title><categories>cs.CG</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an algorithm to compute a certified $G^1$ rational parametric
approximation for algebraic space curves is given by extending the local
generic position method for solving zero dimensional polynomial equation
systems to the case of dimension one. By certified, we mean the approximation
curve and the original curve have the same topology and their Hausdauff
distance is smaller than a given precision. Thus, the method also gives a new
algorithm to compute the topology for space algebraic curves. The main
advantage of the algorithm, inhering from the local generic method, is that
topology computation and approximation for a space curve is directly reduced to
the same tasks for two plane curves. In particular, the error bound of the
approximation space curve is obtained from the error bounds of the
approximation plane curves explicitly. Nontrivial examples are used to show the
effectivity of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0908</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0908</id><created>2012-04-04</created><authors><author><keyname>Adsul</keyname><forenames>Bharat</forenames></author><author><keyname>Machchhar</keyname><forenames>Jinesh</forenames></author><author><keyname>Sohoni</keyname><forenames>Milind</forenames></author></authors><title>A procedural framework and mathematical analysis for solid sweeps</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sweeping is a powerful and versatile method of designing objects. Boundary of
volumes (henceforth envelope) obtained by sweeping solids have been extensively
investigated in the past, though, obtaining an accurate parametrization of the
envelope remained computationally hard. The present work reports our approach
to this problem as well as the important problem of identifying
self-intersections within the envelope. Parametrization of the envelope is, of
course, necessary for its use in most current CAD systems. We take the more
interesting case when the solid is composed of several faces meeting smoothly.
We show that the face structure of the envelope mimics locally that of the
solid. We adopt the procedural approach at defining the geometry in this work
which has the advantage of being accurate as well as computationally efficient.
The problem of detecting local self-intersections is central to a robust
implementation of the solid sweep. This has been addressed by computing a
subtle mathematical invariant which detects self-intersections, and which is
computationally benign and requires only point queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0939</identifier>
 <datestamp>2012-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0939</id><created>2012-04-04</created><authors><author><keyname>Aupy</keyname><forenames>Guillaume</forenames></author><author><keyname>Benoit</keyname><forenames>Anne</forenames></author><author><keyname>Dufoss&#xe9;</keyname><forenames>Fanny</forenames></author><author><keyname>Robert</keyname><forenames>Yves</forenames></author></authors><title>Reclaiming the energy of a schedule: models and algorithms</title><categories>cs.DC cs.CC</categories><comments>A two-page extended abstract of this work appeared as a short
  presentation in SPAA'2011, while the long version has been accepted for
  publication in &quot;Concurrency and Computation: Practice and Experience&quot;</comments><report-no>INRIA Research report 7598</report-no><doi>10.1002/cpe.2889</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a task graph to be executed on a set of processors. We assume
that the mapping is given, say by an ordered list of tasks to execute on each
processor, and we aim at optimizing the energy consumption while enforcing a
prescribed bound on the execution time. While it is not possible to change the
allocation of a task, it is possible to change its speed. Rather than using a
local approach such as backfilling, we consider the problem as a whole and
study the impact of several speed variation models on its complexity. For
continuous speeds, we give a closed-form formula for trees and series-parallel
graphs, and we cast the problem into a geometric programming problem for
general directed acyclic graphs. We show that the classical dynamic voltage and
frequency scaling (DVFS) model with discrete modes leads to a NP-complete
problem, even if the modes are regularly distributed (an important particular
case in practice, which we analyze as the incremental model). On the contrary,
the VDD-hopping model leads to a polynomial solution. Finally, we provide an
approximation algorithm for the incremental model, which we extend for the
general DVFS model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0944</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0944</id><created>2012-04-04</created><updated>2013-11-12</updated><authors><author><keyname>Gur</keyname><forenames>Tom</forenames></author><author><keyname>Tamuz</keyname><forenames>Omer</forenames></author></authors><title>Testing Booleanity and the Uncertainty Principle</title><categories>cs.DM cs.CC cs.DS</categories><comments>15 pages</comments><journal-ref>Chicago Journal of Theoretical Computer Science 2013, Article 14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let f:{-1,1}^n -&gt; R be a real function on the hypercube, given by its
discrete Fourier expansion, or, equivalently, represented as a multilinear
polynomial. We say that it is Boolean if its image is in {-1,1}.
  We show that every function on the hypercube with a sparse Fourier expansion
must either be Boolean or far from Boolean. In particular, we show that a
multilinear polynomial with at most k terms must either be Boolean, or output
values different than -1 or 1 for a fraction of at least 2/(k+2)^2 of its
domain.
  It follows that given oracle access to f, together with the guarantee that
its representation as a multilinear polynomial has at most k terms, one can
test Booleanity using O(k^2) queries. We show an \Omega(k) queries lower bound
for this problem.
  Our proof crucially uses Hirschman's entropic version of Heisenberg's
uncertainty principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0949</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0949</id><created>2012-04-04</created><authors><author><keyname>Guillon</keyname><forenames>Pierre</forenames></author><author><keyname>Zinoviadis</keyname><forenames>Charalampos</forenames></author></authors><title>Densities and entropies in cellular automata</title><categories>cs.CC math.DS</categories><comments>10 pages + 8 pages appendix, cited in bib; published in CiE 2012</comments><msc-class>37B15, 37B40, 37B50</msc-class><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following work by Hochman and Meyerovitch on multidimensional SFT, we give
computability-theoretic characterizations of the real numbers that can appear
as the topological entropies of one-dimensional and two-dimensional cellular
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0957</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0957</id><created>2012-04-04</created><updated>2014-05-16</updated><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Approximation Limits of Linear Programs (Beyond Hierarchies)</title><categories>cs.CC math.CO</categories><comments>23 pages, 2 figures</comments><msc-class>90C05, 68W25, 90C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework for approximation limits of polynomial-size linear
programs from lower bounds on the nonnegative ranks of suitably defined
matrices. This framework yields unconditional impossibility results that are
applicable to any linear program as opposed to only programs generated by
hierarchies. Using our framework, we prove that O(n^{1/2-eps})-approximations
for CLIQUE require linear programs of size 2^{n^\Omega(eps)}. (This lower bound
applies to linear programs using a certain encoding of CLIQUE as a linear
optimization problem.) Moreover, we establish a similar result for
approximations of semidefinite programs by linear programs. Our main ingredient
is a quantitative improvement of Razborov's rectangle corruption lemma for the
high error regime, which gives strong lower bounds on the nonnegative rank of
certain perturbations of the unique disjointness matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0958</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0958</id><created>2012-04-04</created><authors><author><keyname>Decreusefond</keyname><forenames>Laurent</forenames><affiliation>LTCI</affiliation></author><author><keyname>Ferraz</keyname><forenames>Eduardo</forenames><affiliation>LTCI</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>LTCI</affiliation></author><author><keyname>Vu</keyname><forenames>Thanh-Tung</forenames><affiliation>LTCI</affiliation></author></authors><title>Robust methods for LTE and WiMAX dimensioning</title><categories>cs.RO cs.NI cs.PF</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an analytic model for dimensioning OFDMA based networks
like WiMAX and LTE systems. In such a system, users require a number of
subchannels which depends on their \SNR, hence of their position and the
shadowing they experience. The system is overloaded when the number of required
subchannels is greater than the number of available subchannels. We give an
exact though not closed expression of the loss probability and then give an
algorithmic method to derive the number of subchannels which guarantees a loss
probability less than a given threshold. We show that Gaussian approximation
lead to optimistic values and are thus unusable. We then introduce Edgeworth
expansions with error bounds and show that by choosing the right order of the
expansion, one can have an approximate dimensioning value easy to compute but
with guaranteed performance. As the values obtained are highly dependent from
the parameters of the system, which turned to be rather undetermined, we
provide a procedure based on concentration inequality for Poisson functionals,
which yields to conservative dimensioning. This paper relies on recent results
on concentration inequalities and establish new results on Edgeworth
expansions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0982</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0982</id><created>2012-04-04</created><updated>2012-04-05</updated><authors><author><keyname>Gast</keyname><forenames>Mikael</forenames></author><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author></authors><title>Approximability of the Vertex Cover Problem in Power Law Graphs</title><categories>cs.DS cs.SI</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we construct an approximation algorithm for the Minimum Vertex
Cover Problem (Min-VC) with an expected approximation ratio of 2-f(beta) for
random Power Law Graphs (PLG) in the (alpha,beta)-model of Aiello et. al.,
where f(beta) is a strictly positive function of the parameter beta. We obtain
this result by combining the Nemhauser and Trotter approach for Min-VC with a
new deterministic rounding procedure which achieves an approximation ratio of
3/2 on a subset of low degree vertices for which the expected contribution to
the cost of the associated linear program is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0986</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0986</id><created>2012-04-04</created><authors><author><keyname>Singh</keyname><forenames>Shweta</forenames></author><author><keyname>Bhatt</keyname><forenames>Ravindara</forenames></author></authors><title>Adjacency Matrix Based Energy Efficient Scheduling using S-MAC Protocol
  in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>20 pages, 2 figures, 14 tables, 5 equations, International Journal of
  Computer Networks &amp; Communications (IJCNC),March 2012, Volume 4, No. 2, March
  2012</comments><msc-class>68Wxx</msc-class><acm-class>D.4.0; H.4.1</acm-class><doi>10.5121/ijcnc.2012.4210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication is the main motive in any Networks whether it is Wireless
Sensor Network, Ad-Hoc networks, Mobile Networks, Wired Networks, Local Area
Network, Metropolitan Area Network, Wireless Area Network etc, hence it must be
energy efficient. The main parameters for energy efficient communication are
maximizing network lifetime, saving energy at the different nodes, sending the
packets in minimum time delay, higher throughput etc. This paper focuses mainly
on the energy efficient communication with the help of Adjacency Matrix in the
Wireless Sensor Networks. The energy efficient scheduling can be done by
putting the idle node in to sleep node so energy at the idle node can be saved.
The proposed model in this paper first forms the adjacency matrix and
broadcasts the information about the total number of existing nodes with depths
to the other nodes in the same cluster from controller node. When every node
receives the node information about the other nodes for same cluster they
communicate based on the shortest depths and schedules the idle node in to
sleep mode for a specific time threshold so energy at the idle nodes can be
saved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0987</identifier>
 <datestamp>2012-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0987</id><created>2012-04-04</created><authors><author><keyname>Plaga</keyname><forenames>Rainer</forenames></author><author><keyname>Koob</keyname><forenames>Frank</forenames></author></authors><title>A formal definition and a new security mechanism of physical unclonable
  functions</title><categories>cs.CR</categories><comments>13 pages, 1 figure, Conference Proceedings MMB &amp; DFT 2012,
  Kaiserslautern, Germany</comments><journal-ref>Lecture Notes in Computer Science, 2012, Volume 7201/2012, 288-301</journal-ref><doi>10.1007/978-3-642-28540-0_24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The characteristic novelty of what is generally meant by a &quot;physical
unclonable function&quot; (PUF) is precisely defined, in order to supply a firm
basis for security evaluations and the proposal of new security mechanisms. A
PUF is defined as a hardware device which implements a physical function with
an output value that changes with its argument. A PUF can be clonable, but a
secure PUF must be unclonable. This proposed meaning of a PUF is cleanly
delineated from the closely related concepts of &quot;conventional unclonable
function&quot;, &quot;physically obfuscated key&quot;, &quot;random-number generator&quot;, &quot;controlled
PUF&quot; and &quot;strong PUF&quot;. The structure of a systematic security evaluation of a
PUF enabled by the proposed formal definition is outlined. Practically all
current and novel physical (but not conventional) unclonable physical functions
are PUFs by our definition. Thereby the proposed definition captures the
existing intuition about what is a PUF and remains flexible enough to encompass
further research. In a second part we quantitatively characterize two classes
of PUF security mechanisms, the standard one, based on a minimum secret
read-out time, and a novel one, based on challenge-dependent erasure of stored
information. The new mechanism is shown to allow in principle the construction
of a &quot;quantum-PUF&quot;, that is absolutely secure while not requiring the storage
of an exponentially large secret. The construction of a PUF that is
mathematically and physically unclonable in principle does not contradict the
laws of physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.0992</identifier>
 <datestamp>2012-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.0992</id><created>2012-04-04</created><authors><author><keyname>Osgood</keyname><forenames>Brad</forenames></author><author><keyname>Siripuram</keyname><forenames>Aditya</forenames></author><author><keyname>Wu</keyname><forenames>William</forenames></author></authors><title>Discrete Sampling and Interpolation: Universal Sampling Sets for
  Discrete Bandlimited Spaces</title><categories>cs.IT math.IT</categories><comments>24 pages, 5 figures, Accepted for publication in IEEE Transactions on
  Information Theory</comments><doi>10.1109/TIT.2012.2193871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of interpolating all values of a discrete signal f of
length N when d&lt;N values are known, especially in the case when the Fourier
transform of the signal is zero outside some prescribed index set J; these
comprise the (generalized) bandlimited spaces B^J. The sampling pattern for f
is specified by an index set I, and is said to be a universal sampling set if
samples in the locations I can be used to interpolate signals from B^J for any
J. When N is a prime power we give several characterizations of universal
sampling sets, some structure theorems for such sets, an algorithm for their
construction, and a formula that counts them. There are also natural
applications to additive uncertainty principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1002</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1002</id><created>2012-04-04</created><updated>2012-06-06</updated><authors><author><keyname>Martelot</keyname><forenames>Erwan Le</forenames></author><author><keyname>Hankin</keyname><forenames>Chris</forenames></author></authors><title>Fast Multi-Scale Detection of Relevant Communities</title><categories>cs.DS cs.SI physics.soc-ph</categories><comments>19 pages, 3 figures, 1 table, 4 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, networks are almost ubiquitous. In the past decade, community
detection received an increasing interest as a way to uncover the structure of
networks by grouping nodes into communities more densely connected internally
than externally. Yet most of the effective methods available do not consider
the potential levels of organisation, or scales, a network may encompass and
are therefore limited. In this paper we present a method compatible with global
and local criteria that enables fast multi-scale community detection. The
method is derived in two algorithms, one for each type of criterion, and
implemented with 6 known criteria. Uncovering communities at various scales is
a computationally expensive task. Therefore this work puts a strong emphasis on
the reduction of computational complexity. Some heuristics are introduced for
speed-up purposes. Experiments demonstrate the efficiency and accuracy of our
method with respect to each algorithm and criterion by testing them against
large generated multi-scale networks. This study also offers a comparison
between criteria and between the global and local approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1025</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1025</id><created>2012-04-04</created><updated>2013-01-29</updated><authors><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author><author><keyname>Post</keyname><forenames>Ian</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>Online submodular welfare maximization: Greedy is optimal</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that no online algorithm (even randomized, against an oblivious
adversary) is better than 1/2-competitive for welfare maximization with
coverage valuations, unless $NP = RP$. Since the Greedy algorithm is known to
be 1/2-competitive for monotone submodular valuations, of which coverage is a
special case, this proves that Greedy provides the optimal competitive ratio.
On the other hand, we prove that Greedy in a stochastic setting with
i.i.d.items and valuations satisfying diminishing returns is
$(1-1/e)$-competitive, which is optimal even for coverage valuations, unless
$NP=RP$. For online budget-additive allocation, we prove that no algorithm can
be 0.612-competitive with respect to a natural LP which has been used
previously for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1069</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1069</id><created>2012-04-04</created><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author></authors><title>Convergence and Equivalence results for the Jensen's inequality -
  Application to time-delay and sampled-data systems</title><categories>cs.SY math.DS math.OC</categories><journal-ref>IEEE Transactions on Automatic Control, Vol. 56(7), pp. 1660-1665,
  2011</journal-ref><doi>10.1109/TAC.2011.2121410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Jensen's inequality plays a crucial role in the analysis of time-delay
and sampled-data systems. Its conservatism is studied through the use of the
Gr\&quot;{u}ss Inequality. It has been reported in the literature that fragmentation
(or partitioning) schemes allow to empirically improve the results. We prove
here that the Jensen's gap can be made arbitrarily small provided that the
order of uniform fragmentation is chosen sufficiently large. Non-uniform
fragmentation schemes are also shown to speed up the convergence in certain
cases. Finally, a family of bounds is characterized and a comparison with other
bounds of the literature is provided. It is shown that the other bounds are
equivalent to Jensen's and that they exhibit interesting well-posedness and
linearity properties which can be exploited to obtain better numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1079</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1079</id><created>2012-04-04</created><updated>2012-04-16</updated><authors><author><keyname>Thapper</keyname><forenames>Johan</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>The Power of Linear Programming for Valued CSPs</title><categories>cs.CC</categories><comments>Corrected a few typos</comments><acm-class>F.2.m</acm-class><journal-ref>Proc. of FOCS'12 669-678 (2012)</journal-ref><doi>10.1109/FOCS.2012.25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of valued constraint satisfaction problems (VCSPs) is characterised
by a valued constraint language, a fixed set of cost functions on a finite
domain. An instance of the problem is specified by a sum of cost functions from
the language with the goal to minimise the sum. This framework includes and
generalises well-studied constraint satisfaction problems (CSPs) and maximum
constraint satisfaction problems (Max-CSPs).
  Our main result is a precise algebraic characterisation of valued constraint
languages whose instances can be solved exactly by the basic linear programming
relaxation. Using this result, we obtain tractability of several novel and
previously widely-open classes of VCSPs, including problems over valued
constraint languages that are: (1) submodular on arbitrary lattices; (2)
bisubmodular (also known as k-submodular) on arbitrary finite domains; (3)
weakly (and hence strongly) tree-submodular on arbitrary trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1080</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1080</id><created>2012-04-04</created><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Sename</keyname><forenames>Olivier</forenames></author><author><keyname>Lafay</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Memory Resilient Gain-scheduled State-Feedback Control of Uncertain
  LTI/LPV Systems with Time-Varying Delays</title><categories>cs.SY math.CA math.DS math.OC</categories><journal-ref>Systems &amp; Control Letters, Vol. 59(8), pp. 664-671, 2010</journal-ref><doi>10.1016/j.sysconle.2010.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stabilization of uncertain LTI/LPV time delay systems with time varying
delays by state-feedback controllers is addressed. At the difference of other
works in the literature, the proposed approach allows for the synthesis of
resilient controllers with respect to uncertainties on the implemented delay.
It is emphasized that such controllers unify memoryless and exact-memory
controllers usually considered in the literature. The solutions to the
stability and stabilization problems are expressed in terms of LMIs which allow
to check the stability of the closed-loop system for a given bound on the
knowledge error and even optimize the uncertainty radius under some performance
constraints; in this paper, the $\mathcal{H}_\infty$ performance measure is
considered. The interest of the approach is finally illustrated through several
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1082</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1082</id><created>2012-04-04</created><updated>2013-08-16</updated><authors><author><keyname>Bar-Noy</keyname><forenames>Amotz</forenames></author><author><keyname>Baumer</keyname><forenames>Ben</forenames></author><author><keyname>Rawitz</keyname><forenames>Dror</forenames></author></authors><title>Set It and Forget It: Approximating the Set Once Strip Cover Problem</title><categories>cs.DS</categories><comments>briefly announced at SPAA 2013</comments><msc-class>68W40, 68Q25</msc-class><acm-class>F.2.2; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Set Once Strip Cover problem, in which n wireless sensors are
deployed over a one-dimensional region. Each sensor has a fixed battery that
drains in inverse proportion to a radius that can be set just once, but
activated at any time. The problem is to find an assignment of radii and
activation times that maximizes the length of time during which the entire
region is covered. We show that this problem is NP-hard. Second, we show that
RoundRobin, the algorithm in which the sensors simply take turns covering the
entire region, has a tight approximation guarantee of 3/2 in both Set Once
Strip Cover and the more general Strip Cover problem, in which each radius may
be set finitely-many times. Moreover, we show that the more general class of
duty cycle algorithms, in which groups of sensors take turns covering the
entire region, can do no better. Finally, we give an optimal O(n^2 log n)-time
algorithm for the related Set Radius Strip Cover problem, in which all sensors
must be activated immediately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1085</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1085</id><created>2012-04-04</created><authors><author><keyname>Puigt</keyname><forenames>Matthieu</forenames></author><author><keyname>Griffin</keyname><forenames>Anthony</forenames></author><author><keyname>Mouchtaris</keyname><forenames>Athanasios</forenames></author></authors><title>Post-Nonlinear Sparse Component Analysis Using Single-Source Zones and
  Functional Data Clustering</title><categories>cs.IT math.IT</categories><comments>11 pages, submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a general extension of linear sparse component
analysis (SCA) approaches to postnonlinear (PNL) mixtures. In particular, and
contrary to the state-of-art methods, our approaches use a weak sparsity source
assumption: we look for tiny temporal zones where only one source is active. We
investigate two nonlinear single-source confidence measures, using the mutual
information and a local linear tangent space approximation (LTSA). For this
latter measure, we derive two extensions of linear single-source measures,
respectively based on correlation (LTSA-correlation) and eigenvalues
(LTSA-PCA). A second novelty of our approach consists of applying functional
data clustering techniques to the scattered observations in the above
single-source zones, thus allowing us to accurately estimate them.We first
study a classical approach using a B-spline approximation, and then two
approaches which locally approximate the nonlinear functions as lines. Finally,
we extend our PNL methods to more general nonlinear mixtures. Combining
single-source zones and functional data clustering allows us to tackle speech
signals, which has never been performed by other PNL-SCA methods. We
investigate the performance of our approaches with simulated PNL mixtures of
real speech signals. Both the mutual information and the LTSA-correlation
measures are better-suited to detecting single-source zones than the LTSA-PCA
measure. We also find local-linear-approximation-based clustering approaches to
be more flexible and more accurate than the B-spline one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1086</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1086</id><created>2012-04-04</created><updated>2013-05-18</updated><authors><author><keyname>Pettie</keyname><forenames>Seth</forenames></author></authors><title>Sharp Bounds on Davenport-Schinzel Sequences of Every Order</title><categories>cs.DM cs.CG math.CO</categories><comments>A 10-page extended abstract will appear in the Proceedings of the
  Symposium on Computational Geometry, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the longest-standing open problems in computational geometry is to
bound the lower envelope of $n$ univariate functions, each pair of which
crosses at most $s$ times, for some fixed $s$. This problem is known to be
equivalent to bounding the length of an order-$s$ Davenport-Schinzel sequence,
namely a sequence over an $n$-letter alphabet that avoids alternating
subsequences of the form $a \cdots b \cdots a \cdots b \cdots$ with length
$s+2$. These sequences were introduced by Davenport and Schinzel in 1965 to
model a certain problem in differential equations and have since been applied
to bounding the running times of geometric algorithms, data structures, and the
combinatorial complexity of geometric arrangements.
  Let $\lambda_s(n)$ be the maximum length of an order-$s$ DS sequence over $n$
letters. What is $\lambda_s$ asymptotically? This question has been answered
satisfactorily (by Hart and Sharir, Agarwal, Sharir, and Shor, Klazar, and
Nivasch) when $s$ is even or $s\le 3$. However, since the work of Agarwal,
Sharir, and Shor in the mid-1980s there has been a persistent gap in our
understanding of the odd orders.
  In this work we effectively close the problem by establishing sharp bounds on
Davenport-Schinzel sequences of every order $s$. Our results reveal that,
contrary to one's intuition, $\lambda_s(n)$ behaves essentially like
$\lambda_{s-1}(n)$ when $s$ is odd. This refutes conjectures due to Alon et al.
(2008) and Nivasch (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1091</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1091</id><created>2012-04-04</created><updated>2013-03-21</updated><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Load-Aware Modeling and Analysis of Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><comments>to appear, IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random spatial models are attractive for modeling heterogeneous cellular
networks (HCNs) due to their realism, tractability, and scalability. A major
limitation of such models to date in the context of HCNs is the neglect of
network traffic and load: all base stations (BSs) have typically been assumed
to always be transmitting. Small cells in particular will have a lighter load
than macrocells, and so their contribution to the network interference may be
significantly overstated in a fully loaded model. This paper incorporates a
flexible notion of BS load by introducing a new idea of conditionally thinning
the interference field. For a K-tier HCN where BSs across tiers differ in terms
of transmit power, supported data rate, deployment density, and now load, we
derive the coverage probability for a typical mobile, which connects to the
strongest BS signal. Conditioned on this connection, the interfering BSs of the
$i^{th}$ tier are assumed to transmit independently with probability $p_i$,
which models the load. Assuming - reasonably - that smaller cells are more
lightly loaded than macrocells, the analysis shows that adding such access
points to the network always increases the coverage probability. We also
observe that fully loaded models are quite pessimistic in terms of coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1096</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1096</id><created>2012-04-04</created><authors><author><keyname>Mukherjee</keyname><forenames>Amitav</forenames></author><author><keyname>Pei</keyname><forenames>Minyan</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>MIMO Precoding in Underlay Cognitive Radio Systems with Completely
  Unknown Primary CSI</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a novel underlay MIMO cognitive radio (CR) system, where
the instantaneous or statistical channel state information (CSI) of the
interfering channels to the primary receivers (PRs) is completely unknown to
the CR. For the single underlay receiver scenario, we assume a minimum
information rate must be guaranteed on the CR main channel whose CSI is known
at the CR transmitter. We first show that low-rank CR interference is
preferable for improving the throughput of the PRs compared with spreading less
power over more transmit dimensions. Based on this observation, we then propose
a rank minimization CR transmission strategy assuming a minimum information
rate must be guaranteed on the CR main channel. We propose a simple solution
referred to as frugal waterfilling (FWF) that uses the least amount of power
required to achieve the rate constraint with a minimum-rank transmit covariance
matrix. We also present two heuristic approaches that have been used in prior
work to transform rank minimization problems into convex optimization problems.
The proposed schemes are then generalized to an underlay MIMO CR downlink
network with multiple receivers. Finally, a theoretical analysis of the
interference temperature and leakage rate outage probabilities at the PR is
presented for Rayleigh fading channels.We demonstrate that the direct FWF
solution leads to higher PR throughput even though it has higher interference
&quot;temperature (IT) compared with the heuristic methods and classic waterfilling,
which calls into question the use of IT as a metric for CR interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1098</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1098</id><created>2012-04-04</created><updated>2013-04-12</updated><authors><author><keyname>Saks</keyname><forenames>Michael</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Space efficient streaming algorithms for the distance to monotonicity
  and asymmetric edit distance</title><categories>cs.DS cs.DM</categories><comments>Final SODA 2013 version. Fixed bugs. We get a \delta n-additive
  approximation for edit distance, not multiplicative as said in the earlier
  tech report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximating the length of the longest increasing sequence (LIS) of an array
is a well-studied problem. We study this problem in the data stream model,
where the algorithm is allowed to make a single left-to-right pass through the
array and the key resource to be minimized is the amount of additional memory
used. We present an algorithm which, for any $\delta &gt; 0$, given streaming
access to an array of length $n$ provides a $(1+\delta)$-multiplicative
approximation to the \emph{distance to monotonicity} ($n$ minus the length of
the LIS), and uses only $O((\log^2 n)/\delta)$ space. The previous best known
approximation using polylogarithmic space was a multiplicative 2-factor. Our
algorithm can be used to estimate the length of the LIS to within an additive
$\delta n$ for any $\delta &gt;0$ while previous algorithms could only achieve
additive error $n(1/2-o(1))$.
  Our algorithm is very simple, being just 3 lines of pseudocode, and has a
small update time. It is essentially a polylogarithmic space approximate
implementation of a classic dynamic program that computes the LIS.
  We also give a streaming algorithm for approximating $LCS(x,y)$, the length
of the longest common subsequence between strings $x$ and $y$, each of length
$n$. Our algorithm works in the asymmetric setting (inspired by \cite{AKO10}),
in which we have random access to $y$ and streaming access to $x$, and runs in
small space provided that no single symbol appears very often in $y$. More
precisely, it gives an additive-$\delta n$ approximation to $LCS(x,y)$ (and
hence also to $E(x,y) = n-LCS(x,y)$, the edit distance between $x$ and $y$ when
insertions and deletions, but not substitutions, are allowed), with space
complexity $O(k(\log^2 n)/\delta)$, where $k$ is the maximum number of times
any one symbol appears in $y$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1106</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1106</id><created>2012-04-04</created><authors><author><keyname>Kraning</keyname><forenames>Matt</forenames></author><author><keyname>Chu</keyname><forenames>Eric</forenames></author><author><keyname>Lavaei</keyname><forenames>Javad</forenames></author><author><keyname>Boyd</keyname><forenames>Stephen</forenames></author></authors><title>Message Passing for Dynamic Network Energy Management</title><categories>math.OC cs.DC cs.SY</categories><comments>Submitted to IEEE Transactions on Smart grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network of devices, such as generators, fixed loads, deferrable
loads, and storage devices, each with its own dynamic constraints and
objective, connected by lossy capacitated lines. The problem is to minimize the
total network objective subject to the device and line constraints, over a
given time horizon. This is a large optimization problem, with variables for
consumption or generation in each time period for each device. In this paper we
develop a decentralized method for solving this problem. The method is
iterative: At each step, each device exchanges simple messages with its
neighbors in the network and then solves its own optimization problem,
minimizing its own objective function, augmented by a term determined by the
messages it has received. We show that this message passing method converges to
a solution when the device objective and constraints are convex. The method is
completely decentralized, and needs no global coordination other than
synchronizing iterations; the problems to be solved by each device can
typically be solved extremely efficiently and in parallel. The method is fast
enough that even a serial implementation can solve substantial problems in
reasonable time frames. We report results for several numerical experiments,
demonstrating the method's speed and scaling, including the solution of a
problem instance with over 30 million variables in 52 minutes for a serial
implementation; with decentralized computing, the solve time would be less than
one second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1111</identifier>
 <datestamp>2012-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1111</id><created>2012-04-04</created><updated>2012-04-09</updated><authors><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author></authors><title>Faster Algorithms for Rectangular Matrix Multiplication</title><categories>cs.DS cs.CC cs.SC</categories><comments>37 pages; v2: some additions in the acknowledgments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let {\alpha} be the maximal value such that the product of an n x n^{\alpha}
matrix by an n^{\alpha} x n matrix can be computed with n^{2+o(1)} arithmetic
operations. In this paper we show that \alpha&gt;0.30298, which improves the
previous record \alpha&gt;0.29462 by Coppersmith (Journal of Complexity, 1997).
More generally, we construct a new algorithm for multiplying an n x n^k matrix
by an n^k x n matrix, for any value k\neq 1. The complexity of this algorithm
is better than all known algorithms for rectangular matrix multiplication. In
the case of square matrix multiplication (i.e., for k=1), we recover exactly
the complexity of the algorithm by Coppersmith and Winograd (Journal of
Symbolic Computation, 1990).
  These new upper bounds can be used to improve the time complexity of several
known algorithms that rely on rectangular matrix multiplication. For example,
we directly obtain a O(n^{2.5302})-time algorithm for the all-pairs shortest
paths problem over directed graphs with small integer weights, improving over
the O(n^{2.575})-time algorithm by Zwick (JACM 2002), and also improve the time
complexity of sparse square matrix multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1113</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1113</id><created>2012-04-04</created><updated>2012-04-11</updated><authors><author><keyname>Bi</keyname><forenames>Jingguo</forenames></author><author><keyname>Cheng</keyname><forenames>Qi</forenames></author><author><keyname>Rojas</keyname><forenames>J. Maurice</forenames></author></authors><title>Sub-Linear Root Detection, and New Hardness Results, for Sparse
  Polynomials Over Finite Fields</title><categories>math.NT cs.CC</categories><comments>15 pages total (cover page, 10 pages, references, and 3 short
  appendices). This version corrects various minor typos, and improves the
  statement of the first main theorem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic 2^O(t)q^{(t-2)(t-1)+o(1)} algorithm to decide
whether a univariate polynomial f, with exactly t monomial terms and degree &lt;q,
has a root in F_q. A corollary of our method --- the first with complexity
sub-linear in q when t is fixed --- is that the nonzero roots in F_q can be
partitioned into at most 2 \sqrt{t-1} (q-1)^{(t-2)(t-1)} cosets of two
subgroups S_1,S_2 of F^*_q, with S_1 in S_2. Another corollary is the first
deterministic sub-linear algorithm for detecting common degree one factors of
k-tuples of t-nomials in F_q[x] when k and t are fixed.
  When t is not fixed we show that each of the following problems is NP-hard
with respect to BPP-reductions, even when p is prime: (1) detecting roots in
F_p for f, (2) deciding whether the square of a degree one polynomial in F_p[x]
divides f, (3) deciding whether the discriminant of f vanishes, (4) deciding
whether the gcd of two t-nomials in F_p[x] has positive degree. Finally, we
prove that if the complexity of root detection is sub-linear (in a refined
sense), relative to the straight-line program encoding, then NEXP is not in
P/Poly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1134</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1134</id><created>2012-04-05</created><authors><author><keyname>Carlucci</keyname><forenames>Lorenzo</forenames></author><author><keyname>Zdanowski</keyname><forenames>Konrad</forenames></author></authors><title>The strength of Ramsey Theorem for coloring relatively large sets</title><categories>math.LO cs.LO math.CO</categories><msc-class>03D25, 05D10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the computational content and the proof-theoretic strength of
a Ramsey-type theorem for bi-colorings of so-called {\em exactly large} sets.
An {\it exactly large} set is a set $X\subset\Nat$ such that
$\card(X)=\min(X)+1$. The theorem we analyze is as follows. For every infinite
subset $M$ of $\Nat$, for every coloring $C$ of the exactly large subsets of
$M$ in two colors, there exists and infinite subset $L$ of $M$ such that $C$ is
constant on all exactly large subsets of $L$. This theorem is essentially due
to Pudl\`ak and R\&quot;odl and independently to Farmaki. We prove that --- over
Computable Mathematics --- this theorem is equivalent to closure under the
$\omega$ Turing jump (i.e., under arithmetical truth). Natural combinatorial
theorems at this level of complexity are rare. Our results give a complete
characterization of the theorem from the point of view of Computable
Mathematics and of the Proof Theory of Arithmetic. This nicely extends the
current knowledge about the strength of Ramsey Theorem. We also show that
analogous results hold for a related principle based on the Regressive Ramsey
Theorem. In addition we give a further characterization in terms of truth
predicates over Peano Arithmetic. We conjecture that analogous results hold for
larger ordinals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1136</identifier>
 <datestamp>2012-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1136</id><created>2012-04-05</created><updated>2012-07-10</updated><authors><author><keyname>Kosowski</keyname><forenames>Adrian</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>Faster Walks in Graphs: A $\tilde O(n^2)$ Time-Space Trade-off for
  Undirected s-t Connectivity</title><categories>cs.DS</categories><comments>Version 3 makes use of the Metropolis-Hastings walk</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we make use of the Metropolis-type walks due to Nonaka et al.
(2010) to provide a faster solution to the $S$-$T$-connectivity problem in
undirected graphs (USTCON). As our main result, we propose a family of
randomized algorithms for USTCON which achieves a time-space product of $S\cdot
T = \tilde O(n^2)$ in graphs with $n$ nodes and $m$ edges (where the $\tilde
O$-notation disregards poly-logarithmic terms). This improves the previously
best trade-off of $\tilde O(n m)$, due to Feige (1995). Our algorithm consists
in deploying several short Metropolis-type walks, starting from landmark nodes
distributed using the scheme of Broder et al. (1994) on a modified input graph.
In particular, we obtain an algorithm running in time $\tilde O(n+m)$ which is,
in general, more space-efficient than both BFS and DFS. We close the paper by
showing how to fine-tune the Metropolis-type walk so as to match the
performance parameters (e.g., average hitting time) of the unbiased random walk
for any graph, while preserving a worst-case bound of $\tilde O(n^2)$ on cover
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1140</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1140</id><created>2012-04-05</created><authors><author><keyname>Ristov</keyname><forenames>Sasko</forenames></author><author><keyname>Gusev</keyname><forenames>Marjan</forenames></author><author><keyname>Kostoska</keyname><forenames>Magdalena</forenames></author></authors><title>Cloud Computing Security in Business Information Systems</title><categories>cs.CR cs.DC</categories><journal-ref>S. Ristov, M. Gusev, and M. Kostoska, &quot;Cloud computing security in
  business information systems,&quot; International Journal of Network Security &amp;
  Its Applications (IJNSA), vol. 4, no. 2, pp. 75-93, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing providers' and customers' services are not only exposed to
existing security risks, but, due to multi-tenancy, outsourcing the application
and data, and virtualization, they are exposed to the emergent, as well.
Therefore, both the cloud providers and customers must establish information
security system and trustworthiness each other, as well as end users. In this
paper we analyze main international and industrial standards targeting
information security and their conformity with cloud computing security
challenges. We evaluate that almost all main cloud service providers (CSPs) are
ISO 27001:2005 certified, at minimum. As a result, we propose an extension to
the ISO 27001:2005 standard with new control objective about virtualization, to
retain generic, regardless of company's type, size and nature, that is, to be
applicable for cloud systems, as well, where virtualization is its baseline. We
also define a quantitative metric and evaluate the importance factor of ISO
27001:2005 control objectives if customer services are hosted on-premise or in
cloud. The conclusion is that obtaining the ISO 27001:2005 certificate (or if
already obtained) will further improve CSP and CC information security systems,
and introduce mutual trust in cloud services but will not cover all relevant
issues. In this paper we also continue our efforts in business continuity
detriments cloud computing produces, and propose some solutions that mitigate
the risks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1147</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1147</id><created>2012-04-05</created><authors><author><keyname>Gawlitza</keyname><forenames>Thomas Martin</forenames></author><author><keyname>Seidl</keyname><forenames>Helmut</forenames></author></authors><title>Numerical Invariants through Convex Relaxation and Max-Strategy
  Iteration</title><categories>cs.PL</categories><comments>42 pages, conference version appears in the proceedings of the Static
  Analysis Symposium 2010</comments><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we develop a max-strategy improvement algorithm for computing
least fixpoints of operators on on the reals that are point-wise maxima of
finitely many monotone and order-concave operators. Computing the uniquely
determined least fixpoint of such operators is a problem that occurs frequently
in the context of numerical program/systems verification/analysis. As an
example for an application we discuss how our algorithm can be applied to
compute numerical invariants of programs by abstract interpretation based on
quadratic templates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1156</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1156</id><created>2012-04-05</created><authors><author><keyname>Krithika</keyname><forenames>V.</forenames></author><author><keyname>Kaur</keyname><forenames>Dr. Arshinder</forenames></author><author><keyname>Sekaran</keyname><forenames>Dr. K. Chandra</forenames></author></authors><title>Web Services Supply Chains: A Literature Review</title><categories>cs.SY</categories><comments>22 pages, 2 diagrams, 8 tables</comments><journal-ref>2. V. Krithika, ArshinderKaur, K. Chandra Sekaran, &quot;Web Services
  Supply Chains: A Literature Review&quot;, International Journal of Web Services
  Computing, March 2012, Volume3, Issue 1, ISSN : 0976 - 9811</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The aim of this review paper is to bring into light a potential area i.e.,
web services supply chains for research by analyzing the existing state of art
in this. It is observed from the review process that there seems to be much
less work done in the area of web service supply chains as compared to
e-commerce and product oriented service supply chains. The service quality
assurance models, end to end Quality of Service (QoS) models, attempts made to
QoS attributes are also found to be from individual perspectives of
participating entities in a service process rather than a collective
perspective considering individual QoS attributes rather than multiple QoS
attributes. In light of these gaps we highlight the comparison between product
oriented and pure online/ web service supply chains, a need for quality driven
optimization in the web services supply chains, perceived complexities in the
existing work and propose a conceptual model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1158</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1158</id><created>2012-04-05</created><updated>2013-07-15</updated><authors><author><keyname>Dedecius</keyname><forenames>K.</forenames></author><author><keyname>Se&#x10d;k&#xe1;rov&#xe1;</keyname><forenames>V.</forenames></author></authors><title>Dynamic Bayesian diffusion estimation</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures. Originally accepted to IFAC SYSID 2012. Updated:
  corrected typos</comments><msc-class>60G35 (Primary) 94A12 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapidly increasing complexity of (mainly wireless) ad-hoc networks
stresses the need of reliable distributed estimation of several variables of
interest. The widely used centralized approach, in which the network nodes
communicate their data with a single specialized point, suffers from high
communication overheads and represents a potentially dangerous concept with a
single point of failure needing special treatment. This paper's aim is to
contribute to another quite recent method called diffusion estimation. By
decentralizing the operating environment, the network nodes communicate just
within a close neighbourhood. We adopt the Bayesian framework to modelling and
estimation, which, unlike the traditional approaches, abstracts from a
particular model case. This leads to a very scalable and universal method,
applicable to a wide class of different models. A particularly interesting case
- the Gaussian regressive model - is derived as an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1160</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1160</id><created>2012-04-05</created><authors><author><keyname>Maity</keyname><forenames>Suman Kalyan</forenames></author><author><keyname>Manoj</keyname><forenames>T. Venkat</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>Opinion formation in time-varying social networks: The case of Naming
  Game</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 15 figures and 1 table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We study the dynamics of the Naming Game as an opinion formation model on
time-varying social networks. This agent-based model captures the essential
features of the agreement dynamics by means of a memory-based negotiation
process. Our study focuses on the impact of time-varying properties of the
social network of the agents on the Naming Game dynamics. We investigate the
outcomes of the dynamics on two different types of time-varying data - (i) the
networks vary across days and (ii) the networks vary within very short
intervals of time (20 seconds). In the first case, we find that networks with
strong community structure hinder the system from reaching global agreement;
the evolution of the Naming Game in these networks maintains clusters of
coexisting opinions indefinitely leading to metastability. In the second case,
we investigate the evolution of the Naming Game in perfect synchronization with
the time evolution of the underlying social network shedding new light on the
traditional emergent properties of the game that differ largely from what has
been reported in the existing literature
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1162</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1162</id><created>2012-04-05</created><authors><author><keyname>Hajjar</keyname><forenames>Abd El Salam Al</forenames></author><author><keyname>Ismail</keyname><forenames>Anis</forenames></author><author><keyname>Hajjar</keyname><forenames>Mohammad</forenames></author><author><keyname>El-Sayed</keyname><forenames>Mazen</forenames></author></authors><title>Performance of the Google Desktop, Arabic Google Desktop and Peer to
  Peer Application in Arabic Language</title><categories>cs.IR</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Arabic language is a complex language; it is different from Western
languages especially at the morphological and spelling variations. Indeed, the
performance of information retrieval systems in the Arabic language is still a
problem. For this reason, we are interested in studying the performance of the
most famous search engine, which is a Google Desktop, while searching in Arabic
language documents. Then, we propose an update to the Google Desktop to take
into consideration in search the Arabic words that have the same root. After
that, we evaluate the performance of the Google Desktop in this context. Also,
we are interested in evaluation the performance of peer-to-peer application in
two ways. The first one uses a simple indexation that indexes Arabic documents
without taking in consideration the root of words. The second way takes in
consideration the roots in the indexation of Arabic documents. This evaluation
is done by using a corpus of ten thousand documents and one hundred different
queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1169</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1169</id><created>2012-04-05</created><authors><author><keyname>Sosnowski</keyname><forenames>Janusz</forenames></author></authors><title>Exploring Application Logs</title><categories>cs.OH</categories><comments>2 pages in two column format, will be presented as FasAbstract during
  EDCC 2012 conference, May 8-11, 2012</comments><acm-class>C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of analyzing application event logs in
relevance to dependability evaluation. We present the significance of
application logs as a valuable source of information on operational profiles,
anomalies and errors. They can enhance classical approaches based on monitoring
system logs and performance variables. Keywords; event monitoring, operational
profiles, anomalies
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1172</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1172</id><created>2012-04-05</created><authors><author><keyname>Abri</keyname><forenames>Karima Ben Hamida El</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ammar</forenames></author></authors><title>Timing acquisition and demodulation of an UWB system based on the
  differential scheme</title><categories>cs.IT math.IT</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind synchronization constitutes a major challenge in realizing highly
efficient ultra wide band (UWB) systems because of the short pulse duration
which requires a fast synchronization algorithm to accommodate several
asynchronous users. In this paper, we present a new Code Block Synchronization
Algorithm (CBSA) based on a particular code design for a non coherent
transmission. Synchronization algorithm is applied directly on received signal
to estimate timing offset, without needing any training sequence. Different
users can share the available bandwidth by means of different spreading codes
with different lengths. This allows the receiver to separate users, and to
recover the timing information of the transmitted symbols. Simulation results
and comparisons validate the promising performance of the proposed scheme even
in a multi user scenario. In fact, the proposed algorithm offers a gain of
about 3 dB in comparison with reference [5].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1177</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1177</id><created>2012-04-05</created><authors><author><keyname>Khan</keyname><forenames>Aamir</forenames></author><author><keyname>Farooq</keyname><forenames>Hasan</forenames></author></authors><title>Principal Component Analysis-Linear Discriminant Analysis Feature
  Extractor for Pattern Recognition</title><categories>cs.CV</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 6, No 2, November 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robustness of embedded biometric systems is of prime importance with the
emergence of fourth generation communication devices and advancement in
security systems This paper presents the realization of such technologies which
demands reliable and error-free biometric identity verification systems. High
dimensional patterns are not permitted due to eigen-decomposition in high
dimensional image space and degeneration of scattering matrices in small size
sample. Generalization, dimensionality reduction and maximizing the margins are
controlled by minimizing weight vectors. Results show good pattern by
multimodal biometric system proposed in this paper. This paper is aimed at
investigating a biometric identity system using Principal Component Analysis
and Lindear Discriminant Analysis with K-Nearest Neighbor and implementing such
system in real-time using SignalWAVE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1178</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1178</id><created>2012-04-05</created><authors><author><keyname>Ishii</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Inoie</keyname><forenames>Atsushi</forenames></author></authors><title>An initial peer configuration algorithm for multi-streaming peer-to-peer
  networks</title><categories>cs.NI cs.DC</categories><comments>10 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of the Internet technology enables us to use network applications
for streaming audio and video. Especially, real-time streaming services using
peer-to-peer (P2P) technology are currently emerging. An important issue on P2P
streaming is how to construct a logical network (overlay network) on a physical
network (IP network). In this paper, we propose an initial peer configuration
algorithm for a multi-streaming peer-to-peer network. The proposed algorithm is
based on a mesh-pull approach where any node has multiple parent and child
nodes as neighboring nodes, and content transmitted between these neighboring
nodes depends on their parent-child relationships. Our simulation experiments
show that the proposed algorithm improves the number of joining node and
traffic load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1179</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1179</id><created>2012-04-05</created><authors><author><keyname>Akram</keyname><forenames>Muhammad Adeel</forenames></author><author><keyname>Khan</keyname><forenames>Aamir</forenames></author><author><keyname>Sarfaraz</keyname><forenames>Muhammad Masood</forenames></author></authors><title>C-slow Technique vs Multiprocessor in designing Low Area Customized
  Instruction set Processor for Embedded Applications</title><categories>cs.AR</categories><journal-ref>International Journal of Computer Applications (0975 - 8887)
  Volume 36 - No.7, December 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand for high performance embedded processors, for consumer
electronics, is rapidly increasing for the past few years. Many of these
embedded processors depend upon custom built Instruction Ser Architecture (ISA)
such as game processor (GPU), multimedia processors, DSP processors etc.
Primary requirement for consumer electronic industry is low cost with high
performance and low power consumption. A lot of research has been evolved to
enhance the performance of embedded processors through parallel computing. But
some of them focus superscalar processors i.e. single processors with more
resources like Instruction Level Parallelism (ILP) which includes Very Long
Instruction Word (VLIW) architecture, custom instruction set extensible
processor architecture and others require more number of processing units on a
single chip like Thread Level Parallelism (TLP) that includes Simultaneous
Multithreading (SMT), Chip Multithreading (CMT) and Chip Multiprocessing (CMP).
In this paper, we present a new technique, named C-slow, to enhance performance
for embedded processors for consumer electronics by exploiting multithreading
technique in single core processors. Without resulting into the complexity of
micro controlling with Real Time Operating system (RTOS), C-slowed processor
can execute multiple threads in parallel using single datapath of Instruction
Set processing element. This technique takes low area &amp; approach complexity of
general purpose processor running RTOS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1185</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1185</id><created>2012-04-05</created><authors><author><keyname>Budikova</keyname><forenames>Petra</forenames></author><author><keyname>Batko</keyname><forenames>Michal</forenames></author><author><keyname>Zezula</keyname><forenames>Pavel</forenames></author></authors><title>Query Language for Complex Similarity Queries</title><categories>cs.DB cs.IR cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For complex data types such as multimedia, traditional data management
methods are not suitable. Instead of attribute matching approaches, access
methods based on object similarity are becoming popular. Recently, this
resulted in an intensive research of indexing and searching methods for the
similarity-based retrieval. Nowadays, many efficient methods are already
available, but using them to build an actual search system still requires
specialists that tune the methods and build the system manually. Several
attempts have already been made to provide a more convenient high-level
interface in a form of query languages for such systems, but these are limited
to support only basic similarity queries. In this paper, we propose a new
language that allows to formulate content-based queries in a flexible way,
taking into account the functionality offered by a particular search engine in
use. To ensure this, the language is based on a general data model with an
abstract set of operations. Consequently, the language supports various
advanced query operations such as similarity joins, reverse nearest neighbor
queries, or distinct kNN queries, as well as multi-object and multi-modal
queries. The language is primarily designed to be used with the MESSIF
framework for content-based searching but can be employed by other retrieval
systems as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1196</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1196</id><created>2012-04-05</created><updated>2012-06-12</updated><authors><author><keyname>G&#xf6;ller</keyname><forenames>Stefan</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Mundhenk</keyname><forenames>Martin</forenames></author><author><keyname>Schneider</keyname><forenames>Thomas</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Weiss</keyname><forenames>Felix</forenames></author></authors><title>The Complexity of Monotone Hybrid Logics over Linear Frames and the
  Natural Numbers</title><categories>cs.CC</categories><comments>19 pages + 15 pages appendix, 3 figures</comments><msc-class>03B45, 68Q17</msc-class><acm-class>F.2; F.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid logic with binders is an expressive specification language. Its
satisfiability problem is undecidable in general. If frames are restricted to N
or general linear orders, then satisfiability is known to be decidable, but of
non-elementary complexity. In this paper, we consider monotone hybrid logics
(i.e., the Boolean connectives are conjunction and disjunction only) over N and
general linear orders. We show that the satisfiability problem remains
non-elementary over linear orders, but its complexity drops to
PSPACE-completeness over N. We categorize the strict fragments arising from
different combinations of modal and hybrid operators into NP-complete and
tractable (i.e. complete for NC1or LOGSPACE). Interestingly, NP-completeness
depends only on the fragment and not on the frame. For the cases above NP,
satisfiability over linear orders is harder than over N, while below NP it is
at most as hard. In addition we examine model-theoretic properties of the
fragments in question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1198</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1198</id><created>2012-04-05</created><authors><author><keyname>Omee</keyname><forenames>Farjana Yeasmin</forenames></author><author><keyname>Himel</keyname><forenames>Shiam Shabbir</forenames></author><author><keyname>Bikas</keyname><forenames>Md. Abu Naser</forenames></author></authors><title>A Complete Workflow for Development of Bangla OCR</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Applications, Volume 21, No.9,
  May 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Developing a Bangla OCR requires bunch of algorithm and methods. There were
many effort went on for developing a Bangla OCR. But all of them failed to
provide an error free Bangla OCR. Each of them has some lacking. We discussed
about the problem scope of currently existing Bangla OCR's. In this paper, we
present the basic steps required for developing a Bangla OCR and a complete
workflow for development of a Bangla OCR with mentioning all the possible
algorithms required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1201</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1201</id><created>2012-04-05</created><authors><author><keyname>Paul</keyname><forenames>Bijan</forenames></author><author><keyname>Ibrahim</keyname><forenames>Md.</forenames></author><author><keyname>Bikas</keyname><forenames>Md. Abu Naser</forenames></author></authors><title>VANET Routing Protocols: Pros and Cons</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Applications, Volume 20, No.3,
  April 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  VANET (Vehicular Ad-hoc Network) is a new technology which has taken enormous
attention in the recent years. Due to rapid topology changing and frequent
disconnection makes it difficult to design an efficient routing protocol for
routing data among vehicles, called V2V or vehicle to vehicle communication and
vehicle to road side infrastructure, called V2I. The existing routing protocols
for VANET are not efficient to meet every traffic scenarios. Thus design of an
efficient routing protocol has taken significant attention. So, it is very
necessary to identify the pros and cons of routing protocols which can be used
for further improvement or development of any new routing protocol. This paper
presents the pros and cons of VANET routing protocols for inter vehicle
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1206</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1206</id><created>2012-04-05</created><authors><author><keyname>Paul</keyname><forenames>Bijan</forenames></author><author><keyname>Ibrahim</keyname><forenames>Md.</forenames></author><author><keyname>Bikas</keyname><forenames>Md. Abu Naser</forenames></author></authors><title>Experimental Analysis of AODV &amp; DSR over TCP &amp; CBR Connections with
  Varying Speed and Node Density in VANET</title><categories>cs.NI</categories><comments>International Journal of Computer Applications, Volume 24 - No.4,
  June 2011</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Vehicular adhoc network or VANET is special types of adhoc network consists
of moving cars referred to as nodes; provide a way to exchange any information
between cars without depending on fixed infrastructure. For efficient
communication between nodes various routing protocols and mobility models have
been proposed based on different scenarios. Due to rapid topology changing and
frequent disconnection makes it difficult to select suitable mobility model and
routing protocols. Hence performance evaluation and comparison between routing
protocols is required to understand any routing protocol as well as to develop
a new routing protocol. In this research paper, the performance of two
on-demand routing protocols AODV &amp; DSR has been analyzed by means of packet
delivery ratio, loss packet ratio &amp; average end-to-end delay with varying speed
limit and node density under TCP &amp; CBR connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1207</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1207</id><created>2012-04-05</created><authors><author><keyname>Paul</keyname><forenames>Bijan</forenames></author><author><keyname>Ibrahim</keyname><forenames>Md.</forenames></author><author><keyname>Bikas</keyname><forenames>Md. Abu Naser</forenames></author></authors><title>Performance Evaluation of Aodv&amp;DSR with Varying Pause Time &amp; Node
  Density Over TCP&amp;CBR Connections in Vanet</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1203.3240</comments><journal-ref>International Journal of Computer Science and Network Security,
  VOL.11, No.7, July 2011</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Vehicular ad hoc network is formed by cars which are called nodes; allow them
to communicate with one another without using any fixed road side unit. It has
some unique characteristics which make it different from other ad hoc network
as well as difficult to define any exact mobility model and routing protocols
because of their high mobility and changing mobility pattern. Hence performance
of routing protocols can vary with the various parameters such as speed, pause
time, node density and traffic scenarios. In this research paper, the
performance of two on-demand routing protocols AODV &amp; DSR has been analyzed by
means of packet delivery ratio, loss packet ratio &amp; average end-to-end delay
with varying pause time and node density under TCP &amp; CBR connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1215</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1215</id><created>2012-04-05</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author></authors><title>On the Value of Multiple Read/Write Streams for Data Compression</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study whether, when restricted to using polylogarithmic memory and
polylogarithmic passes, we can achieve qualitatively better data compression
with multiple read/write streams than we can with only one. We first show how
we can achieve universal compression using only one pass over one stream. We
then show that one stream is not sufficient for us to achieve good
grammar-based compression. Finally, we show that two streams are necessary and
sufficient for us to achieve entropy-only bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1216</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1216</id><created>2012-04-05</created><updated>2014-06-05</updated><authors><author><keyname>Fung</keyname><forenames>Adonis P. H.</forenames></author><author><keyname>Wang</keyname><forenames>Tielei</forenames></author><author><keyname>Cheung</keyname><forenames>K. W.</forenames></author><author><keyname>Wong</keyname><forenames>T. Y.</forenames></author></authors><title>Scanning of Rich Web Applications for Parameter Tampering
  Vulnerabilities</title><categories>cs.CR</categories><comments>12 pages, 2 tables, 3 figures To appear in ACM ASIA CCS'14, Kyoto,
  Japan</comments><acm-class>H.3.5; K.4.4</acm-class><doi>10.1145/2590296.2590324</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web applications require exchanging parameters between a client and a server
to function properly. In real-world systems such as online banking transfer,
traversing multiple pages with parameters contributed by both the user and
server is a must, and hence the applications have to enforce workflow and
parameter dependency controls across multiple requests. An application that
applies insufficient server-side input validations is however vulnerable to
parameter tampering attacks, which manipulate the exchanged parameters.
Existing fuzzing-based scanning approaches however neglected these important
controls, and this caused their fuzzing requests to be dropped before they can
reach any vulnerable code.
  In this paper, we propose a novel approach to identify the workflow and
parameter dependent constraints, which are then maintained and leveraged for
automatic detection of server acceptances during fuzzing. We realized the
approach by building a generic blackbox parameter tampering scanner. It
successfully uncovered a number of severe vulnerabilities, including one from
the largest multi-national banking website, which other scanners miss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1225</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1225</id><created>2012-04-05</created><authors><author><keyname>Emami</keyname><forenames>Masnida</forenames></author><author><keyname>Setayesh</keyname><forenames>Ali</forenames></author><author><keyname>Jaberi</keyname><forenames>Nasrin</forenames></author></authors><title>Distributed computing of Seismic Imaging Algorithms</title><categories>cs.DC physics.geo-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary use of technical computing in the oil and gas industries is for
seismic imaging of the earth's subsurface, driven by the business need for
making well-informed drilling decisions during petroleum exploration and
production. Since each oil/gas well in exploration areas costs several tens of
millions of dollars, producing high-quality seismic images in a reasonable time
can significantly reduce the risk of drilling a &quot;dry hole&quot;. Similarly, these
images are important as they can improve the position of wells in a
billion-dollar producing oil field. However seismic imaging is very data- and
compute-intensive which needs to process terabytes of data and require
Gflop-years of computation (using &quot;flop&quot; to mean floating point operation per
second). Due to the data/computing intensive nature of seismic imaging,
parallel computing are used to process data to reduce the time compilation.
  With introducing of Cloud computing, MapReduce programming model has been
attracted a lot of attention in parallel and distributed systems [1, 2] to
execute massive processing algorithms such as Bioinformatics[3], Astronomy[4],
Geology[5] and so on. In this report, we will investigate and discuss current
approaches to fit seismic algorithms to MapReduce programming model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1231</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1231</id><created>2012-04-05</created><updated>2012-08-13</updated><authors><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>How Many Vote Operations Are Needed to Manipulate A Voting System?</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a framework to study a general class of strategic
behavior in voting, which we call vote operations. We prove the following
theorem: if we fix the number of alternatives, generate $n$ votes i.i.d.
according to a distribution $\pi$, and let $n$ go to infinity, then for any
$\epsilon &gt;0$, with probability at least $1-\epsilon$, the minimum number of
operations that are needed for the strategic individual to achieve her goal
falls into one of the following four categories: (1) 0, (2) $\Theta(\sqrt n)$,
(3) $\Theta(n)$, and (4) $\infty$. This theorem holds for any set of vote
operations, any individual vote distribution $\pi$, and any integer generalized
scoring rule, which includes (but is not limited to) almost all commonly
studied voting rules, e.g., approval voting, all positional scoring rules
(including Borda, plurality, and veto), plurality with runoff, Bucklin,
Copeland, maximin, STV, and ranked pairs.
  We also show that many well-studied types of strategic behavior fall under
our framework, including (but not limited to) constructive/destructive
manipulation, bribery, and control by adding/deleting votes, margin of victory,
and minimum manipulation coalition size. Therefore, our main theorem naturally
applies to these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1232</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1232</id><created>2012-04-05</created><authors><author><keyname>Stoicescu</keyname><forenames>Miruna</forenames></author><author><keyname>Fabre</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Roy</keyname><forenames>Matthieu</forenames></author></authors><title>Experimenting with Component-Based Middleware for Adaptive Fault
  Tolerant Computing</title><categories>cs.SE</categories><comments>2p, EDCC 2012 Fast Abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper describes early experiments to validate the capabilities of
a component-based platform to observe and control a software architecture in
the small. This is part of a whole process for resilient computing, i.e.
targeting the adaptation of fault-tolerance mechanisms at runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1240</identifier>
 <datestamp>2013-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1240</id><created>2012-04-05</created><updated>2013-02-05</updated><authors><author><keyname>Luo</keyname><forenames>Shixin</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Lim</keyname><forenames>Teng Joon</forenames></author></authors><title>Optimal Save-Then-Transmit Protocol for Energy Harvesting Wireless
  Transmitters</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the design of a wireless communication device relying
exclusively on energy harvesting is considered. Due to the inability of
rechargeable energy sources to charge and discharge at the same time, a
constraint we term the energy half-duplex constraint, two rechargeable energy
storage devices (ESDs) are assumed so that at any given time, there is always
one ESD being recharged. The energy harvesting rate is assumed to be a random
variable that is constant over the time interval of interest. A
save-then-transmit (ST) protocol is introduced, in which a fraction of time
{\rho} (dubbed the save-ratio) is devoted exclusively to energy harvesting,
with the remaining fraction 1 - {\rho} used for data transmission. The ratio of
the energy obtainable from an ESD to the energy harvested is termed the energy
storage efficiency, {\eta}. We address the practical case of the secondary ESD
being a battery with {\eta} &lt; 1, and the main ESD being a super-capacitor with
{\eta} = 1. The optimal save-ratio that minimizes outage probability is
derived, from which some useful design guidelines are drawn. In addition, we
compare the outage performance of random power supply to that of constant power
supply over the Rayleigh fading channel. The diversity order with random power
is shown to be the same as that of constant power, but the performance gap can
be large. Furthermore, we extend the proposed ST protocol to wireless networks
with multiple transmitters. It is shown that the system-level outage
performance is critically dependent on the relationship between the number of
transmitters and the optimal save-ratio for single-channel outage minimization.
Numerical results are provided to validate our proposed study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1241</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1241</id><created>2012-04-05</created><authors><author><keyname>Kuribayashi</keyname><forenames>Shin-ichi</forenames></author></authors><title>Reducing Total Power Consumption Method in Cloud Computing Environments</title><categories>cs.NI cs.SY</categories><comments>16 pages</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.2, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread use of cloud computing services is expected to increase the
power consumed by ICT equipment in cloud computing environments rapidly. This
paper first identifies the need of the collaboration among servers, the
communication network and the power network, in order to reduce the total power
consumption by the entire ICT equipment in cloud computing environments. Five
fundamental policies for the collaboration are proposed and the algorithm to
realize each collaboration policy is outlined. Next, this paper proposes
possible signaling sequences to exchange information on power consumption
between network and servers, in order to realize the proposed collaboration
policy. Then, in order to reduce the power consumption by the network, this
paper proposes a method of estimating the volume of power consumption by all
network devices simply and assigning it to an individual user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1243</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1243</id><created>2012-04-05</created><authors><author><keyname>Kuribayashi</keyname><forenames>Shin-ichi</forenames></author></authors><title>Proposed congestion control method for cloud computing environments</title><categories>cs.NI cs.SY</categories><comments>16 pages</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.3, No.5, Sep 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As cloud computing services rapidly expand their customer base, it has become
important to share cloud resources, so as to provide them economically. In
cloud computing services, multiple types of resources, such as processing
ability, bandwidth and storage, need to be allocated simultaneously. If there
is a surge of requests, a competition will arise between these requests for the
use of cloud resources. This leads to the disruption of the service and it is
necessary to consider a measure to avoid or relieve congestion of cloud
computing environments.
  This paper proposes a new congestion control method for cloud computing
environments which reduces the size of required resource for congested resource
type instead of restricting all service requests as in the existing networks.
Next, this paper proposes the user service specifications for the proposed
congestion control method, and clarifies the algorithm to decide the optimal
size of required resource to be reduced, based on the load offered to the
system. It is demonstrated by simulation evaluations that the proposed method
can handle more requests compared with the conventional methods and relieve the
congestion. Then, this paper proposes to enhance the proposed method, so as to
enable the fair resource allocation among users in congested situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1245</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1245</id><created>2012-04-05</created><authors><author><keyname>Kuribayashi</keyname><forenames>Shin-ichi</forenames></author></authors><title>Proposed optimal LSP selection method in MPLS networks</title><categories>cs.NI cs.SY</categories><comments>10 pages</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.3, No.1, January 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Protocol Label Switching (MPLS) had been deployed by many data
networking service providers, including the next-generation mobile backhaul
networks, because of its undeniable potential in terms of virtual private
network (VPN) management, traffic engineering, etc. In MPLS networks, IP
packets are transmitted along a Label Switched Path (LSP) established between
edge nodes. To improve the efficiency of resource use in MPLS networks, it is
essential to utilize the LSPs efficiently.
  This paper proposes a method of selecting the optimal LSP pair from among
multiple LSP pairs which are established between the same pair of edge nodes,
on the assumption that both the upward and downward LSPs are established as a
pair (both-way operation). It is supposed that both upward and downward
bandwidths are allocated simultaneously in the selected LSP pair for each
service request. It is demonstrated by simulation evaluations that the proposal
method could reduce the total amount of the bandwidth required by up to 15%
compared with the conventional selection method. The proposed method can also
reuse the know-how and management tools in many existing networks which are
based on both-way operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1255</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1255</id><created>2012-04-05</created><updated>2012-11-09</updated><authors><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Kannan</keyname><forenames>Sampath</forenames></author></authors><title>The Exponential Mechanism for Social Welfare: Private, Truthful, and
  Nearly Optimal</title><categories>cs.GT</categories><comments>Appeared in FOCS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that for any mechanism design problem with the
objective of maximizing social welfare, the exponential mechanism can be
implemented as a truthful mechanism while still preserving differential
privacy. Our instantiation of the exponential mechanism can be interpreted as a
generalization of the VCG mechanism in the sense that the VCG mechanism is the
extreme case when the privacy parameter goes to infinity. To our knowledge,
this is the first general tool for designing mechanisms that are both truthful
and differentially private.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1259</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1259</id><created>2012-04-05</created><updated>2013-04-04</updated><authors><author><keyname>Hidasi</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Tikk</keyname><forenames>Domonkos</forenames></author></authors><title>Fast ALS-based tensor factorization for context-aware recommendation
  from implicit feedback</title><categories>cs.LG cs.IR cs.NA</categories><comments>Accepted for ECML/PKDD 2012, presented on 25th September 2012,
  Bristol, UK</comments><journal-ref>Proceedings of the 2012 European conference on Machine Learning
  and Knowledge Discovery in Databases - Volume Part II</journal-ref><doi>10.1007/978-3-642-33486-3_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Albeit, the implicit feedback based recommendation problem - when only the
user history is available but there are no ratings - is the most typical
setting in real-world applications, it is much less researched than the
explicit feedback case. State-of-the-art algorithms that are efficient on the
explicit case cannot be straightforwardly transformed to the implicit case if
scalability should be maintained. There are few if any implicit feedback
benchmark datasets, therefore new ideas are usually experimented on explicit
benchmarks. In this paper, we propose a generic context-aware implicit feedback
recommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensor
factorization learning method that scales linearly with the number of non-zero
elements in the tensor. The method also allows us to incorporate diverse
context information into the model while maintaining its computational
efficiency. In particular, we present two such context-aware implementation
variants of iTALS. The first incorporates seasonality and enables to
distinguish user behavior in different time intervals. The other views the user
history as sequential information and has the ability to recognize usage
pattern typical to certain group of items, e.g. to automatically tell apart
product types or categories that are typically purchased repetitively
(collectibles, grocery goods) or once (household appliances). Experiments
performed on three implicit datasets (two proprietary ones and an implicit
variant of the Netflix dataset) show that by integrating context-aware
information with our factorization framework into the state-of-the-art implicit
recommender algorithm the recommendation quality improves significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1276</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1276</id><created>2012-04-05</created><updated>2013-09-18</updated><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>Distribution-Dependent Sample Complexity of Large Margin Learning</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1011.5053</comments><journal-ref>S. Sabato, N. Srebro and N. Tishby, &quot;Distribution-Dependent Sample
  Complexity of Large Margin Learning&quot;, Journal of Machine Learning Research,
  14(Jul):2119-2149, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We obtain a tight distribution-specific characterization of the sample
complexity of large-margin classification with L2 regularization: We introduce
the margin-adapted dimension, which is a simple function of the second order
statistics of the data distribution, and show distribution-specific upper and
lower bounds on the sample complexity, both governed by the margin-adapted
dimension of the data distribution. The upper bounds are universal, and the
lower bounds hold for the rich family of sub-Gaussian distributions with
independent features. We conclude that this new quantity tightly characterizes
the true sample complexity of large-margin classification. To prove the lower
bound, we develop several new tools of independent interest. These include new
connections between shattering and hardness of learning, new properties of
shattering with linear classifiers, and a new lower bound on the smallest
eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our
results can be used to quantitatively compare large margin learning to other
learning rules, and to improve the effectiveness of methods that use sample
complexity bounds, such as active learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1277</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1277</id><created>2012-04-05</created><authors><author><keyname>Kumar</keyname><forenames>Vikram</forenames></author><author><keyname>Niyazi</keyname><forenames>Kamran</forenames></author><author><keyname>Mahe</keyname><forenames>Swapnil</forenames></author><author><keyname>Vyawahare</keyname><forenames>Swapnil</forenames></author></authors><title>Mouse Simulation Using Two Coloured Tapes</title><categories>cs.AI cs.CV</categories><comments>5 pages</comments><journal-ref>International Journal of Information Sciences and Techniques
  (IJIST) Vol.2, No.2, March 2012</journal-ref><doi>10.5121/ijist.2012.2206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach for Human Computer Interaction
(HCI) where, we control cursor movement using a real-time camera. Current
methods involve changing mouse parts such as adding more buttons or changing
the position of the tracking ball. Instead, our method is to use a camera and
computer vision technology, such as image segmentation and gesture recognition,
to control mouse tasks (left and right clicking, double-clicking, and
scrolling) and we show how it can perform everything as current mouse devices
can. The software will be developed in JAVA language. Recognition and pose
estimation in this system are user independent and robust as we will be using
colour tapes on our finger to perform actions. The software can be used as an
intuitive input interface to applications that require multi-dimensional
control e.g. computer games etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1290</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1290</id><created>2012-04-05</created><authors><author><keyname>Rhif</keyname><forenames>Ahmed</forenames></author></authors><title>A Sliding Mode Control for a Sensorless Tracker: Application on a
  Photovoltaic System</title><categories>cs.SY</categories><doi>10.5121/ijctcm.2012.2201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The photovoltaic sun tracker allows us to increase the energy production. The
sun tracker considered in this study has two degrees of freedom (2-DOF) and
especially specified by the lack of sensors. In this way, the tracker will have
as a set point the sun position at every second during the day for a period of
five years. After sunset, the tracker goes back to the initial position (which
of sunrise). The sliding mode control (SMC) will be applied to ensure at best
the tracking mechanism and, in another hand, the sliding mode observer will
replace the velocity sensor which suffers from a lot of measurement
disturbances. Experimental measurements show that this autonomic dual axis Sun
Tracker increases the power production by over 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1292</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1292</id><created>2012-04-05</created><authors><author><keyname>Biasse</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>An L(1/3) algorithm for discrete logarithm computation and principality
  testing in certain number fields</title><categories>math.NT cs.CC</categories><comments>13 pages</comments><msc-class>Primary: 58F15, 58F17, Secondary: 53C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the complexity of solving the discrete logarithm problem and of
testing the principality of ideals in a certain class of number fields. We
achieve the subexponential complexity in $O(L(1/3,O(1)))$ when both the
discriminant and the degree of the extension tend to infinity by using
techniques due to Enge, Gaudry and Thom\'{e} in the context of algebraic curves
over finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1294</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1294</id><created>2012-04-05</created><authors><author><keyname>Biasse</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Fieker</keyname><forenames>Claus</forenames></author></authors><title>New techniques for computing the ideal class group and a system of
  fundamental units in number fields</title><categories>math.NT cs.SC</categories><comments>17 pages</comments><msc-class>Primary 54C40, 14E20, Secondary 46E25, 20C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm for computing the ideal class group, the
regulator and a system of fundamental units in number fields under the
generalized Riemann hypothesis. We use sieving techniques adapted from the
number field sieve algorithm to derive relations between elements of the ideal
class group, and $p$-adic approximations to manage the loss of precision during
the computation of units. This new algorithm is particularily efficient for
number fields of small degree for which a speed-up of an order of magnitude is
achieved with respect to the standard methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1296</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1296</id><created>2012-04-05</created><authors><author><keyname>Eichler</keyname><forenames>J&#xf6;rn</forenames></author></authors><title>Towards a Security Engineering Process Model for Electronic Business
  Processes</title><categories>cs.CR</categories><comments>Ninth European Dependable Computing Conference (EDCC 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business process management (BPM) and accompanying systems aim at enabling
enterprises to become adaptive. In spite of the dependency of enterprises on
secure business processes, BPM languages and techniques provide only little
support for security. Several complementary approaches have been proposed for
security in the domain of BPM. Nevertheless, support for a systematic procedure
for the development of secure electronic business processes is still missing.
In this paper, we pinpoint the need for a security engineering process model in
the domain of BPM and identify key requirements for such process model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1298</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1298</id><created>2012-04-05</created><authors><author><keyname>Biasse</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Fieker</keyname><forenames>Claus</forenames></author></authors><title>A polynomial time algorithm for computing the HNF of a module over the
  integers of a number field</title><categories>cs.SC cs.CC math.NT</categories><comments>11 pages</comments><msc-class>Primary 54C40, 14E20, Secondary 46E25, 20C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a variation of the modular algorithm for computing the Hermite
Normal Form of an $\OK$-module presented by Cohen, where $\OK$ is the ring of
integers of a number field K. The modular strategy was conjectured to run in
polynomial time by Cohen, but so far, no such proof was available in the
literature. In this paper, we provide a new method to prevent the coefficient
explosion and we rigorously assess its complexity with respect to the size of
the input and the invariants of the field K.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1300</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1300</id><created>2012-04-05</created><authors><author><keyname>Biasse</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Improvements in the computation of ideal class groups of imaginary
  quadratic number fields</title><categories>math.NT cs.SC</categories><comments>14 pages, 5 figures</comments><msc-class>Primary: 58F15, 58F17, Secondary: 53C35</msc-class><journal-ref>J.-F. Biasse, Practical improvements to ideal class group
  computation in imaginary quadratic number fields, Advances in Mathematics of
  Comunications 4 (2), 2010, pp. 141-154</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate improvements to the algorithm for the computation of ideal
class groups described by Jacobson in the imaginary quadratic case. These
improvements rely on the large prime strategy and a new method for performing
the linear algebra phase. We achieve a significant speed-up and are able to
compute ideal class groups with discriminants of 110 decimal digits in less
than a week.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1336</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1336</id><created>2012-04-05</created><authors><author><keyname>Hoque</keyname><forenames>Mohammad Sazzadul</forenames></author><author><keyname>Mukit</keyname><forenames>Md. Abdul</forenames></author><author><keyname>Bikas</keyname><forenames>Md. Abu Naser</forenames></author></authors><title>An Implementation of Intrusion Detection System Using Genetic Algorithm</title><categories>cs.CR cs.NE cs.NI</categories><journal-ref>International Journal of Network Security &amp; Its Applications,
  Volume 4, Number 2, pages 109 - 120, March 2012</journal-ref><doi>10.5121/ijnsa.2012.4208</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nowadays it is very important to maintain a high level security to ensure
safe and trusted communication of information between various organizations.
But secured data communication over internet and any other network is always
under threat of intrusions and misuses. So Intrusion Detection Systems have
become a needful component in terms of computer and network security. There are
various approaches being utilized in intrusion detections, but unfortunately
any of the systems so far is not completely flawless. So, the quest of
betterment continues. In this progression, here we present an Intrusion
Detection System (IDS), by applying genetic algorithm (GA) to efficiently
detect various types of network intrusions. Parameters and evolution processes
for GA are discussed in details and implemented. This approach uses evolution
theory to information evolution in order to filter the traffic data and thus
reduce the complexity. To implement and measure the performance of our system
we used the KDD99 benchmark dataset and obtained reasonable detection rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1351</identifier>
 <datestamp>2014-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1351</id><created>2012-04-05</created><updated>2012-06-20</updated><authors><author><keyname>Arnold</keyname><forenames>Douglas N.</forenames></author><author><keyname>Cohn</keyname><forenames>Henry</forenames></author></authors><title>Mathematicians take a stand</title><categories>math.HO cs.GL</categories><comments>5 pages</comments><proxy>Henry Cohn</proxy><journal-ref>Notices of the American Mathematical Society 59 (2012), 828-833</journal-ref><doi>10.1090/noti857</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the reasons for the ongoing boycott of the publisher Elsevier. We
examine Elsevier's pricing and bundling policies, restrictions on dissemination
by authors, and lapses in ethics and peer review, and we conclude with thoughts
about the future of mathematical publishing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1367</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1367</id><created>2012-04-05</created><updated>2013-03-29</updated><authors><author><keyname>Bhowmick</keyname><forenames>Abhishek</forenames></author><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>New Lower Bounds for Matching Vector Codes</title><categories>cs.CC cs.DM math.CO</categories><comments>Fixed typos and small bugs</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Matching Vector (MV) family modulo $m$ is a pair of ordered lists
$U=(u_1,...,u_t)$ and $V=(v_1,...,v_t)$ where $u_i,v_j \in \mathbb{Z}_m^n$ with
the following inner product pattern: for any $i$, $&lt; u_i,v_i&gt;=0$, and for any
$i \ne j$, $&lt; u_i,v_j&gt; \ne 0$. A MV family is called $q$-restricted if inner
products $&lt; u_i,v_j&gt;$ take at most $q$ different values.
  Our interest in MV families stems from their recent application in the
construction of sub-exponential locally decodable codes (LDCs). There,
$q$-restricted MV families are used to construct LDCs with $q$ queries, and
there is special interest in the regime where $q$ is constant. When $m$ is a
prime it is known that such constructions yield codes with exponential block
length. However, for composite $m$ the behaviour is dramatically different. A
recent work by Efremenko [STOC 2009] (based on an approach initiated by
Yekhanin [JACM 2008]) gives the first sub-exponential LDC with constant
queries. It is based on a construction of a MV family of super-polynomial size
by Grolmusz [Combinatorica 2000] modulo composite $m$.
  In this work, we prove two lower bounds on the block length of LDCs which are
based on black box construction using MV families. When $q$ is constant (or
sufficiently small), we prove that such LDCs must have a quadratic block
length. When the modulus $m$ is constant (as it is in the construction of
Efremenko) we prove a super-polynomial lower bound on the block-length of the
LDCs, assuming a well-known conjecture in additive combinatorics, the
polynomial Freiman-Ruzsa conjecture over $\mathbb{Z}_m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1369</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1369</id><created>2012-04-05</created><authors><author><keyname>Olsen</keyname><forenames>Martin</forenames></author><author><keyname>Viglas</keyname><forenames>Anastasios</forenames></author><author><keyname>Zvedeniouk</keyname><forenames>Ilia</forenames></author></authors><title>An approximation algorithm for the link building problem</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of maximizing the PageRank of a given
target node in a graph by adding $k$ new links. We consider the case that the
new links must point to the given target node (backlinks). Previous work shows
that this problem has no fully polynomial time approximation schemes unless
$P=NP$. We present a polynomial time algorithm yielding a PageRank value within
a constant factor from the optimal. We also consider the naive algorithm where
we choose backlinks from nodes with high PageRank values compared to the
outdegree and show that the naive algorithm performs much worse on certain
graphs compared to the constant factor approximation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1372</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1372</id><created>2012-04-05</created><authors><author><keyname>Moelius</keyname><forenames>Samuel E.</forenames><suffix>III</suffix></author></authors><title>Characteristics of Minimal Effective Programming Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Rogers semilattice of effective programming systems (epses) is the
collection of all effective numberings of the partial computable functions
ordered such that \theta\ is less than or equal to \psi\ whenever
\theta-programs can be algorithmically translated into \psi-programs. Herein,
it is shown that an eps \psi\ is minimal in this ordering if and only if, for
each translation function t into \psi, there exists a computably enumerable
equivalence relation (ceer) R such that (i) R is a subrelation of \psi's
program equivalence relation, and (ii) R equates each \psi-program to some
program in the range of t. It is also shown that there exists a minimal eps for
which no single such R does the work for all such t. In fact, there exists a
minimal eps \psi\ such that, for each ceer R, either R contradicts \psi's
program equivalence relation, or there exists a translation function t into
\psi\ such that the range of t fails to intersect infinitely many of R's
equivalence classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1373</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1373</id><created>2012-04-05</created><authors><author><keyname>Borges</keyname><forenames>Miguel</forenames></author><author><keyname>Jesus</keyname><forenames>Paulo</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author><author><keyname>Almeida</keyname><forenames>Paulo S&#xe9;rgio</forenames></author></authors><title>Spectra: Robust Estimation of Distribution Functions in Networks</title><categories>cs.DC cs.DS</categories><comments>Full version of the paper published at 12th IFIP International
  Conference on Distributed Applications and Interoperable Systems (DAIS),
  Stockholm (Sweden), June 2012</comments><acm-class>C.2.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed aggregation allows the derivation of a given global aggregate
property from many individual local values in nodes of an interconnected
network system. Simple aggregates such as minima/maxima, counts, sums and
averages have been thoroughly studied in the past and are important tools for
distributed algorithms and network coordination. Nonetheless, this kind of
aggregates may not be comprehensive enough to characterize biased data
distributions or when in presence of outliers, making the case for richer
estimates of the values on the network. This work presents Spectra, a
distributed algorithm for the estimation of distribution functions over large
scale networks. The estimate is available at all nodes and the technique
depicts important properties, namely: robust when exposed to high levels of
message loss, fast convergence speed and fine precision in the estimate. It can
also dynamically cope with changes of the sampled local property, not requiring
algorithm restarts, and is highly resilient to node churn. The proposed
approach is experimentally evaluated and contrasted to a competing state of the
art distribution aggregation technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1383</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1383</id><created>2012-04-05</created><authors><author><keyname>Lahby</keyname><forenames>Mohamed</forenames></author><author><keyname>Cherkaoui</keyname><forenames>Leghris</forenames></author><author><keyname>Adib</keyname><forenames>Abdellah</forenames></author></authors><title>An Intelligent Network Selection Strategy Based on MADM Methods in
  Heterogeneous Networks</title><categories>cs.NI</categories><comments>14 pages, 21 figures</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012, 83-96</journal-ref><doi>10.5121/ijwmn.2012.4106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing service continuity to the end users with best quality is a very
important issue in the next generation wireless communications. With the
evolution of the mobile devices towards a multimode architecture and the
coexistence of multitude of radio access technologies (RAT's), the users are
able to benefit simultaneously from these RAT's. However, the major issue in
heterogeneous wireless communications is how to choose the most suitable access
network for mobile's user which can be used as long as possible for
communication. To achieve this issue, this paper proposes an intelligent
network selection strategy which combines two multi attribute decision making
(MADM) methods such as analytic network process (ANP) and the technique for
order preference by similarity to an ideal solution (TOPSIS) method. The ANP
method is used to find the differentiate weights of available networks by
considering each criterion and the TOPSIS method is applied to rank the
alternatives. Our new strategy for network selection can dealing with the
limitations of MADM methods which are the ranking abnormality and the ping-ponf
effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1385</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1385</id><created>2012-04-05</created><authors><author><keyname>Susanto</keyname><forenames>Heru</forenames></author><author><keyname>Muhaya</keyname><forenames>Fahad Bin</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Tuan</keyname><forenames>Yong Chee</forenames></author></authors><title>Refinement of Strategy and Technology Domains STOPE View on ISO 27001</title><categories>cs.CR</categories><comments>Susanto et al, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is imperative for organizations to us Information Security Management
System (ISMS) to effectively manage their information assets. ISMS starts with
a set of policies that dictate the usage computer resources. It starts with the
&quot;21 essential security controls&quot; of ISO 27001, which give the basic standard
requirements of information security management. Our research is concerned with
the assessment of the application of these controls to organizations. STOPE
(Strategy, Technology Organization, People and Environment) methodologies were
used to integrated domains as a framework for this assessment. The controls are
mapped on these domains and subsequently refined into &quot;246 simple and easily
comprehended elements&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1393</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1393</id><created>2012-04-05</created><authors><author><keyname>Yamaguchi</keyname><forenames>Koichiro</forenames></author><author><keyname>Hazan</keyname><forenames>Tamir</forenames></author><author><keyname>McAllester</keyname><forenames>David</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author></authors><title>Continuous Markov Random Fields for Robust Stereo Estimation</title><categories>cs.CV</categories><acm-class>I.2.10; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel slanted-plane MRF model which reasons
jointly about occlusion boundaries as well as depth. We formulate the problem
as the one of inference in a hybrid MRF composed of both continuous (i.e.,
slanted 3D planes) and discrete (i.e., occlusion boundaries) random variables.
This allows us to define potentials encoding the ownership of the pixels that
compose the boundary between segments, as well as potentials encoding which
junctions are physically possible. Our approach outperforms the
state-of-the-art on Middlebury high resolution imagery as well as in the more
challenging KITTI dataset, while being more efficient than existing slanted
plane MRF-based methods, taking on average 2 minutes to perform inference on
high resolution imagery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1398</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1398</id><created>2012-04-05</created><authors><author><keyname>Xie</keyname><forenames>Xiaohu</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author><author><keyname>Borno</keyname><forenames>Mazen Al</forenames></author></authors><title>Partial LLL Reduction</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures; IEEE GLOBECOM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lenstra-Lenstra-Lovasz (LLL) reduction has wide applications in digital
communications. It can greatly improve the speed of the sphere decoding (SD)
algorithms for solving an integer least squares (ILS) problem and the
performance of the Babai integer point, a suboptimal solution to the ILS
problem. Recently Ling and Howgrave-Graham proposed the so-called effective LLL
(ELLL) reduction. It has less computational complexity than LLL, while it has
the same effect on the performance of the Babai integer point as LLL. In this
paper we propose a partial LLL (PLLL) reduction. PLLL avoids the numerical
stability problem with ELLL, which may result in very poor performance of the
Babai integer point. Furthermore, numerical simulations indicated that it is
faster than ELLL. We also show that in theory PLLL and ELLL have the same
effect on the search speed of a typical SD algorithm as LLL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1400</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1400</id><created>2012-04-05</created><updated>2012-10-03</updated><authors><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Anderson</keyname><forenames>Brian Do</forenames></author></authors><title>Connectivity of Large Wireless Networks under A Generic Connection Model</title><categories>cs.NI cs.IT math.IT</categories><comments>accepted to appear in IEEE Transactions on Information Theory. arXiv
  admin note: substantial text overlap with arXiv:1103.1994, arXiv:1012.5693,
  arXiv:1103.1991, arXiv:1012.5723</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a necessary and sufficient condition for a random network
with nodes Poissonly distributed on a unit square and a pair of nodes directly
connected following a generic random connection model to be asymptotically
almost surely connected. The results established in this paper expand recent
results obtained for connectivity of random geometric graphs from the unit disk
model and the fewer results from the log-normal model to the more generic and
more practical random connection model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1406</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1406</id><created>2012-04-06</created><authors><author><keyname>Roul</keyname><forenames>R. K.</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>An Effective Information Retrieval for Ambiguous Query</title><categories>cs.IR</categories><comments>11 Pages, 1 figure</comments><journal-ref>AJCSIT, Vol. 2, No. 3, P. 26-30, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engine returns thousands of web pages for a single user query, in
which most of them are not relevant. In this context, effective information
retrieval from the expanding web is a challenging task, in particular, if the
query is ambiguous. The major question arises here is that how to get the
relevant pages for an ambiguous query. We propose an approach for the effective
result of an ambiguous query by forming community vector based on association
concept of data minning using vector space model and the freedictionary. We
develop clusters by computing the similarity between community vectors and
document vectors formed from the extracted web pages by the search engine. We
use Gensim package to implement the algorithm because of its simplicity and
robust nature. Analysis shows that our approach is an effective way to form
clusters for an ambiguous query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1407</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1407</id><created>2012-04-06</created><authors><author><keyname>Breen</keyname><forenames>Stephen</forenames></author><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author></authors><title>Column Reordering for Box-Constrained Integer Least Squares Problems</title><categories>cs.IT math.IT</categories><comments>6 pages; IEEE GLOBECOM 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The box-constrained integer least squares problem (BILS) arises in MIMO
wireless communications applications. Typically a sphere decoding algorithm (a
tree search algorithm) is used to solve the problem. In order to make the
search algorithm more efficient, the columns of the channel matrix in the BILS
problem have to be reordered. To our knowledge, there are currently two
algorithms for column reordering that provide the best known results. Both use
all available information, but they were derived respectively from geometric
and algebraic points of view and look different. In this paper we modify one to
make it more computationally efficient and easier to comprehend. Then we prove
the modified one and the other actually give the same column reordering in
theory. Finally we propose a new mathematically equivalent algorithm, which is
more computationally efficient and is still easy to understand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1413</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1413</id><created>2012-04-06</created><authors><author><keyname>Suri</keyname><forenames>Pushpa R.</forenames></author><author><keyname>Taneja</keyname><forenames>Harmunish</forenames></author></authors><title>An integrated ranking algorithm for efficient information computing in
  social networks</title><categories>cs.SI</categories><comments>14 pages, International Journal on Web Service Computing (IJWSC),
  Vol.3, No.1, March 2012</comments><doi>10.5121/ijwsc.2012.3103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have ensured the expanding disproportion between the face of
WWW stored traditionally in search engine repositories and the actual ever
changing face of Web. Exponential growth of web users and the ease with which
they can upload contents on web highlights the need of content controls on
material published on the web. As definition of search is changing,
socially-enhanced interactive search methodologies are the need of the hour.
Ranking is pivotal for efficient web search as the search performance mainly
depends upon the ranking results. In this paper new integrated ranking model
based on fused rank of web object based on popularity factor earned over only
valid interlinks from multiple social forums is proposed. This model identifies
relationships between web objects in separate social networks based on the
object inheritance graph. Experimental study indicates the effectiveness of
proposed Fusion based ranking algorithm in terms of better search results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1414</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1414</id><created>2012-04-06</created><authors><author><keyname>Legnain</keyname><forenames>Rajab M.</forenames></author><author><keyname>Hafez</keyname><forenames>Roshdy H. M.</forenames></author><author><keyname>Legnain</keyname><forenames>Abdelgader M.</forenames></author></authors><title>Improved Spatial Modulation for High Spectral Efficiency</title><categories>cs.SY cs.IT math.IT</categories><comments>7 pages, 4 figures, 1 table, International Journal of Distributed and
  Parallel Systems (IJDPS) Vol.3, No.2, March 2012</comments><doi>10.5121/ijdps.2012.3202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial Modulation (SM) is a technique that can enhance the capacity of MIMO
schemes by exploiting the index of transmit antenna to convey information bits.
In this paper, we describe this technique, and present a new MIMO transmission
scheme that combines SM and spatial multiplexing. In the basic form of SM, only
one out of MT available antennas is selected for transmission in any given
symbol interval. We propose to use more than one antenna to transmit several
symbols simultaneously. This would increase the spectral efficiency. At the
receiver, an optimal detector is employed to jointly estimate the transmitted
symbols as well as the index of the active transmit antennas. In this paper we
evaluate the performance of this scheme in an uncorrelated Rayleigh fading
channel. The simulations results show that the proposed scheme outperforms the
optimal SM and V-BLAST (Vertical Bell Laboratories Layered space-time at high
signal-to-noise ratio (SNR). For example, if we seek a spectral efficiency of 8
bits/s/Hz at bit error rate (BER) of 10^-5, the proposed scheme provides 5dB
and 7dB improvements over SM and V-BLAST, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1417</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1417</id><created>2012-04-06</created><authors><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author><author><keyname>Philip</keyname><forenames>Geevarghese</forenames></author></authors><title>A single-exponential FPT algorithm for the $K_4$-minor cover problem</title><categories>cs.DS cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an input graph G and an integer k, the parameterized K_4-minor cover
problem asks whether there is a set S of at most k vertices whose deletion
results in a K_4-minor-free graph, or equivalently in a graph of treewidth at
most 2. This problem is inspired by two well-studied parameterized vertex
deletion problems, Vertex Cover and Feedback Vertex Set, which can also be
expressed as Treewidth-t Vertex Deletion problems: t=0 for Vertex Cover and t=1
for Feedback Vertex Set. While a single-exponential FPT algorithm has been
known for a long time for \textsc{Vertex Cover}, such an algorithm for Feedback
Vertex Set was devised comparatively recently. While it is known to be unlikely
that Treewidth-t Vertex Deletion can be solved in time c^{o(k)}.n^{O(1)}, it
was open whether the K_4-minor cover problem could be solved in
single-exponential FPT time, i.e. in c^k.n^{O(1)} time. This paper answers this
question in the affirmative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1420</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1420</id><created>2012-04-06</created><authors><author><keyname>Alshehab</keyname><forenames>Abdullah</forenames></author><author><keyname>Wu</keyname><forenames>Chiu Tung</forenames></author><author><keyname>Kobayashi</keyname><forenames>Nao</forenames></author><author><keyname>Sok</keyname><forenames>Sikieng</forenames></author><author><keyname>Shimamoto</keyname><forenames>Shigeru</forenames></author></authors><title>Intra-bodyhybrid communication scheme for healthcare systems</title><categories>cs.ET</categories><comments>International Journal on Bioinformatics &amp; Biosciences (IJBB) Vol.2,
  No.1, March 2012</comments><doi>10.5121/ijbb.2012.21011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intra-body communication (IBC) is a type of Body Area Network (BAN)that
utilizes human body as the medium for data transmission. Thelow power
requirements of intra-body communication (IBC) as compared to near field
electromagnetic waves showed that it can be a suitable solution for Medical
Body Area Networks (MBANs) in a mobile health care system.In this paper, we
investigate the transmission characteristics of the human body as a conductor
of signals by considering different data transmission rates of multi-point to
point network in order to reduce overall power consumption of the
BAN.Furthermore, we utilize IBC and propose a new scheme to combines Slotted
ALOHA, TDMA, and Reservation ALOHA together to increase the throughput and
decrease the delay. By using our new hybrid scheme with the movable boundary
designed for health status monitoring, we are able to increase the efficiency
of data transmission by prioritizing the more critical data from the sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1423</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1423</id><created>2012-04-06</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>D-iteration: application to differential equations</title><categories>cs.NA math.NA</categories><comments>5 pages</comments><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study how the D-iteration algorithm can be applied to
numerically solve the differential equations such as heat equation in 2D or 3D.
The method can be applied on the class of problems that can be addressed by the
Gauss-Seidel iteration, based on the linear approximation of the differential
equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1425</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1425</id><created>2012-04-06</created><authors><author><keyname>Makhlughian</keyname><forenames>Molood</forenames></author><author><keyname>Hashemi</keyname><forenames>Seyyed Mohsen</forenames></author><author><keyname>Rastegari</keyname><forenames>Yousef</forenames></author><author><keyname>Pejman</keyname><forenames>Emad</forenames></author></authors><title>Web service selection based on ranking of qos using associative
  classification</title><categories>cs.SE</categories><comments>14 pages; International Journal on Web Service Computing (IJWSC),
  March 2012, Volume 3, Number 1</comments><doi>10.5121/ijwsc.2012.3101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the explosive growth of the number of services published over the
Internet, it is difficult to select satisfactory web services among the
candidate web services which provide similar functionalities. Quality of
Service (QoS) is considered as the most important non-functional criterion for
service selection. But this criterion is no longer considered as the only
criterion to rank web services, satisfying user's preferences. The similarity
measure (outputs-inputs similarity) between concepts based on ontology in an
interconnected network of semantic Web services involved in a composition can
be used as a distinguishing criterion to estimate the semantic quality of
selected services for the composite service. Coupling the semantic similarity
as the functional aspect and quality of services allows us to further constrain
and select services for the valid composite services. In this paper, we present
an overall service selection and ranking framework which firstly classify
candidate web services to different QoS levels respect to user's QoS
requirements and preferences with an Associative Classification algorithm and
then rank the most qualified candidate services based on their functional
quality through semantic matching. The experimental results show that proposed
framework can satisfy service requesters' non-functional requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1428</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1428</id><created>2012-04-06</created><authors><author><keyname>Thai</keyname><forenames>Tuan Tran</forenames><affiliation>DMIA</affiliation></author><author><keyname>Lochin</keyname><forenames>Emmanuel</forenames><affiliation>DMIA</affiliation></author><author><keyname>Lacan</keyname><forenames>Jerome</forenames><affiliation>DMIA</affiliation></author></authors><title>Online multipath convolutional coding for real-time transmission</title><categories>cs.NI cs.MM</categories><comments>Online multipath convolutional coding for real-time transmission
  (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of multipath multimedia streaming proposals use Forward Error Correction
(FEC) approach to protect from packet losses. However, FEC does not sustain
well burst of losses even when packets from a given FEC block are spread over
multiple paths. In this article, we propose an online multipath convolutional
coding for real-time multipath streaming based on an on-the-fly coding scheme
called Tetrys. We evaluate the benefits brought out by this coding scheme
inside an existing FEC multipath load splitting proposal known as Encoded
Multipath Streaming (EMS). We demonstrate that Tetrys consistently outperforms
FEC in both uniform and burst losses with EMS scheme. We also propose a
modification of the standard EMS algorithm that greatly improves the
performance in terms of packet recovery. Finally, we analyze different
spreading policies of the Tetrys redundancy traffic between available paths and
observe that the longer propagation delay path should be preferably used to
carry repair packets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1433</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1433</id><created>2012-04-06</created><authors><author><keyname>Hassan</keyname><forenames>Ahmed Hassan M.</forenames></author><author><keyname>Dai</keyname><forenames>Bin</forenames></author><author><keyname>Huang</keyname><forenames>Benxiong</forenames></author><author><keyname>Eisa</keyname><forenames>Edriss</forenames></author><author><keyname>Azhar</keyname><forenames>Muhammad</forenames></author></authors><title>Relay selection for multiple access relay channel with decode-forward
  and analog network coding</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures; International Journal of Distributed and
  Parallel Systems (IJDPS) Vol.3, No.2, March 2012</comments><doi>10.5121/ijdps.2012.3201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a relay selection for decode-and-forward based on network
coding (DF-NC) and analog-NC protocols in general scheme of cellular network
system. In the propose scheme the two source node simultaneously transmit their
own information to all the relays as well as the destination node, and then, a
single relay i.e. best with a minimum symbol error rate (SER) will be selected
to forward the new version of the received signal. Simulation results show
that, the DF-NC scheme with considerable performance has exactness over
analog-NC scheme. To improve the system performance, optimal power allocation
between the two sources and the best relay is determined based on the
asymptotic SER. By increasing the number of relays node, the optimum power
allocation achieve better performance than asymptotic SER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1437</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1437</id><created>2012-04-06</created><authors><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Fast projections onto mixed-norm balls with applications</title><categories>stat.ML cs.LG math.OC</categories><comments>Preprint of paper under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint sparsity offers powerful structural cues for feature selection,
especially for variables that are expected to demonstrate a &quot;grouped&quot; behavior.
Such behavior is commonly modeled via group-lasso, multitask lasso, and related
methods where feature selection is effected via mixed-norms. Several mixed-norm
based sparse models have received substantial attention, and for some cases
efficient algorithms are also available. Surprisingly, several constrained
sparse models seem to be lacking scalable algorithms. We address this
deficiency by presenting batch and online (stochastic-gradient) optimization
methods, both of which rely on efficient projections onto mixed-norm balls. We
illustrate our methods by applying them to the multitask lasso. We conclude by
mentioning some open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1458</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1458</id><created>2012-04-06</created><authors><author><keyname>Bartsch</keyname><forenames>Steffen</forenames></author><author><keyname>Sohr</keyname><forenames>Karsten</forenames></author><author><keyname>Bunke</keyname><forenames>Michaela</forenames></author><author><keyname>Hofrichter</keyname><forenames>Oliver</forenames></author><author><keyname>Berger</keyname><forenames>Bernhard</forenames></author></authors><title>The Transitivity of Trust Problem in the Interaction of Android
  Applications</title><categories>cs.CR cs.HC</categories><report-no>TZI-2012-64</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phones have developed into complex platforms with large numbers of
installed applications and a wide range of sensitive data. Application security
policies limit the permissions of each installed application. As applications
may interact, restricting single applications may create a false sense of
security for the end users while data may still leave the mobile phone through
other applications. Instead, the information flow needs to be policed for the
composite system of applications in a transparent and usable manner. In this
paper, we propose to employ static analysis based on the software architecture
and focused data flow analysis to scalably detect information flows between
components. Specifically, we aim to reveal transitivity of trust problems in
multi-component mobile platforms. We demonstrate the feasibility of our
approach with Android applications, although the generalization of the analysis
to similar composition-based architectures, such as Service-oriented
Architecture, can also be explored in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1461</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1461</id><created>2012-04-06</created><authors><author><keyname>McEwan</keyname><forenames>Ian</forenames></author><author><keyname>Sheets</keyname><forenames>David</forenames></author><author><keyname>Gustavson</keyname><forenames>Stefan</forenames></author><author><keyname>Richardson</keyname><forenames>Mark</forenames></author></authors><title>Efficient computational noise in GLSL</title><categories>cs.GR</categories><doi>10.1080/2151237X.2012.649621</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present GLSL implementations of Perlin noise and Perlin simplex noise that
run fast enough for practical consideration on current generation GPU hardware.
The key benefits are that the functions are purely computational, i.e. they use
neither textures nor lookup tables, and that they are implemented in GLSL
version 1.20, which means they are compatible with all current GLSL-capable
platforms, including OpenGL ES 2.0 and WebGL 1.0. Their performance is on par
with previously presented GPU implementations of noise, they are very
convenient to use, and they scale well with increasing parallelism in present
and upcoming GPU architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1464</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1464</id><created>2012-04-06</created><authors><author><keyname>Gerbner</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author><author><keyname>Wiener</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Density-based group testing</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a new, generalized version of the well-known group
testing problem. In the classical model of group testing we are given n
objects, some of which are considered to be defective. We can test certain
subsets of the objects whether they contain at least one defective element. The
goal is usually to find all defectives using as few tests as possible. In our
model the presence of defective elements in a test set Q can be recognized if
and only if their number is large enough compared to the size of Q. More
precisely for a test Q the answer is 'yes' if and only if there are at least
\alpha |Q| defective elements in Q for some fixed \alpha.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1467</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1467</id><created>2012-04-06</created><authors><author><keyname>Mohammadi</keyname><forenames>Ali Soltan</forenames></author><author><keyname>Asadzadeh</keyname><forenames>L.</forenames></author><author><keyname>Rezaee</keyname><forenames>D. D.</forenames></author></authors><title>Learning Fuzzy {\beta}-Certain and {\beta}-Possible rules from
  incomplete quantitative data by rough sets</title><categories>cs.DS cs.LG</categories><comments>hi thanks for attention</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rough-set theory proposed by Pawlak, has been widely used in dealing with
data classification problems. The original rough-set model is, however, quite
sensitive to noisy data. Tzung thus proposed deals with the problem of
producing a set of fuzzy certain and fuzzy possible rules from quantitative
data with a predefined tolerance degree of uncertainty and misclassification.
This model allowed, which combines the variable precision rough-set model and
the fuzzy set theory, is thus proposed to solve this problem. This paper thus
deals with the problem of producing a set of fuzzy certain and fuzzy possible
rules from incomplete quantitative data with a predefined tolerance degree of
uncertainty and misclassification. A new method, incomplete quantitative data
for rough-set model and the fuzzy set theory, is thus proposed to solve this
problem. It first transforms each quantitative value into a fuzzy set of
linguistic terms using membership functions and then finding incomplete
quantitative data with lower and the fuzzy upper approximations. It second
calculates the fuzzy {\beta}-lower and the fuzzy {\beta}-upper approximations.
The certain and possible rules are then generated based on these fuzzy
approximations. These rules can then be used to classify unknown objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1495</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1495</id><created>2012-04-06</created><authors><author><keyname>Charfi</keyname><forenames>Faiza</forenames></author><author><keyname>Bouyahi</keyname><forenames>Mohamed</forenames></author></authors><title>Performance evaluation of beacon enabled IEEE 802.15.4 under NS2</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand for real-time applications has made the Quality of
Service (Qos) support for wireless sensor networks (WSN) a fairly new research
framework. In this paper, we propose an extended model of the Beacon enabled
IEEE 802.15.4 including the Guaranteed Time Slot GTS allocation mechanism in
the aim to analyze and evaluate network performances. Series of extensive
simulations were performed to analyze the impact of the Beacon Order BO and the
Superframe Order SO on the network performance based on commonly known metrics.
In particular, we examine data packet delivery performance and the throughput
for different duty cycle rates. Also, we analyze the impact of the number of
nodes on collision probability. Thus, for high number of nodes, collision
becomes higher and the reachability begins to decline slightly. We discuss and
compare simulation results conducted under various parameter settings to the
IEEE 802.11 network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1505</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1505</id><created>2012-04-06</created><updated>2013-01-18</updated><authors><author><keyname>Kerenidis</keyname><forenames>Iordanis</forenames></author><author><keyname>Laplante</keyname><forenames>Sophie</forenames></author><author><keyname>Lerays</keyname><forenames>Virginie</forenames></author><author><keyname>Roland</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Xiao</keyname><forenames>David</forenames></author></authors><title>Lower bounds on information complexity via zero-communication protocols
  and applications</title><categories>cs.CC quant-ph</categories><comments>22 pages</comments><journal-ref>In 53rd Annual IEEE Symposium on Foundations of Computer Science
  (FOCS'12), pages 500-509, 2012</journal-ref><doi>10.1109/FOCS.2012.68</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that almost all known lower bound methods for communication
complexity are also lower bounds for the information complexity. In particular,
we define a relaxed version of the partition bound of Jain and Klauck and prove
that it lower bounds the information complexity of any function. Our relaxed
partition bound subsumes all norm based methods (e.g. the factorization norm
method) and rectangle-based methods (e.g. the rectangle/corruption bound, the
smooth rectangle bound, and the discrepancy bound), except the partition bound.
Our result uses a new connection between rectangles and zero-communication
protocols where the players can either output a value or abort. We prove the
following compression lemma: given a protocol for a function f with information
complexity I, one can construct a zero-communication protocol that has
non-abort probability at least 2^{-O(I)} and that computes f correctly with
high probability conditioned on not aborting. Then, we show how such a
zero-communication protocol relates to the relaxed partition bound. We use our
main theorem to resolve three of the open questions raised by Braverman. First,
we show that the information complexity of the Vector in Subspace Problem is
{\Omega}(n^{1/3}), which, in turn, implies that there exists an exponential
separation between quantum communication complexity and classical information
complexity. Moreover, we provide an {\Omega}(n) lower bound on the information
complexity of the Gap Hamming Distance Problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1516</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1516</id><created>2012-04-06</created><authors><author><keyname>Bawa</keyname><forenames>Rajesh Kumar</forenames></author><author><keyname>Sharma</keyname><forenames>Gaurav</forenames></author></authors><title>Reliable Resource Selection in Grid Environment</title><categories>cs.DC</categories><comments>IJGCA, March 2012, Volume 3, Number 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary concern in area of computational grid is security and resources.
Most of the existing grids address this problem by authenticating the users,
hosts and their interactions in an appropriate manner. A secured system is
compulsory for the efficient utilization of grid services. The high degree of
strangeness has been identified as the problem factors in the secured selection
of grid. Without the assurance of a higher degree of trust relationship,
competent resource selection and utilization cannot be achieved. In this paper
we proposed an approach which is providing reliability and reputation aware
security for resource selection in grid environment. In this approach, the
self-protection capability and reputation weightage is utilized to obtain the
Reliability Factor (RF) value. Therefore jobs are allocated to the resources
that posses higher RF values. Extensive experimental evaluation shows that as
higher trust and reliable nodes are selected the chances of failure decreased
drastically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1528</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1528</id><created>2012-04-06</created><authors><author><keyname>Marinho</keyname><forenames>Leandro Balby</forenames></author><author><keyname>Baptista</keyname><forenames>Cl&#xe1;udio de Souza</forenames></author><author><keyname>Sandholm</keyname><forenames>Thomas</forenames></author><author><keyname>Nunes</keyname><forenames>Iury</forenames></author><author><keyname>N&#xf3;brega</keyname><forenames>Caio</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Jord&#xe3;o</forenames></author></authors><title>Extracting Geospatial Preferences Using Relational Neighbors</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing popularity of location-based social media applications
and devices that automatically tag generated content with locations, large
repositories of collaborative geo-referenced data are appearing on-line.
Efficiently extracting user preferences from these data to determine what
information to recommend is challenging because of the sheer volume of data as
well as the frequency of updates. Traditional recommender systems focus on the
interplay between users and items, but ignore contextual parameters such as
location. In this paper we take a geospatial approach to determine locational
preferences and similarities between users. We propose to capture the
geographic context of user preferences for items using a relational graph,
through which we are able to derive many new and state-of-the-art
recommendation algorithms, including combinations of them, requiring changes
only in the definition of the edge weights. Furthermore, we discuss several
solutions for cold-start scenarios. Finally, we conduct experiments using two
real-world datasets and provide empirical evidence that many of the proposed
algorithms outperform existing location-aware recommender algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1529</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1529</id><created>2012-04-06</created><authors><author><keyname>S</keyname><forenames>Sunish Kumar O</forenames></author></authors><title>Ensuring QOS Guarantees in a Hybrid OCS/OBS Network</title><categories>cs.NI</categories><comments>International Journal for Next Generation Networks 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The bursting aggregation assembly in edge nodes is one of the key
technologies in OBS (Optical Burst Switching) network, which has a direct
impact on flow characteristics and packet loss rate. An optical burst assembly
technique supporting QoS is presented through this paper, which can
automatically adjust the threshold along with the increasing and decreasing
volume of business, reduce the operational burst, and generate corresponding
BDP (Burst Data Packet) and BCP (Burst Control Packet). In addition to the
burst aggregation technique a packet recovery technique by restoration method
is also described. The data packet loss due to the physical optical link
failure is not currently included in the QoS descriptions. This link failure is
also a severe problem which reduces the data throughput of the transmitter
node. A mechanism for data recovery from this link failure is vital for
guaranteeing the QoS demanded by each user. So this paper will also discusses a
specific protocol for reducing the packet loss by utilizing the features of
both optical circuit switching (OCS) and Optical Burst switching (OBS)
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1548</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1548</id><created>2012-04-06</created><authors><author><keyname>Ahmadi</keyname><forenames>Behzad</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Choudhuri</keyname><forenames>Chiranjib</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>On Cascade Source Coding with A Side Information &quot;Vending Machine&quot;</title><categories>cs.IT math.IT</categories><comments>A shorter version has been submitted to ITW 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of a side information &quot;vending machine&quot; accounts for scenarios in
which acquiring side information is costly and thus should be done efficiently.
In this paper, the three-node cascade source coding problem is studied under
the assumption that a side information vending machine is available either at
the intermediate or at the end node. In both cases, a single-letter
characterization of the available trade-offs among the rate, the distortions in
the reconstructions at the intermediate and at the end node, and the cost in
acquiring the side information are derived under given conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1559</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1559</id><created>2012-04-06</created><authors><author><keyname>Melo</keyname><forenames>Nolmar</forenames></author></authors><title>Goppa goemetry codes via elementary methods (In Portuguese)</title><categories>cs.IT math.AG math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The central objective of this dissertation was to present the Goppa Geometry
Codes via elementary methods which were introduced by J.H. van Lint
,R.Pellikaan and T. Hohold about 1998. On the first part of such dissertation
are presented the fundamental concepts about bodies of rational functions of an
algebraic curve in the direction as to define the Goppa Codes on a classical
manner. In this study we based ourselves mainly on the book ? Algebraic
Function Fields and Codes? of H. Stichtenoth. The second part is initiated with
an introduction about
  the functions weight, degree and order which are fundamental for the study of
the Goppa Codes through elementary methods of linear algebra and of semigroups
and such study was based on ? Algebraic Geometry Codes ? of J.H. van
Lint,R.Pellikaan and T. Hohold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1561</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1561</id><created>2012-04-06</created><authors><author><keyname>Sasvari</keyname><forenames>Peter</forenames></author></authors><title>The macroeconomic effect of the information and communication technology
  in Hungary</title><categories>cs.CY q-fin.GN stat.OT</categories><comments>7 pages, 3 figures</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications (IJACSA), Vol. 2, No. 12, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was not until the beginning of the 1990s that the effects of information
and communication technology on economic growth as well as on the profitability
of enterprises raised the interest of researchers. After giving a general
description on the relationship between a more intense use of ICT devices and
dynamic economic growth, the author identified and explained those four
channels that had a robust influence on economic growth and productivity. When
comparing the use of information technonology devices in developed as well as
in developing countries, the author highlighted the importance of the available
additional human capital and the elimination of organizational inflexibilities
in the attempt of narrowing the productivity gap between the developed and
developing nations. By processing a large quantitiy of information gained from
Hungarian enterprises operating in several economic sectors, the author made an
attempt to find a strong correlation between the development level of using ICT
devices and profitability together with total factor productivity. Although the
impact of using ICT devices cannot be measured unequivocally at the
microeconomic level because of certain statistical and methodological
imperfections, by applying such analytical methods as cluster analysis and
correlation and regression calculation, the author managed to prove that both
the correlation coefficient and the gradient of the regression trend line
showed a positive relationship between the extensive use of information and
communication technology and the profitability of enterprises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1563</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1563</id><created>2012-04-06</created><updated>2014-12-28</updated><authors><author><keyname>Huang</keyname><forenames>Dayu</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Generalized Error Exponents For Small Sample Universal Hypothesis
  Testing</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>43 pages, 4 figures</comments><journal-ref>IEEE Transactions on Information Theory, vol.59, no.12,
  pp.8157,8181, Dec. 2013</journal-ref><doi>10.1109/TIT.2013.2283266</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The small sample universal hypothesis testing problem is investigated in this
paper, in which the number of samples $n$ is smaller than the number of
possible outcomes $m$. The goal of this work is to find an appropriate
criterion to analyze statistical tests in this setting. A suitable model for
analysis is the high-dimensional model in which both $n$ and $m$ increase to
infinity, and $n=o(m)$. A new performance criterion based on large deviations
analysis is proposed and it generalizes the classical error exponent applicable
for large sample problems (in which $m=O(n)$). This generalized error exponent
criterion provides insights that are not available from asymptotic consistency
or central limit theorem analysis. The following results are established for
the uniform null distribution:
  (i) The best achievable probability of error $P_e$ decays as
$P_e=\exp\{-(n^2/m) J (1+o(1))\}$ for some $J&gt;0$.
  (ii) A class of tests based on separable statistics, including the
coincidence-based test, attains the optimal generalized error exponents.
  (iii) Pearson's chi-square test has a zero generalized error exponent and
thus its probability of error is asymptotically larger than the optimal test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1564</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1564</id><created>2012-04-06</created><updated>2012-12-17</updated><authors><author><keyname>Tilles</keyname><forenames>Paulo F. C.</forenames></author><author><keyname>Fontanari</keyname><forenames>Jose F.</forenames></author></authors><title>Minimal model of associative learning for cross-situational lexicon
  acquisition</title><categories>q-bio.NC cs.LG</categories><journal-ref>J. Math. Psych. 56, 396-403 (2012)</journal-ref><doi>10.1016/j.jmp.2012.11.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An explanation for the acquisition of word-object mappings is the associative
learning in a cross-situational scenario. Here we present analytical results of
the performance of a simple associative learning algorithm for acquiring a
one-to-one mapping between $N$ objects and $N$ words based solely on the
co-occurrence between objects and words. In particular, a learning trial in our
learning scenario consists of the presentation of $C + 1 &lt; N$ objects together
with a target word, which refers to one of the objects in the context. We find
that the learning times are distributed exponentially and the learning rates
are given by $\ln{[\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ target
words are sampled randomly and by $\frac{1}{N} \ln [\frac{N-1}{C}] $ in the
case they follow a deterministic presentation sequence. This learning
performance is much superior to those exhibited by humans and more realistic
learning algorithms in cross-situational experiments. We show that introduction
of discrimination limitations using Weber's law and forgetting reduce the
performance of the associative algorithm to the human level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1566</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1566</id><created>2012-04-06</created><authors><author><keyname>Yekache</keyname><forenames>Yacine</forenames></author><author><keyname>Mekelleche</keyname><forenames>Yekhlef</forenames></author><author><keyname>Kouninef</keyname><forenames>Belkacem</forenames></author></authors><title>Towards Quranic reader controlled by speech</title><categories>cs.OH</categories><comments>4 pages, 5 figures, (IJACSA) International Journal of Advanced
  Computer Science and Applications, Vol. 2, No. 11, 2011. arXiv admin note:
  text overlap with arXiv:0704.2083 by other authors</comments><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we describe the process of designing a task-oriented continuous
speech recognition system for Arabic, based on CMU Sphinx4, to be used in the
voice interface of Quranic reader. The concept of the Quranic reader controlled
by speech is presented, the collection of the corpus and creation of acoustic
model are described in detail taking into account a specificities of Arabic
language and the desired application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1568</identifier>
 <datestamp>2014-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1568</id><created>2012-04-06</created><updated>2014-05-06</updated><authors><author><keyname>Moser</keyname><forenames>Georg</forenames></author><author><keyname>Schaper</keyname><forenames>Michael</forenames></author></authors><title>A Complexity Preserving Transformation from Jinja Bytecode to Rewrite
  Systems</title><categories>cs.PL</categories><comments>36 pages</comments><acm-class>F.3.2; F.4.1; F.4.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We revisit known transformations from Jinja bytecode to rewrite systems from
the viewpoint of runtime complexity. Suitably generalising the constructions
proposed in the literature, we define an alternative representation of Jinja
bytecode (JBC) executions as &quot;computation graphs&quot; from which we obtain a novel
representation of JBC executions as &quot;constrained rewrite systems&quot;. We prove
non-termination and complexity preservation of the transformation. We restrict
to well-formed JBC programs that only make use of non-recursive methods and
expect tree-shaped objects as input. Our approach allows for simplified
correctness proofs and provides a framework for the combination of the
computation graph method with standard techniques from static program analysis
like for example &quot;reachability analysis&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1576</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1576</id><created>2012-04-06</created><authors><author><keyname>Jha</keyname><forenames>Sanjeev Kumar</forenames></author></authors><title>Development of knowledge Base Expert System for Natural treatment of
  Diabetes disease</title><categories>cs.AI</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA)Volume 3 Issue 3 March 2012 Published</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of expert system for treatment of Diabetes disease by using
natural methods is new information technology derived from Artificial
Intelligent research using ESTA (Expert System Text Animation) System. The
proposed expert system contains knowledge about various methods of natural
treatment methods (Massage, Herbal/Proper Nutrition, Acupuncture, Gems) for
Diabetes diseases of Human Beings. The system is developed in the ESTA (Expert
System shell for Text Animation) which is Visual Prolog 7.3 Application. The
knowledge for the said system will be acquired from domain experts, texts and
other related sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1580</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1580</id><created>2012-04-06</created><updated>2012-09-10</updated><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Dobriban</keyname><forenames>Edgar</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Sawin</keyname><forenames>William F.</forenames></author></authors><title>Certifying the restricted isometry property is hard</title><categories>math.FA cs.CC cs.IT math.IT</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper is concerned with an important matrix condition in compressed
sensing known as the restricted isometry property (RIP). We demonstrate that
testing whether a matrix satisfies RIP is NP-hard. As a consequence of our
result, it is impossible to efficiently test for RIP provided P \neq NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1581</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1581</id><created>2012-04-06</created><authors><author><keyname>Maalal</keyname><forenames>Sara</forenames></author><author><keyname>Addou</keyname><forenames>Malika</forenames></author></authors><title>A new approach of designing Multi-Agent Systems</title><categories>cs.MA cs.AI</categories><comments>10 pages, 12 figures, A practical application of a method of
  designing multi-agent systems based on the AUML language and the MDA approach
  at &quot;the 4th IEEE Workshop on Information Technologies and Communication
  (WOTIC'11)&quot;, Casablanca, 13 - 15 October 2011, International Journal of
  Advanced Computer Science and Applications(IJACSA) Volume 2 No. 11 November
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent technology is a software paradigm that permits to implement large and
complex distributed applications. In order to assist analyzing, conception and
development or implementation phases of multi-agent systems, we've tried to
present a practical application of a generic and scalable method of a MAS with
a component-oriented architecture and agent-based approach that allows MDA to
generate source code from a given model. We've designed on AUML the class
diagrams as a class meta-model of different agents of a MAS. Then we generated
the source code of the models developed using an open source tool called
AndroMDA. This agent-based and evolutive approach enhances the modularity and
genericity developments and promotes their reusability in future developments.
This property distinguishes our design methodology of existing methodologies in
that it is constrained by any particular agent-based model while providing a
library of generic models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1586</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1586</id><created>2012-04-06</created><authors><author><keyname>Phan</keyname><forenames>Anh Huy</forenames></author><author><keyname>Tichavsk&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>On Fast Computation of Gradients for CANDECOMP/PARAFAC Algorithms</title><categories>cs.NA math.NA</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product between mode-$n$ unfolding $\bY_{(n)}$ of an $N$-D tensor $\tY$ and
Khatri-Rao products of $(N-1)$ factor matrices $\bA^{(m)}$, $m = 1,..., n-1,
n+1, ..., N$ exists in algorithms for CANDECOMP/PARAFAC (CP). If $\tY$ is an
error tensor of a tensor approximation, this product is the gradient of a cost
function with respect to factors, and has the largest workload in most CP
algorithms. In this paper, a fast method to compute this product is proposed.
Experimental verification shows that the fast CP gradient can accelerate the
CP_ALS algorithm 2 times and 8 times faster for factorizations of 3-D and 4-D
tensors, and the speed-up ratios can be 20-30 times for higher dimensional
tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1594</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1594</id><created>2012-04-07</created><authors><author><keyname>Rao</keyname><forenames>N. Mallikharjuna</forenames></author><author><keyname>Sasidhar</keyname><forenames>C.</forenames></author><author><keyname>Kumar</keyname><forenames>V. Sathyendra</forenames></author></authors><title>Cloud Computing Through Mobile-Learning</title><categories>cs.CY</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA)-2010</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Cloud computing is the new technology that has various advantages and it is
an adoptable technology in this present scenario. The main advantage of the
cloud computing is that this technology reduces the cost effectiveness for the
implementation of the Hardware, software and License for all. This is the
better peak time to analyze the cloud and its implementation and better use it
for the development of the quality and low cost education for all over the
world. In this paper, we discuss how to influence on cloud computing and
influence on this technology to take education to a wider mass of students over
the country. We believe cloud computing will surely improve the current system
of education and improve quality at an affordable cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1595</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1595</id><created>2012-04-07</created><authors><author><keyname>Golrezaei</keyname><forenames>Negin</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Femtocaching and Device-to-Device Collaboration: A New Architecture for
  Wireless Video Distribution</title><categories>cs.NI cs.IT math.IT</categories><comments>7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new architecture to handle the ongoing explosive increase in the
demand for video content in wireless networks. It is based on distributed
caching of the content in femto-basestations with small or non-existing
backhaul capacity but with considerable storage space, called helper nodes. We
also consider using the mobile terminals themselves as caching helpers, which
can distribute video through device-to-device communications. This approach
allows an improvement in the video throughput without deployment of any
additional infrastructure. The new architecture can improve video throughput by
one to two orders-of-magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1596</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1596</id><created>2012-04-07</created><authors><author><keyname>Rao</keyname><forenames>N. Mallikharjuna</forenames></author></authors><title>An Intelligent Location Management approaches in GSM Mobile Network</title><categories>cs.NI cs.AI</categories><comments>International Journal of Advanced Computer Science and
  Applications(IJACSA)-IJARAI-2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location management refers to the problem of updating and searching the
current location of mobile nodes in a wireless network. To make it efficient,
the sum of update costs of location database must be minimized. Previous work
relying on fixed location databases is unable to fully exploit the knowledge of
user mobility patterns in the system so as to achieve this minimization. The
study presents an intelligent location management approach which has interacts
between intelligent information system and knowledge-base technologies, so we
can dynamically change the user patterns and reduce the transition between the
VLR and HLR. The study provides algorithms are ability to handle location
registration and call delivery
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1597</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1597</id><created>2012-04-07</created><authors><author><keyname>Rao</keyname><forenames>N. Mallikharjuna</forenames></author></authors><title>An Intelligent Software Workflow Process Design for Location Management
  on Mobile Devices</title><categories>cs.NI cs.SE</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA)-2010</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Advances in the technologies of networking, wireless communication and
trimness of computers lead to the rapid development in mobile communication
infrastructure, and have drastically changed information processing on mobile
devices. Users carrying portable devices can freely move around, while still
connected to the network. This provides flexibility in accessing information
anywhere at any time. For improving more flexibility on mobile device, the new
challenges in designing software systems for mobile networks include location
and mobility management, channel allocation, power saving and security. In this
paper, we are proposing intelligent software tool for software design on mobile
devices to fulfill the new challenges on mobile location and mobility
management. In this study, the proposed Business Process Redesign (BPR) concept
is aims at an extension of the capabilities of an existing, widely used process
modeling tool in industry with 'Intelligent' capabilities to suggest favorable
alternatives to an existing software workflow design for improving
flexibilities on mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1598</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1598</id><created>2012-04-07</created><authors><author><keyname>Apte</keyname><forenames>Tejaswini</forenames></author><author><keyname>Ingle</keyname><forenames>Dr. Maya</forenames></author><author><keyname>Goyal</keyname><forenames>Dr. A. K.</forenames></author></authors><title>Improving Seek Time for Column Store Using MMH Algorithm</title><categories>cs.DB cs.PF</categories><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications Vol. 3, No.2, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hash based search has, proven excellence on large data warehouses stored in
column store. Data distribution has significant impact on hash based search. To
reduce impact of data distribution, we have proposed Memory Managed Hash (MMH)
algorithm that uses shift XOR group for Queries and Transactions in column
store. Our experiments show that MMH improves read and write throughput by 22%
for TPC-H distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1601</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1601</id><created>2012-04-07</created><authors><author><keyname>Elfattah</keyname><forenames>Marwa M. A.</forenames></author><author><keyname>Youssif</keyname><forenames>Aliaa A. A.</forenames></author><author><keyname>Sarhan</keyname><forenames>Ebada</forenames></author></authors><title>Handsets Malware Threats and Facing Techniques</title><categories>cs.CR</categories><comments>7 pages</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications,Vol. 2, No. 12, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, mobile handsets combine the functionality of mobile phones and
PDAs. Unfortunately, mobile handsets development process has been driven by
market demand, focusing on new features and neglecting security. So, it is
imperative to study the existing challenges that facing the mobile handsets
threat containment process, and the different techniques and methodologies that
used to face those challenges and contain the mobile handsets malwares. This
paper also presents a new approach to group the different malware containment
systems according to their typologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1603</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1603</id><created>2012-04-07</created><authors><author><keyname>Pareek</keyname><forenames>Narendra K</forenames></author></authors><title>Design and Analysis of a Novel Digital Image Encryption Scheme</title><categories>cs.CR</categories><comments>15 pages, 5 figures</comments><doi>10.5121/ijnsa.2012.4207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new image encryption scheme using a secret key of 144-bits
is proposed. In the substitution process of the scheme, image is divided into
blocks and subsequently into color components. Each color component is modified
by performing bitwise operation which depends on secret key as well as a few
most significant bits of its previous and next color component. Three rounds
are taken to complete substitution process. To make cipher more robust, a
feedback mechanism is also applied by modifying used secret key after
encrypting each block. Further, resultant image is partitioned into several key
based dynamic sub-images. Each sub-image passes through the scrambling process
where pixels of sub-image are reshuffled within itself by using a generated
magic square matrix. Five rounds are taken for scrambling process. The propose
scheme is simple, fast and sensitive to the secret key. Due to high order of
substitution and permutation, common attacks like linear and differential
cryptanalysis are infeasible. The experimental results show that the proposed
encryption technique is efficient and has high security features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1611</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1611</id><created>2012-04-07</created><authors><author><keyname>Ng</keyname><forenames>Choon Boon</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author><author><keyname>Goi</keyname><forenames>Bok Min</forenames></author></authors><title>Vision-based Human Gender Recognition: A Survey</title><categories>cs.CV</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gender is an important demographic attribute of people. This paper provides a
survey of human gender recognition in computer vision. A review of approaches
exploiting information from face and whole body (either from a still image or
gait sequence) is presented. We highlight the challenges faced and survey the
representative methods of these approaches. Based on the results, good
performance have been achieved for datasets captured under controlled
environments, but there is still much work that can be done to improve the
robustness of gender recognition under real-life environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1614</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1614</id><created>2012-04-07</created><authors><author><keyname>Chowdhury</keyname><forenames>Prasun</forenames></author><author><keyname>Misra</keyname><forenames>Iti Saha</forenames></author><author><keyname>Sanyal</keyname><forenames>Salil K</forenames></author></authors><title>Cross Layer QoS Support Architecture with Integrated CAC and Scheduling
  Algorithms for WiMAX BWA Networks</title><categories>cs.NI</categories><comments>17 pages, (IJACSA) International Journal of Advanced Computer Science
  and Applications, Vol. 3, No. 1, 2012. arXiv admin note: text overlap with
  arXiv:1012.2518 and arXiv:1110.0147 by other authors</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 1, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new technique for cross layer design, based on present Eb/N0
(bit energy per noise density) ratio of the connections and target values of
the Quality of Service (QoS) information parameters from MAC layer, is proposed
to dynamically select the Modulation and Coding Scheme (MCS) at the PHY layer
for WiMAX Broadband Wireless Access (BWA) networks. The QoS information
parameter includes New Connection Blocking Probability (NCBP), Hand off
Connection Dropping Probability (HCDP) and Connection Outage Probability (COP).
In addition, a Signal to Interference plus Noise Ratio (SINR) based Call
Admission Control (CAC) algorithm and Queue based Scheduling algorithm are
integrated for the cross layer design. An analytical model using the Continuous
Time Markov Chain (CTMC) is developed for performance evaluation of the
algorithms under various MCS. The effect of Eb/No is observed for QoS
information parameters in order to determine its optimum range. Simulation
results show that the integrated CAC and packet Scheduling model maximizes the
bandwidth utilization and fair allocation of the system resources for all types
of MCS and guarantees the QoS to the connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1615</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1615</id><created>2012-04-07</created><authors><author><keyname>Haboubi</keyname><forenames>Sofiene</forenames></author><author><keyname>Maddouri</keyname><forenames>Samia</forenames></author><author><keyname>Amiri</keyname><forenames>Hamid</forenames></author></authors><title>Discrimination between Arabic and Latin from bilingual documents</title><categories>cs.CV cs.CL cs.IR</categories><comments>5 pages</comments><doi>10.1109/CCCA.2011.6031496</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  2011 International Conference on Communications, Computing and Control
Applications (CCCA)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1616</identifier>
 <datestamp>2012-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1616</id><created>2012-04-07</created><updated>2012-08-17</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Gabow</keyname><forenames>Harold N.</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author></authors><title>Algorithmic Applications of Baur-Strassen's Theorem: Shortest Cycles,
  Diameter and Matchings</title><categories>cs.DS</categories><comments>To appear in FOCS 2012</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a directed or an undirected graph with integral edge weights from
the set [-W, W], that does not contain negative weight cycles. In this paper,
we introduce a general framework for solving problems on such graphs using
matrix multiplication. The framework is based on the usage of Baur-Strassen's
theorem and of Strojohann's determinant algorithm. It allows us to give new and
simple solutions to the following problems:
  * Finding Shortest Cycles -- We give a simple \tilde{O}(Wn^{\omega}) time
algorithm for finding shortest cycles in undirected and directed graphs. For
directed graphs (and undirected graphs with non-negative weights) this matches
the time bounds obtained in 2011 by Roditty and Vassilevska-Williams. On the
other hand, no algorithm working in \tilde{O}(Wn^{\omega}) time was previously
known for undirected graphs with negative weights. Furthermore our algorithm
for a given directed or undirected graph detects whether it contains a negative
weight cycle within the same running time.
  * Computing Diameter and Radius -- We give a simple \tilde{O}(Wn^{\omega})
time algorithm for computing a diameter and radius of an undirected or directed
graphs. To the best of our knowledge no algorithm with this running time was
known for undirected graphs with negative weights.
  * Finding Minimum Weight Perfect Matchings -- We present an
\tilde{O}(Wn^{\omega}) time algorithm for finding minimum weight perfect
matchings in undirected graphs. This resolves an open problem posted by
Sankowski in 2006, who presented such an algorithm but only in the case of
bipartite graphs.
  In order to solve minimum weight perfect matching problem we develop a novel
combinatorial interpretation of the dual solution which sheds new light on this
problem. Such a combinatorial interpretation was not know previously, and is of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1624</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1624</id><created>2012-04-07</created><authors><author><keyname>Jouini</keyname><forenames>Wassim</forenames></author><author><keyname>Moy</keyname><forenames>Christophe</forenames></author></authors><title>UCB Algorithm for Exponential Distributions</title><categories>stat.ML cs.LG</categories><comments>10 pages. Introduces Multiplicative Upper Confidence Bound (MUCB)
  algorithms for Multi-Armed Bandit problems</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce in this paper a new algorithm for Multi-Armed Bandit (MAB)
problems. A machine learning paradigm popular within Cognitive Network related
topics (e.g., Spectrum Sensing and Allocation). We focus on the case where the
rewards are exponentially distributed, which is common when dealing with
Rayleigh fading channels. This strategy, named Multiplicative Upper Confidence
Bound (MUCB), associates a utility index to every available arm, and then
selects the arm with the highest index. For every arm, the associated index is
equal to the product of a multiplicative factor by the sample mean of the
rewards collected by this arm. We show that the MUCB policy has a low
complexity and is order optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1628</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1628</id><created>2012-04-07</created><updated>2012-07-15</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Stable marriage and roommate problems with individual-based stability</title><categories>cs.GT</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research regarding the stable marriage and roommate problem has a long and
distinguished history in mathematics, computer science and economics. Stability
in this context is predominantly core stability or one of its variants in which
each deviation is by a group of players. There has been little focus in
matching theory on stability concepts such as Nash stability and individual
stability in which the deviation is by a single player. Such stability concepts
are suitable especially when trust for the other party is limited, complex
coordination is not feasible, or when only unmatched agents can be approached.
Furthermore, weaker stability notions such as individual stability may in
principle circumvent the negative existence and computational complexity
results in matching theory. We characterize the computational complexity of
checking the existence and computing individual-based stable matchings for the
marriage and roommate settings. One of our key computational results for the
stable marriage setting also carries over to different classes of hedonic games
for which individual-based stability has already been of much interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1629</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1629</id><created>2012-04-07</created><authors><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>kalti</keyname><forenames>karim</forenames></author></authors><title>Image segmentation by adaptive distance based on EM algorithm</title><categories>cs.CV</categories><comments>6 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications, Special Issue on Image Processing and Analysis, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a Bayesian image segmentation algorithm based on finite
mixtures. An EM algorithm is developed to estimate parameters of the Gaussian
mixtures. The finite mixture is a flexible and powerful probabilistic modeling
tool. It can be used to provide a model-based clustering in the field of
pattern recognition. However, the application of finite mixtures to image
segmentation presents some difficulties; especially it's sensible to noise. In
this paper we propose a variant of this method which aims to resolve this
problem. Our approach proceeds by the characterization of pixels by two
features: the first one describes the intrinsic properties of the pixel and the
second characterizes the neighborhood of pixel. Then the classification is made
on the base on adaptive distance which privileges the one or the other features
according to the spatial position of the pixel in the image. The obtained
results have shown a significant improvement of our approach compared to the
standard version of EM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1631</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1631</id><created>2012-04-07</created><authors><author><keyname>jayech</keyname><forenames>Khlifia</forenames></author><author><keyname>mahjoub</keyname><forenames>mohamed ali</forenames></author></authors><title>New approach using Bayesian Network to improve content based image
  classification systems</title><categories>cs.CV cs.IR</categories><comments>10 pages, IJCSI International Journal of Computer Science Issues,
  Vol. 7, Issue 6, November 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new approach based on augmented naive Bayes for image
classification. Initially, each image is cutting in a whole of blocks. For each
block, we compute a vector of descriptors. Then, we propose to carry out a
classification of the vectors of descriptors to build a vector of labels for
each image. Finally, we propose three variants of Bayesian Networks such as
Naive Bayesian Network (NB), Tree Augmented Naive Bayes (TAN) and Forest
Augmented Naive Bayes (FAN) to classify the image using the vector of labels.
The results showed a marked improvement over the FAN, NB and TAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1634</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1634</id><created>2012-04-07</created><authors><author><keyname>zayane</keyname><forenames>Oussema</forenames></author><author><keyname>jouini</keyname><forenames>besma</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author></authors><title>Automatic liver segmentation method in CT images</title><categories>cs.CV</categories><comments>4 pages</comments><journal-ref>Canadian Journal on Image Processing &amp; Computer Vision Vol. 2, No.
  8, 1923-1717 December 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this work is to develop a method for automatic segmentation of the
liver based on a priori knowledge of the image, such as location and shape of
the liver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1637</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1637</id><created>2012-04-07</created><authors><author><keyname>ghanmy</keyname><forenames>Nabil</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Amara</keyname><forenames>Najoua Essoukri Ben</forenames></author></authors><title>Characterization of Dynamic Bayesian Network</title><categories>cs.AI</categories><comments>9 pages, (IJACSA) International Journal of Advanced Computer Science
  and Applications, Vol. 2, No. 7, 2011</comments><report-no>2156-5570</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a
model that tries to incorporate temporal dimension with uncertainty. We start
with basics of DBN where we especially focus in Inference and Learning concepts
and algorithms. Then we will present different levels and methods of creating
DBNs as well as approaches of incorporating temporal dimension in static
Bayesian network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1644</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1644</id><created>2012-04-07</created><authors><author><keyname>Alaraj</keyname><forenames>Abdullah M.</forenames></author></authors><title>Optimizing One Fair Document Exchange Protocol</title><categories>cs.CR</categories><comments>12 pages, 2 figures, Journal article</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.4, No.1, January 2012</journal-ref><doi>10.5121/ijnsa.2012.4101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an efficient fair document exchange protocol. The
exchange of the documents will be between two parties. The protocol is based on
the verifiable and recoverable encryption of a document's key. This verifiable
and recoverable encryption of the document's key will allow one party to verify
the encrypted key. It will also ensure this party that the Semi Trusted Third
Party will be able to recover the key if the other party misbehaves. The
protocol also incorporates the concept of enforcing the honesty of one party.
The proposed protocol consists of only three messages and is more efficient
than related protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1646</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1646</id><created>2012-04-07</created><authors><author><keyname>Alaraj</keyname><forenames>Abdullah M.</forenames></author></authors><title>Simple and Efficient Contract Signing Protocol</title><categories>cs.CR</categories><comments>5 pages, 2 figures, Journal article (IJACSA) International Journal of
  Advanced Computer Science and Applications, Vol. 3, No. 3, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new contract signing protocol is proposed based on the RSA
signature scheme. The protocol will allow two parties to sign the same contract
and then exchange their digital signatures. The protocol ensures fairness in
that it offers parties greater security: either both parties receive each
other's signatures or neither does. The protocol is based on offline Trusted
Third Party (TTP) that will be brought into play only if one party fails to
sign the contract. Otherwise, the TTP remains inactive. The protocol consists
of only three messages that are exchanged between the two parties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1649</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1649</id><created>2012-04-07</created><authors><author><keyname>Elouafiq</keyname><forenames>Ali</forenames></author></authors><title>Design and Engineering of a Chess-Robotic Arm</title><categories>cs.RO math.AG</categories><comments>22 pages, Pseudo-code, State-of-the-art design</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the scope of the &quot;Chess-Bot&quot; project, this study's goal is to choose the
right model for the robotic arm that the &quot;the Chess-Bot&quot; will use to move the
pawn from a cell to another. In this paper, there is the definition and the
structure of a robot arm. Also, the different engineering and kinematics
fundamentals of the robot and its components will be detailed. Furthermore, the
different structures of robotic arms will be presented and compared based on
different criteria. Finally, a model for &quot;the Chess-Bot&quot; arm will be
synthesized based on accurate algorithms and equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1650</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1650</id><created>2012-04-07</created><authors><author><keyname>Elouafiq</keyname><forenames>Ali</forenames></author></authors><title>The Lego Mindstorms Robotics Invention Systems 2.0 Toolkit: A Study Case</title><categories>cs.RO</categories><comments>19 pages, source-code, testing measures, guidelines, referential
  work, measures and standards, physical properties</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper reviews the aspects of the LEGO\textregistered
Mindstorms\trademark robotics invention system 2.0 \trademark (RIS), by
presenting the different elements of the kit, and relating them to actual robot
components and norms. Furthermore a comparison between the LCS and Java is
made, as well as comparing the RCX board to other technologies, specifically
LEGO \textregistered NXT and MIT's &quot;Handy Board&quot;. Also, concrete examples of
application using the RIS are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1651</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1651</id><created>2012-04-07</created><authors><author><keyname>Elouafiq</keyname><forenames>Ali</forenames></author></authors><title>Authentication and Encryption in GSM and 3GUMTS: An Emphasis on
  Protocols and Algorithms</title><categories>cs.CR cs.NI</categories><comments>12 pages, reference for 3G/2G telecommunications cryptography</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Mobile communication touches every aspect of our life, it become one of the
major dependencies that the 21st Century civilizations rely on. Thereby,
security is a major issue that should be targeted by communication
technologies. In this paper we will target authentication and encryption in GSM
and 3G/UMTS. In order to understand clearly how things work, we will start by
presenting the architecture of each network, its major components, its
authentication algorithms, protocols used, and KASUMI Block Cipher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1653</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1653</id><created>2012-04-07</created><authors><author><keyname>Elouafiq</keyname><forenames>Ali</forenames></author></authors><title>Machine Cognition Models: EPAM and GPS</title><categories>cs.AI</categories><comments>EPAM, General Problem solver</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Through history, the human being tried to relay its daily tasks to other
creatures, which was the main reason behind the rise of civilizations. It
started with deploying animals to automate tasks in the field of
agriculture(bulls), transportation (e.g. horses and donkeys), and even
communication (pigeons). Millenniums after, come the Golden age with
&quot;Al-jazari&quot; and other Muslim inventors, which were the pioneers of automation,
this has given birth to industrial revolution in Europe, centuries after. At
the end of the nineteenth century, a new era was to begin, the computational
era, the most advanced technological and scientific development that is driving
the mankind and the reason behind all the evolutions of science; such as
medicine, communication, education, and physics. At this edge of technology
engineers and scientists are trying to model a machine that behaves the same as
they do, which pushed us to think about designing and implementing &quot;Things
that-Thinks&quot;, then artificial intelligence was. In this work we will cover each
of the major discoveries and studies in the field of machine cognition, which
are the &quot;Elementary Perceiver and Memorizer&quot;(EPAM) and &quot;The General Problem
Solver&quot;(GPS). The First one focus mainly on implementing the human-verbal
learning behavior, while the second one tries to model an architecture that is
able to solve problems generally (e.g. theorem proving, chess playing, and
arithmetic). We will cover the major goals and the main ideas of each model, as
well as comparing their strengths and weaknesses, and finally giving their
fields of applications. And Finally, we will suggest a real life implementation
of a cognitive machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1656</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1656</id><created>2012-04-07</created><authors><author><keyname>Schuh</keyname><forenames>Bernd R.</forenames></author></authors><title>Phase Transition in Unrestricted Random SAT</title><categories>cs.CC math.LO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For random CNF formulae with m clauses, n variables and an unrestricted
number of literals per clause the transition from high to low satisfiability
can be determined exactly for large n. The critical density m/n turns out to be
strongly n-dependent, ccr = ln(2)/(1-p)^^n, where pn is the mean number of
positive literals per clause.This is in contrast to restricted random SAT
problems (random K-SAT), where the critical ratio m/n is a constant. All
transition lines are calculated by the second moment method applied to the
number of solutions N of a formula. In contrast to random K-SAT, the method
does not fail for the unrestricted model, because long range interactions
between solutions are not cut off by disorder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1658</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1658</id><created>2012-04-07</created><authors><author><keyname>Verma</keyname><forenames>Anshul</forenames></author><author><keyname>Srivastava</keyname><forenames>Dr. Anurag</forenames></author></authors><title>Integrated Routing Protocol for Opportunistic Networks</title><categories>cs.NI</categories><comments>8 pages, 12 figures, (IJACSA) International Journal of Advanced
  Computer Science and Applications</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 2, No.3, March 2011, pp. 85-92</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In opportunistic networks the existence of a simultaneous path is not assumed
to transmit a message between a sender and a receiver. Information about the
context in which the users communicate is a key piece of knowledge to design
efficient routing protocols in opportunistic networks. But this kind of
information is not always available. When users are very isolated, context
information cannot be distributed, and cannot be used for taking efficient
routing decisions. In such cases, context oblivious based schemes are only way
to enable communication between users. As soon as users become more social,
context data spreads in the network, and context based routing becomes an
efficient solution. In this paper we design an integrated routing protocol that
is able to use context data as soon as it becomes available and falls back to
dissemination based routing when context information is not available. Then, we
provide a comparison between Epidemic and PROPHET, these are representative of
context oblivious and context aware routing protocols. Our results show that
integrated routing protocol is able to provide better result in term of message
delivery probability and message delay in both cases when context information
about users is available or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1672</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1672</id><created>2012-04-07</created><updated>2012-06-18</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author></authors><title>A Characterization of Bispecial Sturmian Words</title><categories>cs.FL cs.CG cs.DM</categories><comments>Accepted to MFCS 2012</comments><msc-class>68R15</msc-class><journal-ref>LNCS 7464, pp. 383-394, 2012</journal-ref><doi>10.1007/978-3-642-32589-2_35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite Sturmian word w over the alphabet {a,b} is left special (resp. right
special) if aw and bw (resp. wa and wb) are both Sturmian words. A bispecial
Sturmian word is a Sturmian word that is both left and right special. We show
as a main result that bispecial Sturmian words are exactly the maximal internal
factors of Christoffel words, that are words coding the digital approximations
of segments in the Euclidean plane. This result is an extension of the known
relation between central words and primitive Christoffel words. Our
characterization allows us to give an enumerative formula for bispecial
Sturmian words. We also investigate the minimal forbidden words for the set of
Sturmian words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1677</identifier>
 <datestamp>2012-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1677</id><created>2012-04-07</created><updated>2012-05-30</updated><authors><author><keyname>Livni</keyname><forenames>Idan</forenames></author><author><keyname>Khina</keyname><forenames>Anatoly</forenames></author><author><keyname>Hitron</keyname><forenames>Ayal</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>Space-Time MIMO Multicasting</title><categories>cs.IT math.IT</categories><comments>ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicasting is the general method of conveying the same information to
multiple users over a broadcast channel. In this work, the Gaussian MIMO
broadcast channel is considered, with multiple users and any number of antennas
at each node. A &quot;closed loop&quot; scenario is assumed, for which a practical
capacity-achieving multicast scheme is constructed. In the proposed scheme,
linear modulation is carried over time and space together, which allows to
transform the problem into that of transmission over parallel scalar
sub-channels, the gains of which are equal, except for a fraction of
sub-channels that vanishes with the number of time slots used. Over these
sub-channels, off-the-shelf fixed-rate AWGN codes can be used to approach
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1678</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1678</id><created>2012-04-07</created><authors><author><keyname>Charfi</keyname><forenames>Moncef</forenames></author><author><keyname>Kherallah</keyname><forenames>Monji</forenames></author><author><keyname>Baati</keyname><forenames>Abdelkarim El</forenames></author><author><keyname>Alimi</keyname><forenames>Adel M.</forenames></author></authors><title>A New Approach for Arabic Handwritten Postal Addresses Recognition</title><categories>cs.CV</categories><comments>7 pages, 7 figures; (IJACSA) International Journal of Advanced
  Computer Science and Applications, Vol. 3, No. 3, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an automatic analysis system for the Arabic
handwriting postal addresses recognition, by using the beta elliptical model.
Our system is divided into different steps: analysis, pre-processing and
classification. The first operation is the filtering of image. In the second,
we remove the border print, stamps and graphics. After locating the address on
the envelope, the address segmentation allows the extraction of postal code and
city name separately. The pre-processing system and the modeling approach are
based on two basic steps. The first step is the extraction of the temporal
order in the image of the handwritten trajectory. The second step is based on
the use of Beta-Elliptical model for the representation of handwritten script.
The recognition system is based on Graph-matching algorithm. Our modeling and
recognition approaches were validated by using the postal code and city names
extracted from the Tunisian postal envelopes data. The recognition rate
obtained is about 98%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1679</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1679</id><created>2012-04-07</created><authors><author><keyname>Jayech</keyname><forenames>Khlifia</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author></authors><title>Clustering and Bayesian network for image of faces classification</title><categories>cs.CV cs.AI</categories><comments>12 pages</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Special Issue on Image processing and Analysis, pp 35-44 May
  2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a content based image classification system, target images are sorted by
feature similarities with respect to the query (CBIR). In this paper, we
propose to use new approach combining distance tangent, k-means algorithm and
Bayesian network for image classification. First, we use the technique of
tangent distance to calculate several tangent spaces representing the same
image. The objective is to reduce the error in the classification phase.
Second, we cut the image in a whole of blocks. For each block, we compute a
vector of descriptors. Then, we use K-means to cluster the low-level features
including color and texture information to build a vector of labels for each
image. Finally, we apply five variants of Bayesian networks classifiers
(Na\&quot;ive Bayes, Global Tree Augmented Na\&quot;ive Bayes (GTAN), Global Forest
Augmented Na\&quot;ive Bayes (GFAN), Tree Augmented Na\&quot;ive Bayes for each class
(TAN), and Forest Augmented Na\&quot;ive Bayes for each class (FAN) to classify the
image of faces using the vector of labels. In order to validate the feasibility
and effectively, we compare the results of GFAN to FAN and to the others
classifiers (NB, GTAN, TAN). The results demonstrate FAN outperforms than GFAN,
NB, GTAN and TAN in the overall classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1681</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1681</id><created>2012-04-07</created><authors><author><keyname>Lamine</keyname><forenames>Fradj Ben</forenames></author><author><keyname>Kalti</keyname><forenames>Karim</forenames></author><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author></authors><title>The threshold EM algorithm for parameter learning in bayesian network
  with incomplete data</title><categories>cs.AI cs.LG stat.ML</categories><comments>6 pages</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 2, No. 7, pp 86-90, July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks (BN) are used in a big range of applications but they have
one issue concerning parameter learning. In real application, training data are
always incomplete or some nodes are hidden. To deal with this problem many
learning parameter algorithms are suggested foreground EM, Gibbs sampling and
RBE algorithms. In order to limit the search space and escape from local maxima
produced by executing EM algorithm, this paper presents a learning parameter
algorithm that is a fusion of EM and RBE algorithms. This algorithm
incorporates the range of a parameter into the EM algorithm. This range is
calculated by the first step of RBE algorithm allowing a regularization of each
parameter in bayesian network after the maximization step of the EM algorithm.
The threshold EM algorithm is applied in brain tumor diagnosis and show some
advantages and disadvantages over the EM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1685</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1685</id><created>2012-04-07</created><updated>2013-05-24</updated><authors><author><keyname>Azizyan</keyname><forenames>Martin</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Density-sensitive semisupervised inference</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1092 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1092</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 2, 751-771</journal-ref><doi>10.1214/13-AOS1092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semisupervised methods are techniques for using labeled data
$(X_1,Y_1),\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\ldots,X_N$
to make predictions. These methods invoke some assumptions that link the
marginal distribution $P_X$ of X to the regression function f(x). For example,
it is common to assume that f is very smooth over high density regions of
$P_X$. Many of the methods are ad-hoc and have been shown to work in specific
examples but are lacking a theoretical foundation. We provide a minimax
framework for analyzing semisupervised methods. In particular, we study methods
based on metrics that are sensitive to the distribution $P_X$. Our model
includes a parameter $\alpha$ that controls the strength of the semisupervised
assumption. We then use the data to adapt to $\alpha$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1688</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1688</id><created>2012-04-07</created><updated>2013-11-26</updated><authors><author><keyname>Duchi</keyname><forenames>John C.</forenames></author><author><keyname>Mackey</keyname><forenames>Lester</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>The asymptotics of ranking algorithms</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1142 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1142</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 5, 2292-2323</journal-ref><doi>10.1214/13-AOS1142</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the predictive problem of supervised ranking, where the task is
to rank sets of candidate items returned in response to queries. Although there
exist statistical procedures that come with guarantees of consistency in this
setting, these procedures require that individuals provide a complete ranking
of all items, which is rarely feasible in practice. Instead, individuals
routinely provide partial preference information, such as pairwise comparisons
of items, and more practical approaches to ranking have aimed at modeling this
partial preference data directly. As we show, however, such an approach raises
serious theoretical challenges. Indeed, we demonstrate that many commonly used
surrogate losses for pairwise comparison data do not yield consistency;
surprisingly, we show inconsistency even in low-noise settings. With these
negative results as motivation, we present a new approach to supervised ranking
based on aggregation of partial preferences, and we develop $U$-statistic-based
empirical risk minimization procedures. We present an asymptotic analysis of
these new procedures, showing that they yield consistency results that parallel
those available for classification. We complement our theoretical results with
an experiment studying the new procedures in a large-scale web-ranking task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1704</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1704</id><created>2012-04-07</created><authors><author><keyname>Somasundaram</keyname><forenames>K.</forenames></author><author><keyname>Vimala</keyname><forenames>S.</forenames></author></authors><title>Multi-Level Coding Efficiency with Improved Quality for Image
  Compression based on AMBTC</title><categories>cs.CV</categories><comments>10 Pages, 3 Figures, 2 Tables</comments><journal-ref>International Journal of Information Sciences and Techniques
  (IJIST) Vol.2, No.2, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have proposed an extended version of Absolute Moment Block
Truncation Coding (AMBTC) to compress images. Generally the elements of a
bitplane used in the variants of Block Truncation Coding (BTC) are of size 1
bit. But it has been extended to two bits in the proposed method. Number of
statistical moments preserved to reconstruct the compressed has also been
raised from 2 to 4. Hence, the quality of the reconstructed images has been
improved significantly from 33.62 to 38.12 with the increase in bpp by 1. The
increased bpp (3) is further reduced to 1.75in multiple levels: in one level,
by dropping 4 elements of the bitplane in such a away that the pixel values of
the dropped elements can easily be interpolated with out much of loss in the
quality, in level two, eight elements are dropped and reconstructed later and
in level three, the size of the statistical moments is reduced. The experiments
were carried over standard images of varying intensities. In all the cases, the
proposed method outperforms the existing AMBTC technique in terms of both PSNR
and bpp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1706</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1706</id><created>2012-04-08</created><authors><author><keyname>Azghadi</keyname><forenames>Mostafa Rahimi</forenames></author><author><keyname>Al-Sarawi</keyname><forenames>Said</forenames></author><author><keyname>Iannella</keyname><forenames>Nicolangelo</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Efficient Design of Triplet Based Spike-Timing Dependent Plasticity</title><categories>cs.NE</categories><journal-ref>Proceedings of the International Joint Conference on Neural
  Networks, held in Brisbane, 10-15 June, 2012: pp.1-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spike-Timing Dependent Plasticity (STDP) is believed to play an important
role in learning and the formation of computational function in the brain. The
classical model of STDP which considers the timing between pairs of
pre-synaptic and post-synaptic spikes (p-STDP) is incapable of reproducing
synaptic weight changes similar to those seen in biological experiments which
investigate the effect of either higher order spike trains (e.g. triplet and
quadruplet of spikes), or, simultaneous effect of the rate and timing of spike
pairs on synaptic plasticity. In this paper, we firstly investigate synaptic
weight changes using a p-STDP circuit and show how it fails to reproduce the
mentioned complex biological experiments. We then present a new STDP VLSI
circuit which acts based on the timing among triplets of spikes (t-STDP) that
is able to reproduce all the mentioned experimental results. We believe that
our new STDP VLSI circuit improves upon previous circuits, whose learning
capacity exceeds current designs due to its capability of mimicking the
outcomes of biological experiments more closely; thus plays a significant role
in future VLSI implementation of neuromorphic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1710</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1710</id><created>2012-04-08</created><authors><author><keyname>Jain</keyname><forenames>Dhyanendra</forenames></author></authors><title>Hiding Sensitive Association Rules without Altering the Support of
  Sensitive Item(s)</title><categories>cs.DB cs.DC</categories><comments>10 pages</comments><doi>10.5121/ijaia.2012.3207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Association rule mining is an important data-mining technique that finds
interesting association among a large set of data items. Since it may disclose
patterns and various kinds of sensitive knowledge that are difficult to find
otherwise, it may pose a threat to the privacy of discovered confidential
information. Such information is to be protected against unauthorized access.
Many strategies had been proposed to hide the information. Some use distributed
databases over several sites, data perturbation, clustering, and data
distortion techniques. Hiding sensitive rules problem, and still not
sufficiently investigated, is the requirement to balance the confidentiality of
the disclosed data with the legitimate needs of the user. The proposed approach
uses the data distortion technique where the position of the sensitive items is
altered but its support is never changed. The size of the database remains the
same. It uses the idea of representative rules to prune the rules first and
then hides the sensitive rules. Advantage of this approach is that it hides
maximum number of rules however, the existing approaches fail to hide all the
desired rules, which are supposed to be hidden in minimum number of passes. The
paper also compares of the proposed approach with existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1715</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1715</id><created>2012-04-08</created><authors><author><keyname>Awate</keyname><forenames>Yogesh P.</forenames></author></authors><title>Improved theoretical guarantees regarding a class of two-row cutting
  planes</title><categories>math.OC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The corner polyhedron is described by minimal valid inequalities from maximal
lattice-free convex sets. For the Relaxed Corner Polyhedron (RCP) with two free
integer variables and any number of non-negative continuous variables, it is
known that such facet defining inequalities arise from maximal lattice-free
splits, triangles and quadrilaterals. We improve on the tightest known upper
bound for the approximation of the RCP, purely by minimal valid inequalities
from maximal lattice-free quadrilaterals, from 2 to 1.71. We generalize the
tightest known lower bound of 1.125 for the approximation of the RCP, purely by
minimal valid inequalities from maximal lattice-free triangles, to an infinite
subclass of quadrilaterals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1718</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1718</id><created>2012-04-08</created><authors><author><keyname>Collier</keyname><forenames>Nathan</forenames></author><author><keyname>Pardo</keyname><forenames>David</forenames></author><author><keyname>Paszynski</keyname><forenames>Maciej</forenames></author><author><keyname>Calo</keyname><forenames>Victor M.</forenames></author></authors><title>Computational complexity and memory usage for multi-frontal direct
  solvers in structured mesh finite elements</title><categories>cs.NA math.NA</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multi-frontal direct solver is the state-of-the-art algorithm for the
direct solution of sparse linear systems. This paper provides computational
complexity and memory usage estimates for the application of the multi-frontal
direct solver algorithm on linear systems resulting from B-spline-based
isogeometric finite elements, where the mesh is a structured grid. Specifically
we provide the estimates for systems resulting from $C^{p-1}$ polynomial
B-spline spaces and compare them to those obtained using $C^0$ spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1726</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1726</id><created>2012-04-08</created><authors><author><keyname>Kr&#xe4;mer</keyname><forenames>Lukas</forenames></author><author><keyname>Di Napoli</keyname><forenames>Edoardo</forenames></author><author><keyname>Galgon</keyname><forenames>Martin</forenames></author><author><keyname>Lang</keyname><forenames>Bruno</forenames></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames></author></authors><title>Dissecting the FEAST algorithm for generalized eigenproblems</title><categories>cs.NA math.NA</categories><comments>11 Pages, 5 Figures. Submitted to Journal of Computational and
  Applied Mathematics</comments><msc-class>65F15, 65F50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the FEAST method for computing selected eigenvalues and
eigenvectors of large sparse matrix pencils. After establishing the close
connection between FEAST and the well-known Rayleigh-Ritz method, we identify
several critical issues that influence convergence and accuracy of the solver:
the choice of the starting vector space, the stopping criterion, how the inner
linear systems impact the quality of the solution, and the use of FEAST for
computing eigenpairs from multiple intervals. We complement the study with
numerical examples, and hint at possible improvements to overcome the existing
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1730</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1730</id><created>2012-04-08</created><authors><author><keyname>Arafa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed K.</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>El-Sherif</keyname><forenames>Amr A.</forenames></author></authors><title>A Soft Sensing-Based Cognitive Access Scheme Exploiting Primary Feedback</title><categories>math.OC cs.NI</categories><comments>Accepted for publication in WiOpt 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine a cognitive spectrum access scheme in which
secondary users exploit the primary feedback information. We consider an
overlay secondary network employing a random access scheme in which secondary
users access the channel by certain access probabilities that are function of
the spectrum sensing metric. In setting our problem, we assume that secondary
users can eavesdrop on the primary link's feedback. We study the cognitive
radio network from a queuing theory point of view. Access probabilities are
determined by solving a secondary throughput maximization problem subject to a
constraint on the primary queues' stability. First, we formulate our problem
which is found to be non-convex. Yet, we solve it efficiently by exploiting the
structure of the secondary throughput equation. Our scheme yields improved
results in, both, the secondary user throughput and the primary user packet
delay. In addition, it comes very close to the optimal genie-aided scheme in
which secondary users act upon the presumed perfect knowledge of the primary
user's activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1739</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1739</id><created>2012-04-08</created><authors><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Relay Placement for Physical Layer Security: A Secure Connection
  Perspective</title><categories>cs.IT math.IT</categories><comments>4 pages, 6 figures, Accepted by IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the problem of secure connection in cooperative wireless
communication with two relay strategies, decode-and-forward (DF) and
randomize-and-forward (RF). The four-node scenario and cellular scenario are
considered. For the typical four-node (source, destination, relay, and
eavesdropper) scenario, we derive the optimal power allocation for the DF
strategy and find that the RF strategy is always better than the DF to enhance
secure connection. In cellular networks, we show that without relay, it is
difficult to establish secure connections from the base station to the cell
edge users. The effect of relay placement for the cell edge users is
demonstrated by simulation. For both scenarios, we find that the benefit of
relay transmission increases when path loss becomes severer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1746</identifier>
 <datestamp>2012-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1746</id><created>2012-04-08</created><updated>2012-06-05</updated><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author></authors><title>Removal of Quantifiers by Elimination of Boundary Points</title><categories>cs.LO cs.DM</categories><comments>The only change with respect to the previous version is a
  modification of the acknowledgement section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of elimination of existential quantifiers from a
Boolean CNF formula. Our approach is based on the following observation. One
can get rid of dependency on a set of variables of a quantified CNF formula F
by adding resolvent clauses of F eliminating boundary points. This approach is
similar to the method of quantifier elimination described in [9]. The
difference of the method described in the present paper is twofold: {\bullet}
branching is performed only on quantified variables, {\bullet} an explicit
search for boundary points is performed by calls to a SAT-solver Although we
published the paper [9] before this one, chrono- logically the method of the
present report was developed first. Preliminary presentations of this method
were made in [10], [11]. We postponed a publication of this method due to
preparation of a patent application [8].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1748</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1748</id><created>2012-04-08</created><authors><author><keyname>Agrawal</keyname><forenames>Rohit</forenames></author><author><keyname>Vasalya</keyname><forenames>Ashesh</forenames></author></authors><title>Bluetooth Navigation System using Wi-Fi Access Points</title><categories>cs.NI</categories><comments>8 pages 2 figures and 1 table International Journal of Distributed
  and Parallel Systems</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There have been various navigation and tracking systems being developed with
the help of technologies like GPS, GSM, Bluetooth, IR, Wi-Fi and Radar. Outdoor
positioning systems have been deployed quite successfully using GPS but
positioning systems for indoor environments still do not have widespread
deployment due to various reasons. Most of these use only a single technology
for positioning but using more than one in cooperation with each other is
always advantageous for obtaining greater accuracy. Particularly, the ones
which use Bluetooth are better since they would enhance the scalability of such
a system because of the fact that this technology is in use by the common
people so it would always be easy to track them. Moreover it would also reduce
the hardware installation cost to some extent. The system that has been
introduced here uses Bluetooth primarily for positioning and tracking in
combination with Wi-Fi access points. The reason that makes the commercial
application of such a system easier and cheaper is that most of the localized
areas today like college campus, offices are being provided with internet
connectivity using these access points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1749</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1749</id><created>2012-04-08</created><authors><author><keyname>Gunji</keyname><forenames>Yukio-Pegio</forenames></author><author><keyname>Nishiyama</keyname><forenames>Yuta</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Robust Soldier Crab Ball Gate</title><categories>cs.ET nlin.CG</categories><journal-ref>Complex Systems 20 (2011) 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soldier crabs Mictyris guinotae exhibit pronounced swarming behaviour. The
swarms of the crabs tolerant of perturbations. In computer models and
laboratory experiments we demonstrate that swarms of soldier crabs can
implement logical gates when placed in a geometrically constrained environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1751</identifier>
 <datestamp>2012-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1751</id><created>2012-04-08</created><updated>2012-11-16</updated><authors><author><keyname>Singh</keyname><forenames>Rishabh</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author><author><keyname>Solar-Lezama</keyname><forenames>Armando</forenames></author></authors><title>Automated Feedback Generation for Introductory Programming Assignments</title><categories>cs.PL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for automatically providing feedback for introductory
programming problems. In order to use this method, we need a reference
implementation of the assignment, and an error model consisting of potential
corrections to errors that students might make. Using this information, the
system automatically derives minimal corrections to student's incorrect
solutions, providing them with a quantifiable measure of exactly how incorrect
a given solution was, as well as feedback about what they did wrong.
  We introduce a simple language for describing error models in terms of
correction rules, and formally define a rule-directed translation strategy that
reduces the problem of finding minimal corrections in an incorrect program to
the problem of synthesizing a correct program from a sketch. We have evaluated
our system on thousands of real student attempts obtained from 6.00 and 6.00x.
Our results show that relatively simple error models can correct on average 65%
of all incorrect submissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1754</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1754</id><created>2012-04-08</created><authors><author><keyname>Afrati</keyname><forenames>Foto N.</forenames></author><author><keyname>Sarma</keyname><forenames>Anish Das</forenames></author><author><keyname>Salihoglu</keyname><forenames>Semih</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author></authors><title>Vision Paper: Towards an Understanding of the Limits of Map-Reduce
  Computation</title><categories>cs.DB cs.DC</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant amount of recent research work has addressed the problem of
solving various data management problems in the cloud. The major algorithmic
challenges in map-reduce computations involve balancing a multitude of factors
such as the number of machines available for mappers/reducers, their memory
requirements, and communication cost (total amount of data sent from mappers to
reducers). Most past work provides custom solutions to specific problems, e.g.,
performing fuzzy joins in map-reduce, clustering, graph analyses, and so on.
While some problems are amenable to very efficient map-reduce algorithms, some
other problems do not lend themselves to a natural distribution, and have
provable lower bounds. Clearly, the ease of &quot;map-reducability&quot; is closely
related to whether the problem can be partitioned into independent pieces,
which are distributed across mappers/reducers. What makes a problem
distributable? Can we characterize general properties of problems that
determine how easy or hard it is to find efficient map-reduce algorithms?
  This is a vision paper that attempts to answer the questions described above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1756</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1756</id><created>2012-04-08</created><authors><author><keyname>Ma</keyname><forenames>Ruina</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Bennis</keyname><forenames>Fouad</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Ma</keyname><forenames>Liang</forenames><affiliation>IRCCyN, DIE</affiliation></author></authors><title>Human Muscle Fatigue Model in Dynamic Motions</title><categories>cs.RO</categories><comments>Advances in Robot Kinematics, Innsbruck : Austria (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human muscle fatigue is considered to be one of the main reasons for
Musculoskeletal Disorder (MSD). Recent models have been introduced to define
muscle fatigue for static postures. However, the main drawbacks of these models
are that the dynamic effect of the human and the external load are not taken
into account. In this paper, each human joint is assumed to be controlled by
two muscle groups to generate motions such as push/pull. The joint torques are
computed using Lagrange's formulation to evaluate the dynamic factors of the
muscle fatigue model. An experiment is defined to validate this assumption and
the result for one person confirms its feasibility. The evaluation of this
model can predict the fatigue and MSD risk in industry production quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1757</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1757</id><created>2012-04-08</created><authors><author><keyname>Klimchik</keyname><forenames>Alexandr</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Pashkevich</keyname><forenames>Anatoly</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Hovland</keyname><forenames>Geir</forenames></author></authors><title>Compensation of compliance errors in parallel manipulators composed of
  non-perfect kinematic chains</title><categories>cs.RO</categories><comments>Advances in Robot Kinematics, France (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the compliance errors compensation for parallel
manipulators under external loading. Proposed approach is based on the
non-linear stiffness modeling and reduces to a proper adjusting of a target
trajectory. In contrast to previous works, in addition to compliance errors
caused by machining forces, the problem of assembling errors caused by
inaccuracy in the kinematic chains is considered. The advantages and practical
significance of the proposed approach are illustrated by examples that deal
with groove milling with Orthoglide manipulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1764</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1764</id><created>2012-04-08</created><authors><author><keyname>Chermakani</keyname><forenames>Deepak Ponvel</forenames></author></authors><title>A Non-Triviality Certificate for Scalars and its application to Linear
  Systems</title><categories>cs.CC math.AG</categories><comments>6 pages, 10 figures, 1 Theorem on the non-triviality Certificate for
  Scalars</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach of taking a linear weighted Average of N given
scalars, such that this Average is zero, if and only if, all N scalars are
zero. The weights for the scalars in this Average vary asymptotically with
respect to a large positive real. We use this approach with a previous result
on Asymptotic Linear Programming, to develop an O(M^4) Algorithm that decides
whether or not a system of M Linear Inequalities is feasible, and, whether or
not any desired subset of the variables in this system, is permitted to have a
non-trivial solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1789</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1789</id><created>2012-04-09</created><authors><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Agrawal</keyname><forenames>N. K.</forenames></author></authors><title>R.F. Pollution Reduction in Cellular Communication</title><categories>cs.CY</categories><comments>Erroneous submission in violation of copyright</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erroneous submission in violation of copyright removed by arXiv admin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1790</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1790</id><created>2012-04-09</created><authors><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Agrawal</keyname><forenames>N. K.</forenames></author></authors><title>Smart Antenna for Cellular Mobile Communication</title><categories>cs.NI cs.CY</categories><comments>12 pages, 8 figures, international journal</comments><journal-ref>VSRD International Journal of Electrical, Electronics &amp; Comm.
  Engg. Vol. 1 (9), 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The adoption of smart / adaptive antenna techniques in future wireless
systems is expected to have a significant impact on the efficient use of the
spectrum, the minimization of the cost of establishing new wireless networks,
the optimization of service quality and realization of transparent operation
across multi technology wireless networks [1]. This paper presents brief
account on smart antenna (SA) system. SAs can place nulls in the direction of
interferers via adaptive updating of weights linked to each antenna element.
SAs thus cancel out most of the co-channel interference resulting in better
quality of reception and lower dropped calls. SAs can also track the user
within a cell via direction of arrival algorithms [2]. This paper explains the
architecture, evolution and how the smart / adaptive antenna differs from the
basic format of antenna. The paper further explains about the radiation pattern
of the antenna and why it is highly preferred in its relative field. The
capabilities of smart / adaptive antenna are easily employable to Cognitive
Radio and OFDMA system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1797</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1797</id><created>2012-04-09</created><authors><author><keyname>Sarkar</keyname><forenames>Arindam</forenames></author><author><keyname>Karforma</keyname><forenames>S.</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Object Oriented Modelling of Idea using GA based efficient key
  generation for e-governance security (OOMIG)</title><categories>cs.CR</categories><comments>13 Pages, 4 Figures</comments><journal-ref>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.2, March 2012, PAGE NO: 171-183</journal-ref><doi>10.5121/ijdps.2012.3215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays different state and central government in India as well as abroad
are taking initiative to deliver different kind of services electronically
specially using Information and Communication Technology (ICT). Intruders or
hackers can steal or modify the information communicated between Government and
consumer through internet. To implement privacy and confidentiality of the
information we must use suitable encryption technique. In this paper an Object
Oriented Modelling of International Data Encryption Algorithm (IDEA) using GA
based efficient key generation technique has been proposed to incorporate
privacy and confidentiality of information which would be communicated between
government and consumer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1800</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1800</id><created>2012-04-09</created><updated>2013-04-01</updated><authors><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>On Power-law Kernels, corresponding Reproducing Kernel Hilbert Space and
  Applications</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>7 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The role of kernels is central to machine learning. Motivated by the
importance of power-law distributions in statistical modeling, in this paper,
we propose the notion of power-law kernels to investigate power-laws in
learning problem. We propose two power-law kernels by generalizing Gaussian and
Laplacian kernels. This generalization is based on distributions, arising out
of maximization of a generalized information measure known as nonextensive
entropy that is very well studied in statistical mechanics. We prove that the
proposed kernels are positive definite, and provide some insights regarding the
corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical
significance of both kernels in classification and regression, and present some
simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1802</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1802</id><created>2012-04-09</created><authors><author><keyname>Maan</keyname><forenames>Jitendra</forenames></author></authors><title>Mobile Web - Strategy for Enterprise Success</title><categories>cs.OH</categories><comments>9 pages, 1 figure, International Journal on Web Service Computing
  (IJWSC), Vol.3, No.1, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, enterprises are faced with increased global competition in an
environment where customers are demanding faster delivery, better service and
also want to gain significant and immediate business value by increasing
productivity and reducing operational cost. Spurred by unprecedented customer
demand, each Industry cluster has developed its own source of comparative
advantage. Even within a single organization, the business value chain is
geographically fragmented. Such diversification and fragmentation of value
chain drives the need for cross-platform Web applications over mobile channel.
Mobile Web is the next logical transition in this evolutionary process and
Mobile Web applications will continue to gain more prominence in the
enterprises not just to improve the return on investment in their existing
system landscape, but also to expand global reach and improve operational
efficiency of their mobile workforce. This paper outlines the critical business
needs to rapidly create flexible Mobile web solutions across all lines of
business. The paper enlightens the benefits offered by enabling web
applications on Mobile devices and also addresses the current business
challenges in developing Mobile Web applications. This paper is intended for
all business domains irrespective of application portfolios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1808</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1808</id><created>2012-04-09</created><authors><author><keyname>Maqsood</keyname><forenames>Asad</forenames></author><author><keyname>Khan</keyname><forenames>Rehanullah</forenames></author></authors><title>Vehicular Ad-hoc Networks</title><categories>cs.NI</categories><comments>International Journal of Computer Science Issues (IJCSI), Volume 9,
  Issue 1, No 3, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern day's vehicles require advanced communication system on board to
enable passengers benefit the most from available services. IEEE 802.11p is the
new extension of IEEE 802.11 standards; especially proposed for the high
vehicular environment. The WAVE documentation represents enhancements to the
Media Access Control (MAC) and Physical (PHY) layer of IEEE 802.11 standards to
work efficiently in high vehicular environment. In this research work, the main
emphasis is on the new IEEE 802.11p enhancement of MAC and PHY layers. More
specifically, the target of this research is to setup a simulation environment
which will allow us to investigate the use of real time voice application,
using IEEE 802.11p (WAVE) enhance setting, in a single hop and multi-hop
environment where nodes are not directly connected. Also, the evaluation of
transmission between moving nodes are tested by simply sending and receiving
FTP file between them with varying speed of the moving nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1811</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1811</id><created>2012-04-09</created><authors><author><keyname>Khan</keyname><forenames>Rehanullah</forenames></author><author><keyname>Maqsood</keyname><forenames>Asad</forenames></author><author><keyname>Khan</keyname><forenames>Zeeshan</forenames></author><author><keyname>Ishaq</keyname><forenames>Muhammad</forenames></author><author><keyname>Arif</keyname><forenames>Arsalan</forenames></author></authors><title>Skin-color based videos categorization</title><categories>cs.CV cs.AI</categories><comments>International Journal of Computer Science Issues (IJCSI), Volume 9,
  Issue 1, No 3, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On dedicated websites, people can upload videos and share it with the rest of
the world. Currently these videos are cat- egorized manually by the help of the
user community. In this paper, we propose a combination of color spaces with
the Bayesian network approach for robust detection of skin color followed by an
automated video categorization. Exper- imental results show that our method can
achieve satisfactory performance for categorizing videos based on skin color.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1815</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1815</id><created>2012-04-09</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Using Nyquist or Nyquist-Like Plot to Predict Three Typical
  Instabilities in DC-DC Converters</title><categories>cs.SY math.DS nlin.CD</categories><comments>Submitted to an IEEE journal in 2011</comments><journal-ref>Journal of the Franklin Institute, 350(10), pp. 3293-3312, Dec.
  2013</journal-ref><doi>10.1016/j.jfranklin.2013.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By transforming an exact stability condition, a new Nyquist-like plot is
proposed to predict occurrences of three typical instabilities in DC-DC
converters. The three instabilities are saddle-node bifurcation (coexistence of
multiple solutions), period-doubling bifurcation (subharmonic oscillation), and
Neimark bifurcation (quasi-periodic oscillation). In a single plot, it
accurately predicts whether an instability occurs and what type the instability
is. The plot is equivalent to the Nyquist plot, and it is a useful design tool
to avoid these instabilities. Nine examples are used to illustrate the accuracy
of this new plot to predict instabilities in the buck or boost converter with
fixed or variable switching frequency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1820</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1820</id><created>2012-04-09</created><authors><author><keyname>Preetha</keyname><forenames>K. G.</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>A.</forenames></author><author><keyname>Jacob</keyname><forenames>K. Poulose</forenames></author></authors><title>A probabilistic approach to reduce the route establishment overhead in
  AODV algorithm for manet</title><categories>cs.NI</categories><comments>International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.3, No.2, March 2012</comments><doi>10.5121/ijdps.2012.3218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad-hoc Networks (MANETS) is a collection of wireless nodes without any
infrastructure support. The nodes in MANET can act as either router or source
and the control of the network is distributed among nodes. The nodes in MANETS
are highly mobile and it maintains dynamic interconnection between those mobile
nodes. MANTEs have been considered as isolated stand-alone network. This can
turn the dream of networking &quot;at any time and at any where&quot; into reality. The
main purpose of this paper is to study the issues in route discovery process in
AODV protocol for MANET. Flooding of route request message imposes major
concern in route establishment. This paper suggests a new approach to reduce
the routing overhead during the route discovery phase. By considering the
previous behaviour of the network, the new protocol reduces the unwanted
searches during route establishment process
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1821</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1821</id><created>2012-04-09</created><updated>2013-06-26</updated><authors><author><keyname>Haruna</keyname><forenames>Taichi</forenames></author><author><keyname>Nakajima</keyname><forenames>Kohei</forenames></author></authors><title>Permutation Complexity and Coupling Measures in Hidden Markov Models</title><categories>nlin.CD cs.IT math.IT physics.data-an</categories><comments>26 pages</comments><doi>10.3390/e15093910</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [Haruna, T. and Nakajima, K., 2011. Physica D 240, 1370-1377], the authors
introduced the duality between values (words) and orderings (permutations) as a
basis to discuss the relationship between information theoretic measures for
finite-alphabet stationary stochastic processes and their permutation
analogues. It has been used to give a simple proof of the equality between the
entropy rate and the permutation entropy rate for any finite-alphabet
stationary stochastic process and show some results on the excess entropy and
the transfer entropy for finite-alphabet stationary ergodic Markov processes.
In this paper, we extend our previous results to hidden Markov models and show
the equalities between various information theoretic complexity and coupling
measures and their permutation analogues. In particular, we show the following
two results within the realm of hidden Markov models with ergodic internal
processes: the two permutation analogues of the transfer entropy, the symbolic
transfer entropy and the transfer entropy on rank vectors, are both equivalent
to the transfer entropy if they are considered as the rates, and the directed
information theory can be captured by the permutation entropy approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1824</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1824</id><created>2012-04-09</created><authors><author><keyname>Almeida</keyname><forenames>Fernando</forenames></author></authors><title>Web 2.0 Technologies and Social Networking Security Fears in Enterprises</title><categories>cs.CR</categories><comments>5 pages, 2 figures, 1 table; (IJACSA) International Journal of
  Advanced Computer Science and Applications, Vol. 3, No. 2, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web 2.0 systems have drawn the attention of corporation, many of which now
seek to adopt Web 2.0 technologies and transfer its benefits to their
organizations. However, with the number of different social networking
platforms appearing, privacy and security continuously has to be taken into
account and looked at from different perspectives. This paper presents the most
common security risks faced by the major Web 2.0 applications. Additionally, it
introduces the most relevant paths and best practices to avoid these identified
security risks in a corporate environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1832</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1832</id><created>2012-04-09</created><updated>2012-04-12</updated><authors><author><keyname>Xie</keyname><forenames>Hong</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Mathematical Modeling of Competitive Group Recommendation Systems with
  Application to Peer Review Systems</title><categories>cs.IR cs.PF</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a mathematical model to capture various factors
which may influence the accuracy of a competitive group recommendation system.
We apply this model to peer review systems, i.e., conference or research grants
review, which is an essential component in our scientific community. We explore
number of important questions, i.e., how will the number of reviews per paper
affect the accuracy of the overall recommendation? Will the score aggregation
policy influence the final recommendation? How reviewers' preference may affect
the accuracy of the final recommendation? To answer these important questions,
we formally analyze our model. Through this analysis, we obtain the insight on
how to design a randomized algorithm which is both computationally efficient
and asymptotically accurate in evaluating the accuracy of a competitive group
recommendation system. We obtain number of interesting observations: i.e., for
a medium tier conference, three reviews per paper is sufficient for a high
accuracy recommendation. For prestigious conferences, one may need at least
seven reviews per paper to achieve high accuracy. We also propose a
heterogeneous review strategy which requires equal or less reviewing workload,
but can improve over a homogeneous review strategy in recommendation accuracy
by as much as 30% . We believe our models and methodology are important
building blocks to study competitive group recommendation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1839</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1839</id><created>2012-04-09</created><authors><author><keyname>Hanafi</keyname><forenames>Hafizul Fahri</forenames></author><author><keyname>Samsudin</keyname><forenames>Khairulanuar</forenames></author></authors><title>Mobile Learning Environment System (MLES): The Case of Android-based
  Learning Application on Undergraduates' Learning</title><categories>cs.CY</categories><comments>5 pages, 3 figures, 2 tables, International Journal of Advanced
  Computer Science and Applications(IJACSA) Vol.3, No.3, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Of late, mobile technology has introduced new, novel environment that can be
capitalized to further enrich the teaching and learning process in classrooms.
Taking cognizance of this promising setting, a study was undertaken to
investigate the impact of such an environment enabled by android platform on
the learning process among undergraduates of Sultan Idris Education University,
Malaysia; in particular, this paper discusses critical aspects of the design
and implementation of the android learning system. Data were collected through
a survey involving 56 respondents, and these data were analyzed by using SPSS
12.0. Findings showed that the respondents were very receptive to the
interactivity, accessibility, and convenience of the system, but they were
quite frustrated with the occasional interruptions due to internet connectivity
problems. Overall, the mobile learning system can be utilized as an inexpensive
but potent learning tool that complements undergraduates' learning process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1840</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1840</id><created>2012-04-09</created><authors><author><keyname>Azghadi</keyname><forenames>Mostafa Rahimi</forenames></author><author><keyname>Al-Sarawi</keyname><forenames>Said</forenames></author><author><keyname>Iannella</keyname><forenames>Nicolangelo</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author></authors><title>Design and Implementation of BCM Rule Based on Spike-Timing Dependent
  Plasticity</title><categories>cs.OH</categories><journal-ref>Proceedings of the International Joint Conference on Neural
  Networks, held in Brisbane, 10-15 June, 2012: pp.1-7</journal-ref><doi>10.1109/IJCNN.2012.6252778</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bienenstock-Cooper-Munro (BCM) and Spike Timing-Dependent Plasticity
(STDP) rules are two experimentally verified form of synaptic plasticity where
the alteration of synaptic weight depends upon the rate and the timing of pre-
and post-synaptic firing of action potentials, respectively. Previous studies
have reported that under specific conditions, i.e. when a random train of
Poissonian distributed spikes are used as inputs, and weight changes occur
according to STDP, it has been shown that the BCM rule is an emergent property.
Here, the applied STDP rule can be either classical pair-based STDP rule, or
the more powerful triplet-based STDP rule. In this paper, we demonstrate the
use of two distinct VLSI circuit implementations of STDP to examine whether BCM
learning is an emergent property of STDP. These circuits are stimulated with
random Poissonian spike trains. The first circuit implements the classical
pair-based STDP, while the second circuit realizes a previously described
triplet-based STDP rule. These two circuits are simulated using 0.35 um CMOS
standard model in HSpice simulator. Simulation results demonstrate that the
proposed triplet-based STDP circuit significantly produces the threshold-based
behaviour of the BCM. Also, the results testify to similar behaviour for the
VLSI circuit for pair-based STDP in generating the BCM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1845</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1845</id><created>2012-04-09</created><authors><author><keyname>Azghadi</keyname><forenames>Mostafa Rahimi</forenames></author><author><keyname>Kavehie</keyname><forenames>O.</forenames></author><author><keyname>Navi</keyname><forenames>K.</forenames></author></authors><title>A Novel Design for Quantum-dot Cellular Automata Cells and Full Adders</title><categories>cs.ET</categories><comments>Erroneous submission in violation of copyright</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erroneous submission in violation of copyright removed by arXiv admin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1846</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1846</id><created>2012-04-09</created><updated>2014-05-27</updated><authors><author><keyname>Hart</keyname><forenames>Sergiu</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Approximate Revenue Maximization with Multiple Items</title><categories>cs.GT</categories><comments>Presented in ACM EC conference, 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Myerson's classic result provides a full description of how a seller can
maximize revenue when selling a single item. We address the question of revenue
maximization in the simplest possible multi-item setting: two items and a
single buyer who has independently distributed values for the items, and an
additive valuation. In general, the revenue achievable from selling two
independent items may be strictly higher than the sum of the revenues
obtainable by selling each of them separately. In fact, the structure of
optimal (i.e., revenue-maximizing) mechanisms for two items even in this simple
setting is not understood.
  In this paper we obtain approximate revenue optimization results using two
simple auctions: that of selling the items separately, and that of selling them
as a single bundle. Our main results (which are of a &quot;direct sum&quot; variety, and
apply to any distributions) are as follows. Selling the items separately
guarantees at least half the revenue of the optimal auction; for identically
distributed items, this becomes at least 73% of the optimal revenue. For the
case of k&gt;2 items, we show that selling separately guarantees at least a
c/log^2(k) fraction of the optimal revenue; for identically distributed items,
the bundling auction yields at least a c/log(k) fraction of the optimal
revenue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1848</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1848</id><created>2012-04-09</created><updated>2013-11-16</updated><authors><author><keyname>Song</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Godskesen</keyname><forenames>Jens Chr.</forenames></author></authors><title>Bisimulations and Logical Characterizations on Continuous-time Markov
  Decision Processes</title><categories>cs.LO</categories><comments>The conference version of this paper was published at VMCAI 2014</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study strong and weak bisimulation equivalences for
continuous-time Markov decision processes (CTMDPs) and the logical
characterizations of these relations with respect to the continuous-time
stochastic logic (CSL). For strong bisimulation, it is well known that it is
strictly finer than CSL equivalence. In this paper we propose strong and weak
bisimulations for CTMDPs and show that for a subclass of CTMDPs, strong and
weak bisimulations are both sound and complete with respect to the equivalences
induced by CSL and the sub-logic of CSL without next operator respectively. We
then consider a standard extension of CSL, and show that it and its sub-logic
without X can be fully characterized by strong and weak bisimulations
respectively over arbitrary CTMDPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1851</identifier>
 <datestamp>2015-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1851</id><created>2012-04-09</created><updated>2013-04-29</updated><authors><author><keyname>Skarlatidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Artikis</keyname><forenames>Alexander</forenames></author><author><keyname>Filippou</keyname><forenames>Jason</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author></authors><title>A Probabilistic Logic Programming Event Calculus</title><categories>cs.AI</categories><comments>Accepted for publication in the Theory and Practice of Logic
  Programming (TPLP) journal</comments><doi>10.1017/S1471068413000690</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system for recognising human activity given a symbolic
representation of video content. The input of our system is a set of
time-stamped short-term activities (STA) detected on video frames. The output
is a set of recognised long-term activities (LTA), which are pre-defined
temporal combinations of STA. The constraints on the STA that, if satisfied,
lead to the recognition of a LTA, have been expressed using a dialect of the
Event Calculus. In order to handle the uncertainty that naturally occurs in
human activity recognition, we adapted this dialect to a state-of-the-art
probabilistic logic programming framework. We present a detailed evaluation and
comparison of the crisp and probabilistic approaches through experimentation on
a benchmark dataset of human surveillance videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1868</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1868</id><created>2012-04-09</created><authors><author><keyname>Chorianopoulos</keyname><forenames>Konstantinos</forenames></author></authors><title>User-based key frame detection in social web video</title><categories>cs.MM cs.HC cs.IR</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video search results and suggested videos on web sites are represented with a
video thumbnail, which is manually selected by the video up-loader among three
randomly generated ones (e.g., YouTube). In contrast, we present a grounded
user-based approach for automatically detecting interesting key-frames within a
video through aggregated users' replay interactions with the video player.
Previous research has focused on content-based systems that have the benefit of
analyzing a video without user interactions, but they are monolithic, because
the resulting video thumbnails are the same regardless of the user preferences.
We constructed a user interest function, which is based on aggregate video
replays, and analyzed hundreds of user interactions. We found that the local
maximum of the replaying activity stands for the semantics of information rich
videos, such as lecture, and how-to. The concept of user-based key-frame
detection could be applied to any video on the web, in order to generate a
user-based and dynamic video thumbnail in search results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1873</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1873</id><created>2012-04-09</created><updated>2013-10-13</updated><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>A Characterization Theorem and An Algorithm for A Convex Hull Problem</title><categories>cs.CG</categories><comments>42 pages, 17 figures, 2 tables. This revision only corrects minor
  typos</comments><msc-class>90C05, 90C25, 65D18, 32C37</msc-class><acm-class>G.1.6; I.3.5; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $S= \{v_1, \dots, v_n\} \subset \mathbb{R} ^m$ and $p \in \mathbb{R}
^m$, testing if $p \in conv(S)$, the convex hull of $S$, is a fundamental
problem in computational geometry and linear programming. First, we prove a
Euclidean {\it distance duality}, distinct from classical separation theorems
such as Farkas Lemma: $p$ lies in $conv(S)$ if and only if for each $p' \in
conv(S)$ there exists a {\it pivot}, $v_j \in S$ satisfying $d(p',v_j) \geq
d(p,v_j)$. Equivalently, $p \not \in conv(S)$ if and only if there exists a
{\it witness}, $p' \in conv(S)$ whose Voronoi cell relative to $p$ contains
$S$. A witness separates $p$ from $conv(S)$ and approximate $d(p, conv(S))$ to
within a factor of two. Next, we describe the {\it Triangle Algorithm}: given
$\epsilon \in (0,1)$, an {\it iterate}, $p' \in conv(S)$, and $v \in S$, if
$d(p, p') &lt; \epsilon d(p,v)$, it stops. Otherwise, if there exists a pivot
$v_j$, it replace $v$ with $v_j$ and $p'$ with the projection of $p$ onto the
line $p'v_j$. Repeating this process, the algorithm terminates in $O(mn \min
\{\epsilon^{-2}, c^{-1}\ln \epsilon^{-1} \})$ arithmetic operations, where $c$
is the {\it visibility factor}, a constant satisfying $c \geq \epsilon^2$ and
$\sin (\angle pp'v_j) \leq 1/\sqrt{1+c}$, over all iterates $p'$. Additionally,
(i) we prove a {\it strict distance duality} and a related minimax theorem,
resulting in more effective pivots; (ii) describe $O(mn \ln
\epsilon^{-1})$-time algorithms that may compute a witness or a good
approximate solution; (iii) prove {\it generalized distance duality} and
describe a corresponding generalized Triangle Algorithm; (iv) prove a {\it
sensitivity theorem} to analyze the complexity of solving LP feasibility via
the Triangle Algorithm. The Triangle Algorithm is practical and competitive
with the simplex method, sparse greedy approximation and first-order methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1880</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1880</id><created>2012-04-09</created><updated>2012-04-15</updated><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author><author><keyname>Okoudjou</keyname><forenames>Kasso A.</forenames></author><author><keyname>Philipp</keyname><forenames>Friedrich</forenames></author><author><keyname>Tuley</keyname><forenames>Elizabeth K.</forenames></author></authors><title>Scalable Frames</title><categories>math.NA cs.IT cs.NA math.FA math.IT</categories><comments>19 pages</comments><msc-class>12D10, 14P05, 15A03, 15A12, 42C15, 65F08</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tight frames can be characterized as those frames which possess optimal
numerical stability properties. In this paper, we consider the question of
modifying a general frame to generate a tight frame by rescaling its frame
vectors; a process which can also be regarded as perfect preconditioning of a
frame by a diagonal operator. A frame is called scalable, if such a diagonal
operator exists. We derive various characterizations of scalable frames,
thereby including the infinite-dimensional situation. Finally, we provide a
geometric interpretation of scalability in terms of conical surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1881</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1881</id><created>2012-04-09</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author></authors><title>Four Conceptions of Instruction Sequence Faults</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of an instruction sequence fault is considered from various
perspectives. Four different viewpoints on what constitutes a fault, or how to
use the notion of a fault, are formulated. An integration of these views is
proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1894</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1894</id><created>2012-04-09</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Accounting for the Uncertainty in the Evaluation of Percentile Ranks</title><categories>cs.DL stat.AP</categories><comments>Journal of the American Society for Information Science and
  Technology (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper entitled &quot;Inconsistencies of Recently Proposed Citation
Impact Indicators and how to Avoid Them,&quot; Schreiber (2012, at arXiv:1202.3861)
proposed (i) a method to assess tied ranks consistently and (ii) fractional
attribution to percentile ranks in the case of relatively small samples (e.g.,
for n &lt; 100). Schreiber's solution to the problem of how to handle tied ranks
is convincing, in my opinion (cf. Pudovkin &amp; Garfield, 2009). The fractional
attribution, however, is computationally intensive and cannot be done manually
for even moderately large batches of documents. Schreiber attributed scores
fractionally to the six percentile rank classes used in the Science and
Engineering Indicators of the U.S. National Science Board, and thus missed, in
my opinion, the point that fractional attribution at the level of hundred
percentiles-or equivalently quantiles as the continuous random variable-is only
a linear, and therefore much less complex problem. Given the quantile-values,
the non-linear attribution to the six classes or any other evaluation scheme is
then a question of aggregation. A new routine based on these principles
(including Schreiber's solution for tied ranks) is made available as software
for the assessment of documents retrieved from the Web of Science (at
http://www.leydesdorff.net/software/i3).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1909</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1909</id><created>2012-04-09</created><authors><author><keyname>Tran-Thanh</keyname><forenames>Long</forenames></author><author><keyname>Chapman</keyname><forenames>Archie</forenames></author><author><keyname>Rogers</keyname><forenames>Alex</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In budget-limited multi-armed bandit (MAB) problems, the learner's actions
are costly and constrained by a fixed budget. Consequently, an optimal
exploitation policy may not be to pull the optimal arm repeatedly, as is the
case in other variants of MAB, but rather to pull the sequence of different
arms that maximises the agent's total reward within the budget. This difference
from existing MABs means that new approaches to maximising the total reward are
required. Given this, we develop two pulling policies, namely: (i) KUBE; and
(ii) fractional KUBE. Whereas the former provides better performance up to 40%
in our experimental settings, the latter is computationally less expensive. We
also prove logarithmic upper bounds for the regret of both policies, and show
that these bounds are asymptotically optimal (i.e. they only differ from the
best possible regret by a constant factor).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1910</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1910</id><created>2012-04-09</created><updated>2012-04-18</updated><authors><author><keyname>Geng</keyname><forenames>Yanfeng</forenames></author><author><keyname>Cassandras</keyname><forenames>Christos G.</forenames></author></authors><title>Multi-intersection Traffic Light Control Using Infinitesimal
  Perturbation Analysis</title><categories>cs.SY</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the traffic light control problem for multiple intersections in
tandem by viewing it as a stochastic hybrid system and developing a Stochastic
Flow Model (SFM) for it. Using Infinitesimal Perturbation Analysis (IPA), we
derive on-line gradient estimates of a cost metric with respect to the
controllable green and red cycle lengths. The IPA estimators obtained require
counting traffic light switchings and estimating car flow rates only when
specific events occur. The estimators are used to iteratively adjust light
cycle lengths to improve performance and, in conjunction with a standard
gradient-based algorithm, to obtain optimal values which adapt to changing
traffic conditions. Simulation results are included to illustrate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1912</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1912</id><created>2012-04-09</created><authors><author><keyname>Chern</keyname><forenames>Bobbie</forenames></author><author><keyname>Ochoa</keyname><forenames>Idoia</forenames></author><author><keyname>Manolakos</keyname><forenames>Alexandros</forenames></author><author><keyname>No</keyname><forenames>Albert</forenames></author><author><keyname>Venkat</keyname><forenames>Kartik</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Reference Based Genome Compression</title><categories>cs.IT math.IT</categories><comments>5 pages; Submitted to the IEEE Information Theory Workshop (ITW) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DNA sequencing technology has advanced to a point where storage is becoming
the central bottleneck in the acquisition and mining of more data. Large
amounts of data are vital for genomics research, and generic compression tools,
while viable, cannot offer the same savings as approaches tuned to inherent
biological properties. We propose an algorithm to compress a target genome
given a known reference genome. The proposed algorithm first generates a
mapping from the reference to the target genome, and then compresses this
mapping with an entropy coder. As an illustration of the performance: applying
our algorithm to James Watson's genome with hg18 as a reference, we are able to
reduce the 2991 megabyte (MB) genome down to 6.99 MB, while Gzip compresses it
to 834.8 MB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1920</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1920</id><created>2012-04-09</created><updated>2013-10-17</updated><authors><author><keyname>Sidorov</keyname><forenames>Nikita</forenames></author></authors><title>Supercritical holes for the doubling map</title><categories>math.DS cs.DM</categories><comments>This is a new version, where a full characterization of supercritical
  holes for the doubling map is obtained</comments><journal-ref>Acta Math. Hung. 143 (2014), 298-312</journal-ref><doi>10.1007/s10474-014-0403-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a map $S:X\to X$ and an open connected set ($=$ a hole) $H\subset X$ we
define $\mathcal J_H(S)$ to be the set of points in $X$ whose $S$-orbit avoids
$H$. We say that a hole $H_0$ is supercritical if (i) for any hole $H$ such
that $\bar{H_0}\subset H$ the set $\mathcal J_H(S)$ is either empty or contains
only fixed points of $S$; (ii) for any hole $H$ such that $\barH\subset H_0$
the Hausdorff dimension of $\mathcal J_H(S)$ is positive.
  The purpose of this note to completely characterize all supercritical holes
for the doubling map $Tx=2x\bmod1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1924</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1924</id><created>2012-04-09</created><authors><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Two-Way Communication with Energy Exchange</title><categories>cs.IT math.IT</categories><comments>A shorter version has been submitted to Information Theory Workshop,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional assumption made in the design of communication systems is
that the energy used to transfer information between a sender and a recipient
cannot be reused for future communication tasks. A notable exception to this
norm is given by passive RFID systems, in which a reader can transfer both
information and energy via the transmitted radio signal. Conceivably, any
system that exchanges information via the transfer of given physical resources
(radio waves, particles, qubits) can potentially reuse, at least part, of the
received resources for communication later on. In this paper, a two-way
communication system is considered that operates with a given initial number of
physical resources, referred to as energy units. The energy units are not
replenished from outside the system, and are assumed, for simplicity, to be
constant over time. A node can either send an &quot;on&quot; symbol (or &quot;1&quot;), which costs
one unit of energy, or an &quot;off&quot; signal (or &quot;0&quot;), which does not require any
energy expenditure. Upon reception of a &quot;1&quot; signal, the recipient node
&quot;harvests&quot; the energy contained in the signal and stores it for future
communication tasks. Inner and outer bounds on the achievable rates are
derived, and shown via numerical results to coincide if the number of energy
units is large enough.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1933</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1933</id><created>2012-04-09</created><authors><author><keyname>Kapetanovic</keyname><forenames>D.</forenames></author><author><keyname>Cheng</keyname><forenames>H. V.</forenames></author><author><keyname>Mow</keyname><forenames>W. H.</forenames></author><author><keyname>Rusek</keyname><forenames>F.</forenames></author></authors><title>A Lattice-Theoretic Characterization of Optimal Minimum-Distance Linear
  Precoders</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates linear precoding over non-singular linear channels
with additive white Gaussian noise, with lattice-type inputs. The aim is to
maximize the minimum distance of the received lattice points, where the
precoder is subject to an energy constraint. It is shown that the optimal
precoder only produces a finite number of different lattices, namely perfect
lattices, at the receiver. The well-known densest lattice packings are
instances of perfect lattices, however it is analytically shown that the
densest lattices are not always the solution. This is a counter-intuitive
result at first sight, since previous work in the area showed a tight
connection between densest lattices and minimum distance. Since there are only
finitely many different perfect lattices, they can theoretically be enumerated
off-line. A new upper bound on the optimal minimum distance is derived, which
significantly improves upon a previously reported bound. Based on this bound,
we propose an enumeration algorithm that produces a finite codebook of optimal
precoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1935</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1935</id><created>2012-04-09</created><authors><author><keyname>Chen</keyname><forenames>Xinjia</forenames></author></authors><title>New Sequential Methods for Detecting Portscanners</title><categories>stat.AP cs.CR</categories><comments>11 pages, 5 figures, the mathematical theory of the detection
  algorithm has been presented in SPIE conferences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose new sequential methods for detecting port-scan
attackers which routinely perform random &quot;portscans&quot; of IP addresses to find
vulnerable servers to compromise. In addition to rigorously control the
probability of falsely implicating benign remote hosts as malicious, our method
performs significantly faster than other current solutions. Moreover, our
method guarantees that the maximum amount of observational time is bounded. In
contrast to the previous most effective method, Threshold Random Walk
Algorithm, which is explicit and analytical in nature, our proposed algorithm
involve parameters to be determined by numerical methods. We have developed
computational techniques such as iterative minimax optimization for quick
determination of the parameters of the new detection algorithm. A framework of
multi-valued decision for testing portscanners is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1939</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1939</id><created>2012-04-09</created><updated>2012-05-27</updated><authors><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Cooper</keyname><forenames>Colin</forenames></author><author><keyname>Friedetzky</keyname><forenames>Tom</forenames></author></authors><title>Random walks which prefer unvisited edges. Exploring high girth even
  degree expanders in linear time</title><categories>cs.DS cs.DC math.CO math.PR</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a modified random walk which uses unvisited edges whenever
possible, and makes a simple random walk otherwise. We call such a walk an
edge-process. We assume there is a rule A, which tells the walk which unvisited
edge to use whenever there is a choice. In the simplest case, A is a uniform
random choice over unvisited edges incident with the current walk position.
However we do not exclude arbitrary choices of rule A. For example, the rule
could be determined on-line by an adversary, or could vary from vertex to
vertex.
  For even degree expander graphs, of bounded maximum degree, we have the
following result. Let G be an n vertex even degree expander graph, for which
every vertex is in at least one vertex induced cycle of length L. Any
edge-process on G has cover time (n+ (n log n)/L). This result is independent
of the rule A used to select the order of the unvisited edges, which can be
chosen on-line by an adversary.
  As an example, With high probability, random r-regular graphs, (r at least 4,
even), are expanders for which L = Omega(log n). Thus, for almost all such
graphs, the vertex cover time of the edge-process is Theta(n). This improves
the vertex cover time of such graphs by a factor of log n, compared to the
Omega(n log n) cover time of any weighted random walk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1949</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1949</id><created>2012-04-09</created><authors><author><keyname>Hu</keyname><forenames>Xiao</forenames></author><author><keyname>Chen</keyname><forenames>Chuibo</forenames></author><author><keyname>Chen</keyname><forenames>Xiaolong</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author></authors><title>Social Recommender Systems Based on Coupling Network Structure Analysis</title><categories>cs.IR cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past few years has witnessed the great success of recommender systems,
which can significantly help users find relevant and interesting items for them
in the information era. However, a vast class of researches in this area mainly
focus on predicting missing links in bipartite user-item networks (represented
as behavioral networks). Comparatively, the social impact, especially the
network structure based properties, is relatively lack of study. In this paper,
we firstly obtain five corresponding network-based features, including user
activity, average neighbors' degree, clustering coefficient, assortative
coefficient and discrimination, from social and behavioral networks,
respectively. A hybrid algorithm is proposed to integrate those features from
two respective networks. Subsequently, we employ a machine learning process to
use those features to provide recommendation results in a binary classifier
method. Experimental results on a real dataset, Flixster, suggest that the
proposed method can significantly enhance the algorithmic accuracy. In
addition, as network-based properties consider not only the social activities,
but also take into account user preferences in the behavioral networks,
therefore, it performs much better than that from either social or behavioral
networks. Furthermore, since the features based on the behavioral network
contain more diverse and meaningfully structural information, they play a vital
role in uncovering users' potential preference, which, might show light in
deeply understanding the structure and function of the social and behavioral
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1956</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1956</id><created>2012-04-09</created><updated>2012-04-09</updated><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author></authors><title>Learning Topic Models - Going beyond SVD</title><categories>cs.LG cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic Modeling is an approach used for automatic comprehension and
classification of data in a variety of settings, and perhaps the canonical
application is in uncovering thematic structure in a corpus of documents. A
number of foundational works both in machine learning and in theory have
suggested a probabilistic model for documents, whereby documents arise as a
convex combination of (i.e. distribution on) a small number of topic vectors,
each topic vector being a distribution on words (i.e. a vector of
word-frequencies). Similar models have since been used in a variety of
application areas; the Latent Dirichlet Allocation or LDA model of Blei et al.
is especially popular.
  Theoretical studies of topic modeling focus on learning the model's
parameters assuming the data is actually generated from it. Existing approaches
for the most part rely on Singular Value Decomposition(SVD), and consequently
have one of two limitations: these works need to either assume that each
document contains only one topic, or else can only recover the span of the
topic vectors instead of the topic vectors themselves.
  This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main
tool in this context, which is an analog of SVD where all vectors are
nonnegative. Using this tool we give the first polynomial-time algorithm for
learning topic models without the above two limitations. The algorithm uses a
fairly mild assumption about the underlying topic matrix called separability,
which is usually found to hold in real-life data. A compelling feature of our
algorithm is that it generalizes to models that incorporate topic-topic
correlations, such as the Correlated Topic Model and the Pachinko Allocation
Model.
  We hope that this paper will motivate further theoretical results that use
NMF as a replacement for SVD - just as NMF has come to replace SVD in many
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1957</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1957</id><created>2012-04-09</created><updated>2012-04-22</updated><authors><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author><author><keyname>Nicholson</keyname><forenames>Patrick K.</forenames></author></authors><title>Succinct Posets</title><categories>cs.DS</categories><comments>12 pages lncs format + short appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm for compressing a partially ordered set, or
\emph{poset}, so that it occupies space matching the information theory lower
bound (to within lower order terms), in the worst case. Using this algorithm,
we design a succinct data structure for representing a poset that, given two
elements, can report whether one precedes the other in constant time. This is
equivalent to succinctly representing the transitive closure graph of the
poset, and we note that the same method can also be used to succinctly
represent the transitive reduction graph. For an $n$ element poset, the data
structure occupies $n^2/4 + o(n^2)$ bits, in the worst case, which is roughly
half the space occupied by an upper triangular matrix. Furthermore, a slight
extension to this data structure yields a succinct oracle for reachability in
arbitrary directed graphs. Thus, using roughly a quarter of the space required
to represent an arbitrary directed graph, reachability queries can be supported
in constant time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1958</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1958</id><created>2012-04-09</created><updated>2013-01-14</updated><authors><author><keyname>Yang</keyname><forenames>Qingxuan</forenames></author><author><keyname>Ellis</keyname><forenames>John</forenames></author><author><keyname>Mamakani</keyname><forenames>Khalegh</forenames></author><author><keyname>Ruskey</keyname><forenames>Frank</forenames></author></authors><title>Parallel and sequential in-place permuting and perfect shuffling using
  involutions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any permutation of ${1,2,...,N}$ can be written as the product
of two involutions. As a consequence, any permutation of the elements of an
array can be performed in-place in parallel in time O(1). In the case where the
permutation is the $k$-way perfect shuffle we develop two methods for
efficiently computing such a pair of involutions.
  The first method works whenever $N$ is a power of $k$; in this case the time
is O(N) and space $O(\log^2 N)$. The second method applies to the general case
where $N$ is a multiple of $k$; here the time is $O(N \log N)$ and the space is
$O(\log^2 N)$. If $k=2$ the space usage of the first method can be reduced to
$O(\log N)$ on a machine that has a SADD (population count) instruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1967</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1967</id><created>2012-04-09</created><authors><author><keyname>Lee</keyname><forenames>Junha</forenames></author><author><keyname>Lee</keyname><forenames>Donghun</forenames></author><author><keyname>Kim</keyname><forenames>Dae-Kyoo</forenames></author><author><keyname>Park</keyname><forenames>Sooyong</forenames></author></authors><title>A Semantic-Based Approach for Detecting and Decomposing God Classes</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cohesion is a core design quality that has a great impact on posterior
development and maintenance. By the nature of software, the cohesion of a
system is diminished as the system evolves. God classes are code defects
resulting from software evolution, having heterogeneous responsibilities highly
coupled with other classes and often large in size, which makes it difficult to
maintain the system. The existing work on identifying and decomposing God
classes heavily relies on internal class information to identify God classes
and responsibilities. However, in object-oriented systems, responsibilities
should be analyzed with respect to not only internal class information, but
also method interactions. In this paper, we present a novel approach for
detecting God classes and decomposing their responsibilities based on the
semantics of methods and method interactions. We evaluate the approach using
JMeter v2.5.1 and the results are promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1990</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1990</id><created>2012-04-09</created><updated>2015-03-24</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames></author><author><keyname>Otto</keyname><forenames>Martin</forenames></author></authors><title>Pebble Games and Linear Equations</title><categories>cs.LO cs.CC</categories><msc-class>68Q19, 03C13</msc-class><acm-class>F.4.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new, simplified and detailed account of the correspondence between
levels of the Sherali-Adams relaxation of graph isomorphism and levels of
pebble-game equivalence with counting (higher-dimensional Weisfeiler-Lehman
colour refinement). The correspondence between basic colour refinement and
fractional isomorphism, due to Tinhofer (1986, 1991) and Ramana, Scheinerman
and Ullman (1994), is re-interpreted as the base level of Sherali-Adams and
generalised to higher levels in this sense by Atserias and Maneva (2012) and
Malkin (2014), who prove that the two resulting hierarchies interleave.
  In carrying this analysis further, we here give
  (a) a precise characterisation of the level-k Sherali-Adams relaxation in
terms of a modified counting pebble game;
  (b) a variant of the Sherali-Adams levels that precisely match the k-pebble
counting game;
  (c) a proof that the interleaving between these two hierarchies is strict.
  We also investigate the variation based on boolean arithmetic instead of
real/rational arithmetic and obtain analogous correspondences and separations
for plain k-pebble equivalence (without counting). Our results are driven by
considerably simplified accounts of the underlying combinatorics and linear
algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.1995</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.1995</id><created>2012-04-09</created><authors><author><keyname>Wollbold</keyname><forenames>Johannes</forenames></author></authors><title>Attribute Exploration of Gene Regulatory Processes</title><categories>q-bio.MN cs.CE cs.LO math.LO</categories><comments>111 pages, 9 figures, file size 2.1 MB, PhD thesis University of
  Jena, Germany, Faculty of Mathematics and Computer Science, 2011. Online
  available at http://www.db-thueringen.de/servlets/DocumentServlet?id=19601</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This thesis aims at the logical analysis of discrete processes, in particular
of such generated by gene regulatory networks. States, transitions and
operators from temporal logics are expressed in the language of Formal Concept
Analysis. By the attribute exploration algorithm, an expert or a computer
program is enabled to validate a minimal and complete set of implications, e.g.
by comparison of predictions derived from literature with observed data. Here,
these rules represent temporal dependencies within gene regulatory networks
including coexpression of genes, reachability of states, invariants or possible
causal relationships. This new approach is embedded into the theory of
universal coalgebras, particularly automata, Kripke structures and Labelled
Transition Systems. A comparison with the temporal expressivity of Description
Logics is made. The main theoretical results concern the integration of
background knowledge into the successive exploration of the defined data
structures (formal contexts). Applying the method a Boolean network from
literature modelling sporulation of Bacillus subtilis is examined. Finally, we
developed an asynchronous Boolean network for extracellular matrix formation
and destruction in the context of rheumatoid arthritis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2003</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2003</id><created>2012-04-09</created><updated>2015-03-11</updated><authors><author><keyname>Quinn</keyname><forenames>Christopher J.</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author><author><keyname>Coleman</keyname><forenames>Todd P.</forenames></author></authors><title>Directed Information Graphs</title><categories>cs.IT cs.AI math.IT stat.ML</categories><comments>41 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a graphical model for representing networks of stochastic
processes, the minimal generative model graph. It is based on reduced
factorizations of the joint distribution over time. We show that under
appropriate conditions, it is unique and consistent with another type of
graphical model, the directed information graph, which is based on a
generalization of Granger causality. We demonstrate how directed information
quantifies Granger causality in a particular sequential prediction setting. We
also develop efficient methods to estimate the topological structure from data
that obviate estimating the joint statistics. One algorithm assumes
upper-bounds on the degrees and uses the minimal dimension statistics
necessary. In the event that the upper-bounds are not valid, the resulting
graph is nonetheless an optimal approximation. Another algorithm uses
near-minimal dimension statistics when no bounds are known but the distribution
satisfies a certain criterion. Analogous to how structure learning algorithms
for undirected graphical models use mutual information estimates, these
algorithms use directed information estimates. We characterize the
sample-complexity of two plug-in directed information estimators and obtain
confidence intervals. For the setting when point estimates are unreliable, we
propose an algorithm that uses confidence intervals to identify the best
approximation that is robust to estimation error. Lastly, we demonstrate the
effectiveness of the proposed algorithms through analysis of both synthetic
data and real data from the Twitter network. In the latter case, we identify
which news sources influence users in the network by merely analyzing tweet
times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2009</identifier>
 <datestamp>2014-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2009</id><created>2012-04-09</created><updated>2014-06-17</updated><authors><author><keyname>Chang</keyname><forenames>Xiao-Wen</forenames></author><author><keyname>Wen</keyname><forenames>Jinming</forenames></author><author><keyname>Xie</keyname><forenames>Xiaohu</forenames></author></authors><title>Effects of the LLL reduction on the success probability of the Babai
  point and on the complexity of sphere decoding</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The common method to estimate an unknown integer parameter vector in a linear
model is to solve an integer least squares (ILS) problem. A typical approach to
solving an ILS problem is sphere decoding. To make a sphere decoder faster, the
well-known LLL reduction is often used as preprocessing. The Babai point
produced by the Babai nearest plan algorithm is a suboptimal solution of the
ILS problem. First we prove that the success probability of the Babai point as
a lower bound on the success probability of the ILS estimator is sharper than
the lower bound given by Hassibi and Boyd [1]. Then we show rigorously that
applying the LLL reduction algorithm will increase the success probability of
the Babai point. Finally we show rigorously that applying the LLL reduction
algorithm will also reduce the computational complexity of sphere decoders,
which is measured approximately by the number of nodes in the search tree in
the literature
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2018</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2018</id><created>2012-04-09</created><authors><author><keyname>Subbotin</keyname><forenames>Igor Ya.</forenames></author><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author></authors><title>Applications of fuzzy logic to Case-Based Reasoning</title><categories>cs.AI</categories><msc-class>03E72, 97A99</msc-class><acm-class>I.2.3; I.5.1; H.3</acm-class><journal-ref>International Journal of Applications of Fuzzy Sets (ISSN
  2241-1240) Vol. 1 ( 2011), 7-18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article discusses some applications of fuzzy logic ideas to formalizing
of the Case-Based Reasoning (CBR) process and to measuring the effectiveness of
CBR systems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2021</identifier>
 <datestamp>2012-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2021</id><created>2012-04-09</created><updated>2012-11-05</updated><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Approximating the Expansion Profile and Almost Optimal Local Graph
  Clustering</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral partitioning is a simple, nearly-linear time, algorithm to find
sparse cuts, and the Cheeger inequalities provide a worst-case guarantee for
the quality of the approximation found by the algorithm. Local graph
partitioning algorithms [ST08,ACL06,AP09] run in time that is nearly linear in
the size of the output set, and their approximation guarantee is worse than the
guarantee provided by the Cheeger inequalities by a polylogarithmic
$\log^{\Omega(1)} n$ factor. It has been a long standing open problem to design
a local graph clustering algorithm with an approximation guarantee close to the
guarantee of the Cheeger inequalities and with a running time nearly linear in
the size of the output.
  In this paper we solve this problem; we design an algorithm with the same
guarantee (up to a constant factor) as the Cheeger inequality, that runs in
time slightly super linear in the size of the output. This is the first
sublinear (in the size of the input) time algorithm with almost the same
guarantee as the Cheeger's inequality. As a byproduct of our results, we prove
a bicriteria approximation algorithm for the expansion profile of any graph.
Let $\phi(\gamma) = \min_{\mu(S) \leq \gamma}\phi(S)$. There is a polynomial
time algorithm that, for any $\gamma,\epsilon&gt;0$, finds a set $S$ of measure
$\mu(S)\leq 2\gamma^{1+\epsilon}$, and expansion $\phi(S)\leq
\sqrt{2\phi(\gamma)/\epsilon}$. Our proof techniques also provide a simpler
proof of the structural result of Arora, Barak, Steurer [ABS10], that can be
applied to irregular graphs.
  Our main technical tool is that for any set $S$ of vertices of a graph, a
lazy $t$-step random walk started from a randomly chosen vertex of $S$, will
remain entirely inside $S$ with probability at least $(1-\phi(S)/2)^t$. This
itself provides a new lower bound to the uniform mixing time of any finite
states reversible markov chain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2026</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2026</id><created>2012-04-09</created><updated>2014-12-14</updated><authors><author><keyname>Cui</keyname><forenames>Peng</forenames></author></authors><title>Strengthened Hardness for Approximating Minimum Unique Game and Small
  Set Expansion</title><categories>cs.CC</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the author puts forward a variation of Feige's Hypothesis,
which claims that it is hard on average refuting Unbalanced Max 3-XOR under
biased assignments on a natural distribution. Under this hypothesis, the author
strengthens the previous known hardness for approximating Minimum Unique Game,
$5/4-\epsilon$, by proving that Min 2-Lin-2 is hard to within $3/2-\epsilon$
and strengthens the previous known hardness for approximating Small Set
Expansion, $4/3-\epsilon$, by proving that Min Bisection is hard to approximate
within $3-\epsilon$. In addition, the author discusses the limitation of this
method to show that it can strengthen the hardness for approximating Minimum
Unique Game to $2-\kappa$ where $\kappa$ is a small absolute positive, but is
short of proving $\omega_k(1)$ hardness for Minimum Unique Game (or Small Set
Expansion), by assuming a generalization of this hypothesis on Unbalanced Max
k-CSP with Samorodnitsky-Trevisan hypergraph predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2031</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2031</id><created>2012-04-09</created><authors><author><keyname>Basu</keyname><forenames>Amitabh</forenames></author><author><keyname>De Loera</keyname><forenames>Jesus</forenames></author><author><keyname>Junod</keyname><forenames>Mark</forenames></author></authors><title>On Chubanov's method for Linear Programming</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the method recently proposed by S. Chubanov for the linear
feasibility problem. We present new, concise proofs and interpretations of some
of his results. We then show how our proofs can be used to find strongly
polynomial time algorithms for special classes of linear feasibility problems.
Under certain conditions, these results provide new proofs of classical results
obtained by Tardos, and Vavasis and Ye.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2032</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2032</id><created>2012-04-09</created><updated>2012-10-16</updated><authors><author><keyname>Zeng</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Li</forenames></author></authors><title>Multi-Output Recommender: Items, Groups and Friends, and Their Mutual
  Contributing Effects</title><categories>cs.IR</categories><comments>withdraw the article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the development of social media technology, it becomes easier for
users to gather together to form groups. Take the Last.fm for example, users
can join groups they may be interested where they can share their loved songs
and discuss topics about songs and singers. However, the number of groups grows
over time, users need effective groups recommendations in order to meet more
like-minded users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2033</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2033</id><created>2012-04-09</created><authors><author><keyname>Tune</keyname><forenames>Paul</forenames></author></authors><title>Computing Constrained Cramer Rao Bounds</title><categories>cs.IT cs.DS math.IT</categories><comments>Tech report version. Journal version was submitted to IEEE
  Transactions on Signal Processing. This version includes derivation of the
  iterations of the constrained majorization-minimization algoirthm, subject to
  parameter equality constraints. 3 figures</comments><doi>10.1109/TSP.2012.2204258</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of computing submatrices of the Cram\'er-Rao bound
(CRB), which lower bounds the variance of any unbiased estimator of a vector
parameter $\vth$. We explore iterative methods that avoid direct inversion of
the Fisher information matrix, which can be computationally expensive when the
dimension of $\vth$ is large. The computation of the bound is related to the
quadratic matrix program, where there are highly efficient methods for solving
it. We present several methods, and show that algorithms in prior work are
special instances of existing optimization algorithms. Some of these methods
converge to the bound monotonically, but in particular, algorithms converging
non-monotonically are much faster. We then extend the work to encompass the
computation of the CRB when the Fisher information matrix is singular and when
the parameter $\vth$ is subject to constraints. As an application, we consider
the design of a data streaming algorithm for network measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2034</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2034</id><created>2012-04-09</created><authors><author><keyname>Barbay</keyname><forenames>J.</forenames></author><author><keyname>Navarro</keyname><forenames>G.</forenames></author><author><keyname>P&#xe9;rez-Lantero</keyname><forenames>P.</forenames></author></authors><title>Adaptive Techniques to find Optimal Planar Boxes</title><categories>cs.CG cs.DS</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set $P$ of $n$ planar points, two axes and a real-valued score
function $f()$ on subsets of $P$, the Optimal Planar Box problem consists in
finding a box (i.e. axis-aligned rectangle) $H$ maximizing $f(H\cap P)$. We
consider the case where $f()$ is monotone decomposable, i.e. there exists a
composition function $g()$ monotone in its two arguments such that
$f(A)=g(f(A_1),f(A_2))$ for every subset $A\subseteq P$ and every partition
$\{A_1,A_2\}$ of $A$. In this context we propose a solution for the Optimal
Planar Box problem which performs in the worst case $O(n^2\lg n)$ score
compositions and coordinate comparisons, and much less on other classes of
instances defined by various measures of difficulty. A side result of its own
interest is a fully dynamic \textit{MCS Splay tree} data structure supporting
insertions and deletions with the \emph{dynamic finger} property, improving
upon previous results [Cort\'es et al., J.Alg. 2009].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2035</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2035</id><created>2012-04-09</created><updated>2012-10-31</updated><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chua</keyname><forenames>Kee-Chaing</forenames></author></authors><title>Wireless Information Transfer with Opportunistic Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Wireless Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting is a promising solution to prolong the operation of
energy-constrained wireless networks. In particular, scavenging energy from
ambient radio signals, namely wireless energy harvesting (WEH), has recently
drawn significant attention. In this paper, we consider a point-to-point
wireless link over the narrowband flat-fading channel subject to time-varying
co-channel interference. It is assumed that the receiver has no fixed power
supplies and thus needs to replenish energy opportunistically via WEH from the
unintended interference and/or the intended signal sent by the transmitter. We
further assume a single-antenna receiver that can only decode information or
harvest energy at any time due to the practical circuit limitation. Therefore,
it is important to investigate when the receiver should switch between the two
modes of information decoding (ID) and energy harvesting (EH), based on the
instantaneous channel and interference condition. In this paper, we derive the
optimal mode switching rule at the receiver to achieve various trade-offs
between wireless information transfer and energy harvesting. Specifically, we
determine the minimum transmission outage probability for delay-limited
information transfer and the maximum ergodic capacity for no-delay-limited
information transfer versus the maximum average energy harvested at the
receiver, which are characterized by the boundary of so-called &quot;outage-energy&quot;
region and &quot;rate-energy&quot; region, respectively. Moreover, for the case when the
channel state information (CSI) is known at the transmitter, we investigate the
joint optimization of transmit power control, information and energy transfer
scheduling, and the receiver's mode switching. Our results provide useful
guidelines for the efficient design of emerging wireless communication systems
powered by opportunistic WEH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2040</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2040</id><created>2012-04-10</created><authors><author><keyname>Hu</keyname><forenames>Gengran</forenames></author><author><keyname>Pan</keyname><forenames>Yanbin</forenames></author></authors><title>A New Reduction from Search SVP to Optimization SVP</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  It is well known that search SVP is equivalent to optimization SVP. However,
the former reduction from search SVP to optimization SVP by Kannan needs
polynomial times calls to the oracle that solves the optimization SVP. In this
paper, a new rank-preserving reduction is presented with only one call to the
optimization SVP oracle. It is obvious that the new reduction needs the least
calls, and improves Kannan's classical result. What's more, the idea also leads
a similar direct reduction from search CVP to optimization CVP with only one
call to the oracle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2041</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2041</id><created>2012-04-10</created><updated>2012-04-24</updated><authors><author><keyname>Ramalakshmi</keyname><forenames>R.</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>S.</forenames></author></authors><title>Improving Route Discovery Using Stable Connected Dominating Set in
  MANETs</title><categories>cs.NI</categories><comments>International Conference on NetCom-3.0</comments><journal-ref>International journal on applications of graph theory in wireless
  ad hoc networks and sensor networks(GRAPH-HOC), Vol 4, No.1, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Connected Dominating Set (CDS) based virtual backbone plays an important
role in wireless ad hoc networks for efficient routing and broadcasting. Each
node in the network can select some of its 1-hop neighbors as Multi Point Relay
(MPR) to cover all its 2-hop neighbors. A MPR based CDS is a promising approach
for broadcasting. A node in the CDS consumes more energy and the energy
depletes quickly than non dominating nodes. Although previous CDS construction
algorithms achieve good results in terms of the size of CDS, a minimum size CDS
does not necessarily guarantee an optimal network performance from an energy
efficient point of view. In this paper, we propose a distributed algorithm for
energy efficient stable MPR based CDS construction to extend the lifetime of ad
hoc wireless networks by considering energy and velocity of nodes. We have also
implemented route discovery protocol to make use of the CDS nodes to relay
route request messages. The simulation results show that our algorithm
increases the lifetime up to 25% than previous works and 60% reduction in the
route request messages during route discovery process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2048</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2048</id><created>2012-04-10</created><authors><author><keyname>Azghadi</keyname><forenames>Mostafa Rahimi</forenames></author><author><keyname>Kavehie</keyname><forenames>O.</forenames></author><author><keyname>Navi</keyname><forenames>K.</forenames></author></authors><title>A Novel Design for Quantum-dot Cellular Automata Cells and Full Adders</title><categories>cs.ET</categories><comments>arXiv admin note: text overlap with arXiv:cond-mat/0104406</comments><journal-ref>Journal of Applied Sciences, Vol. 7, No. 22, pp. 3460-3468, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum dot Cellular Automata (QCA) is a novel and potentially attractive
technology for implementing computing architectures at the nanoscale. The basic
Boolean primitive in QCA is the majority gate. In this paper we present a novel
design for QCA cells and another possible and unconventional scheme for
majority gates. By applying these items, the hardware requirements for a QCA
design can be reduced and circuits can be simpler in level and gate counts. As
an example, a 1-bit QCA adder is constructed by applying our new scheme and is
compared to the other existing implementation. Beside, some Boolean functions
are expressed as examples and it has been shown, how our reduction method by
using new proposed item, decreases gate counts and levels in comparison to the
other previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2058</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2058</id><created>2012-04-10</created><authors><author><keyname>Puri</keyname><forenames>Shalini</forenames></author><author><keyname>Kaushik</keyname><forenames>Sona</forenames></author></authors><title>A technical study and analysis on fuzzy similarity based models for text
  classification</title><categories>cs.IR cs.LG</categories><comments>15 pages, 3 tables, 1 figure</comments><journal-ref>International Journal of Data Mining &amp; Knowledge Management
  Process (IJDKP) March, 2012, Vol. 2, Number 2,pp. 1-15</journal-ref><doi>10.5121/ijdkp.2012.2201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this new and current era of technology, advancements and techniques,
efficient and effective text document classification is becoming a challenging
and highly required area to capably categorize text documents into mutually
exclusive categories. Fuzzy similarity provides a way to find the similarity of
features among various documents. In this paper, a technical review on various
fuzzy similarity based models is given. These models are discussed and compared
to frame out their use and necessity. A tour of different methodologies is
provided which is based upon fuzzy similarity related concerns. It shows that
how text and web documents are categorized efficiently into different
categories. Various experimental results of these models are also discussed.
The technical comparisons among each model's parameters are shown in the form
of a 3-D chart. Such study and technical review provide a strong base of
research work done on fuzzy similarity based text document categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2061</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2061</id><created>2012-04-10</created><authors><author><keyname>Puri</keyname><forenames>Shalini</forenames></author></authors><title>A Fuzzy Similarity Based Concept Mining Model for Text Classification</title><categories>cs.IR cs.LG</categories><comments>7 Pages, 3 Figures, 2 Tables, International Journal of Advanced
  Computer Science and Applications(IJACSA)</comments><journal-ref>Volume 2, Number 11, pp. 115 - 121, November, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text Classification is a challenging and a red hot field in the current
scenario and has great importance in text categorization applications. A lot of
research work has been done in this field but there is a need to categorize a
collection of text documents into mutually exclusive categories by extracting
the concepts or features using supervised learning paradigm and different
classification algorithms. In this paper, a new Fuzzy Similarity Based Concept
Mining Model (FSCMM) is proposed to classify a set of text documents into pre -
defined Category Groups (CG) by providing them training and preparing on the
sentence, document and integrated corpora levels along with feature reduction,
ambiguity removal on each level to achieve high system performance. Fuzzy
Feature Category Similarity Analyzer (FFCSA) is used to analyze each extracted
feature of Integrated Corpora Feature Vector (ICFV) with the corresponding
categories or classes. This model uses Support Vector Machine Classifier (SVMC)
to classify correctly the training data patterns into two groups; i. e., + 1
and - 1, thereby producing accurate and correct results. The proposed model
works efficiently and effectively with great performance and high - accuracy
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2062</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2062</id><created>2012-04-10</created><authors><author><keyname>Patil</keyname><forenames>Babasaheb G.</forenames></author><author><keyname>Subbaraman</keyname><forenames>Shaila</forenames></author></authors><title>SVD-EBP Algorithm for Iris Pattern Recognition</title><categories>cs.CV</categories><comments>Dec2011-volume2.Issue 12 (IJACSA)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper proposes a neural network approach based on Error Back Propagation
(EBP) for classification of different eye images. To reduce the complexity of
layered neural network the dimensions of input vectors are optimized using
Singular Value Decomposition (SVD). The main of this work is to provide for
best method for feature extraction and classification. The details of this
combined system named as SVD-EBP system, and results thereof are presented in
this paper.
  Keywords- Singular value decomposition(SVD), Error back Propagation(EBP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2069</identifier>
 <datestamp>2014-02-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2069</id><created>2012-04-10</created><updated>2014-02-19</updated><authors><author><keyname>Yamazaki</keyname><forenames>Keisuke</forenames></author></authors><title>Asymptotic Accuracy of Distribution-Based Estimation for Latent
  Variables</title><categories>stat.ML cs.LG</categories><comments>25pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical statistical models are widely employed in information science
and data engineering. The models consist of two types of variables: observable
variables that represent the given data and latent variables for the
unobservable labels. An asymptotic analysis of the models plays an important
role in evaluating the learning process; the result of the analysis is applied
not only to theoretical but also to practical situations, such as optimal model
selection and active learning. There are many studies of generalization errors,
which measure the prediction accuracy of the observable variables. However, the
accuracy of estimating the latent variables has not yet been elucidated. For a
quantitative evaluation of this, the present paper formulates
distribution-based functions for the errors in the estimation of the latent
variables. The asymptotic behavior is analyzed for both the maximum likelihood
and the Bayes methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2073</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2073</id><created>2012-04-10</created><authors><author><keyname>Khandait</keyname><forenames>S. P.</forenames></author><author><keyname>Thool</keyname><forenames>R. C.</forenames></author><author><keyname>Khandait</keyname><forenames>P. D.</forenames></author></authors><title>Automatic facial feature extraction and expression recognition based on
  neural network</title><categories>cs.CV</categories><comments>6 pages,pp. 113-118, (IJACSA) International Journal of Advanced
  Computer Science and Applications, Vol. 2, No.1, January 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an approach to the problem of automatic facial feature
extraction from a still frontal posed image and classification and recognition
of facial expression and hence emotion and mood of a person is presented. Feed
forward back propagation neural network is used as a classifier for classifying
the expressions of supplied face into seven basic categories like surprise,
neutral, sad, disgust, fear, happy and angry. For face portion segmentation and
localization, morphological image processing operations are used. Permanent
facial features like eyebrows, eyes, mouth and nose are extracted using SUSAN
edge detection operator, facial geometry, edge projection analysis. Experiments
are carried out on JAFFE facial expression database and gives better
performance in terms of 100% accuracy for training set and 95.26% accuracy for
test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2079</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2079</id><created>2012-04-10</created><authors><author><keyname>Yanes</keyname><forenames>Nacim</forenames></author><author><keyname>Sassi</keyname><forenames>Sihem Ben</forenames></author><author><keyname>Ghezala</keyname><forenames>Henda Hajjami Ben</forenames></author></authors><title>A Theoretical and Empirical Evaluation of Software Component Search
  Engines, Semantic Search Engines and Google Search Engine in the Context of
  COTS-Based Development</title><categories>cs.IR cs.SE</categories><comments>9 pages, 4 figures, 11 tables</comments><acm-class>D.2.13</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  COTS-based development is a component reuse approach promising to reduce
costs and risks, and ensure higher quality. The growing availability of COTS
components on the Web has concretized the possibility of achieving these
objectives. In this multitude, a recurrent problem is the identification of the
COTS components that best satisfy the user requirements. Finding an adequate
COTS component implies searching among heterogeneous descriptions of the
components within a broad search space. Thus, the use of search engines is
required to make more efficient the COTS components identification. In this
paper, we investigate, theoretically and empirically, the COTS component search
performance of eight software component search engines, nine semantic search
engines and a conventional search engine (Google). Our empirical evaluation is
conducted with respect to precision and normalized recall. We defined ten
queries for the assessed search engines. These queries were carefully selected
to evaluate the capability of each search engine for handling COTS component
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2080</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2080</id><created>2012-04-10</created><authors><author><keyname>Rezk</keyname><forenames>Zouheir</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Ergodic Capacity of Cognitive Radio under Imperfect Channel State
  Information</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE TVT. 12 pages, 8 figures, 1 table. Matlab codes to
  reproduce results are available upon request. Please contact one of the
  authors for this purpose</comments><journal-ref>IEEE Transactions on Vehicular Technology, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spectrum-sharing communication system where the secondary user is aware of
the instantaneous channel state information (CSI) of the secondary link, but
knows only the statistics and an estimated version of the secondary
transmitter-primary receiver (ST-PR) link, is investigated. The optimum power
profile and the ergodic capacity of the secondary link are derived for general
fading channels (with continuous probability density function) under average
and peak transmit-power constraints and with respect to two different
interference constraints: an interference outage constraint and a
signal-to-interference outage constraint. When applied to Rayleigh fading
channels, our results show, for instance, that the interference constraint is
harmful at high-power regime in the sense that the capacity does not increase
with the power, whereas at low-power regime, it has a marginal impact and
no-interference performance corresponding to the ergodic capacity under average
or peak transmit power constraint in absence of the primary user, may be
achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2083</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2083</id><created>2012-04-10</created><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Kurniawan</keyname><forenames>Ernest</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Primary Rate-Splitting Achieves Capacity for the Gaussian Cognitive
  Interference Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cognitive interference channel models cognitive overlay radio systems,
where cognitive radios overhear the transmission of neighboring nodes. Capacity
for this channel is not known in general. For the Gaussian case capacity is
known in three regimes, usually denoted as the &quot;weak interference&quot;, &quot;very
strong interference&quot; and &quot;primary decodes cognitive&quot;. This paper provides a new
capacity result, based on rate-splitting of the primary user's message into a
public and private part and that generalizes the capacity results in the &quot;very
strong interference&quot; and &quot;primary decodes cognitive&quot; regimes. This result
indicates that capacity of the cognitive interference channel not only depends
on channel conditions but also the level of cooperation with the primary user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2087</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2087</id><created>2012-04-10</created><updated>2012-07-14</updated><authors><author><keyname>Bozianu</keyname><forenames>Rodica</forenames></author><author><keyname>Dima</keyname><forenames>C&#x103;t&#x103;lin</forenames></author><author><keyname>Enea</keyname><forenames>Constantin</forenames></author></authors><title>Model-checking an Epistemic \mu-calculus with Synchronous and Perfect
  Recall Semantics</title><categories>cs.LO</categories><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the model-checking problem is decidable for a fragment of the
epistemic \mu-calculus. The fragment allows free variables within the scope of
epistemic modalities in a restricted form that avoids constructing formulas
embodying any form of common knowledge. Our calculus subsumes known decidable
fragments of epistemic CTL/LTL. Its modal variant can express winning
strategies in two-player games with one player having imperfect information and
non-observable objectives, and, with a suitable encoding, decidable instances
of the model-checking problem for ATL with imperfect information and perfect
recall can be encoded as instances of the model-checking problem for this
epistemic \mu-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2097</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2097</id><created>2012-04-10</created><authors><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Jain</keyname><forenames>Prof. R. K.</forenames></author><author><keyname>Agrawal</keyname><forenames>Prof. N. K.</forenames></author></authors><title>An Intelligent Approach for Dense Urban Area in existing 2G / 2.5G</title><categories>cs.CY</categories><comments>6 pages, 7 figures. arXiv admin note: substantial text overlap with
  arXiv:1110.2627</comments><journal-ref>International Journal of Scientific &amp; Engineering Research -
  Volume 2, Issue 12, December -2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the prevailing scenario audio, video, data services (i.e. internet),
multimedia and broadcasting etc. are being integrated. Decreasing cell size
increases capacity but at the same time increases fluctuation and interference
too. The intelligence approach is the only answer in developing countries where
frequency and power are scarce resources. In this paper, we have tried to
integrate all proven technologies in networking such as in-building network,
micro zone, intelligent micro cell, deployment along city streets, tunnels,
subway coverage etc. along with adaptive frequency allocation in hierarchical
approach with the help of adaptive / intelligent antenna system. A-SDMA
approach will further enhance spectral efficiency as well as QoS (Quality of
Service). It can be proved beyond doubt that this integrated approach will
convert 2G / 2.5G systems capable of handling the prevailing demand at reduced
cost. In addition to it, integrated approach will save power and reduce RF
pollution. In this paper we have explained the ill effect of cellular growth in
terms of health hazard and increased power consumption. We have also suggested
ways and means to overcome these problems (spectral density / capacity, QoS,
power consumption and RF pollution etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2101</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2101</id><created>2012-04-10</created><authors><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Agrawal</keyname><forenames>N. K.</forenames></author></authors><title>R.F. Pollution Reduction in Cellular Communication</title><categories>cs.CY cs.NI</categories><comments>6 pages, 7 figures, international journal, International Journal of
  Scientific &amp; Engineering Research, Volume 3, Issue 3, March -2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  R. F. pollution has been recognized as health hazard in India in the
prevailing circumstances. There is lot of hue and cry against cellular towers
installed in residential area. Recently high court in India has issued an order
not to install towers in residential areas. For meeting the exponential demand
of cellular communication in India this will be a set back for future growth.
An appropriate solution has to be developed for meeting demand as well as RF
pollution concern of the society. This paper deals with the installation of low
power base stations in residential areas instead of high power macro cell base
stations. Macro stations are proposed to be used for fast traffic, low power
micro cell for a slow traffic / pedestrian and pico cell / femto cell for
indoor use. These cells will be in hierarchical structure along with adaptive
frequency allocation techniques and A-SDMA approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2114</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2114</id><created>2012-04-10</created><authors><author><keyname>Ng</keyname><forenames>Jun Yee</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author></authors><title>Image-based Vehicle Classification System</title><categories>cs.CV</categories><comments>The 11th Asia-Pacific ITS Forum and Exhibition (ITS-AP 2011),
  Kaoshiung, Taiwan. June 8-11, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic toll collection (ETC) system has been a common trend used for toll
collection on toll road nowadays. The implementation of electronic toll
collection allows vehicles to travel at low or full speed during the toll
payment, which help to avoid the traffic delay at toll road. One of the major
components of an electronic toll collection is the automatic vehicle detection
and classification (AVDC) system which is important to classify the vehicle so
that the toll is charged according to the vehicle classes. Vision-based vehicle
classification system is one type of vehicle classification system which adopt
camera as the input sensing device for the system. This type of system has
advantage over the rest for it is cost efficient as low cost camera is used.
The implementation of vision-based vehicle classification system requires lower
initial investment cost and very suitable for the toll collection trend
migration in Malaysia from single ETC system to full-scale multi-lane free flow
(MLFF). This project includes the development of an image-based vehicle
classification system as an effort to seek for a robust vision-based vehicle
classification system. The techniques used in the system include
scale-invariant feature transform (SIFT) technique, Canny's edge detector,
K-means clustering as well as Euclidean distance matching. In this project, a
unique way to image description as matching medium is proposed. This
distinctiveness of method is analogous to the human DNA concept which is highly
unique. The system is evaluated on open datasets and return promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2124</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2124</id><created>2012-04-10</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Lidick&#xfd;</keyname><forenames>Bernard</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Finding vertex-surjective graph homomorphisms</title><categories>cs.DM cs.CC math.CO</categories><comments>13 pages, 5 figures</comments><msc-class>05C60, 05C85</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Surjective Homomorphism problem is to test whether a given graph G called
the guest graph allows a vertex-surjective homomorphism to some other given
graph H called the host graph. The bijective and injective homomorphism
problems can be formulated in terms of spanning subgraphs and subgraphs, and as
such their computational complexity has been extensively studied. What about
the surjective variant? Because this problem is NP-complete in general, we
restrict the guest and the host graph to belong to graph classes G and H,
respectively. We determine to what extent a certain choice of G and H
influences its computational complexity. We observe that the problem is
polynomial-time solvable if H is the class of paths, whereas it is NP-complete
if G is the class of paths. Moreover, we show that the problem is even
NP-complete on many other elementary graph classes, namely linear forests,
unions of complete graphs, cographs, proper interval graphs, split graphs and
trees of pathwidth at most 2. In contrast, we prove that the problem is
fixed-parameter tractable in k if G is the class of trees and H is the class of
trees with at most k leaves, or if G and H are equal to the class of graphs
with vertex cover number at most k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2131</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2131</id><created>2012-04-10</created><authors><author><keyname>Rink</keyname><forenames>Michael</forenames></author></authors><title>On Thresholds for the Appearance of 2-cores in Mixed Hypergraphs</title><categories>cs.DS</categories><acm-class>G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study thresholds for the appearance of a 2-core in random hypergraphs that
are a mixture of a constant number of random uniform hypergraphs each with a
linear number of edges but with different edge sizes. For the case of two
overlapping hypergraphs we give a solution for the optimal (expected) number of
edges of each size such that the 2-core threshold for the resulting mixed
hypergraph is maximized. We show that for adequate edge sizes this threshold
exceeds the maximum 2-core threshold for any random uniform hypergraph, which
can be used to improve the space utilization of several data structures that
rely on this parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2134</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2134</id><created>2012-04-10</created><authors><author><keyname>Meyer</keyname><forenames>Fernand</forenames></author></authors><title>The steepest watershed: from graphs to images</title><categories>cs.CV</categories><msc-class>68U10, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The watershed is a powerful tool for segmenting objects whose contours appear
as crest lines on a gradient image. The watershed transform associates to a
topographic surface a partition into catchment basins, defined as attraction
zones of a drop of water falling on the relief and following a line of steepest
descent. Unfortunately, catchment basins may overlap and do not form a
partition. Moreover, current watershed algorithms, being shortsighted, do not
correctly estimate the steepness of the downwards trajectories and overestimate
the overlapping zones of catchment basins. An arbitrary division of these zones
between adjacent catchment basin results in a poor localization of the
contours. We propose an algorithm without myopia, which considers the total
length of a trajectory for estimating its steepness. We first consider
topographic surfaces defined on node weighted graphs. The graphs are pruned in
order to eliminate all downwards trajectories which are not the steepest. An
iterative algorithm with simple neighborhood operations performs the pruning
and constructs the catchment basins. The algorithm is then adapted to gray tone
images. The graph structure itself is encoded as an image thanks to the fixed
neighborhood structure of grids. A pair of adaptative erosions and dilations
prune the graph and extend the catchment basins. As a result one obtains a
precise detection of the catchment basins and a graph of the steepest
trajectories. A last iterative algorithm allows to follow selected downwards
trajectories in order to detect particular structures such as rivers or thalweg
lines of the topographic surface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2136</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2136</id><created>2012-04-10</created><updated>2012-08-18</updated><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author></authors><title>The Johnson-Lindenstrauss Transform Itself Preserves Differential
  Privacy</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves that an &quot;old dog&quot;, namely -- the classical
Johnson-Lindenstrauss transform, &quot;performs new tricks&quot; -- it gives a novel way
of preserving differential privacy. We show that if we take two databases, $D$
and $D'$, such that (i) $D'-D$ is a rank-1 matrix of bounded norm and (ii) all
singular values of $D$ and $D'$ are sufficiently large, then multiplying either
$D$ or $D'$ with a vector of iid normal Gaussians yields two statistically
close distributions in the sense of differential privacy. Furthermore, a small,
deterministic and \emph{public} alteration of the input is enough to assert
that all singular values of $D$ are large.
  We apply the Johnson-Lindenstrauss transform to the task of approximating
cut-queries: the number of edges crossing a $(S,\bar S)$-cut in a graph. We
show that the JL transform allows us to \emph{publish a sanitized graph} that
preserves edge differential privacy (where two graphs are neighbors if they
differ on a single edge) while adding only $O(|S|/\epsilon)$ random noise to
any given query (w.h.p). Comparing the additive noise of our algorithm to
existing algorithms for answering cut-queries in a differentially private
manner, we outperform all others on small cuts ($|S| = o(n)$).
  We also apply our technique to the task of estimating the variance of a given
matrix in any given direction. The JL transform allows us to \emph{publish a
sanitized covariance matrix} that preserves differential privacy w.r.t bounded
changes (each row in the matrix can change by at most a norm-1 vector) while
adding random noise of magnitude independent of the size of the matrix (w.h.p).
In contrast, existing algorithms introduce an error which depends on the matrix
dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2138</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2138</id><created>2012-04-10</created><authors><author><keyname>Trivedi</keyname><forenames>Munesh Chandra</forenames></author><author><keyname>Khanum</keyname><forenames>Mohammadi Akheela</forenames></author></authors><title>Role of context in usability evaluations: A review</title><categories>cs.HC</categories><comments>10 pages, 2 tables, ACIJ- Vol3.No.2 March 2012</comments><doi>10.5121/acij.2012.3208</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usability is often defined as the ability of a system to carry out specific
tasks by specific users in a specific context. Usability evaluation involves
testing the system for its expected usability. Usability testing is performed
in natural environment (field) or artificial environment (laboratory). The
result of usability evaluation is affected by the environment in which it is
carried out. Previous studies have focused only on the physical environment
(lab and field) effect on the results but rarely focused on the effect of
social environment (people present during testing). Therefore, this study aims
to review how important it is to take context into account during usability
evaluation. Context is explored through the theory of behaviour settings,
according to which behaviour of individuals is strongly influenced by the
physical as well as the social environment in which they function. The result
of this review indicates that the physical and social context plays a
substantial role in usability evaluations. Further, it also suggests that the
usability evaluation model should encompass context as an important component
in the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2139</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2139</id><created>2012-04-10</created><authors><author><keyname>Bazargani</keyname><forenames>Mosab</forenames></author><author><keyname>Anjos</keyname><forenames>Ant&#xf3;nio dos</forenames></author><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author><author><keyname>Mollahosseini</keyname><forenames>Ali</forenames></author><author><keyname>Shahbazkia</keyname><forenames>Hamid Reza</forenames></author></authors><title>Affine Image Registration Transformation Estimation Using a Real Coded
  Genetic Algorithm with SBX</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the application of a real coded genetic algorithm (GA)
to align two or more 2-D images by means of image registration. The proposed
search strategy is a transformation parameters-based approach involving the
affine transform. The real coded GA uses Simulated Binary Crossover (SBX), a
parent-centric recombination operator that has shown to deliver a good
performance in many optimization problems in the continuous domain. In
addition, we propose a new technique for matching points between a warped and
static images by using a randomized ordering when visiting the points during
the matching procedure. This new technique makes the evaluation of the
objective function somewhat noisy, but GAs and other population-based search
algorithms have been shown to cope well with noisy fitness evaluations. The
results obtained are competitive to those obtained by state-of-the-art
classical methods in image registration, confirming the usefulness of the
proposed noisy objective function and the suitability of SBX as a recombination
operator for this type of problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2150</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2150</id><created>2012-04-10</created><updated>2012-04-23</updated><authors><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author></authors><title>Analog Network Coding in General SNR Regime: Performance of Network
  Simplification</title><categories>cs.IT math.IT</categories><comments>11 pages, 3 figures. Substantially revised content in Section III and
  IV, tighter upper-bounds. arXiv admin note: substantial text overlap with
  arXiv:1203.2297 and arXiv:1202.0372</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a communication scenario where a source communicates with a
destination over a directed layered relay network. Each relay performs analog
network coding where it scales and forwards the signals received at its input.
In this scenario, we address the question: What portion of the maximum
end-to-end achievable rate can be maintained if only a fraction of relay nodes
available at each layer are used?
  We consider, in particular, the Gaussian diamond network (layered network
with a single layer of relay nodes) and a class of symmetric layered networks.
For these networks we show that each relay layer increases the additive gap
between the optimal analog network coding performance with and without network
simplification (using k instead of N relays in each layer, k &lt; N) by no more
than log(N/k)^2 bits and the corresponding multiplicative gap by no more than a
factor of (N/k)^2, asymptotically (in source power). To the best of our
knowledge, this work offers the first characterization of the performance of
network simplification in general layered amplify-and-forward relay networks.
Further, unlike most of the current approximation results that attempt to bound
optimal rates either within an additive gap or a multiplicative gap, our
results suggest a new rate approximation scheme that allows for the
simultaneous computation of additive and multiplicative gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2172</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2172</id><created>2012-04-10</created><updated>2012-10-07</updated><authors><author><keyname>Brummitt</keyname><forenames>Charles D.</forenames></author><author><keyname>Rowland</keyname><forenames>Eric</forenames></author></authors><title>Boundary growth in one-dimensional cellular automata</title><categories>nlin.CG cs.DM math.CO</categories><comments>26 pages, 11 figures</comments><msc-class>68Q80, 68R15, 82C41</msc-class><journal-ref>Complex Systems 21 (2012) 85-116</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We systematically study the boundaries of one-dimensional, 2-color cellular
automata depending on 4 cells, begun from simple initial conditions. We
determine the exact growth rates of the boundaries that appear to be reducible.
Morphic words characterize the reducible boundaries. For boundaries that appear
to be irreducible, we apply curve-fitting techniques to compute an empirical
growth exponent and (in the case of linear growth) a growth rate. We find that
the random walk statistics of irreducible boundaries exhibit surprising
regularities and suggest that a threshold separates two classes. Finally, we
construct a cellular automaton whose growth exponent does not exist, showing
that a strict classification by exponent is not possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2180</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2180</id><created>2012-04-10</created><authors><author><keyname>Axenovich</keyname><forenames>Maria</forenames></author><author><keyname>Person</keyname><forenames>Yury</forenames></author><author><keyname>Puzynina</keyname><forenames>Svetlana</forenames></author></authors><title>A regularity lemma and twins in words</title><categories>math.CO cs.DM q-bio.QM</categories><msc-class>05D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a word $S$, let $f(S)$ be the largest integer $m$ such that there are two
disjoints identical (scattered) subwords of length $m$. Let $f(n, \Sigma) =
\min \{f(S): S \text{is of length} n, \text{over alphabet} \Sigma \}$. Here, it
is shown that \[2f(n, \{0,1\}) = n-o(n)\] using the regularity lemma for words.
  I.e., any binary word of length $n$ can be split into two identical subwords
(referred to as twins) and, perhaps, a remaining subword of length $o(n)$. A
similar result is proven for $k$ identical subwords of a word over an alphabet
with at most $k$ letters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2201</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2201</id><created>2012-04-10</created><authors><author><keyname>Condon</keyname><forenames>Anne</forenames></author><author><keyname>Ma&#x148;uch</keyname><forenames>J&#xe1;n</forenames></author><author><keyname>Thachuk</keyname><forenames>Chris</forenames></author></authors><title>The complexity of string partitioning</title><categories>cs.CC cs.FL</categories><comments>14 pages main text + 13 pages appendix. Full version with proofs of
  an article appearing in the Proceedings of the 23rd Annual Symposium on
  Combinatorial Pattern Matching (CPM 2012), Helsinki, Finland, July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a string $w$ over a finite alphabet $\Sigma$ and an integer $K$, can
$w$ be partitioned into strings of length at most $K$, such that there are no
\emph{collisions}? We refer to this question as the \emph{string partition}
problem and show it is \NP-complete for various definitions of collision and
for a number of interesting restrictions including $|\Sigma|=2$. This
establishes the hardness of an important problem in contemporary synthetic
biology, namely, oligo design for gene synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2202</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2202</id><created>2012-04-10</created><authors><author><keyname>Jiang</keyname><forenames>Minghui</forenames></author></authors><title>Clique in 3-track interval graphs is APX-hard</title><categories>cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Butman, Hermelin, Lewenstein, and Rawitz proved that Clique in t-interval
graphs is NP-hard for t &gt;= 3. We strengthen this result to show that Clique in
3-track interval graphs is APX-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2203</identifier>
 <datestamp>2012-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2203</id><created>2012-04-10</created><updated>2012-08-01</updated><authors><author><keyname>David</keyname><forenames>Istvan</forenames></author></authors><title>A model-driven approach for processing complex events</title><categories>cs.SE cs.PL</categories><acm-class>D.2.13</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By adequate employing of complex event processing (CEP), valuable information
can be extracted from the underlying complex system and used in controlling and
decision situations. An example application area is management of IT systems
for maintaining required dependability attributes of services based on the
infrastructure. In practice, one usually faces the problem of the vast number
of distributed event sources, which makes depicting complex event patterns a
non-trivial task. In this paper, I present a novel, model-driven approach to
define complex event patterns and directly generate event processing
configuration for an open source CEP engine widely used in the industry. One of
the key results of my research work is a textual modeling language called
Complex Event Description Language (CEDL), which will be presented by its
algebraic semantics and some typical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2204</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2204</id><created>2012-04-10</created><updated>2012-06-18</updated><authors><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Benatallah</keyname><forenames>Boualem</forenames></author></authors><title>Programming Cloud Resource Orchestration Framework: Operations and
  Research Challenges</title><categories>cs.DC</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of cloud computing over the past five years is potentially one
of the breakthrough advances in the history of computing. It delivers hardware
and software resources as virtualization-enabled services and in which
administrators are free from the burden of worrying about the low level
implementation or system administration details. Although cloud computing
offers considerable opportunities for the users (e.g. application developers,
governments, new startups, administrators, consultants, scientists, business
analyst, etc.) such as no up-front investment, lowering operating cost, and
infinite scalability, it has many unique research challenges that need to be
carefully addressed in the future. In this paper, we present a survey on key
cloud computing concepts, resource abstractions, and programming operations for
orchestrating resources and associated research challenges, wherever
applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2214</identifier>
 <datestamp>2013-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2214</id><created>2012-04-10</created><updated>2013-07-25</updated><authors><author><keyname>Vasic</keyname><forenames>Bata</forenames></author><author><keyname>Vasic</keyname><forenames>Bane</forenames></author></authors><title>Simplification Resilient LDPC-Coded Sparse-QIM Watermarking for
  3D-Meshes</title><categories>cs.MM cs.CG cs.CR</categories><comments>Submitted, revised and Copyright transfered to IEEE Transactions on
  Multimedia, October 9th 2012</comments><report-no>MM-004273 - 11984709_File000000_225764737</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a blind watermarking scheme for 3-D meshes which combines sparse
quantization index modulation (QIM) with deletion correction codes. The QIM
operates on the vertices in rough concave regions of the surface thus ensuring
impeccability, while the deletion correction code recovers the data hidden in
the vertices which is removed by mesh optimization and/or simplification. The
proposed scheme offers two orders of magnitude better performance in terms of
recovered watermark bit error rate compared to the existing schemes of similar
payloads and fidelity constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2218</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2218</id><created>2012-04-10</created><authors><author><keyname>Melo</keyname><forenames>Nolmar</forenames></author><author><keyname>Santiago</keyname><forenames>Douglas F. G.</forenames></author><author><keyname>Portugal</keyname><forenames>Renato</forenames></author></authors><title>Decoder for Nonbinary CWS Quantum Codes</title><categories>cs.IT math.IT quant-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present a decoder for nonbinary CWS quantum codes using the structure of
union codes. The decoder runs in two steps: first we use a union of stabilizer
codes to detect a sequence of errors, and second we build a new code, called
union code, that allows to correct the errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2225</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2225</id><created>2012-04-10</created><authors><author><keyname>Sandhyarani</keyname><forenames>Ramancha</forenames></author><author><keyname>Rajkumar</keyname><forenames>Bodakuntla</forenames></author><author><keyname>Gyani</keyname><forenames>Jayadev</forenames></author></authors><title>Construction of Community Web Directories based on Web usage Data</title><categories>cs.OH</categories><comments>8 pages,5 figures</comments><journal-ref>Advanced Computing: An International Journal (ACIJ), Vol.3, No.2,
  March 2012</journal-ref><doi>10.5121/acij.2012.3205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper support the concept of a community Web directory, as a Web
directory that is constructed according to the needs and interests of
particular user communities. Furthermore, it presents the complete method for
the construction of such directories by using web usage data. User community
models take the form of thematic hierarchies and are constructed by employing
clustering approach. We applied our methodology to the ODP directory and also
to an artificial Web directory, which was generated by clustering Web pages
that appear in the access log of an Internet Service Provider. For the
discovery of the community models, we introduced a new criterion that combines
a priori thematic informativeness of the Web directory categories with the
level of interest observed in the usage data. In this context, we introduced
and evaluated new clustering method. We have tested the methodology using
access log files which are collected from the proxy servers of an Internet
Service Provider and provided results that indicates the usability of the
community Web directories. The proposed clustering methodology is evaluated
both on a specialized artificial and a community Web directory, indicating its
value to the user of the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2231</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2231</id><created>2012-04-10</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Mercer</keyname><forenames>Robert E.</forenames></author></authors><title>Investigating Keyphrase Indexing with Text Denoising</title><categories>cs.DL cs.IR</categories><comments>The full paper submitted to 12th ACM/ IEEE-CS Joint Conference on
  Digital Libraries (JCDL2012)</comments><acm-class>H.3.1; H.3.3; H.3.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we report on indexing performance by a state-of-the-art
keyphrase indexer, Maui, when paired with a text extraction procedure called
text denoising. Text denoising is a method that extracts the denoised text,
comprising the content-rich sentences, from full texts. The performance of the
keyphrase indexer is demonstrated on three standard corpora collected from
three domains, namely food and agriculture, high energy physics, and biomedical
science. Maui is trained using the full texts and denoised texts. The indexer,
using its trained models, then extracts keyphrases from test sets comprising
full texts, and their denoised and noise parts (i.e., the part of texts that
remains after denoising). Experimental findings show that against a gold
standard, the denoised-text-trained indexer indexing full texts, performs
either better than or as good as its benchmark performance produced by a
full-text-trained indexer indexing full texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2235</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2235</id><created>2012-04-10</created><authors><author><keyname>Vaughan</keyname><forenames>Richard</forenames></author><author><keyname>Wawerla</keyname><forenames>Jens</forenames></author></authors><title>Publishing Identifiable Experiment Code And Configuration Is Important,
  Good and Easy</title><categories>cs.RO cs.AI cs.DL</categories><comments>11 pages</comments><report-no>Simon Fraser University Computing Science Technical Report
  (RV-2012-1)</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We argue for the value of publishing the exact code, configuration and data
processing scripts used to produce empirical work in robotics. In particular,
we recommend publishing a unique identifier for the code package in the paper
itself, as a promise to the reader that this is the relavant code. We review
some recent discussion of best practice for reproducibility in various
professional organisations and journals, and discuss the current reward
structure for publishing code in robotics, along with some ideas for
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2240</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2240</id><created>2012-04-10</created><authors><author><keyname>del R&#xed;o</keyname><forenames>Ana Fern&#xe1;ndez</forenames></author><author><keyname>Korutcheva</keyname><forenames>Elka</forenames></author><author><keyname>de la Rubia</keyname><forenames>Javier</forenames></author></authors><title>Interdependent binary choices under social influence: phase diagram for
  homogeneous unbiased populations</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>17 pages, 3 figures. This is the pre-peer reviewed version of the
  following article: Ana Fern\'andez del R\'io, Elka Korutcheva and Javier de
  la Rubia, Interdependent binary choices under social influence, Wiley's
  Complexity, 2012; which has been published in final form at
  http://onlinelibrary.wiley.com/doi/10.1002/cplx.21397/abstract</comments><doi>10.1002/cplx.21397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coupled Ising models are studied in a discrete choice theory framework, where
they can be understood to represent interdependent choice making processes for
homogeneous populations under social influence. Two different coupling schemes
are considered. The nonlocal or group interdependence model is used to study
two interrelated groups making the same binary choice. The local or individual
interdependence model represents a single group where agents make two binary
choices which depend on each other. For both models, phase diagrams, and their
implications in socioeconomic contexts, are described and compared in the
absence of private deterministic utilities (zero opinion fields).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2242</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2242</id><created>2012-04-10</created><authors><author><keyname>Siddique</keyname><forenames>Isnain</forenames></author><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A.</forenames></author></authors><title>Performance Enhancement of Ad Hoc Networks with Janitor Based Routing</title><categories>cs.NI</categories><journal-ref>1st IEEE International Conference on Computer and Communication
  Engineering (ICCCE2006)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose and analyze a new on the fly strategy that discovers, repairs and
maintains routes in hierarchical and distributed fashion called Janitor Based
Routing (JBR). The main motivation behind our JBR protocol is to decrease
flooding and routing overhead and increase efficiencies in packet movement. An
analytical model for the proposed JBR is presented and detailed simulation is
used to observe the performance of JBR. This route discovery and maintenance
protocol clearly achieved improvement in terms of reduction of flooding,
routing overhead, and, hence, provides enhanced reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2245</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2245</id><created>2012-04-10</created><authors><author><keyname>Shams</keyname><forenames>Rushdi</forenames></author><author><keyname>Elsayed</keyname><forenames>Adel</forenames></author></authors><title>Development of a Conceptual Structure for a Domain-Specific Corpus</title><categories>cs.IR</categories><comments>3rd International Conference on Concept Maps (CMC2008)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The corpus reported in this paper was developed for the evaluation of a
domain-specific Text to Knowledge Mapping (TKM) prototype. The TKM prototype
operates on the basis of both a combinatory categorical grammar (CCG)
linguistic model and a knowledge model that consists of three layers: ontology,
qualitative and quantitative layers. In the course of this evaluation it was
necessary to populate these initial models with lexical items and semantic
relations. Both elements, the lexicon and semantic relations, are meant to
reflect the domain of the prototype; hence both had to be extracted from the
corpus. While dealing with the lexicon was straight forward, the identification
and extraction of appropriate semantic relations was much more involved. It was
necessary, therefore, to manually develop a conceptual structure for the domain
which was then used to formulate a domain-specific framework of semantic
relations. The conceptual structure was developed using the Cmap tool of IHMC.
The framework of semantic relations- that has resulted from this study
consisted of 55 relations, out of which 42 have inverse relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2248</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2248</id><created>2012-04-10</created><authors><author><keyname>Xu</keyname><forenames>Jun-Ming</forenames></author><author><keyname>Bhargava</keyname><forenames>Aniruddha</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaojin</forenames></author></authors><title>Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media</title><categories>cs.AI cs.SI</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world phenomena can be represented by a spatio-temporal signal:
where, when, and how much. Social media is a tantalizing data source for those
who wish to monitor such signals. Unlike most prior work, we assume that the
target phenomenon is known and we are given a method to count its occurrences
in social media. However, counting is plagued by sample bias, incomplete data,
and, paradoxically, data scarcity -- issues inadequately addressed by prior
work. We formulate signal recovery as a Poisson point process estimation
problem. We explicitly incorporate human population bias, time delays and
spatial distortions, and spatio-temporal regularization into the model to
address the noisy count issues. We present an efficient optimization algorithm
and discuss its theoretical properties. We show that our model is more accurate
than commonly-used baselines. Finally, we present a case study on wildlife
roadkill monitoring, where our model produces qualitatively convincing results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2250</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2250</id><created>2012-04-10</created><authors><author><keyname>Khelifi</keyname><forenames>Manel</forenames></author><author><keyname>Djabelkhir</keyname><forenames>Assia</forenames></author></authors><title>LMEEC: Layered Multi-Hop Energy Efficient Cluster-based Routing Protocol
  for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>The 31st Annual IEEE International Conference on Computer
  Communications: INFOCOM'2012 Student Posters, Orlando, USA</comments><journal-ref>IEEE INFOCOM'2012 Student Posters</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose LMEEC, a cluster-based routing protocol with low
energy consumption for wireless sensor networks. Our protocol is based on a
strategy which aims to provide a more reasonable exploitation of the selected
nodes (cluster-heads) energy. Simulation results show the effectiveness of
LMEEC in decreasing the energy consumption, and in prolonging the network
lifetime, compared to LEACH.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2252</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2252</id><created>2012-04-10</created><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mahnoosh</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Coordinated Home Energy Management for Real-Time Power Balancing</title><categories>cs.SY cs.ET</categories><comments>accepted by 2012 IEEE Power Engineering Society (PES) General Meeting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a coordinated home energy management system (HEMS)
architecture where the distributed residential units cooperate with each other
to achieve real-time power balancing. The economic benefits for the retailer
and incentives for the customers to participate in the proposed coordinated
HEMS program are given. We formulate the coordinated HEMS design problem as a
dynamic programming (DP) and use approximate DP approaches to efficiently
handle the design problem. A distributed implementation algorithm based on the
convex optimization based dual decomposition technique is also presented. Our
focus in the current paper is on the deferrable appliances, such as Plug-in
(Hybrid) Electric Vehicles (PHEV), in view of their higher impact on the grid
stability. Simulation results shows that the proposed coordinated HEMS
architecture can efficiently improve the real-time power balancing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2255</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2255</id><created>2012-04-10</created><authors><author><keyname>Solava</keyname><forenames>Ryan W.</forenames></author><author><keyname>Michaels</keyname><forenames>Ryan P.</forenames></author><author><keyname>Milenkovic</keyname><forenames>Tijana</forenames></author></authors><title>Identifying edge clusters in networks via edge graphlet degree vectors
  (edge-GDVs) and edge-GDV-similarities</title><categories>q-bio.MN cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference of new biological knowledge, e.g., prediction of protein function,
from protein-protein interaction (PPI) networks has received attention in the
post-genomic era. A popular strategy has been to cluster the network into
functionally coherent groups of proteins and predict protein function from the
clusters. Traditionally, network research has focused on clustering of nodes.
However, why favor nodes over edges, when clustering of edges may be preferred?
For example, nodes belong to multiple functional groups, but clustering of
nodes typically cannot capture the group overlap, while clustering of edges
can. Clustering of adjacent edges that share many neighbors was proposed
recently, outperforming different node clustering methods. However, since some
biological processes can have characteristic &quot;signatures&quot; throughout the
network, not just locally, it may be of interest to consider edges that are not
necessarily adjacent. Hence, we design a sensitive measure of the &quot;topological
similarity&quot; of edges that can deal with edges that are not necessarily
adjacent. We cluster edges that are similar according to our measure in
different baker's yeast PPI networks, outperforming existing node and edge
clustering approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2260</identifier>
 <datestamp>2012-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2260</id><created>2012-04-08</created><authors><author><keyname>Strano</keyname><forenames>Emanuele</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Jones</keyname><forenames>Jeff</forenames></author></authors><title>Vie Physarale: Evaluation of Roman roads with slime mould</title><categories>cs.CG nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roman Empire is renowned for sharp logical design and outstanding building
quality of its road system. Many roads built by Romans are still use in
continental Europe and UK. The Roman roads were built for military
transportations with efficiency in mind, as straight as possible. Thus the
roads make an ideal test-bed for developing experimental laboratory techniques
for evaluating man-made transport systems using living creatures. We imitate
development of road networks in Iron Age Italy using slime mould Physarum
polycephalum. We represent ten Roman cities with oat flakes, inoculate the
slime mould in Roma, wait till slime mould spans all flakes-cities with its
network of protoplasmic tubes, and analyse structures of the protoplasmic
networks. We found that most Roman roads, apart of those linking Placentia to
Bononia and Genua to Florenzia are represented in development of Physarum
polycephalum. Transport networks developed by Romans and by slime mould show
strong affinity of planar proximity graphs, and particular minimum spanning
tree. Based on laboratory experiments we reconstructed a speculative sequence
of road development in Iron Age Italy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2274</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2274</id><created>2012-04-10</created><authors><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Zepernick</keyname><forenames>Hans-Jurgen</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Beamforming in Two-Way Fixed Gain Amplify-and-Forward Relay Systems with
  CCI</title><categories>cs.IT math.IT</categories><comments>Accepted for presentation on IEEE International Conference on
  Communications (ICC 2012), Ottawa, Canada, June 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the outage performance of a two-way fixed gain amplify-and-forward
(AF) relay system with beamforming, arbitrary antenna correlation, and
co-channel interference (CCI). Assuming CCI at the relay, we derive the exact
individual user outage probability in closed-form. Additionally, while
neglecting CCI, we also investigate the system outage probability of the
considered network, which is declared if any of the two users is in
transmission outage. Our results indicate that in this system, the position of
the relay plays an important role in determining the user as well as the system
outage probability via such parameters as signal-to-noise imbalance, antenna
configuration, spatial correlation, and CCI power. To render further insights
into the effect of antenna correlation and CCI on the diversity and array
gains, an asymptotic expression which tightly converges to exact results is
also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2280</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2280</id><created>2012-04-10</created><authors><author><keyname>B&#xe9;dorf</keyname><forenames>Jeroen</forenames></author><author><keyname>Gaburov</keyname><forenames>Evghenii</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author></authors><title>Bonsai: A GPU Tree-Code</title><categories>astro-ph.IM cs.DC</categories><comments>5 pages, 2 figures. Proceedings of &quot;Advances in Computational
  Astrophysics: methods, tools and outcomes&quot;, June 13-17, 2011, Cefalu, Sicily,
  Italy, eds. Capuzzo Dolcetta, Limongi, Tornambe and Giobbi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a gravitational hierarchical N-body code that is designed to run
efficiently on Graphics Processing Units (GPUs). All parts of the algorithm are
executed on the GPU which eliminates the need for data transfer between the
Central Processing Unit (CPU) and the GPU. Our tests indicate that the
gravitational tree-code outperforms tuned CPU code for all parts of the
algorithm and show an overall performance improvement of more than a factor 20,
resulting in a processing rate of more than 2.8 million particles per second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2294</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2294</id><created>2012-04-10</created><authors><author><keyname>Bejuri</keyname><forenames>Wan Mohd Yaakob Wan</forenames></author><author><keyname>Mohamad</keyname><forenames>Mohd Murtadha</forenames></author><author><keyname>Sapri</keyname><forenames>Maimunah</forenames></author><author><keyname>Rosly</keyname><forenames>Mohd Adly</forenames></author></authors><title>Ubiquitous WLAN/Camera Positioning using Inverse Intensity Chromaticity
  Space-based Feature Detection and Matching: A Preliminary Result</title><categories>cs.CV</categories><comments>International Conference on Man-Machine Systems 2012 (ICOMMS 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper present our new intensity chromaticity space-based feature
detection and matching algorithm. This approach utilizes hybridization of
wireless local area network and camera internal sensor which to receive signal
strength from a access point and the same time retrieve interest point
information from hallways. This information is combined by model fitting
approach in order to find the absolute of user target position. No conventional
searching algorithm is required, thus it is expected reducing the computational
complexity. Finally we present pre-experimental results to illustrate the
performance of the localization system for an indoor environment set-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2306</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2306</id><created>2012-04-10</created><authors><author><keyname>Lu</keyname><forenames>Changhong</forenames></author><author><keyname>Zhou</keyname><forenames>Qing</forenames></author></authors><title>Path covering number and L(2,1)-labeling number of graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\it path covering} of a graph $G$ is a set of vertex disjoint paths of $G$
containing all the vertices of $G$. The {\it path covering number} of $G$,
denoted by $P(G)$, is the minimum number of paths in a path covering of $G$. An
{\sl $k$-L(2,1)-labeling} of a graph $G$ is a mapping $f$ from $V(G)$ to the
set ${0,1,...,k}$ such that $|f(u)-f(v)|\ge 2$ if $d_G(u,v)=1$ and
$|f(u)-f(v)|\ge 1$ if $d_G(u,v)=2$. The {\sl L(2,1)-labeling number $\lambda
(G)$} of $G$ is the smallest number $k$ such that $G$ has a
$k$-L(2,1)-labeling. The purpose of this paper is to study path covering number
and L(2,1)-labeling number of graphs. Our main work extends most of results in
[On island sequences of labelings with a condition at distance two, Discrete
Applied Maths 158 (2010), 1-7] and can answer an open problem in [On the
structure of graphs with non-surjective L(2,1)-labelings, SIAM J. Discrete
Math. 19 (2005), 208-223].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2310</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2310</id><created>2012-04-10</created><authors><author><keyname>Wu</keyname><forenames>Yue</forenames></author><author><keyname>Zhou</keyname><forenames>Yicong</forenames></author><author><keyname>Noonan</keyname><forenames>Joseph P.</forenames></author><author><keyname>Agaian</keyname><forenames>Sos</forenames></author><author><keyname>Chen</keyname><forenames>C. L. Philip</forenames></author></authors><title>A Novel Latin Square Image Cipher</title><categories>cs.CR cs.IT math.IT</categories><comments>26 pages, 17 figures, and 7 tables</comments><journal-ref>Information Sciences 264 (2014): 317-339</journal-ref><doi>10.1016/j.ins.2013.11.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a symmetric-key Latin square image cipher (LSIC)
for grayscale and color images. Our contributions to the image encryption
community include 1) we develop new Latin square image encryption primitives
including Latin Square Whitening, Latin Square S-box and Latin Square P-box ;
2) we provide a new way of integrating probabilistic encryption in image
encryption by embedding random noise in the least significant image bit-plane;
and 3) we construct LSIC with these Latin square image encryption primitives
all on one keyed Latin square in a new loom-like substitution-permutation
network. Consequently, the proposed LSIC achieve many desired properties of a
secure cipher including a large key space, high key sensitivities, uniformly
distributed ciphertext, excellent confusion and diffusion properties,
semantically secure, and robustness against channel noise. Theoretical analysis
show that the LSIC has good resistance to many attack models including
brute-force attacks, ciphertext-only attacks, known-plaintext attacks and
chosen-plaintext attacks. Experimental analysis under extensive simulation
results using the complete USC-SIPI Miscellaneous image dataset demonstrate
that LSIC outperforms or reach state of the art suggested by many peer
algorithms. All these analysis and results demonstrate that the LSIC is very
suitable for digital image encryption. Finally, we open source the LSIC MATLAB
code under webpage https://sites.google.com/site/tuftsyuewu/source-code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2311</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2311</id><created>2012-04-10</created><authors><author><keyname>Shen</keyname><forenames>Bin</forenames></author><author><keyname>Si</keyname><forenames>Luo</forenames></author><author><keyname>Ji</keyname><forenames>Rongrong</forenames></author><author><keyname>Liu</keyname><forenames>Baodi</forenames></author></authors><title>Robust Nonnegative Matrix Factorization via $L_1$ Norm Regularization</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative Matrix Factorization (NMF) is a widely used technique in many
applications such as face recognition, motion segmentation, etc. It
approximates the nonnegative data in an original high dimensional space with a
linear representation in a low dimensional space by using the product of two
nonnegative matrices. In many applications data are often partially corrupted
with large additive noise. When the positions of noise are known, some existing
variants of NMF can be applied by treating these corrupted entries as missing
values. However, the positions are often unknown in many real world
applications, which prevents the usage of traditional NMF or other existing
variants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization
(RobustNMF) algorithm that explicitly models the partial corruption as large
additive noise without requiring the information of positions of noise. In
practice, large additive noise can be used to model outliers. In particular,
the proposed method jointly approximates the clean data matrix with the product
of two nonnegative matrices and estimates the positions and values of
outliers/noise. An efficient iterative optimization algorithm with a solid
theoretical justification has been proposed to learn the desired matrix
factorization. Experimental results demonstrate the advantages of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2320</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2320</id><created>2012-04-10</created><authors><author><keyname>Adnan</keyname><forenames>Muhammad Abdullah</forenames></author><author><keyname>Sugihara</keyname><forenames>Ryo</forenames></author><author><keyname>Gupta</keyname><forenames>Rajesh</forenames></author></authors><title>Energy Efficient Geographical Load Balancing via Dynamic Deferral of
  Workload</title><categories>cs.NI cs.DC</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing popularity of Cloud computing and Mobile computing,
individuals, enterprises and research centers have started outsourcing their IT
and computational needs to on-demand cloud services. Recently geographical load
balancing techniques have been suggested for data centers hosting cloud
computation in order to reduce energy cost by exploiting the electricity price
differences across regions. However, these algorithms do not draw distinction
among diverse requirements for responsiveness across various workloads. In this
paper, we use the flexibility from the Service Level Agreements (SLAs) to
differentiate among workloads under bounded latency requirements and propose a
novel approach for cost savings for geographical load balancing. We investigate
how much workload to be executed in each data center and how much workload to
be delayed and migrated to other data centers for energy saving while meeting
deadlines. We present an offline formulation for geographical load balancing
problem with dynamic deferral and give online algorithms to determine the
assignment of workload to the data centers and the migration of workload
between data centers in order to adapt with dynamic electricity price changes.
We compare our algorithms with the greedy approach and show that significant
cost savings can be achieved by migration of workload and dynamic deferral with
future electricity price prediction. We validate our algorithms on MapReduce
traces and show that geographic load balancing with dynamic deferral can
provide 20-30% cost-savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2321</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2321</id><created>2012-04-10</created><updated>2013-08-30</updated><authors><author><keyname>Ter-Sarkisov</keyname><forenames>Aram</forenames></author><author><keyname>Marsland</keyname><forenames>Stephen</forenames></author></authors><title>Derivation of Upper Bounds on Optimization Time of Population-Based
  Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism
  Levels Traverse Mechanism</title><categories>cs.NE cs.AI</categories><comments>This paper will be replaced by a new version with a different title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article a tool for the analysis of population-based EAs is used to
derive asymptotic upper bounds on the optimization time of the algorithm
solving Royal Roads problem, a test function with plateaus of fitness. In
addition to this, limiting distribution of a certain subset of the population
is approximated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2331</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2331</id><created>2012-04-10</created><authors><author><keyname>Zhao</keyname><forenames>Lei</forenames></author><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Compression with Actions</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the setting where actions can be used to modify a state sequence
before compression. The minimum rate needed to losslessly describe the optimal
modified sequence is characterized when the state sequence is either
non-causally or causally available at the action encoder. The achievability is
closely related to the optimal channel coding strategy for channel with states.
We also extend the analysis to the the lossy case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2335</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2335</id><created>2012-04-11</created><authors><author><keyname>Baydin</keyname><forenames>Atilim Gunes</forenames></author><author><keyname>de Mantaras</keyname><forenames>Ramon Lopez</forenames></author><author><keyname>Ontanon</keyname><forenames>Santiago</forenames></author></authors><title>Automated Generation of Cross-Domain Analogies via Evolutionary
  Computation</title><categories>cs.NE nlin.AO</categories><comments>Conference submission, International Conference on Computational
  Creativity 2012 (8 pages, 6 figures)</comments><msc-class>92D15, 91E40, 68T20, 68T30</msc-class><acm-class>I.2.4; I.2.6; G.1.6; J.4; J.3</acm-class><journal-ref>In Proceedings of the Third International Conference on
  Computational Creativity, Dublin, Ireland, May 30-June 1, 2012. Dublin:
  University College Dublin, 2012, pp. 25-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analogy plays an important role in creativity, and is extensively used in
science as well as art. In this paper we introduce a technique for the
automated generation of cross-domain analogies based on a novel evolutionary
algorithm (EA). Unlike existing work in computational analogy-making restricted
to creating analogies between two given cases, our approach, for a given case,
is capable of creating an analogy along with the novel analogous case itself.
Our algorithm is based on the concept of &quot;memes&quot;, which are units of culture,
or knowledge, undergoing variation and selection under a fitness measure, and
represents evolving pieces of knowledge as semantic networks. Using a fitness
function based on Gentner's structure mapping theory of analogies, we
demonstrate the feasibility of spontaneously generating semantic networks that
are analogous to a given base network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2336</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2336</id><created>2012-04-11</created><authors><author><keyname>Chary</keyname><forenames>R. Venkata Ramana</forenames></author><author><keyname>Lakshmi</keyname><forenames>D. Rajya</forenames></author><author><keyname>Sunitha</keyname><forenames>K. V. N.</forenames></author></authors><title>Feature Extraction Methods for Color Image Similarity</title><categories>cs.CV</categories><comments>11 pages, Advanced Computing: An International Journal (ACIJ), Vol.3,
  No.2, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many User interactive systems are proposed all methods are trying to
implement as a user friendly and various approaches proposed but most of the
systems not reached to the use specifications like user friendly systems with
user interest, all proposed method implemented basic techniques some are
improved methods also propose but not reaching to the user specifications. In
this proposed paper we concentrated on image retrieval system with in early
days many user interactive systems performed with basic concepts but such
systems are not reaching to the user specifications and not attracted to the
user so a lot of research interest in recent years with new specifications,
recent approaches have user is interested in friendly interacted methods are
expecting, many are concentrated for improvement in all methods. In this
proposed system we focus on the retrieval of images within a large image
collection based on color projections and different mathematical approaches are
introduced and applied for retrieval of images. before Appling proposed methods
images are sub grouping using threshold values, in this paper R G B color
combinations considered for retrieval of images, in proposed methods are
implemented and results are included, through results it is observed that we
obtaining efficient results comparatively previous and existing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2342</identifier>
 <datestamp>2014-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2342</id><created>2012-04-11</created><updated>2014-07-31</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Morisset</keyname><forenames>Charles</forenames></author></authors><title>Towards A Generic Formal Framework for Access Control Systems</title><categories>cs.CR cs.LO</categories><comments>Short version (without proofs) published at STM 2014</comments><acm-class>D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been many proposals for access control models and authorization
policy languages, which are used to inform the design of access control
systems. Most, if not all, of these proposals impose restrictions on the
implementation of access control systems, thereby limiting the type of
authorization requests that can be processed or the structure of the
authorization policies that can be specified. In this paper, we develop a
formal characterization of the features of an access control model that imposes
few restrictions of this nature. Our characterization is intended to be a
generic framework for access control, from which we may derive access control
models and reason about the properties of those models. In this paper, we
consider the properties of monotonicity and completeness, the first being
particularly important for attribute-based access control systems. XACML, an
XML-based language and architecture for attribute-based access control, is
neither monotonic nor complete. Using our framework, we define attribute-based
access control models, in the style of XACML, that are, respectively, monotonic
and complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2348</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2348</id><created>2012-04-11</created><authors><author><keyname>Liew</keyname><forenames>Sing</forenames></author></authors><title>Introducing convex layers to the Traveling Salesman Problem</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will propose convex layers to the Traveling Salesman
Problem (TSP). Firstly, we will focus on human performance on the TSP.
Experimental data shows that untrained humans appear to have the ability to
perform well in the TSP. On the other hand, experimental data also supports the
hypothesis of convex hull i.e. human relies on convex hull to search for the
optimal tour for the TSP. Secondly, from the paper published by Bonabeau,
Dorigo and Theraulaz, social insect behavior would be able to help in some of
the optimizing problems, especially the TSP. Thus, we propose convex layers to
the TSP based on the argument that, by the analogy to the social insect
behavior, untrained humans' cognition should be able to help in the TSP.
Lastly, we will use Tour Improvement algorithms on convex layers to search for
an optimal tour for a 13-cities problem to demonstrate the idea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2350</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2350</id><created>2012-04-11</created><authors><author><keyname>Liew</keyname><forenames>Sing</forenames></author></authors><title>Applying convex layers, nearest neighbor and triangle inequality to the
  Traveling Salesman Problem (TSP)</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The author would like to propose a simple but yet effective method, convex
layers, nearest neighbor and triangle inequality, to approach the Traveling
Salesman Problem (TSP). No computer is needed in this method. This method is
designed for plain folks who faced the TSP everyday but do not have the
sophisticated knowledge of computer science, programming language or applied
mathematics. The author also hopes that it would give some insights to
researchers who are interested in the TSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2352</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2352</id><created>2012-04-11</created><authors><author><keyname>Liew</keyname><forenames>Sing</forenames></author></authors><title>Optimal tree for Genetic Algorithms in the Traveling Salesman Problem
  (TSP)</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the author proposes optimal tree as a &quot;gauge&quot; for the
generation of the initial population at random in the Genetic Algorithms (GA)
to benchmark against the good and the bad parent tours. Thus, without having
the so-called bad parent tours in the initiate population, it will speed up the
GA. The characteristics of the gauge (algorithm, complexity time, trade-off,
etc.) will be discussed in this paper as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2356</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2356</id><created>2012-04-11</created><authors><author><keyname>Loshchilov</keyname><forenames>Ilya</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, MSR - INRIA</affiliation></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author></authors><title>Self-Adaptive Surrogate-Assisted Covariance Matrix Adaptation Evolution
  Strategy</title><categories>cs.NE</categories><comments>Genetic and Evolutionary Computation Conference (GECCO 2012) (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel mechanism to adapt surrogate-assisted
population-based algorithms. This mechanism is applied to ACM-ES, a recently
proposed surrogate-assisted variant of CMA-ES. The resulting algorithm,
saACM-ES, adjusts online the lifelength of the current surrogate model (the
number of CMA-ES generations before learning a new surrogate) and the surrogate
hyper-parameters. Both heuristics significantly improve the quality of the
surrogate model, yielding a significant speed-up of saACM-ES compared to the
ACM-ES and CMA-ES baselines. The empirical validation of saACM-ES on the
BBOB-2012 noiseless testbed demonstrates the efficiency and the scalability
w.r.t the problem dimension and the population size of the proposed approach,
that reaches new best results on some of the benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2358</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2358</id><created>2012-04-11</created><updated>2014-03-10</updated><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Yang</keyname><forenames>Meng</forenames></author><author><keyname>Feng</keyname><forenames>Xiangchu</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>David</forenames></author></authors><title>Collaborative Representation based Classification for Face Recognition</title><categories>cs.CV</categories><comments>It is a substantial revision of a previous conference paper (L.
  Zhang, M. Yang, et al. &quot;Sparse Representation or Collaborative
  Representation: Which Helps Face Recognition?&quot; in ICCV 2011)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  By coding a query sample as a sparse linear combination of all training
samples and then classifying it by evaluating which class leads to the minimal
coding residual, sparse representation based classification (SRC) leads to
interesting results for robust face recognition. It is widely believed that the
l1- norm sparsity constraint on coding coefficients plays a key role in the
success of SRC, while its use of all training samples to collaboratively
represent the query sample is rather ignored. In this paper we discuss how SRC
works, and show that the collaborative representation mechanism used in SRC is
much more crucial to its success of face classification. The SRC is a special
case of collaborative representation based classification (CRC), which has
various instantiations by applying different norms to the coding residual and
coding coefficient. More specifically, the l1 or l2 norm characterization of
coding residual is related to the robustness of CRC to outlier facial pixels,
while the l1 or l2 norm characterization of coding coefficient is related to
the degree of discrimination of facial features. Extensive experiments were
conducted to verify the face recognition accuracy and efficiency of CRC with
different instantiations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2359</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2359</id><created>2012-04-11</created><authors><author><keyname>Al-Sabateen</keyname><forenames>Jaafer</forenames></author><author><keyname>Alomari</keyname><forenames>Saleh Ali</forenames></author><author><keyname>Sumari</keyname><forenames>Putra</forenames></author></authors><title>An Overview of Video Allocation Algorithms for Flash-based SSD Storage
  Systems</title><categories>cs.MM</categories><comments>6 pages, 2 figure, 3 algorithm, 3 table, (IJACSA) International
  Journal of Advanced Computer Science and Applications, Vol. 3, No. 3, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the fact that Solid State Disk (SSD) data storage media had offered a
revolutionary property storages community, but the unavailability of a
comprehensive allocation strategy in SSDs storage media, leads to consuming the
available space, random writing processes, time-consuming reading processes,
and system resources consumption. In order to overcome these challenges, an
efficient allocation algorithm is a desirable option. In this paper, we had
executed an intensive investigation on the SSD-based allocation algorithms that
had been proposed by the knowledge community. An explanatory comparison had
been made between these algorithms. We reviewed these algorithms in order to
building advanced knowledge armature that would help in inventing new
allocation algorithms for this type of storage media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2376</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2376</id><created>2012-04-11</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Evolutionarily Stable Spectrum Access</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1103.1026</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we design distributed spectrum access mechanisms with both
complete and incomplete network information. We propose an evolutionary
spectrum access mechanism with complete network information, and show that the
mechanism achieves an equilibrium that is globally evolutionarily stable. With
incomplete network information, we propose a distributed learning mechanism,
where each user utilizes local observations to estimate the expected throughput
and learns to adjust its spectrum access strategy adaptively over time. We show
that the learning mechanism converges to the same evolutionary equilibrium on
the time average. Numerical results show that the proposed mechanisms are
robust to the perturbations of users' channel selections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2385</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2385</id><created>2012-04-11</created><authors><author><keyname>Hatanaka</keyname><forenames>Takeshi</forenames></author><author><keyname>Nishi</keyname><forenames>Takayuki</forenames></author><author><keyname>Fujita</keyname><forenames>Masayuki</forenames></author></authors><title>Vision-Based Cooperative Estimation of Averaged 3D Target Pose under
  Imperfect Visibility</title><categories>cs.SY</categories><comments>9 pages, 9 figures, submitted to IFAC NecSys '12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates vision-based cooperative estimation of a 3D target
object pose for visual sensor networks. In our previous works, we presented an
estimation mechanism called networked visual motion observer achieving
averaging of local pose estimates in real time. This paper extends the
mechanism so that it works even in the presence of cameras not viewing the
target due to the limited view angles and obstructions in order to fully take
advantage of the networked vision system. Then, we analyze the averaging
performance attained by the proposed mechanism and clarify a relation between
the feedback gains in the algorithm and the performance. Finally, we
demonstrate the effectiveness of the algorithm through simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2386</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2386</id><created>2012-04-11</created><updated>2012-04-26</updated><authors><author><keyname>Bruttomesso</keyname><forenames>Roberto</forenames><affiliation>Universit&#xe0; degli Studi di Milano, Department of Computer Science</affiliation></author><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames><affiliation>Universit&#xe0; degli Studi di Milano, Department of Computer Science</affiliation></author><author><keyname>Ranise</keyname><forenames>Silvio</forenames><affiliation>Fondazione Bruno Kessler, Trento</affiliation></author></authors><title>Quantifier-Free Interpolation of a Theory of Arrays</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1, F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (April 27,
  2012) lmcs:934</journal-ref><doi>10.2168/LMCS-8(2:4)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of interpolants in model checking is becoming an enabling technology
to allow fast and robust verification of hardware and software. The application
of encodings based on the theory of arrays, however, is limited by the
impossibility of deriving quantifier- free interpolants in general. In this
paper, we show that it is possible to obtain quantifier-free interpolants for a
Skolemized version of the extensional theory of arrays. We prove this in two
ways: (1) non-constructively, by using the model theoretic notion of
amalgamation, which is known to be equivalent to admit quantifier-free
interpolation for universal theories; and (2) constructively, by designing an
interpolating procedure, based on solving equations between array updates.
(Interestingly, rewriting techniques are used in the key steps of the solver
and its proof of correctness.) To the best of our knowledge, this is the first
successful attempt of computing quantifier- free interpolants for a variant of
the theory of arrays with extensionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2401</identifier>
 <datestamp>2012-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2401</id><created>2012-04-11</created><updated>2012-04-12</updated><authors><author><keyname>Yan</keyname><forenames>Gang</forenames></author><author><keyname>Ren</keyname><forenames>Jie</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author><author><keyname>Lai</keyname><forenames>Choy-Heng</forenames></author><author><keyname>Li</keyname><forenames>Baowen</forenames></author></authors><title>Controlling complex networks: How much energy is needed?</title><categories>physics.soc-ph cs.SI cs.SY</categories><comments>4 pages paper + 5 pages supplement. accepted for publication in
  Physical Review Letters;
  http://link.aps.org/doi/10.1103/PhysRevLett.108.218703</comments><journal-ref>Phys. Rev. Lett. 108, 218703 (2012)</journal-ref><doi>10.1103/PhysRevLett.108.218703</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The outstanding problem of controlling complex networks is relevant to many
areas of science and engineering, and has the potential to generate
technological breakthroughs as well. We address the physically important issue
of the energy required for achieving control by deriving and validating scaling
laws for the lower and upper energy bounds. These bounds represent a reasonable
estimate of the energy cost associated with control, and provide a step forward
from the current research on controllability toward ultimate control of complex
networked dynamical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2404</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2404</id><created>2012-04-11</created><authors><author><keyname>Elyassami</keyname><forenames>Sanaa</forenames></author><author><keyname>Idri</keyname><forenames>Ali</forenames></author></authors><title>Investigating Effort Prediction of Software Projects on the ISBSG
  Dataset</title><categories>cs.SE</categories><comments>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.3, No.2, March 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many cost estimation models have been proposed over the last three decades.
In this study, we investigate fuzzy ID3 decision tree as a method for software
effort estimation. Fuzzy ID software effort estimation model is designed by
incorporating the principles of ID3 decision tree and the concepts of the fuzzy
settheoretic; permitting the model to handle uncertain and imprecise data when
presenting the software projects. MMRE (Mean Magnitude of Relative Error) and
Pred(l) (Prediction at level l) are used, as measures of prediction accuracy,
for this study. A series of experiments is reported using ISBSG software
projects dataset. Fuzzy trees are grown using different fuzziness control
thresholds. Results showed that optimizing the fuzzy ID3 parameters can improve
greatly the accuracy of the generated software cost estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2413</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2413</id><created>2012-04-11</created><authors><author><keyname>Tiu</keyname><forenames>Alwen</forenames></author><author><keyname>Ianovski</keyname><forenames>Egor</forenames></author><author><keyname>Gore</keyname><forenames>Rajeev</forenames></author></authors><title>Grammar Logics in Nested Sequent Calculus: Proof Theory and Decision
  Procedures</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A grammar logic refers to an extension to the multi-modal logic K in which
the modal axioms are generated from a formal grammar. We consider a proof
theory, in nested sequent calculus, of grammar logics with converse, i.e.,
every modal operator [a] comes with a converse. Extending previous works on
nested sequent systems for tense logics, we show all grammar logics (with or
without converse) can be formalised in nested sequent calculi, where the axioms
are internalised in the calculi as structural rules. Syntactic cut-elimination
for these calculi is proved using a procedure similar to that for display
logics. If the grammar is context-free, then one can get rid of all structural
rules, in favor of deep inference and additional propagation rules. We give a
novel semi-decision procedure for context-free grammar logics, using nested
sequent calculus with deep inference, and show that, in the case where the
given context-free grammar is regular, this procedure terminates. Unlike all
other existing decision procedures for regular grammar logics in the
literature, our procedure does not assume that a finite state automaton
encoding the axioms is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2420</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2420</id><created>2012-04-11</created><authors><author><keyname>Hernando</keyname><forenames>A.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Variational Principle underlying Scale Invariant Social Systems</title><categories>stat.AP cs.SI physics.soc-ph</categories><doi>10.1140/epjb/e2012-30313-x</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  MaxEnt's variational principle, in conjunction with Shannon's logarithmic
information measure, yields only exponential functional forms in
straightforward fashion. In this communication we show how to overcome this
limitation via the incorporation, into the variational process, of suitable
dynamical information. As a consequence, we are able to formulate a somewhat
generalized Shannonian Maximum Entropy approach which provides a unifying
&quot;thermodynamic-like&quot; explanation for the scale-invariant phenomena observed in
social contexts, as city-population distributions. We confirm the MaxEnt
predictions by means of numerical experiments with random walkers, and compare
them with some empirical data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2422</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2422</id><created>2012-04-11</created><authors><author><keyname>Hernando</keyname><forenames>A.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Scale-invariance underlying the logistic equation and its social
  applications</title><categories>stat.AP cs.SI physics.soc-ph</categories><doi>10.1016/j.physleta.2012.10.054</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  On the basis of dynamical principles we derive the Logistic Equation (LE),
widely employed (among multiple applications) in the simulation of population
growth, and demonstrate that scale-invariance and a mean-value constraint are
sufficient and necessary conditions for obtaining it. We also generalize the LE
to multi-component systems and show that the above dynamical mechanisms
underlie large number of scale-free processes. Examples are presented regarding
city-populations, diffusion in complex networks, and popularity of
technological products, all of them obeying the multi-component logistic
equation in an either stochastic or deterministic way. So as to assess the
predictability-power of our present formalism, we advance a prediction,
regarding the next 60 months, for the number of users of the three main web
browsers (Explorer, Firefox and Chrome) popularly referred as &quot;Browser Wars&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2428</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2428</id><created>2012-04-11</created><authors><author><keyname>Tang</keyname><forenames>Liang</forenames></author><author><keyname>Chen</keyname><forenames>Yunfei</forenames></author><author><keyname>Hines</keyname><forenames>Evor L.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Performance Analysis of Spectrum Sensing With Multiple Status Changes in
  Primary User Traffic</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, 4 figures, accepted by IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the impact of primary user traffic with multiple status
changes on the spectrum sensing performance is analyzed. Closed-form
expressions for the probabilities of false alarm and detection are derived.
Numerical results show that the multiple status changes of the primary user
cause considerable degradation in the sensing performance. This degradation
depends on the number of changes, the primary user traffic model, the primary
user traffic intensity and the signal-to-noise ratio of the received signal.
Numerical results also show that the amount of degradation decreases when the
number of changes increases, and converges to a minimum sensing performance due
to the limited sensing period and primary holding time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2433</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2433</id><created>2012-04-11</created><authors><author><keyname>Bhatnagar</keyname><forenames>Manav R.</forenames></author></authors><title>Decode-and-Forward Based Differential Modulation for Cooperative
  Communication System with Unitary and Non-Unitary Constellations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive a maximum likelihood (ML) decoder of the
differential data in a decode-and-forward (DF) based cooperative communication
system utilizing uncoded transmissions. This decoder is applicable to
complex-valued unitary and non-unitary constellations suitable for differential
modulation. The ML decoder helps in improving the diversity of the DF based
differential cooperative system using an erroneous relaying node. We also
derive a piecewise linear (PL) decoder of the differential data transmitted in
the DF based cooperative system. The proposed PL decoder significantly reduces
the decoding complexity as compared to the proposed ML decoder without any
significant degradation in the receiver performance. Existing ML and PL
decoders of the differentially modulated uncoded data in the DF based
cooperative communication system are only applicable to binary modulated
signals like binary phase shift keying (BPSK) and binary frequency shift keying
(BFSK), whereas, the proposed decoders are applicable to complex-valued unitary
and non-unitary constellations suitable for differential modulation under
uncoded transmissions. We derive a closedform expression of the uncoded average
symbol error rate (SER) of the proposed PL decoder with M-PSK constellation in
a cooperative communication system with a single relay and one
source-destination pair. An approximate average SER by ignoring higher order
noise terms is also derived for this set-up. It is analytically shown on the
basis of the derived approximate SER that the proposed PL decoder provides full
diversity of second order. In addition, we also derive approximate SER of the
differential DF system with multiple relays at asymptotically high
signal-to-noise ratio of the source-relay links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2435</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2435</id><created>2012-04-11</created><updated>2013-05-13</updated><authors><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Chiani</keyname><forenames>Marco</forenames></author><author><keyname>Fossorier</keyname><forenames>Marc P. C.</forenames></author></authors><title>Spectral Shape of Doubly-Generalized LDPC Codes: Efficient and Exact
  Evaluation</title><categories>cs.IT math.IT</categories><comments>17 pages, 6 figures. To appear in IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the asymptotic exponent of the weight spectrum for
irregular doubly-generalized LDPC (D-GLDPC) codes. In the process, an efficient
numerical technique for its evaluation is presented, involving the solution of
a 4 x 4 system of polynomial equations. The expression is consistent with
previous results, including the case where the normalized weight or stopping
set size tends to zero. The spectral shape is shown to admit a particularly
simple form in the special case where all variable nodes are repetition codes
of the same degree, a case which includes Tanner codes; for this case it is
also shown how certain symmetry properties of the local weight distribution at
the CNs induce a symmetry in the overall weight spectral shape function.
Finally, using these new results, weight and stopping set size spectral shapes
are evaluated for some example generalized and doubly-generalized LDPC code
ensembles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2447</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2447</id><created>2012-04-11</created><updated>2014-07-12</updated><authors><author><keyname>Farkas</keyname><forenames>L&#xf3;r&#xe1;nt</forenames></author><author><keyname>K&#xf3;i</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>On Capacity Regions of Discrete Asynchronous Multiple Access Channels</title><categories>cs.IT math.IT</categories><comments>It has been presented in part at ISIT 2011, Saint Petersburg. This
  extended version is accepted for publication in Kybernetika</comments><doi>10.14736/kyb-2014-6-1003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general formalization is given for asynchronous multiple access channels
which admits different assumptions on delays. This general framework allows the
analysis of so far unexplored models leading to new interesting capacity
regions. In particular, a single letter characterization is given for the
capacity region in case of 3 senders, 2 synchronous with each other and the
third not synchronous with them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2465</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2465</id><created>2012-04-11</created><authors><author><keyname>Barreto</keyname><forenames>Fernando</forenames></author><author><keyname>Wille</keyname><forenames>Emilio C. G.</forenames></author><author><keyname>Nacamura</keyname><forenames>Luiz</forenames><suffix>Jr</suffix></author></authors><title>Fast emergency paths schema to overcome transient link failures in ospf
  routing</title><categories>cs.NI</categories><comments>18 pages</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.4, No.2, March 2012 17-34</journal-ref><doi>10.5121/ijcnc.2012.4202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reliable network infrastructure must be able to sustain traffic flows, even
when a failure occurs and changes the network topology. During the occurrence
of a failure, routing protocols, like OSPF, take from hundreds of milliseconds
to various seconds in order to converge. During this convergence period,
packets might traverse a longer path or even a loop. An even worse transient
behaviour is that packets are dropped even though destinations are reachable.
In this context, this paper describes a proactive fast rerouting approach,
named Fast Emergency Paths Schema (FEP-S), to overcome problems originating
from transient link failures in OSPF routing. Extensive experiments were done
using several network topologies with different dimensionality degrees. Results
show that the recovery paths, obtained by FEPS, are shorter than those from
other rerouting approaches and can improve the network reliability by reducing
the packet loss rate during the routing protocols convergence caused by a
failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2477</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2477</id><created>2012-04-11</created><authors><author><keyname>Johnson</keyname><forenames>Matthew James</forenames></author></authors><title>A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov
  Models</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple linear algebraic explanation of the algorithm in &quot;A Spectral
Algorithm for Learning Hidden Markov Models&quot; (COLT 2009). Most of the content
is in Figure 2; the text just makes everything precise in four nearly-trivial
claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2485</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2485</id><created>2012-04-11</created><updated>2012-06-20</updated><authors><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author></authors><title>Evolution of optimal L\'evy-flight strategies in human mental searches</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>8 pages, 4 figures</comments><journal-ref>Phys. Rev. E 85, 061121 (2012)</journal-ref><doi>10.1103/PhysRevE.85.061121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent analysis of empirical data [F. Radicchi, A. Baronchelli &amp; L.A.N.
Amaral. PloS ONE 7, e029910 (2012)] showed that humans adopt L\'evy flight
strategies when exploring the bid space in on-line auctions. A game theoretical
model proved that the observed L\'evy exponents are nearly optimal, being close
to the exponent value that guarantees the maximal economical return to players.
Here, we rationalize these findings by adopting an evolutionary perspective. We
show that a simple evolutionary process is able to account for the empirical
measurements with the only assumption that the reproductive fitness of a player
is proportional to her search ability. Contrarily to previous modeling, our
approach describes the emergence of the observed exponent without resorting to
any strong assumptions on the initial searching strategies. Our results
generalize earlier research, and open novel questions in cognitive, behavioral
and evolutionary sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2495</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2495</id><created>2012-04-11</created><updated>2013-10-18</updated><authors><author><keyname>Figueira</keyname><forenames>Diego</forenames></author></authors><title>Satisfiability for two-variable logic with two successor relations on
  finite linear orders</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the finitary satisfiability problem for first order logic with two
variables and two binary relations, corresponding to the induced successor
relations of two finite linear orders. We show that the problem is decidable in
NEXPTIME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2516</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2516</id><created>2012-04-11</created><authors><author><keyname>Sadr</keyname><forenames>Ali</forenames></author><author><keyname>Zolfaghari-Nejad</keyname><forenames>Mostafa</forenames></author></authors><title>Physical Unclonable Function (PUF) Based Random Number Generator</title><categories>cs.CR</categories><comments>7 pages, 7 figures</comments><journal-ref>Advanced Computing: An International Journal (ACIJ), Vol.3, No.2,
  March 2012, 139-145</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical Unclonable Functions (PUFs) are widely used to generate random
Numbers. In this paper we propose a new architecture in which an Arbiter Based
PUF has been employed as a nonlinear function in Nonlinear Feedback Shift
Register (NFSR) to generate true random numbers. The rate of producing the
output bit streams is 10 million bits per second. The proposed RNG is able to
pass all NIST tests and the entropy of the output stream is 7.999837 bits per
byte. The proposed circuit has very low resource usage of 193 Slices that makes
it suitable for lightweight applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2518</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2518</id><created>2012-04-11</created><updated>2013-01-31</updated><authors><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author></authors><title>Distributed Function Computation with Confidentiality</title><categories>cs.IT cs.CR math.IT</categories><comments>To Appear in IEEE JSAC: In-Network Computation: Exploring the
  Fundamental Limits, April 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of terminals observe correlated data and seek to compute functions of
the data using interactive public communication. At the same time, it is
required that the value of a private function of the data remains concealed
from an eavesdropper observing this communication. In general, the private
function and the functions computed by the nodes can be all different. We show
that a class of functions are securely computable if and only if the
conditional entropy of data given the value of private function is greater than
the least rate of interactive communication required for a related
multiterminal source-coding task. A single-letter formula is provided for this
rate in special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2519</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2519</id><created>2012-04-11</created><updated>2013-01-03</updated><authors><author><keyname>Kr&#xe1;l'</keyname><forenames>Daniel</forenames><affiliation>DIMAP</affiliation></author><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames><affiliation>LORIA</affiliation></author><author><keyname>Sereni</keyname><forenames>Jean-S&#xe9;bastien</forenames><affiliation>LORIA</affiliation></author><author><keyname>Whalen</keyname><forenames>Peter</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Yilma</keyname><forenames>Zelealem</forenames><affiliation>LIAFA</affiliation></author></authors><title>A new bound for the 2/3 conjecture</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that any n-vertex complete graph with edges colored with three colors
contains a set of at most four vertices such that the number of the neighbors
of these vertices in one of the colors is at least 2n/3. The previous best
value, proved by Erdos, Faudree, Gould, Gy\'arf\'as, Rousseau and Schelp in
1989, is 22. It is conjectured that three vertices suffice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2523</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2523</id><created>2012-04-11</created><authors><author><keyname>El-Arini</keyname><forenames>Khalid</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author><author><keyname>Guestrin</keyname><forenames>Carlos</forenames></author></authors><title>Concept Modeling with Superwords</title><categories>stat.ML cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In information retrieval, a fundamental goal is to transform a document into
concepts that are representative of its content. The term &quot;representative&quot; is
in itself challenging to define, and various tasks require different
granularities of concepts. In this paper, we aim to model concepts that are
sparse over the vocabulary, and that flexibly adapt their content based on
other relevant semantic information such as textual structure or associated
image features. We explore a Bayesian nonparametric model based on nested beta
processes that allows for inferring an unknown number of strictly sparse
concepts. The resulting model provides an inherently different representation
of concepts than a standard LDA (or HDP) based topic model, and allows for
direct incorporation of semantic features. We demonstrate the utility of this
representation on multilingual blog data and the Congressional Record.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2536</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2536</id><created>2012-04-11</created><authors><author><keyname>H&#xf6;fling</keyname><forenames>Benedikt</forenames></author><author><keyname>Reiser</keyname><forenames>Hans P.</forenames></author></authors><title>SecureSMART: A Security Architecture for BFT Replication Libraries</title><categories>cs.CR cs.DC</categories><comments>2 pages, EDCC 2012 fast abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several research projects have shown that Byzantine fault tolerance (BFT) is
practical today in terms of performance. Deficiencies in other aspects might
still be an obstacle to a more wide-spread deployment in real-world
applications. One of these aspects is an over-all security architecture beyond
the low-level protocol. This paper proposes the security architecture
SecureSMART, which provides dynamic key distribution, internal and external
integrity and confidentiality measures, as well as mechanisms for availability
and access control. For this purpose, it implements security mechanism among
clients, nodes and an external trust center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2541</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2541</id><created>2012-04-11</created><authors><author><keyname>Volny</keyname><forenames>Petr</forenames></author><author><keyname>Novak</keyname><forenames>David</forenames></author><author><keyname>Zezula</keyname><forenames>Pavel</forenames></author></authors><title>Employing Subsequence Matching in Audio Data Processing</title><categories>cs.SD cs.DB</categories><report-no>FIMU-RS-2011-04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We overview current problems of audio retrieval and time-series subsequence
matching. We discuss the usage of subsequence matching approaches in audio data
processing, especially in automatic speech recognition (ASR) area and we aim at
improving performance of the retrieval process. To overcome the problems known
from the time-series area like the occurrence of implementation bias and data
bias we present a Subsequence Matching Framework as a tool for fast
prototyping, building, and testing similarity search subsequence matching
applications. The framework is build on top of MESSIF (Metric Similarity Search
Implementation Framework) and thus the subsequence matching algorithms can
exploit advanced similarity indexes in order to significantly increase their
query processing performance. To prove our concept we provide a design of
query-by-example spoken term detection type of application with the usage of
phonetic posteriograms and subsequence matching approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2545</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2545</id><created>2012-04-11</created><updated>2012-06-29</updated><authors><author><keyname>Wen</keyname><forenames>He</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Noise based logic: why noise? A comparative study of the necessity of
  randomness out of orthogonality</title><categories>cs.OH cs.ET</categories><comments>corrected mistyped author's order in arxiv abstract page; accepted
  for publication in Fluctuation and Noise Letters</comments><journal-ref>Fluctuation and Noise Letters 11 (2012) 1250021</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although noise-based logic shows potential advantages of reduced power
dissipation and the ability of large parallel operations with low hardware and
time complexity the question still persist: is randomness really needed out of
orthogonality? In this Letter, after some general thermodynamical
considerations, we show relevant examples where we compare the computational
complexity of logic systems based on orthogonal noise and sinusoidal signals,
respectively. The conclusion is that in certain special-purpose applications
noise-based logic is exponentially better than its sinusoidal version: its
computational complexity can be exponentially smaller to perform the same task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2566</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2566</id><created>2012-04-11</created><authors><author><keyname>Lange</keyname><forenames>Julien</forenames></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames></author></authors><title>Synthesising Choreographies from Local Session Types (extended version)</title><categories>cs.PL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing and analysing multiparty distributed interactions can be achieved
either by means of a global view (e.g. in choreography-based approaches) or by
composing available computational entities (e.g. in service orchestration).
This paper proposes a typing systems which allows, under some conditions, to
synthesise a choreography (i.e. a multiparty global type) from a set of local
session types which describe end-point behaviours (i.e. local types).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2577</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2577</id><created>2012-04-11</created><authors><author><keyname>Cui</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Wang</keyname><forenames>Zhongfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Xinmiao</forenames></author></authors><title>Reduced-Complexity Column-Layered Decoding and Implementation for LDPC
  Codes</title><categories>cs.IT math.IT</categories><comments>25 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Layered decoding is well appreciated in Low-Density Parity-Check (LDPC)
decoder implementation since it can achieve effectively high decoding
throughput with low computation complexity. This work, for the first time,
addresses low complexity column-layered decoding schemes and VLSI architectures
for multi-Gb/s applications. At first, the Min-Sum algorithm is incorporated
into the column-layered decoding. Then algorithmic transformations and
judicious approximations are explored to minimize the overall computation
complexity. Compared to the original column-layered decoding, the new approach
can reduce the computation complexity in check node processing for high-rate
LDPC codes by up to 90% while maintaining the fast convergence speed of layered
decoding. Furthermore, a relaxed pipelining scheme is presented to enable very
high clock speed for VLSI implementation. Equipped with these new techniques,
an efficient decoder architecture for quasi-cyclic LDPC codes is developed and
implemented with 0.13um CMOS technology. It is shown that a decoding throughput
of nearly 4 Gb/s at maximum of 10 iterations can be achieved for a (4096, 3584)
LDPC code. Hence, this work has facilitated practical applications of
column-layered decoding and particularly made it very attractive in high-speed,
high-rate LDPC decoder implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2581</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2581</id><created>2012-04-11</created><authors><author><keyname>Gao</keyname><forenames>Sheng</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Modeling Relational Data via Latent Factor Blockmodel</title><categories>cs.DS cs.LG stat.ML</categories><comments>10 pages, 12 figures</comments><msc-class>15A83</msc-class><acm-class>H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of modeling relational data, which
appear in many applications such as social network analysis, recommender
systems and bioinformatics. Previous studies either consider latent feature
based models but disregarding local structure in the network, or focus
exclusively on capturing local structure of objects based on latent blockmodels
without coupling with latent characteristics of objects. To combine the
benefits of the previous work, we propose a novel model that can simultaneously
incorporate the effect of latent features and covariates if any, as well as the
effect of latent structure that may exist in the data. To achieve this, we
model the relation graph as a function of both latent feature factors and
latent cluster memberships of objects to collectively discover globally
predictive intrinsic properties of objects and capture latent block structure
in the network to improve prediction performance. We also develop an
optimization transfer algorithm based on the generalized EM-style strategy to
learn the latent factors. We prove the efficacy of our proposed model through
the link prediction task and cluster analysis task, and extensive experiments
on the synthetic data and several real world datasets suggest that our proposed
LFBM model outperforms the other state of the art approaches in the evaluated
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2587</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2587</id><created>2012-04-11</created><updated>2012-12-13</updated><authors><author><keyname>Dey</keyname><forenames>Bikash Kumar</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Sarwate</keyname><forenames>Anand D.</forenames></author></authors><title>Upper Bounds on the Capacity of Binary Channels with Causal Adversaries</title><categories>cs.IT cs.CR math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory; shortened
  version appeared at ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the communication of information in the presence of
a causal adversarial jammer. In the setting under study, a sender wishes to
communicate a message to a receiver by transmitting a codeword $(x_1,...,x_n)$
bit-by-bit over a communication channel. The sender and the receiver do not
share common randomness. The adversarial jammer can view the transmitted bits
$x_i$ one at a time, and can change up to a $p$-fraction of them. However, the
decisions of the jammer must be made in a causal manner. Namely, for each bit
$x_i$ the jammer's decision on whether to corrupt it or not must depend only on
$x_j$ for $j \leq i$. This is in contrast to the &quot;classical&quot; adversarial
jamming situations in which the jammer has no knowledge of $(x_1,...,x_n)$, or
knows $(x_1,...,x_n)$ completely. In this work, we present upper bounds (that
hold under both the average and maximal probability of error criteria) on the
capacity which hold for both deterministic and stochastic encoding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2588</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2588</id><created>2012-04-11</created><authors><author><keyname>Gao</keyname><forenames>Sheng</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Probabilistic Latent Tensor Factorization Model for Link Pattern
  Prediction in Multi-relational Networks</title><categories>cs.SI cs.LG stat.ML</categories><comments>19pages, 5 figures</comments><msc-class>15A69</msc-class><acm-class>H.2.8; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at the problem of link pattern prediction in collections of
objects connected by multiple relation types, where each type may play a
distinct role. While common link analysis models are limited to single-type
link prediction, we attempt here to capture the correlations among different
relation types and reveal the impact of various relation types on performance
quality. For that, we define the overall relations between object pairs as a
\textit{link pattern} which consists in interaction pattern and connection
structure in the network, and then use tensor formalization to jointly model
and predict the link patterns, which we refer to as \textit{Link Pattern
Prediction} (LPP) problem. To address the issue, we propose a Probabilistic
Latent Tensor Factorization (PLTF) model by introducing another latent factor
for multiple relation types and furnish the Hierarchical Bayesian treatment of
the proposed probabilistic model to avoid overfitting for solving the LPP
problem. To learn the proposed model we develop an efficient Markov Chain Monte
Carlo sampling method. Extensive experiments are conducted on several real
world datasets and demonstrate significant improvements over several existing
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2601</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2601</id><created>2012-04-11</created><authors><author><keyname>Calder&#xf3;n</keyname><forenames>C.</forenames></author><author><keyname>Delaye</keyname><forenames>L.</forenames></author><author><keyname>Mireles</keyname><forenames>V.</forenames></author><author><keyname>Miramontes</keyname><forenames>P.</forenames></author></authors><title>Detecting lateral genetic material transfer</title><categories>cs.NE cs.AI q-bio.GN</categories><comments>Submitted to Applied Computational Intelligence and Soft Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bioinformatical methods to detect lateral gene transfer events are mainly
based on functional coding DNA characteristics. In this paper, we propose the
use of DNA traits not depending on protein coding requirements. We introduce
several semilocal variables that depend on DNA primary sequence and that
reflect thermodynamic as well as physico-chemical magnitudes that are able to
tell apart the genome of different organisms. After combining these variables
in a neural classificator, we obtain results whose power of resolution go as
far as to detect the exchange of genomic material between bacteria that are
phylogenetically close.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2604</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2604</id><created>2012-04-11</created><authors><author><keyname>Xu</keyname><forenames>Jun-Ming</forenames></author><author><keyname>Xu</keyname><forenames>Min</forenames></author></authors><title>The Forwarding Indices of Graphs -- a Survey</title><categories>math.CO cs.DM</categories><comments>19 pages, 42 references</comments><msc-class>05C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A routing $R$ of a given connected graph $G$ of order $n$ is a collection of
$n(n-1)$ simple paths connecting every ordered pair of vertices of $G$. The
vertex-forwarding index $\xi(G,R)$ of $G$ with respect to $R$ is defined as the
maximum number of paths in $R$ passing through any vertex of $G$. The
vertex-forwarding index $\xi(G)$ of $G$ is defined as the minimum $\xi(G,R)$
over all routing $R$'s of $G$. Similarly, the edge-forwarding index $ \pi(G,R)$
of $G$ with respect to $R$ is the maximum number of paths in $R$ passing
through any edge of $G$. The edge-forwarding index $\pi(G)$ of $G$ is the
minimum $\pi(G,R)$ over all routing $R$'s of $G$. The vertex-forwarding index
or the edge-forwarding index corresponds to the maximum load of the graph.
Therefore, it is important to find routings minimizing these indices and thus
has received much research attention in the past ten years and more. In this
paper we survey some known results on these forwarding indices, further
research problems and several conjectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2606</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2606</id><created>2012-04-11</created><authors><author><keyname>Kenthapadi</keyname><forenames>Krishnaram</forenames></author><author><keyname>Korolova</keyname><forenames>Aleksandra</forenames></author><author><keyname>Mironov</keyname><forenames>Ilya</forenames></author><author><keyname>Mishra</keyname><forenames>Nina</forenames></author></authors><title>Privacy via the Johnson-Lindenstrauss Transform</title><categories>cs.DS cs.CY cs.DB cs.SI</categories><comments>24 pages</comments><acm-class>K.4.1; F.2; H.3.5; G.3; I.5.3; H.3.3; H.2.8; E.1; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that party A collects private information about its users, where each
user's data is represented as a bit vector. Suppose that party B has a
proprietary data mining algorithm that requires estimating the distance between
users, such as clustering or nearest neighbors. We ask if it is possible for
party A to publish some information about each user so that B can estimate the
distance between users without being able to infer any private bit of a user.
Our method involves projecting each user's representation into a random,
lower-dimensional space via a sparse Johnson-Lindenstrauss transform and then
adding Gaussian noise to each entry of the lower-dimensional representation. We
show that the method preserves differential privacy---where the more privacy is
desired, the larger the variance of the Gaussian noise. Further, we show how to
approximate the true distances between users via only the lower-dimensional,
perturbed data. Finally, we consider other perturbation methods such as
randomized response and draw comparisons to sketch-based methods. While the
goal of releasing user-specific data to third parties is more broad than
preserving distances, this work shows that distance computations with privacy
is an achievable goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2609</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2609</id><created>2012-04-11</created><updated>2012-04-15</updated><authors><author><keyname>Li</keyname><forenames>Xiong</forenames></author><author><keyname>Lee</keyname><forenames>Tai Sing</forenames></author><author><keyname>Liu</keyname><forenames>Yuncai</forenames></author></authors><title>Stochastic Feature Mapping for PAC-Bayes Classification</title><categories>cs.LG</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic generative modeling of data distributions can potentially
exploit hidden information which is useful for discriminative classification.
This observation has motivated the development of approaches that couple
generative and discriminative models for classification. In this paper, we
propose a new approach to couple generative and discriminative models in an
unified framework based on PAC-Bayes risk theory. We first derive the
model-parameter-independent stochastic feature mapping from a practical MAP
classifier operating on generative models. Then we construct a linear
stochastic classifier equipped with the feature mapping, and derive the
explicit PAC-Bayes risk bounds for such classifier for both supervised and
semi-supervised learning. Minimizing the risk bound, using an EM-like iterative
procedure, results in a new posterior over hidden variables (E-step) and the
update rules of model parameters (M-step). The derivation of the posterior is
always feasible due to the way of equipping feature mapping and the explicit
form of bounding risk. The derived posterior allows the tuning of generative
models and subsequently the feature mappings for better classification. The
derived update rules of the model parameters are same to those of the uncoupled
models as the feature mapping is model-parameter-independent. Our experiments
show that the coupling between data modeling generative model and the
discriminative classifier via a stochastic feature mapping in this framework
leads to a general classification tool with state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2610</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2610</id><created>2012-04-11</created><authors><author><keyname>Kiran</keyname><forenames>P.</forenames></author><author><keyname>Kumar</keyname><forenames>S Sathish</forenames></author><author><keyname>Kavya</keyname><forenames>N. P.</forenames></author></authors><title>A Novel Framework using Elliptic Curve Cryptography for Extremely Secure
  Transmission in Distributed Privacy Preserving Data Mining</title><categories>cs.DB cs.CR</categories><comments>8 pages</comments><journal-ref>Advanced Computing: An International Journal ( ACIJ ), Vol.3,
  No.2, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Privacy Preserving Data Mining is a method which ensures privacy of
individual information during mining. Most important task involves retrieving
information from multiple data bases which is distributed. The data once in the
data warehouse can be used by mining algorithms to retrieve confidential
information. The proposed framework has two major tasks, secure transmission
and privacy of confidential information during mining. Secure transmission is
handled by using elliptic curve cryptography and data distortion for privacy
preservation ensuring highly secure environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2611</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2611</id><created>2012-04-12</created><updated>2014-12-21</updated><authors><author><keyname>Zhu</keyname><forenames>Junan</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author></authors><title>Recovery from Linear Measurements with Complexity-Matching Universal
  Signal Estimation</title><categories>cs.IT math.IT</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the compressed sensing (CS) signal estimation problem where an input
signal is measured via a linear matrix multiplication under additive noise.
While this setup usually assumes sparsity or compressibility in the input
signal during recovery, the signal structure that can be leveraged is often not
known a priori. In this paper, we consider universal CS recovery, where the
statistics of a stationary ergodic signal source are estimated simultaneously
with the signal itself. Inspired by Kolmogorov complexity and minimum
description length, we focus on a maximum a posteriori (MAP) estimation
framework that leverages universal priors to match the complexity of the
source. Our framework can also be applied to general linear inverse problems
where more measurements than in CS might be needed. We provide theoretical
results that support the algorithmic feasibility of universal MAP estimation
using a Markov chain Monte Carlo implementation, which is computationally
challenging. We incorporate some techniques to accelerate the algorithm while
providing comparable and in many cases better reconstruction quality than
existing algorithms. Experimental results show the promise of universality in
CS, particularly for low-complexity sources that do not exhibit standard
sparsity or compressibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2613</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2613</id><created>2012-04-12</created><authors><author><keyname>V</keyname><forenames>Suma.</forenames></author><author><keyname>Deshpande</keyname><forenames>Bhagavant</forenames></author><author><keyname>M</keyname><forenames>Vaidehi.</forenames></author><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author></authors><title>Cloud Computing For Microfinances</title><categories>cs.OH</categories><comments>3 Pages, 2 Figures, International Conference On Systemics,
  Cybernetics and Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolution of Science and Engineering has led to the growth of several
commercial applications. The wide spread implementation of commercial based
applications has in turn directed the emergence of advanced technologies such
as cloud computing. India has well proven itself as a potential hub for
advanced technologies including cloud based industrial market. Microfinance
system has emerged out as a panacea to Indian economy since the population
encompasses of people who come under poverty and below poverty index. However,
one of the key challenges in successful operation of microfinance system in
India has given rise to integration of financial services using sophisticated
cloud computing model. This paper, therefore propose a fundamental cloud-based
microfinance model in order to reduce high transaction risks involved during
microfinance operations in an inexpensive and efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2616</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2616</id><created>2012-04-12</created><authors><author><keyname>Nair</keyname><forenames>T. R. Gopalakrishnan</forenames></author><author><keyname>V</keyname><forenames>Suma</forenames></author><author><keyname>S</keyname><forenames>Manas</forenames></author></authors><title>Genetic Algorithm to Make Persistent Security and Quality of Image in
  Steganography from RS Analysis</title><categories>cs.MM cs.CR</categories><comments>8 Pages, 4 Figures, Swarm Evolutionary and Memetric Computing
  Conference (SEMCCO), Vishakhapatnam</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retention of secrecy is one of the significant features during communication
activity. Steganography is one of the popular methods to achieve secret
communication between sender and receiver by hiding message in any form of
cover media such as an audio, video, text, images etc. Least significant bit
encoding is the simplest encoding method used by many steganography programs to
hide secret message in 24bit, 8bit colour images and grayscale images.
Steganalysis is a method of detecting secret message hidden in a cover media
using steganography. RS steganalysis is one of the most reliable steganalysis
which performs statistical analysis of the pixels to successfully detect the
hidden message in an image. However, existing steganography method protects the
information against RS steganalysis in grey scale images. This paper presents a
steganography method using genetic algorithm to protect against the RS attack
in colour images. Stego image is divided into number of blocks. Subsequently,
with the implementation of natural evolution on the stego image using genetic
algorithm enables to achieve optimized security and image quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2621</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2621</id><created>2012-04-12</created><authors><author><keyname>Han</keyname><forenames>Youngae</forenames></author></authors><title>An efficient solver for volumetric scattering based on fast spherical
  harmonics transforms</title><categories>cs.NA math.NA physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Helmholtz equation arises in the study of electromagnetic radiation,
optics, acoustics, etc. In spherical coordinates, its general solution can be
written as a spherical harmonic series which satisfies the radiation condition
at infinity, ensuring that the wave is outgoing. The boundary condition at
infinity is hard to enforce with a finite element method since a suitable
approximation needs to be made within reasonable distance from scatterers.
  Luckily, the Helmholtz equation can be represented as a Lippmann-Schwinger
integral equation which removes the necessity of the boundary approximations
and its Green's function can be expanded as a spherical harmonic series which
leads to our numerical scheme based on spherical harmonic polynomial transform.
In this paper, we present an efficient solver for the Helmholtz equation which
costs $O(N\log N)$ operations, where $N$ is the number of the discretization
points. We use the fast spherical harmonic transforms which are originally
developed in \cite{suda}. The convergence order of the method is tied to the
global regularity of the solution. At the lower end, it is second order
accurate for discontinuous material properties. The order increases with
increasing regularity leading to spectral convergence for globally smooth
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2622</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2622</id><created>2012-04-12</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Singhal</keyname><forenames>Tanuj</forenames></author><author><keyname>Ghanshyam</keyname></author><author><keyname>Ahlawat</keyname><forenames>Khyati</forenames></author><author><keyname>Noble</keyname></author></authors><title>Application Independent Energy Efficient Data Aggregation in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1201.4943</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 2, No 1, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor networks are dense networks of small, low-cost sensors, which
collect and disseminate environmental data and thus facilitate monitoring and
controlling of physical environment from remote locations with better accuracy.
The major challenge is to achieve energy efficiency during the communication
among the nodes. This paper aims at proposing a solution to schedule the node's
activities to reduce the energy consumption. We propose the construction of a
decentralized lifetime maximizing tree within clusters. We aim at minimizing
the distance of transmission with minimization of energy consumption. The
sensor network is distributed into clusters based on the close proximity of the
nodes. Data transfer among the nodes is done with a hybrid technique of both
TDMA/ FDMA which leads to efficient utilization of bandwidth and maximizing
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2634</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2634</id><created>2012-04-12</created><authors><author><keyname>De</keyname><forenames>Minati</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Nandy</keyname><forenames>Subhas C.</forenames></author></authors><title>Space-efficient Algorithms for Visibility Problems in Simple Polygon</title><categories>cs.CG</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simple polygon $P$ consisting of $n$ vertices, we study the problem
of designing space-efficient algorithms for computing (i) the visibility
polygon of a point inside $P$, (ii) the weak visibility polygon of a line
segment inside $P$ and (iii) the minimum link path between a pair of points
inside $P$. For problem (i) two algorithms are proposed. The first one is an
in-place algorithm where the input array may be lost. It uses only O(1) extra
space apart from the input array. The second one assumes that the input is
given in a read-only array, and it needs $O(\sqrt{n})$ extra space. The time
complexity of both the algorithms are O(n). For problem (ii), we have assumed
that the input polygon is given in a read-only array. Our proposed algorithm
runs in $O(n^2)$ time using O(1) extra space. For problem (iii) the time and
space complexities of our proposed algorithm are $O(kn)$ and O(1) respectively;
$k$ is the length (number of links) in a minimum link path between the given
pair of points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2637</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2637</id><created>2012-04-12</created><authors><author><keyname>Chablat</keyname><forenames>Damien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Moroz</keyname><forenames>Guillaume</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Arakelian</keyname><forenames>Vigen</forenames><affiliation>DGMA</affiliation></author><author><keyname>Briot</keyname><forenames>S&#xe9;bastien</forenames><affiliation>IRCCyN</affiliation></author><author><keyname>Wenger</keyname><forenames>Philippe</forenames><affiliation>IRCCyN</affiliation></author></authors><title>Solution regions in the parameter space of a 3-RRR decoupled robot for a
  prescribed workspace</title><categories>cs.RO</categories><proxy>ccsd</proxy><journal-ref>Advances in Robot Kinematics, Innsbruck : Austria (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new design method to determine the feasible set of
parameters of translational or position/orientation decoupled parallel robots
for a prescribed singularity-free workspace of regular shape. The suggested
method uses Groebner bases to define the singularities and the cylindrical
algebraic decomposition to characterize the set of parameters. It makes it
possible to generate all the robot designs. A 3-RRR decoupled robot is used to
validate the proposed design method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2649</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2649</id><created>2012-04-12</created><authors><author><keyname>Shaqfeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Alnuweiri</keyname><forenames>Hussein</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Multiuser Switched Diversity Scheduling Schemes</title><categories>cs.IT math.IT</categories><comments>Accepted at IEEE Transactions on Communications, to appear 2012,
  funded by NPRP grant 08-577-2-241 from QNRF</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiuser switched-diversity scheduling schemes were recently proposed in
order to overcome the heavy feedback requirements of conventional opportunistic
scheduling schemes by applying a threshold-based, distributed, and ordered
scheduling mechanism. The main idea behind these schemes is that slight
reduction in the prospected multiuser diversity gains is an acceptable
trade-off for great savings in terms of required channel-state-information
feedback messages. In this work, we characterize the achievable rate region of
multiuser switched diversity systems and compare it with the rate region of
full feedback multiuser diversity systems. We propose also a novel proportional
fair multiuser switched-based scheduling scheme and we demonstrate that it can
be optimized using a practical and distributed method to obtain the feedback
thresholds. We finally demonstrate by numerical examples that
switched-diversity scheduling schemes operate within 0.3 bits/sec/Hz from the
ultimate network capacity of full feedback systems in Rayleigh fading
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2651</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2651</id><created>2012-04-12</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Song</keyname><forenames>S. H.</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Cooperative Cognitive Networks: Optimal, Distributed and Low-Complexity
  Algorithms</title><categories>cs.IT math.IT</categories><comments>27 pages, 7 figures. Revision submitted to IEEE Transactions on
  Signal Processing</comments><doi>10.1109/TSP.2013.2257762</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the cooperation between a cognitive system and a primary
system where multiple cognitive base stations (CBSs) relay the primary user's
(PU) signals in exchange for more opportunity to transmit their own signals.
The CBSs use amplify-and-forward (AF) relaying and coordinated beamforming to
relay the primary signals and transmit their own signals. The objective is to
minimize the overall transmit power of the CBSs given the rate requirements of
the PU and the cognitive users (CUs). We show that the relaying matrices have
unit rank and perform two functions: Matched filter receive beamforming and
transmit beamforming. We then develop two efficient algorithms to find the
optimal solution. The first one has linear convergence rate and is suitable for
distributed implementation, while the second one enjoys superlinear convergence
but requires centralized processing. Further, we derive the beamforming vectors
for the linear conventional zero-forcing (CZF) and prior zero-forcing (PZF)
schemes, which provide much simpler solutions. Simulation results demonstrate
the improvement in terms of outage performance due to the cooperation between
the primary and cognitive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2652</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2652</id><created>2012-04-12</created><updated>2013-06-27</updated><authors><author><keyname>Podolskii</keyname><forenames>Vladimir V.</forenames><affiliation>Steklov Mathematical Institute</affiliation></author></authors><title>Lower Bound on Weights of Large Degree Threshold Functions</title><categories>cs.CC</categories><comments>17 pages, to appear in LMCS</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (June 28,
  2013) lmcs:722</journal-ref><doi>10.2168/LMCS-9(2:13)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integer polynomial $p$ of $n$ variables is called a \emph{threshold gate}
for a Boolean function $f$ of $n$ variables if for all $x \in \zoon$ $f(x)=1$
if and only if $p(x)\geq 0$. The \emph{weight} of a threshold gate is the sum
of its absolute values.
  In this paper we study how large a weight might be needed if we fix some
function and some threshold degree. We prove $2^{\Omega(2^{2n/5})}$ lower bound
on this value. The best previous bound was $2^{\Omega(2^{n/8})}$ (Podolskii,
2009).
  In addition we present substantially simpler proof of the weaker
$2^{\Omega(2^{n/4})}$ lower bound. This proof is conceptually similar to other
proofs of the bounds on weights of nonlinear threshold gates, but avoids a lot
of technical details arising in other proofs. We hope that this proof will help
to show the ideas behind the construction used to prove these lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2660</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2660</id><created>2012-04-12</created><authors><author><keyname>Shayovitz</keyname><forenames>Shachar</forenames></author><author><keyname>Raphaeli</keyname><forenames>Dan</forenames></author></authors><title>Efficient Iterative Decoding of LDPC in the Presence of Strong Phase
  Noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new efficient message passing algorithm for
decoding LDPC transmitted over a channel with strong phase noise. The algorithm
performs approximate bayesian inference on a factor graph representation of the
channel and code joint posterior. The approximate inference is based on an
improved canonical model for the messages of the Sum &amp; Product Algorithm, and a
method for clustering the messages using the directional statistics framework.
The proposed canonical model includes treatment for phase slips which can limit
the performance of tracking algorithms. We show simulation results and
complexity analysis for the proposed algorithm demonstrating its superiority
over some of the current state of the art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2676</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2676</id><created>2012-04-12</created><updated>2012-05-24</updated><authors><author><keyname>Bui</keyname><forenames>Huyen-Chi</forenames></author><author><keyname>Meric</keyname><forenames>Hugo</forenames></author><author><keyname>Lacan</keyname><forenames>Jerome</forenames></author><author><keyname>Boucheret</keyname><forenames>Marie-Laure</forenames></author></authors><title>A Cooperative Network Coding Strategy for the Interference Relay Channel</title><categories>cs.NI</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study an interference relay network with a satellite as
relay. We propose a cooperative strategy based on physical layer network coding
and superposition modulation decoding for uni-directional communications among
users. The performance of our solution in terms of throughput is evaluated
through capacity analysis and simulations that include practical constraints
such as the lack of synchronization in time and frequency. We demonstrate
throughputs significantly larger than the classical time sharing case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2677</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2677</id><created>2012-04-12</created><authors><author><keyname>Lee</keyname><forenames>Conrad</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>The Geographic Flow of Music</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>8 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social media website last.fm provides a detailed snapshot of what its
users in hundreds of cities listen to each week. After suitably normalizing
this data, we use it to test three hypotheses related to the geographic flow of
music. The first is that although many of the most popular artists are listened
to around the world, music preferences are closely related to nationality,
language, and geographic location. We find support for this hypothesis, with a
couple of minor, yet interesting, exceptions. Our second hypothesis is that
some cities are consistently early adopters of new music (and early to snub
stale music). To test this hypothesis, we adapt a method previously used to
detect the leadership networks present in flocks of birds. We find empirical
support for the claim that a similar leadership network exists among cities,
and this finding is the main contribution of the paper. Finally, we test the
hypothesis that large cities tend to be ahead of smaller cities-we find only
weak support for this hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2692</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2692</id><created>2012-04-12</created><authors><author><keyname>Xia</keyname><forenames>Xiaochen</forenames></author><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Asynchronous Physical-layer Network Coding Scheme for Two-way OFDM Relay</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, Submitted to GLOBECOM 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In two-way OFDM relay, carrier frequency offsets (CFOs) between relay and
terminal nodes introduce severe intercarrier interference (ICI) which degrades
the performance of traditional physical-layer network coding (PLNC). Moreover,
traditional algorithm to compute the posteriori probability in the presence of
ICI would incur prohibitive computational complexity at the relay node. In this
paper, we proposed a two-step asynchronous PLNC scheme at the relay to mitigate
the effect of CFOs. In the first step, we intend to reconstruct the ICI
component, in which space-alternating generalized expectationmaximization
(SAGE) algorithm is used to jointly estimate the needed parameters. In the
second step, a channel-decoding and network-coding scheme is proposed to
transform the received signal into the XOR of two terminals' transmitted
information using the reconstructed ICI. It is shown that the proposed scheme
greatly mitigates the impact of CFOs with a relatively lower computational
complexity in two-way OFDM relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2710</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2710</id><created>2012-04-12</created><updated>2013-01-03</updated><authors><author><keyname>Johnsen</keyname><forenames>Trygve</forenames></author><author><keyname>Verdure</keyname><forenames>Hugues</forenames></author></authors><title>Stanley-Reisner resolution of constant weight linear codes</title><categories>cs.DM math.CO</categories><msc-class>94B05, 05E45, 05B35, 13F55</msc-class><doi>10.1007/s10623-012-9767-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a constant weight linear code, we investigate its weight hierarchy and
the Stanley-Reisner resolution of its associated matroid regarded as a
simplicial complex. We also exhibit conditions on the higher weights sufficient
to conclude that the code is of constant weight
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2712</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2712</id><created>2012-04-12</created><authors><author><keyname>Fujita</keyname><forenames>Sumio</forenames></author><author><keyname>Dupret</keyname><forenames>Georges</forenames></author><author><keyname>Baeza-Yates</keyname><forenames>Ricardo</forenames></author></authors><title>Learning to Rank Query Recommendations by Semantic Similarities</title><categories>cs.AI cs.HC cs.IR</categories><comments>2nd International Workshop on Usage Analysis and the Web of Data
  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),
  Lyon, France, April 17th, 2012</comments><report-no>WWW2012USEWOD/2012/fuduba</report-no><acm-class>H.3.3; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logs of the interactions with a search engine show that users often
reformulate their queries. Examining these reformulations shows that
recommendations that precise the focus of a query are helpful, like those based
on expansions of the original queries. But it also shows that queries that
express some topical shift with respect to the original query can help user
access more rapidly the information they need. We propose a method to identify
from the query logs of past users queries that either focus or shift the
initial query topic. This method combines various click-based, topic-based and
session based ranking strategies and uses supervised learning in order to
maximize the semantic similarities between the query and the recommendations,
while at the same diversifying them. We evaluate our method using the
query/click logs of a Japanese web search engine and we show that the
combination of the three methods proposed is significantly better than any of
them taken individually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2713</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2713</id><created>2012-04-12</created><updated>2012-04-17</updated><authors><author><keyname>Hoxha</keyname><forenames>Julia</forenames></author><author><keyname>Junghans</keyname><forenames>Martin</forenames></author><author><keyname>Agarwal</keyname><forenames>Sudhir</forenames></author></authors><title>Enabling Semantic Analysis of User Browsing Patterns in the Web of Data</title><categories>cs.AI cs.HC cs.IR</categories><comments>2nd International Workshop on Usage Analysis and the Web of Data
  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),
  Lyon, France, April 17th, 2012</comments><report-no>WWW2012USEWOD/2012/hojuag</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A useful step towards better interpretation and analysis of the usage
patterns is to formalize the semantics of the resources that users are
accessing in the Web. We focus on this problem and present an approach for the
semantic formalization of usage logs, which lays the basis for eective
techniques of querying expressive usage patterns. We also present a query
answering approach, which is useful to nd in the logs expressive patterns of
usage behavior via formulation of semantic and temporal-based constraints. We
have processed over 30 thousand user browsing sessions extracted from usage
logs of DBPedia and Semantic Web Dog Food. All these events are formalized
semantically using respective domain ontologies and RDF representations of the
Web resources being accessed. We show the eectiveness of our approach through
experimental results, providing in this way an exploratory analysis of the way
users browse theWeb of Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2715</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2715</id><created>2012-04-12</created><authors><author><keyname>Knuth</keyname><forenames>Magnus</forenames></author><author><keyname>Hercher</keyname><forenames>Johannes</forenames></author><author><keyname>Sack</keyname><forenames>Harald</forenames></author></authors><title>Collaboratively Patching Linked Data</title><categories>cs.IR cs.DL cs.HC</categories><comments>2nd International Workshop on Usage Analysis and the Web of Data
  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),
  Lyon, France, April 17th, 2012</comments><report-no>WWW2012USEWOD/2012/knhesa</report-no><acm-class>H.3.5; C.2.4; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's Web of Data is noisy. Linked Data often needs extensive preprocessing
to enable efficient use of heterogeneous resources. While consistent and valid
data provides the key to efficient data processing and aggregation we are
facing two main challenges: (1st) Identification of erroneous facts and
tracking their origins in dynamically connected datasets is a difficult task,
and (2nd) efforts in the curation of deficient facts in Linked Data are
exchanged rather rarely. Since erroneous data often is duplicated and
(re-)distributed by mashup applications it is not only the responsibility of a
few original publishers to keep their data tidy, but progresses to be a mission
for all distributers and consumers of Linked Data too. We present a new
approach to expose and to reuse patches on erroneous data to enhance and to add
quality information to the Web of Data. The feasibility of our approach is
demonstrated by example of a collaborative game that patches statements in
DBpedia data and provides notifications for relevant changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2718</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2718</id><created>2012-04-12</created><authors><author><keyname>Thalhammer</keyname><forenames>Andreas</forenames></author><author><keyname>Toma</keyname><forenames>Ioan</forenames></author><author><keyname>Roa-Valverde</keyname><forenames>Antonio</forenames></author><author><keyname>Fensel</keyname><forenames>Dieter</forenames></author></authors><title>Leveraging Usage Data for Linked Data Movie Entity Summarization</title><categories>cs.AI cs.HC cs.IR</categories><comments>2nd International Workshop on Usage Analysis and the Web of Data
  (USEWOD2012) in the 21st International World Wide Web Conference (WWW2012),
  Lyon, France, April 17th, 2012</comments><report-no>WWW2012USEWOD/2012/thtorofe</report-no><acm-class>H.1.2; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel research in the field of Linked Data focuses on the problem of entity
summarization. This field addresses the problem of ranking features according
to their importance for the task of identifying a particular entity. Next to a
more human friendly presentation, these summarizations can play a central role
for semantic search engines and semantic recommender systems. In current
approaches, it has been tried to apply entity summarization based on patterns
that are inherent to the regarded data.
  The proposed approach of this paper focuses on the movie domain. It utilizes
usage data in order to support measuring the similarity between movie entities.
Using this similarity it is possible to determine the k-nearest neighbors of an
entity. This leads to the idea that features that entities share with their
nearest neighbors can be considered as significant or important for these
entities. Additionally, we introduce a downgrading factor (similar to TF-IDF)
in order to overcome the high number of commonly occurring features. We
exemplify the approach based on a movie-ratings dataset that has been linked to
Freebase entities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2727</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2727</id><created>2012-04-11</created><updated>2014-12-04</updated><authors><author><keyname>Brazil</keyname><forenames>Emilio Vital</forenames></author><author><keyname>da Fonseca</keyname><forenames>Guilherme D.</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Celina</forenames></author><author><keyname>Sasaki</keyname><forenames>Diana</forenames></author></authors><title>The Cost of Perfection for Matchings in Graphs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perfect matchings and maximum weight matchings are two fundamental
combinatorial structures. We consider the ratio between the maximum weight of a
perfect matching and the maximum weight of a general matching. Motivated by the
computer graphics application in triangle meshes, where we seek to convert a
triangulation into a quadrangulation by merging pairs of adjacent triangles, we
focus mainly on bridgeless cubic graphs. First, we characterize graphs that
attain the extreme ratios. Second, we present a lower bound for all bridgeless
cubic graphs. Third, we present upper bounds for subclasses of bridgeless cubic
graphs, most of which are shown to be tight. Additionally, we present tight
bounds for the class of regular bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2731</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2731</id><created>2012-04-12</created><authors><author><keyname>Gross</keyname><forenames>Anika</forenames></author><author><keyname>Hartung</keyname><forenames>Michael</forenames></author><author><keyname>Thor</keyname><forenames>Andreas</forenames></author><author><keyname>Rahm</keyname><forenames>Erhard</forenames></author></authors><title>How do Ontology Mappings Change in the Life Sciences?</title><categories>cs.DB</categories><comments>Keywords: mapping evolution, ontology matching, ontology evolution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mappings between related ontologies are increasingly used to support data
integration and analysis tasks. Changes in the ontologies also require the
adaptation of ontology mappings. So far the evolution of ontology mappings has
received little attention albeit ontologies change continuously especially in
the life sciences. We therefore analyze how mappings between popular life
science ontologies evolve for different match algorithms. We also evaluate
which semantic ontology changes primarily affect the mappings. We further
investigate alternatives to predict or estimate the degree of future mapping
changes based on previous ontology and mapping transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2741</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2741</id><created>2012-04-12</created><authors><author><keyname>Barbu</keyname><forenames>Andrei</forenames></author><author><keyname>Michaux</keyname><forenames>Aaron</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Siddharth</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Simultaneous Object Detection, Tracking, and Event Recognition</title><categories>cs.CV cs.AI</categories><journal-ref>Advances in Cognitive Systems, Vol. 2, pp. 203-220, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The common internal structure and algorithmic organization of object
detection, detection-based tracking, and event recognition facilitates a
general approach to integrating these three components. This supports
multidirectional information flow between these components allowing object
detection to influence tracking and event recognition and event recognition to
influence tracking and object detection. The performance of the combination can
exceed the performance of the components in isolation. This can be done with
linear asymptotic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2742</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2742</id><created>2012-04-12</created><authors><author><keyname>Barbu</keyname><forenames>Andrei</forenames></author><author><keyname>Bridge</keyname><forenames>Alexander</forenames></author><author><keyname>Burchill</keyname><forenames>Zachary</forenames></author><author><keyname>Coroian</keyname><forenames>Dan</forenames></author><author><keyname>Dickinson</keyname><forenames>Sven</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Michaux</keyname><forenames>Aaron</forenames></author><author><keyname>Mussman</keyname><forenames>Sam</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Siddharth</forenames></author><author><keyname>Salvi</keyname><forenames>Dhaval</forenames></author><author><keyname>Schmidt</keyname><forenames>Lara</forenames></author><author><keyname>Shangguan</keyname><forenames>Jiangnan</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author><author><keyname>Waggoner</keyname><forenames>Jarrell</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author><author><keyname>Wei</keyname><forenames>Jinlian</forenames></author><author><keyname>Yin</keyname><forenames>Yifan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiqi</forenames></author></authors><title>Video In Sentences Out</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system that produces sentential descriptions of video: who did
what to whom, and where and how they did it. Action class is rendered as a
verb, participant objects as noun phrases, properties of those objects as
adjectival modifiers in those noun phrases,spatial relations between those
participants as prepositional phrases, and characteristics of the event as
prepositional-phrase adjuncts and adverbial modifiers. Extracting the
information needed to render these linguistic entities requires an approach to
event recognition that recovers object tracks, the track-to-role assignments,
and changing body posture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2765</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2765</id><created>2012-04-12</created><updated>2012-08-18</updated><authors><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Kornai</keyname><forenames>Andr&#xe1;s</forenames></author><author><keyname>Kert&#xe9;sz</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>A practical approach to language complexity: a Wikipedia case study</title><categories>cs.CL physics.data-an physics.soc-ph</categories><comments>2 new figures, 1 new section, and 2 new supporting texts</comments><journal-ref>PLoS ONE 7(11): e48386 (2012)</journal-ref><doi>10.1371/journal.pone.0048386</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present statistical analysis of English texts from
Wikipedia. We try to address the issue of language complexity empirically by
comparing the simple English Wikipedia (Simple) to comparable samples of the
main English Wikipedia (Main). Simple is supposed to use a more simplified
language with a limited vocabulary, and editors are explicitly requested to
follow this guideline, yet in practice the vocabulary richness of both samples
are at the same level. Detailed analysis of longer units (n-grams of words and
part of speech tags) shows that the language of Simple is less complex than
that of Main primarily due to the use of shorter sentences, as opposed to
drastically simplified syntax or vocabulary. Comparing the two language
varieties by the Gunning readability index supports this conclusion. We also
report on the topical dependence of language complexity, e.g. that the language
is more advanced in conceptual articles compared to person-based (biographical)
and object-based articles. Finally, we investigate the relation between
conflict and language complexity by analyzing the content of the talk pages
associated to controversial and peacefully developing articles, concluding that
controversy has the effect of reducing language complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2768</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2768</id><created>2012-04-12</created><authors><author><keyname>Filipiuk</keyname><forenames>Piotr</forenames></author><author><keyname>Nielson</keyname><forenames>Flemming</forenames></author><author><keyname>Nielson</keyname><forenames>Hanne Riis</forenames></author></authors><title>Layered Fixed Point Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a logic for the specification of static analysis problems that
goes beyond the logics traditionally used. Its most prominent feature is the
direct support for both inductive computations of behaviors as well as
co-inductive specifications of properties. Two main theoretical contributions
are a Moore Family result and a parametrized worst case time complexity result.
We show that the logic and the associated solver can be used for rapid
prototyping and illustrate a wide variety of applications within Static
Analysis, Constraint Satisfaction Problems and Model Checking. In all cases the
complexity result specializes to the worst case time complexity of the
classical methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2772</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2772</id><created>2012-04-12</created><authors><author><keyname>Alipour</keyname><forenames>Mehdi</forenames></author><author><keyname>Taghdisi</keyname><forenames>Hojjat</forenames></author></authors><title>Effect of Thread Level Parallelism on the Performance of Optimum
  Architecture for Embedded Applications</title><categories>cs.AR cs.PF</categories><comments>International Journal of Embedded Systems and Applications (IJESA),
  http://airccse.org/journal/ijesa/current2012.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to the increasing complexity of network application and internet
traffic, network processor as a subset of embedded processors have to process
more computation intensive tasks. By scaling down the feature size and emersion
of chip multiprocessors (CMP) that are usually multi-thread processors, the
performance requirements are somehow guaranteed. As multithread processors are
the heir of uni-thread processors and there isn't any general design flow to
design a multithread embedded processor, in this paper we perform a
comprehensive design space exploration for an optimum uni-thread embedded
processor based on the limited area and power budgets. Finally we run multiple
threads on this architecture to find out the maximum thread level parallelism
(TLP) based on performance per power and area optimum uni-thread architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2774</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2774</id><created>2012-04-12</created><authors><author><keyname>Xing</keyname><forenames>Xinyu</forenames></author><author><keyname>Ahn</keyname><forenames>Junho</forenames></author><author><keyname>Lee</keyname><forenames>Wenke</forenames></author><author><keyname>Han</keyname><forenames>Richard</forenames></author><author><keyname>Mishra</keyname><forenames>Shivakant</forenames></author></authors><title>An Empirical Study of Spam and Prevention Mechanisms in Online Video
  Chat Services</title><categories>cs.CR</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, online video chat services are becoming increasingly popular. While
experiencing tremendous growth, online video chat services have also become yet
another spamming target. Unlike spam propagated via traditional medium like
emails and social networks, we find that spam propagated via online video chat
services is able to draw much larger attention from the users. We have
conducted several experiments to investigate spam propagation on Chatroulette -
the largest online video chat website. We have found that the largest spam
campaign on online video chat websites is dating scams. Our study indicates
that spam carrying dating or pharmacy scams have much higher clickthrough rates
than email spam carrying the same content. In particular, dating scams reach a
clickthrough rate of 14.97%. We also examined and analysed spam prevention
mechanisms that online video chat websites have designed and implemented. Our
study indicates that the prevention mechanisms either harm legitimate user
experience or can be easily bypassed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2775</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2775</id><created>2012-04-12</created><updated>2013-03-02</updated><authors><author><keyname>Morgenshtern</keyname><forenames>Veniamin I.</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Lin</keyname><forenames>Shaowei</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Capacity Pre-Log of Noncoherent SIMO Channels via Hironaka's Theorem</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We find the capacity pre-log of a temporally correlated Rayleigh block-fading
SIMO channel in the noncoherent setting. It is well known that for block-length
L and rank of the channel covariance matrix equal to Q, the capacity pre-log in
the SISO case is given by 1-Q/L. Here, Q/L can be interpreted as the pre-log
penalty incurred by channel uncertainty. Our main result reveals that, by
adding only one receive antenna, this penalty can be reduced to 1/L and can,
hence, be made to vanish in the large-L limit, even if Q/L remains constant as
L goes to infinity. Intuitively, even though the SISO channels between the
transmit antenna and the two receive antennas are statistically independent,
the transmit signal induces enough statistical dependence between the
corresponding receive signals for the second receive antenna to be able to
resolve the uncertainty associated with the first receive antenna's channel and
thereby make the overall system appear coherent. The proof of our main theorem
is based on a deep result from algebraic geometry known as Hironaka's Theorem
on the Resolution of Singularities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2798</identifier>
 <datestamp>2012-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2798</id><created>2012-04-12</created><updated>2012-09-18</updated><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Roy</keyname><forenames>Bimal</forenames></author><author><keyname>Laha</keyname><forenames>Anirban</forenames></author></authors><title>An Efficient Cryptographic Hash Algorithm (BSA)</title><categories>cs.CR</categories><comments>The paper is available at
  http://subhabrata-mukherjee.webs.com/publications.htm</comments><journal-ref>In Proceedings of The 10th National Workshop on Cryptology (NWCR),
  2010, PSG College of Technology, Peelamedu,Coimbatore, September 2-4, 2010</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent cryptanalytic attacks have exposed the vulnerabilities of some widely
used cryptographic hash functions like MD5 and SHA-1. Attacks in the line of
differential attacks have been used to expose the weaknesses of several other
hash functions like RIPEMD, HAVAL. In this paper we propose a new efficient
hash algorithm that provides a near random hash output and overcomes some of
the earlier weaknesses. Extensive simulations and comparisons with some
existing hash functions have been done to prove the effectiveness of the BSA,
which is an acronym for the name of the 3 authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2801</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2801</id><created>2012-04-12</created><authors><author><keyname>Narayanaswamy</keyname><forenames>Siddharth</forenames></author><author><keyname>Barbu</keyname><forenames>Andrei</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>Seeing Unseeability to See the Unseeable</title><categories>cs.CV cs.AI cs.RO</categories><journal-ref>Advances in Cognitive Systems, Vol. 2, pp. 77-94, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework that allows an observer to determine occluded portions
of a structure by finding the maximum-likelihood estimate of those occluded
portions consistent with visible image evidence and a consistency model. Doing
this requires determining which portions of the structure are occluded in the
first place. Since each process relies on the other, we determine a solution to
both problems in tandem. We extend our framework to determine confidence of
one's assessment of which portions of an observed structure are occluded, and
the estimate of that occluded structure, by determining the sensitivity of
one's assessment to potential new observations. We further extend our framework
to determine a robotic action whose execution would allow a new observation
that would maximally increase one's confidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2804</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2804</id><created>2012-04-12</created><authors><author><keyname>Ott</keyname><forenames>Myle</forenames></author><author><keyname>Cardie</keyname><forenames>Claire</forenames></author><author><keyname>Hancock</keyname><forenames>Jeff</forenames></author></authors><title>Estimating the Prevalence of Deception in Online Review Communities</title><categories>cs.SI cs.CL cs.CY</categories><comments>10 pages, 4 figures, 3 tables, to appear at WWW 2012</comments><acm-class>I.2.7; J.4; K.4.1; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consumers' purchase decisions are increasingly influenced by user-generated
online reviews. Accordingly, there has been growing concern about the potential
for posting &quot;deceptive opinion spam&quot; -- fictitious reviews that have been
deliberately written to sound authentic, to deceive the reader. But while this
practice has received considerable public attention and concern, relatively
little is known about the actual prevalence, or rate, of deception in online
review communities, and less still about the factors that influence it.
  We propose a generative model of deception which, in conjunction with a
deception classifier, we use to explore the prevalence of deception in six
popular online review communities: Expedia, Hotels.com, Orbitz, Priceline,
TripAdvisor, and Yelp. We additionally propose a theoretical model of online
reviews based on economic signaling theory, in which consumer reviews diminish
the inherent information asymmetry between consumers and producers, by acting
as a signal to a product's true, unknown quality. We find that deceptive
opinion spam is a growing problem overall, but with different growth rates
across communities. These rates, we argue, are driven by the different
signaling costs associated with deception for each review community, e.g.,
posting requirements. When measures are taken to increase signaling cost, e.g.,
filtering reviews written by first-time reviewers, deception prevalence is
effectively reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2809</identifier>
 <datestamp>2012-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2809</id><created>2012-04-12</created><authors><author><keyname>Alipour</keyname><forenames>Mehdi</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa E.</forenames></author></authors><title>Performance-Optimum Superscalar Architecture for Embedded Applications</title><categories>cs.AR</categories><journal-ref>http://indianjournals.com/ijor.aspx 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedded applications are widely used in portable devices such as wireless
phones, personal digital assistants, laptops, etc. High throughput and real
time requirements are especially important in such data-intensive tasks.
Therefore, architectures that provide the required performance are the most
desirable. On the other hand, processor performance is severely related to the
average memory access delay, number of processor registers and also size of the
instruction window and superscalar parameters. Therefore, cache, register file
and superscalar parameters are the major architectural concerns in designing a
superscalar architecture for embedded processors. Although increasing cache and
register file size leads to performance improvements in high performance
embedded processors, the increased area, power consumption and memory delay are
the overheads of these techniques. This paper explores the effect of cache,
register file and superscalar parameters on the processor performance to
specify the optimum size of these parameters for embedded applications.
Experimental results show that although having bigger size of these parameters
is one of the performance improvement approaches in embedded processors,
however, by increasing the size of some parameters over a threshold value,
performance improvement is saturated and especially in cache size, increments
over this threshold value decrease the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2837</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2837</id><created>2012-04-12</created><authors><author><keyname>Meyer</keyname><forenames>Fernand</forenames></author></authors><title>Watersheds, waterfalls, on edge or node weighted graphs</title><categories>cs.CV cs.DM</categories><msc-class>68U10, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algebraic approach to the watershed adapted to edge or node
weighted graphs. Starting with the flooding adjunction, we introduce the
flooding graphs, for which node and edge weights may be deduced one from the
other. Each node weighted or edge weighted graph may be transformed in a
flooding graph, showing that there is no superiority in using one or the other,
both being equivalent. We then introduce pruning operators extract subgraphs of
increasing steepness. For an increasing steepness, the number of never
ascending paths becomes smaller and smaller. This reduces the watershed zone,
where catchment basins overlap. A last pruning operator called scissor
associates to each node outside the regional minima one and only one edge. The
catchment basins of this new graph do not overlap and form a watershed
partition. Again, with an increasing steepness, the number of distinct
watershed partitions contained in a graph becomes smaller and smaller.
Ultimately, for natural image, an infinite steepness leads to a unique
solution, as it is not likely that two absolutely identical non ascending paths
of infinite steepness connect a node with two distinct minima. It happens that
non ascending paths of a given steepness are the geodesics of lexicographic
distance functions of a given depth. This permits to extract the watershed
partitions as skeletons by zone of influence of the minima for such
lexicographic distances. The waterfall hierarchy is obtained by a sequence of
operations. The first constructs the minimum spanning forest which spans an
initial watershed partition. The contraction of the trees into one node
produces a reduced graph which may be submitted to the same treatment. The
process is iterated until only one region remains. The union of the edges of
all forests produced constitutes a minimum spanning tree of the initial graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2844</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2844</id><created>2012-04-12</created><authors><author><keyname>Chuzhoy</keyname><forenames>Julia</forenames></author></authors><title>On Vertex Sparsifiers with Steiner Nodes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G=(V,E)$ with edge capacities $c_e\geq 1$ for
$e\in E$ and a subset $T$ of $k$ vertices called terminals, we say that a graph
$H$ is a quality-$q$ cut sparsifier for $G$ iff $T\subseteq V(H)$, and for any
partition $(A,B)$ of $T$, the values of the minimum cuts separating $A$ and $B$
in graphs $G$ and $H$ are within a factor $q$ from each other. We say that $H$
is a quality-$q$ flow sparsifier for $G$ iff $T\subseteq V(H)$, and for any set
$D$ of demands over the terminals, the values of the minimum edge congestion
incurred by fractionally routing the demands in $D$ in graphs $G$ and $H$ are
within a factor $q$ from each other.
  So far vertex sparsifiers have been studied in a restricted setting where the
sparsifier $H$ is not allowed to contain any non-terminal vertices, that is
$V(H)=T$. For this setting, efficient algorithms are known for constructing
quality-$O(\log k/\log\log k)$ cut and flow vertex sparsifiers, as well as a
lower bound of $\tilde{\Omega}(\sqrt{\log k})$ on the quality of any flow or
cut sparsifier.
  We study flow and cut sparsifiers in the more general setting where Steiner
vertices are allowed, that is, we no longer require that $V(H)=T$. We show
algorithms to construct constant-quality cut sparsifiers of size $O(C^3)$ in
time $\poly(n)\cdot 2^C$, and constant-quality flow sparsifiers of size
$C^{O(\log\log C)}$ in time $n^{O(\log C)}\cdot 2^C$, where $C$ is the total
capacity of the edges incident on the terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2847</identifier>
 <datestamp>2012-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2847</id><created>2012-04-12</created><updated>2012-05-19</updated><authors><author><keyname>Fournier</keyname><forenames>Chris</forenames></author><author><keyname>Inkpen</keyname><forenames>Diana</forenames></author></authors><title>Segmentation Similarity and Agreement</title><categories>cs.CL</categories><comments>10 pages, LaTeX, corrected a typo in equation 4</comments><journal-ref>Proceedings of the 2012 Conference of the North American Chapter
  of the Association for Computational Linguistics: Human Language
  Technologies, pages 152-161, Montr\'eal, Canada, June 3-8, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new segmentation evaluation metric, called segmentation
similarity (S), that quantifies the similarity between two segmentations as the
proportion of boundaries that are not transformed when comparing them using
edit distance, essentially using edit distance as a penalty function and
scaling penalties by segmentation size. We propose several adapted
inter-annotator agreement coefficients which use S that are suitable for
segmentation. We show that S is configurable enough to suit a wide variety of
segmentation evaluations, and is an improvement upon the state of the art. We
also propose using inter-annotator agreement coefficients to evaluate automatic
segmenters in terms of human performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2854</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2854</id><created>2012-04-12</created><authors><author><keyname>Katti</keyname><forenames>Rajendra S.</forenames></author><author><keyname>Ababei</keyname><forenames>Cristinel</forenames></author></authors><title>Secure Comparison Without Explicit XOR</title><categories>cs.CR</categories><comments>Ninth European Dependable Computing Conference, Sibiu, Romania</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient protocol for secure comparison of integers when both
integers are shared between two parties. Such protocols are useful for
implementing secure auctions. The proposed protocol's computational complexity
is roughly half the complexity of the best known efficient protocol. The
efficiency of the proposed protocol stems from the removal of the XOR
computation which is a time consuming operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2857</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2857</id><created>2012-04-12</created><authors><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Saha</keyname><forenames>Indranil</forenames></author><author><keyname>Zamani</keyname><forenames>Majid</forenames></author></authors><title>Synthesis of Minimal Error Control Software</title><categories>cs.SY cs.SC</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software implementations of controllers for physical systems are at the core
of many embedded systems. The design of controllers uses the theory of
dynamical systems to construct a mathematical control law that ensures that the
controlled system has certain properties, such as asymptotic convergence to an
equilibrium point, while optimizing some performance criteria. However, owing
to quantization errors arising from the use of fixed-point arithmetic, the
implementation of this control law can only guarantee practical stability:
under the actions of the implementation, the trajectories of the controlled
system converge to a bounded set around the equilibrium point, and the size of
the bounded set is proportional to the error in the implementation. The problem
of verifying whether a controller implementation achieves practical stability
for a given bounded set has been studied before. In this paper, we change the
emphasis from verification to automatic synthesis. Using synthesis, the need
for formal verification can be considerably reduced thereby reducing the design
time as well as design cost of embedded control software.
  We give a methodology and a tool to synthesize embedded control software that
is Pareto optimal w.r.t. both performance criteria and practical stability
regions. Our technique is a combination of static analysis to estimate
quantization errors for specific controller implementations and stochastic
local search over the space of possible controllers using particle swarm
optimization. The effectiveness of our technique is illustrated using examples
of various standard control systems: in most examples, we achieve controllers
with close LQR-LQG performance but with implementation errors, hence regions of
practical stability, several times as small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2879</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2879</id><created>2012-04-13</created><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Naskar</keyname><forenames>Mrinal K.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Amitava</forenames></author></authors><title>Adaptive Framework for Data Distribution in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>Accepted in the International Congress for Global Science and
  Technology (ICGST) in the Journal of Computer Network and Internet Research
  (CNIR), 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In recent years, the wireless sensor network (WSN) is playing a key role in
sensing, collecting and disseminating information in various applications. An
important feature associated with WSN is to develop an efficient data
distribution and routing scheme to ensure better quality of service (QoS) that
reduces the power consumption and the end-to-end data delivery time. In this
work, we propose an adaptive framework to transmit data packets from a source
to the sink in WSN across multiples paths with strategically distributed data
packets so as to minimize the power consumption as well as the end-to-end data
delivery time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2880</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2880</id><created>2012-04-13</created><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Saha</keyname><forenames>Amrita</forenames></author><author><keyname>Naskar</keyname><forenames>Mrinal K.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Amitava</forenames></author></authors><title>Multisource Adaptive Data Distribution and Routing in Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>Accepted in The Second International Conference on Networks &amp;
  Communications (NetCoM - 2.0), 2010, Bangalore, India</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The wireless sensor network is a collection of energy-constrained nodes.
Their objective is to sense, collect and process information for some ad-hoc
purpose. Typically the nodes are deployed in geographically inaccessible
regions. Thus the most challenging task is to design a network with minimal
power consumption. As the nodes have to collect and process data very fast,
minimizing data delivery time is another objective. In addition to this, when
multiple sources transmit data simultaneously, the network load gradually
increases and it may lead to congestion. In this paper we have proposed an
adaptive framework in which multiple sources transmit data simultaneously with
minimal end-to-end data delivery time and minimal energy consumption besides
ensuring that congestion remains at an optimum low so that minimal number of
data packets are dropped. This paper presents an adaptive framework to achieve
the above-mentioned objectives. This framework has been used over Mac 802.11
and extensive simulations have been carried out in NS2 to prove the
effectiveness of the framework over traditional Mac as well as few other
existing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2882</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2882</id><created>2012-04-13</created><authors><author><keyname>Mukherjee</keyname><forenames>Subhabrata</forenames></author><author><keyname>Seetharam</keyname><forenames>Anand</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Abhishek</forenames></author><author><keyname>Naskar</keyname><forenames>Mrinal. K.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Amitava</forenames></author></authors><title>Designing an Energy Efficient Framework for Data Gathering in Wireless
  Sensor Network</title><categories>cs.NI</categories><comments>Accepted in the International Congress for Global Science and
  Technology (ICGST) in the Journal of Computer Network and Internet Research
  (CNIR), 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless sensor network (WSN) is a collection of nodes which can communicate
with each other without any prior infrastructure along with the ability to
collect data autonomously and effectively after being deployed in an ad-hoc
fashion to monitor a given area. One major problem encountered in data
gathering wireless systems is to obtain an optimal balance among the number of
nodes deployed, energy efficiency and lifetime as energy of nodes cannot be
replenished. In this paper we propose first a scheme to estimate the number of
nodes to be deployed in a WSN for a predetermined lifetime so that total energy
utilization and complete connectivity are ensured under all circumstances. This
scheme also guarantees that during each data gathering cycle, every node
dissipates the requisite amount of energy, which thus minimizes the number of
nodes required to achieve the desired network lifetime. Second, this paper has
proposed a framework to conduct data gathering in WSN. Extensive simulations
have been carried out in ns2 to establish the effectiveness of this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2903</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2903</id><created>2012-04-13</created><updated>2015-06-19</updated><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Disconnectivity and Relative Positions in Simultaneous Embeddings</title><categories>cs.DS cs.DM</categories><comments>34 pages, 8 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem Simultaneous Embedding with Fixed Edges (SEFE) asks for two
planar graph $G^1 = (V^1, E^1)$ and $G^2 = (V^2, E^2)$ sharing a common
subgraph $G = G^1 \cap G^2$ whether they admit planar drawings such that the
common graph is drawn the same in both. Previous results on this problem
require $G$, $G^1$ and $G^2$ to be connected. This paper is a first step
towards solving instances where these graphs are disconnected.
  First, we show that an instance of the general SEFE-problem can be reduced in
linear time to an equivalent instance where $V^1 = V^2$ and $G^1$ and $G^2$ are
connected. This shows that it can be assumed without loss of generality that
both input graphs are connected. Second, we consider instances where $G$ is
disconnected. We show that SEFE can be solved in linear time if $G$ is a family
of disjoint cycles by introducing the CC-tree, which represents all
simultaneous embeddings. We extend these results (including the CC-tree) to the
case where $G$ consists of arbitrary connected components, each with a fixed
embedding.
  Note that previous results require $G$ to be connected and thus do not need
to care about relative positions of connected components. By contrast, we
assume the embedding of each connected component to be fixed and thus focus on
these relative positions. As SEFE requires to deal with both, embeddings of
connected components and their relative positions, this complements previous
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2912</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2912</id><created>2012-04-13</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Non-sparse Linear Representations for Visual Tracking with Online
  Reservoir Metric Learning</title><categories>cs.CV</categories><comments>Appearing in IEEE Conf. Computer Vision and Pattern Recognition, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most sparse linear representation-based trackers need to solve a
computationally expensive L1-regularized optimization problem. To address this
problem, we propose a visual tracker based on non-sparse linear
representations, which admit an efficient closed-form solution without
sacrificing accuracy. Moreover, in order to capture the correlation information
between different feature dimensions, we learn a Mahalanobis distance metric in
an online fashion and incorporate the learned metric into the optimization
problem for obtaining the linear representation. We show that online metric
learning using proximity comparison significantly improves the robustness of
the tracking, especially on those sequences exhibiting drastic appearance
changes. Furthermore, in order to prevent the unbounded growth in the number of
training samples for the metric learning, we design a time-weighted reservoir
sampling method to maintain and update limited-sized foreground and background
sample buffers for balancing sample diversity and adaptability. Experimental
results on challenging videos demonstrate the effectiveness and robustness of
the proposed tracker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2915</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2915</id><created>2012-04-13</created><authors><author><keyname>Jel&#xed;nek</keyname><forenames>V&#xed;t</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>A Kuratowski-Type Theorem for Planarity of Partially Embedded Graphs</title><categories>cs.DM</categories><comments>45 pages, 18 figures</comments><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partially embedded graph (or PEG) is a triple (G,H,\H), where G is a graph,
H is a subgraph of G, and \H is a planar embedding of H. We say that a PEG
(G,H,\H) is planar if the graph G has a planar embedding that extends the
embedding \H.
  We introduce a containment relation of PEGs analogous to graph minor
containment, and characterize the minimal non-planar PEGs with respect to this
relation. We show that all the minimal non-planar PEGs except for finitely many
belong to a single easily recognizable and explicitly described infinite
family. We also describe a more complicated containment relation which only has
a finite number of minimal non-planar PEGs.
  Furthermore, by extending an existing planarity test for PEGs, we obtain a
polynomial-time algorithm which, for a given PEG, either produces a planar
embedding or identifies an obstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2922</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2922</id><created>2012-04-13</created><authors><author><keyname>Salimi</keyname><forenames>Somayeh</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Secret Key Agreement Using Correlated Sources over the Generalized
  Multiple Access Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A secret key agreement setup between three users is considered in which each
of the users 1 and 2 intends to share a secret key with user 3 and users 1 and
2 are eavesdroppers with respect to each other. The three users observe i.i.d.
outputs of correlated sources and there is a generalized discrete memoryless
multiple access channel (GDMMAC) from users 1 and 2 to user 3 for communication
between the users. The secret key agreement is established using the correlated
sources and the GDMMAC. In this setup, inner and outer bounds of the secret key
capacity region are investigated. Moreover, for a special case where the
channel inputs and outputs and the sources form Markov chains in some order,
the secret key capacity region is derived. Also a Gaussian case is considered
in this setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2927</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2927</id><created>2012-04-13</created><updated>2012-08-20</updated><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author></authors><title>Diversity versus Channel Knowledge at Finite Block-Length</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in Information Theory Workshop (ITW) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the maximal achievable rate R*(n, \epsilon) for a given block-length
n and block error probability \epsilon over Rayleigh block-fading channels in
the noncoherent setting and in the finite block-length regime. Our results show
that for a given block-length and error probability, R*(n, \epsilon) is not
monotonic in the channel's coherence time, but there exists a rate maximizing
coherence time that optimally trades between diversity and cost of estimating
the channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2932</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2932</id><created>2012-04-13</created><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Gaiser</keyname><forenames>Andreas</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author></authors><title>Proving Termination of Probabilistic Programs Using Patterns</title><categories>cs.LO</categories><comments>A shorter version of the paper appeared in the Proceedings of
  Computer Aided Verification (CAV) 2012</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proving programs terminating is a fundamental computer science challenge.
Recent research has produced powerful tools that can check a wide range of
programs for termination. The analog for probabilistic programs, namely
termination with probability one (&quot;almost-sure termination&quot;), is an equally
important property for randomized algorithms and probabilistic protocols. We
suggest a novel algorithm for proving almost-sure termination of probabilistic
programs. Our algorithm exploits the power of state-of-the-art model checkers
and termination provers for nonprobabilistic programs: it calls such tools
within a refinement loop and thereby iteratively constructs a &quot;terminating
pattern&quot;, which is a set of terminating runs with probability one. We report on
various case studies illustrating the effectiveness of our algorithm. As a
further application, our algorithm can improve lower bounds on reachability
probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2933</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2933</id><created>2012-04-13</created><authors><author><keyname>Fung</keyname><forenames>Stanley P. Y.</forenames></author><author><keyname>Poon</keyname><forenames>Chung Keung</forenames></author><author><keyname>Zheng</keyname><forenames>Feifeng</forenames></author></authors><title>Improved Randomized Online Scheduling of Intervals and Jobs</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the online preemptive scheduling of intervals and jobs (with
restarts). Each interval or job has an arrival time, a deadline, a length and a
weight. The objective is to maximize the total weight of completed intervals or
jobs. While the deterministic case for intervals was settled a long time ago,
the randomized case remains open. In this paper we first give a 2-competitive
randomized algorithm for the case of equal length intervals. The algorithm is
barely random in the sense that it randomly chooses between two deterministic
algorithms at the beginning and then sticks with it thereafter. Then we extend
the algorithm to cover several other cases of interval scheduling including
monotone instances, C-benevolent instances and D-benevolent instances, giving
the same competitive ratio. These algorithms are surprisingly simple but have
the best competitive ratio against all previous (fully or barely) randomized
algorithms. Next we extend the idea to give a 3-competitive algorithm for equal
length jobs. Finally, we prove a lower bound of 2 on the competitive ratio of
all barely random algorithms that choose between two deterministic algorithms
for scheduling equal length intervals (and hence jobs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2942</identifier>
 <datestamp>2014-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2942</id><created>2012-04-13</created><updated>2014-09-25</updated><authors><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author><author><keyname>Friedman</keyname><forenames>Eric J.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>An Equilibrium Analysis of Scrip Systems</title><categories>cs.GT</categories><comments>32 pages. Forthcoming in ACM Transactions on Economics and
  Computation. Preliminary versions of this material appeared in the
  Proceedings of the 7th and 8th ACM Conferences on Electronic Commerce [14,
  26] and the Proceedings of the First Conference on Auctions, Market
  Mechanisms and Their Applications [27]. These are also available as
  arXiv:0705.4094, arXiv:0705.4110, and arXiv:0903.2278</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A game-theoretic model of scrip (artificial currency) systems is analyzed. It
is shown that relative entropy can be used to characterize the distribution of
agent wealth when all agents use threshold strategies---that is, they volunteer
to do work iff they have below a threshold amount of money. Monotonicity of
agents' best-reply functions is used to show that scrip systems have pure
strategy equilibria where all agents use threshold strategies. An algorithm is
given that can compute such an equilibrium and the resulting distribution of
wealth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2955</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2955</id><created>2012-04-13</created><updated>2012-04-28</updated><authors><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Tang</keyname><forenames>Jijun</forenames></author></authors><title>Label-Guided Graph Exploration with Adjustable Ratio of Labels</title><categories>cs.DS</categories><comments>20 pages, 7 figures. Accepted by International Journal of Foundations
  of Computer Science</comments><report-no>MengZhang-2012-J-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph exploration problem is to visit all the nodes of a connected graph
by a mobile entity, e.g., a robot. The robot has no a priori knowledge of the
topology of the graph or of its size. Cohen et al. \cite{Ilcinkas08} introduced
label guided graph exploration which allows the system designer to add short
labels to the graph nodes in a preprocessing stage; these labels can guide the
robot in the exploration of the graph. In this paper, we address the problem of
adjustable 1-bit label guided graph exploration. We focus on the labeling
schemes that not only enable a robot to explore the graph but also allow the
system designer to adjust the ratio of the number of different labels. This
flexibility is necessary when maintaining different labels may have different
costs or when the ratio is pre-specified. We present 1-bit labeling (two
colors, namely black and white) schemes for this problem along with a labeling
algorithm for generating the required labels. Given an $n$-node graph and a
rational number $\rho$, we can design a 1-bit labeling scheme such that
$n/b\geq \rho$ where $b$ is the number of nodes labeled black. The robot uses
$O(\rho\log\Delta)$ bits of memory for exploring all graphs of maximum degree
$\Delta$. The exploration is completed in time
$O(n\Delta^{\frac{16\rho+7}{3}}/\rho+\Delta^{\frac{40\rho+10}{3}})$. Moreover,
our labeling scheme can work on graphs containing loops and multiple edges,
while that of Cohen et al. focuses on simple graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2974</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2974</id><created>2012-04-13</created><authors><author><keyname>Breitner</keyname><forenames>Joachim</forenames></author></authors><title>Tackling the testing migration problem with SAT-Solvers</title><categories>cs.SE</categories><comments>13 pages</comments><acm-class>D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that it is feasible to formulate the testing migration problem as a
practically solvable PMAX-SAT instance, when package dependencies and conflicts
are pre-processed sensibly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2980</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2980</id><created>2012-04-13</created><authors><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Kourtellaris</keyname><forenames>Christos K.</forenames></author></authors><title>Realizable Rate Distortion Function and Bayesian FIltering Theory</title><categories>cs.IT math.FA math.IT math.PR</categories><comments>5 pages, 3 figures, 1 table, 1 graph, submitted to Information Theory
  Workshop 2012</comments><msc-class>28A33</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The relation between rate distortion function (RDF) and Bayesian filtering
theory is discussed. The relation is established by imposing a causal or
realizability constraint on the reconstruction conditional distribution of the
RDF, leading to the definition of a causal RDF. Existence of the optimal
reconstruction distribution of the causal RDF is shown using the topology of
weak convergence of probability measures. The optimal non-stationary causal
reproduction conditional distribution of the causal RDF is derived in closed
form; it is given by a set of recursive equations which are computed backward
in time. The realization of causal RDF is described via the source-channel
matching approach, while an example is briefly discussed to illustrate the
concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2983</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2983</id><created>2012-04-13</created><authors><author><keyname>Dantchev</keyname><forenames>Stefan</forenames></author><author><keyname>Martin</keyname><forenames>Barnaby</forenames></author></authors><title>Parameterized Resolution with bounded conjunction</title><categories>cs.LO</categories><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide separations between the parameterized versions of Res(1)
(Resolution) and Res(2). Using a different set of parameterized contradictions,
we also separate the parameterized versions of Res*(1) (tree-Resolution) and
Res*(2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2989</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2989</id><created>2012-04-12</created><authors><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author></authors><title>STP/HAMPI and Computer Security</title><categories>cs.CR cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past several years I have written two SMT solvers called STP and HAMPI
that have found widespread use in computer security research by leading groups
in academia, industry and the government. In this brief note I summarize the
features of STP/HAMPI that make them particularly suited for computer security
research, and a listing of some of the more important projects that use them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2990</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2990</id><created>2012-04-12</created><authors><author><keyname>Echenim</keyname><forenames>Mnacho</forenames></author><author><keyname>Peltier</keyname><forenames>Nicolas</forenames></author></authors><title>Reasoning on Schemata of Formulae</title><categories>cs.LO</categories><msc-class>68T15, 03B35</msc-class><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A logic is presented for reasoning on iterated sequences of formulae over
some given base language. The considered sequences, or &quot;schemata&quot;, are defined
inductively, on some algebraic structure (for instance the natural numbers, the
lists, the trees etc.). A proof procedure is proposed to relate the
satisfiability problem for schemata to that of finite disjunctions of base
formulae. It is shown that this procedure is sound, complete and terminating,
hence the basic computational properties of the base language can be carried
over to schemata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2991</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2991</id><created>2012-04-11</created><updated>2012-08-25</updated><authors><author><keyname>Malone</keyname><forenames>Thomas W.</forenames></author><author><keyname>von Ahn</keyname><forenames>Luis</forenames></author></authors><title>Collective Intelligence 2012: Proceedings</title><categories>cs.SI</categories><comments>Papers presented at CI2012. Proceedings chair Michael Bernstein</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume holds the proceedings of the Collective Intelligence 2012
conference in Cambridge, Massachusetts. It contains the full papers, poster
papers, and plenary abstracts.
  Collective intelligence has existed at least as long as humans have, because
families, armies, countries, and companies have all - at least sometimes -
acted collectively in ways that seem intelligent. But in the last decade or so
a new kind of collective intelligence has emerged: groups of people and
computers, connected by the Internet, collectively doing intelligent things.
For example, Google technology harvests knowledge generated by millions of
people creating and linking web pages and then uses this knowledge to answer
queries in ways that often seem amazingly intelligent. Or in Wikipedia,
thousands of people around the world have collectively created a very large and
high quality intellectual product with almost no centralized control, and
almost all as volunteers! These early examples of Internet-enabled collective
intelligence are not the end of the story but just the beginning. And in order
to understand the possibilities and constraints of these new kinds of
intelligence, we need a new interdisciplinary field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2994</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2994</id><created>2012-04-13</created><authors><author><keyname>Chakrabarti</keyname><forenames>Ayan</forenames></author><author><keyname>Zickler</keyname><forenames>Todd</forenames></author></authors><title>Image Restoration with Signal-dependent Camera Noise</title><categories>cs.CV stat.AP</categories><comments>6 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes a fast iterative algorithm for image denoising and
deconvolution with signal-dependent observation noise. We use an optimization
strategy based on variable splitting that adapts traditional Gaussian
noise-based restoration algorithms to account for the observed image being
corrupted by mixed Poisson-Gaussian noise and quantization errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.2995</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.2995</id><created>2012-04-13</created><authors><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author><author><keyname>Karger</keyname><forenames>David R.</forenames></author><author><keyname>Miller</keyname><forenames>Robert C.</forenames></author><author><keyname>Brandt</keyname><forenames>Joel</forenames></author></authors><title>Analytic Methods for Optimizing Realtime Crowdsourcing</title><categories>cs.SI cs.HC physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012</comments><report-no>CollectiveIntelligence/2012/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Realtime crowdsourcing research has demonstrated that it is possible to
recruit paid crowds within seconds by managing a small, fast-reacting worker
pool. Realtime crowds enable crowd-powered systems that respond at interactive
speeds: for example, cameras, robots and instant opinion polls. So far, these
techniques have mainly been proof-of-concept prototypes: research has not yet
attempted to understand how they might work at large scale or optimize their
cost/performance trade-offs. In this paper, we use queueing theory to analyze
the retainer model for realtime crowdsourcing, in particular its expected wait
time and cost to requesters. We provide an algorithm that allows requesters to
minimize their cost subject to performance requirements. We then propose and
analyze three techniques to improve performance: push notifications, shared
retainer pools, and precruitment, which involves recalling retainer workers
before a task actually arrives. An experimental validation finds that
precruited workers begin a task 500 milliseconds after it is posted, delivering
results below the one-second cognitive threshold for an end-user to stay in
flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3005</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3005</id><created>2012-04-13</created><authors><author><keyname>Jouini</keyname><forenames>Wassim</forenames></author><author><keyname>Di Felice</keyname><forenames>Marco</forenames></author><author><keyname>Bononi</keyname><forenames>Luciano</forenames></author><author><keyname>Moy</keyname><forenames>Christophe</forenames></author></authors><title>Collaboration and Coordination in Secondary Networks for Opportunistic
  Spectrum Access</title><categories>stat.AP cs.NI stat.ML</categories><comments>28 pages. Paper submitted to a journal</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we address the general case of a coordinated secondary network
willing to exploit communication opportunities left vacant by a licensed
primary network. Since secondary users (SU) usually have no prior knowledge on
the environment, they need to learn the availability of each channel through
sensing techniques, which however can be prone to detection errors. We argue
that cooperation among secondary users can enable efficient learning and
coordination mechanisms in order to maximize the spectrum exploitation by SUs,
while minimizing the impact on the primary network. To this goal, we provide
three novel contributions in this paper. First, we formulate the spectrum
selection in secondary networks as an instance of the Multi-Armed Bandit (MAB)
problem, and we extend the analysis to the collaboration learning case, in
which each SU learns the spectrum occupation, and shares this information with
other SUs. We show that collaboration among SUs can mitigate the impact of
sensing errors on system performance, and improve the convergence of the
learning process to the optimal solution. Second, we integrate the learning
algorithms with two collaboration techniques based on modified versions of the
Hungarian algorithm and of the Round Robin algorithm that allows reducing the
interference among SUs. Third, we derive fundamental limits to the performance
of cooperative learning algorithms based on Upper Confidence Bound (UCB)
policies in a symmetric scenario where all SU have the same perception of the
quality of the resources. Extensive simulation results confirm the
effectiveness of our joint learning-collaboration algorithm in protecting the
operations of Primary Users (PUs), while maximizing the performance of SUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3010</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3010</id><created>2012-04-13</created><authors><author><keyname>Schneider</keyname><forenames>Christian M.</forenames></author><author><keyname>Kesselring</keyname><forenames>Tobias A.</forenames></author><author><keyname>Andrade</keyname><forenames>Jose S.</forenames><suffix>Jr.</suffix></author><author><keyname>Herrmann</keyname><forenames>Hans J.</forenames></author></authors><title>Optimal box-covering algorithm for fractal dimension of complex networks</title><categories>physics.comp-ph cs.SI physics.soc-ph</categories><comments>5 pages, 6 figures</comments><doi>10.1103/PhysRevE.86.016707</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The self-similarity of complex networks is typically investigated through
computational algorithms the primary task of which is to cover the structure
with a minimal number of boxes. Here we introduce a box-covering algorithm that
not only outperforms previous ones, but also finds optimal solutions. For the
two benchmark cases tested, namely, the E. Coli and the WWW networks, our
results show that the improvement can be rather substantial, reaching up to 15%
in the case of the WWW network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3020</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3020</id><created>2012-04-12</created><authors><author><keyname>Grozin</keyname><forenames>Andrey</forenames></author></authors><title>TeXmacs-Reduce interface</title><categories>cs.MS</categories><comments>html exported from TeXmacs (alas, arXiv does not accept TeXmacs
  files)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This tutorial (based on the talk at the TeXmacs workshop in Faro, Portugal,
February 26 - March 2, 2012) describes the new and improved Reduce plugin in
GNU TeXmacs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3022</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3022</id><created>2012-04-13</created><updated>2013-11-11</updated><authors><author><keyname>Dawar</keyname><forenames>Anuj</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Kopczynski</keyname><forenames>Eryk</forenames><affiliation>Warsaw University</affiliation></author><author><keyname>Holm</keyname><forenames>Bjarki</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Gr&#xe4;del</keyname><forenames>Erich</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Pakusa</keyname><forenames>Wied</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Definability of linear equation systems over groups and rings</title><categories>cs.LO cs.CC</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 4 (November
  14, 2013) lmcs:725</journal-ref><doi>10.2168/LMCS-9(4:12)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the quest for a logic for PTIME and recent insights that the
descriptive complexity of problems from linear algebra is a crucial aspect of
this problem, we study the solvability of linear equation systems over finite
groups and rings from the viewpoint of logical (inter-)definability. All
problems that we consider are decidable in polynomial time, but not expressible
in fixed-point logic with counting. They also provide natural candidates for a
separation of polynomial time from rank logics, which extend fixed-point logics
by operators for determining the rank of definable matrices and which are
sufficient for solvability problems over fields. Based on the structure theory
of finite rings, we establish logical reductions among various solvability
problems. Our results indicate that all solvability problems for linear
equation systems that separate fixed-point logic with counting from PTIME can
be reduced to solvability over commutative rings. Moreover, we prove closure
properties for classes of queries that reduce to solvability over rings, which
provides normal forms for logics extended with solvability operators. We
conclude by studying the extent to which fixed-point logic with counting can
express problems in linear algebra over finite commutative rings, generalising
known results on the logical definability of linear-algebraic problems over
finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3040</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3040</id><created>2012-04-13</created><updated>2012-05-29</updated><authors><author><keyname>Pichler</keyname><forenames>Reinhard</forenames></author><author><keyname>R&#xfc;mmele</keyname><forenames>Stefan</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Tractable Answer-Set Programming with Weight Constraints: Bounded
  Treewidth is not Enough</title><categories>cs.LO cs.AI cs.CC</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><journal-ref>Theory and Practice of Logic Programming, Volume 14, Issue 02, pp
  141-164, 2014</journal-ref><doi>10.1017/S1471068412000099</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardinality constraints or, more generally, weight constraints are well
recognized as an important extension of answer-set programming. Clearly, all
common algorithmic tasks related to programs with cardinality or weight
constraints - like checking the consistency of a program - are intractable.
Many intractable problems in the area of knowledge representation and reasoning
have been shown to become linear time tractable if the treewidth of the
programs or formulas under consideration is bounded by some constant. The goal
of this paper is to apply the notion of treewidth to programs with cardinality
or weight constraints and to identify tractable fragments. It will turn out
that the straightforward application of treewidth to such class of programs
does not suffice to obtain tractability. However, by imposing further
restrictions, tractability can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3046</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3046</id><created>2012-04-13</created><updated>2012-04-30</updated><authors><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author></authors><title>The DoF Region of the Multiple-Antenna Time Correlated Interference
  Channel with Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>30 pages, 2 figures, with detailed proof of Theorem 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the time-correlated multiple-antenna interference channel where
the transmitters have (i) delayed channel state information (CSI) obtained from
a latency-prone feedback channel as well as (ii) imperfect current CSIT,
obtained e.g. from prediction on the basis of these past channel samples. We
derive the degrees of freedom (DoF) region for the two-user multiple-antenna
interference channel under such conditions. The proposed DoF achieving scheme
exploits a particular combination of the space-time alignment protocol designed
for fully outdated CSIT feedback channels (initially developed for the
broadcast channel by Maddah-Ali et al, later extended to the interference
channel by Vaze et al. and Ghasemi et al.) together with the use of simple
zero-forcing (ZF) precoders. The essential ingredient lies in the quantization
and feedback of the residual interference left after the application of the
initial imperfect ZF precoder. Our focus is on the MISO setting albeit
extensions to certain MIMO cases are also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3048</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3048</id><created>2012-04-13</created><authors><author><keyname>Huschenbett</keyname><forenames>Martin</forenames></author></authors><title>The Rank of Tree-Automatic Linear Orderings</title><categories>cs.LO math.LO</categories><comments>20 pages, 3 figures</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalise Delhomm\'e's result that each tree-automatic ordinal is
strictly below \omega^\omega^\omega{} by showing that any tree-automatic linear
ordering has FC-rank strictly below \omega^\omega. We further investigate a
restricted form of tree-automaticity and prove that every linear ordering which
admits a tree-automatic presentation of branching complexity at most k has
FC-rank strictly below \omega^k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3052</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3052</id><created>2012-04-13</created><authors><author><keyname>Raja</keyname><forenames>Chittampally Vasanth</forenames></author><author><keyname>Balasubramanian</keyname><forenames>Srinivas</forenames></author><author><keyname>Raghavendra</keyname><forenames>Prakash S</forenames></author></authors><title>Heterogeneous Highly Parallel Implementation of Matrix Exponentiation
  Using GPU</title><categories>cs.DC cs.MS cs.NA</categories><comments>15 pages, 12 figures, International Journal of Distributed and
  Parallel systems (IJDPS) ISSN : 0976 - 9757 [Online] ; 2229 - 3957 [Print]</comments><journal-ref>International Journal of Distributed and Parallel Systems, Vol 3,
  No. 2, March 2012 issue</journal-ref><doi>10.5121/ijdps.2012.3209</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The vision of super computer at every desk can be realized by powerful and
highly parallel CPUs or GPUs or APUs. Graphics processors once specialized for
the graphics applications only, are now used for the highly computational
intensive general purpose applications. Very expensive GFLOPs and TFLOP
performance has become very cheap with the GPGPUs. Current work focuses mainly
on the highly parallel implementation of Matrix Exponentiation. Matrix
Exponentiation is widely used in many areas of scientific community ranging
from highly critical flight, CAD simulations to financial, statistical
applications. Proposed solution for Matrix Exponentiation uses OpenCL for
exploiting the hyper parallelism offered by the many core GPGPUs. It employs
many general GPU optimizations and architectural specific optimizations. This
experimentation covers the optimizations targeted specific to the Scientific
Graphics cards (Tesla-C2050). Heterogeneous Highly Parallel Matrix
Exponentiation method has been tested for matrices of different sizes and with
different powers. The devised Kernel has shown 1000X speedup and 44 fold
speedup with the naive GPU Kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3057</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3057</id><created>2012-04-13</created><updated>2012-08-31</updated><authors><author><keyname>Randriambololona</keyname><forenames>Hugues</forenames></author></authors><title>Asymptotically good binary linear codes with asymptotically good
  self-intersection spans</title><categories>cs.IT math.CO math.IT</categories><comments>18 pages; v2-&gt;v3: expanded introduction and bibliography + various
  minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If C is a binary linear code, let C^2 be the linear code spanned by
intersections of pairs of codewords of C. We construct an asymptotically good
family of binary linear codes such that, for C ranging in this family, the C^2
also form an asymptotically good family. For this we use algebraic-geometry
codes, concatenation, and a fair amount of bilinear algebra.
  More precisely, the two main ingredients used in our construction are, first,
a description of the symmetric square of an odd degree extension field in terms
only of field operations of small degree, and second, a recent result of
Garcia-Stichtenoth-Bassa-Beelen on the number of points of curves on such an
odd degree extension field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3058</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3058</id><created>2012-04-13</created><authors><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames></author><author><keyname>Flocchini</keyname><forenames>Paola</forenames></author><author><keyname>Mans</keyname><forenames>Bernard</forenames></author><author><keyname>Santoro</keyname><forenames>Nicola</forenames></author></authors><title>Building Fastest Broadcast Trees in Periodically-Varying Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-tolerant networks (DTNs) are characterized by a possible absence of
end-to-end communication routes at any instant. Still, connectivity can
generally be established over time and space. The optimality of a temporal path
(journey) in this context can be defined in several terms, including
topological (e.g. {\em shortest} in hops) and temporal (e.g. {\em fastest,
foremost}). The combinatorial problem of computing shortest, foremost, and
fastest journeys {\em given full knowledge} of the network schedule was
addressed a decade ago (Bui-Xuan {\it et al.}, 2003). A recent line of research
has focused on the distributed version of this problem, where foremost,
shortest or fastest {\em broadcast} are performed without knowing the schedule
beforehand. In this paper we show how to build {\em fastest} broadcast trees
(i.e., trees that minimize the global duration of the broadcast, however late
the departure is) in Time-Varying Graphs where intermittent edges are available
periodically (it is known that the problem is infeasible in the general case
even if various parameters of the graph are know). We address the general case
where contacts between nodes can have arbitrary durations and thus fastest
routes may consist of a mixture of {\em continuous} and {\em discontinuous}
segments (a more complex scenario than when contacts are {\em punctual} and
thus routes are only discontinuous). Using the abstraction of \tclocks to
compute the temporal distances, we solve the fastest broadcast problem by first
learning, at the emitter, what is its time of {\em minimum temporal
eccentricity} (i.e. the fastest time to reach all the other nodes), and second
by building a {\em foremost} broadcast tree relative to this particular
emission date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3069</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3069</id><created>2012-04-13</created><authors><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author></authors><title>An Outer Bound for the Memoryless Two-user Interference Channel with
  General Cooperation</title><categories>cs.IT math.IT</categories><comments>a shorter version of this paper has been submitted to ITW 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interference channel models a wireless network where several
source-destination pairs compete for the same resources. When nodes transmit
simultaneously the destinations experience interference. This paper considers a
4-node network, where two nodes are sources and the other two are destinations.
All nodes are full-duplex and cooperate to mitigate interference. A sum-rate
outer bound is derived, which is shown to unify a number of previously derived
outer bounds for special cases of cooperation. The approach is shown to extend
to cooperative interference networks with more than two source-destination
pairs and for any partial sum-rate. How the derived bound relates to similar
bounds for channel models including cognitive nodes, i.e., nodes that have
non-causal knowledge of the messages of some other node, is also discussed.
Finally, the bound is evaluated for the Gaussian noise channel and used to
compare different modes of cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3074</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3074</id><created>2012-04-13</created><updated>2015-07-13</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Ning</forenames></author></authors><title>Time-Critical Influence Maximization in Social Networks with
  Time-Delayed Diffusion Process</title><categories>cs.SI physics.soc-ph</categories><comments>26 pages, 9 figures. Conference version appears in the proceedings of
  AAAI 2012. This new version includes Appendix B, on the modeling and
  computation of time-delayed influence propagation with login events</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization is a problem of finding a small set of highly
influential users, also known as seeds, in a social network such that the
spread of influence under certain propagation models is maximized. In this
paper, we consider time-critical influence maximization, in which one wants to
maximize influence spread within a given deadline. Since timing is considered
in the optimization, we also extend the Independent Cascade (IC) model and the
Linear Threshold (LT) model to incorporate the time delay aspect of influence
diffusion among individuals in social networks. We show that time-critical
influence maximization under the time-delayed IC and LT models maintains
desired properties such as submodularity, which allows a greedy approximation
algorithm to achieve an approximation ratio of $1-1/e$. To overcome the
inefficiency of the greedy algorithm, we design two heuristic algorithms: the
first one is based on a dynamic programming procedure that computes exact
influence in tree structures and directed acyclic subgraphs, while the second
one converts the problem to one in the original models and then applies
existing fast heuristic algorithms to it. Our simulation results demonstrate
that our algorithms achieve the same level of influence spread as the greedy
algorithm while running a few orders of magnitude faster, and they also
outperform existing fast heuristics that disregard the deadline constraint and
delays in diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3097</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3097</id><created>2012-04-13</created><authors><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>Technical Report: Observability of a Linear System under Sparsity
  Constraints</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an n-dimensional linear system where it is known that there are at
most k&lt;n non-zero components in the initial state. The observability problem,
that is the recovery of the initial state, for such a system is considered. We
obtain sufficient conditions on the number of the available observations to be
able to recover the initial state exactly for such a system. Both deterministic
and stochastic setups are considered for system dynamics. In the former
setting, the system matrices are known deterministically, whereas in the latter
setting, all of the matrices are picked from a randomized class of matrices.
The main message is that, one does not need to obtain full n observations to be
able to uniquely identify the initial state of the linear system, even when the
observations are picked randomly, when the initial condition is known to be
sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3100</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3100</id><created>2012-04-13</created><updated>2014-07-01</updated><authors><author><keyname>Demirel</keyname><forenames>Burak</forenames></author><author><keyname>Zou</keyname><forenames>Zhenhua</forenames></author><author><keyname>Soldati</keyname><forenames>Pablo</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Modular design of jointly optimal controllers and forwarding policies
  for wireless control</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the joint design of packet forwarding policies and controllers
for wireless control loops where sensor measurements are sent to the controller
over an unreliable and energy-constrained multi-hop wireless network. For fixed
sampling rate of the sensor, the co-design problem separates into two
well-defined and independent subproblems: transmission scheduling for
maximizing the deadline-constrained reliability and optimal control under
packet loss. We develop optimal and implementable solutions for these
subproblems and show that the optimally co-designed system can be efficiently
found. Numerical examples highlight the many trade-offs involved and
demonstrate the power of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3105</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3105</id><created>2012-04-13</created><authors><author><keyname>Luo</keyname><forenames>Yuancheng</forenames></author><author><keyname>Duraiswami</keyname><forenames>Ramani</forenames></author></authors><title>Alternative Tilings for the Fast Multipole Method on the Plane</title><categories>cs.NA cs.CG</categories><msc-class>41A58, 65D18, 68U05, 52C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fast multipole method (FMM) performs fast approximate kernel summation to
a specified tolerance $\epsilon$ by using a hierarchical division of the
domain, which groups source and receiver points into regions that satisfy local
separation and the well-separated pair decomposition properties. While square
tilings and quadtrees are commonly used in 2D, we investigate alternative
tilings and associated spatial data structures: regular hexagons (septree) and
triangles (triangle-quadtree). We show that both structures satisfy separation
properties for the FMM and prove their theoretical error bounds and
computational costs. Empirical runtime and error analysis of our
implementations are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3113</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3113</id><created>2012-04-13</created><authors><author><keyname>Ferreira</keyname><forenames>Carlos Eduardo</forenames></author><author><keyname>Franco</keyname><forenames>&#xc1;lvaro Junio Pereira</forenames></author></authors><title>Algorithms for Junctions in Directed Acyclic Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pair of distinct vertices u, v in a graph G, we say that s is a
junction of u, v if there are in G internally vertex disjoint directed paths
from s to u and from s to v. We show how to characterize junctions in directed
acyclic graphs. We also consider the two problems in the following and derive
efficient algorithms to solve them. Given a directed acyclic graph G and a
vertex s in G, how can we find all pairs of vertices of G such that s is a
junction of them? And given a directed acyclic graph G and k pairs of vertices
of G, how can we preprocess G such that all junctions of k given pairs of
vertices could be listed quickly? All junctions of k pairs problem arises in an
application in Anthropology and we apply our algorithm to find such junctions
on kinship networks of some brazilian indian ethnic groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3114</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3114</id><created>2012-04-13</created><updated>2013-02-07</updated><authors><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>On the Role of Mobility for Multi-message Gossip</title><categories>cs.SI cs.IT cs.NI math.IT</categories><comments>accepted to IEEE Transactions on Information Theory, 2013</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 59, No. 6, pp.
  3953-3970, 2013</journal-ref><doi>10.1109/TIT.2013.2247462</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider information dissemination in a large $n$-user wireless network in
which $k$ users wish to share a unique message with all other users. Each of
the $n$ users only has knowledge of its own contents and state information;
this corresponds to a one-sided push-only scenario. The goal is to disseminate
all messages efficiently, hopefully achieving an order-optimal spreading rate
over unicast wireless random networks. First, we show that a random-push
strategy -- where a user sends its own or a received packet at random -- is
order-wise suboptimal in a random geometric graph: specifically,
$\Omega(\sqrt{n})$ times slower than optimal spreading. It is known that this
gap can be closed if each user has &quot;full&quot; mobility, since this effectively
creates a complete graph. We instead consider velocity-constrained mobility
where at each time slot the user moves locally using a discrete random walk
with velocity $v(n)$ that is much lower than full mobility. We propose a simple
two-stage dissemination strategy that alternates between individual message
flooding (&quot;self promotion&quot;) and random gossiping. We prove that this scheme
achieves a close to optimal spreading rate (within only a logarithmic gap) as
long as the velocity is at least $v(n)=\omega(\sqrt{\log n/k})$. The key
insight is that the mixing property introduced by the partial mobility helps
users to spread in space within a relatively short period compared to the
optimal spreading time, which macroscopically mimics message dissemination over
a complete graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3141</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3141</id><created>2012-04-14</created><authors><author><keyname>Fard</keyname><forenames>Ali P.</forenames></author><author><keyname>Nabaee</keyname><forenames>Mahdy</forenames></author></authors><title>Secure Tracking in Sensor Networks using Adaptive Extended Kalman Filter</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location information of sensor nodes has become an essential part of many
applications in Wireless Sensor Networks (WSN). The importance of location
estimation and object tracking has made them the target of many security
attacks. Various methods have tried to provide location information with high
accuracy, while lots of them have neglected the fact that WSNs may be deployed
in hostile environments. In this paper, we address the problem of securely
tracking a Mobile Node (MN) which has been noticed very little previously. A
novel secure tracking algorithm is proposed based on Extended Kalman Filter
(EKF) that is capable of tracking a Mobile Node (MN) with high resolution in
the presence of compromised or colluding malicious beacon nodes. It filters out
and identifies the malicious beacon data in the process of tracking. The
proposed method considerably outperforms the previously proposed secure
algorithms in terms of either detection rate or MSE. The experimental data
based on different settings for the network has shown promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3158</identifier>
 <datestamp>2014-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3158</id><created>2012-04-14</created><updated>2014-04-30</updated><authors><author><keyname>Salikhmetov</keyname><forenames>Anton</forenames></author></authors><title>A recursive normalizing one-step reduction strategy for the distributive
  lambda calculus</title><categories>cs.LO</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We positively answer the question A.1.6 in J. Klop's &quot;Ustica Notes&quot;: &quot;Is
there a recursive normalizing one-step reduction strategy for micro
$\lambda$-calculus?&quot; Micro $\lambda$-calculus refers to an implementation of
the $\lambda$-calculus due to Revesz, implementing $\beta$-reduction by means
of &quot;micro steps&quot; recursively distributing a $\beta$-redex $(\lambda x.M)\ N$
over its body $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3164</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3164</id><created>2012-04-14</created><authors><author><keyname>Castro</keyname><forenames>Cornelia</forenames></author><author><keyname>Andrade</keyname><forenames>Antonio</forenames></author></authors><title>Teaching Chemistry in a Social Learning Environment: Facing Drivers and
  Barriers</title><categories>cs.OH</categories><comments>9 pages, 6 figures; Proceedings of ICERI2011 Conference; 14th-16th
  November 2011, Madrid, Spain</comments><msc-class>97C70</msc-class><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Portuguese Technological Plan for Education (TPE) was established to
modernize schools and to consolidate the role of Information and Communication
Technologies (ICT) in order to promote the academic success of students and
allow schools to be transformed into technological enriched environments
through a significant learning and knowledge building in a participatory,
collaborative and sharing logic. With this work we aimed to establish dynamical
interactions students-content- teacher in order to overcome a diagnosed
students' lack of effort towards studying curriculum chemistry content. Our
methodology design is a theoretical and descriptive one, carried out in a
secondary school during the 2009/2010 school year, in order to answer the
question &quot;How to improve the engagement of K-12 students in chemistry
classes?&quot;. Students, gathered in small groups were asked to create digital
learning resources (DLR) during classes. The teacher assumed the role of the
supervisor, coacher and facilitator of every task that had to be taken or
chosen by the students. To enhance interaction student-student and
student-teacher, a Twitter account and a Ning site were created for the class.
Both supported the Social Learning Environment (SLE) that was intended to be
created. The data collected led us to satisfactory results in what concerns the
goals of the study. The affordances and constraints of SLE as an open
architecture that has potential to facilitate collaborative learning are
delineated. Future work should focus on mechanisms that allow assessment both
of the methodology used and the students' generated content in order to improve
students' learning in this environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3167</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3167</id><created>2012-04-14</created><updated>2012-12-06</updated><authors><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>An Analytical Framework for Multi-Cell Cooperation via Stochastic
  Geometry and Large Deviations</title><categories>cs.IT math.IT</categories><comments>Double column, 16 pages, to appear in IEEE Transactions on
  Information Theory. This paper has been presented in part at IEEE Globecom
  2011 and IEEE Intl. Conf. on Communications 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-cell cooperation (MCC) is an approach for mitigating inter-cell
interference in dense cellular networks. Existing studies on MCC performance
typically rely on either over-simplified Wyner-type models or complex
system-level simulations. The promising theoretical results (typically using
Wyner models) seem to materialize neither in complex simulations nor in
practice. To more accurately investigate the theoretical performance of MCC,
this paper models an entire plane of interfering cells as a Poisson random
tessellation. The base stations (BSs) are then clustered using a regular
lattice, whereby BSs in the same cluster mitigate mutual interference by
beamforming with perfect channel state information. Techniques from stochastic
geometry and large deviation theory are applied to analyze the outage
probability as a function of the mobile locations, scattering environment, and
the average number of cooperating BSs per cluster, L. For mobiles near the
centers of BS clusters, it is shown that as L increases, outage probability
diminishes sub-exponentially if scattering is sparse, and following a power law
with an exponent proportional to the signal diversity order if scattering is
rich. For randomly located mobiles, regardless of scattering, outage
probability is shown to scale with increasing L following a power law with an
exponent no larger than 0.5. These results confirm analytically that
cluster-edge mobiles are the bottleneck for network coverage and provide a
plausible analytic framework for more realistic analysis of other multi-cell
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3180</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3180</id><created>2012-04-14</created><authors><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author><author><keyname>Le</keyname><forenames>Anh N.</forenames></author><author><keyname>Nguyen</keyname><forenames>Thanh-Nhan</forenames></author></authors><title>Analyzing Nonblocking Switching Networks using Linear Programming
  (Duality)</title><categories>cs.DM cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The main task in analyzing a switching network design (including circuit-,
multirate-, and photonic-switching) is to determine the minimum number of some
switching components so that the design is non-blocking in some sense (e.g.,
strict- or wide-sense). We show that, in many cases, this task can be
accomplished with a simple two-step strategy: (1) formulate a linear program
whose optimum value is a bound for the minimum number we are seeking, and (2)
specify a solution to the dual program, whose objective value by weak duality
immediately yields a sufficient condition for the design to be non-blocking.
  We illustrate this technique through a variety of examples, ranging from
circuit to multirate to photonic switching, from unicast to $f$-cast and
multicast, and from strict- to wide-sense non-blocking. The switching
architectures in the examples are of Clos-type and Banyan-type, which are the
two most popular architectural choices for designing non-blocking switching
networks.
  To prove the result in the multirate Clos network case, we formulate a new
problem called {\sc dynamic weighted edge coloring} which generalizes the {\sc
dynamic bin packing} problem. We then design an algorithm with competitive
ratio 5.6355 for the problem. The algorithm is analyzed using the linear
programming technique. A new upper-bound for multirate wide-sense non-blocking
Clos networks follow, improving upon a decade-old bound on the same problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3198</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3198</id><created>2012-04-14</created><updated>2012-09-28</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author><author><keyname>Hern&#xe1;ndez-Fern&#xe1;ndez</keyname><forenames>Antoni</forenames></author></authors><title>The failure of the law of brevity in two New World primates. Statistical
  caveats</title><categories>q-bio.NC cs.CL</categories><comments>Little improvements in the statistical arguments</comments><journal-ref>Statistical caveats. Glottotheory 4 (1), 45-55 (2013)</journal-ref><doi>10.1524/glot.2013.0004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallels of Zipf's law of brevity, the tendency of more frequent words to be
shorter, have been found in bottlenose dolphins and Formosan macaques. Although
these findings suggest that behavioral repertoires are shaped by a general
principle of compression, common marmosets and golden-backed uakaris do not
exhibit the law. However, we argue that the law may be impossible or difficult
to detect statistically in a given species if the repertoire is too small, a
problem that could be affecting golden backed uakaris, and show that the law is
present in a subset of the repertoire of common marmosets. We suggest that the
visibility of the law will depend on the subset of the repertoire under
consideration or the repertoire size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3200</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3200</id><created>2012-04-14</created><authors><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Bosch</keyname><forenames>Olav ten</forenames></author><author><keyname>Doorn</keyname><forenames>Peter</forenames></author></authors><title>Looking at a digital research data archive - Visual interfaces to EASY</title><categories>cs.DL physics.soc-ph</categories><comments>Submitted to the TPDL 2012</comments><msc-class>37E25</msc-class><acm-class>H.3.5, H.3.7</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we explore visually the structure of the collection of a
digital research data archive in terms of metadata for deposited datasets. We
look into the distribution of datasets over different scientific fields; the
role of main depositors (persons and institutions) in different fields, and
main access choices for the deposited datasets. We argue that visual analytics
of metadata of collections can be used in multiple ways: to inform the archive
about structure and growth of its collection; to foster collections strategies;
and to check metadata consistency. We combine visual analytics and visual
enhanced browsing introducing a set of web-based, interactive visual interfaces
to the archive's collection. We discuss how text based search combined with
visual enhanced browsing enhances data access, navigation, and reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3210</identifier>
 <datestamp>2013-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3210</id><created>2012-04-14</created><updated>2012-09-27</updated><authors><author><keyname>Delestre</keyname><forenames>Olivier</forenames><affiliation>JAD</affiliation></author><author><keyname>Cordier</keyname><forenames>St&#xe9;phane</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Darboux</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>USS</affiliation></author><author><keyname>Du</keyname><forenames>Mingxuan</forenames><affiliation>MAPMO</affiliation></author><author><keyname>James</keyname><forenames>Francois</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Laguerre</keyname><forenames>Christian</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Lucas</keyname><forenames>Carine</forenames><affiliation>MAPMO</affiliation></author><author><keyname>Planchon</keyname><forenames>Olivier</forenames><affiliation>IRD</affiliation></author></authors><title>FullSWOF: A software for overland flow simulation / FullSWOF : un
  logiciel pour la simulation du ruissellement</title><categories>math.NA cs.CE cs.NA math.AP</categories><comments>9 pages</comments><proxy>ccsd</proxy><journal-ref>Advances in Hydroinformatics - SIMHYDRO 2012 - New Frontiers of
  Simulation, Gourbesville, P.; Cunge, J. and Caignaert, G. (Ed.) (2014)
  221-231</journal-ref><doi>10.1007/978-981-4451-42-0_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overland flow on agricultural fields may have some undesirable effects such
as soil erosion, flood and pollutant transport. To better understand this
phenomenon and limit its consequences, we developed a code using
state-of-the-art numerical methods: FullSWOF (Full Shallow Water equations for
Overland Flow), an object oriented code written in C++. It has been made
open-source and can be downloaded from
http://www.univ-orleans.fr/mapmo/soft/FullSWOF/. The model is based on the
classical system of Shallow Water (SW) (or Saint-Venant system). Numerical
difficulties come from the numerous dry/wet transitions and the highly-variable
topography encountered inside a field. It includes runon and rainfall inputs,
infiltration (modified Green-Ampt equation), friction (Darcy-Weisbach and
Manning formulas). First we present the numerical method for the resolution of
the Shallow Water equations integrated in FullSWOF_2D (the two-dimension
version). This method is based on hydrostatic reconstruction scheme, coupled
with a semi-implicit friction term treatment. FullSWOF_2D has been previously
validated using analytical solutions from the SWASHES library (Shallow Water
Analytic Solutions for Hydraulic and Environmental Studies). Finally,
FullSWOF_2D is run on a real topography measured on a runoff plot located in
Thies (Senegal). Simulation results are compared with measured data. This
experimental benchmark demonstrate the capabilities of FullSWOF to simulate
adequately overland flow. FullSWOF could also be used for other environmental
issues, such as river floods and dam-breaks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3221</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3221</id><created>2012-04-14</created><authors><author><keyname>Lakhman</keyname><forenames>Konstantin</forenames></author><author><keyname>Burtsev</keyname><forenames>Mikhail</forenames></author></authors><title>Neuroevolution Results in Emergence of Short-Term Memory for
  Goal-Directed Behavior</title><categories>cs.NE cs.AI nlin.AO</categories><comments>Manuscript was submitted to the 12th International Conference on the
  Simulation of Adaptive Behavior 2012; 10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Animals behave adaptively in the environment with multiply competing goals.
Understanding of the mechanisms underlying such goal-directed behavior remains
a challenge for neuroscience as well for adaptive system research. To address
this problem we developed an evolutionary model of adaptive behavior in the
multigoal stochastic environment. Proposed neuroevolutionary algorithm is based
on neuron's duplication as a basic mechanism of agent's recurrent neural
network development. Results of simulation demonstrate that in the course of
evolution agents acquire the ability to store the short-term memory and,
therefore, use it in behavioral strategies with alternative actions. We found
that evolution discovered two mechanisms for short-term memory. The first
mechanism is integration of sensory signals and ongoing internal neural
activity, resulting in emergence of cell groups specialized on alternative
actions. And the second mechanism is slow neurodynamical processes that makes
possible to code the previous behavioral choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3223</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3223</id><created>2012-04-14</created><authors><author><keyname>Tlili</keyname><forenames>Oussama</forenames></author><author><keyname>Sassi</keyname><forenames>Minyar</forenames></author><author><keyname>Ounelli</keyname><forenames>Habib</forenames></author></authors><title>Intelligent Database Flexible Querying System by Approximate Query
  Processing</title><categories>cs.DB</categories><comments>8 pages, 5 figures, 9 tables</comments><journal-ref>The Third International Conference on Advances in Databases,
  Knowledge, and Data Applications (DBKDA 2011), January 23-28, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database flexible querying is an alternative to the classic one for users.
The use of Formal Concepts Analysis (FCA) makes it possible to make approximate
answers that those turned over by a classic DataBase Management System (DBMS).
Some applications do not need exact answers. However, flexible querying can be
expensive in response time. This time is more significant when the flexible
querying require the calculation of aggregate functions (&quot;Sum&quot;, &quot;Avg&quot;, &quot;Count&quot;,
&quot;Var&quot; etc.). In this paper, we propose an approach which tries to solve this
problem by using Approximate Query Processing (AQP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3224</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3224</id><created>2012-04-14</created><authors><author><keyname>Sassi</keyname><forenames>Minyar</forenames></author></authors><title>Towards Fuzzy-Hard Clustering Mapping Processes</title><categories>cs.OH</categories><comments>22 pages, 6 Tables</comments><journal-ref>Advances in Fuzzy Sets and Systems, Volume 9, Number 1, 2011, pp.
  37-63, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the validation step can appear crucial in the case of clustering
adopting fuzzy approaches, the problem of the partition validity obtained by
those adopting the hard ones was not tackled. To cure this problem, we propose
in this paper fuzzy-hard mapping processes of clustering while benefitting from
those adopting the fuzzy case. These mapping processes concern: (1) local and
global clustering evaluation measures: the first for the detection of the
&quot;worst&quot; clusters to merging or splitting them. The second relates to the
evaluation of the obtained partition for each iteration, (2) merging and
splitting processes taking into account the proposed measures, and (3)
automatic clustering algorithms implementing these new concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3230</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3230</id><created>2012-04-14</created><authors><author><keyname>Lovejoy</keyname><forenames>Kristen</forenames></author><author><keyname>Saxton</keyname><forenames>Gregory D.</forenames></author></authors><title>Information, Community, and Action: How Nonprofit Organizations Use
  Social Media</title><categories>cs.CY cs.SI</categories><journal-ref>Journal of Computer Mediated Communication, vol. 17, pp. 337-353,
  2012</journal-ref><doi>10.1111/j.1083-6101.2012.01576.x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid diffusion of &quot;microblogging&quot; services such as Twitter is ushering
in a new era of possibilities for organizations to communicate with and engage
their core stakeholders and the general public. To enhance understanding of the
communicative functions microblogging serves for organizations, this study
examines the Twitter utilization practices of the 100 largest nonprofit
organizations in the United States. The analysis reveals there are three key
functions of microblogging updates-&quot;information,&quot; &quot;community,&quot; and &quot;action.&quot;
Though the informational use of microblogging is extensive, nonprofit
organizations are better at using Twitter to strategically engage their
stakeholders via dialogic and community-building practices than they have been
with traditional websites. The adoption of social media appears to have
engendered new paradigms of public engagement.
  Keywords: microblogging; Twitter; social media; stakeholder relations;
organizational communication; organization-public relations; nonprofit
organizations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3236</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3236</id><created>2012-04-15</created><authors><author><keyname>Kochanski</keyname><forenames>Greg</forenames></author></authors><title>Using Mimicry to Learn about Mental Representations</title><categories>cs.SD q-bio.OT</categories><comments>36 pages, plus extra figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Phonology typically describes speech in terms of discrete signs like
features. The field of intonational phonology uses discrete accents to describe
intonation and prosody. But, are such representations useful? The results of
mimicry experiments indicate that discrete signs are not a useful
representation of the shape of intonation contours. Human behaviour seems to be
better represented by a attractors where memory retains substantial fine detail
about an utterance. There is no evidence that discrete abstract representations
that might be formed that have an effect on the speech that is subsequently
produced. This paper also discusses conditions under which a discrete phonology
can arise from an attractor model and why - for intonation - attractors can be
inferred without the implying a discrete phonology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3238</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3238</id><created>2012-04-15</created><authors><author><keyname>Yazdani</keyname><forenames>Raman</forenames></author><author><keyname>Ardakani</keyname><forenames>Masoud</forenames></author></authors><title>Reliable communication over non-binary insertion/deletion channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications (under 2nd round of
  review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reliable communication over non-binary
insertion/deletion channels where symbols are randomly deleted from or inserted
in the transmitted sequence and all symbols are corrupted by additive white
Gaussian noise. To this end, we utilize the inherent redundancy achievable in
non-binary symbol sets by first expanding the symbol set and then allocating
part of the bits associated with each symbol to watermark symbols. The
watermark sequence, known at the receiver, is then used by a forward-backward
algorithm to provide soft information for an outer code which decodes the
transmitted sequence. Through numerical results and discussions, we evaluate
the performance of the proposed solution and show that it leads to significant
system ability to detect and correct insertions/deletions. We also provide
estimates of the maximum achievable information rates of the system, compare
them with the available bounds, and construct practical codes capable of
approaching these limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3239</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3239</id><created>2012-04-15</created><authors><author><keyname>Bhakta</keyname><forenames>Prateek</forenames></author><author><keyname>Miracle</keyname><forenames>Sarah</forenames></author><author><keyname>Randall</keyname><forenames>Dana</forenames></author><author><keyname>Streib</keyname><forenames>Amanda Pascoe</forenames></author></authors><title>Mixing Times of Self-Organizing Lists and Biased Permutations</title><categories>cs.DM</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling permutations from S_n is a fundamental problem from probability
theory. The nearest neighbor transposition chain \cal{M}}_{nn} is known to
converge in time \Theta(n^3 \log n) in the uniform case and time \Theta(n^2) in
the constant bias case, in which we put adjacent elements in order with
probability p \neq 1/2 and out of order with probability 1-p. Here we consider
the variable bias case where we put adjacent elements x&lt;y in order with
probability p{x,y} and out of order with probability 1-p_{x,y}. The problem of
bounding the mixing rate of M_{nn} was posed by Fill and was motivated by the
Move-Ahead-One self-organizing list update algorithm. It was conjectured that
the chain would always be rapidly mixing if 1/2 \leq p_{x,y} \leq 1 for all x &lt;
y, but this was only known in the case of constant bias or when p_{x,y} is
equal to 1/2 or 1, a case that corresponds to sampling linear extensions of a
partial order. We prove the chain is rapidly mixing for two classes: &quot;Choose
Your Weapon,&quot; where we are given r_1,..., r_{n-1} with r_i \geq 1/2 and
p_{x,y}=r_x for all x&lt;y (so the dominant player chooses the game, thus fixing
his or her probability of winning), and &quot;League Hierarchies,&quot; where there are
two leagues and players from the A-league have a fixed probability of beating
players from the B-league, players within each league are similarly divided
into sub-leagues with a possibly different fixed probability, and so forth
recursively. Both of these classes include permutations with constant bias as a
special case. Moreover, we also prove that the most general conjecture is false
by constructing a counterexample where 1/2 \leq p_{x,y} \leq 1 for all x&lt; y,
but for which the nearest neighbor transposition chain requires exponential
time to converge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3240</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3240</id><created>2012-04-15</created><authors><author><keyname>Leng&#xe1;l</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>An Efficient Finite Tree Automata Library</title><categories>cs.FL cs.DS</categories><comments>Master's thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous computer systems use dynamic control and data structures of
unbounded size. These data structures have often the character of trees or they
can be encoded as trees with some additional pointers. This is exploited by
some currently intensively studied techniques of formal verification that
represent an infinite number of states using a finite tree automaton. However,
currently there is no tree automata library implementation that would provide
an efficient and flexible support for such methods. Thus the aim of this
Master's Thesis is to provide such a library. The present paper first describes
the theoretical background of finite tree automata and regular tree languages.
Then it surveys the current implementations of tree automata libraries and
studies various verification techniques, outlining requirements for the
library. Representation of a finite tree automaton and algorithms that perform
standard language operations on this representation are proposed in the next
part, which is followed by description of library implementation. Through a
series of experiments it is shown that the library can compete with other
available tree automata libraries, in certain areas being even significantly
superior to them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3241</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3241</id><created>2012-04-15</created><authors><author><keyname>Aristov</keyname><forenames>Vladimir</forenames></author><author><keyname>Stroganov</keyname><forenames>Andrey</forenames></author></authors><title>Computing without a computer: a new approach for solving nonlinear
  differential equations</title><categories>cs.NA math.NA</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well-known Turing machine is an example of a theoretical digital
computer, and it was the logical basis of constructing real electronic
computers. In the present paper we propose an alternative, namely, by
formalising arithmetic operations in the ordinary computing device, we attempt
to go to the analytical procedure (for calculations). The method creates
possibilities for solving nonlinear differential equations and systems. Our
theoretical computer model requires retaining a finite number of terms to
represent numbers, and utilizes digit carry procedure. The solution is
represented in the form of a segment of a series in the powers of the step size
of the independent variable in the finite-difference scheme. The algorithm
generates a schematic representation that approximates the convergent
finite-difference scheme, which, in turn, approximates the equation under
consideration. The use of probabilistic methods allows us to average the
recurrent calculations and exclude intermediate levels of computation. All the
stages of formalizing operations of the classical computer result in &quot;the
method of the computer analogy&quot;. The proposed method leads to an explicit
analytical representation of the solution. We present the general features of
the algorithm which are illustrated by an example of solutions for a system of
nonlinear equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3245</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3245</id><created>2012-04-15</created><authors><author><keyname>Iskander</keyname><forenames>Azhmukhamedov</forenames></author></authors><title>Monografia</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developed structural scheme implementation of an integrated security and
formulated principles for the creation and development of an effective system
of information security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3249</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3249</id><created>2012-04-15</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Process algebra with conditionals in the presence of epsilon</title><categories>cs.LO</categories><comments>41 pages</comments><acm-class>D.1.3; D.2.1; D.2.4; F.1.2; F.3.1; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper, we presented several extensions of ACP with conditional
expressions, including one with a retrospection operator on conditions to allow
for looking back on conditions under which preceding actions have been
performed. In this paper, we add a constant for a process that is only capable
of terminating successfully to those extensions of ACP, which can be very
useful in applications. It happens that in all cases the addition of this
constant is unproblematic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3251</identifier>
 <datestamp>2012-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3251</id><created>2012-04-15</created><updated>2012-06-28</updated><authors><author><keyname>Fedorova</keyname><forenames>Valentina</forenames></author><author><keyname>Gammerman</keyname><forenames>Alex</forenames></author><author><keyname>Nouretdinov</keyname><forenames>Ilia</forenames></author><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Plug-in martingales for testing exchangeability on-line</title><categories>cs.LG stat.ME</categories><comments>8 pages, 7 figures; ICML 2012 Conference Proceedings</comments><report-no>On-line Compression Modelling Project (New Series), Working Paper 04</report-no><msc-class>62G10</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard assumption in machine learning is the exchangeability of data,
which is equivalent to assuming that the examples are generated from the same
probability distribution independently. This paper is devoted to testing the
assumption of exchangeability on-line: the examples arrive one by one, and
after receiving each example we would like to have a valid measure of the
degree to which the assumption of exchangeability has been falsified. Such
measures are provided by exchangeability martingales. We extend known
techniques for constructing exchangeability martingales and show that our new
method is competitive with the martingales introduced before. Finally we
investigate the performance of our testing method on two benchmark datasets,
USPS and Statlog Satellite data; for the former, the known techniques give
satisfactory results, but for the latter our new more flexible method becomes
necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3255</identifier>
 <datestamp>2014-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3255</id><created>2012-04-15</created><updated>2013-05-02</updated><authors><author><keyname>Jaeger</keyname><forenames>Manfred</forenames></author></authors><title>Lower Complexity Bounds for Lifted Inference</title><categories>cs.AI</categories><comments>To appear in Theory and Practice of Logic Programming (TPLP)</comments><doi>10.1017/S1471068413000707</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the big challenges in the development of probabilistic relational (or
probabilistic logical) modeling and learning frameworks is the design of
inference techniques that operate on the level of the abstract model
representation language, rather than on the level of ground, propositional
instances of the model. Numerous approaches for such &quot;lifted inference&quot;
techniques have been proposed. While it has been demonstrated that these
techniques will lead to significantly more efficient inference on some specific
models, there are only very recent and still quite restricted results that show
the feasibility of lifted inference on certain syntactically defined classes of
models. Lower complexity bounds that imply some limitations for the feasibility
of lifted inference on more expressive model classes were established early on
in (Jaeger 2000). However, it is not immediate that these results also apply to
the type of modeling languages that currently receive the most attention, i.e.,
weighted, quantifier-free formulas. In this paper we extend these earlier
results, and show that under the assumption that NETIME =/= ETIME, there is no
polynomial lifted inference algorithm for knowledge bases of weighted,
quantifier- and function-free formulas. Further strengthening earlier results,
this is also shown to hold for approximate inference, and for knowledge bases
not containing the equality predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3256</identifier>
 <datestamp>2012-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3256</id><created>2012-04-15</created><updated>2012-08-01</updated><authors><author><keyname>Malik</keyname><forenames>Salman</forenames></author><author><keyname>Jacquet</keyname><forenames>Philippe</forenames></author><author><keyname>Adjih</keyname><forenames>Cedric</forenames></author></authors><title>Optimizing the Medium Access Control in Multi-hop Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>To be submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of geometric optimization of medium access control in
multi-hop wireless network. We discuss the optimal placements of simultaneous
transmitters in the network and our general framework allows us to evaluate the
performance gains of highly managed medium access control schemes that would be
required to implement these placements. In a wireless network consisting of
randomly distributed nodes, our performance metrics are the optimum
transmission range that achieves the most optimal tradeoff between the progress
of packets in desired directions towards their respective destinations and the
total number of transmissions required to transport packets to their
destinations. We evaluate ALOHA based scheme where simultaneous transmitters
are dispatched according to a uniform Poisson distribution and compare it with
various grid pattern based schemes where simultaneous transmitters are
positioned in specific regular patterns. Our results show that optimizing the
medium access control in multi-hop network should take into account the
parameters like signal-to-interference ratio threshold and attenuation
coefficient. For instance, at typical values of signal-to-interference ratio
threshold and attenuation coefficient, the most optimal scheme is based on
triangular grid pattern and, under no fading channel model, the most optimal
transmission range and network capacity are higher than the optimum
transmission range and capacity achievable with ALOHA based scheme by factors
of two and three respectively. Later on, we also identify the optimal medium
access control schemes when signal-to-interference ratio threshold and
attenuation coefficient approach the extreme values and discuss how fading
impacts the performance of all schemes we evaluate in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3259</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3259</id><created>2012-04-15</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author><author><keyname>Andrushevich</keyname><forenames>Aliaksei</forenames></author><author><keyname>Kistler</keyname><forenames>Rolf</forenames></author><author><keyname>Klapproth</keyname><forenames>Alexander</forenames></author></authors><title>Combinatorial Evolution and Forecasting of Communication Protocol ZigBee</title><categories>cs.NI cs.SY math.OC</categories><comments>6 pages, 13 figures, 4 tables</comments><msc-class>68T20, 68M12, 94C30, 90B50, 90C27</msc-class><acm-class>C.2.2; D.2.2; G.2.3; I.2.8; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article addresses combinatorial evolution and forecasting of
communication protocol for wireless sensor networks (ZigBee). Morphological
tree structure (a version of and-or tree) is used as a hierarchical model for
the protocol. Three generations of ZigBee protocol are examined. A set of
protocol change operations is generated and described. The change operations
are used as items for forecasting based on combinatorial problems (e.g.,
clustering, knapsack problem, multiple choice knapsack problem). Two kinds of
preliminary forecasts for the examined communication protocol are considered:
(i) direct expert (expert judgment) based forecast, (ii) computation of the
forecast(s) (usage of multicriteria decision making and combinatorial
optimization problems). Finally, aggregation of the obtained preliminary
forecasts is considered (two aggregation strategies are used).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3261</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3261</id><created>2012-04-15</created><authors><author><keyname>Wood</keyname><forenames>Lloyd</forenames></author><author><keyname>Ivancic</keyname><forenames>Will</forenames></author><author><keyname>Eddy</keyname><forenames>Wes</forenames></author><author><keyname>Stewart</keyname><forenames>Dave</forenames></author><author><keyname>Northam</keyname><forenames>James</forenames></author><author><keyname>Jackson</keyname><forenames>Chris</forenames></author></authors><title>Investigating operation of the Internet in orbit: Five years of
  collaboration around CLEO</title><categories>cs.NI astro-ph.IM cs.SY</categories><comments>2 pages</comments><journal-ref>Article for IEEE Communications Society Satellite and Space
  Communications Technical Committee newsletter, vol. 18 no. 2, pp. 10-11,
  November 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cisco router in Low Earth Orbit (CLEO) was launched into space as an
experimental secondary payload onboard the UK Disaster Monitoring Constellation
(UK-DMC) satellite in September 2003. The UK-DMC satellite is one of an
increasing number of DMC satellites in orbit that rely on the Internet Protocol
(IP) for command and control and for delivery of data from payloads. The DMC
satellites, built by Surrey Satellite Technology Ltd (SSTL), have imaged the
effects of Hurricane Katrina, the Indian Ocean Tsunami, and other events for
disaster relief under the International Space and Major Disasters Charter. It
was possible to integrate the Cisco mobile access router into the UK-DMC
satellite as a result of the DMC satellites' adoption of existing commercial
networking standards, using IP over Frame Relay over standard High-Level Data
Link Control, or HDLC (ISO 13239) on standard serial interfaces. This approach
came from work onboard SSTL's earlier UoSAT-12 satellite
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3263</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3263</id><created>2012-04-15</created><authors><author><keyname>Wood</keyname><forenames>Lloyd</forenames></author></authors><title>Saratoga: scalable, speedy data delivery for sensor networks</title><categories>cs.NI astro-ph.IM</categories><comments>2 pages; First Annual CCSR Research Symposium (CRS 2011), Centre for
  Communication Systems Research, 30 June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A networking transport protocol, named Saratoga, has been developed at the
University of Surrey for efficient delivery of imagery from
Internet-Protocol-based remote-sensing satellites. Saratoga is now being
implemented and evaluated for use for the high-end data-delivery needs of
astronomers using large, advanced, radio telescopes. These telescopes are
expected to take advantage of Internet technologies. This brief paper outlines
the reasons for the creation and adoption of this protocol, discusses how it
differs from and complements other protocols, and summarises the worldwide
collaboration that is making this development possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3264</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3264</id><created>2012-04-15</created><authors><author><keyname>Wood</keyname><forenames>Lloyd</forenames></author></authors><title>Assessing and improving an approach to delay-tolerant networking</title><categories>cs.NI</categories><comments>2 pages; First Annual CCSR Research Symposium (CRS 2011), Centre for
  Communication Systems Research, 30 June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-tolerant networking (DTN) is a term invented to describe and encompass
all types of long-delay, disconnected, disrupted or intermittently-connected
networks, where mobility and outages or scheduled contacts may be experienced.
'DTN' is also used to refer to the Bundle Protocol, which has been proposed as
the one unifying solution for disparate DTN networking scenarios, after
originally being designed solely for use in deep space for the 'Interplanetary
Internet.' We evaluated the Bundle Protocol by testing it in space and on the
ground. We have found architectural weaknesses in the Bundle Protocol that may
prevent engineering deployment of this protocol in realistic delay-tolerant
networking scenarios, and have proposed approaches to address these weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3265</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3265</id><created>2012-04-15</created><authors><author><keyname>Wood</keyname><forenames>Lloyd</forenames></author></authors><title>SaVi: satellite constellation visualization</title><categories>astro-ph.IM cs.OH</categories><comments>2 pages; First Annual CCSR Research Symposium (CRS 2011), Centre for
  Communication Systems Research, 30 June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SaVi, a program for visualizing satellite orbits, movement, and coverage, is
maintained at the University of Surrey. This tool has been used for research in
academic papers, and by industry companies designing and intending to deploy
satellite constellations. It has also proven useful for demonstrating aspects
of satellite constellations and their geometry, coverage and movement for
educational and teaching purposes. SaVi is introduced and described briefly
here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3283</identifier>
 <datestamp>2012-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3283</id><created>2012-04-15</created><updated>2012-10-18</updated><authors><author><keyname>Randour</keyname><forenames>Mickael</forenames></author></authors><title>Automated synthesis of reliable and efficient systems through game
  theory: a case study</title><categories>cs.GT cs.LO</categories><comments>Published in ECCS 2012 (European Conference on Complex Systems)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive computer systems bear inherent complexity due to continuous
interactions with their environment. While this environment often proves to be
uncontrollable, we still want to ensure that critical computer systems will not
fail, no matter what they face. Examples are legion: railway traffic, power
plants, plane navigation systems, etc. Formal verification of a system may
ensure that it satisfies a given specification, but only applies to an already
existing model of a system. In this work, we address the problem of synthesis:
starting from a specification of the desired behavior, we show how to build a
suitable system controller that will enforce this specification. In particular,
we discuss recent developments of that approach for systems that must ensure
Boolean behaviors (e.g., reachability, liveness) along with quantitative
requirements over their execution (e.g., never drop out of fuel, ensure a
suitable mean response time). We notably illustrate a powerful, practically
useable algorithm for the automated synthesis of provably safe reactive
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3284</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3284</id><created>2012-04-15</created><authors><author><keyname>Boskos</keyname><forenames>D.</forenames></author><author><keyname>Tsinias</keyname><forenames>J.</forenames></author></authors><title>Observer design for nonlinear triangular systems with unobservable
  linearization</title><categories>math.OC cs.SY</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the observer design problem for a wide class of
triangular time-varying nonlinear systems, with unobservable linearization.
Sufficient conditions are derived for the existence of a Luenberger-type
observer, when it is a priori known that the initial state of the system
belongs to a given nonempty bounded subset of the state space. For the general
case, the state estimation is exhibited by means of a switching sequence of
time-varying dynamics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3293</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3293</id><created>2012-04-15</created><authors><author><keyname>Kontorovich</keyname><forenames>Aryeh</forenames></author><author><keyname>Trachtenberg</keyname><forenames>Ari</forenames></author></authors><title>Efficiently decoding strings from their shingles</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining whether an unordered collection of overlapping substrings (called
shingles) can be uniquely decoded into a consistent string is a problem that
lies within the foundation of a broad assortment of disciplines ranging from
networking and information theory through cryptography and even genetic
engineering and linguistics. We present three perspectives on this problem: a
graph theoretic framework due to Pevzner, an automata theoretic approach from
our previous work, and a new insight that yields a time-optimal streaming
algorithm for determining whether a string of $n$ characters over the alphabet
$\Sigma$ can be uniquely decoded from its two-character shingles. Our algorithm
achieves an overall time complexity $\Theta(n)$ and space complexity
$O(|\Sigma|)$. As an application, we demonstrate how this algorithm can be
extended to larger shingles for efficient string reconciliation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3318</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3318</id><created>2012-02-09</created><updated>2012-06-03</updated><authors><author><keyname>Wen</keyname><forenames>H.</forenames></author><author><keyname>Kish</keyname><forenames>L. B.</forenames></author><author><keyname>Schmera</keyname><forenames>G.</forenames></author></authors><title>High-dimensional noise-based logical controller</title><categories>physics.gen-ph cs.ET</categories><comments>Texas A&amp;M - US Navy patent disclosure; TAMUS-3660</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a scheme for controlling physical and other quantities;
utilizing noise-based logic for control-and-optimization with high
dimensionality, similarly how the Hilbert space of quantum informatics can be
utilized for such purpose. As a concrete realization of the noise-based control
scheme, we introduce &quot;Dictatorial control&quot; where noise-based logic results in
an exponential speedup of operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3328</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3328</id><created>2012-04-15</created><authors><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author><author><keyname>Elzantout</keyname><forenames>Moustafa</forenames></author><author><keyname>Elkhouly</keyname><forenames>Reem</forenames></author><author><keyname>Lotfy</keyname><forenames>Amal</forenames></author></authors><title>Ubiquitous Indoor Localization and Worldwide Automatic Construction of
  Floor Plans</title><categories>cs.NI</categories><comments>Under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although GPS has been considered a ubiquitous outdoor localization
technology, we are still far from a similar technology for indoor environments.
While a number of technologies have been proposed for indoor localization, they
are isolated efforts that are way from a true ubiquitous localization system. A
ubiquitous indoor positioning system is envisioned to be deployed on a large
scale worldwide, with minimum overhead, to work with heterogeneous devices, and
to allow users to roam seamlessly from indoor to outdoor environments. Such a
system will enable a wide set of applications including worldwide seamless
direction finding between indoor locations, enhancing first responders' safety
by providing anywhere localization and floor plans, and providing a richer
environment for location-aware social networking applications.
  We describe an architecture for the ubiquitous indoor positioning system
(IPS) and the challenges that have to be addressed to materialize it. We then
focus on the feasibility of automating the construction of a worldwide indoor
floor plan and fingerprint database which, as we believe, is one of the main
challenges that limit the existence of a ubiquitous IPS system. Our proof of
concept uses a crowd-sourcing approach that leverages the embedded sensors in
today's cell phones as a worldwide distributed floor plan generation tool. This
includes constructing the floor plans and determining the areas of interest
(corridors, offices, meeting rooms, elevators, etc). The cloud computing
concepts are also adopted for the processing and storage of the huge amount of
data generated and requested by the system's users. Our results show the
ability of the system to construct an accurate floor plan and identify the
areas of interest with more than 90% accuracy. We also identify different
research directions for addressing the challenges of realizing a true
ubiquitous IPS system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3337</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3337</id><created>2012-04-15</created><authors><author><keyname>Iwen</keyname><forenames>Mark A.</forenames></author><author><keyname>Maggioni</keyname><forenames>Mauro</forenames></author></authors><title>Approximation of Points on Low-Dimensional Manifolds Via Random Linear
  Projections</title><categories>cs.IT cs.DS math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the approximate reconstruction of points, x \in R^D,
which are close to a given compact d-dimensional submanifold, M, of R^D using a
small number of linear measurements of x. In particular, it is shown that a
number of measurements of x which is independent of the extrinsic dimension D
suffices for highly accurate reconstruction of a given x with high probability.
Furthermore, it is also proven that all vectors, x, which are sufficiently
close to M can be reconstructed with uniform approximation guarantees when the
number of linear measurements of x depends logarithmically on D. Finally, the
proofs of these facts are constructive: A practical algorithm for
manifold-based signal recovery is presented in the process of proving the two
main results mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3341</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3341</id><created>2012-04-15</created><authors><author><keyname>Thomas</keyname><forenames>Russell C.</forenames></author><author><keyname>Gero</keyname><forenames>John S.</forenames></author></authors><title>Patterns of Social Influence in a Network of Situated Cognitive Agents</title><categories>cs.SI cs.AI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/30</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the results of computational experiments on the effects
of social influence on individual and systemic behavior of situated cognitive
agents in a product-consumer environment. Paired experiments were performed
with identical initial conditions to compare social agents with non- social
agents. Experiment results show that social agents are more productive in
consuming available products, both in terms of aggregate unit consumption and
aggregate utility. But this comes at a cost of individual average utility per
unit consumed. In effect, social interaction achieved higher productivity by
'lowering the standards' of individual consumers. While still at an early stage
of development, such an agent-based model laboratory is shown to be an
effective research tool to investigate rich collective behavior in the context
of demanding cognitive tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3342</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3342</id><created>2012-04-15</created><authors><author><keyname>Starbird</keyname><forenames>Kate</forenames></author></authors><title>What &quot;Crowdsourcing&quot; Obscures: Exposing the Dynamics of Connected Crowd
  Work during Disaster</title><categories>cs.SI cs.CY</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/71</report-no><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to demonstrate that the current understanding of
crowdsourcing may not be broad enough to capture the diversity of crowd work
during disasters, or specific enough to highlight the unique dynamics of
information organizing by the crowd in that context. In making this argument,
this paper first unpacks the crowdsourcing term, examining its roots in open
source development and outsourcing business models, and tying it to related
concepts of human computation and collective intelligence. The paper then
attempts to characterize several examples of crowd work during disasters using
current definitions of crowdsourcing and existing models for human computation
and collective intelligence, exposing a need for future research towards a
framework for understanding crowd work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3343</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3343</id><created>2012-04-15</created><authors><author><keyname>Gegenhuber</keyname><forenames>Thomas</forenames></author><author><keyname>Hrelja</keyname><forenames>Marko</forenames></author></authors><title>Broadcast Search in Innovation Contests: Case for Hybrid Models</title><categories>cs.SI cs.CY</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/76</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Organizations use broadcast search to identify new avenues of innovation.
Research on innovation contests provides insights on why excellent ideas are
created in a broadcast search. However, there is little research on how
excellent ideas are selected. Drawing from the brainstorming literature we find
that the selection of excellent ideas needs further investigation. We propose
that a hybrid model may lead to selection of better ideas. The hybrid model is
a broadcast search approach that exploits the strengths of different actors and
procedures in idea generation and the selection phase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3348</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3348</id><created>2012-04-15</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Symmetry Breaking Constraints: Recent Results</title><categories>cs.AI cs.CC</categories><comments>To appear in Proceedings of Twenty-Sixth Conference on Artificial
  Intelligence (AAAI-12)</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetry is an important problem in many combinatorial problems. One way of
dealing with symmetry is to add constraints that eliminate symmetric solutions.
We survey recent results in this area, focusing especially on two common and
useful cases: symmetry breaking constraints for row and column symmetry, and
symmetry breaking constraints for eliminating value symmetry
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3352</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3352</id><created>2012-04-15</created><authors><author><keyname>Kane</keyname><forenames>Gerald C.</forenames></author><author><keyname>Ransbotham</keyname><forenames>Sam</forenames></author></authors><title>Collaborative Development in Wikipedia</title><categories>cs.SI</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/74</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using 16,068 articles in Wikipedia's Medicine Wikiproject, we study the
relationship between collaboration and quality. We assess whether certain
collaborative patterns are associated with information quality in terms of
self-evaluated quality and article viewership. We find that the number of
contributors has a curvilinear relationship to information quality, more
contributors improving quality but only up to a certain point. Other articles
that its collaborators work on also influences the quality of an information
artifact, creating an interdependent network of artifacts and contributors.
Finally, we see evidence of a recursive relationship between information
quality and contributor activity, but that this recursive relationship
attenuates over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3353</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3353</id><created>2012-04-15</created><authors><author><keyname>Russell</keyname><forenames>Terrell G.</forenames></author></authors><title>Collective Cognitive Authority: Expertise Location via Social Labeling</title><categories>cs.SI cs.HC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991), 8 pages, 7 figures</comments><report-no>CollectiveIntelligence/2012/46</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of knowing who knows what is multi-faceted. Knowledge and
expertise lie on a spectrum and one's expertise in one topic area may have
little bearing on one's knowledge in a disparate topic area. In addition, we
continue to learn new things over time. Each of us see but a sliver of our
acquaintances' and co-workers' areas of expertise. By making explicit and
visible many individual perceptions of cognitive authority, this work shows
that a group can know what its members know about in a relatively efficient and
inexpensive manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3362</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3362</id><created>2012-04-16</created><updated>2013-06-28</updated><authors><author><keyname>Bauer</keyname><forenames>Andreas</forenames></author><author><keyname>Wolff</keyname><forenames>Christian</forenames></author></authors><title>Event based classification of Web 2.0 text streams</title><categories>cs.IR</categories><comments>11 pages, 3 figures, 2 tables</comments><acm-class>H.3.3; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web 2.0 applications like Twitter or Facebook create a continuous stream of
information. This demands new ways of analysis in order to offer insight into
this stream right at the moment of the creation of the information, because
lots of this data is only relevant within a short period of time. To address
this problem real time search engines have recently received increased
attention. They take into account the continuous flow of information
differently than traditional web search by incorporating temporal and social
features, that describe the context of the information during its creation.
Standard approaches where data first get stored and then is processed from a
peristent storage suffer from latency. We want to address the fluent and rapid
nature of text stream by providing an event based approach that analyses
directly the stream of information. In a first step we want to define the
difference between real time search and traditional search to clarify the
demands in modern text filtering. In a second step we want to show how event
based features can be used to support the tasks of real time search engines.
Using the example of Twitter we present in this paper a way how to combine an
event based approach with text mining and information filtering concepts in
order to classify incoming information based on stream features. We calculate
stream dependant features and feed them into a neural network in order to
classify the text streams. We show the separative capabilities of event based
features as the foundation for a real time search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3367</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3367</id><created>2012-04-16</created><authors><author><keyname>Rudoy</keyname><forenames>Dmitry</forenames></author><author><keyname>Goldman</keyname><forenames>Dan B.</forenames></author><author><keyname>Shechtman</keyname><forenames>Eli</forenames></author><author><keyname>Zelnik-Manor</keyname><forenames>Lihi</forenames></author></authors><title>Crowdsourcing Gaze Data Collection</title><categories>cs.SI cs.HC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/106</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing where people look is a useful tool in many various image and video
applications. However, traditional gaze tracking hardware is expensive and
requires local study participants, so acquiring gaze location data from a large
number of participants is very problematic. In this work we propose a
crowdsourced method for acquisition of gaze direction data from a virtually
unlimited number of participants, using a robust self-reporting mechanism (see
Figure 1). Our system collects temporally sparse but spatially dense
points-of-attention in any visual information. We apply our approach to an
existing video data set and demonstrate that we obtain results similar to
traditional gaze tracking. We also explore the parameter ranges of our method,
and collect gaze tracking data for a large set of YouTube videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3372</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3372</id><created>2012-04-16</created><authors><author><keyname>Salikhmetov</keyname><forenames>Anton</forenames></author></authors><title>Blind graph rewriting systems</title><categories>cs.LO cs.FL</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a simple (probably, the simplest) structure for random access
memory. This structure can be used to construct a universal system with nearly
void processor, namely, we demonstrate that the processor of such a system may
have empty instruction set, in a more strong manner than the existing ZISC
(zero instruction set computer based on ideas for artificial neural networks)
and NISC architecture (no instruction set computing). More precisely, the
processor will be forbidden to analyze any information stored in the memory,
the latter being the only state of such a machine. This particular paper is to
cover an isolated aspect of the idea, specifically, to provide the logical
operations embedded into a system without any built-in conditional statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3374</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3374</id><created>2012-04-16</created><authors><author><keyname>Shwarts-Asher</keyname><forenames>Daphna</forenames></author></authors><title>Social Aspects of Virtual Teams</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/3</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a transformation from individual work to team work in the last
few decades (Ilgen, 1999), and many organizations use teams for many activities
done by individuals in the past (Boyett &amp; Conn, 1992 ; Katzenbach &amp; Smith,
1993). In recent years, there has been a renewed interest in computer-mediated
groups because of the increases in globalization of business operations leading
to geographically dispersed executives and decision makers. However, what seems
to be lacking is some focus in terms of problem settings and corresponding
tools to support collaborative decision making. The research question of this
study deals with the dynamics of virtual teams' members. A model, suggesting
that team dynamics can increase the teams' output, is presented, and a
methodology to examine the model is illustrated. An experiment was performed,
in which subjects, who were grouped into teams, had to share information in
order to complete a task. The findings indicate that the social aspect of the
virtual team's discussion is negative than the social aspect of the
face-to-face team's discussion, and that the virtual team's output is inferior
to the face-to-face team's output. The virtual team is a common way of working
nowadays, and with the growing use of Internet applications and firms'
globalization it will expand in the future. Thus, the importance of the
theoretical and practical implementation of the research will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3375</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3375</id><created>2012-04-16</created><authors><author><keyname>Fuehres</keyname><forenames>Hauke</forenames></author><author><keyname>Gloor</keyname><forenames>Peter A.</forenames></author><author><keyname>Henninger</keyname><forenames>Michael</forenames></author><author><keyname>Kleeb</keyname><forenames>Reto</forenames></author><author><keyname>Nemoto</keyname><forenames>Keiichi</forenames></author></authors><title>Galaxysearch - Discovering the Knowledge of Many by Using Wikipedia as a
  Meta-Searchindex</title><categories>cs.SI</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/73</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dynamic map of knowledge generated from Wikipedia pages and the
Web URLs contained therein. GalaxySearch provides answers to the questions we
don't know how to ask, by constructing a semantic network of the most relevant
pages in Wikipedia related to a search term. This search graph is constructed
based on the Wikipedia bidirectional link structure, the most recent edits on
the pages, the importance of the page, and the article quality; search results
are then ranked by the centrality of their network position. GalaxySearch
provides the results in three related ways: (1) WikiSearch - identifying the
most prominent Wikipedia pages and Weblinks for a chosen topic, (2) WikiMap -
creating a visual temporal map of the changes in the semantic network generated
by the search results over the lifetime of the returned Wikipedia articles, and
(3) WikiPulse - finding the most recent and most relevant changes and updates
about a topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3379</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3379</id><created>2012-04-16</created><authors><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Fiorina</keyname><forenames>Jocelyn</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author></authors><title>A New Low-Complexity Decodable Rate-1 Full-Diversity 4 x 4 STBC with
  Nonvanishing Determinants</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, and 1 table; IEEE Transactions on Wireless
  Communications, Vol. 10, No. 8, AUGUST 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Space-time coding techniques have become common-place in wireless
communication standards as they provide an effective way to mitigate the fading
phenomena inherent in wireless channels. However, the use of Space-Time Block
Codes (STBCs) increases significantly the optimal detection complexity at the
receiver unless the low complexity decodability property is taken into
consideration in the STBC design. In this letter we propose a new
low-complexity decodable rate-1 full-diversity 4 x 4 STBC. We provide an
analytical proof that the proposed code has the Non-Vanishing-Determinant (NVD)
property, a property that can be exploited through the use of adaptive
modulation which changes the transmission rate according to the wireless
channel quality. We compare the proposed code to existing low-complexity
decodable rate-1 full-diversity 4 x 4 STBCs in terms of performance over
quasi-static Rayleigh fading channels, detection complexity and Peak-to-Average
Power Ratio (PAPR). Our code is found to provide the best performance and the
smallest PAPR which is that of the used QAM constellation at the expense of a
slight increase in detection complexity w.r.t. certain previous codes but this
will only penalize the proposed code for high-order QAM constellations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3384</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3384</id><created>2012-04-16</created><authors><author><keyname>Chen</keyname><forenames>Zhao</forenames></author><author><keyname>Xu</keyname><forenames>Mai</forenames></author><author><keyname>Yin</keyname><forenames>Luiguo</forenames></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>Unequal Error Protected JPEG 2000 Broadcast Scheme with Progressive
  Fountain Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel scheme, based on progressive fountain codes, for
broadcasting JPEG 2000 multimedia. In such a broadcast scheme, progressive
resolution levels of images/video have been unequally protected when
transmitted using the proposed progressive fountain codes. With progressive
fountain codes applied in the broadcast scheme, the resolutions of images (JPEG
2000) or videos (MJPEG 2000) received by different users can be automatically
adaptive to their channel qualities, i.e. the users with good channel qualities
are possible to receive the high resolution images/vedio while the users with
bad channel qualities may receive low resolution images/vedio. Finally, the
performance of the proposed scheme is evaluated with the MJPEG 2000 broadcast
prototype.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3388</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3388</id><created>2012-04-16</created><authors><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Fiorina</keyname><forenames>Jocelyn</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author></authors><title>A Novel Construction of Multi-group Decodable Space-Time Block Codes</title><categories>cs.IT math.IT</categories><comments>12 pages, and 5 tables, accepted for publication in IEEE transactions
  on communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex Orthogonal Design (COD) codes are known to have the lowest detection
complexity among Space-Time Block Codes (STBCs). However, the rate of square
COD codes decreases exponentially with the number of transmit antennas. The
Quasi-Orthogonal Design (QOD) codes emerged to provide a compromise between
rate and complexity as they offer higher rates compared to COD codes at the
expense of an increase of decoding complexity through partially relaxing the
orthogonality conditions. The QOD codes were then generalized with the so
called g-symbol and g-group decodable STBCs where the number of orthogonal
groups of symbols is no longer restricted to two as in the QOD case. However,
the adopted approach for the construction of such codes is based on sufficient
but not necessary conditions which may limit the achievable rates for any
number of orthogonal groups. In this paper, we limit ourselves to the case of
Unitary Weight (UW)-g-group decodable STBCs for 2^a transmit antennas where the
weight matrices are required to be single thread matrices with non-zero entries
in {1,-1,j,-j} and address the problem of finding the highest achievable rate
for any number of orthogonal groups. This special type of weight matrices
guarantees full symbol-wise diversity and subsumes a wide range of existing
codes in the literature. We show that in this case an exhaustive search can be
applied to find the maximum achievable rates for UW-g-group decodable STBCs
with g&gt;1. For this purpose, we extend our previously proposed approach for
constructing UW-2-group decodable STBCs based on necessary and sufficient
conditions to the case of UW-g-group decodable STBCs in a recursive manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3391</identifier>
 <datestamp>2012-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3391</id><created>2012-04-16</created><updated>2012-06-12</updated><authors><author><keyname>Chen</keyname><forenames>Zhao</forenames></author><author><keyname>Yin</keyname><forenames>Liuguo</forenames></author><author><keyname>Xu</keyname><forenames>Mai</forenames></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>Rateless Codes with Progressive Recovery for Layered Multimedia Delivery</title><categories>cs.IT cs.MM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel approach, based on unequal error protection, to
enhance rateless codes with progressive recovery for layered multimedia
delivery. With a parallel encoding structure, the proposed Progressive Rateless
codes (PRC) assign unequal redundancy to each layer in accordance with their
importance. Each output symbol contains information from all layers, and thus
the stream layers can be recovered progressively at the expected received
ratios of output symbols. Furthermore, the dependency between layers is
naturally considered. The performance of the PRC is evaluated and compared with
some related UEP approaches. Results show that our PRC approach provides better
recovery performance with lower overhead both theoretically and numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3401</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3401</id><created>2012-04-16</created><authors><author><keyname>Salminen</keyname><forenames>Juho</forenames></author></authors><title>Collective Intelligence in Humans: A Literature Review</title><categories>cs.CY cs.SI</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/99</report-no><acm-class>H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This literature review focuses on collective intelligence in humans. A
keyword search was performed on the Web of Knowledge and selected papers were
reviewed in order to reveal themes relevant to collective intelligence. Three
levels of abstraction were identified in discussion about the phenomenon: the
micro-level, the macro-level and the level of emergence. Recurring themes in
the literature were categorized under the above-mentioned framework and
directions for future research were identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3410</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3410</id><created>2012-04-16</created><authors><author><keyname>Randimbivololona</keyname><forenames>Famantanantsoa</forenames></author><author><keyname>Brahmi</keyname><forenames>Abderrahmane</forenames></author><author><keyname>Meur</keyname><forenames>Philippe Le</forenames></author></authors><title>Airborne software tests on a fully virtual platform</title><categories>cs.SE</categories><comments>EDCC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the early deployment of a fully virtual platform to
perform the tests of certified airborne software. This is an alternative to the
current approach based on the use of dedicated hardware platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3413</identifier>
 <datestamp>2014-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3413</id><created>2012-04-16</created><updated>2014-03-27</updated><authors><author><keyname>Fischer</keyname><forenames>Eldar</forenames></author><author><keyname>Goldhirsh</keyname><forenames>Yonatan</forenames></author><author><keyname>Lachish</keyname><forenames>Oded</forenames></author></authors><title>Testing Formula Satisfaction</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the query complexity of testing for properties defined by read once
formulas, as instances of {\em massively parametrized properties}, and prove
several testability and non-testability results. First we prove the testability
of any property accepted by a Boolean read-once formula involving any bounded
arity gates, with a number of queries exponential in $\epsilon$, doubly
exponential in the arity, and independent of all other parameters. When the
gates are limited to being monotone, we prove that there is an {\em estimation}
algorithm, that outputs an approximation of the distance of the input from
satisfying the property. For formulas only involving And/Or gates, we provide a
more efficient test whose query complexity is only quasipolynomial in
$\epsilon$. On the other hand, we show that such testability results do not
hold in general for formulas over non-Boolean alphabets; specifically we
construct a property defined by a read-once arity $2$ (non-Boolean) formula
over an alphabet of size $4$, such that any $1/4$-test for it requires a number
of queries depending on the formula size. We also present such a formula over
an alphabet of size $5$ that additionally satisfies a strong monotonicity
condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3431</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3431</id><created>2012-04-16</created><authors><author><keyname>Katiyar</keyname><forenames>Sumit</forenames></author><author><keyname>Jain</keyname><forenames>R. K.</forenames></author><author><keyname>Agrawal</keyname><forenames>N. K.</forenames></author></authors><title>Green Cellular Network Deployment To Reduce RF Pollution</title><categories>cs.CY</categories><comments>6 pages, 6 figures. arXiv admin note: substantial text overlap with
  arXiv:1204.2101, arXiv:1110.2627, and with arXiv:0803.0952 and
  arXiv:0803.0952 by other authors</comments><doi>10.5120/8414-9361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the mobile telecommunication systems are growing tremendously all over the
world, the numbers of handheld and base stations are also rapidly growing and
it became very popular to see these base stations distributed everywhere in the
neighborhood and on roof tops which has caused a considerable amount of panic
to the public in Palestine concerning wither the radiated electromagnetic
fields from these base stations may cause any health effect or hazard. Recently
UP High Court in India ordered for removal of BTS towers from residential area,
it has created panic among cellular communication network designers too. Green
cellular networks could be a solution for the above problem. This paper deals
with green cellular networks with the help of multi-layer overlaid hierarchical
structure (macro / micro / pico / femto cells). Macrocell for area coverage,
micro for pedestrian and a slow moving traffic while pico for indoor use and
femto for individual high capacity users. This could be the answer of the
problem of energy conservation and enhancement of spectral density also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3432</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3432</id><created>2012-04-16</created><updated>2014-11-04</updated><authors><author><keyname>Gogacz</keyname><forenames>T.</forenames></author><author><keyname>Marcinkowski</keyname><forenames>J.</forenames></author></authors><title>Converging to the Chase - a Tool for Finite Controllability</title><categories>cs.DB</categories><msc-class>68P15</msc-class><doi>10.1109/LICS.2013.61</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve a problem, stated in [CGP10], showing that Sticky Datalog, defined
in the cited paper as an element of the Datalog\pm project, has the finite
controllability property. In order to do that, we develop a technique, which we
believe can have further applications, of approximating Chase(D, T), for a
database instance D and some sets of tuple generating dependencies T, by an
infinite sequence of finite structures, all of them being models of T.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3436</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3436</id><created>2012-04-16</created><authors><author><keyname>Burjorjee</keyname><forenames>Keki M.</forenames></author></authors><title>Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The
  Hyperclimbing Hypothesis</title><categories>cs.NE cs.AI</categories><comments>22 pages, 5 figures</comments><acm-class>I.2.8; F.2</acm-class><doi>10.1145/2460239.2460244</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hyperclimbing hypothesis is a hypothetical explanation for adaptation in
genetic algorithms with uniform crossover (UGAs). Hyperclimbing is an
intuitive, general-purpose, non-local search heuristic applicable to discrete
product spaces with rugged or stochastic cost functions. The strength of this
heuristic lie in its insusceptibility to local optima when the cost function is
deterministic, and its tolerance for noise when the cost function is
stochastic. Hyperclimbing works by decimating a search space, i.e. by
iteratively fixing the values of small numbers of variables. The hyperclimbing
hypothesis holds that UGAs work by implementing efficient hyperclimbing. Proof
of concept for this hypothesis comes from the use of a novel analytic technique
involving the exploitation of algorithmic symmetry. We have also obtained
experimental results that show that a simple tweak inspired by the
hyperclimbing hypothesis dramatically improves the performance of a UGA on
large, random instances of MAX-3SAT and the Sherrington Kirkpatrick Spin
Glasses problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3447</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3447</id><created>2012-04-16</created><updated>2013-01-15</updated><authors><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Fleming</keyname><forenames>Philip</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Towards Understanding the Fundamentals of Mobility in Cellular Networks</title><categories>cs.NI</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the central role of mobility in wireless networks, analytical study
on its impact on network performance is notoriously difficult. This paper aims
to address this gap by proposing a random waypoint (RWP) mobility model defined
on the entire plane and applying it to analyze two key cellular network
parameters: handover rate and sojourn time. We first analyze the stochastic
properties of the proposed model and compare it to two other models: the
classical RWP mobility model and a synthetic truncated Levy walk model which is
constructed from real mobility trajectories. The comparison shows that the
proposed RWP mobility model is more appropriate for the mobility simulation in
emerging cellular networks, which have ever-smaller cells. Then we apply the
proposed model to cellular networks under both deterministic (hexagonal) and
random (Poisson) base station (BS) models. We present analytic expressions for
both handover rate and sojourn time, which have the expected property that the
handover rate is proportional to the square root of BS density. Compared to an
actual BS distribution, we find that the Poisson-Voronoi model is about as
accurate in terms of mobility evaluation as hexagonal model, though being more
pessimistic in that it predicts a higher handover rate and lower sojourn time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3453</identifier>
 <datestamp>2012-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3453</id><created>2012-04-16</created><updated>2012-07-10</updated><authors><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author></authors><title>There is No Deadline - Time Evolution of Wikipedia Discussions</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>10 pages, 7 figures; Proceedings of WikiSym 2012</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia articles are by definition never finished: at any moment their
content can be edited, or discussed in the associated talk pages. In this study
we analyse the evolution of these discussions to unveil patterns of collective
participation along the temporal dimension, and to shed light on the process of
content creation on different topics. At a micro-scale, we investigate peaks in
the discussion activity and we observe a non-trivial relationship with edit
activity. At a larger scale, we introduce a measure to account for how fast
discussions grow in complexity, and we find speeds that span three orders of
magnitude for different articles. Our analysis should help the community in
tasks such as early detection of controversies and assessment of discussion
maturity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3457</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3457</id><created>2012-04-16</created><authors><author><keyname>Blohm</keyname><forenames>Ivo</forenames></author><author><keyname>Riedl</keyname><forenames>Christoph</forenames></author><author><keyname>F&#xfc;ller</keyname><forenames>Johann</forenames></author><author><keyname>K&#xf6;roglu</keyname><forenames>Orhan</forenames></author><author><keyname>Leimeister</keyname><forenames>Jan Marco</forenames></author><author><keyname>Krcmar</keyname><forenames>Helmut</forenames></author></authors><title>The Effects of Prediction Market Design and Price Elasticity on Trading
  Performance of Users: An Experimental Analysis</title><categories>cs.SI q-fin.GN</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/77</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We employ a 2x3 factorial experiment to study two central factors in the
design of prediction markets (PMs) for idea evaluation: the overall design of
the PM, and the elasticity of market prices set by a market maker. The results
show that 'multi-market designs' on which each contract is traded on a separate
PM lead to significantly higher trading performance than 'single-markets' that
handle all contracts one on PM. Price elasticity has no direct effect on
trading performance, but a significant interaction effect with market design
implies that the performance difference between the market designs is highest
in settings of moderate price elasticity. We contribute to the emerging
research stream of PM design through an unprecedented experiment which compares
current market designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3458</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3458</id><created>2012-04-16</created><authors><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>The logic of quantum mechanics - Take II</title><categories>quant-ph cs.CL cs.LO math.CT math.LO</categories><comments>23 pages, many figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We put forward a new take on the logic of quantum mechanics, following
Schroedinger's point of view that it is composition which makes quantum theory
what it is, rather than its particular propositional structure due to the
existence of superpositions, as proposed by Birkhoff and von Neumann. This
gives rise to an intrinsically quantitative kind of logic, which truly deserves
the name `logic' in that it also models meaning in natural language, the latter
being the origin of logic, that it supports automation, the most prominent
practical use of logic, and that it supports probabilistic inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3463</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3463</id><created>2012-04-16</created><authors><author><keyname>Mavrodiev</keyname><forenames>Pavlin</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author></authors><title>Effects of Social Influence on the Wisdom of Crowds</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/61</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wisdom of crowds refers to the phenomenon that the aggregate prediction or
forecast of a group of individuals can be surprisingly more accurate than most
individuals in the group, and sometimes - than any of the individuals
comprising it. This article models the impact of social influence on the wisdom
of crowds. We build a minimalistic representation of individuals as Brownian
particles coupled by means of social influence. We demonstrate that the model
can reproduce results of a previous empirical study. This allows us to draw
more fundamental conclusions about the role of social influence: In particular,
we show that the question of whether social influence has a positive or
negative net effect on the wisdom of crowds is ill-defined. Instead, it is the
starting configuration of the population, in terms of its diversity and
accuracy, that directly determines how beneficial social influence actually is.
The article further examines the scenarios under which social influence
promotes or impairs the wisdom of crowds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3471</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3471</id><created>2012-04-16</created><authors><author><keyname>Raj</keyname><forenames>Arockia Anand</forenames></author><author><keyname>Mala</keyname><forenames>T.</forenames></author></authors><title>Cloudpress 2.0: A MapReduce Approach for News Retrieval on the Cloud</title><categories>cs.DC cs.IR</categories><comments>17 pages, 12 figures</comments><acm-class>H.3.1; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this era of the Internet, the amount of news articles added every minute
of everyday is humongous. As a result of this explosive amount of news
articles, news retrieval systems are required to process the news articles
frequently and intensively. The news retrieval systems that are in-use today
are not capable of coping up with these data-intensive computations. Cloudpress
2.0 presented here, is designed and implemented to be scalable, robust and
fault tolerant. It is designed in such a way that, all the processes involved
in news retrieval such as fetching, pre-processing, indexing, storing and
summarizing, exploit MapReduce paradigm and use the power of the Cloud
computing. It uses novel approaches for parallel processing, for storing the
news articles in a distributed database and for visualizing them as a 3D
visual. It uses Lucene-based indexing for efficient and faster retrieval. It
also includes a novel query expansion feature for searching the news articles.
Cloudpress 2.0 also allows on-the-fly, extractive summarization of news
articles based on the input query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3481</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3481</id><created>2012-04-16</created><authors><author><keyname>Morris</keyname><forenames>Robert R.</forenames></author><author><keyname>Picard</keyname><forenames>Rosalind</forenames></author></authors><title>Crowdsourcing Collective Emotional Intelligence</title><categories>cs.SI cs.HC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/81</report-no><acm-class>J.4; D.0; H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the hallmarks of emotional intelligence is the ability to regulate
emotions. Research suggests that cognitive reappraisal - a technique that
involves reinterpreting the meaning of a thought or situation - can
down-regulate negative emotions, without incurring significant psychological or
physiological costs. Habitual use of this strategy is also linked to many key
indices of physical and emotional health. Unfortunately, this technique is not
always easy to apply. Thinking flexibly about stressful thoughts and situations
requires creativity and poise, faculties that often elude us when we need them
the most. In this paper, we propose an assistive technology that coordinates
collective intelligence on demand, to help individuals reappraise stressful
thoughts and situations. In two experiments, we assess key features of our
design and we demonstrate the feasibility of crowdsourcing empathetic
reappraisals with on demand workforces, such as Amazon's Mechanical Turk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3488</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3488</id><created>2012-04-16</created><updated>2014-01-28</updated><authors><author><keyname>da Fonseca</keyname><forenames>Guilherme D.</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Celina M. H.</forenames></author><author><keyname>de S&#xe1;</keyname><forenames>Vin&#xed;cius G. P.</forenames></author><author><keyname>Machado</keyname><forenames>Raphael</forenames></author></authors><title>Efficient sub-5 approximations for minimum dominating sets in unit disk
  graphs</title><categories>cs.DS</categories><comments>An extended abstract of this paper appeared in the proceedings of the
  10th Workshop on Approximation and Online Algorithms (WAOA 2012), Lecture
  Notes in Computer Science 7846 (2013), pp. 82-92</comments><doi>10.1016/j.tcs.2014.01.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A unit disk graph is the intersection graph of n congruent disks in the
plane. Dominating sets in unit disk graphs are widely studied due to their
application in wireless ad-hoc networks. Because the minimum dominating set
problem for unit disk graphs is NP-hard, numerous approximation algorithms have
been proposed in the literature, including some PTAS. However, since the
proposal of a linear-time 5-approximation algorithm in 1995, the lack of
efficient algorithms attaining better approximation factors has aroused
attention. We introduce a linear-time O(n+m) approximation algorithm that takes
the usual adjacency representation of the graph as input and outputs a
44/9-approximation. This approximation factor is also attained by a second
algorithm, which takes the geometric representation of the graph as input and
runs in O(n log n) time regardless of the number of edges. Additionally, we
propose a 43/9-approximation which can be obtained in O(n^2 m) time given only
the graph's adjacency representation. It is noteworthy that the dominating sets
obtained by our algorithms are also independent sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3491</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3491</id><created>2012-04-16</created><authors><author><keyname>Xiao</keyname><forenames>Lu</forenames></author></authors><title>Rationale awareness for quality assurance in iterative human computation
  processes</title><categories>cs.HC cs.SI</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/84</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human computation refers to the outsourcing of computation tasks to human
workers. It offers a new direction for solving a variety of problems and calls
for innovative ways of managing human computation processes. The majority of
human computation tasks take a parallel approach, whereas the potential of an
iterative approach, i.e., having workers iteratively build on each other's
work, has not been sufficiently explored. This study investigates whether and
how human workers' awareness of previous workers' rationales affects the
performance of the iterative approach in a brainstorming task and a rating
task. Rather than viewing this work as a conclusive piece, the author believes
that this research endeavor is just the beginning of a new research focus that
examines and supports meta-cognitive processes in crowdsourcing activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3494</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3494</id><created>2012-04-16</created><authors><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author><author><keyname>Friedman</keyname><forenames>Eric J.</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Optimizing Scrip Systems: Crashes, Altruists, Hoarders, Sybils and
  Collusion</title><categories>cs.GT cs.DC</categories><comments>24 pages. Forthcoming in Distributed Computing. Premliminary versions
  of this material appeared in the Proceedings of the 7th and 8th ACM
  Conferences on Electronic Commerce [14,26] and the Proceedings of the First
  Conference on Auctions, Market Mechanisms and Their Applications [27]. These
  are also available as arXiv:0705.4094, arXiv:0705.4110, and arXiv:0903.2278</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scrip, or artificial currency, is a useful tool for designing systems that
are robust to selfish behavior by users. However, it also introduces problems
for a system designer, such as how the amount of money in the system should be
set. In this paper, the effect of varying the total amount of money in a scrip
system on efficiency (i.e., social welfare---the total utility of all the
agents in the system) is analyzed, and it is shown that by maintaining the
appropriate ratio between the total amount of money and the number of agents,
efficiency is maximized. This ratio can be found by increasing the money supply
to just below the point that the system would experience a &quot;monetary crash,&quot;
where money is sufficiently devalued that no agent is willing to perform a
service. The implications of the presence of altruists, hoarders, sybils, and
collusion on the performance of the system are examined. Approaches are
discussed to identify the strategies and types of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3495</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3495</id><created>2012-04-16</created><authors><author><keyname>Dyrkolbotn</keyname><forenames>Sjur</forenames></author><author><keyname>Ka&#x17a;mierczak</keyname><forenames>Piotr</forenames></author><author><keyname>Parmann</keyname><forenames>Erik</forenames></author><author><keyname>Pedersen</keyname><forenames>Truls</forenames></author></authors><title>No big deal: introducing roles to reduce the size of ATL models</title><categories>cs.LO cs.MA</categories><comments>Accepted for presentation at LAMAS 2012 workshop on June 5, 2012 in
  Valencia, Spain</comments><msc-class>68T27, 68T42</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the following paper we present a new semantics for the well-known
strategic logic ATL. It is based on adding roles to concurrent game structures,
that is at every state, each agent belongs to exactly one role, and the role
specifies what actions are available to him at that state. We show advantages
of the new semantics, analyze model checking complexity and prove equivalence
between standard ATL semantics and our new approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3498</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3498</id><created>2012-04-16</created><updated>2012-04-17</updated><authors><author><keyname>Qazvinian</keyname><forenames>Vahed</forenames></author><author><keyname>Radev</keyname><forenames>Dragomir R.</forenames></author></authors><title>A Computational Analysis of Collective Discourse</title><categories>cs.SI cs.CL physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/59</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is focused on the computational analysis of collective discourse,
a collective behavior seen in non-expert content contributions in online social
media. We collect and analyze a wide range of real-world collective discourse
datasets from movie user reviews to microblogs and news headlines to scientific
citations. We show that all these datasets exhibit diversity of perspective, a
property seen in other collective systems and a criterion in wise crowds. Our
experiments also confirm that the network of different perspective
co-occurrences exhibits the small-world property with high clustering of
different perspectives. Finally, we show that non-expert contributions in
collective discourse can be used to answer simple questions that are otherwise
hard to answer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3511</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3511</id><created>2012-04-16</created><authors><author><keyname>Della Penna</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Reid</keyname><forenames>Mark D.</forenames></author></authors><title>Crowd &amp; Prejudice: An Impossibility Theorem for Crowd Labelling without
  a Gold Standard</title><categories>cs.SI cs.GT</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/33</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common use of crowd sourcing is to obtain labels for a dataset. Several
algorithms have been proposed to identify uninformative members of the crowd so
that their labels can be disregarded and the cost of paying them avoided. One
common motivation of these algorithms is to try and do without any initial set
of trusted labeled data. We analyse this class of algorithms as mechanisms in a
game-theoretic setting to understand the incentives they create for workers. We
find an impossibility result that without any ground truth, and when workers
have access to commonly shared 'prejudices' upon which they agree but are not
informative of true labels, there is always equilibria where all agents report
the prejudice. A small amount amount of gold standard data is found to be
sufficient to rule out these equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3513</identifier>
 <datestamp>2012-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3513</id><created>2012-04-16</created><updated>2012-09-17</updated><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>Clarke</keyname><forenames>Edmund</forenames></author></authors><title>Delta-Complete Decision Procedures for Satisfiability over the Reals</title><categories>cs.LO cs.SC</categories><comments>A shorter version appears in IJCAR 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of &quot;\delta-complete decision procedures&quot; for solving
SMT problems over the real numbers, with the aim of handling a wide range of
nonlinear functions including transcendental functions and solutions of
Lipschitz-continuous ODEs. Given an SMT problem \varphi and a positive rational
number \delta, a \delta-complete decision procedure determines either that
\varphi is unsatisfiable, or that the &quot;\delta-weakening&quot; of \varphi is
satisfiable. Here, the \delta-weakening of \varphi is a variant of \varphi that
allows \delta-bounded numerical perturbations on \varphi. We prove the
existence of \delta-complete decision procedures for bounded SMT over reals
with functions mentioned above. For functions in Type 2 complexity class C,
under mild assumptions, the bounded \delta-SMT problem is in NP^C.
\delta-Complete decision procedures can exploit scalable numerical methods for
handling nonlinearity, and we propose to use this notion as an ideal
requirement for numerically-driven decision procedures. As a concrete example,
we formally analyze the DPLL&lt;ICP&gt; framework, which integrates Interval
Constraint Propagation (ICP) in DPLL(T), and establish necessary and sufficient
conditions for its \delta-completeness. We discuss practical applications of
\delta-complete decision procedures for correctness-critical applications
including formal verification and theorem proving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3514</identifier>
 <datestamp>2012-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3514</id><created>2012-04-16</created><updated>2012-05-25</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Fine</keyname><forenames>Shai</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Distributed Learning, Communication Complexity and Privacy</title><categories>cs.LG cs.DS</categories><comments>19 pages</comments><acm-class>F.2.2; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of PAC-learning from distributed data and analyze
fundamental communication complexity questions involved. We provide general
upper and lower bounds on the amount of communication needed to learn well,
showing that in addition to VC-dimension and covering number, quantities such
as the teaching-dimension and mistake-bound of a class play an important role.
We also present tight results for a number of common concept classes including
conjunctions, parity functions, and decision lists. For linear separators, we
show that for non-concentrated distributions, we can use a version of the
Perceptron algorithm to learn with much less communication than the number of
updates given by the usual margin bound. We also show how boosting can be
performed in a generic manner in the distributed setting to achieve
communication with only logarithmic dependence on 1/epsilon for any concept
class, and demonstrate how recent work on agnostic learning from
class-conditional queries can be used to achieve low communication in agnostic
settings as well. We additionally present an analysis of privacy, considering
both differential privacy and a notion of distributional privacy that is
especially appealing in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3516</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3516</id><created>2012-04-16</created><authors><author><keyname>Sun</keyname><forenames>Yu-An</forenames></author><author><keyname>Dance</keyname><forenames>Christopher</forenames></author></authors><title>When majority voting fails: Comparing quality assurance methods for
  noisy human computation environment</title><categories>cs.SI cs.AI</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/56</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality assurance remains a key topic in human computation research. Prior
work indicates that majority voting is effective for low difficulty tasks, but
has limitations for harder tasks. This paper explores two methods of addressing
this problem: tournament selection and elimination selection, which exploit 2-,
3- and 4-way comparisons between different answers to human computation tasks.
Our experimental results and statistical analyses show that both methods
produce the correct answer in noisy human computation environment more often
than majority voting. Furthermore, we find that the use of 4-way comparisons
can significantly reduce the cost of quality assurance relative to the use of
2-way comparisons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3523</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3523</id><created>2012-04-16</created><authors><author><keyname>Daume</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Saha</keyname><forenames>Avishek</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Efficient Protocols for Distributed Classification and Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed learning, the goal is to perform a learning task over data
distributed across multiple nodes with minimal (expensive) communication. Prior
work (Daume III et al., 2012) proposes a general model that bounds the
communication required for learning classifiers while allowing for $\eps$
training error on linearly separable data adversarially distributed across
nodes.
  In this work, we develop key improvements and extensions to this basic model.
Our first result is a two-party multiplicative-weight-update based protocol
that uses $O(d^2 \log{1/\eps})$ words of communication to classify distributed
data in arbitrary dimension $d$, $\eps$-optimally. This readily extends to
classification over $k$ nodes with $O(kd^2 \log{1/\eps})$ words of
communication. Our proposed protocol is simple to implement and is considerably
more efficient than baselines compared, as demonstrated by our empirical
results.
  In addition, we illustrate general algorithm design paradigms for doing
efficient learning over distributed data. We show how to solve
fixed-dimensional and high dimensional linear programming efficiently in a
distributed setting where constraints may be distributed across nodes. Since
many learning problems can be viewed as convex optimization problems where
constraints are generated by individual points, this models many typical
distributed learning scenarios. Our techniques make use of a novel connection
from multipass streaming, as well as adapting the multiplicative-weight-update
framework more generally to a distributed setting. As a consequence, our
methods extend to the wide range of problems solvable using these techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3529</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3529</id><created>2012-04-16</created><updated>2014-03-11</updated><authors><author><keyname>Boros</keyname><forenames>Endre</forenames></author><author><keyname>Gruber</keyname><forenames>Aritanan</forenames></author></authors><title>Hardness Results for Approximate Pure Horn CNF Formulae Minimization</title><categories>cs.CC cs.AI</categories><comments>39 pages, 1 figure</comments><msc-class>06E30, 68Q17, 68R01, 68T01, 68T27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the hardness of approximation of clause minimum and literal minimum
representations of pure Horn functions in $n$ Boolean variables. We show that
unless P=NP, it is not possible to approximate in polynomial time the minimum
number of clauses and the minimum number of literals of pure Horn CNF
representations to within a factor of $2^{\log^{1-o(1)} n}$. This is the case
even when the inputs are restricted to pure Horn 3-CNFs with
$O(n^{1+\varepsilon})$ clauses, for some small positive constant $\varepsilon$.
Furthermore, we show that even allowing sub-exponential time computation, it is
still not possible to obtain constant factor approximations for such problems
unless the Exponential Time Hypothesis turns out to be false.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3534</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3534</id><created>2012-04-16</created><authors><author><keyname>Varshney</keyname><forenames>Lav R.</forenames></author></authors><title>Toward a Comparative Cognitive History: Archimedes and D. H. J. Polymath</title><categories>cs.SI math.HO physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/91</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is collective intelligence just individual intelligence writ large, or are
there fundamental differences? This position paper argues that a cognitive
history methodology can shed light into the nature of collective intelligence
and its differences from individual intelligence. To advance this proposed area
of research, a small case study on the structure of argument and proof is
presented. Quantitative metrics from network science are used to compare the
artifacts of deduction from two sources. The first is the work of Archimedes of
Syracuse, putatively an individual, and of other ancient Greek mathematicians.
The second is work of the Polymath Project, a massively collaborative
mathematics project that used blog posts and comments to prove new results in
combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3543</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3543</id><created>2012-04-16</created><authors><author><keyname>Sahito</keyname><forenames>Farhan</forenames></author><author><keyname>Slany</keyname><forenames>Wolfgang</forenames></author></authors><title>Functional Magnetic Resonance Imaging and the Challenge of Balancing
  Human Security with State Security</title><categories>cs.CR</categories><journal-ref>Human Security Perspectives 1, 2012 (1):38--66</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent reports reveal that violent extremists are trying to obtain insider
positions that may increase the impact of any attack on critical infrastructure
and could potentially endanger state services, people's lives and even
democracy. It is of utmost importance to be able to adopt extreme security
measures in certain high-risk situations in order to secure critical
infrastructure and thus lower the level of terrorist threats while preserving
the rights of citizens. To counter these threats, our research is aiming for
extreme measures to analyse and evaluate human threats related assessment
methods for employee screening and evaluations using cognitive analysis
technology, in particular functional Magnetic Resonance Imaging (fMRI). The
development of fMRI has led some researchers to conclude that this technology
has forensic potential and may be useful in investing personality traits,
mental illness, psychopathology, racial prejudice and religious extremism.
However, critics claim that this technology may present many new human rights
and ethical dilemmas and could result in potentially disastrous outcomes. The
main thrust of the research is to counter above concerns and harmful
consequences by presenting a set of ethical and professional guidelines that
will substantially reduce the risk of unethical use of this technology. The
significance of this research is to ensure the limits of the
state/organisation's right to peer into an individual's thought process with
and without consent, to define the parameters of a person's right to ensure
that fMRI scans do not pose more than an appropriate threat to cognitive
liberty, and the proper use of such information in civil, forensic and security
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3549</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3549</id><created>2012-04-16</created><updated>2012-10-08</updated><authors><author><keyname>Brinkmann</keyname><forenames>Gunnar</forenames></author><author><keyname>Coolsaet</keyname><forenames>Kris</forenames></author><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>Melot</keyname><forenames>Hadrien</forenames></author></authors><title>House of Graphs: a database of interesting graphs</title><categories>math.CO cs.DM</categories><comments>8 pages; added a figure</comments><journal-ref>Discrete Appl. Math. 161 (2013) 311-314</journal-ref><doi>10.1016/j.dam.2012.07.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we present House of Graphs (http://hog.grinvin.org) which is a
new database of graphs. The key principle is to have a searchable database and
offer -- next to complete lists of some graph classes -- also a list of special
graphs that already turned out to be interesting and relevant in the study of
graph theoretic problems or as counterexamples to conjectures. This list can be
extended by users of the database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3553</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3553</id><created>2012-04-16</created><authors><author><keyname>Lawrence</keyname><forenames>B. N.</forenames></author><author><keyname>Bennett</keyname><forenames>V.</forenames></author><author><keyname>Churchill</keyname><forenames>J.</forenames></author><author><keyname>Juckes</keyname><forenames>M.</forenames></author><author><keyname>Kershaw</keyname><forenames>P.</forenames></author><author><keyname>Oliver</keyname><forenames>P.</forenames></author><author><keyname>Pritchard</keyname><forenames>M.</forenames></author><author><keyname>Stephens</keyname><forenames>A.</forenames></author></authors><title>The JASMIN super-data-cluster</title><categories>cs.DC physics.comp-ph physics.geo-ph</categories><comments>Submitted to Supercomputing 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The JASMIN super-data-cluster is being deployed to support the data analysis
requirements of the UK and European climate and earth system modelling
community. Physical colocation of the core JASMIN resource with significant
components of the facility for Climate and Environmental Monitoring from Space
(CEMS) provides additional support for the earth observation community, as well
as facilitating further comparison and evaluation of models with data. JASMIN
and CEMS together centrally deploy 9.3 PB of storage - 4.6 PB of Panasas fast
disk storage alongside the STFC Atlas Tape Store. Over 370 computing cores
provide local computation. Remote JASMIN resources at Bristol, Leeds and
Reading provide additional distributed storage and compute configured to
support local workflow as a stepping stone to using the central JASMIN system.
Fast network links from JASMIN provide reliable communication between the UK
supercomputers MONSooN (at the Met Office) and HECToR (at the University of
Edinburgh). JASMIN also supports European users via a light path to KNMI in the
Netherlands. The functional components of the JASMIN infrastructure have been
designed to support and integrate workflows for three main goals: (1) the
efficient operation of data curation and facilitation at the STFC Centre for
Environmental Data Archival; (2) efficient data analysis by the UK and European
climate and earth system science communities, and; (3) flexible access for the
climate impacts and earth observation communities to complex data and
concomitant services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3554</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3554</id><created>2012-04-16</created><updated>2012-06-01</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author></authors><title>Robust stability and stabilization of uncertain linear positive systems
  via Integral Linear Constraints: L1- and Linfinity-gains characterization</title><categories>cs.SY math.CA math.DS math.OC</categories><comments>Accepted in the International Journal of Robust and Nonlinear Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Copositive linear Lyapunov functions are used along with dissipativity theory
for stability analysis and control of uncertain linear positive systems. Unlike
usual results on linear systems, linear supply-rates are employed here for
robustness and performance analysis using L1- and Linfinity-gains. Robust
stability analysis is performed using Integral Linear Constraints (ILCs) for
which several classes of uncertainties are discussed. The approach is then
extended to robust stabilization and performance optimization. The obtained
results are expressed in terms of robust linear programming problems that are
equivalently turned into finite dimensional ones using Handelman's Theorem.
Several examples are provided for illustration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3569</identifier>
 <datestamp>2012-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3569</id><created>2012-04-16</created><updated>2012-08-01</updated><authors><author><keyname>Librino</keyname><forenames>Federico</forenames></author><author><keyname>Levorato</keyname><forenames>Marco</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>An Algorithmic Solution for Computing Circle Intersection Areas and its
  Applications to Wireless Communications</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel iterative algorithm for the efficient computation of the intersection
areas of an arbitrary number of circles is presented. The algorithm, based on a
trellis-structure, hinges on two geometric results which allow the
existence-check and the computation of the area of the intersection regions
generated by more than three circles by simple algebraic manipulations of the
intersection areas of a smaller number of circles. The presented algorithm is a
powerful tool for the performance analysis of wireless networks, and finds many
applications, ranging from sensor to cellular networks. As an example of
practical application, an insightful study of the uplink outage probability of
in a wireless network with cooperative access points as a function of the
transmission power and access point density is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3581</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3581</id><created>2012-04-16</created><authors><author><keyname>Grossi</keyname><forenames>Roberto</forenames></author><author><keyname>Ottaviano</keyname><forenames>Giuseppe</forenames></author></authors><title>The Wavelet Trie: Maintaining an Indexed Sequence of Strings in
  Compressed Space</title><categories>cs.DS cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An indexed sequence of strings is a data structure for storing a string
sequence that supports random access, searching, range counting and analytics
operations, both for exact matches and prefix search. String sequences lie at
the core of column-oriented databases, log processing, and other storage and
query tasks. In these applications each string can appear several times and the
order of the strings in the sequence is relevant. The prefix structure of the
strings is relevant as well: common prefixes are sought in strings to extract
interesting features from the sequence. Moreover, space-efficiency is highly
desirable as it translates directly into higher performance, since more data
can fit in fast memory.
  We introduce and study the problem of compressed indexed sequence of strings,
representing indexed sequences of strings in nearly-optimal compressed space,
both in the static and dynamic settings, while preserving provably good
performance for the supported operations.
  We present a new data structure for this problem, the Wavelet Trie, which
combines the classical Patricia Trie with the Wavelet Tree, a succinct data
structure for storing a compressed sequence. The resulting Wavelet Trie
smoothly adapts to a sequence of strings that changes over time. It improves on
the state-of-the-art compressed data structures by supporting a dynamic
alphabet (i.e. the set of distinct strings) and prefix queries, both crucial
requirements in the aforementioned applications, and on traditional indexes by
reducing space occupancy to close to the entropy of the sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3596</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3596</id><created>2012-04-16</created><authors><author><keyname>Spiro</keyname><forenames>Ian</forenames></author><author><keyname>Huston</keyname><forenames>Thomas</forenames></author><author><keyname>Bregler</keyname><forenames>Christoph</forenames></author></authors><title>Markerless Motion Capture in the Crowd</title><categories>cs.SI cs.HC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/51</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work uses crowdsourcing to obtain motion capture data from video
recordings. The data is obtained by information workers who click repeatedly to
indicate body configurations in the frames of a video, resulting in a model of
2D structure over time. We discuss techniques to optimize the tracking task and
strategies for maximizing accuracy and efficiency. We show visualizations of a
variety of motions captured with our pipeline then apply reconstruction
techniques to derive 3D structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3598</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3598</id><created>2012-04-16</created><authors><author><keyname>Murthy</keyname><forenames>Dhiraj</forenames></author><author><keyname>Gross</keyname><forenames>Alexander</forenames></author><author><keyname>Bond</keyname><forenames>Stephanie</forenames></author></authors><title>Visualizing Collective Discursive User Interactions in Online Life
  Science Communities</title><categories>cs.SI</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/6</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper highlights the rationale for the development of BioViz, a tool to
help visualize the existence of collective user interactions in online life
science communities. The first community studied has approximately 22,750
unique users and the second has 35,000. Making sense of the number of
interactions between actors in these networks in order to discern patterns of
collective organization and intelligent behavior is challenging. One of the
complications is that forums - our object of interest - can vary in their
purpose and remit (e.g. the role of gender in the life sciences to forums of
praxis such as one exploring the cell line culturing) and this shapes the
structure of the forum organization itself. Our approach took a random sample
of 53 forums which were manually analyzed by our research team and interactions
between actors were recorded as arcs between nodes. The paper focuses on a
discussion of the utility of our approach, but presents some brief results to
highlight the forms of knowledge that can be gained in identifying collective
group formations. Specifically, we found that by using a matrix-based
visualization approach, we were able to see patterns of collective behavior
which we believe is valuable both to the study of collective intelligence and
the design of virtual organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3600</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3600</id><created>2012-04-16</created><updated>2012-11-10</updated><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Kollias</keyname><forenames>Giorgos</forenames></author><author><keyname>Kais</keyname><forenames>Sabre</forenames></author></authors><title>Universal Programmable Quantum Circuit Schemes to Emulate an Operator</title><categories>quant-ph cs.OH</categories><comments>combined with former arXiv:1207.1740</comments><journal-ref>J. Chem. Phys. 137, 234112 (2012)</journal-ref><doi>10.1063/1.4772185</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike fixed designs, programmable circuit designs support an infinite number
of operators. The functionality of a programmable circuit can be altered by
simply changing the angle values of the rotation gates in the circuit. Here, we
present a new quantum circuit design technique resulting in two general
programmable circuit schemes. The circuit schemes can be used to simulate any
given operator by setting the angle values in the circuit. This provides a
fixed circuit design whose angles are determined from the elements of the given
matrix-which can be non-unitary-in an efficient way. We also give both the
classical and quantum complexity analysis for these circuits and show that the
circuits require a few classical computations. They have almost the same
quantum complexities as non-general circuits. Since the presented circuit
designs are independent from the matrix decomposition techniques and the global
optimization processes used to find quantum circuits for a given operator, high
accuracy simulations can be done for the unitary propagators of molecular
Hamiltonians on quantum computers. As an example, we show how to build the
circuit design for the hydrogen molecule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3611</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3611</id><created>2012-04-16</created><authors><author><keyname>Ertekin</keyname><forenames>Seyda</forenames></author><author><keyname>Hirsh</keyname><forenames>Haym</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author></authors><title>Learning to Predict the Wisdom of Crowds</title><categories>cs.SI cs.LG</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/62</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of &quot;approximating the crowd&quot; is that of estimating the crowd's
majority opinion by querying only a subset of it. Algorithms that approximate
the crowd can intelligently stretch a limited budget for a crowdsourcing task.
We present an algorithm, &quot;CrowdSense,&quot; that works in an online fashion to
dynamically sample subsets of labelers based on an exploration/exploitation
criterion. The algorithm produces a weighted combination of a subset of the
labelers' votes that approximates the crowd's opinion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3616</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3616</id><created>2012-04-16</created><authors><author><keyname>Barbu</keyname><forenames>Andrei</forenames></author><author><keyname>Bridge</keyname><forenames>Alexander</forenames></author><author><keyname>Coroian</keyname><forenames>Dan</forenames></author><author><keyname>Dickinson</keyname><forenames>Sven</forenames></author><author><keyname>Mussman</keyname><forenames>Sam</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Siddharth</forenames></author><author><keyname>Salvi</keyname><forenames>Dhaval</forenames></author><author><keyname>Schmidt</keyname><forenames>Lara</forenames></author><author><keyname>Shangguan</keyname><forenames>Jiangnan</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author><author><keyname>Waggoner</keyname><forenames>Jarrell</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author><author><keyname>Wei</keyname><forenames>Jinlian</forenames></author><author><keyname>Yin</keyname><forenames>Yifan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiqi</forenames></author></authors><title>Large-Scale Automatic Labeling of Video Events with Verbs Based on
  Event-Participant Interaction</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to labeling short video clips with English verbs as
event descriptions. A key distinguishing aspect of this work is that it labels
videos with verbs that describe the spatiotemporal interaction between event
participants, humans and objects interacting with each other, abstracting away
all object-class information and fine-grained image characteristics, and
relying solely on the coarse-grained motion of the event participants. We apply
our approach to a large set of 22 distinct verb classes and a corpus of 2,584
videos, yielding two surprising outcomes. First, a classification accuracy of
greater than 70% on a 1-out-of-22 labeling task and greater than 85% on a
variety of 1-out-of-10 subsets of this labeling task is independent of the
choice of which of two different time-series classifiers we employ. Second, we
achieve this level of accuracy using a highly impoverished intermediate
representation consisting solely of the bounding boxes of one or two event
participants as a function of time. This indicates that successful event
recognition depends more on the choice of appropriate features that
characterize the linguistic invariants of the event classes than on the
particular classifier algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3618</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3618</id><created>2012-04-13</created><authors><author><keyname>Tofighi</keyname><forenames>Mohammad</forenames></author><author><keyname>Ayremlou</keyname><forenames>Ali</forenames></author><author><keyname>Marvasti</keyname><forenames>Farokh</forenames></author></authors><title>Compensating Interpolation Distortion by Using New Optimized Modular
  Method</title><categories>cs.CV cs.MM</categories><comments>7 pages. Journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A modular method was suggested before to recover a band limited signal from
the sample and hold and linearly interpolated (or, in general, an
nth-order-hold) version of the regular samples. In this paper a novel approach
for compensating the distortion of any interpolation based on modular method
has been proposed. In this method the performance of the modular method is
optimized by adding only some simply calculated coefficients. This approach
causes drastic improvement in terms of signal-to-noise ratios with fewer
modules compared to the classical modular method. Simulation results clearly
confirm the improvement of the proposed method and also its superior robustness
against additive noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3658</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3658</id><created>2012-04-16</created><authors><author><keyname>Yang</keyname><forenames>En-Hui</forenames></author><author><keyname>Meng</keyname><forenames>Jin</forenames></author></authors><title>Jar Decoding: Non-Asymptotic Converse Coding Theorems, Taylor-Type
  Expansion, and Optimality</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transaction on Information Theory in April, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a new decoding rule called jar decoding was proposed; under jar
decoding, a non-asymptotic achievable tradeoff between the coding rate and word
error probability was also established for any discrete input memoryless
channel with discrete or continuous output (DIMC). Along the path of
non-asymptotic analysis, in this paper, it is further shown that jar decoding
is actually optimal up to the second order coding performance by establishing
new non-asymptotic converse coding theorems, and determining the Taylor
expansion of the (best) coding rate $R_n (\epsilon)$ of finite block length for
any block length $n$ and word error probability $\epsilon$ up to the second
order. Finally, based on the Taylor-type expansion and the new converses, two
approximation formulas for $R_n (\epsilon)$ (dubbed &quot;SO&quot; and &quot;NEP&quot;) are
provided; they are further evaluated and compared against some of the best
bounds known so far, as well as the normal approximation of $R_n (\epsilon)$
revisited recently in the literature. It turns out that while the normal
approximation is all over the map, i.e. sometime below achievable bounds and
sometime above converse bounds, the SO approximation is much more reliable as
it is always below converses; in the meantime, the NEP approximation is the
best among the three and always provides an accurate estimation for $R_n
(\epsilon)$. An important implication arising from the Taylor-type expansion of
$R_n (\epsilon)$ is that in the practical non-asymptotic regime, the optimal
marginal codeword symbol distribution is not necessarily a capacity achieving
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3661</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3661</id><created>2012-04-16</created><authors><author><keyname>Yang</keyname><forenames>En-Hui</forenames></author><author><keyname>Meng</keyname><forenames>Jin</forenames></author></authors><title>Non-asymptotic Equipartition Properties for Independent and Identically
  Distributed Sources</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transaction on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an independent and identically distributed source $X = \{X_i
\}_{i=1}^{\infty}$ with finite Shannon entropy or differential entropy (as the
case may be) $H(X)$, the non-asymptotic equipartition property (NEP) with
respect to $H(X)$ is established, which characterizes, for any finite block
length $n$, how close $-{1\over n} \ln p(X_1 X_2...X_n)$ is to $H(X)$ by
determining the information spectrum of $X_1 X_2...X_n $, i.e., the
distribution of $-{1\over n} \ln p(X_1 X_2...X_n)$. Non-asymptotic
equipartition properties (with respect to conditional entropy, mutual
information, and relative entropy) in a similar nature are also established.
These non-asymptotic equipartition properties are instrumental to the
development of non-asymptotic coding (including both source and channel coding)
results in information theory in the same way as the asymptotic equipartition
property to all asymptotic coding theorems established so far in information
theory. As an example, the NEP with respect to $H(X)$ is used to establish a
non-asymptotic fixed rate source coding theorem, which reveals, for any finite
block length $n$, a complete picture about the tradeoff between the minimum
rate of fixed rate coding of $X_1...X_n$ and error probability when the error
probability is a constant, or goes to 0 with block length $n$ at a
sub-polynomial, polynomial or sub-exponential speed. With the help of the NEP
with respect to other information quantities, non-asymptotic channel coding
theorems of similar nature will be established in a separate paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3663</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3663</id><created>2012-04-16</created><authors><author><keyname>Peng</keyname><forenames>Huan-Kai</forenames></author><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Pirolli</keyname><forenames>Peter</forenames></author><author><keyname>Hogg</keyname><forenames>Tad</forenames></author></authors><title>Thermodynamic Principles in Social Collaborations</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/55</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A thermodynamic framework is presented to characterize the evolution of
efficiency, order, and quality in social content production systems, and this
framework is applied to the analysis of Wikipedia. Contributing editors are
characterized by their (creative) energy levels in terms of number of edits. We
develop a definition of entropy that can be used to analyze the efficiency of
the system as a whole, and relate it to the evolution of power-law
distributions and a metric of quality. The concept is applied to the analysis
of eight years of Wikipedia editing data and results show that (1) Wikipedia
has become more efficient during its evolution and (2) the entropy-based
efficiency metric has high correlation with observed readership of Wikipedia
pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3673</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3673</id><created>2012-04-16</created><authors><author><keyname>Roberts</keyname><forenames>Michael E.</forenames></author><author><keyname>Cheesman</keyname><forenames>Sam</forenames></author><author><keyname>McMullen</keyname><forenames>Patrick</forenames></author></authors><title>Group Foraging in Dynamic Environments</title><categories>cs.SI physics.soc-ph q-bio.PE</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991) 6 pages, 2 figures, 4 tables</comments><report-no>CollectiveIntelligence/2012/39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous human foraging experiments have shown that human groups routinely
undermatch environmental resources much like other animal species. In this
experiment, we test whether humans also selectively rely on others as
information sources when the environmental state is uncertain, and we also test
whether overt signals of other foragers' success influences group matching
behavior and group adaptation to a changing environment. The results show
evidence of reliance on social information in specific conditions, but
participants were primarily influenced by their individual assessments of food
location rather than the success of other foragers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3677</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3677</id><created>2012-04-16</created><authors><author><keyname>Hu</keyname><forenames>Yuheng</forenames></author><author><keyname>De</keyname><forenames>Sushovan</forenames></author><author><keyname>Chen</keyname><forenames>Yi</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Bayesian Data Cleaning for Web Data</title><categories>cs.DB cs.IR</categories><comments>6 pages, 7 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Cleaning is a long standing problem, which is growing in importance with
the mass of uncurated web data. State of the art approaches for handling
inconsistent data are systems that learn and use conditional functional
dependencies (CFDs) to rectify data. These methods learn data
patterns--CFDs--from a clean sample of the data and use them to rectify the
dirty/inconsistent data. While getting a clean training sample is feasible in
enterprise data scenarios, it is infeasible in web databases where there is no
separate curated data. CFD based methods are unfortunately particularly
sensitive to noise; we will empirically demonstrate that the number of CFDs
learned falls quite drastically with even a small amount of noise. In order to
overcome this limitation, we propose a fully probabilistic framework for
cleaning data. Our approach involves learning both the generative and error
(corruption) models of the data and using them to clean the data. For
generative models, we learn Bayes networks from the data. For error models, we
consider a maximum entropy framework for combing multiple error processes. The
generative and error models are learned directly from the noisy data. We
present the details of the framework and demonstrate its effectiveness in
rectifying web data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3678</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3678</id><created>2012-04-16</created><updated>2012-04-18</updated><authors><author><keyname>Lasecki</keyname><forenames>Walter S.</forenames></author><author><keyname>White</keyname><forenames>Samuel C.</forenames></author><author><keyname>Murray</keyname><forenames>Kyle I.</forenames></author><author><keyname>Bigham</keyname><forenames>Jeffrey P.</forenames></author></authors><title>Crowd Memory: Learning in the Collective</title><categories>cs.SI cs.HC physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/27</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd algorithms often assume workers are inexperienced and thus fail to
adapt as workers in the crowd learn a task. These assumptions fundamentally
limit the types of tasks that systems based on such algorithms can handle. This
paper explores how the crowd learns and remembers over time in the context of
human computation, and how more realistic assumptions of worker experience may
be used when designing new systems. We first demonstrate that the crowd can
recall information over time and discuss possible implications of crowd memory
in the design of crowd algorithms. We then explore crowd learning during a
continuous control task. Recent systems are able to disguise dynamic groups of
workers as crowd agents to support continuous tasks, but have not yet
considered how such agents are able to learn over time. We show, using a
real-time gaming setting, that crowd agents can learn over time, and `remember'
by passing strategies from one generation of workers to the next, despite high
turnover rates in the workers comprising them. We conclude with a discussion of
future research directions for crowd memory and learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3682</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3682</id><created>2012-04-16</created><authors><author><keyname>Collier</keyname><forenames>Benjamin</forenames></author><author><keyname>Kraut</keyname><forenames>Robert</forenames></author></authors><title>Leading the Collective: Social Capital and the Development of Leaders in
  Core-Periphery Organizations</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/40</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia and open source software projects have been cited as canonical
examples of collectively intelligent organizations. Both organizations rely on
large crowds of contributors to create knowledge goods. The crowds that emerge
in both cases are not flat, but form a core-periphery network in which a few
leaders contribute a large portion of the production and coordination work.
This paper explores the social network processes by which leaders emerge from
crowd-based organizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3685</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3685</id><created>2012-04-16</created><authors><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author></authors><title>Improving Customer Service in Healthcare with CRM 2.0</title><categories>cs.OH</categories><comments>Global Science And Technology Forum (GSTF) Business Review, Volume 1
  No. 2, October 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Healthcare industry is undergoing a paradigm shift from healthcare
institution-centred care to a citizen-centred care that emphasises on
continuity of care from prevention to rehabilitation. The recent development of
Information and Communication Technology (ICT), especially the Internet and its
related technologies has become the main driver of the paradigm shift. Managing
relationship with customers (patients) is becoming more important in the new
paradigm. The paper discusses Customer Relationship Management (CRM) in
healthcare and proposes a Social CRM or CRM 2.0 model to take advantage of the
multi-way relationships created by Web 2.0 and its widespread use in improving
customer services for mutual benefits between healthcare providers and their
customers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3689</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3689</id><created>2012-04-16</created><authors><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author></authors><title>Evaluating CRM Implementation in Healthcare Organization</title><categories>cs.SE</categories><comments>Proceedings of 2011 International Conference on Economics and
  Business Information (ICEBI 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, many healthcare organizations are adopting CRM as a strategy, which
involves using technology to organize, automate, and coordinate business
processes, in managing interactions with their patients. CRM with the Web
technology provides healthcare providers the ability to broaden their services
beyond usual practices, and thus offers suitable environment using latest
technology to achieve superb patient care. This paper discusses and
demonstrates how a new approach in CRM based on Web 2.0 will help the
healthcare providers improving their customer support, avoiding conflict, and
promoting better health to patient. With this new approach patients will
benefit from the customized personal service with full information access to
perform self managed their own health. It also helps healthcare providers
retaining the right customer. A conceptual framework of the new approach will
be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3691</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3691</id><created>2012-04-16</created><authors><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Wint</keyname><forenames>Zaw</forenames></author><author><keyname>Low</keyname><forenames>Patrick Kim Cheng</forenames></author><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author></authors><title>E-Health Initiative and Customer's Expectation: Case Brunei</title><categories>cs.OH</categories><comments>CiiT International Journal of Automation and Autonomous System, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is to determine the dimension of e-health services in Brunei
Darussalam (Brunei) from customers' perspective. It is to identify, understand,
analyze and evaluate public's expectation on e-health in Brunei. A
questionnaire was designed to gather quantitative and qualitative data to
survey patients, patient's family, and health practitioners at hospitals,
clinics, or home care centers in Brunei starting from February to March, 2011.
A 25-item Likert-type survey instrument was specifically developed for this
study and administered to a sample of 366 patients. The data were analyzed to
provide initial ideas and recommendation to policy makers on how to move
forward with the e-health initiative as a mean to improve healthcare services.
The survey revealed that there exists a high demand and expectation from people
in Brunei to have better healthcare services accessible through an e-health
system in order to improve health literacy as well as quality and efficiency of
healthcare. Regardless of the limitations of the survey, the general public has
responded with a great support for the capabilities of an e-health system
listed from the questionnaires. The results of the survey provide a solid
foundation for our on going research project to proceed further to develop a
model of e-health and subsequently develop a system prototype that incorporate
expectations from the people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3698</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3698</id><created>2012-04-17</created><authors><author><keyname>Dong</keyname><forenames>Wen</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Automatic Prediction Of Small Group Performance In Information Sharing
  Tasks</title><categories>cs.SI cs.HC physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/96</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we describe a novel approach, based on Markov jump processes,
to model small group conversational dynamics and to predict small group
performance. More precisely, we estimate conversational events such as turn
taking, backchannels, turn-transitions at the micro-level (1 minute windows)
and then we bridge the micro-level behavior and the macro-level performance. We
tested our approach with a cooperative task, the Information Sharing task, and
we verified the relevance of micro- level interaction dynamics in determining a
good group performance (e.g. higher speaking turns rate and more balanced
participation among group members).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3700</identifier>
 <datestamp>2012-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3700</id><created>2012-04-17</created><updated>2012-11-11</updated><authors><author><keyname>Li</keyname><forenames>Shidong</forenames></author><author><keyname>Liu</keyname><forenames>Yulong</forenames></author><author><keyname>Mi</keyname><forenames>Tiebin</forenames></author></authors><title>Fast thresholding algorithms with feedbacks for sparse signal recovery</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We provide another framework of iterative algorithms based on thresholding,
feedback and null space tuning for sparse signal recovery arising in sparse
representations and compressed sensing. Several thresholding algorithms with
various feedbacks are derived, which are seen as exceedingly effective and
fast. Convergence results are also provided. The core algorithm is shown to
converge in finite many steps under a (preconditioned) restricted isometry
condition. The algorithms are seen as particularly effective for large scale
problems. Numerical studies about the effectiveness and the speed of the
algorithms are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3711</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3711</id><created>2012-04-17</created><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author><author><keyname>Mueller</keyname><forenames>Ralf R.</forenames></author><author><keyname>Kawabata</keyname><forenames>Tsutomu</forenames></author></authors><title>Large-System Analysis of Joint User Selection and Vector Precoding for
  Multiuser MIMO Downlink</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint user selection (US) and vector precoding (US-VP) is proposed for
multiuser multiple-input multiple-output (MU-MIMO) downlink. The main
difference between joint US-VP and conventional US is that US depends on data
symbols for joint US-VP, whereas conventional US is independent of data
symbols. The replica method is used to analyze the performance of joint US-VP
in the large-system limit, where the numbers of transmit antennas, users, and
selected users tend to infinity while their ratios are kept constant. The
analysis under the assumptions of replica symmetry (RS) and 1-step replica
symmetry breaking (1RSB) implies that optimal data-independent US provides
nothing but the same performance as random US in the large-system limit,
whereas data-independent US is capacity-achieving as only the number of users
tends to infinity. It is shown that joint US-VP can provide a substantial
reduction of the energy penalty in the large-system limit. Consequently, joint
US-VP outperforms separate US-VP in terms of the achievable sum rate, which
consists of a combination of vector precoding (VP) and data-independent US. In
particular, data-dependent US can be applied to general modulation, and
implemented with a greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3716</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3716</id><created>2012-04-17</created><authors><author><keyname>Zhou</keyname><forenames>Qing F.</forenames></author><author><keyname>Zhang</keyname><forenames>Q. T.</forenames></author></authors><title>On the Blind Interference Alignment over Homogeneous Block Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Commun. Lett</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Staggered fading pattern between different users is crucial to interference
alignment without CSIT, or so-called blind interference alignment (BIA). This
special fading structure naturally arises from heterogeneous block fading
setting, in which different users experience independent block fading with
different coherent times. Jafar et al. prove that BIA can be applied in some
special heterogeneous block fading channels, which are formed naturally or
constructed artificially. In this paper, we show that in the context of a
2-user 2x1 broadcasting (BC) channel, staggered fading pattern can also be
found in homogeneous block fading setting, in which both users experience
independent fading with the same coherent time; and we propose a scheme to
achieve the optimal 4/3 DoF for the homogenous setting by using BIA. Applying
the proposed scheme, we further study a 2x1 BC network with K users undergoing
homogeneous block fading. When K&gt;=4, we show it is almost guaranteed that the
transmitter can find two users among the K users to form a 2-user 2x1 BC
channel which can apply BIA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3717</identifier>
 <datestamp>2012-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3717</id><created>2012-04-17</created><updated>2012-07-25</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Felt</keyname><forenames>Ulrike</forenames></author></authors><title>Edited Volumes, Monographs, and Book Chapters in the Book Citation Index
  (BKCI) and Science Citation Index (SCI, SoSCI, A&amp;HCI)</title><categories>cs.DL</categories><comments>Journal of Scientometric Research, 2012, in press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2011, Thomson-Reuters introduced the Book Citation Index (BKCI) as part of
the Science Citation Index (SCI). The interface of the Web of Science version 5
enables users to search for both &quot;Books&quot; and &quot;Book Chapters&quot; as new categories.
Books and book chapters, however, were always among the cited references, and
book chapters have been included in the database since 2005. We explore the two
categories with both BKCI and SCI, and in the sister social sciences (SoSCI)
and the arts &amp; humanities (A&amp;HCI) databases. Book chapters in edited volumes
can be highly cited. Books contain many citing references but are relatively
less cited. This may find its origin in the slower circulation of books than of
journal articles. It is possible to distinguish between monographs and edited
volumes among the &quot;Books&quot; scientometrically. Monographs may be underrated in
terms of citation impact or overrated using publication performance indicators
because individual chapters are counted as contributions separately in terms of
articles, reviews, and/or book chapters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3719</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3719</id><created>2012-04-17</created><updated>2012-07-27</updated><authors><author><keyname>Yilmaz</keyname><forenames>Ferkan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On the Computation of the Higher Order Statistics of the Channel
  Capacity over Generalized Fading Channels</title><categories>cs.IT cs.PF math.IT math.PR math.ST stat.TH</categories><comments>Submitted to IEEE Wireless Communications Letter, February 18, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The higher-order statistics (HOS) of the channel capacity
$\mu_n=\mathbb{E}[\log^n(1+\gamma_{end})]$, where $n\in\mathbb{N}$ denotes the
order of the statistics, has received relatively little attention in the
literature, due in part to the intractability of its analysis. In this letter,
we propose a novel and unified analysis, which is based on the moment
generating function (MGF) technique, to exactly compute the HOS of the channel
capacity. More precisely, our mathematical formalism can be readily applied to
maximal-ratio-combining (MRC) receivers operating in generalized fading
environments (i.e., the sum of the correlated noncentral chi-squared
distributions / the correlated generalized Rician distributions). The
mathematical formalism is illustrated by some numerical examples focussing on
the correlated generalized fading environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3724</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3724</id><created>2012-04-17</created><authors><author><keyname>Paul</keyname><forenames>Sharoda A.</forenames></author><author><keyname>Hong</keyname><forenames>Lichan</forenames></author><author><keyname>Chi</keyname><forenames>Ed H.</forenames></author></authors><title>Who is Authoritative? Understanding Reputation Mechanisms in Quora</title><categories>cs.SI cs.HC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/36</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As social Q&amp;A sites gain popularity, it is important to understand how users
judge the authoritativeness of users and content, build reputation, and
identify and promote high quality content. We conducted a study of emerging
social Q&amp;A site Quora. First, we describe user activity on Quora by analyzing
data across 60 question topics and 3917 users. Then we provide a rich
understanding of issues of authority, reputation, and quality from in-depth
interviews with ten Quora users. Our results show that primary sources of
information on Quora are judged authoritative. Also, users judge the reputation
of other users based on their past contributions. Social voting helps users
identify and promote good content but is prone to preferential attachment.
Combining social voting with sophisticated algorithms for ranking content might
enable users to better judge others' reputation and promote high quality
content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3726</identifier>
 <datestamp>2012-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3726</id><created>2012-04-17</created><updated>2012-05-21</updated><authors><author><keyname>Raschia</keyname><forenames>Guillaume</forenames></author><author><keyname>Theobald</keyname><forenames>Martin</forenames></author><author><keyname>Manolescu</keyname><forenames>Ioana</forenames></author></authors><title>Proceedings of the first International Workshop On Open Data, WOD-2012</title><categories>cs.DL cs.DB</categories><comments>Website of the workshop : https://sites.google.com/site/opendata2012/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  WOD-2012 aims at facilitating new trends and ideas from a broad range of
topics concerned within the widely-spread Open Data movement, from the
viewpoint of computer science research.
  While being most commonly known from the recent Linked Open Data movement,
the concept of publishing data explicitly as Open Data has meanwhile developed
many variants and facets that go beyond publishing large and highly structured
RDF/S repositories. Open Data comprises text and semi-structured data, but also
open multi-modal contents, including music, images, and videos. With the
increasing amount of data that is published by governments (see, e.g.,
data.gov, data.gov.uk or data.gouv.fr), by international organizations
(data.worldbank.org or data.undp.org) and by scientific communities (tdar.org,
cds.u-strasbg.fr, GenBank, IRIS or KNB) explicitly under an Open Data policy,
new challenges arise not only due to the scale at which this data becomes
available.
  A number of community-based conferences accommodate tracks or workshops which
are dedicated to Open Data. However, WOD aims to be a premier venue to gather
researchers and practitioners who are contributing to and interested in the
emerging field of managing Open Data from a computer science perspective.
Hence, it is a unique opportunity to find in a single place up-to-date
scientific works on Web-scale Open Data issues that have so far only partially
been addressed by different research communities such as Databases, Data Mining
and Knowledge Management, Distributed Systems, Data Privacy, and Data
Visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3731</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3731</id><created>2012-04-17</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Spina</keyname><forenames>Damiano</forenames></author><author><keyname>Amig&#xf3;</keyname><forenames>Enrique</forenames></author><author><keyname>Gonzalo</keyname><forenames>Julio</forenames></author></authors><title>Towards Real-Time Summarization of Scheduled Events from Twitter Streams</title><categories>cs.IR cs.CL cs.SI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper explores the real-time summarization of scheduled events such as
soccer games from torrential flows of Twitter streams. We propose and evaluate
an approach that substantially shrinks the stream of tweets in real-time, and
consists of two steps: (i) sub-event detection, which determines if something
new has occurred, and (ii) tweet selection, which picks a representative tweet
to describe each sub-event. We compare the summaries generated in three
languages for all the soccer games in &quot;Copa America 2011&quot; to reference live
reports offered by Yahoo! Sports journalists. We show that simple text analysis
methods which do not involve external knowledge lead to summaries that cover
84% of the sub-events on average, and 100% of key types of sub-events (such as
goals in soccer). Our approach should be straightforwardly applicable to other
kinds of scheduled events such as other sports, award ceremonies, keynote
talks, TV shows, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3735</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3735</id><created>2012-04-17</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Computational linear algebra over finite fields</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>Handbook of Finite Fields, Daniel Panario et Gary L. Mullen (Ed.)
  (2013) 514-528</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here algorithms for efficient computation of linear algebra
problems over finite fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3740</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3740</id><created>2012-04-17</created><updated>2013-02-15</updated><authors><author><keyname>Flaut</keyname><forenames>Cristina</forenames></author></authors><title>Cyclic codes over some special rings</title><categories>cs.IT math.IT math.RA</categories><comments>accepted in the Bulletin of the Korean Mathematical Society</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we will study cyclic codes over some special rings:
F_{q}[u]/(u^{i}), F_{q}[u_1,...u_{i}]/(u_1^2,u_2^2,...,u_{i}^2, u_1 u_2 - u_2
u_1,...,u_{i}u_{j} - u_{j}u_{i},...), F_{q}[u,v]/(u^{i},v^{j},uv-vu), q=p^{r},
where p is a prime number, r\in N-{0} and F_{q} is a field with q elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3742</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3742</id><created>2012-04-17</created><authors><author><keyname>Badiu</keyname><forenames>Mihai-Alin</forenames></author><author><keyname>Manch&#xf3;n</keyname><forenames>Carles Navarro</forenames></author><author><keyname>Bota</keyname><forenames>Vasile</forenames></author><author><keyname>Fleury</keyname><forenames>Bernard Henri</forenames></author></authors><title>Distributed Iterative Processing for Interference Channels with Receiver
  Cooperation</title><categories>cs.IT math.IT stat.ML</categories><comments>Submitted to 7th International Symposium on Turbo Codes &amp; Iterative
  Information Processing (ISTC 2012) for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for the derivation and evaluation of distributed
iterative algorithms for receiver cooperation in interference-limited wireless
systems. Our approach views the processing within and collaboration between
receivers as the solution to an inference problem in the probabilistic model of
the whole system. The probabilistic model is formulated to explicitly
incorporate the receivers' ability to share information of a predefined type.
We employ a recently proposed unified message-passing tool to infer the
variables of interest in the factor graph representation of the probabilistic
model. The exchange of information between receivers arises in the form of
passing messages along some specific edges of the factor graph; the rate of
updating and passing these messages determines the communication overhead
associated with cooperation. Simulation results illustrate the high performance
of the proposed algorithm even with a low number of message exchanges between
receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3748</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3748</id><created>2012-04-17</created><authors><author><keyname>Frick</keyname><forenames>Klaus</forenames></author><author><keyname>Marnitz</keyname><forenames>Philipp</forenames></author><author><keyname>Munk</keyname><forenames>Axel</forenames></author></authors><title>Statistical Multiresolution Estimation for Variational Imaging: With an
  Application in Poisson-Biophotonics</title><categories>stat.AP cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a spatially-adaptive method for image reconstruction
that is based on the concept of statistical multiresolution estimation as
introduced in [Frick K, Marnitz P, and Munk A. &quot;Statistical multiresolution
Dantzig estimation in imaging: Fundamental concepts and algorithmic framework&quot;.
Electron. J. Stat., 6:231-268, 2012]. It constitutes a variational
regularization technique that uses an supremum-type distance measure as
data-fidelity combined with a convex cost functional. The resulting convex
optimization problem is approached by a combination of an inexact alternating
direction method of multipliers and Dykstra's projection algorithm. We describe
a novel method for balancing data-fit and regularity that is fully automatic
and allows for a sound statistical interpretation. The performance of our
estimation approach is studied for various problems in imaging. Among others,
this includes deconvolution problems that arise in Poisson nanoscale
fluorescence microscopy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3752</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3752</id><created>2012-04-17</created><authors><author><keyname>Lu</keyname><forenames>Chenguang</forenames></author></authors><title>GPS Information and Rate Tolerance - Clarifying Relationship between
  Rate Distortion and Complexity Distortion</title><categories>cs.IT cs.CC math.IT</categories><comments>6 pages, 4 figures</comments><acm-class>H.1.1; H.1.2; I.4.2; I.5.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I proposed rate tolerance and discussed its relation to rate distortion in my
book &quot;A Generalized Information Theory&quot; published in 1993. Recently, I examined
the structure function and the complexity distortion based on Kolmogorov's
complexity theory. It is my understanding now that complexity-distortion is
only a special case of rate tolerance while constraint sets change from fuzzy
sets into clear sets that look like balls with the same radius. It is not true
that the complexity distortion is generally equivalent to rate distortion as
claimed by the researchers of complexity theory. I conclude that a rate
distortion function can only be equivalent to a rate tolerance function and
both of them can be described by a generalized mutual information formula where
P(Y|X) is equal to P(Y|Tolerance). The paper uses GPS as an example to derive
generalized information formulae and proves the above conclusions using
mathematical analyses and a coding example. The similarity between the formula
for measuring GPS information and the formula for rate distortion function can
deepen our understanding the generalized information measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3769</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3769</id><created>2012-04-17</created><authors><author><keyname>Salah</keyname><forenames>Almila Akdag</forenames></author><author><keyname>Gao</keyname><forenames>Cheng</forenames></author><author><keyname>Suchecki</keyname><forenames>Krzysztof</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author><author><keyname>Smiraglia</keyname><forenames>Richard P.</forenames></author></authors><title>The evolution of classification systems: Ontogeny of the UDC</title><categories>cs.DL physics.soc-ph</categories><comments>ISKO conference 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To classify is to put things in meaningful groups, but the criteria for doing
so can be problematic. Study of evolution of classification includes
ontogenetic analysis of change in classification over time. We present an
empirical analysis of the UDC over the entire period of its development. We
demonstrate stability in main classes, with major change driven by 20th century
scientific developments. But we also demonstrate a vast increase in the
complexity of auxiliaries. This study illustrates an alternative to Tennis'
&quot;scheme-versioning&quot; method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3773</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3773</id><created>2012-04-17</created><authors><author><keyname>Zhang</keyname><forenames>Zhi-Yong</forenames></author><author><keyname>Yuan</keyname><forenames>Chun-Ming</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author></authors><title>Matrix Formula of Differential Resultant for First Order Generic
  Ordinary Differential Polynomials</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a matrix representation for the differential resultant of two
generic ordinary differential polynomials $f_1$ and $f_2$ in the differential
indeterminate $y$ with order one and arbitrary degree is given. That is, a
non-singular matrix is constructed such that its determinant contains the
differential resultant as a factor. Furthermore, the algebraic sparse resultant
of $f_1, f_2, \delta f_1, \delta f_2$ treated as polynomials in $y, y', y&quot;$ is
shown to be a non-zero multiple of the differential resultant of $f_1, f_2$.
Although very special, this seems to be the first matrix representation for a
class of nonlinear generic differential polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3799</identifier>
 <datestamp>2012-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3799</id><created>2012-04-17</created><updated>2012-07-04</updated><authors><author><keyname>Arag&#xf3;n</keyname><forenames>Pablo</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author><author><keyname>Volkovich</keyname><forenames>Yana</forenames></author></authors><title>Biographical Social Networks on Wikipedia - A cross-cultural study of
  links that made history</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>4 pages, 3 figures</comments><acm-class>J.4; G.2.2</acm-class><journal-ref>Proceedings of WikiSym, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is arguable whether history is made by great men and women or vice versa,
but undoubtably social connections shape history. Analysing Wikipedia, a global
collective memory place, we aim to understand how social links are recorded
across cultures. Starting with the set of biographies in the English Wikipedia
we focus on the networks of links between these biographical articles on the 15
largest language Wikipedias. We detect the most central characters in these
networks and point out culture-related peculiarities. Furthermore, we reveal
remarkable similarities between distinct groups of language Wikipedias and
highlight the shared knowledge about connections between persons across
cultures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3800</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3800</id><created>2012-04-17</created><authors><author><keyname>Kalyanaraman</keyname><forenames>Srinivasan</forenames></author></authors><title>Indus script corpora, archaeo-metallurgy and Meluhha (Mleccha)</title><categories>cs.CL</categories><comments>49 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jules Bloch's work on formation of the Marathi language has to be expanded
further to provide for a study of evolution and formation of Indian languages
in the Indian language union (sprachbund). The paper analyses the stages in the
evolution of early writing systems which began with the evolution of counting
in the ancient Near East. A stage anterior to the stage of syllabic
representation of sounds of a language, is identified. Unique geometric shapes
required for tokens to categorize objects became too large to handle to
abstract hundreds of categories of goods and metallurgical processes during the
production of bronze-age goods. About 3500 BCE, Indus script as a writing
system was developed to use hieroglyphs to represent the 'spoken words'
identifying each of the goods and processes. A rebus method of representing
similar sounding words of the lingua franca of the artisans was used in Indus
script. This method is recognized and consistently applied for the lingua
franca of the Indian sprachbund. That the ancient languages of India,
constituted a sprachbund (or language union) is now recognized by many
linguists. The sprachbund area is proximate to the area where most of the Indus
script inscriptions were discovered, as documented in the corpora. That
hundreds of Indian hieroglyphs continued to be used in metallurgy is evidenced
by their use on early punch-marked coins. This explains the combined use of
syllabic scripts such as Brahmi and Kharoshti together with the hieroglyphs on
Rampurva copper bolt, and Sohgaura copper plate from about 6th century
BCE.Indian hieroglyphs constitute a writing system for meluhha language and are
rebus representations of archaeo-metallurgy lexemes. The rebus principle was
employed by the early scripts and can legitimately be used to decipher the
Indus script, after secure pictorial identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3806</identifier>
 <datestamp>2012-10-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3806</id><created>2012-04-17</created><authors><author><keyname>Kandiah</keyname><forenames>Vivek</forenames><affiliation>CNRS, Toulouse</affiliation></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames><affiliation>CNRS, Toulouse</affiliation></author></authors><title>PageRank model of opinion formation on social networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>revtex 10 pages, 16 figs, research at
  http://www.quantware.ups-tlse.fr/</comments><journal-ref>Physica A v.391, p.5779-5793 (2012)</journal-ref><doi>10.1016/j.physa.2012.06.047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the PageRank model of opinion formation and investigate its rich
properties on real directed networks of Universities of Cambridge and Oxford,
LiveJournal and Twitter. In this model the opinion formation of linked electors
is weighted with their PageRank probability. We find that the society elite,
corresponding to the top PageRank nodes, can impose its opinion to a
significant fraction of the society. However, for a homogeneous distribution of
two opinions there exists a bistability range of opinions which depends on a
conformist parameter characterizing the opinion formation. We find that
LiveJournal and Twitter networks have a stronger tendency to a totalitar
opinion formation. We also analyze the Sznajd model generalized for scale-free
networks with the weighted PageRank vote of electors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3812</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3812</id><created>2012-04-17</created><updated>2012-04-20</updated><authors><author><keyname>Inaltekin</keyname><forenames>Hazer</forenames></author></authors><title>Gaussian Approximation for the Wireless Multi-access Interference
  Distribution and Its Applications</title><categories>math.PR cs.IT math.IT</categories><doi>10.1109/TSP.2012.2212014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of Gaussian approximation for the
wireless multi-access interference distribution in large spatial wireless
networks. First, a principled methodology is presented to establish rates of
convergence of the multi-access interference distribution to a Gaussian
distribution for general bounded and power-law decaying path-loss functions.
The model is general enough to also include various random wireless channel
dynamics such as fading and shadowing arising from multipath propagation and
obstacles existing in the communication environment. It is shown that the
wireless multi-access interference distribution converges to the Gaussian
distribution with the same mean and variance at a rate
$\frac{1}{\sqrt{\lambda}}$, where $\lambda&gt;0$ is a parameter controlling the
intensity of the planar (possibly non-stationary) Poisson point process
generating node locations. An explicit expression for the scaling coefficient
is obtained as a function of fading statistics and the path-loss function.
Second, an extensive numerical and simulation study is performed to illustrate
the accuracy of the derived Gaussian approximation bounds. A good statistical
fit between the interference distribution and its Gaussian approximation is
observed for moderate to high values of $\lambda$. Finally, applications of
these approximation results to upper and lower bound the outage capacity and
ergodic sum capacity for spatial wireless networks are illustrated. The derived
performance bounds on these capacity metrics track the network performance
within one nats per second per hertz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3818</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3818</id><created>2012-04-17</created><updated>2013-02-05</updated><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Throughput Optimal Policies for Energy Harvesting Wireless Transmitters
  with Non-Ideal Circuit Power</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Journal on
  Selected Areas in Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing the fundamental tradeoffs for maximizing energy efficiency
(EE) versus spectrum efficiency (SE) is a key problem in wireless
communication. In this paper, we address this problem for a point-to-point
additive white Gaussian noise (AWGN) channel with the transmitter powered
solely via energy harvesting from the environment. In addition, we assume a
practical on-off transmitter model with non-ideal circuit power, i.e., when the
transmitter is on, its consumed power is the sum of the transmit power and a
constant circuit power. Under this setup, we study the optimal transmit power
allocation to maximize the average throughput over a finite horizon, subject to
the time-varying energy constraint and the non-ideal circuit power consumption.
First, we consider the off-line optimization under the assumption that the
energy arrival time and amount are a priori known at the transmitter. Although
this problem is non-convex due to the non-ideal circuit power, we show an
efficient optimal solution that in general corresponds to a two-phase
transmission: the first phase with an EE-maximizing on-off power allocation,
and the second phase with a SE-maximizing power allocation that is
non-decreasing over time, thus revealing an interesting result that both the EE
and SE optimizations are unified in an energy harvesting communication system.
We then extend the optimal off-line algorithm to the case with multiple
parallel AWGN channels, based on the principle of nested optimization. Finally,
inspired by the off-line optimal solution, we propose a new online algorithm
under the practical setup with only the past and present energy state
information (ESI) known at the transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3820</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3820</id><created>2012-04-17</created><updated>2012-09-06</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>LaValle</keyname><forenames>Steven M.</forenames></author></authors><title>Distance Optimal Formation Control on Graphs with a Tight Convergence
  Time Guarantee</title><categories>cs.SY cs.AI cs.RO</categories><comments>Brought to be in-sync with final version submitted to CDC 2012 with
  only minor updates</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the task of moving a set of indistinguishable agents on a connected graph
with unit edge distance to an arbitrary set of goal vertices, free of
collisions, we propose a fast distance optimal control algorithm that guides
the agents into the desired formation. Moreover, we show that the algorithm
also provides a tight convergence time guarantee (time optimality and distance
optimality cannot be simultaneously satisfied). Our generic graph formulation
allows the algorithm to be applied to scenarios such as grids with holes
(modeling obstacles) in arbitrary dimensions. Simulations, available online,
confirm our theoretical developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3830</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3830</id><created>2012-04-17</created><updated>2013-01-17</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author><author><keyname>LaValle</keyname><forenames>Steven M.</forenames></author></authors><title>Planning Optimal Paths for Multiple Robots on Graphs</title><categories>cs.RO cs.AI cs.SY</categories><comments>Changed &quot;agents&quot; to &quot;robots&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of optimal multi-robot path planning
(MPP) on graphs. We propose two multiflow based integer linear programming
(ILP) models that computes minimum last arrival time and minimum total distance
solutions for our MPP formulation, respectively. The resulting algorithms from
these ILP models are complete and guaranteed to yield true optimal solutions.
In addition, our flexible framework can easily accommodate other variants of
the MPP problem. Focusing on the time optimal algorithm, we evaluate its
performance, both as a stand alone algorithm and as a generic heuristic for
quickly solving large problem instances. Computational results confirm the
effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3831</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3831</id><created>2012-04-17</created><authors><author><keyname>Xue</keyname><forenames>Kaiping</forenames></author><author><keyname>Hong</keyname><forenames>Peilin</forenames></author><author><keyname>Ma</keyname><forenames>Changsha</forenames></author></authors><title>A lightweight dynamic pseudonym identity based authentication and key
  agreement protocol without verification tables for multi-server architecture</title><categories>cs.CR</categories><comments>21pages, 2 fingures</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional password based authentication schemes are mostly considered in
single server environments. They are unfitted for the multi-server environments
from two aspects. On the one hand, users need to register in each server and to
store large sets of data, including identities and passwords. On the other
hand, servers are required to store a verification table containing user
identities and passwords. Recently, On the base on Sood et al.'s
protocol(2011), Li et al. proposed an improved dynamic identity based
authentication and key agreement protocol for multi-server architecture(2012).
Li et al. claims that the proposed scheme can make up the security weaknesses
of Sood et al.'s protocol. Unfortunately, our further research shows that Li et
al.'s protocol contains several drawbacks and can not resist some types of
known attacks, such as replay attack, Deny-of-Service attack, internal attack,
eavesdropping attack, masquerade attack, and so on. In this paper, we further
propose a light dynamic pseudonym identity based authentication and key
agreement protocol for multi-server architecture. In our scheme, service
providing servers don't need to maintain verification tables for users. The
proposed protocol provides not only the declared security features in Li et
al.'s paper, but also some other security features, such as traceability and
identity protection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3838</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3838</id><created>2012-04-17</created><authors><author><keyname>Moujahid</keyname><forenames>A.</forenames></author><author><keyname>D'Anjou</keyname><forenames>A.</forenames></author><author><keyname>Torrealdea</keyname><forenames>F. J.</forenames></author><author><keyname>Sarasola</keyname><forenames>C.</forenames></author></authors><title>Energy cost reduction in the synchronization of a pair of nonidentical
  coupled Hindmarsh-Rose neurons</title><categories>cs.AI nlin.CD q-bio.NC</categories><journal-ref>Advances in Intelligent and Soft Computing, 2010, Volume 71/2010,
  657-664</journal-ref><doi>10.1007/978-3-642-12433-4_77</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many biological processes involve synchronization between nonequivalent
systems, i.e, systems where the difference is limited to a rather small
parameter mismatch. The maintenance of the synchronized regime in this cases is
energetically costly \cite{1}. This work studies the energy implications of
synchronization phenomena in a pair of structurally flexible coupled neurons
that interact through electrical coupling. We show that the forced
synchronization between two nonidentical neurons creates appropriate conditions
for an efficient actuation of adaptive laws able to make the neurons
structurally approach their behaviours in order to decrease the flow of energy
required to maintain the synchronization regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3844</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3844</id><created>2012-04-17</created><authors><author><keyname>Cases</keyname><forenames>Blanca</forenames></author><author><keyname>D'Anjou</keyname><forenames>Alicia</forenames></author><author><keyname>Moujahid</keyname><forenames>Abdelmalik</forenames></author></authors><title>On how percolation threshold affects PSO performance</title><categories>cs.AI</categories><journal-ref>LNCS, 2012, Volume 7208/2012, 509-520</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical evidence of the influence of neighborhood topology on the
performance of particle swarm optimization (PSO) algorithms has been shown in
many works. However, little has been done about the implications could have the
percolation threshold in determining the topology of this neighborhood. This
work addresses this problem for individuals that, like robots, are able to
sense in a limited neighborhood around them. Based on the concept of
percolation threshold, and more precisely, the disk percolation model in 2D, we
show that better results are obtained for low values of radius, when
individuals occasionally ask others their best visited positions, with the
consequent decrease of computational complexity. On the other hand, since
percolation threshold is a universal measure, it could have a great interest to
compare the performance of different hybrid PSO algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3850</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3850</id><created>2012-02-16</created><authors><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Disser</keyname><forenames>Yann</forenames></author><author><keyname>Mihal&#xe1;k</keyname><forenames>Mat&#xfa;&#x161;</forenames></author><author><keyname>Widmayer</keyname><forenames>Peter</forenames></author></authors><title>Simple Agents Learn to Find Their Way: An Introduction on Mapping
  Polygons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives an introduction to the problem of mapping simple polygons
with autonomous agents. We focus on minimalistic agents that move from vertex
to vertex along straight lines inside a polygon, using their sensors to gather
local observations at each vertex. Our attention revolves around the question
whether a given configuration of sensors and movement capabilities of the
agents allows them to capture enough data in order to draw conclusions
regarding the global layout of the polygon. In particular, we study the problem
of reconstructing the visibility graph of a simple polygon by an agent moving
either inside or on the boundary of the polygon. Our aim is to provide insight
about the algorithmic challenges faced by an agent trying to map a polygon. We
present an overview of techniques for solving this problem with agents that are
equipped with simple sensorial capabilities. We illustrate these techniques on
examples with sensors that mea- sure angles between lines of sight or identify
the previous location. We give an overview over related problems in
combinatorial geometry as well as graph exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3853</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3853</id><created>2012-04-17</created><updated>2013-02-04</updated><authors><author><keyname>Hittinger</keyname><forenames>J. A. F.</forenames></author><author><keyname>Banks</keyname><forenames>J. W.</forenames></author></authors><title>Block-Structured Adaptive Mesh Refinement Algorithms for Vlasov
  Simulation</title><categories>cs.MS physics.plasm-ph</categories><report-no>LLNL-JRNL-515291</report-no><doi>10.1016/j.jcp.2013.01.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct discretization of continuum kinetic equations, like the Vlasov
equation, are under-utilized because the distribution function generally exists
in a high-dimensional (&gt;3D) space and computational cost increases
geometrically with dimension. We propose to use high-order finite-volume
techniques with block-structured adaptive mesh refinement (AMR) to reduce the
computational cost. The primary complication comes from a solution state
comprised of variables of different dimensions. We develop the algorithms
required to extend standard single-dimension block structured AMR to the
multi-dimension case. Specifically, algorithms for reduction and injection
operations that transfer data between mesh hierarchies of different dimensions
are explained in detail. In addition, modifications to the basic AMR algorithm
that enable the use of high-order spatial and temporal discretizations are
discussed. Preliminary results for a standard 1D+1V Vlasov-Poisson test problem
are presented. Results indicate that there is potential for significant savings
for some classes of Vlasov problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3860</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3860</id><created>2012-04-17</created><authors><author><keyname>Ramamoorthy</keyname><forenames>Subramanian</forenames></author><author><keyname>Salamon</keyname><forenames>Andr&#xe1;s Z.</forenames></author><author><keyname>Santhanam</keyname><forenames>Rahul</forenames></author></authors><title>Macroscopes: models for collective decision making</title><categories>cs.SI cs.CC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991), 8 pages</comments><report-no>CollectiveIntelligence/2012/44</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new model of collective decision making, when a global
decision needs to be made but the parties only possess partial information, and
are unwilling (or unable) to first create a globalcomposite of their local
views. Our macroscope model captures two key features of many real-world
problems: allotment structure (how access to local information is apportioned
between parties, including overlaps between the parties) and the possible
presence of meta-information (what each party knows about the allotment
structure of the overall problem). Using the framework of communication
complexity, we formalize the efficient solution of a macroscope. We present
general results about the macroscope model, and also results that abstract the
essential computational operations underpinning practical applications,
including in financial markets and decentralized sensor networks. We illustrate
the computational problem inherent in real-world collective decision making
processes using results for specific functions, involving detecting a change in
state (constant and step functions), and computing statistical properties (the
mean).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3873</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3873</id><created>2012-04-17</created><updated>2013-09-20</updated><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Singer</keyname><forenames>Amit</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author></authors><title>A Cheeger Inequality for the Graph Connection Laplacian</title><categories>math.SP cs.DS math.CO</categories><comments>To appear in the SIAM Journal on Matrix Analysis and Applications
  (SIMAX)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The O(d) Synchronization problem consists of estimating a set of unknown
orthogonal transformations O_i from noisy measurements of a subset of the
pairwise ratios O_iO_j^{-1}. We formulate and prove a Cheeger-type inequality
that relates a measure of how well it is possible to solve the O(d)
synchronization problem with the spectra of an operator, the graph Connection
Laplacian. We also show how this inequality provides a worst case performance
guarantee for a spectral method to solve this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3874</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3874</id><created>2012-04-10</created><authors><author><keyname>Sarala</keyname><forenames>B.</forenames><affiliation>Department of ECE, M V S R Engineering College, Hyderabad</affiliation></author><author><keyname>Venkateswarulu</keyname><forenames>D. S.</forenames><affiliation>Department of ECE, Progressive Engineering College, Cheekati Mamidi, HMDA, Hyderabad</affiliation></author><author><keyname>Bhandari</keyname><forenames>B. N.</forenames><affiliation>Department of ECE, JNTU, Hyderabad, India</affiliation></author></authors><title>Overview of MC CDMA PAPR Reduction Techniques</title><categories>cs.NI</categories><comments>14 pages, 7 figures, IJDPS March 2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  High Peak to Average Power Ratio (PAPR) of the transmitted signal is a
critical problem in multicarrier modulation systems (MCM) such as Orthogonal
Frequency Division Multiplexing (OFDM), and Multi-Carrier Code Division
Multiple Access (MC CDMA) systems, due to large number of subcarriers. High
PAPR leads to reduced resolution, and battery life. It also deteriorates system
performance. This paper focuses on review of different PAPR reduction
techniques with attendant technical issues as well as criteria for selection of
PAPR reduction technique. To reduce PAPR the constraints are low power
consumption, and low Bit Error Rate (BER). Spectral bandwidth is improved by
better spectral characteristics, and low complexity/cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3881</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3881</id><created>2012-04-17</created><authors><author><keyname>Dorman</keyname><forenames>A. M.</forenames></author></authors><title>Stimulus and correlation matching measurement technique in computer
  based characterization testing</title><categories>cs.OH</categories><comments>18 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructive theory of characterization test is considered. The theory is
applicable to a nano devices characterization: current-voltage, Auger current
dependence. Generally small response of device under test on an applied
stimulus is masked by an unknown deterministic background and a random noise.
Characterization test in this signal corruption scenario should be based on
correlation measurement technique of device response on applied optimal
stimulus with optimal reference signal. Co-synthesis solution of stimulus and
reference signal is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3890</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3890</id><created>2012-04-16</created><authors><author><keyname>Yu</keyname><forenames>Lixiu</forenames></author><author><keyname>Nickerson</keyname><forenames>Jeffrey V.</forenames></author><author><keyname>Sakamoto</keyname><forenames>Yasuaki</forenames></author></authors><title>Collective Creativity: Where we are and where we might go</title><categories>cs.SI cs.HC</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creativity is individual, and it is social. The social aspects of creativity
have become of increasing interest as systems have emerged that mobilize large
numbers of people to engage in creative tasks. We examine research related to
collective intelligence and differentiate work on collective creativity from
other collective activities by analyzing systems with respect to the tasks that
are performed and the outputs that result. Three types of systems are
discussed: games, contests and networks. We conclude by suggesting how systems
that generate collective creativity can be improved and how new systems might
be constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3891</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3891</id><created>2012-04-16</created><authors><author><keyname>Horsethief</keyname><forenames>Christopher</forenames></author></authors><title>Re-differentiation as collective intelligence: The Ktunaxa language
  online community</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/35</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents preliminary results of an investigation of collectively
intelligent behavior in a Native North American speech community. The research
reveals several independently initiated strategies organized around the
collective problem of language endangerment. Specifically, speakers are
engaging in self-organizing efforts to reverse historical language
simplification that resulted from cultural trauma. These acts of collective
intelligence serve to reduce entropy in speech community identity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3918</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3918</id><created>2012-04-17</created><authors><author><keyname>Davies</keyname><forenames>Jessica</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Eliminating the Weakest Link: Making Manipulation Intractable?</title><categories>cs.AI cs.CC cs.GT</categories><comments>To appear in Proceedings of Twenty-Sixth Conference on Artificial
  Intelligence (AAAI-12)</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successive elimination of candidates is often a route to making manipulation
intractable to compute. We prove that eliminating candidates does not
necessarily increase the computational complexity of manipulation. However, for
many voting rules used in practice, the computational complexity increases. For
example, it is already known that it is NP-hard to compute how a single voter
can manipulate the result of single transferable voting (the elimination
version of plurality voting). We show here that it is NP-hard to compute how a
single voter can manipulate the result of the elimination version of veto
voting, of the closely related Coombs' rule, and of the elimination versions of
a general class of scoring rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3920</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3920</id><created>2012-04-17</created><authors><author><keyname>Ataei</keyname><forenames>Mohammad R.</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author><author><keyname>Kunz</keyname><forenames>Thomas</forenames></author></authors><title>Low-Complexity Energy-Efficient Broadcasting in One-Dimensional Wireless
  Networks</title><categories>cs.NI</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the transmission range assignment for N
wireless nodes located on a line (a linear wireless network) for broadcasting
data from one specific node to all the nodes in the network with minimum
energy. Our goal is to find a solution that has low complexity and yet performs
close to optimal. We propose an algorithm for finding the optimal assignment
(which results in the minimum energy consumption) with complexity O(N^2). An
approximation algorithm with complexity O(N) is also proposed. It is shown
that, for networks with uniformly distributed nodes, the linear-time
approximate solution obtained by this algorithm on average performs practically
identical to the optimal assignment. Both the optimal and the suboptimal
algorithms require the full knowledge of the network topology and are thus
centralized. We also propose a distributed algorithm of negligible complexity,
i.e., with complexity O(1), which only requires the knowledge of the adjacent
neighbors at each wireless node. Our simulations demonstrate that the
distributed solution on average performs almost as good as the optimal one for
networks with uniformly distributed nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3921</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3921</id><created>2012-04-17</created><authors><author><keyname>Esteban</keyname><forenames>Javier</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author><author><keyname>McPherson</keyname><forenames>Sean</forenames></author><author><keyname>Sathiamoorthy</keyname><forenames>Maheswaran</forenames></author></authors><title>Analysis of Twitter Traffic based on Renewal Densities</title><categories>cs.CY cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel approach for Twitter traffic analysis based
on renewal theory. Even though twitter datasets are of increasing interest to
researchers, extracting information from message timing remains somewhat
unexplored. Our approach, extending our prior work on anomaly detection, makes
it possible to characterize levels of correlation within a message stream, thus
assessing how much interaction there is between those posting messages.
Moreover, our method enables us to detect the presence of periodic traffic,
which is useful to determine whether there is spam in the message stream.
Because our proposed techniques only make use of timing information and are
amenable to downsampling, they can be used as low complexity tools for data
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3939</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3939</id><created>2012-04-17</created><authors><author><keyname>Garcia</keyname><forenames>Cristobal</forenames></author><author><keyname>Parraguez</keyname><forenames>Pedro</forenames></author><author><keyname>Barahona</keyname><forenames>Matias</forenames></author><author><keyname>Gloor</keyname><forenames>Peter</forenames></author></authors><title>Tracking the 2011 Student-led Collective Movement in Chile through
  Social Media Use</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/53</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using social media archives of the 2011 Chilean student unrest and dynamic
social network analysis, we study how leaders and participants use social media
such as Twitter, and the Web to self-organize and communicate with each other,
and thus generate one of the biggest &quot;smart movements&quot; in the history of Chile.
In this paper we i) describe the basic network topology of the 2011 student-led
social movement in Chile; ii) explore how the student leaders are connected to,
and how are they seen by (a) political leaders, and (b) University authorities;
iii) hypothesize about key success factors and risk variables for the Student
Network Movement's organization process and sustainability over time. We
contend that this social media enabled massive movement is yet another
manifestation of the network era, which leverages agents' socio-technical
networks, and thus accelerates how agents coordinate, mobilize resources and
enact collective intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3946</identifier>
 <datestamp>2012-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3946</id><created>2012-04-17</created><updated>2012-07-23</updated><authors><author><keyname>Chazelle</keyname><forenames>Bernard</forenames></author></authors><title>The Dynamics of Influence Systems</title><categories>nlin.AO cs.MA cs.SI math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence systems form a large class of multiagent systems designed to model
how influence, broadly defined, spreads across a dynamic network. We build a
general analytical framework which we then use to prove that, while sometimes
chaotic, influence dynamics of the diffusive kind is almost always
asymptotically periodic. Besides resolving the dynamics of a popular family of
multiagent systems, the other contribution of this work is to introduce a new
type of renormalization-based bifurcation analysis for multiagent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3968</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3968</id><created>2012-04-17</created><authors><author><keyname>Sermanet</keyname><forenames>Pierre</forenames></author><author><keyname>Chintala</keyname><forenames>Soumith</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Convolutional Neural Networks Applied to House Numbers Digit
  Classification</title><categories>cs.CV cs.LG cs.NE</categories><comments>4 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We classify digits of real-world house numbers using convolutional neural
networks (ConvNets). ConvNets are hierarchical feature learning neural networks
whose structure is biologically inspired. Unlike many popular vision approaches
that are hand-designed, ConvNets can automatically learn a unique set of
features optimized for a given task. We augmented the traditional ConvNet
architecture by learning multi-stage features and by using Lp pooling and
establish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2%
error improvement). Furthermore, we analyze the benefits of different pooling
methods and multi-stage features in ConvNets. The source code and a tutorial
are available at eblearn.sf.net.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3972</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3972</id><created>2012-04-18</created><updated>2013-03-13</updated><authors><author><keyname>Qi</keyname><forenames>Yuan</forenames></author><author><keyname>Dai</keyname><forenames>Bo</forenames></author><author><keyname>Zhu</keyname><forenames>Yao</forenames></author></authors><title>EigenGP: Sparse Gaussian process models with data-dependent
  eigenfunctions</title><categories>cs.LG stat.CO stat.ML</categories><comments>10 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GPs) provide a nonparametric representation of functions.
However, classical GP inference suffers from high computational cost and it is
difficult to design nonstationary GP priors in practice. In this paper, we
propose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve
(KL) expansion of a GP prior. We use the Nystrom approximation to obtain data
dependent eigenfunctions and select these eigenfunctions by evidence
maximization. This selection reduces the number of eigenfunctions in our model
and provides a nonstationary covariance function. To handle nonlinear
likelihoods, we develop an efficient expectation propagation (EP) inference
algorithm, and couple it with expectation maximization for eigenfunction
selection. Because the eigenfunctions of a Gaussian kernel are associated with
clusters of samples - including both the labeled and unlabeled - selecting
relevant eigenfunctions enables EigenGP to conduct semi-supervised learning.
Our experimental results demonstrate improved predictive performance of EigenGP
over alternative state-of-the-art sparse GP and semisupervised learning methods
for regression, classification, and semisupervised classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3986</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3986</id><created>2012-04-18</created><authors><author><keyname>Alobaidi</keyname><forenames>Mizal</forenames><affiliation>Tikrit University, Faculty of Computer Sciences and Mathematics</affiliation></author><author><keyname>Batyiv</keyname><forenames>Andriy</forenames><affiliation>V.N. Karazin Kharkiv National University, School of Mathematics</affiliation></author><author><keyname>Zholtkevych</keyname><forenames>Grygoriy</forenames><affiliation>V.N. Karazin Kharkiv National University, School of Mathematics</affiliation></author></authors><title>Towards the Notion of an Abstract Quantum Automaton</title><categories>cs.CC quant-ph</categories><comments>15 pages, 3 figures</comments><msc-class>68Q05, 81P45, 81P68</msc-class><acm-class>F.1.1</acm-class><journal-ref>Proc. ICTERI 2012 CEUR-WS 848(2012) 17-32</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this paper is to give a rigorous mathematical description of
systems for processing quantum information. To do it authors consider abstract
state machines as models of classical computational systems. This class of
machines is refined by introducing constrains on a state structure, namely, it
is assumed that state of computational process has two components: a control
unit state and a memory state. Then authors modify the class of models by
substituting the deterministic evolutionary mechanism for a stochastic
evolutionary mechanism. This approach can be generalized to the quantum case:
one can replace transformations of a classical memory with quantum operations
on a quantum memory. Hence the authors come to the need to construct a
mathematical model of an operation on the quantum memory. It leads them to the
notion of an abstract quantum automaton. Further the authors demonstrate that a
quantum teleportation process is described as evolutionary process for some
abstract quantum automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3989</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3989</id><created>2012-04-18</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Closed-Form Critical Conditions of Saddle-Node Bifurcations for Buck
  Converters</title><categories>cs.SY math.DS nlin.CD</categories><comments>Submitted to IEEE Transactions on Automatic Control on Jan. 9, 2012.
  Seven of my arXiv manuscripts have a common reviewer</comments><journal-ref>Nonlinear Dynamics, 70(3), pp. 1767-1789, Nov. 2012</journal-ref><doi>10.1007/s11071-012-0572-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general and exact critical condition of saddle-node bifurcation is derived
in closed form for the buck converter. The critical condition is helpful for
the converter designers to predict or prevent some jump instabilities or
coexistence of multiple solutions associated with the saddle-node bifurcation.
Some previously known critical conditions become special cases in this
generalized framework. Given an arbitrary control scheme, a systematic
procedure is proposed to derive the critical condition for that control scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3990</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3990</id><created>2012-04-18</created><authors><author><keyname>Fang</keyname><forenames>Chung-Chieh</forenames></author></authors><title>Comments on &quot;Bifurcations in DC-DC Switching Converters: Review of
  Methods and Applications&quot;</title><categories>cs.SY math.DS nlin.CD</categories><comments>Submitted to International Journal of Bifurcation and Chaos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a review paper [1] (El Aroudi, et al., 2005), two stability conditions for
DC-DC converters are presented. However, these two conditions were published
years earlier at least in a journal paper [2] (Fang and Abed, 2001). In this
note, the similar texts of [1] and [2] are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.3997</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.3997</id><created>2012-04-18</created><authors><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Fiorina</keyname><forenames>Jocelyn</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author></authors><title>A New Low-Complexity Decodable Rate-5/4 STBC for Four Transmit Antennas
  with Nonvanishing Determinants</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures and 1 table; IEEE Global Telecommunications
  Conference (GLOBECOM 2011), 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of Space-Time Block Codes (STBCs) increases significantly the optimal
detection complexity at the receiver unless the low-complexity decodability
property is taken into consideration in the STBC design. In this paper we
propose a new low-complexity decodable rate-5/4 full-diversity 4 x 4 STBC. We
provide an analytical proof that the proposed code has the
Non-Vanishing-Determinant (NVD) property, a property that can be exploited
through the use of adaptive modulation which changes the transmission rate
according to the wireless channel quality. We compare the proposed code to the
best existing low-complexity decodable rate-5/4 full-diversity 4 x 4 STBC in
terms of performance over quasi-static Rayleigh fading channels, worst- case
complexity, average complexity, and Peak-to-Average Power Ratio (PAPR). Our
code is found to provide better performance, lower average decoding complexity,
and lower PAPR at the expense of a slight increase in worst-case decoding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4000</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4000</id><created>2012-04-18</created><authors><author><keyname>Ismail</keyname><forenames>Amr</forenames></author><author><keyname>Fiorina</keyname><forenames>Jocelyn</forenames></author><author><keyname>Sari</keyname><forenames>Hikmet</forenames></author></authors><title>A New Family of Low-Complexity Decodable STBCs for Four Transmit
  Antennas</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures and 1 table. Accepted for publication in IEEE
  International Conference on Communications (ICC 2012), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new construction method for rate-1
Fast-Group-Decodable (FGD) Space-Time-Block Codes (STBC)s for 2^a transmit
antennas. We focus on the case of a=2 and we show that the new FGD rate-1 code
has the lowest worst-case decoding complexity among existing comparable STBCs.
The coding gain of the new rate-1 code is then optimized through constellation
stretching and proved to be constant irrespective of the underlying QAM
constellation prior to normalization. In a second step, we propose a new rate-2
STBC that multiplexes two of our rate-1 codes by the means of a unitary matrix.
A compromise between rate and complexity is then obtained through puncturing
our rate-2 code giving rise to a new rate-3/2 code. The proposed codes are
compared to existing codes in the literature and simulation results show that
our rate-3/2 code has a lower average decoding complexity while our rate-2 code
maintains its lower average decoding complexity in the low SNR region at the
expense of a small performance loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4015</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4015</id><created>2012-04-18</created><authors><author><keyname>Ramesh</keyname><forenames>Amitash</forenames></author><author><keyname>Ramesh</keyname><forenames>Soumya</forenames></author><author><keyname>Iyengar</keyname><forenames>Sudarshan</forenames></author><author><keyname>Sekhar</keyname><forenames>Vinod</forenames></author></authors><title>Human Navigational Performance in a Complex Network with Progressive
  Disruptions</title><categories>physics.soc-ph cs.HC cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current paper is an investigation towards understanding the navigational
performance of humans on a network when the &quot;landmark&quot; nodes are blocked. We
observe that humans learn to cope up, despite the continued introduction of
blockages in the network. The experiment proposed involves the task of
navigating on a word network based on a puzzle called the wordmorph. We
introduce blockages in the network and report an incremental improvement in
performance with respect to time. We explain this phenomenon by analyzing the
evolution of the knowledge in the human participants of the underlying network
as more and more landmarks are removed. We hypothesize that humans learn the
bare essentials to navigate unless we introduce blockages in the network which
would whence enforce upon them the need to explore newer ways of navigating. We
draw a parallel to human problem solving and postulate that obstacles are
catalysts for humans to innovate techniques to solve a restricted variant of a
familiar problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4031</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4031</id><created>2012-04-18</created><authors><author><keyname>Fleischer</keyname><forenames>Lisa</forenames></author><author><keyname>Lyu</keyname><forenames>Yu-Han</forenames></author></authors><title>Approximately Optimal Auctions for Selling Privacy when Costs are
  Correlated with Data</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a scenario in which a database stores sensitive data of users and
an analyst wants to estimate statistics of the data. The users may suffer a
cost when their data are used in which case they should be compensated. The
analyst wishes to get an accurate estimate, while the users want to maximize
their utility. We want to design a mechanism that can estimate statistics
accurately without compromising users' privacy.
  Since users' costs and sensitive data may be correlated, it is important to
protect the privacy of both data and cost. We model this correlation by
assuming that a user's unknown sensitive data determines a distribution from a
set of publicly known distributions and a user's cost is drawn from that
distribution. We propose a stronger model of privacy preserving mechanism where
users are compensated whenever they reveal information about their data to the
mechanism. In this model, we design a Bayesian incentive compatible and privacy
preserving mechanism that guarantees accuracy and protects the privacy of both
cost and data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4044</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4044</id><created>2012-04-18</created><authors><author><keyname>Vila&#xe7;a</keyname><forenames>Xavier</forenames></author><author><keyname>Denysyuk</keyname><forenames>Oksana</forenames></author><author><keyname>Rodrigues</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>Asynchrony and Collusion in the N-party BAR Transfer Problem</title><categories>cs.DC</categories><comments>13 pages, 3 algorithms, to appear in Proceedings of the 19th
  International Colloquium on Structural Information and Communication
  Complexity (SIROCCO 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of reliably transferring data from a set of $N_P$ producers to a
set of $N_C$ consumers in the BAR model, named N-party BAR Transfer (NBART), is
an important building block for volunteer computing systems. An algorithm to
solve this problem in synchronous systems, which provides a Nash equilibrium,
has been presented in previous work. In this paper, we propose an NBART
algorithm for asynchronous systems. Furthermore, we also address the
possibility of collusion among the Rational processes. Our game theoretic
analysis shows that the proposed algorithm tolerates certain degree of
arbitrary collusion, while still fulfilling the NBART properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4047</identifier>
 <datestamp>2012-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4047</id><created>2012-04-18</created><updated>2012-08-22</updated><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Gupta</keyname><forenames>Sushmita</forenames></author><author><keyname>Larsen</keyname><forenames>Kim S.</forenames></author></authors><title>Access Graphs Results for LRU versus FIFO under Relative Worst Order
  Analysis</title><categories>cs.DS</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access graphs, which have been used previously in connection with competitive
analysis to model locality of reference in paging, are considered in connection
with relative worst order analysis. In this model, FWF is shown to be strictly
worse than both LRU and FIFO on any access graph. LRU is shown to be strictly
better than FIFO on paths and cycles, but they are incomparable on some
families of graphs which grow with the length of the sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4051</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4051</id><created>2012-04-18</created><authors><author><keyname>Barth&#xe9;lemy</keyname><forenames>Thibaut</forenames></author><author><keyname>Geiger</keyname><forenames>Martin Josef</forenames></author><author><keyname>Sevaux</keyname><forenames>Marc</forenames></author></authors><title>Solution Representations and Local Search for the bi-objective Inventory
  Routing Problem</title><categories>cs.AI</categories><comments>Proceedings of EU/ME 2012, Workshop on Metaheuristics for Global
  Challenges, May 10-11, 2012, Copenhagen, Denmark</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The solution of the biobjective IRP is rather challenging, even for
metaheuristics. We are still lacking a profound understanding of appropriate
solution representations and effective neighborhood structures. Clearly, both
the delivery volumes and the routing aspects of the alternatives need to be
reflected in an encoding, and must be modified when searching by means of local
search. Our work contributes to the better understanding of such solution
representations. On the basis of an experimental investigation, the advantages
and drawbacks of two encodings are studied and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4054</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4054</id><created>2012-04-18</created><authors><author><keyname>Nikoletseas</keyname><forenames>S.</forenames></author><author><keyname>Raptopoulos</keyname><forenames>C.</forenames></author><author><keyname>Spirakis</keyname><forenames>P. G.</forenames></author></authors><title>Maximum Cliques in Graphs with Small Intersection Number and Random
  Intersection Graphs</title><categories>cs.DM math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we relate the problem of finding a maximum clique to the
intersection number of the input graph (i.e. the minimum number of cliques
needed to edge cover the graph). In particular, we consider the maximum clique
problem for graphs with small intersection number and random intersection
graphs (a model in which each one of $m$ labels is chosen independently with
probability $p$ by each one of $n$ vertices, and there are edges between any
vertices with overlaps in the labels chosen).
  We first present a simple algorithm which, on input $G$ finds a maximum
clique in $O(2^{2^m + O(m)} + n^2 \min\{2^m, n\})$ time steps, where $m$ is an
upper bound on the intersection number and $n$ is the number of vertices.
Consequently, when $m \leq \ln{\ln{n}}$ the running time of this algorithm is
polynomial.
  We then consider random instances of the random intersection graphs model as
input graphs. As our main contribution, we prove that, when the number of
labels is not too large ($m=n^{\alpha}, 0&lt; \alpha &lt;1$), we can use the label
choices of the vertices to find a maximum clique in polynomial time whp. The
proof of correctness for this algorithm relies on our Single Label Clique
Theorem, which roughly states that whp a &quot;large enough&quot; clique cannot be formed
by more than one label. This theorem generalizes and strengthens other related
results in the state of the art, but also broadens the range of values
considered.
  As an important consequence of our Single Label Clique Theorem, we prove that
the problem of inferring the complete information of label choices for each
vertex from the resulting random intersection graph (i.e. the \emph{label
representation of the graph}) is \emph{solvable} whp. Finding efficient
algorithms for constructing such a label representation is left as an
interesting open problem for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4065</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4065</id><created>2012-04-18</created><updated>2012-07-10</updated><authors><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author><author><keyname>Aurell</keyname><forenames>Erik</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars</forenames></author></authors><title>Analysis of Sparse Representations Using Bi-Orthogonal Dictionaries</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures. The main result and numerical examples have been
  revised</comments><doi>10.1109/ITW.2012.6404757</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sparse representation problem of recovering an N dimensional sparse
vector x from M &lt; N linear observations y = Dx given dictionary D is
considered. The standard approach is to let the elements of the dictionary be
independent and identically distributed (IID) zero-mean Gaussian and minimize
the l1-norm of x under the constraint y = Dx. In this paper, the performance of
l1-reconstruction is analyzed, when the dictionary is bi-orthogonal D = [O1
O2], where O1,O2 are independent and drawn uniformly according to the Haar
measure on the group of orthogonal M x M matrices. By an application of the
replica method, we obtain the critical conditions under which perfect
l1-recovery is possible with bi-orthogonal dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4071</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4071</id><created>2012-04-18</created><authors><author><keyname>Chamberlain</keyname><forenames>Jon</forenames></author><author><keyname>Kruschwitz</keyname><forenames>Udo</forenames></author><author><keyname>Poesio</keyname><forenames>Massimo</forenames></author></authors><title>Motivations for Participation in Socially Networked Collective
  Intelligence Systems</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/50</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most significant challenges facing systems of collective
intelligence is how to encourage participation on the scale required to produce
high quality data. This paper details ongoing work with Phrase Detectives, an
online game-with-a-purpose deployed on Facebook, and investigates user
motivations for participation in social network gaming where the wisdom of
crowds produces useful data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4073</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4073</id><created>2012-04-18</created><authors><author><keyname>Rajashekar</keyname><forenames>Rakshith</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Modulation Diversity for Spatial Modulation Using Complex Interleaved
  Orthogonal Design</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose modulation diversity techniques for Spatial
Modulation (SM) system using Complex Interleaved Orthogonal Design (CIOD) meant
for two transmit antennas. Specifically, we show that by using the CIOD for two
transmit antenna system, the standard SM scheme, where only one transmit
antenna is activated in any symbol duration, can achieve a transmit diversity
order of two. We show with our simulation results that the proposed schemes
offer transmit diversity order of two, and hence, give a better Symbol Error
Rate performance than the SM scheme with transmit diversity order of one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4082</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4082</id><created>2012-04-18</created><authors><author><keyname>Harju</keyname><forenames>Manu</forenames></author></authors><title>On probabilities of Risk type board game combats</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Risk is a well-known turn based board game where the primary objective is
nothing less than the world domination. Gameplay is based on battles between
armies located in adjacent territories on the map of Earth. The combat's
outcome is decided by rolling dice, and therefore a probabilistic approach can
be taken. Although several results are derived, the conclusions suggest that
the gameplay is highly depending on luck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4086</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4086</id><created>2012-04-18</created><authors><author><keyname>Gracia</keyname><forenames>Jose</forenames></author><author><keyname>Niethammer</keyname><forenames>Christoph</forenames></author><author><keyname>Hasert</keyname><forenames>Manuel</forenames></author><author><keyname>Brinkmann</keyname><forenames>Steffen</forenames></author><author><keyname>Keller</keyname><forenames>Rainer</forenames></author><author><keyname>Glass</keyname><forenames>Colin W.</forenames></author></authors><title>Hybrid MPI/StarSs - a case study</title><categories>cs.DC</categories><comments>8 pages, accepted for publication in ISPA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid parallel programming models combining distributed and shared memory
paradigms are well established in high-performance computing. The classical
prototype of hybrid programming in HPC is MPI/OpenMP, but many other
combinations are being investigated. Recently, the data-dependency driven, task
parallel model for shared memory parallelisation named StarSs has been
suggested for usage in combination with MPI. In this paper we apply hybrid
MPI/StarSs to a Lattice-Boltzmann code. In particular, we present the hybrid
programming model, the benefits we expect, the challenges in porting, and
finally a comparison of the performance of MPI/StarSs hybrid, MPI/OpenMP hybrid
and the original MPI-only versions of the same code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4092</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4092</id><created>2012-04-17</created><authors><author><keyname>Ferreira</keyname><forenames>Sergio Andre</forenames></author><author><keyname>Andrade</keyname><forenames>Antonio</forenames></author></authors><title>Conception of a management tool of Technology Enhanced Learning
  Environments</title><categories>cs.CY</categories><journal-ref>International Journal of Advanced Computer Science and
  Applications,Vol. 3, No.2, 2012, pp.42-47</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper describes the process of the conception of a software tool of TELE
management. The proposed management tool combines information from two sources:
i) the automatic reports produced by the Learning Content Management System
(LCMS) Blackboard and ii) the views of students and teachers on the use of the
LCMS in the process of teaching and learning. The results show that the
architecture of the proposed management tool has the features of a management
tool, since its potential to control, to reset and to enhance the use of an
LCMS in the process of teaching and learning and teacher training, is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4093</identifier>
 <datestamp>2012-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4093</id><created>2012-04-18</created><updated>2012-08-15</updated><authors><author><keyname>Bennett</keyname><forenames>Casey</forenames></author></authors><title>Utilizing RxNorm to Support Practical Computing Applications: Capturing
  Medication History in Live Electronic Health Records</title><categories>cs.DB cs.HC</categories><comments>Appendix (including SQL/DDL Code) available by author request.
  Keywords: RxNorm; Electronic Health Record; Medication History;
  Interoperability; Unified Medical Language System; Search Optimization</comments><journal-ref>Journal of Biomedical Informatics 45: 634-641 (2012)</journal-ref><doi>10.1016/j.jbi.2012.02.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RxNorm was utilized as the basis for direct-capture of medication history
data in a live EHR system deployed in a large, multi-state outpatient
behavioral healthcare provider in the United States serving over 75,000
distinct patients each year across 130 clinical locations. This tool
incorporated auto-complete search functionality for medications and proper
dosage identification assistance. The overarching goal was to understand if and
how standardized terminologies like RxNorm can be used to support practical
computing applications in live EHR systems. We describe the stages of
implementation, approaches used to adapt RxNorm's data structure for the
intended EHR application, and the challenges faced. We evaluate the
implementation using a four-factor framework addressing flexibility, speed,
data integrity, and medication coverage. RxNorm proved to be functional for the
intended application, given appropriate adaptations to address high-speed
input/output (I/O) requirements of a live EHR and the flexibility required for
data entry in multiple potential clinical scenarios. Future research around
search optimization for medication entry, user profiling, and linking RxNorm to
drug classification schemes holds great potential for improving the user
experience and utility of medication data in EHRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4104</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4104</id><created>2012-04-18</created><updated>2014-01-20</updated><authors><author><keyname>Nandakumar</keyname><forenames>Satyadev</forenames></author><author><keyname>Vangapelli</keyname><forenames>Santhosh Kumar</forenames></author></authors><title>Normality and Finite-state Dimension of Liouville numbers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Liouville numbers were the first class of real numbers which were proven to
be transcendental. It is easy to construct non-normal Liouville numbers. Kano
and Bugeaud have proved, using analytic techniques, that there are normal
Liouville numbers. Here, for a given base k &gt;= 2, we give two simple
constructions of a Liouville number which is normal to the base k.
  The first construction is combinatorial, and is based on de Bruijn sequences.
A real number in the unit interval is normal if and only if its finite-state
dimension is 1. We generalize our construction to prove that for any rational r
in the closed unit interval, there is a Liouville number with finite state
dimension r. This refines Staiger's result that the set of Liouville numbers
has constructive Hausdorff dimension zero, showing a new quantitative
classification of Liouville numbers can be attained using finite-state
dimension.
  In the second number-theoretic construction, we use an arithmetic property of
numbers - the existence of primitive roots - to construct Liouville numbers
normal in finitely many bases, assuming a Generalized Artin's conjecture on
primitive roots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4106</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4106</id><created>2012-04-17</created><updated>2013-02-25</updated><authors><author><keyname>Cooper</keyname><forenames>Colin</forenames></author><author><keyname>Elsasser</keyname><forenames>Robert</forenames></author><author><keyname>Ono</keyname><forenames>Hirotaka</forenames></author><author><keyname>Radzik</keyname><forenames>Tomasz</forenames></author></authors><title>Coalescing random walks and voting on connected graphs</title><categories>cs.DS cs.DM math.CO math.PR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a coalescing random walk, a set of particles make independent random walks
on a graph. Whenever one or more particles meet at a vertex, they unite to form
a single particle, which then continues the random walk through the graph.
Coalescing random walks can be used to achieve consensus in distributed
networks, and is the basis of the self-stabilizing mutual exclusion algorithm
of Israeli and Jalfon.
  Let G=(V,E), be an undirected, connected n vertex graph with m edges. Let
C(n) be the expected time for all particles to coalesce, when initially one
particle is located at each vertex of an n vertex graph.
  We study the problem of bounding the coalescence time C(n) for general
classes of graphs. Our main result is that C(n)= O(1/(1-lambda_2))*((log n)^4
+n/A)), where lambda_2 is the absolute value of the second largest eigenvalue
of the transition matrix of the random walk, A= (sum d^2(v))/(d^2 n), d(v) is
the degree of vertex v, and d is the average node degree. The parameter A is an
indicator of the variability of node degrees. Thus 1 &lt;= A =O(n), with A=1 for
regular graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4107</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4107</id><created>2012-04-18</created><updated>2016-02-15</updated><authors><author><keyname>Preen</keyname><forenames>Richard J.</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Towards the Evolution of Vertical-Axis Wind Turbines using Supershapes</title><categories>cs.NE cs.CG</categories><journal-ref>Evolutionary Intelligence (2014), 7(3):155-167</journal-ref><doi>10.1007/s12065-014-0116-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently presented an initial study of evolutionary algorithms used
to design vertical-axis wind turbines (VAWTs) wherein candidate prototypes are
evaluated under approximated wind tunnel conditions after being physically
instantiated by a 3D printer. That is, unlike other approaches such as
computational fluid dynamics simulations, no mathematical formulations are used
and no model assumptions are made. However, the representation used
significantly restricted the range of morphologies explored. In this paper, we
present initial explorations into the use of a simple generative encoding,
known as Gielis superformula, that produces a highly flexible 3D shape
representation to design VAWT. First, the target-based evolution of 3D
artefacts is investigated and subsequently initial design experiments are
performed wherein each VAWT candidate is physically instantiated and evaluated
under approximated wind tunnel conditions. It is shown possible to produce very
closely matching designs of a number of 3D objects through the evolution of
supershapes produced by Gielis superformula. Moreover, it is shown possible to
use artificial physical evolution to identify novel and increasingly efficient
supershape VAWT designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4111</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4111</id><created>2012-04-18</created><authors><author><keyname>Harks</keyname><forenames>Tobias</forenames></author><author><keyname>Peis</keyname><forenames>Britta</forenames></author></authors><title>Resource Buying Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In resource buying games a set of players jointly buys a subset of a finite
resource set E (e.g., machines, edges, or nodes in a digraph). The cost of a
resource e depends on the number (or load) of players using e, and has to be
paid completely by the players before it becomes available. Each player i needs
at least one set of a predefined family S_i in 2^E to be available. Thus,
resource buying games can be seen as a variant of congestion games in which the
load-dependent costs of the resources can be shared arbitrarily among the
players. A strategy of player i in resource buying games is a tuple consisting
of one of i's desired configurations S_i together with a payment vector p_i in
R^E_+ indicating how much i is willing to contribute towards the purchase of
the chosen resources. In this paper, we study the existence and computational
complexity of pure Nash equilibria (PNE, for short) of resource buying games.
In contrast to classical congestion games for which equilibria are guaranteed
to exist, the existence of equilibria in resource buying games strongly depends
on the underlying structure of the S_i's and the behavior of the cost
functions. We show that for marginally non-increasing cost functions, matroids
are exactly the right structure to consider, and that resource buying games
with marginally non-decreasing cost functions always admit a PNE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4116</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4116</id><created>2012-04-18</created><authors><author><keyname>Kuipers</keyname><forenames>Benjamin</forenames></author></authors><title>An existing, ecologically-successful genus of collectively intelligent
  artificial creatures</title><categories>cs.SI cs.AI cs.MA physics.soc-ph</categories><comments>Presented at Collective Intelligence conference, 2012
  (arXiv:1204.2991)</comments><report-no>CollectiveIntelligence/2012/57</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People sometimes worry about the Singularity [Vinge, 1993; Kurzweil, 2005],
or about the world being taken over by artificially intelligent robots. I
believe the risks of these are very small. However, few people recognize that
we already share our world with artificial creatures that participate as
intelligent agents in our society: corporations. Our planet is inhabited by two
distinct kinds of intelligent beings --- individual humans and corporate
entities --- whose natures and interests are intimately linked. To co-exist
well, we need to find ways to define the rights and responsibilities of both
individual humans and corporate entities, and to find ways to ensure that
corporate entities behave as responsible members of society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4122</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4122</id><created>2012-04-18</created><authors><author><keyname>McNerney</keyname><forenames>James</forenames></author><author><keyname>Fath</keyname><forenames>Brian D.</forenames></author><author><keyname>Silverberg</keyname><forenames>Gerald</forenames></author></authors><title>Network structure of inter-industry flows</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>14 pages, 7 figures</comments><journal-ref>Physica A 392, (2013) 6427-6441</journal-ref><doi>10.1016/j.physa.2013.07.063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the structure of inter-industry relationships using networks of
money flows between industries in 20 national economies. We find these networks
vary around a typical structure characterized by a Weibull link weight
distribution, exponential industry size distribution, and a common community
structure. The community structure is hierarchical, with the top level of the
hierarchy comprising five industry communities: food industries, chemical
industries, manufacturing industries, service industries, and extraction
industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4134</identifier>
 <datestamp>2012-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4134</id><created>2012-04-18</created><authors><author><keyname>Janus Collaboration</keyname></author><author><keyname>Baity-Jesi</keyname><forenames>M.</forenames></author><author><keyname>Banos</keyname><forenames>R. A.</forenames></author><author><keyname>Cruz</keyname><forenames>A.</forenames></author><author><keyname>Fernandez</keyname><forenames>L. A.</forenames></author><author><keyname>Gil-Narvion</keyname><forenames>J. M.</forenames></author><author><keyname>Gordillo-Guerrero</keyname><forenames>A.</forenames></author><author><keyname>Guidetti</keyname><forenames>M.</forenames></author><author><keyname>Iniguez</keyname><forenames>D.</forenames></author><author><keyname>Maiorano</keyname><forenames>A.</forenames></author><author><keyname>Mantovani</keyname><forenames>F.</forenames></author><author><keyname>Marinari</keyname><forenames>E.</forenames></author><author><keyname>Martin-Mayor</keyname><forenames>V.</forenames></author><author><keyname>Monforte-Garcia</keyname><forenames>J.</forenames></author><author><keyname>Sudupe</keyname><forenames>A. Munoz</forenames></author><author><keyname>Navarro</keyname><forenames>D.</forenames></author><author><keyname>Parisi</keyname><forenames>G.</forenames></author><author><keyname>Pivanti</keyname><forenames>M.</forenames></author><author><keyname>Perez-Gaviro</keyname><forenames>S.</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>F.</forenames></author><author><keyname>Ruiz-Lorenzo</keyname><forenames>J. J.</forenames></author><author><keyname>Schifano</keyname><forenames>S. F.</forenames></author><author><keyname>Seoane</keyname><forenames>B.</forenames></author><author><keyname>Tarancon</keyname><forenames>A.</forenames></author><author><keyname>Tellez</keyname><forenames>P.</forenames></author><author><keyname>Tripiccione</keyname><forenames>R.</forenames></author><author><keyname>Yllanes</keyname><forenames>D.</forenames></author></authors><title>Reconfigurable computing for Monte Carlo simulations: results and
  prospects of the Janus project</title><categories>cond-mat.dis-nn cs.AR</categories><comments>19 pages, 3 figures</comments><journal-ref>The European Physical Journal - Special Topics 210, 33-51 (2012)</journal-ref><doi>10.1140/epjst/e2012-01636-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe Janus, a massively parallel FPGA-based computer optimized for the
simulation of spin glasses, theoretical models for the behavior of glassy
materials. FPGAs (as compared to GPUs or many-core processors) provide a
complementary approach to massively parallel computing. In particular, our
model problem is formulated in terms of binary variables, and floating-point
operations can be (almost) completely avoided. The FPGA architecture allows us
to run many independent threads with almost no latencies in memory access, thus
updating up to 1024 spins per cycle. We describe Janus in detail and we
summarize the physics results obtained in four years of operation of this
machine; we discuss two types of physics applications: long simulations on very
large systems (which try to mimic and provide understanding about the
experimental non-equilibrium dynamics), and low-temperature equilibrium
simulations using an artificial parallel tempering dynamics. The time scale of
our non-equilibrium simulations spans eleven orders of magnitude (from
picoseconds to a tenth of a second). On the other hand, our equilibrium
simulations are unprecedented both because of the low temperatures reached and
for the large systems that we have brought to equilibrium. A finite-time
scaling ansatz emerges from the detailed comparison of the two sets of
simulations. Janus has made it possible to perform spin-glass simulations that
would take several decades on more conventional architectures. The paper ends
with an assessment of the potential of possible future versions of the Janus
architecture, based on state-of-the-art technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4140</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4140</id><created>2012-04-18</created><authors><author><keyname>Lee</keyname><forenames>Chul-Ho</forenames></author><author><keyname>Xu</keyname><forenames>Xin</forenames></author><author><keyname>Eun</keyname><forenames>Do Young</forenames></author></authors><title>Beyond Random Walk and Metropolis-Hastings Samplers: Why You Should Not
  Backtrack for Unbiased Graph Sampling</title><categories>stat.ME cs.DS cs.NI cs.SI physics.data-an physics.soc-ph</categories><comments>A short (double-column, 12-page) version of this paper will appear in
  ACM SIGMETRICS/Performance 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph sampling via crawling has been actively considered as a generic and
important tool for collecting uniform node samples so as to consistently
estimate and uncover various characteristics of complex networks. The so-called
simple random walk with re-weighting (SRW-rw) and Metropolis-Hastings (MH)
algorithm have been popular in the literature for such unbiased graph sampling.
However, an unavoidable downside of their core random walks -- slow diffusion
over the space, can cause poor estimation accuracy. In this paper, we propose
non-backtracking random walk with re-weighting (NBRW-rw) and MH algorithm with
delayed acceptance (MHDA) which are theoretically guaranteed to achieve, at
almost no additional cost, not only unbiased graph sampling but also higher
efficiency (smaller asymptotic variance of the resulting unbiased estimators)
than the SRW-rw and the MH algorithm, respectively. In particular, a remarkable
feature of the MHDA is its applicability for any non-uniform node sampling like
the MH algorithm, but ensuring better sampling efficiency than the MH
algorithm. We also provide simulation results to confirm our theoretical
findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4141</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4141</id><created>2012-04-18</created><authors><author><keyname>Akimoto</keyname><forenames>Youhei</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Analysis of a Natural Gradient Algorithm on Monotonic
  Convex-Quadratic-Composite Functions</title><categories>cs.AI math.OC</categories><comments>Genetic and Evolutionary Computation Conference (GECCO 2012) (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the convergence properties of a variant of the
Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our study is based on
the recent theoretical foundation that the pure rank-mu update CMA-ES performs
the natural gradient descent on the parameter space of Gaussian distributions.
We derive a novel variant of the natural gradient method where the parameters
of the Gaussian distribution are updated along the natural gradient to improve
a newly defined function on the parameter space. We study this algorithm on
composites of a monotone function with a convex quadratic function. We prove
that our algorithm adapts the covariance matrix so that it becomes proportional
to the inverse of the Hessian of the original objective function. We also show
the speed of covariance matrix adaptation and the speed of convergence of the
parameters. We introduce a stochastic algorithm that approximates the natural
gradient with finite samples and present some simulated results to evaluate how
precisely the stochastic algorithm approximates the deterministic, ideal one
under finite samples and to see how similarly our algorithm and the CMA-ES
perform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4145</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4145</id><created>2012-04-18</created><authors><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Learning From An Optimization Viewpoint</title><categories>cs.LG cs.GT</categories><comments>Thesis supervisor : Nati Srebro Thesis Committee : David McAllester,
  Arkadi Nemirovski, Alexander Razborov, Nati Srebro</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this dissertation we study statistical and online learning problems from
an optimization viewpoint.The dissertation is divided into two parts :
  I. We first consider the question of learnability for statistical learning
problems in the general learning setting. The question of learnability is well
studied and fully characterized for binary classification and for real valued
supervised learning problems using the theory of uniform convergence. However
we show that for the general learning setting uniform convergence theory fails
to characterize learnability. To fill this void we use stability of learning
algorithms to fully characterize statistical learnability in the general
setting. Next we consider the problem of online learning. Unlike the
statistical learning framework there is a dearth of generic tools that can be
used to establish learnability and rates for online learning problems in
general. We provide online analogs to classical tools from statistical learning
theory like Rademacher complexity, covering numbers, etc. We further use these
tools to fully characterize learnability for online supervised learning
problems.
  II. In the second part, for general classes of convex learning problems, we
provide appropriate mirror descent (MD) updates for online and statistical
learning of these problems. Further, we show that the the MD is near optimal
for online convex learning and for most cases, is also near optimal for
statistical convex learning. We next consider the problem of convex
optimization and show that oracle complexity can be lower bounded by the so
called fat-shattering dimension of the associated linear class. Thus we
establish a strong connection between offline convex optimization problems and
statistical learning problems. We also show that for a large class of high
dimensional optimization problems, MD is in fact near optimal even for convex
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4151</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4151</id><created>2012-04-18</created><authors><author><keyname>Suthisopapan</keyname><forenames>Puripong</forenames></author><author><keyname>Meesomboon</keyname><forenames>Anupap</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Imtawil</keyname><forenames>Virasit</forenames></author></authors><title>Ultra Low Complexity Soft Output Detector for Non-Binary LDPC Coded
  Large MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theoretic results of MIMO capacity tell us that the higher the number of
antennas are employed, the higher the transmission rate is. This makes MIMO
systems with hundreds of antennas very attractive but one of the major problems
that obstructs such large dimensional MIMO systems from the practical
realization is a high complexity of the MIMO detector. We present in this paper
the new soft output MIMO detector based on matched filtering that can be
applied to the large MIMO systems which are coded by the powerful non-binary
LDPC codes. The per-bit complexity of the proposed detector is just 0.28% to
that of low complexity soft output MMSE detector and scales only linearly with
a number of antennas. Furthermore, the coded performances with small
information length 800 bits are within 4.2 dB from the associated MIMO
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4159</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4159</id><created>2012-04-18</created><updated>2012-04-23</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Wulff-Nilsen</keyname><forenames>Christian</forenames></author></authors><title>Connectivity Oracles for Planar Graphs</title><categories>cs.DS</categories><comments>extended abstract to appear in SWAT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dynamic subgraph connectivity problems for planar graphs. In this
model there is a fixed underlying planar graph, where each edge and vertex is
either &quot;off&quot; (failed) or &quot;on&quot; (recovered). We wish to answer connectivity
queries with respect to the &quot;on&quot; subgraph. The model has two natural variants,
one in which there are $d$ edge/vertex failures that precede all connectivity
queries, and one in which failures/recoveries and queries are intermixed.
  We present a $d$-failure connectivity oracle for planar graphs that processes
any $d$ edge/vertex failures in $sort(d,n)$ time so that connectivity queries
can be answered in $pred(d,n)$ time. (Here $sort$ and $pred$ are the time for
integer sorting and integer predecessor search over a subset of $[n]$ of size
$d$.) Our algorithm has two discrete parts. The first is an algorithm tailored
to triconnected planar graphs. It makes use of Barnette's theorem, which states
that every triconnected planar graph contains a degree-3 spanning tree. The
second part is a generic reduction from general (planar) graphs to triconnected
(planar) graphs. Our algorithm is, moreover, provably optimal. An implication
of Patrascu and Thorup's lower bound on predecessor search is that no
$d$-failure connectivity oracle (even on trees) can beat $pred(d,n)$ query
time.
  We extend our algorithms to the subgraph connectivity model where edge/vertex
failures (but no recoveries) are intermixed with connectivity queries. In
triconnected planar graphs each failure and query is handled in $O(\log n)$
time (amortized), whereas in general planar graphs both bounds become $O(\log^2
n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4166</identifier>
 <datestamp>2012-08-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4166</id><created>2012-04-18</created><updated>2012-08-29</updated><authors><author><keyname>Qi</keyname><forenames>Yuan</forenames></author><author><keyname>Guo</keyname><forenames>Yandong</forenames></author></authors><title>Message passing with relaxed moment matching</title><categories>cs.LG stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian learning is often hampered by large computational expense. As a
powerful generalization of popular belief propagation, expectation propagation
(EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP
can be sensitive to outliers and suffer from divergence for difficult cases. To
address this issue, we propose a new approximate inference approach, relaxed
expectation propagation (REP). It relaxes the moment matching requirement of
expectation propagation by adding a relaxation factor into the KL minimization.
We penalize this relaxation with a $l_1$ penalty. As a result, when two
distributions in the relaxed KL divergence are similar, the relaxation factor
will be penalized to zero and, therefore, we obtain the original moment
matching; In the presence of outliers, these two distributions are
significantly different and the relaxation factor will be used to reduce the
contribution of the outlier. Based on this penalized KL minimization, REP is
robust to outliers and can greatly improve the posterior approximation quality
over EP. To examine the effectiveness of REP, we apply it to Gaussian process
classification, a task known to be suitable to EP. Our classification results
on synthetic and UCI benchmark datasets demonstrate significant improvement of
REP over EP and Power EP--in terms of algorithmic stability, estimation
accuracy and predictive performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4176</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4176</id><created>2012-04-18</created><updated>2013-01-10</updated><authors><author><keyname>Chen</keyname><forenames>Ho-Lin</forenames></author><author><keyname>Doty</keyname><forenames>David</forenames></author><author><keyname>Soloveichik</keyname><forenames>David</forenames></author></authors><title>Deterministic Function Computation with Chemical Reaction Networks</title><categories>cs.CC cs.DC</categories><comments>fixed errors in previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemical reaction networks (CRNs) formally model chemistry in a well-mixed
solution. CRNs are widely used to describe information processing occurring in
natural cellular regulatory networks, and with upcoming advances in synthetic
biology, CRNs are a promising language for the design of artificial molecular
control circuitry. Nonetheless, despite the widespread use of CRNs in the
natural sciences, the range of computational behaviors exhibited by CRNs is not
well understood.
  CRNs have been shown to be efficiently Turing-universal when allowing for a
small probability of error. CRNs that are guaranteed to converge on a correct
answer, on the other hand, have been shown to decide only the semilinear
predicates. We introduce the notion of function, rather than predicate,
computation by representing the output of a function f:N^k --&gt; N^l by a count
of some molecular species, i.e., if the CRN starts with n_1,...,n_k molecules
of some &quot;input&quot; species X1,...,Xk, the CRN is guaranteed to converge to having
f(n_1,...,n_k) molecules of the &quot;output&quot; species Y1,...,Yl. We show that a
function f:N^k --&gt; N^l is deterministically computed by a CRN if and only if
its graph {(x,y) | f(x) = y} is a semilinear set. Furthermore, each semilinear
function f can be computed on input x in expected time O(polylog(|x|)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4200</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4200</id><created>2012-04-18</created><updated>2014-10-18</updated><authors><author><keyname>Preen</keyname><forenames>Richard J.</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Discrete Dynamical Genetic Programming in XCS</title><categories>cs.AI cs.LG cs.NE cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1201.5604</comments><acm-class>I.2.6</acm-class><journal-ref>In Proceedings of the 11th annual conference on genetic and
  evolutionary computation, GECCO '09, pp. 1299-1306. ACM, 2009</journal-ref><doi>10.1145/1569901.1570075</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of representation schemes have been presented for use within
Learning Classifier Systems, ranging from binary encodings to neural networks.
This paper presents results from an investigation into using a discrete
dynamical system representation within the XCS Learning Classifier System. In
particular, asynchronous random Boolean networks are used to represent the
traditional condition-action production system rules. It is shown possible to
use self-adaptive, open-ended evolution to design an ensemble of such discrete
dynamical systems within XCS to solve a number of well-known test problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4202</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4202</id><created>2012-04-18</created><authors><author><keyname>Preen</keyname><forenames>Richard J.</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Fuzzy Dynamical Genetic Programming in XCSF</title><categories>cs.AI cs.LG cs.NE cs.SY</categories><comments>2 page GECCO 2011 poster paper</comments><acm-class>I.2.6</acm-class><journal-ref>In Proceedings of the 13th annual conference companion on genetic
  and evolutionary computation, GECCO '11, pp. 167-168. ACM, 2011</journal-ref><doi>10.1145/2001858.2001952</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of representation schemes have been presented for use within
Learning Classifier Systems, ranging from binary encodings to Neural Networks,
and more recently Dynamical Genetic Programming (DGP). This paper presents
results from an investigation into using a fuzzy DGP representation within the
XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic
Networks are used to represent the traditional condition-action production
system rules. It is shown possible to use self-adaptive, open-ended evolution
to design an ensemble of such fuzzy dynamical systems within XCSF to solve
several well-known continuous-valued test problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4204</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4204</id><created>2012-04-18</created><updated>2012-10-21</updated><authors><author><keyname>Buzaglo</keyname><forenames>Sarit</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>Tilings with $n$-Dimensional Chairs and their Applications to Asymmetric
  Codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $n$-dimensional chair consists of an $n$-dimensional box from which a
smaller $n$-dimensional box is removed. A tiling of an $n$-dimensional chair
has two nice applications in coding for write-once memories. The first one is
in the design of codes which correct asymmetric errors with limited-magnitude.
The second one is in the design of $n$ cells $q$-ary write-once memory codes.
We show an equivalence between the design of a tiling with an integer lattice
and the design of a tiling from a generalization of splitting (or of Sidon
sequences). A tiling of an $n$-dimensional chair can define a perfect code for
correcting asymmetric errors with limited-magnitude. We present constructions
for such tilings and prove cases where perfect codes for these type of errors
do not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4209</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4209</id><created>2012-04-18</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>Folded Codes from Function Field Towers and Improved Optimal Rate List
  Decoding</title><categories>cs.IT cs.DS math.AG math.IT math.NT</categories><comments>Conference version appears at STOC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new construction of algebraic codes which are efficiently list
decodable from a fraction $1-R-\eps$ of adversarial errors where $R$ is the
rate of the code, for any desired positive constant $\eps$. The worst-case list
size output by the algorithm is $O(1/\eps)$, matching the existential bound for
random codes up to constant factors. Further, the alphabet size of the codes is
a constant depending only on $\eps$ - it can be made
$\exp(\tilde{O}(1/\eps^2))$ which is not much worse than the lower bound of
$\exp(\Omega(1/\eps))$. The parameters we achieve are thus quite close to the
existential bounds in all three aspects - error-correction radius, alphabet
size, and list-size - simultaneously. Our code construction is Monte Carlo and
has the claimed list decoding property with high probability. Once the code is
(efficiently) sampled, the encoding/decoding algorithms are deterministic with
a running time $O_\eps(N^c)$ for an absolute constant $c$, where $N$ is the
code's block length.
  Our construction is based on a linear-algebraic approach to list decoding
folded codes from towers of function fields, and combining it with a special
form of subspace-evasive sets. Instantiating this with the explicit
&quot;asymptotically good&quot; Garcia-Stichtenoth tower of function fields yields the
above parameters. To illustrate the method in a simpler setting, we also
present a construction based on Hermitian function fields, which offers similar
guarantees with a list and alphabet size polylogarithmic in the block length
$N$. Along the way, we shed light on how to use automorphisms of certain
function fields to enable list decoding of the folded version of the associated
algebraic-geometric codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4223</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4223</id><created>2012-04-18</created><updated>2015-05-13</updated><authors><author><keyname>Xie</keyname><forenames>Yixuan</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Improved Quantum LDPC Decoding Strategies For The Misidentified Quantum
  Depolarizing Channel</title><categories>cs.IT math.IT quant-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:arXiv:1202.0357</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum cryptography via key distribution mechanisms that utilize quantum
entanglement between sender-receiver pairs will form the basis of future
large-scale quantum networks. A key engineering challenge in such networks will
be the ability to correct for decoherence effects in the distributed
entanglement resources. It is widely believed that sophisticated quantum error
correction codes, such as quantum low-density parity-check (LDPC) codes, will
be pivotal in such a role. However, recently the importance of the channel
mismatch effect in degrading the performance of deployed quantum LDPC codes has
been pointed out. In this work we help remedy this situation by proposing new
quantum LDPC decoding strategies that can significantly reduce performance
degradation by as much as $50\%$. Our new strategies for the quantum LDPC
decoder are based on previous insights from classical LDPC decoders in
mismatched channels, where an asymmetry in performance is known as a function
of the estimated channel noise. We show how similar asymmetries carry over to
the quantum depolarizing channel, and how an estimate of the depolarization
flip parameter weighted to larger values leads to significant performance
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4224</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4224</id><created>2012-04-18</created><updated>2013-06-27</updated><authors><author><keyname>Schulte</keyname><forenames>Eric</forenames></author><author><keyname>Fry</keyname><forenames>Zachary P.</forenames></author><author><keyname>Fast</keyname><forenames>Ethan</forenames></author><author><keyname>Weimer</keyname><forenames>Westley</forenames></author><author><keyname>Forrest</keyname><forenames>Stephanie</forenames></author></authors><title>Software Mutational Robustness</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neutral landscapes and mutational robustness are believed to be important
enablers of evolvability in biology. We apply these concepts to software,
defining mutational robustness to be the fraction of random mutations that
leave a program's behavior unchanged. Test cases are used to measure program
behavior and mutation operators are taken from genetic programming. Although
software is often viewed as brittle, with small changes leading to catastrophic
changes in behavior, our results show surprising robustness in the face of
random software mutations.
  The paper describes empirical studies of the mutational robustness of 22
programs, including 14 production software projects, the Siemens benchmarks,
and 4 specially constructed programs. We find that over 30% of random mutations
are neutral with respect to their test suite. The results hold across all
classes of programs, for mutations at both the source code and assembly
instruction levels, across various programming languages, and are only weakly
related to test suite coverage. We conclude that mutational robustness is an
inherent property of software, and that neutral variants (i.e., those that pass
the test suite) often fulfill the program's original purpose or specification.
  Based on these results, we conjecture that neutral mutations can be leveraged
as a mechanism for generating software diversity. We demonstrate this idea by
generating a population of neutral program variants and showing that the
variants automatically repair unknown bugs with high probability. Neutral
landscapes also provide a partial explanation for recent results that use
evolutionary computation to automatically repair software bugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4227</identifier>
 <datestamp>2013-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4227</id><created>2012-04-18</created><updated>2013-02-25</updated><authors><author><keyname>Lopes</keyname><forenames>Miles E.</forenames></author></authors><title>Estimating Unknown Sparsity in Compressed Sensing</title><categories>cs.IT math.IT math.ST stat.ME stat.ML stat.TH</categories><comments>This is version 2. Many aspects of the paper have been revised. The
  restriction to non-negative signals has been removed. 22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the theory of compressed sensing (CS), the sparsity ||x||_0 of the unknown
signal x\in\R^p is commonly assumed to be a known parameter. However, it is
typically unknown in practice. Due to the fact that many aspects of CS depend
on knowing ||x||_0, it is important to estimate this parameter in a data-driven
way. A second practical concern is that ||x||_0 is a highly unstable function
of x. In particular, for real signals with entries not exactly equal to 0, the
value ||x||_0=p is not a useful description of the effective number of
coordinates. In this paper, we propose to estimate a stable measure of sparsity
s(x):=||x||_1^2/||x||_2^2, which is a sharp lower bound on ||x||_0. Our
estimation procedure uses only a small number of linear measurements, does not
rely on any sparsity assumptions, and requires very little computation. A
confidence interval for s(x) is provided, and its width is shown to have no
dependence on the signal dimension p. Moreover, this result extends naturally
to the matrix recovery setting, where a soft version of matrix rank can be
estimated with analogous guarantees. Finally, we show that the use of
randomized measurements is essential to estimating s(x). This is accomplished
by proving that the minimax risk for estimating s(x) with deterministic
measurements is large when n&lt;&lt;p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4230</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4230</id><created>2012-04-18</created><authors><author><keyname>Fomin</keyname><forenames>Fedor</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Planar F-Deletion: Approximation and Optimal FPT Algorithms</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let F be a finite set of graphs. In the F-Deletion problem, we are given an
n-vertex, m-edge graph G and an integer k as input, and asked whether at most k
vertices can be deleted from G such that the resulting graph does not contain a
graph from F as a minor. F-Deletion is a generic problem and by selecting
different sets of forbidden minors F, one can obtain various fundamental
problems such as Vertex Cover, Feedback Vertex Set or Treewidth t-Deletion.
  In this paper we obtain a number of generic algorithmic results about Planar
F-Deletion, when F contains at least one planar graph. The highlights of our
work are
  - A randomized O(nm) time constant factor approximation algorithm for the
optimization version of Planar F-Deletion.
  - A randomized O(2^{O(k)} n) time parameterized algorithm for Planar
F-Deletion when F is connected. Here a family F is called connected if every
graph in F is connected. The algorithm can be made deterministic at the cost of
making the polynomial factor in the running time n\log^2 n rather than linear.
  These algorithms unify, generalize, and improve over a multitude of results
in the literature. Our main results have several direct applications, but also
the methods we develop on the way have applicability beyond the scope of this
paper. Our results -- constant factor approximation and FPT algorithms -- are
stringed together by a common theme of polynomial time preprocessing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4249</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4249</id><created>2012-04-19</created><updated>2014-08-11</updated><authors><author><keyname>Truong</keyname><forenames>Lan V.</forenames></author></authors><title>Posterior Matching Scheme for Gaussian Multiple Access Channel with
  Feedback</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Transactions on Information Theory. A shorter
  version has been accepted to IEEE Information Theory Workshop 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Posterior matching is a method proposed by Ofer Shayevitz and Meir Feder to
design capacity achieving coding schemes for general point-to-point memoryless
channels with feedback. In this paper, we present a way to extend posterior
matching based encoding and variable rate decoding ideas for the Gaussian MAC
with feedback, referred to as time-varying posterior matching scheme, analyze
the achievable rate region and error probabilities of the extended
encoding-decoding scheme. The time-varying posterior matching scheme is a
generalization of the Shayevitz and Feder's posterior matching scheme when the
posterior distributions of the input messages given output are not fixed over
transmission time slots. It turns out that the well-known Ozarow's encoding
scheme, which obtains the capacity of two-user Gaussian channel, is a special
case of our extended posterior matching framework as the Schalkwijk-Kailath's
scheme is a special case of the point-to-point posterior matching mentioned
above. Furthermore, our designed posterior matching also obtains the
linear-feedback sum-capacity for the symmetric multiuser Gaussian MAC. Besides,
the encoding scheme in this paper is designed for the real Gaussian MAC to
obtain that performance, which is different from previous approaches where
encoding schemes are designed for the complex Gaussian MAC. More importantly,
this paper shows potential of posterior matching in designing optimal coding
schemes for multiuser channels with feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4250</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4250</id><created>2012-04-19</created><authors><author><keyname>Zhou</keyname><forenames>Shuming</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Xirong</forenames></author><author><keyname>Xu</keyname><forenames>Jun-Ming</forenames></author></authors><title>Conditional Fault Diagnosis of Bubble Sort Graphs under the PMC Model</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the size of a multiprocessor system increases, processor failure is
inevitable, and fault identification in such a system is crucial for reliable
computing. The fault diagnosis is the process of identifying faulty processors
in a multiprocessor system through testing. For the practical fault diagnosis
systems, the probability that all neighboring processors of a processor are
faulty simultaneously is very small, and the conditional diagnosability, which
is a new metric for evaluating fault tolerance of such systems, assumes that
every faulty set does not contain all neighbors of any processor in the
systems. This paper shows that the conditional diagnosability of bubble sort
graphs $B_n$ under the PMC model is $4n-11$ for $n \geq 4$, which is about four
times its ordinary diagnosability under the PMC model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4253</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4253</id><created>2012-04-19</created><updated>2013-11-02</updated><authors><author><keyname>Chou</keyname><forenames>Chun Tung</forenames></author></authors><title>Extended master equation models for molecular communication networks</title><categories>cs.CE physics.bio-ph q-bio.QM</categories><comments>IEEE Transactions on Nanobioscience, 2013</comments><doi>10.1109/TNB.2013.2237785</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider molecular communication networks consisting of transmitters and
receivers distributed in a fluidic medium. In such networks, a transmitter
sends one or more signalling molecules, which are diffused over the medium, to
the receiver to realise the communication. In order to be able to engineer
synthetic molecular communication networks, mathematical models for these
networks are required. This paper proposes a new stochastic model for molecular
communication networks called reaction-diffusion master equation with exogenous
input (RDMEX). The key idea behind RDMEX is to model the transmitters as time
series of signalling molecule counts, while diffusion in the medium and
chemical reactions at the receivers are modelled as Markov processes using
master equation. An advantage of RDMEX is that it can readily be used to model
molecular communication networks with multiple transmitters and receivers. For
the case where the reaction kinetics at the receivers is linear, we show how
RDMEX can be used to determine the mean and covariance of the receiver output
signals, and derive closed-form expressions for the mean receiver output signal
of the RDMEX model. These closed-form expressions reveal that the output signal
of a receiver can be affected by the presence of other receivers. Numerical
examples are provided to demonstrate the properties of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4257</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4257</id><created>2012-04-19</created><authors><author><keyname>Khan</keyname><forenames>Aamir</forenames></author><author><keyname>Farhan</keyname><forenames>Muhammad</forenames></author><author><keyname>Ali</keyname><forenames>Asar</forenames></author></authors><title>Speech Recognition: Increasing Efficiency of Support Vector Machines</title><categories>cs.CV</categories><comments>5 pages, 11 figures. arXiv admin note: text overlap with
  arXiv:1201.3720 and arXiv:1204.1177</comments><journal-ref>International Journal of Computer Applications 35(7):17-21,
  December 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of communication and security technologies, it has
become crucial to have robustness of embedded biometric systems. This paper
presents the realization of such technologies which demands reliable and
error-free biometric identity verification systems. High dimensional patterns
are not permitted due to eigen-decomposition in high dimensional feature space
and degeneration of scattering matrices in small size sample. Generalization,
dimensionality reduction and maximizing the margins are controlled by
minimizing weight vectors. Results show good pattern by multimodal biometric
system proposed in this paper. This paper is aimed at investigating a biometric
identity system using Support Vector Machines(SVMs) and Lindear Discriminant
Analysis(LDA) with MFCCs and implementing such system in real-time using
SignalWAVE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4262</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4262</id><created>2012-04-19</created><authors><author><keyname>Ren</keyname><forenames>Shaolei</forenames></author><author><keyname>Park</keyname><forenames>Jaeok</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Entry and Spectrum Sharing Scheme Selection in Femtocell Markets</title><categories>cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1008.5367</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on a femtocell communications market, we study the entrant network
service provider's (NSP's) long-term decision: whether to enter the market and
which spectrum sharing technology to select to maximize its profit. This
long-term decision is closely related to the entrant's pricing strategy and the
users' aggregate demand, which we model as medium-term and short-term
decisions, respectively. We consider two markets, one with no incumbent and the
other with one incumbent. For both markets, we show the existence and
uniqueness of an equilibrium point in the user subscription dynamics, and
provide a sufficient condition for the convergence of the dynamics. For the
market with no incumbent, we derive upper and lower bounds on the optimal price
and market share that maximize the entrant's revenue, based on which the
entrant selects an available technology to maximize its long-term profit. For
the market with one incumbent, we model competition between the two NSPs as a
non-cooperative game, in which the incumbent and the entrant choose their
market shares independently, and provide a sufficient condition that guarantees
the existence of at least one pure Nash equilibrium. Finally, we formalize the
problem of entry and spectrum sharing scheme selection for the entrant and
provide numerical results to complement our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4286</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4286</id><created>2012-04-19</created><authors><author><keyname>Gutman</keyname><forenames>Avital</forenames></author><author><keyname>Nisan</keyname><forenames>Noam</forenames></author></authors><title>Fair Allocation Without Trade</title><categories>cs.GT</categories><comments>Published in AAMAS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the age-old problem of allocating items among different agents in
a way that is efficient and fair. Two papers, by Dolev et al. and Ghodsi et
al., have recently studied this problem in the context of computer systems.
Both papers had similar models for agent preferences, but advocated different
notions of fairness. We formalize both fairness notions in economic terms,
extending them to apply to a larger family of utilities. Noting that in
settings with such utilities efficiency is easily achieved in multiple ways, we
study notions of fairness as criteria for choosing between different efficient
allocations. Our technical results are algorithms for finding fair allocations
corresponding to two fairness notions: Regarding the notion suggested by Ghodsi
et al., we present a polynomial-time algorithm that computes an allocation for
a general class of fairness notions, in which their notion is included. For the
other, suggested by Dolev et al., we show that a competitive market equilibrium
achieves the desired notion of fairness, thereby obtaining a polynomial-time
algorithm that computes such a fair allocation and solving the main open
problem raised by Dolev et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4294</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4294</id><created>2012-04-19</created><authors><author><keyname>Jain</keyname><forenames>Brijnesh J.</forenames></author><author><keyname>Obermayer</keyname><forenames>Klaus</forenames></author></authors><title>Learning in Riemannian Orbifolds</title><categories>cs.LG cs.AI cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1001.0921</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning in Riemannian orbifolds is motivated by existing machine learning
algorithms that directly operate on finite combinatorial structures such as
point patterns, trees, and graphs. These methods, however, lack statistical
justification. This contribution derives consistency results for learning
problems in structured domains and thereby generalizes learning in vector
spaces and manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4300</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4300</id><created>2012-04-19</created><authors><author><keyname>Maria</keyname><forenames>Stella</forenames></author><author><keyname>Galinium</keyname><forenames>Maulahikmah</forenames></author><author><keyname>Fahmi</keyname><forenames>Husni</forenames></author><author><keyname>Faidah</keyname><forenames>Haret</forenames></author><author><keyname>Purnama</keyname><forenames>James</forenames></author><author><keyname>Lim</keyname><forenames>Charles</forenames></author><author><keyname>Damar</keyname><forenames>Harya</forenames></author></authors><title>Design and Implementation of the End System to Intermediate System
  (ES-IS) Routing Information Exchange Protocol as a Loadable Kernel Module in
  Linux Kernel 2.6</title><categories>cs.NI</categories><comments>This paper has 6 pages and has been published in the Proceedings of
  MoMM2007 &amp; iiWAS2007 Workshops</comments><journal-ref>2007, Frontiers in Mobile and Web Computing: Proceedings of
  MoMM2007 &amp; iiWAS2007 Workshops, pp. 247-252, published by \&quot;Osterreichische
  Computer Gesellschaft Komitee f\&quot;ur \&quot;Offentlichkeitsarbeit</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a partial implementation of the ES-IS Routing Information
Exchange Protocol packet processing in Linux Kernel 2.6, which is for use in
conjunction with the Connectionless Network Protocol (CLNP) in Aeronautical
Telecommunication Network (ATN). First, we show the data structures involved in
the protocol operation. Second, we describe the map of the packet processing
whose design has been developed in the research. Third, we explain how the
protocol is implemented as a loadable kernel module. Finally, we conclude the
implementation result based on performed tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4301</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4301</id><created>2012-04-19</created><authors><author><keyname>Sugiarto</keyname><forenames>Bunga</forenames></author><author><keyname>Laidi</keyname><forenames>Danny</forenames></author><author><keyname>Rizal</keyname><forenames>Arra'di Nur</forenames></author><author><keyname>Galinium</keyname><forenames>Maulahikmah</forenames></author><author><keyname>Atmadiputra</keyname><forenames>Pradana</forenames></author><author><keyname>Rubianto</keyname><forenames>Melvin</forenames></author><author><keyname>Fahmi</keyname><forenames>Husni</forenames></author><author><keyname>Sampurno</keyname><forenames>Tri</forenames></author><author><keyname>Kisworo</keyname><forenames>Marsudi</forenames></author></authors><title>Design and Implementation of the Connectionless Network Protocol (CLNP)
  as Loadable Kernel Modules in Linux Kernel 2.6</title><categories>cs.NI</categories><comments>7 pages conference paper. Published in Proceedings of MoMM2007 &amp;
  iiWAS2007 Workshops</comments><journal-ref>2007, Frontiers in Mobile and Web Computing: Proceedings of
  MoMM2007 &amp; iiWAS2007 Workshops, pp. 239-245, published by \&quot;Osterreichische
  Computer Gesellschaft Komitee f\&quot;ur \&quot;Offentlichkeitsarbeit</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an implementation of CLNP ground-to-ground packet
processing for ATN in Linux kernel version 2.6. We present the big picture of
CLNP packet processing, the details of input, routing, and output processing
functions, and the implementation of each function based on ISO 8473-1. The
functions implemented in this work are PDU header decomposition, header format
analysis, header error detection, error reporting, reassembly, source routing,
congestion notification, forwarding, composition, segmentation, and transmit to
device functions. Each function is initially implemented and tested as a
separated loadable kernel module. These modules are successfully loaded into
Linux kernel 2.6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4307</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4307</id><created>2012-04-19</created><authors><author><keyname>Maseleno</keyname><forenames>Andino</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mahmud</forenames></author></authors><title>Avian Influenza (H5N1) Warning System using Dempster-Shafer Theory and
  Web Mapping</title><categories>cs.AI math.PR</categories><comments>International Seminar Indonesian Students in ASEAN &quot;Green Technology,
  Social Work and Public Health for the Development of Indonesia&quot;, 28 - 29
  October 2011, Bangkok, Thailand</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Based on Cumulative Number of Confirmed Human Cases of Avian Influenza (H5N1)
Reported to World Health Organization (WHO) in the 2011 from 15 countries,
Indonesia has the largest number death because Avian Influenza which 146
deaths. In this research, the researcher built a Web Mapping and
Dempster-Shafer theory as early warning system of avian influenza. Early
warning is the provision of timely and effective information, through
identified institutions, that allows individuals exposed to a hazard to take
action to avoid or reduce their risk and prepare for effective response. In
this paper as example we use five symptoms as major symptoms which include
depression, combs, wattle, bluish face region, swollen face region, narrowness
of eyes, and balance disorders. Research location is in the Lampung Province,
South Sumatera. The researcher reason to choose Lampung Province in South
Sumatera on the basis that has a high poultry population. Geographically,
Lampung province is located at 103040' to 105050' East Longitude and 6045' -
3045' South latitude, confined with: South Sumatera and Bengkulu on North Side,
Sunda Strait on the Side, Java Sea on the East Side, Indonesia Ocean on the
West Side. Our approach uses Dempster Shafer theory to combine beliefs in
certain hypotheses under conditions of uncertainty and ignorance, and allows
quantitative measurement of the belief and plausibility in our identification
result. Web Mapping is also used for displaying maps on a screen to visualize
the result of the identification process. The result reveal that avian
influenza warning system has successfully identified the existence of avian
influenza and the maps can be displayed as the visualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4311</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4311</id><created>2012-04-19</created><authors><author><keyname>Maseleno</keyname><forenames>Andino</forenames></author><author><keyname>Hasan</keyname><forenames>Md. Mahmud</forenames></author></authors><title>Avian Influenza (H5N1) Expert System using Dempster-Shafer Theory</title><categories>cs.AI math.PR</categories><comments>International Conference on Informatics for Development 2011, 26
  November 2011, Yogyakarta, Indonesia</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Based on Cumulative Number of Confirmed Human Cases of Avian Influenza (H5N1)
Reported to World Health Organization (WHO) in the 2011 from 15 countries,
Indonesia has the largest number death because Avian Influenza which 146
deaths. In this research, the researcher built an Avian Influenza (H5N1) Expert
System for identifying avian influenza disease and displaying the result of
identification process. In this paper, we describe five symptoms as major
symptoms which include depression, combs, wattle, bluish face region, swollen
face region, narrowness of eyes, and balance disorders. We use chicken as
research object. Research location is in the Lampung Province, South Sumatera.
The researcher reason to choose Lampung Province in South Sumatera on the basis
that has a high poultry population. Dempster-Shafer theory to quantify the
degree of belief as inference engine in expert system, our approach uses
Dempster-Shafer theory to combine beliefs under conditions of uncertainty and
ignorance, and allows quantitative measurement of the belief and plausibility
in our identification result. The result reveal that Avian Influenza (H5N1)
Expert System has successfully identified the existence of avian influenza and
displaying the result of identification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4322</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4322</id><created>2012-04-19</created><updated>2012-06-04</updated><authors><author><keyname>Jensen</keyname><forenames>Thomas</forenames><affiliation>INRIA Rennes</affiliation></author><author><keyname>Kirchner</keyname><forenames>Florent</forenames><affiliation>INRIA Rennes</affiliation></author><author><keyname>Pichardie</keyname><forenames>David</forenames><affiliation>INRIA Rennes</affiliation></author></authors><title>Secure the Clones</title><categories>cs.PL</categories><proxy>LMCS</proxy><acm-class>I.1.2, F.3.1, F.3.3, D.3.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 2 (May 31,
  2012) lmcs:801</journal-ref><doi>10.2168/LMCS-8(2:5)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exchanging mutable data objects with untrusted code is a delicate matter
because of the risk of creating a data space that is accessible by an attacker.
Consequently, secure programming guidelines for Java stress the importance of
using defensive copying before accepting or handing out references to an
internal mutable object. However, implementation of a copy method (like
clone()) is entirely left to the programmer. It may not provide a sufficiently
deep copy of an object and is subject to overriding by a malicious sub-class.
Currently no language-based mechanism supports secure object cloning. This
paper proposes a type-based annotation system for defining modular copy
policies for class-based object-oriented programs. A copy policy specifies the
maximally allowed sharing between an object and its clone. We present a static
enforcement mechanism that will guarantee that all classes fulfil their copy
policy, even in the presence of overriding of copy methods, and establish the
semantic correctness of the overall approach in Coq. The mechanism has been
implemented and experimentally evaluated on clone methods from several Java
libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4323</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4323</id><created>2012-04-19</created><updated>2013-04-17</updated><authors><author><keyname>Chattopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Sinha</keyname><forenames>Abhishek</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>Optimal Capacity Relay Node Placement in a Multi-hop Wireless Network on
  a Line</title><categories>cs.NI</categories><comments>22 pages, 12 figures; the initial version of this work was accepted
  in RAWNET 2012 (an workshop of WiOpt 2012); this is a substantial extension
  of the workshop paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use information theoretic achievable rate formulas for the multi-relay
channel to study the problem of optimal placement of relay nodes along the
straight line joining a source node and a sink node. The achievable rate
formulas that we use are for full-duplex radios at the relays and decode-
and-forward relaying. For the single relay case, and individual power
constraints at the source node and the relay node, we provide explicit formulas
for the optimal relay location and the optimal power allocation to the
source-relay channel, for the exponential and the power-law path-loss channel
models. For the multiple relay case, we consider exponential path-loss and a
total power constraint over the source and the relays, and derive an
optimization problem, the solution of which provides the optimal relay
locations. Numerical results suggest that at low attenuation the relays are
mostly clustered close to the source in order to be able to cooperate among
themselves, whereas at high attenuation they are uniformly placed and work as
repeaters.
  The structure of the optimal power allocation for a given placement of the
nodes, then motivates us to formulate the problem of impromptu (&quot;as-you-go&quot;)
placement of relays along a line of exponentially distributed length, with
exponential path- loss, so as to minimize a cost function that is additive over
hops. The hop cost trades off a capacity limiting term, motivated from the
optimal power allocation solution, against the cost of adding a relay node. We
formulate the problem as a total cost Markov decision process, for which we
prove results for the value function, and provide insights into the placement
policy via numerical exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4329</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4329</id><created>2012-04-19</created><authors><author><keyname>Taillandier</keyname><forenames>Patrick</forenames><affiliation>UMMISCO</affiliation></author><author><keyname>Drogoul</keyname><forenames>Alexis</forenames><affiliation>UMMISCO, MSI</affiliation></author></authors><title>Supervised feature evaluation by consistency analysis: application to
  measure sets used to characterise geographic objects</title><categories>cs.LG</categories><proxy>ccsd</proxy><journal-ref>International Conference on Knowledge and Systems Engineering,
  Hanoi : Viet Nam (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, supervised learning is commonly used in many domains. Indeed, many
works propose to learn new knowledge from examples that translate the expected
behaviour of the considered system. A key issue of supervised learning concerns
the description language used to represent the examples. In this paper, we
propose a method to evaluate the feature set used to describe them. Our method
is based on the computation of the consistency of the example base. We carried
out a case study in the domain of geomatic in order to evaluate the sets of
measures used to characterise geographic objects. The case study shows that our
method allows to give relevant evaluations of measure sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4332</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4332</id><created>2012-04-19</created><authors><author><keyname>Taillandier</keyname><forenames>Patrick</forenames><affiliation>UMMISCO</affiliation></author><author><keyname>Gaffuri</keyname><forenames>Julien</forenames><affiliation>COGIT</affiliation></author></authors><title>Designing generalisation evaluation function through human-machine
  dialogue</title><categories>cs.HC cs.LG</categories><proxy>ccsd</proxy><journal-ref>GIScience, Zurich : Switzerland (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated generalisation has known important improvements these last few
years. However, an issue that still deserves more study concerns the automatic
evaluation of generalised data. Indeed, many automated generalisation systems
require the utilisation of an evaluation function to automatically assess
generalisation outcomes. In this paper, we propose a new approach dedicated to
the design of such a function. This approach allows an imperfectly defined
evaluation function to be revised through a man-machine dialogue. The user
gives its preferences to the system by comparing generalisation outcomes.
Machine Learning techniques are then used to improve the evaluation function.
An experiment carried out on buildings shows that our approach significantly
improves generalisation evaluation functions defined by users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4346</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4346</id><created>2012-04-19</created><authors><author><keyname>Cook</keyname><forenames>James</forenames></author><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author><author><keyname>Fabrikant</keyname><forenames>Alex</forenames></author><author><keyname>Tomkins</keyname><forenames>Andrew</forenames></author></authors><title>Your Two Weeks of Fame and Your Grandmother's</title><categories>cs.DL cs.CL cs.SI physics.soc-ph</categories><comments>This version supercedes the short version of this paper published in
  the proceedings of WWW 2012</comments><acm-class>J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Did celebrity last longer in 1929, 1992 or 2009? We investigate the
phenomenon of fame by mining a collection of news articles that spans the
twentieth century, and also perform a side study on a collection of blog posts
from the last 10 years. By analyzing mentions of personal names, we measure
each person's time in the spotlight, using two simple metrics that evaluate,
roughly, the duration of a single news story about a person, and the overall
duration of public interest in a person. We watched the distribution evolve
from 1895 to 2010, expecting to find significantly shortening fame durations,
per the much popularly bemoaned shortening of society's attention spans and
quickening of media's news cycles. Instead, we conclusively demonstrate that,
through many decades of rapid technological and societal change, through the
appearance of Twitter, communication satellites, and the Internet, fame
durations did not decrease, neither for the typical case nor for the extremely
famous, with the last statistically significant fame duration decreases coming
in the early 20th century, perhaps from the spread of telegraphy and telephony.
Furthermore, while median fame durations stayed persistently constant, for the
most famous of the famous, as measured by either volume or duration of media
attention, fame durations have actually trended gently upward since the 1940s,
with statistically significant increases on 40-year timescales. Similar studies
have been done with much shorter timescales specifically in the context of
information spreading on Twitter and similar social networking sites. To the
best of our knowledge, this is the first massive scale study of this nature
that spans over a century of archived data, thereby allowing us to track
changes across decades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4347</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4347</id><created>2012-04-19</created><authors><author><keyname>Sankaranarayanan</keyname><forenames>Sriram</forenames></author></authors><title>Change-Of-Bases Abstractions for Non-Linear Systems</title><categories>cs.SC cs.LO cs.SY</categories><comments>37 pages. Invited submission to National University of Singapore
  Inst. of Mathematical Science (IMS)</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present abstraction techniques that transform a given non-linear dynamical
system into a linear system or an algebraic system described by polynomials of
bounded degree, such that, invariant properties of the resulting abstraction
can be used to infer invariants for the original system. The abstraction
techniques rely on a change-of-basis transformation that associates each state
variable of the abstract system with a function involving the state variables
of the original system. We present conditions under which a given change of
basis transformation for a non-linear system can define an abstraction.
Furthermore, the techniques developed here apply to continuous systems defined
by Ordinary Differential Equations (ODEs), discrete systems defined by
transition systems and hybrid systems that combine continuous as well as
discrete subsystems. The techniques presented here allow us to discover, given
a non-linear system, if a change of bases transformation involving
degree-bounded polynomials yielding an algebraic abstraction exists. If so, our
technique yields the resulting abstract system, as well. This approach is
further extended to search for a change of bases transformation that abstracts
a given non-linear system into a system of linear differential inclusions. Our
techniques enable the use of analysis techniques for linear systems to infer
invariants for non-linear systems. We present preliminary evidence of the
practical feasibility of our ideas using a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4366</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4366</id><created>2012-04-19</created><authors><author><keyname>Robinson</keyname><forenames>Michael</forenames></author></authors><title>Multipath-dominant, pulsed doppler analysis of rotating blades</title><categories>cs.CE</categories><comments>9 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel angular fingerprinting algorithm for detecting changes in
the direction of rotation of a target with a monostatic, stationary sonar
platform. Unlike other approaches, we assume that the target's centroid is
stationary, and exploit doppler multipath signals to resolve the otherwise
unavoidable ambiguities that arise. Since the algorithm is based on an
underlying differential topological theory, it is highly robust to distortions
in the collected data. We demonstrate performance of this algorithm
experimentally, by exhibiting a pulsed doppler sonar collection system that
runs on a smartphone. The performance of this system is sufficiently good to
both detect changes in target rotation direction using angular fingerprints,
and also to form high-resolution inverse synthetic aperature images of the
target.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4368</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4368</id><created>2012-04-19</created><authors><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Muciaccia</keyname><forenames>G.</forenames></author><author><keyname>Yeo</keyname><forenames>A.</forenames></author></authors><title>(Non-)existence of Polynomial Kernels for the Test Cover Problem</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The input of the Test Cover problem consists of a set $V$ of vertices, and a
collection ${\cal E}=\{E_1,..., E_m\}$ of distinct subsets of $V$, called
tests. A test $E_q$ separates a pair $v_i,v_j$ of vertices if $|\{v_i,v_j\}\cap
E_q|=1.$ A subcollection ${\cal T}\subseteq {\cal E}$ is a test cover if each
pair $v_i,v_j$ of distinct vertices is separated by a test in ${\cal T}$. The
objective is to find a test cover of minimum cardinality, if one exists. This
problem is NP-hard.
  We consider two parameterizations the Test Cover problem with parameter $k$:
(a) decide whether there is a test cover with at most $k$ tests, (b) decide
whether there is a test cover with at most $|V|-k$ tests. Both
parameterizations are known to be fixed-parameter tractable. We prove that none
have a polynomial size kernel unless $NP\subseteq coNP/poly$. Our proofs use
the cross-composition method recently introduced by Bodlaender et al. (2011)
and parametric duality introduced by Chen et al. (2005). The result for the
parameterization (a) was an open problem (private communications with Henning
Fernau and Jiong Guo, Jan.-Feb. 2012). We also show that the parameterization
(a) admits a polynomial size kernel if the size of each test is upper-bounded
by a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4374</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4374</id><created>2012-04-19</created><authors><author><keyname>Gemsa</keyname><forenames>Andreas</forenames></author><author><keyname>Lee</keyname><forenames>D. T.</forenames></author><author><keyname>Liu</keyname><forenames>Chih-Hung</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Higher Order City Voronoi Diagrams</title><categories>cs.CG</categories><comments>15 pages, extended version of paper to appear in Proc. 13th
  Scandinavian Symposium and Workshops on Algorithm Theory (SWAT'12), Helsinki,
  Finland, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate higher-order Voronoi diagrams in the city metric. This metric
is induced by quickest paths in the L1 metric in the presence of an
accelerating transportation network of axis-parallel line segments. For the
structural complexity of kth-order city Voronoi diagrams of n point sites, we
show an upper bound of O(k(n - k) + kc) and a lower bound of {\Omega}(n + kc),
where c is the complexity of the transportation network. This is quite
different from the bound O(k(n - k)) in the Euclidean metric. For the special
case where k = n - 1 the complexity in the Euclidean metric is O(n), while that
in the city metric is {\Theta}(nc).
  Furthermore, we develop an O(k^2(n + c) log n)-time iterative algorithm to
compute the kth-order city Voronoi diagram and an O(nc log^2(n + c) log n)-time
divide-and-conquer algorithm to compute the farthest-site city Voronoi diagram.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4379</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4379</id><created>2012-04-19</created><updated>2012-10-30</updated><authors><author><keyname>Christandl</keyname><forenames>Matthias</forenames></author><author><keyname>Doran</keyname><forenames>Brent</forenames></author><author><keyname>Walter</keyname><forenames>Michael</forenames></author></authors><title>Computing Multiplicities of Lie Group Representations</title><categories>cs.CC math.RT quant-ph</categories><comments>10 pages</comments><journal-ref>Proceedings of 2012 IEEE 53rd Annual Symposium on Foundations of
  Computer Science (FOCS'12), p. 639-648</journal-ref><doi>10.1109/FOCS.2012.43</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fixed compact connected Lie groups H \subseteq G, we provide a polynomial
time algorithm to compute the multiplicity of a given irreducible
representation of H in the restriction of an irreducible representation of G.
Our algorithm is based on a finite difference formula which makes the
multiplicities amenable to Barvinok's algorithm for counting integral points in
polytopes.
  The Kronecker coefficients of the symmetric group, which can be seen to be a
special case of such multiplicities, play an important role in the geometric
complexity theory approach to the P vs. NP problem. Whereas their computation
is known to be #P-hard for Young diagrams with an arbitrary number of rows, our
algorithm computes them in polynomial time if the number of rows is bounded. We
complement our work by showing that information on the asymptotic growth rates
of multiplicities in the coordinate rings of orbit closures does not directly
lead to new complexity-theoretic obstructions beyond what can be obtained from
the moment polytopes of the orbit closures. Non-asymptotic information on the
multiplicities, such as provided by our algorithm, may therefore be essential
in order to find obstructions in geometric complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4411</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4411</id><created>2012-04-19</created><updated>2012-04-21</updated><authors><author><keyname>J&#xf6;rgensen</keyname><forenames>Mikael Erik</forenames></author></authors><title>Solutions to the generalized Towers of Hanoi problem</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><msc-class>90C27 (Combinatorial optimization)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to prove the Frame-Stewart algorithm for the
generalized Towers of Hanoi problem as well as finding the number of moves
required to solve the problem and studying the multitude of optimal solutions.
The main idea is to study how to most effectively move away all but the last
disc and use the fact that the total number of moves required to solve the
problem is twice this number plus one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4419</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4419</id><created>2012-04-19</created><updated>2013-08-19</updated><authors><author><keyname>Lavaei</keyname><forenames>Javad</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author></authors><title>Geometry of Power Flows and Optimization in Distribution Networks</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>To Appear in IEEE Transaction on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the geometry of injection regions and its relationship to
optimization of power flows in tree networks. The injection region is the set
of all vectors of bus power injections that satisfy the network and operation
constraints. The geometrical object of interest is the set of Pareto-optimal
points of the injection region. If the voltage magnitudes are fixed, the
injection region of a tree network can be written as a linear transformation of
the product of two-bus injection regions, one for each line in the network.
Using this decomposition, we show that under the practical condition that the
angle difference across each line is not too large, the set of Pareto-optimal
points of the injection region remains unchanged by taking the convex hull.
Moreover, the resulting convexified optimal power flow problem can be
efficiently solved via }{ semi-definite programming or second order cone
relaxations. These results improve upon earlier works by removing the
assumptions on active power lower bounds. It is also shown that our practical
angle assumption guarantees two other properties: (i) the uniqueness of the
solution of the power flow problem, and (ii) the non-negativity of the
locational marginal prices. Partial results are presented for the case when the
voltage magnitudes are not fixed but can lie within certain bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4427</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4427</id><created>2012-04-19</created><authors><author><keyname>Bouzguenda</keyname><forenames>Lotfi</forenames></author><author><keyname>Turki</keyname><forenames>Manel</forenames></author></authors><title>Coupling Clinical Decision Support System with Computerized Prescriber
  Order Entry and their Dynamic Plugging in the Medical Workflow System</title><categories>cs.MA</categories><comments>International Conference on Information Technology and e-services,
  ICITeS'12, IEEE, March 24-26,Sousse-Tunisia, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with coupling Clinical Decision Support System (CDSS) with
Computerized Prescriber Order Entry (CPOE) and their dynamic plugging in the
medical Workflow Management System (WfMS). First, in this paper we argue some
existing CDSS representative of the state of the art in order to emphasize
their inability to deal with coupling with CPOE and medical WfMS. The
multi-agent technology is at the basis of our proposition since (i) it provides
natural abstractions to deal with distribution, heterogeneity and autonomy
which are inherent to the previous systems (CDSS, CPOE and medical WfMS), and
(ii) it introduces powerful concepts such as organizations, goals and roles
useful to describe in details the coordination of the different components
involved in these systems. In this paper, we also propose a Multi-Agent System
(MAS) to support the coupling CDSS with CPOE. Finally, we show how we integrate
the proposed MAS in the medical workflow management system which is also based
on collaborating agents
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4428</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4428</id><created>2012-04-19</created><authors><author><keyname>Vieira</keyname><forenames>Marco</forenames></author><author><keyname>Gashi</keyname><forenames>Ilir</forenames></author></authors><title>EDCC 2012 - Fast Abstracts &amp; Student Forum Proceedings</title><categories>cs.SE</categories><comments>Ninth European Dependable Computing Conference - EDCC 2012, Sibiu,
  Romania, May 8-11, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast Abstracts at EDCC 2012 are short presentations, aiming to serve as a
rapid and flexible mechanism to report on current work that may or may not be
complete, introduce new ideas to the community, and state positions on
controversial issues or open problems. This way, fast abstracts provide an
opportunity to introduce new work, or present radical opinions, and receive
early feedback from the community. Contributions are welcome from both academia
and industry.
  The goal of the Student Forum is to encourage students to attend EDCC 2012
and present their work, exchange ideas with researchers and practitioners, and
get early feedback on their research efforts. All papers were peer-reviewed by
at least three program committee members, and the authors were provided with
detailed comments on their work. In the end we had one accepted paper for the
Student forum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4431</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4431</id><created>2012-04-19</created><authors><author><keyname>Aum&#xfc;ller</keyname><forenames>Martin</forenames></author><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author><author><keyname>Woelfel</keyname><forenames>Philipp</forenames></author></authors><title>Explicit and Efficient Hash Families Suffice for Cuckoo Hashing with a
  Stash</title><categories>cs.DS</categories><comments>18 Pages</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that for cuckoo hashing with a stash as proposed by Kirsch,
Mitzenmacher, and Wieder (2008) families of very simple hash functions can be
used, maintaining the favorable performance guarantees: with stash size $s$ the
probability of a rehash is $O(1/n^{s+1})$, and the evaluation time is $O(s)$.
Instead of the full randomness needed for the analysis of Kirsch et al. and of
Kutzelnigg (2010) (resp. $\Theta(\log n)$-wise independence for standard cuckoo
hashing) the new approach even works with 2-wise independent hash families as
building blocks. Both construction and analysis build upon the work of
Dietzfelbinger and Woelfel (2003). The analysis, which can also be applied to
the fully random case, utilizes a graph counting argument and is much simpler
than previous proofs. As a byproduct, an algorithm for simulating uniform
hashing is obtained. While it requires about twice as much space as the most
space efficient solutions, it is attractive because of its simple and direct
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4443</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4443</id><created>2012-04-19</created><updated>2012-04-22</updated><authors><author><keyname>Matz</keyname><forenames>Oliver</forenames></author></authors><title>First-Order Quantifiers and the Syntactic Monoid of Height Fragments of
  Picture Languages</title><categories>cs.FL</categories><comments>37 pages</comments><msc-class>03C13, 03C85, 20M35</msc-class><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the expressive power of first-order quantifications in the
context of monadic second-order logic over pictures. We show that k+1 set
quantifier alternations allow to define a picture language that cannot be
defined using k set quantifier alternations preceded by arbitrarily many
first-order quantifier alternations. The approach uses, for a given picture
language L and an integer m &gt; 0 the height-m fragment of L, which is defined as
the word language obtained by considering each picture p of height m in L as a
word, where the letters of that word are the columns of p. A key idea is to
measure the complexity of a regular word language by the group complexity of
its syntactic monoid. Given a picture language L, such a word language measure
may be applied to each of its height fragments, so that the complexity of the
picture language is a function that maps each m to the complexity of the
height-m fragment of L. The asymptotic growth rate of that function may be
bounded based on the structure of a monadic second-order formula that defines
L. The core argument for that lower bound proof is based on Straubing's
algebraic characterization of the effect of first-order quantifiers on the
syntactic monoid of word languages by means of Rhodes' and Tilson's block
product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4459</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4459</id><created>2012-04-19</created><authors><author><keyname>Tariq</keyname><forenames>Faisal</forenames></author><author><keyname>Dooley</keyname><forenames>Laurence S.</forenames></author><author><keyname>Poulton</keyname><forenames>Adrian S.</forenames></author></authors><title>An Interference-Aware Virtual Clustering Paradigm for Resource
  Management in Cognitive Femtocell Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Femtocells represent a promising alternative solution for high quality
wireless access in indoor scenarios where conventional cellular system coverage
can be poor. Femtocell access points (FAP) are normally randomly deployed by
the end user, so only post deployment network planning is possible.
Furthermore, this uncoordinated deployment creates the potential for severe
interference to co-located femtocells, especially in dense deployments. This
paper presents a new femtocell network architecture using a generalized virtual
cluster femtocell (GVCF) paradigm, which groups together FAP, which are
allocated to the same femtocell gateway (FGW), into logical clusters. This
guarantees severely interfering and overlapping femtocells are assigned to
different clusters, and since each cluster operates on a different band of
frequencies, the corresponding virtual cluster controller only has to manage
its own FAP members, so the overall system complexity is low. The performance
of the GVCF algorithm is analysed from both a resource availability and cluster
number perspective, and a novel strategy is proposed for dynamically adapting
these to network environment changes, while upholding quality-of-service
requirements. Simulation results conclusively corroborate the superior
performance of the GVCF model in interference mitigation, particularly in high
density FAP scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4465</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4465</id><created>2012-04-19</created><authors><author><keyname>Hou</keyname><forenames>I-Hong</forenames></author></authors><title>Providing End-to-End Delay Guarantees for Multi-hop Wireless Sensor
  Networks over Unreliable Channels</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks have been increasingly used for real-time
surveillance over large areas. In such applications, it is important to support
end-to-end delay constraints for packet deliveries even when the corresponding
flows require multi-hop transmissions. In addition to delay constraints, each
flow of real-time surveillance may require some guarantees on throughput of
packets that meet the delay constraints. Further, as wireless sensor networks
are usually deployed in challenging environments, it is important to
specifically consider the effects of unreliable wireless transmissions.
  In this paper, we study the problem of providing end-to-end delay guarantees
for multi-hop wireless networks. We propose a model that jointly considers the
end-to-end delay constraints and throughput requirements of flows, the need for
multi-hop transmissions, and the unreliable nature of wireless transmissions.
We develop a framework for designing feasibility-optimal policies. We then
demonstrate the utility of this framework by considering two types of systems:
one where sensors are equipped with full-duplex radios, and the other where
sensors are equipped with half-duplex radios. When sensors are equipped with
full-duplex radios, we propose an online distributed scheduling policy and
proves the policy is feasibility-optimal. We also provide a heuristic for
systems where sensors are equipped with half-duplex radios. We show that this
heuristic is still feasibility-optimal for some topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4467</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4467</id><created>2012-04-19</created><authors><author><keyname>Hou</keyname><forenames>I-Hong</forenames></author><author><keyname>Singh</keyname><forenames>Rahul</forenames></author></authors><title>Real-Time Stochastic Processing Networks with Concurrent Resource
  Requirements</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Processing Networks (SPNs) can be used to model communication
networks, manufacturing systems, service systems, etc. We consider a real-time
SPN where tasks generate jobs with strict deadlines according to their traffic
patterns. Each job requires the concurrent usage of some resources to be
processed. The processing time of a job may be stochastic, and may not be known
until the job completes. Finally, each task may require that some portion of
its tasks to be completed on time.
  In this paper, we study the problem of verifying whether it is feasible to
fulfill the requirements of tasks, and of designing scheduling policies that
actually fulfill the requirements. We first address these problems for systems
where there is only one resource. Such systems are analog to ones studied in a
previous work, and, similar to the previous work, we can develop sharp
conditions for feasibility and scheduling policy that is feasibility-optimal.
We then study systems with two resources where there are jobs that require both
resources to be processed. We show that there is a reduction method that turns
systems with two resources into equivalent single-resource systems. Based on
this method, we can also derive sharp feasibility conditions and
feasibility-optimal scheduling policies for systems with two resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4475</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4475</id><created>2012-04-19</created><authors><author><keyname>Neuberger</keyname><forenames>John M.</forenames></author><author><keyname>Sieben</keyname><forenames>Nandor</forenames></author><author><keyname>Swift</keyname><forenames>James W.</forenames></author></authors><title>An MPI Implementation of a Self-Submitting Parallel Job Queue</title><categories>cs.DC math.NA</categories><msc-class>65Y05, 35J60, 37G40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple and easy to apply methodology for using high-level
self-submitting parallel job queues in an MPI environment. Using C++, we
implemented a library of functions, MPQueue, both for testing our concepts and
for use in real applications. In particular, we have applied our ideas toward
solving computational combinatorics problems and for finding bifurcation
diagrams of solutions of partial differential equations (PDE). Our method is
general and can be applied in many situations without a lot of programming
effort. The key idea is that workers themselves can easily submit new jobs to
the currently running job queue. Our applications involve complicated data
structures, so we employ serialization to allow data to be effortlessly passed
between nodes. Using our library, one can solve large problems in parallel
without being an expert in MPI. We demonstrate our methodology and the features
of the library with several example programs, and give some results from our
current PDE research. We show that our techniques are efficient and effective
via overhead and scaling experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4476</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4476</id><created>2012-04-19</created><authors><author><keyname>Chaudhry</keyname><forenames>Rizwan</forenames></author><author><keyname>Hager</keyname><forenames>Gregory</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Dynamic Template Tracking and Recognition</title><categories>cs.CV cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of tracking non-rigid objects whose
local appearance and motion changes as a function of time. This class of
objects includes dynamic textures such as steam, fire, smoke, water, etc., as
well as articulated objects such as humans performing various actions. We model
the temporal evolution of the object's appearance/motion using a Linear
Dynamical System (LDS). We learn such models from sample videos and use them as
dynamic templates for tracking objects in novel videos. We pose the problem of
tracking a dynamic non-rigid object in the current frame as a maximum
a-posteriori estimate of the location of the object and the latent state of the
dynamical system, given the current image features and the best estimate of the
state in the previous frame. The advantage of our approach is that we can
specify a-priori the type of texture to be tracked in the scene by using
previously trained models for the dynamics of these textures. Our framework
naturally generalizes common tracking methods such as SSD and kernel-based
tracking from static templates to dynamic templates. We test our algorithm on
synthetic as well as real examples of dynamic textures and show that our simple
dynamics-based trackers perform at par if not better than the state-of-the-art.
Since our approach is general and applicable to any image feature, we also
apply it to the problem of human action tracking and build action-specific
optical flow trackers that perform better than the state-of-the-art when
tracking a human performing a particular action. Finally, since our approach is
generative, we can use a-priori trained trackers for different texture or
action classes to simultaneously track and recognize the texture or action in
the video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4491</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4491</id><created>2012-04-19</created><updated>2013-01-22</updated><authors><author><keyname>Nguyen</keyname><forenames>Huy</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author></authors><title>On Budgeted Influence Maximization in Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Submitted to JSAC NS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a budget and arbitrary cost for selecting each node, the budgeted
influence maximization (BIM) problem concerns selecting a set of seed nodes to
disseminate some information that maximizes the total number of nodes
influenced (termed as influence spread) in social networks at a total cost no
more than the budget. Our proposed seed selection algorithm for the BIM problem
guarantees an approximation ratio of (1 - 1/sqrt(e)). The seed selection
algorithm needs to calculate the influence spread of candidate seed sets, which
is known to be #P-complex. Identifying the linkage between the computation of
marginal probabilities in Bayesian networks and the influence spread, we devise
efficient heuristic algorithms for the latter problem. Experiments using both
large-scale social networks and synthetically generated networks demonstrate
superior performance of the proposed algorithm with moderate computation costs.
Moreover, synthetic datasets allow us to vary the network parameters and gain
important insights on the impact of graph structures on the performance of
different algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4497</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4497</id><created>2012-04-19</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng-Jun</forenames></author></authors><title>Ranking spreaders by decomposing complex networks</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>6 pages, 5 figures</comments><journal-ref>Phys. Lett. A 377, 1031 (2013)</journal-ref><doi>10.1016/j.physleta.2013.02.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ranking the nodes' ability for spreading in networks is a fundamental problem
which relates to many real applications such as information and disease
control. In the previous literatures, a network decomposition procedure called
k-shell method has been shown to effectively identify the most influential
spreaders. In this paper, we find that the k-shell method have some limitations
when it is used to rank all the nodes in the network. We also find that these
limitations are due to considering only the links between the remaining nodes
(residual degree) while entirely ignoring all the links connecting to the
removed nodes (exhausted degree) when decomposing the networks. Accordingly, we
propose a mixed degree decomposition (MDD) procedure in which both the residual
degree and the exhausted degree are considered. By simulating the epidemic
process on the real networks, we show that the MDD method can outperform the
k-shell and the degree methods in ranking spreaders. Finally, the influence of
the network structure on the performance of the MDD method is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4498</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4498</id><created>2012-04-19</created><authors><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author></authors><title>Diversity Loss due to Interference Correlation</title><categories>cs.IT cs.NI math.IT math.PR</categories><comments>4 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference in wireless systems is both temporally and spatially correlated.
Yet very little research has analyzed the effect of such correlation. Here we
focus on its impact on the diversity in Poisson networks with multi-antenna
receivers. Most work on multi-antenna communication does not consider
interference, and if it is included, it is assumed independent across the
receive antennas. Here we show that interference correlation significantly
reduces the probability of successful reception over SIMO links. The diversity
loss is quantified via the diversity polynomial. For the two-antenna case, we
provide the complete joint SIR distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4509</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4509</id><created>2012-04-19</created><updated>2012-04-24</updated><authors><author><keyname>Nekrich</keyname><forenames>Yakov</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author></authors><title>Sorted Range Reporting</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a variant of the orthogonal range reporting problem
when all points should be reported in the sorted order of their
$x$-coordinates. We show that reporting two-dimensional points with this
additional condition can be organized (almost) as efficiently as the standard
range reporting.
  Moreover, our results generalize and improve the previously known results for
the orthogonal range successor problem and can be used to obtain better
solutions for some stringology problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4518</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4518</id><created>2012-04-19</created><authors><author><keyname>Lertwiram</keyname><forenames>Namzilp</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author></authors><title>A Study of Trade-off between Opportunistic Resource Allocation and
  Interference Alignment in Femtocell Scenarios</title><categories>cs.IT math.IT</categories><comments>This paper is submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main problems in wireless heterogeneous networks is interference
between macro- and femto-cells. Using Orthogonal Frequency-Division Multiple
Access (OFDMA) to create multiple frequency orthogonal sub-channels, this
interference can be completely avoided if each sub-channel is exclusively used
by either macro- or a femto-cell. However, such an orthogonal allocation may be
inefficient. We consider two alternative strategies for interference
management, opportunistic resource allocation (ORA) and interference alignment
(IA). Both of them utilize the fading fluctuations across frequency channels in
different ways. ORA allows the users to interfere, but selecting the channels
where the interference is faded, while the desired signal has a good channel.
IA uses precoding to create interference-free transmissions; however, such a
precoding changes the diversity picture of the communication resources. In this
letter we investigate the interactions and the trade-offs between these two
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4521</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4521</id><created>2012-04-19</created><authors><author><keyname>Acharya</keyname><forenames>Ayan</forenames></author><author><keyname>Hruschka</keyname><forenames>Eduardo R.</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author></authors><title>A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster
  Ensembles</title><categories>cs.LG cs.CV stat.ML</categories><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a privacy-aware Bayesian approach that combines
ensembles of classifiers and clusterers to perform semi-supervised and
transductive learning. We consider scenarios where instances and their
classification/clustering results are distributed across different data sites
and have sharing restrictions. As a special case, the privacy aware computation
of the model when instances of the target data are distributed across different
data sites, is also discussed. Experimental results show that the proposed
approach can provide good classification accuracies while adhering to the
data/model sharing constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4526</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4526</id><created>2012-04-19</created><updated>2013-11-19</updated><authors><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author><author><keyname>Ward</keyname><forenames>Justin</forenames></author></authors><title>A Tight Combinatorial Algorithm for Submodular Maximization Subject to a
  Matroid Constraint</title><categories>cs.DS</categories><msc-class>68W25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an optimal, combinatorial 1-1/e approximation algorithm for
monotone submodular optimization over a matroid constraint. Compared to the
continuous greedy algorithm (Calinescu, Chekuri, Pal and Vondrak, 2008), our
algorithm is extremely simple and requires no rounding. It consists of the
greedy algorithm followed by local search. Both phases are run not on the
actual objective function, but on a related non-oblivious potential function,
which is also monotone submodular. Our algorithm runs in randomized time
O(n^8u), where n is the rank of the given matroid and u is the size of its
ground set. We additionally obtain a 1-1/e-eps approximation algorithm running
in randomized time O (eps^-3n^4u). For matroids in which n = o(u), this
improves on the runtime of the continuous greedy algorithm. The improvement is
due primarily to the time required by the pipage rounding phase, which we avoid
altogether. Furthermore, the independence of our algorithm from pipage rounding
techniques suggests that our general approach may be helpful in contexts such
as monotone submodular maximization subject to multiple matroid constraints.
  Our approach generalizes to the case where the monotone submodular function
has restricted curvature. For any curvature c, we adapt our algorithm to
produce a (1-e^-c)/c approximation. This result complements results of Vondrak
(2008), who has shown that the continuous greedy algorithm produces a
(1-e^-c)/c approximation when the objective function has curvature c. He has
also proved that achieving any better approximation ratio is impossible in the
value oracle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4528</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4528</id><created>2012-04-20</created><authors><author><keyname>Saito</keyname><forenames>Kazumi</forenames></author><author><keyname>Kimura</keyname><forenames>Masahiro</forenames></author><author><keyname>Ohara</keyname><forenames>Kouzou</forenames></author><author><keyname>Motoda</keyname><forenames>Hiroshi</forenames></author></authors><title>Learning Asynchronous-Time Information Diffusion Models and its
  Application to Behavioral Data Analysis over Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>39 pages, 55 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the interesting and important problems of information diffusion over a
large social network is to identify an appropriate model from a limited amount
of diffusion information. There are two contrasting approaches to model
information diffusion: a push type model known as Independent Cascade (IC)
model and a pull type model known as Linear Threshold (LT) model. We extend
these two models (called AsIC and AsLT in this paper) to incorporate
asynchronous time delay and investigate 1) how they differ from or similar to
each other in terms of information diffusion, 2) whether the model itself is
learnable or not from the observed information diffusion data, and 3) which
model is more appropriate to explain for a particular topic (information) to
diffuse/propagate. We first show there can be variations with respect to how
the time delay is modeled, and derive the likelihood of the observed data being
generated for each model. Using one particular time delay model, we show the
model parameters are learnable from a limited amount of observation. We then
propose a method based on predictive accuracy by which to select a model which
better explains the observed data. Extensive evaluations were performed. We
first show using synthetic data with the network structures taken from real
networks that there are considerable behavioral differences between the AsIC
and the AsLT models, the proposed methods accurately and stably learn the model
parameters, and identify the correct diffusion model from a limited amount of
observation data. We next apply these methods to behavioral analysis of topic
propagation using the real blog propagation data, and show there is a clear
indication as to which topic better follows which model although the results
are rather insensitive to the model selected at the level of discussing how far
and fast each topic propagates from the learned parameter values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4535</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4535</id><created>2012-04-20</created><updated>2012-05-03</updated><authors><author><keyname>Shah</keyname><forenames>Jay</forenames></author><author><keyname>Mahalanobis</keyname><forenames>Ayan</forenames></author></authors><title>A New Guess-and-Determine Attack on the A5/1 Stream Cipher</title><categories>cs.CR</categories><comments>14 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Europe and North America, the most widely used stream cipher to ensure
privacy and confidentiality of conversations in GSM mobile phones is the A5/1.
In this paper, we present a new attack on the A5/1 stream cipher with an
average time complexity of 2^(48.5), which is much less than the brute-force
attack with a complexity of 2^(64). The attack has a 100% success rate and
requires about 5.65GB storage. We provide a detailed description of our new
attack along with its implementation and results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4539</identifier>
 <datestamp>2013-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4539</id><created>2012-04-20</created><updated>2013-08-29</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>Supervised Feature Selection in Graphs with Path Coding Penalties and
  Network Flows</title><categories>stat.ML cs.LG math.OC</categories><comments>37 pages; to appear in the Journal of Machine Learning Research
  (JMLR)</comments><journal-ref>Journal of Machine Learning Research 14(Aug) (2013) 2449-2485</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider supervised learning problems where the features are embedded in a
graph, such as gene expressions in a gene network. In this context, it is of
much interest to automatically select a subgraph with few connected components;
by exploiting prior knowledge, one can indeed improve the prediction
performance or obtain results that are easier to interpret. Regularization or
penalty functions for selecting features in graphs have recently been proposed,
but they raise new algorithmic challenges. For example, they typically require
solving a combinatorially hard selection problem among all connected subgraphs.
In this paper, we propose computationally feasible strategies to select a
sparse and well-connected subset of features sitting on a directed acyclic
graph (DAG). We introduce structured sparsity penalties over paths on a DAG
called &quot;path coding&quot; penalties. Unlike existing regularization functions that
model long-range interactions between features in a graph, path coding
penalties are tractable. The penalties and their proximal operators involve
path selection problems, which we efficiently solve by leveraging network flow
optimization. We experimentally show on synthetic, image, and genomic data that
our approach is scalable and leads to more connected subgraphs than other
regularization functions for graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4541</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4541</id><created>2012-04-20</created><authors><author><keyname>Taillandier</keyname><forenames>Patrick</forenames><affiliation>UMMISCO</affiliation></author><author><keyname>Gaffuri</keyname><forenames>Julien</forenames><affiliation>COGIT</affiliation></author></authors><title>Automatic Sampling of Geographic objects</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>GIScience, Zurich : Switzerland (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, one's disposes of large datasets composed of thousands of geographic
objects. However, for many processes, which require the appraisal of an expert
or much computational time, only a small part of these objects can be taken
into account. In this context, robust sampling methods become necessary. In
this paper, we propose a sampling method based on clustering techniques. Our
method consists in dividing the objects in clusters, then in selecting in each
cluster, the most representative objects. A case-study in the context of a
process dedicated to knowledge revision for geographic data generalisation is
presented. This case-study shows that our method allows to select relevant
samples of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4550</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4550</id><created>2012-04-20</created><updated>2012-06-14</updated><authors><author><keyname>Kaniezhil</keyname><forenames>R.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>C.</forenames></author><author><keyname>NithyaRekha</keyname><forenames>S.</forenames></author></authors><title>Performance Evaluation of QoS Parameters in Dynamic Spectrum Sharing for
  Heterogeneous Wireless Communication Networks</title><categories>cs.NI</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 9, Issue
  1, No 2, January 2012 ISSN (Online): 1694-0814 http://www.IJCSI.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio nodes have been proposed as means to improve the spectrum
utilization. It reuses the spectrum of a primary service provider under the
condition that the primary service provider services are not harmfully
interrupted. A cognitive radio can sense its operating environment's conditions
and it is able to reconfigure itself and to communicate with other counterparts
based on the status of the environment and also the requirements of the user to
meet the optimal communication conditions and to keep quality of service (QoS)
as high as possible. The efficiency of spectrum sharing can be improved by
minimizing the interference. The Utility function that captures the cooperative
behavior to minimize the interference and the satisfaction to improve the
throughput is investigated. The dynamic spectrum sharing algorithm can maintain
the quality of service (QoS) of each network while the effective spectrum
utilisation is improved under a fluctuation traffic environment when the
available spectrum is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4560</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4560</id><created>2012-04-20</created><authors><author><keyname>Wagner</keyname><forenames>Markus</forenames></author><author><keyname>Day</keyname><forenames>Jareth</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>A Fast and Effective Local Search Algorithm for Optimizing the Placement
  of Wind Turbines</title><categories>cs.NE</categories><comments>16 pages, 2 algorithms, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The placement of wind turbines on a given area of land such that the wind
farm produces a maximum amount of energy is a challenging optimization problem.
In this article, we tackle this problem, taking into account wake effects that
are produced by the different turbines on the wind farm. We significantly
improve upon existing results for the minimization of wake effects by
developing a new problem-specific local search algorithm. One key step in the
speed-up of our algorithm is the reduction in computation time needed to assess
a given wind farm layout compared to previous approaches. Our new method allows
the optimization of large real-world scenarios within a single night on a
standard computer, whereas weeks on specialized computing servers were required
for previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4562</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4562</id><created>2012-04-20</created><authors><author><keyname>gharibi</keyname><forenames>Wajeb</forenames></author><author><keyname>Xia</keyname><forenames>Yong</forenames></author></authors><title>A Tight Linearization Strategy for Zero-One Quadratic Programming
  Problems</title><categories>cs.DS math.OC</categories><comments>6 pages</comments><journal-ref>International Journal of Computer Science Issues, IJCSI Volume 9,
  Issue 2, March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new approach to linearizing zero-one quadratic
minimization problem which has many applications in computer science and
communications. Our algorithm is based on the observation that the quadratic
term of zero-one variables has two equivalent piece-wise formulations, convex
and concave cases. The convex piece-wise objective function and/or constraints
play a great role in deducing small linearization. Further tight strategies are
also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4563</identifier>
 <datestamp>2013-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4563</id><created>2012-04-20</created><updated>2012-07-18</updated><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames><affiliation>INRIA Saclay - Ile de France, INT - University of Ulm.</affiliation></author><author><keyname>Bezzateev</keyname><forenames>Sergey</forenames><affiliation>SUAI</affiliation></author></authors><title>Describing A Cyclic Code by Another Cyclic Code</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><journal-ref>IEEE International Symposium on Information Theory (ISIT) (2012)
  2896-2900</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach to bound the minimum distance of $q$-ary cyclic codes is
presented. The connection to the BCH and the Hartmann--Tzeng bound is
formulated and it is shown that for several cases an improvement is achieved.
We associate a second cyclic code to the original one and bound its minimum
distance in terms of parameters of the associated code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4564</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4564</id><created>2012-04-20</created><authors><author><keyname>Javelle</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Mhalla</keyname><forenames>Mehdi</forenames></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames></author></authors><title>On the Minimum Degree up to Local Complementation: Bounds and Complexity</title><categories>cs.CC quant-ph</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local minimum degree of a graph is the minimum degree reached by means of
a series of local complementations. In this paper, we investigate on this
quantity which plays an important role in quantum computation and quantum error
correcting codes. First, we show that the local minimum degree of the Paley
graph of order p is greater than sqrt{p} - 3/2, which is, up to our knowledge,
the highest known bound on an explicit family of graphs. Probabilistic methods
allows us to derive the existence of an infinite number of graphs whose local
minimum degree is linear in their order with constant 0.189 for graphs in
general and 0.110 for bipartite graphs. As regards the computational complexity
of the decision problem associated with the local minimum degree, we show that
it is NP-complete and that there exists no k-approximation algorithm for this
problem for any constant k unless P = NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4565</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4565</id><created>2012-04-20</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, IUF</affiliation></author><author><keyname>Zhu</keyname><forenames>Nini</forenames><affiliation>LIP6</affiliation></author></authors><title>Mariages et Trahisons</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>14\`emes Rencontres Francophones sur les Aspects Algorithmiques
  des T\'el\'ecommunications (AlgoTel), France (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A self-stabilizing protocol tolerates by definition transient faults (faults
of finite duration). Recently, a new class of self-stabilizing protocols that
are able to tolerate a given number of permanent faults. In this paper, we
focus on self-stabilizing protocols able to tolerate Byzantine faults, that is
faults that introduce an arbitrary behaviour. We focus on strict-stabilization
in which the system have to contain the effects of Byzantine faults.
Specificaly, we study the possibility to construct in a self-stabilizing way a
maximal matching in a network where an arbitrary number of process may become
Byzantine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4578</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4578</id><created>2012-04-20</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Podolskii</keyname><forenames>Vladimir V.</forenames></author></authors><title>Complexity of tropical and min-plus linear prevarieties</title><categories>cs.CC math.AG</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tropical (or min-plus) semiring is a set $\mathbb{Z}$ (or $\mathbb{Z \cup
\{\infty\}}$) endowed with two operations: $\oplus$, which is just usual
minimum, and $\odot$, which is usual addition. In tropical algebra the vector
$x$ is a solution to a polynomial $g_1(x) \oplus g_2(x) \oplus...\oplus
g_k(x)$, where $g_i(x)$'s are tropical monomials, if the minimum in
$\min_i(g_{i}(x))$ is attained at least twice. In min-plus algebra solutions of
systems of equations of the form $g_1(x)\oplus...\oplus g_k(x) =
h_1(x)\oplus...\oplus h_l(x)$ are studied.
  In this paper we consider computational problems related to tropical linear
system. We show that the solvability problem (both over $\mathbb{Z}$ and
$\mathbb{Z} \cup \{\infty\}$) and the problem of deciding the equivalence of
two linear systems (both over $\mathbb{Z}$ and $\mathbb{Z} \cup \{\infty\}$)
are equivalent under polynomial-time reduction to mean payoff games and are
also equivalent to analogous problems in min-plus algebra. In particular, all
these problems belong to $\mathsf{NP} \cap \mathsf{coNP}$. Thus we provide a
tight connection of computational aspects of tropical linear algebra with mean
payoff games and min-plus linear algebra. On the other hand we show that
computing the dimension of the solution space of a tropical linear system and
of a min-plus linear system are $\mathsf{NP}$-complete.
  We also extend some of our results to the systems of min-plus linear
inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4584</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4584</id><created>2012-04-20</created><updated>2012-12-19</updated><authors><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Mar&#xed;n</keyname><forenames>Ignacio</forenames></author></authors><title>Jerarca: Efficient Analysis of Complex Networks Using Hierarchical
  Clustering</title><categories>q-bio.MN cs.SI physics.soc-ph</categories><comments>23 pages, 3 figures</comments><journal-ref>PLoS ONE 5(7): e11585 (2010)</journal-ref><doi>10.1371/journal.pone.0011585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: How to extract useful information from complex biological
networks is a major goal in many fields, especially in genomics and proteomics.
We have shown in several works that iterative hierarchical clustering, as
implemented in the UVCluster program, is a powerful tool to analyze many of
those networks. However, the amount of computation time required to perform
UVCluster analyses imposed significant limitations to its use.
  Methodology/Principal Findings: We describe the suite Jerarca, designed to
efficiently convert networks of interacting units into dendrograms by means of
iterative hierarchical clustering. Jerarca is divided into three main sections.
First, weighted distances among units are computed using up to three different
approaches: a more efficient version of UVCluster and two new, related
algorithms called RCluster and SCluster. Second, Jerarca builds dendrograms
based on those distances, using well-known phylogenetic algorithms, such as
UPGMA or Neighbor-Joining. Finally, Jerarca provides optimal partitions of the
trees using statistical criteria based on the distribution of intra- and
intercluster connections. Outputs compatible with the phylogenetic software
MEGA and the Cytoscape package are generated, allowing the results to be easily
visualized.
  Conclusions/Significance: The four main advantages of Jerarca in respect to
UVCluster are: 1) Improved speed of a novel UVCluster algorithm; 2) Additional,
alternative strategies to perform iterative hierarchical clustering; 3)
Automatic evaluation of the hierarchical trees to obtain optimal partitions;
and, 4) Outputs compatible with popular software such as MEGA and Cytoscape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4585</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4585</id><created>2012-04-20</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author></authors><title>An Information Theoretic Location Verification System for Wireless
  Networks</title><categories>cs.IT math.IT</categories><doi>10.1109/GLOCOM.2012.6503982</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As location-based applications become ubiquitous in emerging wireless
networks, Location Verification Systems (LVS) are of growing importance. In
this paper we propose, for the first time, a rigorous information-theoretic
framework for an LVS. The theoretical framework we develop illustrates how the
threshold used in the detection of a spoofed location can be optimized in terms
of the mutual information between the input and output data of the LVS. In
order to verify the legitimacy of our analytical framework we have carried out
detailed numerical simulations. Our simulations mimic the practical scenario
where a system deployed using our framework must make a binary Yes/No
&quot;malicious decision&quot; to each snapshot of the signal strength values obtained by
base stations. The comparison between simulation and analysis shows excellent
agreement. Our optimized LVS framework provides a defence against location
spoofing attacks in emerging wireless networks such as those envisioned for
Intelligent Transport Systems, where verification of location information is of
paramount importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4596</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4596</id><created>2012-04-20</created><authors><author><keyname>Ivanyos</keyname><forenames>Gabor</forenames></author><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>New bounds on the classical and quantum communication complexity of some
  graph properties</title><categories>quant-ph cs.CC</categories><comments>12 pages LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the communication complexity of a number of graph properties where
the edges of the graph $G$ are distributed between Alice and Bob (i.e., each
receives some of the edges as input). Our main results are:
  * An Omega(n) lower bound on the quantum communication complexity of deciding
whether an n-vertex graph G is connected, nearly matching the trivial classical
upper bound of O(n log n) bits of communication.
  * A deterministic upper bound of O(n^{3/2}log n) bits for deciding if a
bipartite graph contains a perfect matching, and a quantum lower bound of
Omega(n) for this problem.
  * A Theta(n^2) bound for the randomized communication complexity of deciding
if a graph has an Eulerian tour, and a Theta(n^{3/2}) bound for the quantum
communication complexity of this problem.
  The first two quantum lower bounds are obtained by exhibiting a reduction
from the n-bit Inner Product problem to these graph problems, which solves an
open question of Babai, Frankl and Simon. The third quantum lower bound comes
from recent results about the quantum communication complexity of composed
functions. We also obtain essentially tight bounds for the quantum
communication complexity of a few other problems, such as deciding if G is
triangle-free, or if G is bipartite, as well as computing the determinant of a
distributed matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4619</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4619</id><created>2012-04-20</created><updated>2012-09-25</updated><authors><author><keyname>Klauck</keyname><forenames>Hartmut</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>Fooling One-Sided Quantum Protocols</title><categories>quant-ph cs.CC</categories><comments>10 pages LaTeX. Version: corrected an error in Section 2, slightly
  weakening the result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the venerable &quot;fooling set&quot; method to prove new lower bounds on the
quantum communication complexity of various functions. Let f:X x Y--&gt;{0,1} be a
Boolean function, fool^1(f) its maximal fooling set size among 1-inputs,
Q_1^*(f) its one-sided error quantum communication complexity with prior
entanglement, and NQ(f) its nondeterministic quantum communication complexity
(without prior entanglement; this model is trivial with shared randomness or
entanglement). Our main results are the following, where logs are to base 2:
  * If the maximal fooling set is &quot;upper triangular&quot; (which is for instance the
case for the equality, disjointness, and greater-than functions), then we have
Q_1^*(f)&gt;=(1/2)log fool^1(f) - 1/2, which is essentially optimal by superdense
coding. No super-constant lower bound for equality seems to follow from earlier
techniques.
  * For all f we have Q_1^*(f)&gt;=(1/4)log fool^1(f) - 1/2, which is optimal up
to a factor of 2.
  * NQ(f)&gt;=log \fool^1(f)/2 + 1. We do not know if the factor 1/2 is needed in
this result, but it cannot be replaced by 1: we give an example where
NQ(f)~0.613 log fool^1(f).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4626</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4626</id><created>2012-04-20</created><authors><author><keyname>Barbotin</keyname><forenames>Y.</forenames></author><author><keyname>Vetterli</keyname><forenames>M.</forenames></author></authors><title>Fast and Robust Parametric Estimation of Jointly Sparse Channels</title><categories>cs.SY</categories><comments>11 pages, 9 figures, submitted to IEEE JETCAS special issue on
  Compressed Sensing, Sep. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the joint estimation of multipath channels obtained with a set of
receiving antennas and uniformly probed in the frequency domain. This scenario
fits most of the modern outdoor communication protocols for mobile access or
digital broadcasting among others.
  Such channels verify a Sparse Common Support property (SCS) which was used in
a previous paper to propose a Finite Rate of Innovation (FRI) based sampling
and estimation algorithm. In this contribution we improve the robustness and
computational complexity aspects of this algorithm. The method is based on
projection in Krylov subspaces to improve complexity and a new criterion called
the Partial Effective Rank (PER) to estimate the level of sparsity to gain
robustness.
  If P antennas measure a K-multipath channel with N uniformly sampled
measurements per channel, the algorithm possesses an O(KPNlogN) complexity and
an O(KPN) memory footprint instead of O(PN^3) and O(PN^2) for the direct
implementation, making it suitable for K &lt;&lt; N. The sparsity is estimated online
based on the PER, and the algorithm therefore has a sense of introspection
being able to relinquish sparsity if it is lacking. The estimation performances
are tested on field measurements with synthetic AWGN, and the proposed
algorithm outperforms non-sparse reconstruction in the medium to low SNR range
(&lt; 0dB), increasing the rate of successful symbol decodings by 1/10th in
average, and 1/3rd in the best case. The experiments also show that the
algorithm does not perform worse than a non-sparse estimation algorithm in
non-sparse operating conditions, since it may fall-back to it if the PER
criterion does not detect a sufficient level of sparsity.
  The algorithm is also tested against a method assuming a &quot;discrete&quot; sparsity
model as in Compressed Sensing (CS). The conducted test indicates a trade-off
between speed and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4639</identifier>
 <datestamp>2012-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4639</id><created>2012-04-20</created><authors><author><keyname>Lonati</keyname><forenames>Violetta</forenames></author><author><keyname>Mandrioli</keyname><forenames>Dino</forenames></author><author><keyname>Pradella</keyname><forenames>Matteo</forenames></author></authors><title>Logic Characterization of Floyd Languages</title><categories>cs.FL cs.LO</categories><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Floyd languages (FL), alias Operator Precedence Languages, have recently
received renewed attention thanks to their closure properties and local
parsability which allow one to apply automatic verification techniques (e.g.
model checking) and parallel and incremental parsing. They properly include
various other classes, noticeably Visual Pushdown languages. In this paper we
provide a characterization of FL in terms a monadic second order logic (MSO),
in the same style as Buchi's one for regular languages. We prove the
equivalence between automata recognizing FL and the MSO formalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1204.4647</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1204.4647</id><created>2012-04-20</created><updated>2013-09-30</updated><authors><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>Hanawal</keyname><forenames>Manjesh Kumar</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Regulation of off-network pricing in a nonneutral network</title><categories>cs.NI cs.GT</categories><comments>38 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representatives of several Internet service providers (ISPs) have expressed
their wish to see a substantial change in the pricing policies of the Internet.
In particular, they would like to see content providers (CPs) pay for use of
the network, given the large amount of resources they use. This would be in
clear violation of the &quot;network neutrality&quot; principle that had characterized
the development of the wireline Internet. Our first goal in this paper is to
propose and study possible ways of implementing such payments and of regulating
their amount. We introduce a model that includes the users' behavior, the
utilities of the ISP and of the CPs, and the monetary flow that involves the
content users, the ISP and CP, and in particular, the CP's revenues from
advertisements. We consider various game models and study the resulting
equilibria; they are all combinations of a noncooperative game (in which the
ISPs and CPs determine how much they will charge the users) with a
&quot;cooperative&quot; one on how the CP and the ISP share the payments. We include in
our model a possible asymmetric weighting parameter (that varies between zero
to one). We also study equilibria that arise when one of the CPs colludes with
the ISP. We also study two dynamic game models and study the convergence of
prices to the equilibrium values.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="30000" completeListSize="102538">1122234|31001</resumptionToken>
</ListRecords>
</OAI-PMH>
